arxiv_id,paper_type,period,year,month,pangram_prediction
2001.10386,regular,pre_llm,2020,1,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Taking Recoveries to Task: Recovery-Driven Development for Recipe-based\n  Robot Tasks\n\n  Robot task execution when situated in real-world environments is fragile. As\nsuch, robot architectures must rely on robust error recovery, adding\nnon-trivial complexity to highly-complex robot systems. To handle this\ncomplexity in development, we introduce Recovery-Driven Development (RDD), an\niterative task scripting process that facilitates rapid task and recovery\ndevelopment by leveraging hierarchical specification, separation of nominal\ntask and recovery development, and situated testing. We validate our approach\nwith our challenge-winning mobile manipulator software architecture developed\nusing RDD for the FetchIt! Challenge at the IEEE 2019 International Conference\non Robotics and Automation. We attribute the success of our system to the level\nof robustness achieved using RDD, and conclude with lessons learned for\ndeveloping such systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.09981,regular,pre_llm,2020,1,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Designing a Socially Assistive Robot for Long-Term In-Home Use for\n  Children with Autism Spectrum Disorders\n\n  Socially assistive robotics (SAR) research has shown great potential for\nsupplementing and augmenting therapy for children with autism spectrum\ndisorders (ASD). However, the vast majority of SAR research has been limited to\nshort-term studies in highly controlled environments. The design and\ndevelopment of a SAR system capable of interacting autonomously {\\it in situ}\nfor long periods of time involves many engineering and computing challenges.\nThis paper presents the design of a fully autonomous SAR system for long-term,\nin-home use with children with ASD. We address design decisions based on\nrobustness and adaptability needs, discuss the development of the robot's\ncharacter and interactions, and provide insights from the month-long, in-home\ndata collections with children with ASD. This work contributes to a larger\nresearch program that is exploring how SAR can be used for enhancing the social\nand cognitive development of children with ASD.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.01011,regular,pre_llm,2020,1,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Simulating Ankle Torque during Walking Using a new Bioinspired Muscle\n  Model with Application for Controlling a Powered Exoskeleton\n\n  Human-like motion is a primary goal for many robotic assistive devices.\nEmulating the strategy of the human neuromuscular system may aid the control of\nsuch powered devices, yet many challenges remain. In this study, we\ninvestigated the potential for using the winding filament model (WFM) of muscle\nto predict the net muscle moment of the ankle. The long-term goal is to use\nthis model to improve ankle control of a commercial powered exoskeleton. The\ninnovation aspects of this study are: First, there have been no commercialized\nactive ankle exoskeletons available in the market. All the available\nexoskeletons have passive ankle joints, which cannot mimic human movement,\nespecially in normal and fast walking [1]. Second, the Winding Filament Model\nController (WFMC) is the first control strategy based on a muscle model that\ndoes not use an electromyographic (EMG) signal as an input. The activation,\nwhich is calculated from EMG, is a crucial input parameter for almost all of\nthe control strategies based on muscle modeling. However, the winding filament\nmuscle model can predict muscle force by using muscle length as the primary\ninput, and the activation input signal could be either a square wave [33] or a\nsimple bell shape function like our study. This is one of the most important\nbenefits of the WFMC strategy, since it makes this bioinspired strategy\napplicable for all patient populations, even those with impaired muscle\nactivities or without muscle activities (e.g. stroke, spinal cord injury,\nParkinson disease, etc.). Third, the WFMC is adaptive to different tasks like\nwalking at different speeds, as well as walking over the incline and over\nstairs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.08042,regular,pre_llm,2020,1,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Planning an Efficient and Robust Base Sequence for a Mobile Manipulator\n  Performing Multiple Pick-and-place Tasks\n\n  In this paper, we address efficiently and robustly collecting objects stored\nin different trays using a mobile manipulator. A resolution complete method,\nbased on precomputed reachability database, is proposed to explore\ncollision-free inverse kinematics (IK) solutions and then a resolution complete\nset of feasible base positions can be determined. This method approximates a\nset of representative IK solutions that are especially helpful when solving IK\nand checking collision are treated separately. For real world applications, we\ntake into account the base positioning uncertainty and plan a sequence of base\npositions that reduce the number of necessary base movements for collecting the\ntarget objects, the base sequence is robust in that the mobile manipulator is\nable to complete the part-supply task even there is certain deviation from the\nplanned base positions. Our experiments demonstrate both the efficiency\ncompared to regular base sequence and the feasibility in real world\napplications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.04397,regular,pre_llm,2020,1,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'SMT-based Robot Transition Repair\n\n  State machines are a common model for robot behaviors. Transition functions\noften rely on parameterized conditions to model preconditions for the\ncontrollers, where the correct values of the parameters depend on factors\nrelating to the environment or the specific robot. In the absence of specific\ncalibration procedures a roboticist must painstakingly adjust the parameters\nthrough a series of trial and error experiments. In this process, identifying\nwhen the robot has taken an incorrect action, and what should be done is\nstraightforward, but finding the right parameter values can be difficult. We\npresent an alternative approach that we call, interactive SMT-based Robot\nTransition Repair. During execution we record an execution trace of the\ntransition function, and we ask the roboticist to identify a few instances\nwhere the robot has transitioned incorrectly, and what the correct transition\nshould have been. A user supplies these corrections based on the type of error\nto repair, and an automated analysis of the traces partially evaluates the\ntransition function for each correction. This system of constraints is then\nformulated as a MaxSMT problem, where the solution is a minimal adjustment to\nthe parameters that satisfies the maximum number of constraints. In order to\nidentify a repair that accurately captures user intentions and generalizes to\nnovel scenarios, solutions are explored by iteratively adding constraints to\nthe MaxSMT problem to yield sets of alternative repairs. We test with state\nmachines from multiple domains including robot soccer and autonomous driving,\nand we evaluate solver based repair with respect to solver choice and\noptimization hyperparameters. Our results demonstrate that SRTR can repair a\nvariety of states machines and error types 1) quickly, 2) with small numbers of\ncorrections, while 3) not overcorrecting state machines and harming generalized\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.03605,regular,pre_llm,2020,1,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Real-Time Long Range Trajectory Replanning for MAVs in the Presence of\n  Dynamic Obstacles\n\n  Real-time long-range local planning is a challenging task, especially in the\npresence of dynamics obstacles. We propose a complete system which is capable\nof performing the local replanning in real-time. Desired trajectory is needed\nin the system initialization phase; system starts initializing sub-components\nof the system including point cloud processor, trajectory estimator and\nplanner. Afterwards, the multi-rotary aerial vehicle starts moving on the given\ntrajectory. When it detects obstacles, it replans the trajectory from the\ncurrent pose to pre-defined distance incorporating the desired trajectory.\nPoint cloud processor is employed to identify the closest obstacles around the\nvehicle. For replanning, Rapidly-exploring Random Trees (RRT*) is used with two\nmodifications which allow planning the trajectory in milliseconds scales. Once\nwe replanned the desired path, velocity components(x,y and z) and yaw rate are\ncalculated. Those values are sent to the controller at a constant frequency to\nmaneuver the vehicle autonomously. Finally, we have evaluated each of the\ncomponents separately and tested the complete system in the simulated and real\nenvironments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.08622,regular,pre_llm,2020,1,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'AprilTags 3D: Dynamic Fiducial Markers for Robust Pose Estimation in\n  Highly Reflective Environments and Indirect Communication in Swarm Robotics\n\n  Although fiducial markers give an accurate pose estimation in laboratory\nconditions, where the noisy factors are controlled, using them in field robotic\napplications remains a challenge. This is constrained to the fiducial maker\nsystems, since they only work within the RGB image space. As a result, noises\nin the image produce large pose estimation errors. In robotic applications,\nfiducial markers have been mainly used in its original and simple form, as a\nplane in a printed paper sheet. This setup is sufficient for basic visual\nservoing and augmented reality applications, but not for complex swarm robotic\napplications in which the setup consists of multiple dynamic markers (tags\ndisplayed on LCD screen). This paper describes a novel methodology, called\nAprilTags3D, that improves pose estimation accuracy of AprilTags in field\nrobotics with only RGB sensor by adding a third dimension to the marker\ndetector. Also, presents experimental results from applying the proposed\nmethodology to swarm autonomous robotic boats for latching between them and for\ncreating robotic formations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.09199,regular,pre_llm,2020,1,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'A 3D Reactive Navigation Algorithm for Mobile Robots by Using\n  Tentacle-Based Sampling\n\n  This paper introduces a reactive navigation framework for mobile robots in\n3-dimensional (3D) space. The proposed approach does not rely on the global map\ninformation and achieves fast navigation by employing a tentacle based sampling\nand their heuristic evaluations on-the-fly. This reactive nature of the\napproach comes from the prior arrangement of navigation points on tentacles\n(parametric contours) to sample the navigation space. These tentacles are\nevaluated at each time-step, based on heuristic features such as closeness to\nthe goal, previous tentacle preferences and nearby obstacles in a\nrobot-centered 3D grid. Then, the navigable sampling point on the selected\ntentacle is passed to a controller for the motion execution. The proposed\nframework does not only extend its 2D tentacle-based counterparts into 3D, but\nalso introduces offline and online parameters, whose tuning provides\nversatility and adaptability of the algorithm to work in unknown environments.\nTo demonstrate the superior performance of the proposed algorithm over a\nstate-of-art method, the statistical results from physics-based simulations on\nvarious maps are presented. The video of the work is available at\nhttps://youtu.be/rrF7wHCz-0M.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.03864,regular,pre_llm,2020,1,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'Learning to drive via Apprenticeship Learning and Deep Reinforcement\n  Learning\n\n  With the implementation of reinforcement learning (RL) algorithms, current\nstate-of-art autonomous vehicle technology have the potential to get closer to\nfull automation. However, most of the applications have been limited to game\ndomains or discrete action space which are far from the real world driving.\nMoreover, it is very tough to tune the parameters of reward mechanism since the\ndriving styles vary a lot among the different users. For instance, an\naggressive driver may prefer driving with high acceleration whereas some\nconservative drivers prefer a safer driving style. Therefore, we propose an\napprenticeship learning in combination with deep reinforcement learning\napproach that allows the agent to learn the driving and stopping behaviors with\ncontinuous actions. We use gradient inverse reinforcement learning (GIRL)\nalgorithm to recover the unknown reward function and employ REINFORCE as well\nas Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal\npolicy. The performance of our method is evaluated in simulation-based scenario\nand the results demonstrate that the agent performs human like driving and even\nbetter in some aspects after training.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.03075,regular,pre_llm,2020,1,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'Pivot calibration concept for sensor attached mobile c-arms\n\n  Medical augmented reality has been actively studied for decades and many\nmethods have been proposed torevolutionize clinical procedures. One example is\nthe camera augmented mobile C-arm (CAMC), which providesa real-time video\naugmentation onto medical images by rigidly mounting and calibrating a camera\nto the imagingdevice. Since then, several CAMC variations have been suggested\nby calibrating 2D/3D cameras, trackers, andmore recently a Microsoft HoloLens\nto the C-arm. Different calibration methods have been applied to establishthe\ncorrespondence between the rigidly attached sensor and the imaging device. A\ncrucial step for these methodsis the acquisition of X-Ray images or 3D\nreconstruction volumes; therefore, requiring the emission of ionizingradiation.\nIn this work, we analyze the mechanical motion of the device and propose an\nalternatative methodto calibrate sensors to the C-arm without emitting any\nradiation. Given a sensor is rigidly attached to thedevice, we introduce an\nextended pivot calibration concept to compute the fixed translation from the\nsensor tothe C-arm rotation center. The fixed relationship between the sensor\nand rotation center can be formulated as apivot calibration problem with the\npivot point moving on a locus. Our method exploits the rigid C-arm\nmotiondescribing a Torus surface to solve this calibration problem. We explain\nthe geometry of the C-arm motion andits relation to the attached sensor,\npropose a calibration algorithm and show its robustness against noise, as\nwellas trajectory and observed pose density by computer simulations. We discuss\nthis geometric-based formulationand its potential extensions to different C-arm\napplications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.04134,regular,pre_llm,2020,1,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'NimbRo Logistics -- Project KittingBot\n\n  Recovering the pose of an object from mere point clouds is often hindered by\nthe lack of the information that they provide. In this lab, we address this\nproblem by proposing a method that exploits the symmetry of objects as well as\nusing pictures taken from a static camera of the same scene. We apply this\napproach to detects nuts in a table top scene that includes screws, nuts,\nwashers and several placeholders for grasp planning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.0955,regular,pre_llm,2020,1,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Experimental Evaluation of Human Motion Prediction: Toward Safe and\n  Efficient Human Robot Collaboration\n\n  Human motion prediction is non-trivial in modern industrial settings.\nAccurate prediction of human motion can not only improve efficiency in human\nrobot collaboration, but also enhance human safety in close proximity to\nrobots. Among existing prediction models, the parameterization and\nidentification methods of those models vary. It remains unclear what is the\nnecessary parameterization of a prediction model, whether online adaptation of\nthe model is necessary, and whether prediction can help improve safety and\nefficiency during human robot collaboration. These problems result from the\ndifficulty to quantitatively evaluate various prediction models in a\nclosed-loop fashion in real human-robot interaction settings. This paper\ndevelops a method to evaluate the closed-loop performance of different\nprediction models. In particular, we compare models with different\nparameterizations and models with or without online parameter adaptation.\nExtensive experiments were conducted on a human robot collaboration platform.\nThe experimental results demonstrated that human motion prediction\nsignificantly enhanced the collaboration efficiency and human safety. Adaptable\nprediction models that were parameterized by neural networks achieved the best\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.09242,regular,pre_llm,2020,1,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Multi-Fingered Grasp Planning via Inference in Deep Neural Networks\n\n  We propose a novel approach to multi-fingered grasp planning leveraging\nlearned deep neural network models. We train a voxel-based 3D convolutional\nneural network to predict grasp success probability as a function of both\nvisual information of an object and grasp configuration. We can then formulate\ngrasp planning as inferring the grasp configuration which maximizes the\nprobability of grasp success. In addition, we learn a prior over grasp\nconfigurations as a mixture density network conditioned on our voxel-based\nobject representation.\n  We show that this object conditional prior improves grasp inference when used\nwith the learned grasp success prediction network when compared to a learned,\nobject-agnostic prior, or an uninformed uniform prior. Our work is the first to\ndirectly plan high quality multi-fingered grasps in configuration space using a\ndeep neural network without the need of an external planner. We validate our\ninference method performing multi-finger grasping on a physical robot. Our\nexperimental results show that our planning method outperforms existing grasp\nplanning methods for neural networks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.0862,regular,pre_llm,2020,1,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Trajectory Planning for Connected and Automated Vehicles: Cruising, Lane\n  Changing, and Platooning\n\n  Autonomy and connectivity are considered among the most promising\ntechnologies to improve safety, mobility, fuel and time consumption in\ntransportation systems. Some of the fuel efficiency benefits of connected and\nautomated vehicles (CAVs) can be realized through platooning. A platoon is a\nvirtual train of CAVs that travel together following the platoon head, with\nsmall gaps between them. Vehicles may also reduce travel time by lane changing.\nIn this paper, we devise an optimal control-based trajectory planning model\nthat can provide safe and efficient trajectories for the subject vehicle and\ncan incorporate platooning and lane changing. We embed this trajectory planning\nmodel in a simulation framework to quantify its efficiency benefits as it\nrelates to fuel consumption and travel time, in a dynamic traffic stream.\nFurthermore, we perform extensive numerical experiments to investigate whether,\nand the circumstances under which, the vehicles in upstream of the subject\nvehicle may also experience second-hand fuel efficiency benefits.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.04637,review,pre_llm,2020,1,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""Companion Unmanned Aerial Vehicles: A Survey\n\n  Recent technological advancements in small-scale unmanned aerial vehicles\n(UAVs) have led to the development of companion UAVs. Similar to conventional\ncompanion robots, companion UAVs have the potential to assist us in our daily\nlives and to help alleviating social loneliness issue. In contrast to ground\ncompanion robots, companion UAVs have the capability to fly and possess unique\ninteraction characteristics. Our goals in this work are to have a bird's-eye\nview of the companion UAV works and to identify lessons learned and guidelines\nfor the design of companion UAVs. We tackle two major challenges towards these\ngoals, where we first use a coordinated way to gather top-quality human-drone\ninteraction (HDI) papers from three sources, and then propose to use a\nperceptual map of UAVs to summarize current research efforts in HDI. While\nbeing simple, the proposed perceptual map can cover the efforts have been made\nto realize companion UAVs in a comprehensive manner and lead our discussion\ncoherently. We also discuss patterns we noticed in the literature and some\nlessons learned throughout the review. In addition, we recommend several areas\nthat are worth exploring and suggest a few guidelines to enhance HDI researches\nwith companion UAVs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.00411,regular,pre_llm,2020,1,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Recent Advances in Human-Robot Collaboration Towards Joint Action\n\n  Robots existed as separate entities till now, but the horizons of a symbiotic\nhuman-robot partnership are impending. Despite all the recent technical\nadvances in terms of hardware, robots are still not endowed with desirable\nrelational skills that ensure a social component in their existence. This\narticle draws from our experience as roboticists in Human-Robot Collaboration\n(HRC) with humanoid robots and presents some of the recent advances made\ntowards realizing intuitive robot behaviors and partner-aware control involving\nphysical interactions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.06156,regular,pre_llm,2020,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""A Reliable Gravity Compensation Control Strategy for dVRK Robotic Arms\n  With Nonlinear Disturbance Forces\n\n  External disturbance forces caused by nonlinear springy electrical cables in\nthe Master Tool Manipulator (MTM) of the da Vinci Research Kit (dVRK) limits\nthe usage of the existing gravity compensation methods. Significant motion\ndrifts at the MTM tip are often observed when the MTM is located far from its\nidentification trajectory, preventing the usage of these methods for the entire\nworkspace reliably. In this paper, we propose a general and systematic\nframework to address the problems of the gravity compensation for the MTM of\nthe dVRK. Particularly, high order polynomial models were used to capture the\nhighly nonlinear disturbance forces and integrated with the Multi-step Least\nSquare Estimation (MLSE) framework. This method allows us to identify the\nparameters of both the gravitational and disturbance forces for each link\nsequentially, preventing residual error passing among the links of the MTM with\nuneven mass distribution. A corresponding gravity compensation controller was\ndeveloped to compensate the gravitational and disturbance forces. The method\nwas validated with extensive experiments in the majority of the manipulator's\nworkspace, showing significant performance enhancements over existing methods.\nFinally, a deliverable software package in MATLAB and C++ was integrated with\ndVRK and published in the dVRK community for open-source research and\ndevelopment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.05279,regular,pre_llm,2020,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'DGCM-Net: Dense Geometrical Correspondence Matching Network for\n  Incremental Experience-based Robotic Grasping\n\n  This article presents a method for grasping novel objects by learning from\nexperience. Successful attempts are remembered and then used to guide future\ngrasps such that more reliable grasping is achieved over time. To generalise\nthe learned experience to unseen objects, we introduce the dense geometric\ncorrespondence matching network (DGCM-Net). This applies metric learning to\nencode objects with similar geometry nearby in feature space. Retrieving\nrelevant experience for an unseen object is thus a nearest neighbour search\nwith the encoded feature maps. DGCM-Net also reconstructs 3D-3D correspondences\nusing the view-dependent normalised object coordinate space to transform grasp\nconfigurations from retrieved samples to unseen objects. In comparison to\nbaseline methods, our approach achieves an equivalent grasp success rate.\nHowever, the baselines are significantly improved when fusing the knowledge\nfrom experience with their grasp proposal strategy. Offline experiments with a\ngrasping dataset highlight the capability to generalise within and between\nobject classes as well as to improve success rate over time from increasing\nexperience. Lastly, by learning task-relevant grasps, our approach can\nprioritise grasps that enable the functional use of objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11159,regular,pre_llm,2020,1,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Universally Safe Swerve Manoeuvres for Autonomous Driving\n\n  This paper characterizes safe following distances for on-road driving when\nvehicles can avoid collisions by either braking or by swerving into an adjacent\nlane. In particular, we focus on safety as defined in the\nResponsibility-Sensitive Safety (RSS) framework. We extend RSS by introducing\nswerve manoeuvres as a valid response in addition to the already present brake\nmanoeuvre. These swerve manoeuvres use the more realistic kinematic bicycle\nmodel rather than the double integrator model of RSS. When vehicles are able to\nswerve and brake, it is shown that their required safe following distance at\nhigher speeds is less than that required through braking alone. In addition,\nwhen all vehicles follow this new distance, they are provably safe. The use of\nthe kinematic bicycle model is then validated by comparing these swerve\nmanoeuvres to that of a dynamic single-track model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2001.11196,regular,pre_llm,2020,1,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Model-free vision-based shaping of deformable plastic materials\n\n  We address the problem of shaping deformable plastic materials using\nnon-prehensile actions. Shaping plastic objects is challenging, since they are\ndifficult to model and to track visually. We study this problem, by using\nkinetic sand, a plastic toy material which mimics the physical properties of\nwet sand. Inspired by a pilot study where humans shape kinetic sand, we define\ntwo types of actions: \\textit{pushing} the material from the sides and\n\\textit{tapping} from above. The chosen actions are executed with a robotic arm\nusing image-based visual servoing. From the current and desired view of the\nmaterial, we define states based on visual features such as the outer contour\nshape and the pixel luminosity values. These are mapped to actions, which are\nrepeated iteratively to reduce the image error until convergence is reached.\nFor pushing, we propose three methods for mapping the visual state to an\naction. These include heuristic methods and a neural network, trained from\nhuman actions. We show that it is possible to obtain simple shapes with the\nkinetic sand, without explicitly modeling the material. Our approach is limited\nin the types of shapes it can achieve. A richer set of action types and\nmulti-step reasoning is needed to achieve more sophisticated shapes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.02538,regular,pre_llm,2020,2,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in\n  Flexible Object Manipulation\n\n  This paper addresses a new strategy called Simulation-to-Real-to-Simulation\n(Sim2Real2Sim) to bridge the gap between simulation and real-world, and\nautomate a flexible object manipulation task. This strategy consists of three\nsteps: (1) using the rough environment with the estimated models to develop the\nmethods to complete the manipulation task in the simulation; (2) applying the\nmethods from simulation to real-world and comparing their performance; (3)\nupdating the models and methods in simulation based on the differences between\nthe real world and the simulation. The Plug Task from the 2015 DARPA Robotics\nChallenge Finals is chosen to evaluate our Sim2Real2Sim strategy. A new\nidentification approach for building the model of the linear flexible objects\nis derived from real-world to simulation. The automation of the DRC plug task\nin both simulation and real-world proves the success of the Sim2Real2Sim\nstrategy. Numerical experiments are implemented to validate the simulated\nmodel.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0789,regular,pre_llm,2020,2,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Informative Path Planning for Mobile Sensing with Reinforcement Learning\n\n  Large-scale spatial data such as air quality, thermal conditions and location\nsignatures play a vital role in a variety of applications. Collecting such data\nmanually can be tedious and labour intensive. With the advancement of robotic\ntechnologies, it is feasible to automate such tasks using mobile robots with\nsensing and navigation capabilities. However, due to limited battery lifetime\nand scarcity of charging stations, it is important to plan paths for the robots\nthat maximize the utility of data collection, also known as the informative\npath planning (IPP) problem. In this paper, we propose a novel IPP algorithm\nusing reinforcement learning (RL). A constrained exploration and exploitation\nstrategy is designed to address the unique challenges of IPP, and is shown to\nhave fast convergence and better optimality than a classical reinforcement\nlearning approach. Extensive experiments using real-world measurement data\ndemonstrate that the proposed algorithm outperforms state-of-the-art algorithms\nin most test cases. Interestingly, unlike existing solutions that have to be\nre-executed when any input parameter changes, our RL-based solution allows a\ndegree of transferability across different problem instances.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.03038,regular,pre_llm,2020,2,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory\n  Behaviors\n\n  We present DenseCAvoid, a novel navigation algorithm for navigating a robot\nthrough dense crowds and avoiding collisions by anticipating pedestrian\nbehaviors. Our formulation uses visual sensors and a pedestrian trajectory\nprediction algorithm to track pedestrians in a set of input frames and provide\nbounding boxes that extrapolate the pedestrian positions in a future time. Our\nhybrid approach combines this trajectory prediction with a Deep Reinforcement\nLearning-based collision avoidance method to train a policy to generate\nsmoother, safer, and more robust trajectories during run-time. We train our\npolicy in realistic 3-D simulations of static and dynamic scenarios with\nmultiple pedestrians. In practice, our hybrid approach generalizes well to\nunseen, real-world scenarios and can navigate a robot through dense crowds\n(~1-2 humans per square meter) in indoor scenarios, including narrow corridors\nand lobbies. As compared to cases where prediction was not used, we observe\nthat our method reduces the occurrence of the robot freezing in a crowd by up\nto 48%, and performs comparably with respect to trajectory lengths and mean\narrival times to goal.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.06888,regular,pre_llm,2020,2,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'A Modular Framework to Generate Robust Biped Locomotion: From Planning\n  to Control\n\n  Biped robots are inherently unstable because of their complex kinematics as\nwell as dynamics. Despite the many research efforts in developing biped\nlocomotion, the performance of biped locomotion is still far from the\nexpectations. This paper proposes a model-based framework to generate stable\nbiped locomotion. The core of this framework is an abstract dynamics model\nwhich is composed of three masses to consider the dynamics of stance leg, torso\nand swing leg for minimizing the tracking problems. According to this dynamics\nmodel, we propose a modular walking reference trajectories planner which takes\ninto account obstacles to plan all the references. Moreover, this dynamics\nmodel is used to formulate the controller as a Model Predictive Control (MPC)\nscheme which can consider some constraints in the states of the system, inputs,\noutputs and also mixed input-output. The performance and the robustness of the\nproposed framework are validated by performing several numerical simulations\nusing MATLAB. Moreover, the framework is deployed on a simulated\ntorque-controlled humanoid to verify its performance and robustness. The\nsimulation results show that the proposed framework is capable of generating\nbiped locomotion robustly.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.11807,regular,pre_llm,2020,2,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'GLAS: Global-to-Local Safe Autonomy Synthesis for Multi-Robot Motion\n  Planning with End-to-End Learning\n\n  We present GLAS: Global-to-Local Autonomy Synthesis, a provably-safe,\nautomated distributed policy generation for multi-robot motion planning. Our\napproach combines the advantage of centralized planning of avoiding local\nminima with the advantage of decentralized controllers of scalability and\ndistributed computation. In particular, our synthesized policies only require\nrelative state information of nearby neighbors and obstacles, and compute a\nprovably-safe action. Our approach has three major components: i) we generate\ndemonstration trajectories using a global planner and extract local\nobservations from them, ii) we use deep imitation learning to learn a\ndecentralized policy that can run efficiently online, and iii) we introduce a\nnovel differentiable safety module to ensure collision-free operation, thereby\nallowing for end-to-end policy training. Our numerical experiments demonstrate\nthat our policies have a 20% higher success rate than optimal reciprocal\ncollision avoidance, ORCA, across a wide range of robot and obstacle densities.\nWe demonstrate our method on an aerial swarm, executing the policy on low-end\nmicrocontrollers in real-time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.10853,regular,pre_llm,2020,2,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Learning Machines from Simulation to Real World\n\n  Learning Machines is developing a flexible, cross-industry, advanced\nanalytics platform, targeted during stealth-stage at a limited number of\nspecific vertical applications. In this paper, we aim to integrate a general\nmachine system to learn a variant of tasks from simulation to real world. In\nsuch a machine system, it involves real-time robot vision, sensor fusion, and\nlearning algorithms (reinforcement learning). To this end, we demonstrate the\ngeneral machine system on three fundamental tasks including obstacle avoidance,\nforaging, and predator-prey robot. The proposed solutions are implemented on\nRobobo robots with mobile device (smartphone with camera) as interface and\nbuilt-in infrared (IR) sensors. The agent is trained in a virtual environment.\nIn order to assess its performance, the learned agent is tested in the virtual\nenvironment and reproduce the same results in a real environment. The results\nshow that the reinforcement learning algorithm can be reliably used for a\nvariety of tasks in unknown environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.06344,regular,pre_llm,2020,2,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""Learning Pregrasp Manipulation of Objects from Ungraspable Poses\n\n  In robotic grasping, objects are often occluded in ungraspable configurations\nsuch that no pregrasp pose can be found, eg large flat boxes on the table that\ncan only be grasped from the side. Inspired by humans' bimanual manipulation,\neg one hand to lift up things and the other to grasp, we address this type of\nproblems by introducing pregrasp manipulation - push and lift actions. We\npropose a model-free Deep Reinforcement Learning framework to train control\npolicies that utilize visual information and proprioceptive states of the robot\nto autonomously discover robust pregrasp manipulation. The robot arm learns to\nfirst push the object towards a support surface and establishes a pivot to lift\nup one side of the object, thus creating a clearance between the object and the\ntable for possible grasping solutions. Furthermore, we show the effectiveness\nof our proposed learning framework in training robust pregrasp policies that\ncan directly transfer from simulation to real hardware through suitable design\nof training procedures, state, and action space. Lastly, we evaluate the\neffectiveness and the generalisation ability of the learned policies in\nreal-world experiments, and demonstrate pregrasp manipulation of objects with\nvarious size, shape, weight, and surface friction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.04498,regular,pre_llm,2020,2,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Reaching, Grasping and Re-grasping: Learning Multimode Grasping Skills\n\n  The ability to adapt to uncertainties, recover from failures, and coordinate\nbetween hand and fingers are essential sensorimotor skills for fully autonomous\nrobotic grasping. In this paper, we aim to study a unified feedback control\npolicy for generating the finger actions and the motion of hand to accomplish\nseamlessly coordinated tasks of reaching, grasping and re-grasping. We proposed\na set of quantified metrics for task-orientated rewards to guide the policy\nexploration, and we analyzed and demonstrated the effectiveness of each reward\nterm. To acquire a robust re-grasping motion, we deployed different initial\nstates in training to experience failures that the robot would encounter during\ngrasping due to inaccurate perception or disturbances. The performance of\nlearned policy is evaluated on three different tasks: grasping a static target,\ngrasping a dynamic target, and re-grasping. The quality of learned grasping\npolicy was evaluated based on success rates in different scenarios and the\nrecovery time from failures. The results indicate that the learned policy is\nable to achieve stable grasps of a static or moving object. Moreover, the\npolicy can adapt to new environmental changes on the fly and execute\ncollision-free re-grasp after a failed attempt within a short recovery time\neven in difficult configurations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.10098,regular,pre_llm,2020,2,"{'ai_likelihood': 6.159146626790365e-06, 'text': 'An RLS-Based Instantaneous Velocity Estimator for Extended Radar\n  Tracking\n\n  Radar sensors have become an important part of the perception sensor suite\ndue to their long range and their ability to work in adverse weather\nconditions. However, several shortcomings such as large amounts of noise and\nextreme sparsity of the point cloud result in them not being used to their full\npotential. In this paper, we present a novel Recursive Least Squares (RLS)\nbased approach to estimate the instantaneous velocity of dynamic objects in\nreal-time that is capable of handling large amounts of noise in the input data\nstream. We also present an end-to-end pipeline to track extended objects in\nreal-time that uses the computed velocity estimates for data association and\ntrack initialisation. The approaches are evaluated using several real-world\ninspired driving scenarios that test the limits of these algorithms. It is also\nexperimentally proven that our approaches run in real-time with frame execution\ntime not exceeding 30 ms even in dense traffic scenarios, thus allowing for\ntheir direct implementation on autonomous vehicles.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0444,regular,pre_llm,2020,2,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'Fast Frontier-based Information-driven Autonomous Exploration with an\n  MAV\n\n  Exploration and collision-free navigation through an unknown environment is a\nfundamental task for autonomous robots. In this paper, a novel exploration\nstrategy for Micro Aerial Vehicles (MAVs) is presented. The goal of the\nexploration strategy is the reduction of map entropy regarding occupancy\nprobabilities, which is reflected in a utility function to be maximised. We\nachieve fast and efficient exploration performance with tight integration\nbetween our octree-based occupancy mapping approach, frontier extraction, and\nmotion planning-as a hybrid between frontier-based and sampling-based\nexploration methods. The computationally expensive frontier clustering employed\nin classic frontier-based exploration is avoided by exploiting the implicit\ngrouping of frontier voxels in the underlying octree map representation.\nCandidate next-views are sampled from the map frontiers and are evaluated using\na utility function combining map entropy and travel time, where the former is\ncomputed efficiently using sparse raycasting. These optimisations along with\nthe targeted exploration of frontier-based methods result in a fast and\ncomputationally efficient exploration planner. The proposed method is evaluated\nusing both simulated and real-world experiments, demonstrating clear advantages\nover state-of-the-art approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.01254,regular,pre_llm,2020,2,"{'ai_likelihood': 6.5896246168348525e-06, 'text': 'Tackling Existence Probabilities of Objects with Motion Planning for\n  Automated Urban Driving\n\n  Motion planners take uncertain information about the environment as an input.\nThe environment information is often quite noisy and has a tendency to contain\nfalse positive object detection. State-of-the-art motion planners consider all\nobjects alike, thus producing overcautious behavior. In this paper we present a\nplanning approach that considers alternative maneuvers in a combined fashion\nand plans a motion that is formed by the probabilities of those alternatives.\nThe proposed planner can smoothly react to objects with low existence\nprobability while remaining collision-free in case their existence\nsubstantiates. In this way, it tolerates the faults arising from perception and\nprediction, thus reducing their impact on operational reliability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.08289,regular,pre_llm,2020,2,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Particle robots A new specie of hybrid bio-inspired robotics\n\n  Inspired by a couple of simple organisms without eyes, neither ears. This\npaper presents a novel hybrid bionic robot, called ""particle robot"", which mix\na macro-organism and a micro-organism in the same robot. On one hand, an\ninteresting rather boring animal, the biological Echinoids (sea urchins) is\nmixed with the viruses micro-organisms, in specific the rotaviruses; together\nwith spherical mobile robots. Analogously, from a pure robotic perspective,\nthis bio-inspired robot can be seen as a spherical mobile robot wearing an\nactuated exoskeleton. The robot has two main configurations: when the spines\nare contracted it becomes a spherical mobile robot able to move in a fast pace\non land, embedding all spherical mobile robots properties. On the other hand,\nwhen the spines or legs are extended in a controlled pattern, it can walk on\nflat surfaces as well as move on snow and over rocks as a bionic sea urchin.\nThe spines of the robot are telescopic linear actuators, which combines soft\nand hard 3D print materials to make the actuation unit flexible for compressing\nit in minimal space and rigid for lifting the robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.05017,regular,pre_llm,2020,2,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'GRASPA 1.0: GRASPA is a Robot Arm graSping Performance benchmArk\n\n  The use of benchmarks is a widespread and scientifically meaningful practice\nto validate performance of different approaches to the same task. In the\ncontext of robot grasping the use of common object sets has emerged in recent\nyears, however no dominant protocols and metrics to test grasping pipelines\nhave taken root yet. In this paper, we present version 1.0 of GRASPA, a\nbenchmark to test effectiveness of grasping pipelines on physical robot setups.\nThis approach tackles the complexity of such pipelines by proposing different\nmetrics that account for the features and limits of the test platform. As an\nexample application, we deploy GRASPA on the iCub humanoid robot and use it to\nbenchmark our grasping pipeline. As closing remarks, we discuss how the GRASPA\nindicators we obtained as outcome can provide insight into how different steps\nof the pipeline affect the overall grasping performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.06719,regular,pre_llm,2020,2,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Reliable Trajectories for Dynamic Quadrupeds using Analytical Costs and\n  Learned Initializations\n\n  Dynamic traversal of uneven terrain is a major objective in the field of\nlegged robotics. The most recent model predictive control approaches for these\nsystems can generate robust dynamic motion of short duration; however, planning\nover a longer time horizon may be necessary when navigating complex terrain. A\nrecently-developed framework, Trajectory Optimization for Walking Robots\n(TOWR), computes such plans but does not guarantee their reliability on real\nplatforms, under uncertainty and perturbations. We extend TOWR with analytical\ncosts to generate trajectories that a state-of-the-art whole-body tracking\ncontroller can successfully execute. To reduce online computation time, we\nimplement a learning-based scheme for initialization of the nonlinear program\nbased on offline experience. The execution of trajectories as long as 16\nfootsteps and 5.5 s over different terrains by a real quadruped demonstrates\nthe effectiveness of the approach on hardware. This work builds toward an\nonline system which can efficiently and robustly replan dynamic trajectories.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.02277,regular,pre_llm,2020,2,"{'ai_likelihood': 5.39753172132704e-06, 'text': ""Interpretable Goal-based Prediction and Planning for Autonomous Driving\n\n  We propose an integrated prediction and planning system for autonomous\ndriving which uses rational inverse planning to recognise the goals of other\nvehicles. Goal recognition informs a Monte Carlo Tree Search (MCTS) algorithm\nto plan optimal maneuvers for the ego vehicle. Inverse planning and MCTS\nutilise a shared set of defined maneuvers and macro actions to construct plans\nwhich are explainable by means of rationality principles. Evaluation in\nsimulations of urban driving scenarios demonstrate the system's ability to\nrobustly recognise the goals of other vehicles, enabling our vehicle to exploit\nnon-trivial opportunities to significantly reduce driving times. In each\nscenario, we extract intuitive explanations for the predictions which justify\nthe system's decisions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.08317,regular,pre_llm,2020,2,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'An improved nonlinear FastEuler AHRS estimation based on the SVDCKF\n  algorithm\n\n  In this paper, we present a Singular Value Decomposition Cubature Kalman\nFilter(SVDCKF) fusion algorithm based on the improved nonlinear FastEuler\nAttitude and Heading Reference and System(AHRS) estimation model for small-UAV\nattitude. The contributions of this work are the derivation of the low-cost\nIMU/MAG integrated AHRS model combined with the quaternion attitude\ndetermination, and use the FastEuler to correct the gyroscope attitude update,\nwhich can increase the real-time solution. In addition, the SVDCKF algorithm is\nfused the various raw sensors data in order to improve the filter accuracy\ncompared with the CKF. The simulation and experiment results demonstrate the\nproposed algorithm has the more excellent attitude solution accuracy compared\nwith the CKF in the low and high dynamic flight conditions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.0632,regular,pre_llm,2020,2,"{'ai_likelihood': 0.000789430406358507, 'text': ""Dimension-variable Mapless Navigation with Deep Reinforcement Learning\n\n  Deep reinforcement learning (DRL) has exhibited considerable promise in the\ntraining of control agents for mapless robot navigation. However, DRL-trained\nagents are limited to the specific robot dimensions used during training,\nhindering their applicability when the robot's dimension changes for\ntask-specific requirements. To overcome this limitation, we propose a\ndimension-variable robot navigation method based on DRL. Our approach involves\ntraining a meta agent in simulation and subsequently transferring the meta\nskill to a dimension-varied robot using a technique called dimension-variable\nskill transfer (DVST). During the training phase, the meta agent for the meta\nrobot learns self-navigation skills with DRL. In the skill-transfer phase,\nobservations from the dimension-varied robot are scaled and transferred to the\nmeta agent, and the resulting control policy is scaled back to the\ndimension-varied robot. Through extensive simulated and real-world experiments,\nwe demonstrated that the dimension-varied robots could successfully navigate in\nunknown and dynamic environments without any retraining. The results show that\nour work substantially expands the applicability of DRL-based navigation\nmethods, enabling them to be used on robots with different dimensions without\nthe limitation of a fixed dimension. The video of our experiments can be found\nin the supplementary file.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.08124,regular,pre_llm,2020,2,"{'ai_likelihood': 2.4504131740993925e-06, 'text': ""Act, Perceive, and Plan in Belief Space for Robot Localization\n\n  In this paper, we outline an interleaved acting and planning technique to\nrapidly reduce the uncertainty of the estimated robot's pose by perceiving\nrelevant information from the environment, as recognizing an object or asking\nsomeone for a direction.\n  Generally, existing localization approaches rely on low-level geometric\nfeatures such as points, lines, and planes, while these approaches provide the\ndesired accuracy, they may require time to converge, especially with incorrect\ninitial guesses. In our approach, a task planner computes a sequence of action\nand perception tasks to actively obtain relevant information from the robot's\nperception system. We validate our approach in large state spaces, to show how\nthe approach scales, and in real environments, to show the applicability of our\nmethod on real robots.\n  We prove that our approach is sound, probabilistically complete, and\ntractable in practical cases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.12339,regular,pre_llm,2020,2,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Self-Supervised Deep Pose Corrections for Robust Visual Odometry\n\n  We present a self-supervised deep pose correction (DPC) network that applies\npose corrections to a visual odometry estimator to improve its accuracy.\nInstead of regressing inter-frame pose changes directly, we build on prior work\nthat uses data-driven learning to regress pose corrections that account for\nsystematic errors due to violations of modelling assumptions. Our\nself-supervised formulation removes any requirement for six-degrees-of-freedom\nground truth and, in contrast to expectations, often improves overall\nnavigation accuracy compared to a supervised approach. Through extensive\nexperiments, we show that our self-supervised DPC network can significantly\nenhance the performance of classical monocular and stereo odometry estimators\nand substantially out-performs state-of-the-art learning-only approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2002.06024,regular,pre_llm,2020,2,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""Inner Attention Supported Adaptive Cooperation for Heterogeneous Multi\n  Robots Teaming based on Multi-agent Reinforcement Learning\n\n  Humans can selectively focus on different information based on different\ntasks requirements, other people's abilities and availability. Therefore, they\ncan adapt quickly to a completely different and complex environments. If, like\npeople, robot could obtain the same abilities, then it would greatly increase\ntheir adaptability to new and unexpected situations. Recent efforts in\nHeterogeneous Multi Robots Teaming have try to achieve this ability, such as\nthe methods based on communication and multi-modal information fusion\nstrategies. However, these methods will not only suffer from the exponential\nexplosion problem with the increase of robots number but also need huge\ncomputational resources. To that end, we introduce an inner attention\nactor-critic method that replicates aspects of human flexibly cooperation. By\nbringing attention mechanism on computer vision, natural language process into\nthe realm of multi-robot cooperation, our attention method is able to\ndynamically select which robots to attend to. In order to test the\neffectiveness of our proposed method, several simulation experiments have been\ndesigned. And the results show that inner attention mechanism can enable\nflexible cooperation and lower resources consuming in rescuing tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00695,regular,pre_llm,2020,3,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Efficient Latent Representations using Multiple Tasks for Autonomous\n  Driving\n\n  Driving in the dynamic, multi-agent, and complex urban environment is a\ndifficult task requiring a complex decision policy. The learning of such a\npolicy requires a state representation that can encode the entire environment.\nMid-level representations that encode a vehicle's environment as images have\nbecome a popular choice, but they are quite high-dimensional, which limits\ntheir use in data-scarce cases such as reinforcement learning. In this article,\nwe propose to learn a low dimensional and rich feature representation of the\nenvironment by training an encoder-decoder deep neural network to predict\nmultiple application relevant factors such as trajectories of other agents. We\ndemonstrate that the use of the multi-head encoder-decoder neural network\nresults in a more informative representation compared to a single-head\nencoder-decoder model. In particular, the proposed representation learning\napproach helps the policy network to learn faster, with increased performance\nand with less data, compared to existing approaches using a single-head\nnetwork.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.01466,regular,pre_llm,2020,3,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Bio-mimetic Adaptive Force/Position Control Using Fractal Impedance\n\n  The ability of animals to interact with complex dynamics is unmatched in\nrobots. Especially important to the interaction performances is the online\nadaptation of body dynamics, which can be modeled as an impedance behaviour.\nHowever, the variable impedance controller still possesses a challenge in the\ncurrent control frameworks due to the difficulties of retaining stability when\nadapting the controller gains. The fractal impedance controller has been\nrecently proposed to solve this issue. However, it still has limitations such\nas sudden jumps in force when it starts to converge to the desired position and\nthe lack of a force feedback loop. In this manuscript, two improvements are\nmade to the control framework to solve these limitations. The force\ndiscontinuity has been addressed introducing a modulation of the impedance via\na virtual antagonist that modulates the output force. The force tracking has\nbeen modeled after the parallel force/position controller architecture. In\ncontrast to traditional methods, the fractal impedance controller enables the\nimplementation of a search algorithm on the force feedback to adapt its\nbehaviour on the external environment instead of on relying on \\textit{a\npriori} knowledge of the external dynamics. Preliminary simulation results\npresented in this paper show the feasibility of the proposed approach, and it\nallows to evaluate the trade-off that needs to be made when relying on the\nproposed controller for interaction. In conclusion, the proposed method mimics\nthe behaviour of an agonist/antagonist system adapting to unknown external\ndynamics, and it may find application in computational neuroscience, haptics,\nand interaction control.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.06136,regular,pre_llm,2020,3,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Computationally Efficient Obstacle Avoidance Trajectory Planner for UAVs\n  Based on Heuristic Angular Search Method\n\n  For accomplishing a variety of missions in challenging environments, the\ncapability of navigating with full autonomy while avoiding unexpected obstacles\nis the most crucial requirement for UAVs in real applications. In this paper,\nwe proposed such a computationally efficient obstacle avoidance trajectory\nplanner that can be used in cluttered unknown environments. Because of the\nnarrow view field of single depth camera on a UAV, the information of obstacles\naround is quite limited thus the shortest entire path is difficult to achieve.\nTherefore we focus on the time cost of the trajectory planner and safety rather\nthan other factors. This planner is mainly composed of a point cloud processor,\na waypoint publisher with Heuristic Angular Search(HAS) method and a motion\nplanner with minimum acceleration optimization. Furthermore, we propose several\ntechniques to enhance safety by making the possibility of finding a feasible\ntrajectory as big as possible. The proposed approach is implemented to run\nonboard in real-time and is tested extensively in simulation and the average\ncontrol output calculating time of iteration steps is less than 18 ms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.09224,regular,pre_llm,2020,3,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Probabilistic Visual Navigation with Bidirectional Image Prediction\n\n  Humans can robustly follow a visual trajectory defined by a sequence of\nimages (i.e. a video) regardless of substantial changes in the environment or\nthe presence of obstacles. We aim at endowing similar visual navigation\ncapabilities to mobile robots solely equipped with a RGB fisheye camera. We\npropose a novel probabilistic visual navigation system that learns to follow a\nsequence of images with bidirectional visual predictions conditioned on\npossible navigation velocities. By predicting bidirectionally (from start\ntowards goal and vice versa) our method extends its predictive horizon enabling\nthe robot to go around unseen large obstacles that are not visible in the video\ntrajectory. Learning how to react to obstacles and potential risks in the\nvisual field is achieved by imitating human teleoperators. Since the human\nteleoperation commands are diverse, we propose a probabilistic representation\nof trajectories that we can sample to find the safest path. Integrated into our\nnavigation system, we present a novel localization approach that infers the\ncurrent location of the robot based on the virtual predicted trajectories\nrequired to reach different images in the visual trajectory. We evaluate our\nnavigation system quantitatively and qualitatively in multiple simulated and\nreal environments and compare to state-of-the-art baselines.Our approach\noutperforms the most recent visual navigation methods with a large margin with\nregard to goal arrival rate, subgoal coverage rate, and success weighted by\npath length (SPL). Our method also generalizes to new robot embodiments never\nused during training.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.12148,regular,pre_llm,2020,3,"{'ai_likelihood': 5.9604644775390625e-06, 'text': 'A Flexible Job Shop Scheduling Representation of the Autonomous In-Space\n  Assembly Task Assignment Problem\n\n  As in-space exploration increases, autonomous systems will play a vital role\nin building the necessary facilities to support exploration. To this end, an\nautonomous system must be able to assign tasks in a scheme that efficiently\ncompletes all of the jobs in the desired project. This research proposes a\nflexible job shop problem (FJSP) representation to characterize an autonomous\nassembly project and then proposes both a mixed integer programming (MIP)\nsolution formulation and a reinforcement learning (RL) solution formulation.\nThe MIP formulation encodes all of the constraints and interjob dynamics a\npriori and was able to solve for the optimal solution to minimize the makespan.\nThe RL formulation did not converge to an optimal solution but did successfully\nlearn implicitly interjob dynamics through interaction with the reward\nfunction. Future work will include developing a solution formulation that\nutilizes the strengths of both proposed solution methods to handle scaling in\nsize and complexity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.01496,regular,pre_llm,2020,3,"{'ai_likelihood': 6.5896246168348525e-06, 'text': 'Robust tightly coupled pose estimation based on monocular vision,\n  inertia, and wheel speed\n\n  The visual SLAM method is widely used for self-localization and mapping in\ncomplex environments. Visual-inertia SLAM, which combines a camera with IMU,\ncan significantly improve the robustness and enable scale weak-visibility,\nwhereas monocular visual SLAM is scale-invisible. For ground mobile robots, the\nintroduction of a wheel speed sensor can solve the scale weak-visible problem\nand improve the robustness under abnormal conditions. In this thesis, a\nmulti-sensor fusion SLAM algorithm using monocular vision, inertia, and wheel\nspeed measurements is proposed. The sensor measurements are combined in a\ntightly coupled manner, and a nonlinear optimization method is used to maximize\nthe posterior probability to solve the optimal state estimation. Loop detection\nand back-end optimization are added to help reduce or even eliminate the\ncumulative error of the estimated poses, thus ensuring global consistency of\nthe trajectory and map. The wheel odometer pre-integration algorithm, which\ncombines the chassis speed and IMU angular speed, can avoid repeated\nintegration caused by linearization point changes during iterative\noptimization; state initialization based on the wheel odometer and IMU enables\na quick and reliable calculation of the initial state values required by the\nstate estimator in both stationary and moving states. Comparative experiments\nwere carried out in room-scale scenes, building scale scenes, and visual loss\nscenarios. The results showed that the proposed algorithm has high accuracy,\n2.2 m of cumulative error after moving 812 m (0.28%, loopback optimization\ndisabled), strong robustness, and effective localization capability even in the\nevent of sensor loss such as visual loss. The accuracy and robustness of the\nproposed method are superior to those of monocular visual inertia SLAM and\ntraditional wheel odometers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.12772,regular,pre_llm,2020,3,"{'ai_likelihood': 1.556343502468533e-06, 'text': ""Towards an immersive user interface for waypoint navigation of a mobile\n  robot\n\n  In this paper, we investigate the utility of head-mounted display (HMD)\ninterfaces for navigation of mobile robots. We focus on the selection of\nwaypoint positions for the robot, whilst maintaining an egocentric view of the\nrobot's environment. Inspired by virtual reality (VR) gaming, we propose a\ntarget selection method that uses the 6 degrees-of-freedom tracked controllers\nof a commercial VR headset. This allows an operator to point to the desired\ntarget position, in the vicinity of the robot, which the robot then\nautonomously navigates towards. A user study (37 participants) was conducted to\nexamine the efficacy of this control strategy when compared to direct control,\nboth with and without a communication delay. The results of the experiment\nshowed that participants were able to learn how to use the novel system\nquickly, and the majority of participants reported a preference for waypoint\ncontrol. Across all recorded metrics (task performance, operator workload and\nusability) the proposed waypoint control interface was not significantly\naffected by the communication delay, in contrast to direct control. The\nsimulated experiment indicated that a real-world implementation of the proposed\ninterface could be effective, but also highlighted the need to manage the\nnegative effects of HMDs - particularly VR sickness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.09512,regular,pre_llm,2020,3,"{'ai_likelihood': 2.317958407931858e-06, 'text': ""Design and optimal control of a tiltrotor micro aerial vehicle for\n  efficient omnidirectional flight\n\n  Omnidirectional micro aerial vehicles are a growing field of research, with\ndemonstrated advantages for aerial interaction and uninhibited observation.\nWhile systems with complete pose omnidirectionality and high hover efficiency\nhave been developed independently, a robust system that combines the two has\nnot been demonstrated to date. This paper presents the design and optimal\ncontrol of a novel omnidirectional vehicle that can exert a wrench in any\norientation while maintaining efficient flight configurations. The system\ndesign is motivated by the result of a morphology design optimization. A six\ndegrees of freedom optimal controller is derived, with an actuator allocation\napproach that implements task prioritization, and is robust to singularities.\nFlight experiments demonstrate and verify the system's capabilities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.13165,regular,pre_llm,2020,3,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""In-Hand Object-Dynamics Inference using Tactile Fingertips\n\n  Having the ability to estimate an object's properties through interaction\nwill enable robots to manipulate novel objects. Object's dynamics, specifically\nthe friction and inertial parameters have only been estimated in a lab\nenvironment with precise and often external sensing. Could we infer an object's\ndynamics in the wild with only the robot's sensors? In this paper, we explore\nthe estimation of dynamics of a grasped object in motion, with tactile force\nsensing at multiple fingertips. Our estimation approach does not rely on torque\nsensing to estimate the dynamics. To estimate friction, we develop a control\nscheme to actively interact with the object until slip is detected. To robustly\nperform the inertial estimation, we setup a factor graph that fuses all our\nsensor measurements on physically consistent manifolds and perform inference.\nWe show that tactile fingertips enable in-hand dynamics estimation of low mass\nobjects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.0373,regular,pre_llm,2020,3,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Electronics-Free Pneumatic Logic Circuits for Localized Feedback Control\n  of Multi-Actuator Soft Robots\n\n  The vision of creating entirely-soft robots capable of performing complex\ntasks will be accomplished only when the controllers required for autonomous\noperation can be fully implemented on soft components. Despite recent advances\nin compliant fluidic circuitry for mechanical signal processing, the\napplicability of this technology for soft robot control has been limited by\ncomplicated fabrication and tuning processes, and also the need for external\nsignals such as clocks and digital references. We propose a method to develop\npneumatic soft robots in which coordinated interactions between multiple\nactuators are performed using controllers implemented on components\ndistributedly embedded in the soft structures of the system. In this approach,\nthe notions of binary and multi-valued actuator logic states are introduced. In\nthis way, the physical local dynamical couplings between the analog states of\nthe actuators, established using soft valves of a new type, can be thought of\nas logic-gate-based mappings acting on discretized representations of the\nactuator states. Consequently, techniques for digital logic design can be\napplied to derive the architectures of the localized mechanical couplings that\nintelligently coordinate the oscillation patterns of the actuator responses.\nFor the purposes of controller tuning, the soft valves are conceived so that\ntheir main physical parameters can be adjusted from the exterior of the robot\nthrough simple geometrical changes of the corresponding structural elements. To\ndemonstrate the proposed approach, we present the development of a six-state\nlocomoting soft robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00676,regular,pre_llm,2020,3,"{'ai_likelihood': 1.5066729651557076e-05, 'text': 'Design and Implementation of A Novel Precision Irrigation Robot Based on\n  An Intelligent Path Planning Algorithm\n\n  The agricultural irrigation system is closely related to agricultural\nproduction. There are some problems in nowadays agricultural irrigation system,\nsuch as poor mobility, imprecision and high price. To address these issues, an\nintelligent irrigation robot is designed and implemented in this work. The\nrobot achieves precise irrigation by the irrigation path planning algorithm\nwhich is improved by Bayesian theory. In the proposed algorithm, we utilize as\nmuch information as possible to achieve full coverage irrigation in the complex\nagricultural environment. Besides, we propose the maximum risk to avoid the\nproblem of lack of inspection in certain areas. Finally, We carried out\nsimulation experiments and field experiments to verify the robot and the\nalgorithm. The experimental results indicate that the robot is capable of\nfulfilling the requirements of various agricultural irrigation tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.08595,regular,pre_llm,2020,3,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Formation and Reconfiguration of Tight Multi-Lane Platoons\n\n  Advances in vehicular communication technologies are expected to facilitate\ncooperative driving. Connected and Automated Vehicles (CAVs) are able to\ncollaboratively plan and execute driving maneuvers by sharing their perceptual\nknowledge and future plans. In this paper, an architecture for autonomous\nnavigation of tight multi-lane platoons travelling on public roads is\npresented. Using the proposed approach, CAVs are able to form single or\nmulti-lane platoons of various geometrical configurations. They are able to\nreshape and adjust their configurations according to changes in the\nenvironment. The proposed architecture consists of two main components: an\noffline motion planner system and an online hierarchical control system. The\nmotion planner uses an optimization-based approach for cooperative formation\nand reconfiguration in tight spaces. A constrained optimization scheme is used\nto plan smooth, dynamically feasible and collision-free trajectories for all\nthe vehicles within the platoon. The paper addresses online computation\nlimitations by employing a family of maneuvers precomputed offline and stored\non a look-up table on the vehicles. The online hierarchical control system is\ncomposed of three levels: a traffic operation system (TOS), a decision-maker,\nand a path-follower. The TOS determines the desired platoon reconfiguration.\nThe decision-maker checks the feasibility of the reconfiguration plan. The\nreconfiguration maneuver is executed by a low-level path-following feedback\ncontroller in real-time. The effectiveness of the approach is demonstrated\nthrough simulations of three case studies: 1) formation reconfiguration 2)\nobstacle avoidance, and 3) benchmarking against behavior-based planning in\nwhich the desired formation is achieved using a sequence of motion primitives.\nVideos and software can be found online\nhttps://github.com/RoyaFiroozi/Centralized-Planning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.04087,regular,pre_llm,2020,3,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Selecting and Designing Grippers for an Assembly Task in a Structured\n  Approach\n\n  In this paper, we present a structured approach to selecting and designing a\nset of grippers for an assembly task. Compared to current experience-based\ngripper design method, our approach accelerates the design process by\nautomatically generating a set of initial design options on gripper type and\nparameters according to the CAD models of assembly components. We use mesh\nsegmentation techniques to segment the assembly components and fit the\nsegmented parts with shape primitives, according to the predefined\ncorrespondence between primitive shape and gripper type, suitable gripper types\nand parameters can be selected and extracted from the fitted shape primitives.\nMoreover, we incorporate the assembly constraints in the further evaluation of\nthe initially obtained gripper types and parameters. Considering the affordance\nof the segmented parts and the collision avoidance between the gripper and the\nsubassemblies, applicable gripper types and parameters can be filtered from the\ninitial options. Among the applicable gripper configurations, we further\noptimize number of grippers for performing the assembly task, by exploring the\ngripper that is able to handle multiple assembly components during the\nassembly. Finally, the feasibility of the designed grippers is experimentally\nverified by assembling a part of an industrial product.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.10553,regular,pre_llm,2020,3,"{'ai_likelihood': 0.0, 'text': ""RoboMem: Giving Long Term Memory to Robots\n\n  Robots have the potential to improve health monitoring outcomes for the\nelderly by providing doctors, and caregivers with information about the\nperson's behavior, health activities and their surrounding environment. Over\nthe years, less work has been done to enable robots to preserve information for\nlonger periods of time, on the order of months and years of data, and use this\ncontextual information to answer queries. Time complexity to process this\nmassive sensor data in a timely fashion, inability to anticipate the future\nqueries in advance and imprecision involved in the results have been the main\nimpediments in making progress in this area. We make a contribution by\nintroducing RoboMem, a query answering system for health-care assistance of\nelderly over long term; continuous data feeds that intends to overcome the\nchallenges of giving long term memory to robots. The design for our framework\npreprocesses the sensor data and stores this preprocessed data into the\ndatabase. This data is updated in the database by going through successive\nrefinements, improving its accuracy for responding to queries. If data in the\ndatabase is not enough to answer a query, a small set of relevant frames (also\nobtained from the database) will be reprocessed to obtain the answer. [Our\ninitial prototype of RoboMem stores 3.5MB of data in the database as compared\nto 535.8MB of actual video frames and with minimal data in the database it is\nable to fetch information fundamental to respond to queries in 0.0002 seconds\non average].\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00658,regular,pre_llm,2020,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Socially-Aware Robot Planning via Bandit Human Feedback\n\n  In this paper, we consider the problem of designing collision-free,\ndynamically feasible, and socially-aware trajectories for robots operating in\nenvironments populated by humans. We define trajectories to be social-aware if\nthey do not interfere with humans in any way that causes discomfort. In this\npaper, discomfort is defined broadly and, depending on specific individuals, it\ncan result from the robot being too close to a human or from interfering with\nhuman sight or tasks. Moreover, we assume that human feedback is a bandit\nfeedback indicating a complaint or no complaint on the part of the robot\ntrajectory that interferes with the humans, and it does not reveal any\ncontextual information about the locations of the humans or the reason for a\ncomplaint. Finally, we assume that humans can move in the obstacle-free space\nand, as a result, human utility can change. We formulate this planning problem\nas an online optimization problem that minimizes the social value of the\ntime-varying robot trajectory, defined by the total number of incurred human\ncomplaints. As the human utility is unknown, we employ zeroth order, or\nderivative-free, optimization methods to solve this problem, which we combine\nwith off-the-shelf motion planners to satisfy the dynamic feasibility and\ncollision-free specifications of the resulting trajectories. To the best of our\nknowledge, this is a new framework for socially-aware robot planning that is\nnot restricted to avoiding collisions with humans but, instead, focuses on\nincreasing the social value of the robot trajectories using only bandit human\nfeedback.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.00467,regular,pre_llm,2020,3,"{'ai_likelihood': 3.973642985026042e-06, 'text': 'NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture\n  Recognition\n\n  Developing artificial tactile sensing capabilities that rival human touch is\na long-term goal in robotics and prosthetics. Gradually more elaborate\nbiomimetic tactile sensors are being developed and applied to grasping and\nmanipulation tasks to help achieve this goal. Here we present the neuroTac, a\nnovel neuromorphic optical tactile sensor. The neuroTac combines the biomimetic\nhardware design from the TacTip sensor which mimicks the layered papillae\nstructure of human glabrous skin, with an event-based camera (DAVIS240,\niniVation) and algorithms which transduce contact information in the form of\nspike trains. The performance of the sensor is evaluated on a texture\nclassification task, with four spike coding methods being implemented and\ncompared: Intensive, Spatial, Temporal and Spatiotemporal. We found\ntiming-based coding methods performed with the highest accuracy over both\nartificial and natural textures. The spike-based output of the neuroTac could\nenable the development of biomimetic tactile perception algorithms in robotics\nas well as non-invasive and invasive haptic feedback methods in prosthetics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.07664,regular,pre_llm,2020,3,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""CinemAirSim: A Camera-Realistic Robotics Simulator for Cinematographic\n  Purposes\n\n  Drones and Unmanned Aerial Vehicles (UAV's) are becoming increasingly popular\nin the film and entertainment industries in part because of their\nmaneuverability and the dynamic shots and perspectives they enable. While there\nexists methods for controlling the position and orientation of the drones for\nvisibility, other artistic elements of the filming process, such as focal blur\nand light control, remain unexplored in the robotics community. The lack of\ncinemetographic robotics solutions is partly due to the cost associated with\nthe cameras and devices used in the filming industry, but also because\nstate-of-the-art photo-realistic robotics simulators only utilize a full\nin-focus pinhole camera model which does incorporate these desired artistic\nattributes. To overcome this, the main contribution of this work is to endow\nthe well-known drone simulator, AirSim, with a cinematic camera as well as\nextended its API to control all of its parameters in real time, including\nvarious filming lenses and common cinematographic properties. In this paper, we\ndetail the implementation of our AirSim modification, CinemAirSim, present\nexamples that illustrate the potential of the new tool, and highlight the new\nresearch opportunities that the use of cinematic cameras can bring to research\nin robotics and control. https://github.com/ppueyor/CinematicAirSim\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.10247,regular,pre_llm,2020,3,"{'ai_likelihood': 3.741847144232856e-06, 'text': 'Linear Time-Varying MPC for Nonprehensile Object Manipulation with a\n  Nonholonomic Mobile Robot\n\n  This paper proposes a technique to manipulate an object with a nonholonomic\nmobile robot by pushing, which is a nonprehensile manipulation motion\nprimitive. Such a primitive involves unilateral constraints associated with the\nfriction between the robot and the manipulated object. Violating this\nconstraint produces the slippage of the object during the manipulation,\npreventing the correct achievement of the task. A linear time-varying model\npredictive control is designed to include the unilateral constraint within the\ncontrol action properly. The approach is verified in a dynamic simulation\nenvironment through a Pioneer 3-DX wheeled robot executing the pushing\nmanipulation of a package.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.01369,regular,pre_llm,2020,3,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Traversing the Reality Gap via Simulator Tuning\n\n  The large demand for simulated data has made the reality gap a problem on the\nforefront of robotics. We propose a method to traverse the gap by tuning\navailable simulation parameters. Through the optimisation of physics engine\nparameters, we show that we are able to narrow the gap between simulated\nsolutions and a real world dataset, and thus allow more ready transfer of\nleaned behaviours between the two. We subsequently gain understanding as to the\nimportance of specific simulator parameters, which is of broad interest to the\nrobotic machine learning community. We find that even optimised for different\ntasks that different physics engine perform better in certain scenarios and\nthat friction and maximum actuator velocity are tightly bounded parameters that\ngreatly impact the transference of simulated solutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2003.05395,regular,pre_llm,2020,3,"{'ai_likelihood': 2.9802322387695312e-06, 'text': ""Frozone: Freezing-Free, Pedestrian-Friendly Navigation in Human Crowds\n\n  We present Frozone, a novel algorithm to deal with the Freezing Robot Problem\n(FRP) that arises when a robot navigates through dense scenarios and crowds.\nOur method senses and explicitly predicts the trajectories of pedestrians and\nconstructs a Potential Freezing Zone (PFZ); a spatial zone where the robot\ncould freeze or be obtrusive to humans. Our formulation computes a deviation\nvelocity to avoid the PFZ, which also accounts for social constraints.\nFurthermore, Frozone is designed for robots equipped with sensors with a\nlimited sensing range and field of view. We ensure that the robot's deviation\nis bounded, thus avoiding sudden angular motion which could lead to the loss of\nperception data of the surrounding obstacles. We have combined Frozone with a\nDeep Reinforcement Learning-based (DRL) collision avoidance method and use our\nhybrid approach to handle crowds of varying densities. Our overall approach\nresults in smooth and collision-free navigation in dense environments. We have\nevaluated our method's performance in simulation and on real differential drive\nrobots in challenging indoor scenarios. We highlight the benefits of our\napproach over prior methods in terms of success rates (up to 50% increase),\npedestrian-friendliness (100% increase) and the rate of freezing (> 80%\ndecrease) in challenging scenarios.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.03736,regular,pre_llm,2020,4,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Learning Mixed-Integer Convex Optimization Strategies for Robot Planning\n  and Control\n\n  Mixed-integer convex programming (MICP) has seen significant algorithmic and\nhardware improvements with several orders of magnitude solve time speedups\ncompared to 25 years ago. Despite these advances, MICP has been rarely applied\nto real-world robotic control because the solution times are still too slow for\nonline applications. In this work, we present the CoCo (Combinatorial Offline,\nConvex Online) framework to solve MICPs arising in robotics at very high speed.\nCoCo encodes the combinatorial part of the optimal solution into a strategy.\nUsing data collected from offline problem solutions, we train a multiclass\nclassifier to predict the optimal strategy given problem-specific parameters\nsuch as states or obstacles. Compared to previous approaches, we use\ntask-specific strategies and prune redundant ones to significantly reduce the\nnumber of classes the predictor has to select from, thereby greatly improving\nscalability. Given the predicted strategy, the control task becomes a small\nconvex optimization problem that we can solve in milliseconds. Numerical\nexperiments on a cart-pole system with walls, a free-flying space robot, and\ntask-oriented grasps show that our method provides not only 1 to 2 orders of\nmagnitude speedups compared to state-of-the-art solvers but also performance\nclose to the globally optimal MICP solution.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00269,regular,pre_llm,2020,4,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Coupling of localization and depth data for mapping using Intel\n  RealSense T265 and D435i cameras\n\n  We propose to couple two types of Intel RealSense sensors (tracking T265 and\ndepth D435i) in order to obtain localization and 3D occupancy map of the indoor\nenvironment. We implemented a python-based observer pattern with multi-threaded\napproach for camera data synchronization. We compared different point cloud\n(PC) alignment methods (using transformations obtained from tracking camera and\nfrom ICP family methods). Tracking camera and PC alignment allow us to generate\na set of transformations between frames. Based on these transformations we\nobtained different trajectories and provided their analysis. Finally, having\nposes for all frames, we combined depth data. Firstly we obtained a joint PC\nrepresenting the whole scene. Then we used Octomap representation to build a\nmap.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.02996,regular,pre_llm,2020,4,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Variable Autonomy of Whole-body Control for Inspection and Intervention\n  in Industrial Environments using Legged Robots\n\n  The deployment of robots in industrial and civil scenarios is a viable\nsolution to protect operators from danger and hazards. Shared autonomy is\nparamount to enable remote control of complex systems such as legged robots,\nallowing the operator to focus on the essential tasks instead of overly\ndetailed execution. To realize this, we propose a comprehensive control\nframework for inspection and intervention using a legged robot and validate the\nintegration of multiple loco-manipulation algorithms optimised for improving\nthe remote operation. The proposed control offers 3 operation modes: fully\nautomated, semi-autonomous, and the haptic interface receiving onsite physical\ninteraction for assisting teleoperation. Our contribution is the design of a\nQP-based semi-analytical whole-body control, which is the key to the various\ntask completion subject to internal and external constraints. We demonstrate\nthe versatility of the whole-body control in terms of decoupling tasks,\nsingularity tolerance and constraint satisfaction. We deploy our solution in\nfield trials and evaluate in an emergency setting by an E-stop while the robot\nis clearing road barriers and traversing difficult terrains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00467,regular,pre_llm,2020,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Quasi-Direct Drive Actuation for a Lightweight Hip Exoskeleton with High\n  Backdrivability and High Bandwidth\n\n  High-performance actuators are crucial to enable mechanical versatility of\nlower-limb wearable robots, which are required to be lightweight, highly\nbackdrivable, and with high bandwidth. State-of-the-art actuators, e.g., series\nelastic actuators (SEAs), have to compromise bandwidth to improve compliance\n(i.e., backdrivability). In this paper, we describe the design and human-robot\ninteraction modeling of a portable hip exoskeleton based on our custom\nquasi-direct drive (QDD) actuation (i.e., a high torque density motor with low\nratio gear). We also present a model-based performance benchmark comparison of\nrepresentative actuators in terms of torque capability, control bandwidth,\nbackdrivability, and force tracking accuracy. This paper aims to corroborate\nthe underlying philosophy of ""design for control"", namely meticulous robot\ndesign can simplify control algorithms while ensuring high performance.\nFollowing this idea, we create a lightweight bilateral hip exoskeleton (overall\nmass is 3.4 kg) to reduce joint loadings during normal activities, including\nwalking and squatting. Experimental results indicate that the exoskeleton is\nable to produce high nominal torque (17.5 Nm), high backdrivability (0.4 Nm\nbackdrive torque), high bandwidth (62.4 Hz), and high control accuracy (1.09 Nm\nroot mean square tracking error, i.e., 5.4% of the desired peak torque). Its\ncontroller is versatile to assist walking at different speeds (0.8-1.4 m/s) and\nsquatting at 2 s cadence. This work demonstrates significant improvement in\nbackdrivability and control bandwidth compared with state-of-the-art\nexoskeletons powered by the conventional actuation or SEA.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.07806,regular,pre_llm,2020,4,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'MobiAxis: An Embodied Learning Task for Teaching Multiplication with a\n  Social Robot\n\n  The use of robots in educational settings is growing increasingly popular.\nYet, many of the learning tasks involving social robots do not take full\nadvantage of their physical embodiment. MobiAxis is a proposed learning task\nwhich uses the physical capabilities of a Pepper robot to teach the concepts of\npositive and negative multiplication along a number line. The robot is embodied\nwith a number of multi-modal socially intelligent features and behaviours which\nare designed to enhance learning. This paper is a position paper describing the\ntechnical and theoretical implementation of the task, as well as proposed\ndirections for future studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.03089,regular,pre_llm,2020,4,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Realtime Collision Avoidance for Mobile Robots in Dense Crowds using\n  Implicit Multi-sensor Fusion and Deep Reinforcement Learning\n\n  We present a novel learning-based collision avoidance algorithm, CrowdSteer,\nfor mobile robots operating in dense and crowded environments. Our approach is\nend-to-end and uses multiple perception sensors such as a 2-D lidar along with\na depth camera to sense surrounding dynamic agents and compute collision-free\nvelocities. Our training approach is based on the sim-to-real paradigm and uses\nhigh fidelity 3-D simulations of pedestrians and the environment to train a\npolicy using Proximal Policy Optimization (PPO). We show that our learned\nnavigation model is directly transferable to previously unseen virtual and\ndense real-world environments. We have integrated our algorithm with\ndifferential drive robots and evaluated its performance in narrow scenarios\nsuch as dense crowds, narrow corridors, T-junctions, L-junctions, etc. In\npractice, our approach can perform real-time collision avoidance and generate\nsmooth trajectories in such complex scenarios. We also compare the performance\nwith prior methods based on metrics such as trajectory length, mean time to\ngoal, success rate, and smoothness and observe considerable improvement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.03405,regular,pre_llm,2020,4,"{'ai_likelihood': 3.5100513034396704e-06, 'text': ""Curved patch mapping and tracking for irregular terrain modeling:\n  Application to bipedal robot foot placement\n\n  Legged robots need to make contact with irregular surfaces, when operating in\nunstructured natural terrains. Representing and perceiving these areas to\nreason about potential contact between a robot and its surrounding environment,\nis still largely an open problem. This paper introduces a new framework to\nmodel and map local rough terrain surfaces, for tasks such as bipedal robot\nfoot placement. The system operates in real-time, on data from an RGB-D and an\nIMU sensor. We introduce a set of parametrized patch models and an algorithm to\nfit them in the environment. Potential contacts are identified as bounded\ncurved patches of approximately the same size as the robot's foot sole. This\nincludes sparse seed point sampling, point cloud neighborhood search, and patch\nfitting and validation. We also present a mapping and tracking system, where\npatches are maintained in a local spatial map around the robot as it moves. A\nbio-inspired sampling algorithm is introduced for finding salient contacts. We\ninclude a dense volumetric fusion layer for spatiotemporally tracking, using\nmultiple depth data to reconstruct a local point cloud. We present experimental\nresults on a mini-biped robot that performs foot placements on rocks,\nimplementing a 3D foothold perception system, that uses the developed patch\nmapping and tracking framework.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.016,regular,pre_llm,2020,4,"{'ai_likelihood': 5.099508497450087e-06, 'text': ""VGPN: Voice-Guided Pointing Robot Navigation for Humans\n\n  Pointing gestures are widely used in robot navigationapproaches nowadays.\nHowever, most approaches only use point-ing gestures, and these have two major\nlimitations. Firstly, they need to recognize pointing gestures all the time,\nwhich leads to long processing time and significant system overheads.\nSecondly,the user's pointing direction may not be very accurate, so the robot\nmay go to an undesired place. To relieve these limitations,we propose a\nvoice-guided pointing robot navigation approach named VGPN, and implement its\nprototype on a wheeled robot,TurtleBot 2. VGPN recognizes a pointing gesture\nonly if voice information is insufficient for navigation. VGPN also uses voice\ninformation as a supplementary channel to help determine the target position of\nthe user's pointing gesture. In the evaluation,we compare VGPN to the\npointing-only navigation approach. The results show that VGPN effectively\nreduces the processing timecost when pointing gesture is unnecessary, and\nimproves the usersatisfaction with navigation accuracy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00749,regular,pre_llm,2020,4,"{'ai_likelihood': 6.291601392957899e-07, 'text': ""Learned and Controlled Autonomous Robotic Exploration in an Extreme,\n  Unknown Environment\n\n  Exploring and traversing extreme terrain with surface robots is difficult,\nbut highly desirable for many applications, including exploration of planetary\nsurfaces, search and rescue, among others. For these applications, to ensure\nthe robot can predictably locomote, the interaction between the terrain and\nvehicle, terramechanics, must be incorporated into the model of the robot's\nlocomotion. Modeling terramechanic effects is difficult and may be impossible\nin situations where the terrain is not known a priori. For these reasons,\nlearning a terramechanics model online is desirable to increase the\npredictability of the robot's motion. A problem with previous implementations\nof learning algorithms is that the terramechanics model and corresponding\ngenerated control policies are not easily interpretable or extensible. If the\nmodels were of interpretable form, designers could use the learned models to\ninform vehicle and/or control design changes to refine the robot architecture\nfor future applications. This paper explores a new method for learning a\nterramechanics model and a control policy using a model-based genetic\nalgorithm. The proposed method yields an interpretable model, which can be\nanalyzed using preexisting analysis methods. The paper provides simulation\nresults that show for a practical application, the genetic algorithm\nperformance is approximately equal to the performance of a state-of-the-art\nneural network approach, which does not provide an easily interpretable model.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.05241,regular,pre_llm,2020,4,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'TIE: Time-Informed Exploration For Robot Motion Planning\n\n  Anytime sampling-based methods are an attractive technique for solving\nkino-dynamic motion planning problems. These algorithms scale well to higher\ndimensions and can efficiently handle state and control constraints. However,\nan intelligent exploration strategy is required to accelerate their convergence\nand avoid redundant computations. Using ideas from reachability analysis, this\nwork defines a ""Time-Informed Set"", that focuses the search for time-optimal\nkino-dynamic planning after an initial solution is found. Such a Time-Informed\nSet (TIS) includes all trajectories that can potentially improve the current\nbest solution and hence exploration outside this set is redundant. Benchmarking\nexperiments show that an exploration strategy based on the TIS can accelerate\nthe convergence of sampling-based kino-dynamic motion planners.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.12083,regular,pre_llm,2020,4,"{'ai_likelihood': 6.192260318332249e-06, 'text': 'Non-Linear Trajectory Optimization for Large Step-Ups: Application to\n  the Humanoid Robot Atlas\n\n  Performing large step-ups is a challenging task for a humanoid robot. It\nrequires the robot to perform motions at the limit of its reachable workspace\nwhile straining to move its body upon the obstacle. This paper presents a\nnon-linear trajectory optimization method for generating step-up motions. We\nadopt a simplified model of the centroidal dynamics to generate feasible Center\nof Mass trajectories aimed at reducing the torques required for the step-up\nmotion. The activation and deactivation of contacts at both feet are considered\nexplicitly. The output of the planner is a Center of Mass trajectory plus an\noptimal duration for each walking phase. These desired values are stabilized by\na whole-body controller that determines a set of desired joint torques. We\nexperimentally demonstrate that by using trajectory optimization techniques,\nthe maximum torque required to the full-size humanoid robot Atlas can be\nreduced up to 20% when performing a step-up motion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.08452,regular,pre_llm,2020,4,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'A Subterranean Virtual Cave World for Gazebo based on the DARPA SubT\n  Challenge\n\n  Subterranean environments with lots of obstacles, including narrow passages,\nlarge voids, rock falls and absence of illumination were always challenging for\ncontrol, navigation, and perception of mobile robots. The limited availability\nand access to such environments restricts the development pace of capabilities\nfor robotic platforms to autonomously accomplish tasks in such challenging\nareas. The Subterranean Challenge is a competition focusing on bringing robotic\nexploration a step closer to real life applications for man-made underground\ntunnels, urban areas and natural cave networks, envisioning advanced assistance\ntools for first responders and disaster relief agencies. The challenge offers a\nsoftware-based virtual part to showcase technologies in autonomy perception,\nnetworking and mobility for such areas. Thus, the presented open-source virtual\nworld aims to become a test-bed for evaluating the developed algorithms and\nsoftware and to foster mobile robotics developments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00249,regular,pre_llm,2020,4,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Learning to Place Objects onto Flat Surfaces in Upright Orientations\n\n  We study the problem of placing a grasped object on an empty flat surface in\nan upright orientation, such as placing a cup on its bottom rather than on its\nside. We aim to find the required object rotation such that when the gripper is\nopened after the object makes contact with the surface, the object would be\nstably placed in the upright orientation. We iteratively use two neural\nnetworks. At every iteration, we use a convolutional neural network to estimate\nthe required object rotation, which is executed by the robot, and then a\nseparate convolutional neural network to estimate the quality of a placement in\nits current orientation. Our approach places previously unseen objects in\nupright orientations with a success rate of 98.1% in free space and 90.3% with\na simulated robotic arm, using a dataset of 50 everyday objects in simulation\nexperiments. Real-world experiments were performed, which achieved an 88.0%\nsuccess rate, which serves as a proof-of-concept for direct sim-to-real\ntransfer.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.01269,regular,pre_llm,2020,4,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Series Elastic Force Control for Soft Robotic Fluid Actuators\n\n  Fluid-based soft actuators are an attractive option for lightweight and\nhuman-safe robots. These actuators, combined with fluid pressure force\nfeedback, are in principle a form of series-elastic actuation (SEA), in which\nnearly all driving-point (e.g. motor/gearbox) friction can be eliminated.\nFiber-elastomer soft actuators offer unique low-friction and low-hysteresis\nmechanical properties which are particularly suited to force-control based on\ninternal pressure force feedback, rather than traditional external force\nfeedback using force/tactile sensing, since discontinuous (Coulomb) endpoint\nfriction is unobservable to internal fluid pressure. However, compensation of\nendpoint smooth hysteresis through a model-based feedforward term is possible.\nWe report on internal-pressure force feedback through a disturbance observer\n(DOB) and model-based feedforward compensation of endpoint friction and\nnonlinear hysteresis for a 2-DOF lightweight robotic gripper driven by\nrolling-diaphragm linear actuators coupled to direct-drive brushless motors,\nachieving an active low-frequency endpoint impedance range (""Z-width"") of 50dB.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.14938,regular,pre_llm,2020,4,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Adaptive Robust Kernels for Non-Linear Least Squares Problems\n\n  State estimation is a key ingredient in most robotic systems. Often, state\nestimation is performed using some form of least squares minimization.\nBasically, all error minimization procedures that work on real-world data use\nrobust kernels as the standard way for dealing with outliers in the data. These\nkernels, however, are often hand-picked, sometimes in different combinations,\nand their parameters need to be tuned manually for a particular problem. In\nthis paper, we propose the use of a generalized robust kernel family, which is\nautomatically tuned based on the distribution of the residuals and includes the\ncommon m-estimators. We tested our adaptive kernel with two popular estimation\nproblems in robotics, namely ICP and bundle adjustment. The experiments\npresented in this paper suggest that our approach provides higher robustness\nwhile avoiding a manual tuning of the kernel parameters.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.12962,regular,pre_llm,2020,4,"{'ai_likelihood': 2.2517310248480903e-06, 'text': ""Social and Emotional Skills Training with Embodied Moxie\n\n  We present a therapeutic framework, namely STAR Framework, that leverages\nestablished and evidence-based therapeutic strategies delivered by the Embodied\nMoxie, an animate companion to support children with mental behavioral\ndevelopmental disorders (MBDDs). This therapeutic framework jointly with Moxie\naims to provide an engaging, safe, and secure environment for children aged\nfive to ten years old. Moxie delivers content informed by therapeutic\nstrategies including but not limited to naturalistic Applied Behavior Analysis,\ngraded cueing, and Cognitive Behavior Therapy. Leveraging multimodal input from\na camera and microphones, Moxie is uniquely positioned to be a first-hand\nwitness of a child's progress and struggles alike. Moxie measures skills\ncaptured in state-of-the-art assessment scales, such as the Social\nResponsiveness Scale and Social Skill Improvement Scale, and augments those\nmeasures with quantitatively measured behavior skills, such as eye contact and\nlanguage skills. While preliminary, the present study (N=12) also provides\nevidence that a six-week intervention using the STAR Framework and Moxie had\nsignificant impact on the children's abilities. We present our research in\ndetail and provide an overview of the STAR Framework and all related\ncomponents, such as Moxie and the companion app for parents.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.13154,regular,pre_llm,2020,4,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Voxgraph: Globally Consistent, Volumetric Mapping using Signed Distance\n  Function Submaps\n\n  Globally consistent dense maps are a key requirement for long-term robot\nnavigation in complex environments. While previous works have addressed the\nchallenges of dense mapping and global consistency, most require more\ncomputational resources than may be available on-board small robots. We propose\na framework that creates globally consistent volumetric maps on a CPU and is\nlightweight enough to run on computationally constrained platforms. Our\napproach represents the environment as a collection of overlapping Signed\nDistance Function (SDF) submaps, and maintains global consistency by computing\nan optimal alignment of the submap collection. By exploiting the underlying SDF\nrepresentation, we generate correspondence free constraints between submap\npairs that are computationally efficient enough to optimize the global problem\neach time a new submap is added. We deploy the proposed system on a hexacopter\nMicro Aerial Vehicle (MAV) with an Intel i7-8650U CPU in two realistic\nscenarios: mapping a large-scale area using a 3D LiDAR, and mapping an\nindustrial space using an RGB-D camera. In the large-scale outdoor experiments,\nthe system optimizes a 120x80m map in less than 4s and produces absolute\ntrajectory RMSEs of less than 1m over 400m trajectories. Our complete system,\ncalled voxgraph, is available as open source.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.00946,regular,pre_llm,2020,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Human-Guided Planner for Non-Prehensile Manipulation\n\n  We present a human-guided planner for non-prehensile manipulation in clutter.\nMost recent approaches to manipulation in clutter employs randomized planning,\nhowever, the problem remains a challenging one where the planning times are\nstill in the order of tens of seconds or minutes, and the success rates are low\nfor difficult instances of the problem. We build on these control-based\nrandomized planning approaches, but we investigate using them in conjunction\nwith human-operator input. We show that with a minimal amount of human input,\nthe low-level planner can solve the problem faster and with higher success\nrates.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.05131,regular,pre_llm,2020,4,"{'ai_likelihood': 3.311369154188368e-06, 'text': 'Evaluation of Skid-Steering Kinematic Models for Subarctic Environments\n\n  In subarctic and arctic areas, large and heavy skid-steered robots are\npreferred for their robustness and ability to operate on difficult terrain.\nState estimation, motion control and path planning for these robots rely on\naccurate odometry models based on wheel velocities. However, the\nstate-of-the-art odometry models for skid-steer mobile robots (SSMRs) have\nusually been tested on relatively lightweight platforms. In this paper, we\nfocus on how these models perform when deployed on a large and heavy (590 kg)\nSSMR. We collected more than 2 km of data on both snow and concrete. We compare\nthe ideal differential-drive, extended differential-drive,\nradius-of-curvature-based, and full linear kinematic models commonly deployed\nfor SSMRs. Each of the models is fine-tuned by searching their optimal\nparameters on both snow and concrete. We then discuss the relationship between\nthe parameters, the model tuning, and the final accuracy of the models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2004.05097,regular,pre_llm,2020,4,"{'ai_likelihood': 5.7948960198296445e-06, 'text': ""Residual Policy Learning for Shared Autonomy\n\n  Shared autonomy provides an effective framework for human-robot collaboration\nthat takes advantage of the complementary strengths of humans and robots to\nachieve common goals. Many existing approaches to shared autonomy make\nrestrictive assumptions that the goal space, environment dynamics, or human\npolicy are known a priori, or are limited to discrete action spaces, preventing\nthose methods from scaling to complicated real world environments. We propose a\nmodel-free, residual policy learning algorithm for shared autonomy that\nalleviates the need for these assumptions. Our agents are trained to minimally\nadjust the human's actions such that a set of goal-agnostic constraints are\nsatisfied. We test our method in two continuous control environments: Lunar\nLander, a 2D flight control domain, and a 6-DOF quadrotor reaching task. In\nexperiments with human and surrogate pilots, our method significantly improves\ntask performance without any knowledge of the human's goal beyond the\nconstraints. These results highlight the ability of model-free deep\nreinforcement learning to realize assistive agents suited to continuous control\nsettings with little knowledge of user intent.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.05599,regular,pre_llm,2020,5,"{'ai_likelihood': 3.5762786865234375e-06, 'text': ""A new RCM mechanism for an ear and facial surgical application\n\n  Since the insertion area in the middle ear or in the sinus cavity is very\nnarrow, the mobility of the endoscope is reduced to a rotation around a virtual\npoint and a translation for the insertion of the camera. This article first\npresents the anatomy of these regions obtained from 3D scanning and then a\nmechanism based on the architecture of the agile eye coupled to a double\nparallelogram to create an RCM. This mechanism coupled with a positioning\nmechanism is used to handle an endoscope. This tool is used in parallel to the\nsurgeon to allow him to have better rendering of the medium ear than the use of\nBinocular scope. The mechanism offers a wide working space without singularity\nwhose borders are fixed by joint limits. This feature allows ergonomic\npositioning of the patient's head on the bed as well as for the surgeon and\nallows other applications such as sinus surgery.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.11094,regular,pre_llm,2020,5,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'On the Potential of Smarter Multi-layer Maps\n\n  The most common way for robots to handle environmental information is by\nusing maps. At present, each kind of data is hosted on a separate map, which\ncomplicates planning because a robot attempting to perform a task needs to\naccess and process information from many different maps. Also, most often\ncorrelation among the information contained in maps obtained from different\nsources is not evaluated or exploited. In this paper, we argue that in robotics\na shift from single-source maps to a multi-layer mapping formalism has the\npotential to revolutionize the way robots interact with knowledge about their\nenvironment. This observation stems from the raise in metric-semantic mapping\nresearch, but expands to include in its formulation also layers containing\nother information sources, e.g., people flow, room semantic, or environment\ntopology. Such multi-layer maps, here named hypermaps, not only can ease\nprocessing spatial data information but they can bring added benefits arising\nfrom the interaction between maps. We imagine that a new research direction\ngrounded in such multi-layer mapping formalism for robots can use artificial\nintelligence to process the information it stores to present to the robot\ntask-specific information simplifying planning and bringing us one step closer\nto high-level reasoning in robots.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.07769,regular,pre_llm,2020,5,"{'ai_likelihood': 1.655684577094184e-06, 'text': ""A Multi-State Social Force Based Framework for Vehicle-Pedestrian\n  Interaction in Uncontrolled Pedestrian Crossing Scenarios\n\n  Vehicle-pedestrian interaction (VPI) is one of the most challenging tasks for\nautomated driving systems. The design of driving strategies for such systems\nusually starts with verifying VPI in simulation. This work proposed an improved\nframework for the study of VPI in uncontrolled pedestrian crossing scenarios.\nThe framework admits the mutual effect between the pedestrian and the vehicle.\nA multi-state social force based pedestrian motion model was designed to\ndescribe the microscopic motion of the pedestrian crossing behavior. The\npedestrian model considers major interaction factors such as the accepted gap\nof the pedestrian's decision on when to start crossing, the desired speed of\nthe pedestrian, and the effect of the vehicle on the pedestrian while the\npedestrian is crossing the road. Vehicle driving strategies focus on the\nlongitudinal motion control, for which the feedback obstacle avoidance control\nand the model predictive control were tested and compared in the framework. The\nsimulation results verified that the proposed framework can generate a variety\nof VPI scenarios, consisting of either the pedestrian yielding to the vehicle\nor the vehicle yielding to the pedestrian. The framework can be easily extended\nto apply different approaches to the VPI problems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04191,regular,pre_llm,2020,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Adaptive Motion Planning with Artificial Potential Fields Using a Prior\n  Path\n\n  Motion planning in an autonomous agent is responsible for providing smooth,\nsafe and efficient navigation. Many solutions for dealing this problem have\nbeen offered, one of which is, Artificial Potential Fields (APF). APF is a\nsimple and computationally low cost method which keeps the robot away from the\nobstacles in environment. However, this approach suffers from trapping in local\nminima of potential function and then fails to produce motion plans.\nFurthermore, Oscillation in presence of obstacles or in narrow passages is\nanother disadvantage of the method which makes it unqualified for many planning\nproblems. In this paper we aim to resolve these deficiencies by a novel\napproach which employs a prior path between origin and goal configuration of\nthe robot. Therefore, the planner guarantees to lead the robot to goal area\nwhile the inherent advantages of potential fields remain. For path planning\nstage, we intend to use randomized sampling methods such as Rapidly-exploring\nRandom Trees (RRT) or its derivatives, however, any path planning approach can\nbe utilized. We have also designed an optimization procedure for evolving the\nmotion plans towards optimal solution. Then genetic algorithm is applied to\nfind smoother, safer and shorter plans. In our experiments, we apply a\nsimulated vehicle in Webots simulator to test and evaluate the motion planner.\nOur experiments showed our method to enjoy improving the performance and speed\nin comparison to basic approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.0842,regular,pre_llm,2020,5,"{'ai_likelihood': 1.529852549235026e-05, 'text': ""A Visual Kinematics Calibration Method for Manipulator Based on\n  Nonlinear Optimization\n\n  The traditional kinematic calibration method for manipulators requires\nprecise three-dimensional measuring instruments to measure the end pose, which\nis not only expensive due to the high cost of the measuring instruments but\nalso not applicable to all manipulators. Another calibration method uses a\ncamera, but the system error caused by the camera's parameters affects the\ncalibration accuracy of the kinematics of the robot arm. Therefore, this paper\nproposes a method for calibrating the geometric parameters of a kinematic model\nof a manipulator based on monocular vision. Firstly, the classic\nDenavit-Hartenberg(D-H) modeling method is used to establish the kinematic\nparameters of the manipulator. Secondly, nonlinear optimization and parameter\ncompensation are performed. The three-dimensional positions of the feature\npoints of the calibration plate under each manipulator attitude corresponding\nto the actual kinematic model and the classic D-H kinematic model are mapped\ninto the pixel coordinate system, and the sum of Euclidean distance errors of\nthe pixel coordinates of the two is used as the objective function to be\noptimized. The experimental results show that the pixel deviation of the end\npose corresponding to the optimized D-H kinematic model proposed in this paper\nand the end pose corresponding to the actual kinematic model in the pixel\ncoordinate system is 0.99 pixels. Compared with the 7.9 deviation pixels\nbetween the pixel coordinates calculated by the classic D-H kinematic model and\nthe actual pixel coordinates, the deviation is reduced by nearly 7 pixels for\nan 87% reduction in error. Therefore, the proposed method can effectively avoid\nsystem errors caused by camera parameters in visual calibration, can improve\nthe absolute positioning accuracy of the end of the robotic arm, and has good\neconomy and universality.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.11435,regular,pre_llm,2020,5,"{'ai_likelihood': 6.126032935248481e-06, 'text': 'Ascento: A Two-Wheeled Jumping Robot\n\n  Applications of mobile ground robots demand high speed and agility while\nnavigating in complex indoor environments. These present an ongoing challenge\nin mobile robotics. A system with these specifications would be of great use\nfor a wide range of indoor inspection tasks. This paper introduces Ascento, a\ncompact wheeled bipedal robot that is able to move quickly on flat terrain, and\nto overcome obstacles by jumping. The mechanical design and overall\narchitecture of the system is presented, as well as the development of various\ncontrollers for different scenarios. A series of experiments with the final\nprototype system validate these behaviors in realistic scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.10648,regular,pre_llm,2020,5,"{'ai_likelihood': 4.238552517361111e-06, 'text': 'Accurate position tracking with a single UWB anchor\n\n  Accurate localization and tracking are a fundamental requirement for robotic\napplications. Localization systems like GPS, optical tracking, simultaneous\nlocalization and mapping (SLAM) are used for daily life activities, research,\nand commercial applications. Ultra-wideband (UWB) technology provides another\nvenue to accurately locate devices both indoors and outdoors. In this paper, we\nstudy a localization solution with a single UWB anchor, instead of the\ntraditional multi-anchor setup. Besides the challenge of a single UWB ranging\nsource, the only other sensor we require is a low-cost 9 DoF inertial\nmeasurement unit (IMU). Under such a configuration, we propose continuous\nmonitoring of UWB range changes to estimate the robot speed when moving on a\nline. Combining speed estimation with orientation estimation from the IMU\nsensor, the system becomes temporally observable. We use an Extended Kalman\nFilter (EKF) to estimate the pose of a robot. With our solution, we can\neffectively correct the accumulated error and maintain accurate tracking of a\nmoving robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.03809,regular,pre_llm,2020,5,"{'ai_likelihood': 0.0, 'text': ""A Monte Carlo Approach to Closing the Reality Gap\n\n  We propose a novel approach to the 'reality gap' problem, i.e., modifying a\nrobot simulation so that its performance becomes more similar to observed real\nworld phenomena. This problem arises whether the simulation is being used by\nhuman designers or in an automated policy development mechanism. We expect that\nthe program/policy is developed using simulation, and subsequently deployed on\na real system. We further assume that the program includes a monitor procedure\nwith scalar output to determine when it is achieving its performance\nobjectives. The proposed approach collects simulation and real world\nobservations and builds conditional probability functions. These are used to\ngenerate paired roll-outs to identify points of divergence in behavior. These\nare used to generate {\\it state-space kernels} that coerce the simulation into\nbehaving more like observed reality.\n  The method was evaluated using ROS/Gazebo for simulation and a heavily\nmodified Traaxas platform in outdoor deployment. The results support not just\nthat the kernel approach can force the simulation to behave more like reality,\nbut that the modification is such that an improved control policy tested in the\nmodified simulation also performs better in the real world.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.08567,regular,pre_llm,2020,5,"{'ai_likelihood': 0.0, 'text': 'GenNav: A Generic Indoor Navigation System for any Mobile Robot\n\n  The navigation system is at the heart of any mobile robot it comprises of\nSLAM and path planning units, which is utilized by the robot to generate a map\nof the environment, localize itself within it and determine an optimal a path\nto the destination. This paper describes the conceptualization, development,\nsimulation and hardware implementation of GenNav a generic indoor navigation\nsystem for any mobile aerial or ground robot. The generalization is brought\nabout by modularizing and creating independence between the software\ncomputation and hardware actuation units by providing an alternate source of\nodometry from the LiDAR eliminating the requirement for dedicated odometry\nsensors. The odometry feedback from the LiDAR can be used by the navigation\ncomputation unit and the system can be generalized to a wide variety of robots,\nwith different type and orientation of actuators\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.08298,regular,pre_llm,2020,5,"{'ai_likelihood': 2.5497542487250434e-06, 'text': ""Certifiably Optimal Monocular Hand-Eye Calibration\n\n  Correct fusion of data from two sensors is not possible without an accurate\nestimate of their relative pose, which can be determined through the process of\nextrinsic calibration. When two or more sensors are capable of producing their\nown egomotion estimates (i.e., measurements of their trajectories through an\nenvironment), the 'hand-eye' formulation of extrinsic calibration can be\nemployed. In this paper, we extend our recent work on a convex optimization\napproach for hand-eye calibration to the case where one of the sensors cannot\nobserve the scale of its translational motion (e.g., a monocular camera\nobserving an unmapped environment). We prove that our technique is able to\nprovide a certifiably globally optimal solution to both the known- and\nunknown-scale variants of hand-eye calibration, provided that the measurement\nnoise is bounded. Herein, we focus on the theoretical aspects of the problem,\nshow the tightness and stability of our solution, and demonstrate the\noptimality and speed of our algorithm through experiments with synthetic data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.07237,regular,pre_llm,2020,5,"{'ai_likelihood': 6.722079383002388e-06, 'text': 'Autonomous Planning for Multiple Aerial Cinematographers\n\n  This paper proposes a planning algorithm for autonomous media production with\nmultiple Unmanned Aerial Vehicles (UAVs) in outdoor events. Given filming tasks\nspecified by a media Director, we formulate an optimization problem to maximize\nthe filming time considering battery constraints. As we conjecture that the\nproblem is NP-hard, we consider a discretization version, and propose a\ngraph-based algorithm that can find an optimal solution of the discrete problem\nfor a single UAV in polynomial time. Then, a greedy strategy is applied to\nsolve the problem sequentially for multiple UAVs. We demonstrate that our\nalgorithm is efficient for small teams (3-5 UAVs) and that its performance is\nclose to the optimum. We showcase our system in field experiments carrying out\nactual media production in an outdoor scenario with multiple UAVs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.06715,regular,pre_llm,2020,5,"{'ai_likelihood': 6.606181462605795e-05, 'text': 'Towards the Long-Endurance Flight of an Insect-Inspired, Tailless,\n  Two-Winged, Flapping-Wing Flying Robot\n\n  A hover-capable insect-inspired flying robot that can remain long in the air\nhas shown its potential use for both confined indoor and outdoor applications\nto complete assigned tasks. In this letter, we report improvements in the\nflight endurance of our 15.8 g robot, named KUBeetle-S, using a low-voltage\npower source. The robot is equipped with a simple but effective control\nmechanism that can modulate the stroke plane for attitude stabilization and\ncontrol. Due to the demand for extended flight, we performed a series of\nexperiments on the lift generation and power requirement of the robot with\ndifferent stroke amplitudes and wing areas. We show that a larger wing with\nless inboard wing area improves the lift-to-power ratio and produces a peak\nlift-to-weight ratio of 1.34 at 3.7 V application. Flight tests show that the\nrobot employing the selected wing could hover for 8.8 minutes. Moreover, the\nrobot could perform maneuvers in any direction, fly outdoors, and carry\npayload, demonstrating its ability to enter the next phase of autonomous\nflight.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.03404,regular,pre_llm,2020,5,"{'ai_likelihood': 9.470515780978732e-06, 'text': ""A LiDAR-based real-time capable 3D Perception System for Automated\n  Driving in Urban Domains\n\n  We present a LiDAR-based and real-time capable 3D perception system for\nautomated driving in urban domains. The hierarchical system design is able to\nmodel stationary and movable parts of the environment simultaneously and under\nreal-time conditions. Our approach extends the state of the art by innovative\nin-detail enhancements for perceiving road users and drivable corridors even in\ncase of non-flat ground surfaces and overhanging or protruding elements. We\ndescribe a runtime-efficient pointcloud processing pipeline, consisting of\nadaptive ground surface estimation, 3D clustering and motion classification\nstages. Based on the pipeline's output, the stationary environment is\nrepresented in a multi-feature mapping and fusion approach. Movable elements\nare represented in an object tracking system capable of using multiple\nreference points to account for viewpoint changes. We further enhance the\ntracking system by explicit consideration of occlusion and ambiguity cases. Our\nsystem is evaluated using a subset of the TUBS Road User Dataset. We enhance\ncommon performance metrics by considering application-driven aspects of\nreal-world traffic scenarios. The perception system shows impressive results\nand is able to cope with the addressed scenarios while still preserving\nreal-time capability.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04213,regular,pre_llm,2020,5,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Cascade Attribute Network: Decomposing Reinforcement Learning Control\n  Policies using Hierarchical Neural Networks\n\n  Reinforcement learning methods have been developed to achieve great success\nin training control policies in various automation tasks. However, a main\nchallenge of the wider application of reinforcement learning in practical\nautomation is that the training process is hard and the pretrained policy\nnetworks are hardly reusable in other similar cases. To address this problem,\nwe propose the cascade attribute network (CAN), which utilizes its hierarchical\nstructure to decompose a complicated control policy in terms of the requirement\nconstraints, which we call attributes, encoded in the control tasks. We\nvalidated the effectiveness of our proposed method on two robot control\nscenarios with various add-on attributes. For some control tasks with more than\none add-on attribute attribute, by directly assembling the attribute modules in\ncascade, the CAN can provide ideal control policies in a zero-shot manner.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.02588,regular,pre_llm,2020,5,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'DeepClaw: A Robotic Hardware Benchmarking Platform for Learning Object\n  Manipulation\n\n  We present DeepClaw as a reconfigurable benchmark of robotic hardware and\ntask hierarchy for robot learning. The DeepClaw benchmark aims at a\nmechatronics perspective of the robot learning problem, which features a\nminimum design of robot cell that can be easily reconfigured to host robot\nhardware from various vendors, including manipulators, grippers, cameras,\ndesks, and objects, aiming at a streamlined collection of physical manipulation\ndata and evaluation of the learned skills for hardware benchmarking. We provide\na detailed design of the robot cell with readily available parts to build the\nexperiment environment that can host a wide range of robotic hardware commonly\nadopted for robot learning. We also propose a hierarchical pipeline of software\nintegration, including localization, recognition, grasp planning, and motion\nplanning, to streamline learning-based robot control, data collection, and\nexperiment validation towards shareability and reproducibility. We present\nbenchmarking results of the DeepClaw system for a baseline Tic-Tac-Toe task, a\nbin-clearing task, and a jigsaw puzzle task using three sets of standard\nrobotic hardware. Our results show that tasks defined in DeepClaw can be easily\nreproduced on three robot cells. Under the same task setup, the differences in\nrobotic hardware used will present a non-negligible impact on the performance\nmetrics of robot learning. All design layouts and codes are hosted on Github\nfor open access.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.03076,regular,pre_llm,2020,5,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Guided Policy Search Model-based Reinforcement Learning for Urban\n  Autonomous Driving\n\n  In this paper, we continue our prior work on using imitation learning (IL)\nand model free reinforcement learning (RL) to learn driving policies for\nautonomous driving in urban scenarios, by introducing a model based RL method\nto drive the autonomous vehicle in the Carla urban driving simulator. Although\nIL and model free RL methods have been proved to be capable of solving lots of\nchallenging tasks, including playing video games, robots, and, in our prior\nwork, urban driving, the low sample efficiency of such methods greatly limits\ntheir applications on actual autonomous driving. In this work, we developed a\nmodel based RL algorithm of guided policy search (GPS) for urban driving tasks.\nThe algorithm iteratively learns a parameterized dynamic model to approximate\nthe complex and interactive driving task, and optimizes the driving policy\nunder the nonlinear approximate dynamic model. As a model based RL approach,\nwhen applied in urban autonomous driving, the GPS has the advantages of higher\nsample efficiency, better interpretability, and greater stability. We provide\nextensive experiments validating the effectiveness of the proposed method to\nlearn robust driving policy for urban driving in Carla. We also compare the\nproposed method with other policy search and model free RL baselines, showing\n100x better sample efficiency of the GPS based RL method, and also that the GPS\nbased method can learn policies for harder tasks that the baseline methods can\nhardly learn.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.02042,regular,pre_llm,2020,5,"{'ai_likelihood': 1.9205941094292534e-06, 'text': ""SROM: Simple Real-time Odometry and Mapping using LiDAR data for\n  Autonomous Vehicles\n\n  In this paper, we present SROM, a novel real-time Simultaneous Localization\nand Mapping (SLAM) system for autonomous vehicles. The keynote of the paper\nshowcases SROM's ability to maintain localization at low sampling rates or at\nhigh linear or angular velocities where most popular LiDAR based localization\napproaches get degraded fast. We also demonstrate SROM to be computationally\nefficient and capable of handling high-speed maneuvers. It also achieves low\ndrifts without the need for any other sensors like IMU and/or GPS. Our method\nhas a two-layer structure wherein first, an approximate estimate of the\nrotation angle and translation parameters are calculated using a Phase Only\nCorrelation (POC) method. Next, we use this estimate as an initialization for a\npoint-to-plane ICP algorithm to obtain fine matching and registration. Another\nkey feature of the proposed algorithm is the removal of dynamic objects before\nmatching the scans. This improves the performance of our system as the dynamic\nobjects can corrupt the matching scheme and derail localization. Our SLAM\nsystem can build reliable maps at the same time generating high-quality\nodometry. We exhaustively evaluated the proposed method in many challenging\nhighways/country/urban sequences from the KITTI dataset and the results\ndemonstrate better accuracy in comparisons to other state-of-the-art methods\nwith reduced computational expense aiding in real-time realizations. We have\nalso integrated our SROM system with our in-house autonomous vehicle and\ncompared it with the state-of-the-art methods like LOAM and LeGO-LOAM.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.00739,regular,pre_llm,2020,5,"{'ai_likelihood': 4.6690305074055995e-06, 'text': 'Design-Informed Kinematic Control for Improved Dexterous Teleoperation\n  of a Bilateral Manipulator System\n\n  This paper explores the possibility of improving bilateral robot manipulation\ntask performance through optimizing the robot morphology and configuration of\nthe system through motion. To optimize the design for different scenarios, we\nselect a set of tasks that represent the variability in small scale\nmanipulation (e.g. pick and place, tasks involving positioning and orientation)\nand track the motion to obtain a reproducible trajectory. Kinematic data is\ncaptured through an electromagnetic (EM) tracker system while a human subject\nperforms the tasks. Then, the data is pre-processed and used to optimize the\nmorphology of each symmetric robot arm of the bilateral system. Once optimized,\na kinematic control scheme is used to generate a motion with dexterous\nconfigurations. The dexterity is evaluated along the trajectories with standard\ndexterity metrics. Results show a 10\\% improvement in dexterous maneuverability\nwith the optimized arm design and optimal base configuration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.04644,regular,pre_llm,2020,5,"{'ai_likelihood': 2.3411379920111762e-05, 'text': 'Radar-on-Lidar: metric radar localization on prior lidar maps\n\n  Radar and lidar, provided by two different range sensors, each has pros and\ncons of various perception tasks on mobile robots or autonomous driving. In\nthis paper, a Monte Carlo system is used to localize the robot with a rotating\nradar sensor on 2D lidar maps. We first train a conditional generative\nadversarial network to transfer raw radar data to lidar data, and achieve\nreliable radar points from generator. Then an efficient radar odometry is\nincluded in the Monte Carlo system. Combining the initial guess from odometry,\na measurement model is proposed to match the radar data and prior lidar maps\nfor final 2D positioning. We demonstrate the effectiveness of the proposed\nlocalization framework on the public multi-session dataset. The experimental\nresults show that our system can achieve high accuracy for long-term\nlocalization in outdoor scenes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2005.0812,regular,pre_llm,2020,5,"{'ai_likelihood': 1.519918441772461e-05, 'text': ""A Methodology to Assess the Human Factors Associated with Lunar\n  Teleoperated Assembly Tasks\n\n  Low-latency telerobotics can enable more intricate surface tasks on\nextraterrestrial planetary bodies than has ever been attempted. For humanity to\ncreate a sustainable lunar presence, well-developed collaboration between\nhumans and robots is necessary to perform complex tasks. This paper presents a\nmethodology to assess the human factors, situational awareness (SA) and\ncognitive load (CL), associated with teleoperated assembly tasks. Currently,\ntelerobotic assembly on an extraterrestrial body has never been attempted, and\na valid methodology to assess the associated human factors has not been\ndeveloped. The Telerobotics Laboratory at the University of Colorado-Boulder\ncreated the Telerobotic Simulation System (TSS) which enables remote operation\nof a rover and a robotic arm. The TSS was used in a laboratory experiment\ndesigned as an analog to a lunar mission. The operator's task was to assemble a\nradio interferometer. Each participant completed this task under two\nconditions, remote teleoperation (limited SA) and local operation (optimal SA).\nThe goal of the experiment was to establish a methodology to accurately measure\nthe operator's SA and CL while performing teleoperated assembly tasks. A\nsuccessful methodology would yield results showing greater SA and lower CL\nwhile operating locally. Performance metrics showed greater SA and lower CL in\nthe local environment, supported by a 27% increase in the mean time to\ncompletion of the assembly task when operating remotely. Subjective\nmeasurements of SA and CL did not align with the performance metrics. Results\nfrom this experiment will guide future work attempting to accurately quantify\nthe human factors associated with telerobotic assembly. Once an accurate\nmethodology has been developed, we will be able to measure how new variables\naffect an operator's SA and CL to optimize the efficiency and effectiveness of\ntelerobotic assembly tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04194,regular,pre_llm,2020,6,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Robotic Motion Planning using Learned Critical Sources and Local\n  Sampling\n\n  Sampling based methods are widely used for robotic motion planning.\nTraditionally, these samples are drawn from probabilistic ( or deterministic )\ndistributions to cover the state space uniformly. Despite being\nprobabilistically complete, they fail to find a feasible path in a reasonable\namount of time in constrained environments where it is essential to go through\nnarrow passages (bottleneck regions). Current state of the art techniques train\na learning model (learner) to predict samples selectively on these bottleneck\nregions. However, these algorithms depend completely on samples generated by\nthis learner to navigate through the bottleneck regions. As the complexity of\nthe planning problem increases, the amount of data and time required to make\nthis learner robust to fine variations in the structure of the workspace\nbecomes computationally intractable. In this work, we present (1) an efficient\nand robust method to use a learner to locate the bottleneck regions and (2) two\nalgorithms that use local sampling methods to leverage the location of these\nbottleneck regions for efficient motion planning while maintaining\nprobabilistic completeness.\n  We test our algorithms on 2 dimensional planning problems and 7 dimensional\nrobotic arm planning, and report significant gains over heuristics as well as\nlearned baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.02266,regular,pre_llm,2020,6,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'milliEgo: Single-chip mmWave Radar Aided Egomotion Estimation via Deep\n  Sensor Fusion\n\n  Robust and accurate trajectory estimation of mobile agents such as people and\nrobots is a key requirement for providing spatial awareness for emerging\ncapabilities such as augmented reality or autonomous interaction. Although\ncurrently dominated by optical techniques e.g., visual-inertial odometry, these\nsuffer from challenges with scene illumination or featureless surfaces. As an\nalternative, we propose milliEgo, a novel deep-learning approach to robust\negomotion estimation which exploits the capabilities of low-cost mmWave radar.\nAlthough mmWave radar has a fundamental advantage over monocular cameras of\nbeing metric i.e., providing absolute scale or depth, current single chip\nsolutions have limited and sparse imaging resolution, making existing\npoint-cloud registration techniques brittle. We propose a new architecture that\nis optimized for solving this challenging pose transformation problem.\nSecondly, to robustly fuse mmWave pose estimates with additional sensors, e.g.\ninertial or visual sensors we introduce a mixed attention approach to deep\nfusion. Through extensive experiments, we demonstrate our proposed system is\nable to achieve 1.3% 3D error drift and generalizes well to unseen\nenvironments. We also show that the neural architecture can be made highly\nefficient and suitable for real-time embedded applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.06966,regular,pre_llm,2020,6,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'RISCuer: A Reliable Multi-UAV Search and Rescue Testbed\n\n  We present the Robotics Intelligent Systems & Control (RISC) Lab multiagent\ntestbed for reliable search and rescue and aerial transport in outdoor\nenvironments. The system consists of a team of three multirotor unmanned aerial\nvehicles (UAVs), which are capable of autonomously searching, picking up, and\ntransporting randomly distributed objects in an outdoor field. The method\ninvolves vision based object detection and localization, passive aerial\ngrasping with our novel design, GPS based UAV navigation, and safe release of\nthe objects at the drop zone. Our cooperative strategy ensures safe spatial\nseparation between UAVs at all times and we prevent any conflicts at the drop\nzone using communication enabled consensus. All computation is performed\nonboard each UAV. We describe the complete software and hardware architecture\nfor the system and demonstrate its reliable performance using comprehensive\noutdoor experiments, and by comparing our results with some recent, similar\nworks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.11071,regular,pre_llm,2020,6,"{'ai_likelihood': 1.4967388576931424e-05, 'text': 'Distributed prediction of unsafe reconfiguration scenarios of modular\n  robotic Programmable Matter\n\n  We present a distributed framework for predicting whether a planned\nreconfiguration step of a modular robot will mechanically overload the\nstructure, causing it to break or lose stability under its own weight. The\nalgorithm is executed by the modular robot itself and based on a distributed\niterative solution of mechanical equilibrium equations derived from a\nsimplified model of the robot. The model treats inter-modular connections as\nbeams and assumes no-sliding contact between the modules and the ground. We\nalso provide a procedure for simplified instability detection. The algorithm is\nverified in the Programmable Matter simulator VisibleSim, and in real-life\nexperiments on the modular robotic system Blinky Blocks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.0631,regular,pre_llm,2020,6,"{'ai_likelihood': 3.841188218858507e-06, 'text': 'The Role of Modularity and Neuro-Regulation for the Production of\n  Multiple Behaviors\n\n  This project investigates whether functional specialization or modularity can\nsupport the development of multiple behaviors. In principle, modular solutions\nof this type can facilitate the development of multiple behaviors since each\nmodule is responsible for the production of different behavior. Consequently,\nthe interfaces that arise neural mechanisms supporting the production of\ndifferent behaviors can be reduced. The project involves the implementation of\nregulatory networks of this type and the realization of experiments involving\nthe production of different behaviors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.04778,regular,pre_llm,2020,6,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Shoulder abduction loading affects motor coordination in individuals\n  with chronic stroke, informing targeted rehabilitation\n\n  Individuals post stroke experience motor impairments, such as loss of\nindependent joint control, leading to an overall reduction in arm function.\nTheir motion becomes slower and more discoordinated, making it difficult to\ncomplete timing-sensitive tasks, such as balancing a glass of water or carrying\na bowl with a ball inside it. Understanding how the stroke-induced motor\nimpairments interact with each other can help design assisted training regimens\nfor improved recovery. In this study, we investigate the effects of abnormal\njoint coupling patterns induced by flexion synergy on timing-sensitive motor\ncoordination in the paretic upper limb. We design a virtual ball-in-bowl task\nthat requires fast movements for optimal performance and implement it on a\nrobotic system, capable of providing varying levels of abduction loading at the\nshoulder. We recruit 12 participants (6 individuals with chronic stroke and 6\nunimpaired controls) and assess their skill at the task at 3 levels of loading,\ndefined by the vertical force applied at the robot end-effector. Our results\nshow that, for individuals with stroke, loading has a significant effect on\ntheir ability to generate quick coordinated motion. With increases in loading,\ntheir overall task performance decreases and they are less able to compensate\nfor ball dynamics---frequency analysis of their motion indicates that abduction\nloading weakens their ability to generate movements at the resonant frequency\nof the dynamic task. This effect is likely due to an increased reliance on\nlower resolution indirect motor pathways in individuals post stroke. Given the\ninter-dependency of loading and dynamic task performance, we can create\ntargeted robot-aided training protocols focused on improving timing-sensitive\nmotor control, similar to existing progressive loading therapies, which have\nshown efficacy for expanding reachable workspace post stroke.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.01952,regular,pre_llm,2020,6,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Learning Active Task-Oriented Exploration Policies for Bridging the\n  Sim-to-Real Gap\n\n  Training robotic policies in simulation suffers from the sim-to-real gap, as\nsimulated dynamics can be different from real-world dynamics. Past works\ntackled this problem through domain randomization and online\nsystem-identification. The former is sensitive to the manually-specified\ntraining distribution of dynamics parameters and can result in behaviors that\nare overly conservative. The latter requires learning policies that\nconcurrently perform the task and generate useful trajectories for system\nidentification. In this work, we propose and analyze a framework for learning\nexploration policies that explicitly perform task-oriented exploration actions\nto identify task-relevant system parameters. These parameters are then used by\nmodel-based trajectory optimization algorithms to perform the task in the real\nworld. We instantiate the framework in simulation with the Linear Quadratic\nRegulator as well as in the real world with pouring and object dragging tasks.\nExperiments show that task-oriented exploration helps model-based policies\nadapt to systems with initially unknown parameters, and it leads to better task\nperformance than task-agnostic exploration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.16732,review,pre_llm,2020,6,"{'ai_likelihood': 8.377763960096572e-06, 'text': 'Formalizing and Guaranteeing* Human-Robot Interaction\n\n  Robot capabilities are maturing across domains, from self-driving cars, to\nbipeds and drones. As a result, robots will soon no longer be confined to\nsafety-controlled industrial settings; instead, they will directly interact\nwith the general public. The growing field of Human-Robot Interaction (HRI)\nstudies various aspects of this scenario - from social norms to joint action to\nhuman-robot teams and more. Researchers in HRI have made great strides in\ndeveloping models, methods, and algorithms for robots acting with and around\nhumans, but these ""computational HRI"" models and algorithms generally do not\ncome with formal guarantees and constraints on their operation. To enable\nhuman-interactive robots to move from the lab to real-world deployments, we\nmust address this gap.\n  This article provides an overview of verification, validation and synthesis\ntechniques used to create demonstrably trustworthy systems, describes several\nHRI domains that could benefit from such techniques, and provides a roadmap for\nthe challenges and the research needed to create formalized and guaranteed\nhuman-robot interaction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.06314,regular,pre_llm,2020,6,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Geometric and Stiffness Modeling and Design of Calibration Experiments\n  for the 7 dof Serial Manipulator KUKA iiwa 14 R820\n\n  The present project deals with the elastostatic modeling and calibration\nexperiment of spacial industrial manipulators using an optimal selection of\nmeasurements pose, for the calibration procedure, the optimal pose selection\naims to the efficiency improvement of identification procedure for serial\nmanipulators which reduces noise impact on the parameters identification\nprecision, it is usually used for planar manipulators, our work is mainly to\nextend the approach for a more complicated manipulator in 3D space using a wise\ndecomposition of the spacial manipulator into a set of serial sub-chains, the\noptimal pose configuration is then used in the calibration procedure using the\ncomplete and irreducible model for the 7 DOF serial manipulator. The\nmethodology is illustrated with the anthropomorphic industrial robot KUKA\niiwa14 R820 for which, we performed the calibration and constructed the\nstiffness modeling using two different approaches namely VJM (Virtual Joint\nModeling) and MSA (Matrix Structural Analysis).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.04962,regular,pre_llm,2020,6,"{'ai_likelihood': 3.8080745273166235e-06, 'text': 'A Novel Navigation System for an Autonomous Mobile Robot in an Uncertain\n  Environment\n\n  In this paper, we developed a new navigation system, which detects obstacles\nin a sliding window with an adaptive threshold clustering algorithm, classifies\nthe detected obstacles with a decision tree, heuristically predicts potential\ncollision and finds optimal path with a simplified Mophin algorithm. This\nsystem has the merits of optimal free-collision path, small memory size and\nless computing complexity, compared with the state of the arts in robot\nnavigation. The experiments on simulation and a robot for eight scenarios\ndemonstrate that the robot can effectively and efficiently avoid potential\ncollisions with any static or dynamic obstacles in its surrounding environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.01987,regular,pre_llm,2020,6,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""Impact-Aware Task-Space Quadratic-Programming Control\n\n  Robots usually establish contacts at rigid surfaces with near-zero relative\nvelocities. Otherwise, impact-induced energy propagates in the robot's linkage\nand may cause irreversible damage to the hardware. Moreover, abrupt changes in\ntask-space contact velocity and peak impact forces also result in abrupt\nchanges in robot joint velocities and torques; which can compromise\ncontrollers' stability, especially for those based on smooth models. In\nreality, several tasks would require establishing contact with moderately high\nvelocity. We propose to enhance task-space multi-objective controllers\nformulated as a quadratic program to be resilient to frictional impacts in\nthree dimensions. We devise new constraints and reformulate the usual ones to\nbe robust to the abrupt joint state changes mentioned earlier. The impact event\nbecomes a controlled process once the optimal control search space is aware of:\n(1) the hardware-affordable impact bounds and (2) analytically-computed\nfeasible set (polyhedra) that constrain post-impact critical states. Prior to\nand nearby the targeted contact spot, we assume, at each control cycle, that\nthe impact will occur at the next iteration. This somewhat one-step preview\nmakes our controller robust to impact time and location. To assess our\napproach, we experimented its resilience to moderate impacts with the Panda\nmanipulator and achieved swift grabbing tasks with the HRP-4 humanoid robot.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.03537,regular,pre_llm,2020,6,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'A Soft Humanoid Hand with In-Finger Visual Perception\n\n  We present a novel underactued humanoid five finger soft hand, the KIT\n\\softhand, which is equipped with cameras in the fingertips and integrates a\nhigh performance embedded system for visual processing and control. We describe\nthe actuation mechanism of the hand and the tendon-driven soft finger design\nwith internally routed high-bandwidth flat-flex cables. For efficient on-board\nparallel processing of visual data from the cameras in each fingertip, we\npresent a hybrid embedded architecture consisting of a field programmable logic\narray (FPGA) and a microcontroller that allows the realization of visual object\nsegmentation based on convolutional neural networks. We evaluate the hand\ndesign by conducting durability experiments with one finger and quantify the\ngrasp performance in terms of grasping force, speed and grasp success. The\nresults show that the hand exhibits a grasp force of 31.8 N and a mechanical\ndurability of the finger of more than 15.000 closing cycles. Finally, we\nevaluate the accuracy of visual object segmentation during the different phases\nof the grasping process using five different objects. Hereby, an accuracy above\n90 % can be achieved.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.05768,regular,pre_llm,2020,6,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Deep Drone Acrobatics\n\n  Performing acrobatic maneuvers with quadrotors is extremely challenging.\nAcrobatic flight requires high thrust and extreme angular accelerations that\npush the platform to its physical limits. Professional drone pilots often\nmeasure their level of mastery by flying such maneuvers in competitions. In\nthis paper, we propose to learn a sensorimotor policy that enables an\nautonomous quadrotor to fly extreme acrobatic maneuvers with only onboard\nsensing and computation. We train the policy entirely in simulation by\nleveraging demonstrations from an optimal controller that has access to\nprivileged information. We use appropriate abstractions of the visual input to\nenable transfer to a real quadrotor. We show that the resulting policy can be\ndirectly deployed in the physical world without any fine-tuning on real data.\nOur methodology has several favorable properties: it does not require a human\nexpert to provide demonstrations, it cannot harm the physical system during\ntraining, and it can be used to learn maneuvers that are challenging even for\nthe best human pilots. Our approach enables a physical quadrotor to fly\nmaneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during\nwhich it incurs accelerations of up to 3g.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15975,review,pre_llm,2020,6,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Robots and COVID-19: Challenges in integrating robots for collaborative\n  automation\n\n  Objective: The status of human-robot collaboration for assembly applications\nis reviewed and key current challenges for the research community and\npractitioners are presented. Background: As the pandemic of COVID-19 started to\nsurface the manufacturers went under pressure to address demand challenges.\nSocial distancing measures made fewer people available to work. In such\nsituations, robots were pointed at to support humans to address a shortage in\nsupply. An important activity where humans are needed in a manufacturing value\nchain is assembly. HRC assembly systems are supposed to safeguard coexisting\nhumans, perform a range of actions, and often need to be reconfigured to handle\nproduct variety. This requires them to be resilient and adaptable to various\nconfigurations during their operational life. Besides the potential advantages\nof using robots the challenges of using them in an industrial assembly are\nenormous. Methods: This mini-review summarizes the challenges of industrial\ndeployment of collaborative robots for assembly applications. Applications: The\ndocumented challenges highlight the future research directions in human-robot\ninteraction for industrial applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.02116,regular,pre_llm,2020,6,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Aerial Manipulation Using Hybrid Force and Position NMPC Applied to\n  Aerial Writing\n\n  Aerial manipulation aims at combining the manoeuvrability of aerial vehicles\nwith the manipulation capabilities of robotic arms. This, however, comes at the\ncost of the additional control complexity due to the coupling of the dynamics\nof the two systems. In this paper we present a NMPC specifically designed for\nMAVs equipped with a robotic arm. We formulate a hybrid control model for the\ncombined MAV-arm system which incorporates interaction forces acting on the end\neffector. We explain the practical implementation of our algorithm and show\nextensive experimental results of our custom built system performing multiple\naerial-writing tasks on a whiteboard, revealing accuracy in the order of\nmillimetres.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.06117,regular,pre_llm,2020,6,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Geometric Solutions for General Actuator Routing on Inflated-Beam Soft\n  Growing Robots\n\n  Continuum and soft robots can leverage complex actuator shapes to take on\nuseful shapes while actuating only a few of their many degrees of freedom.\nContinuum robots that also grow increase the range of potential shapes that can\nbe actuated and enable easier access to constrained environments. Existing\nmodels for describing the complex kinematics involved in general actuation of\ncontinuum robots rely on simulation or well-behaved stress-strain\nrelationships, but the non-linear behavior of the thin-walled inflated-beams\nused in growing robots makes these techniques difficult to apply. Here we\nderive kinematic models of single, generally routed tendon paths on a soft\npneumatic backbone of inextensible but flexible material from geometric\nrelationships alone. This allows for forward modeling of the resulting shapes\nwith only knowledge of the geometry of the system. We show that this model can\naccurately predict the shape of the whole robot body and how the model changes\nwith actuation type. We also demonstrate the use of this kinematic model for\ninverse design, where actuator designs are found based on desired final robot\nshapes. We deploy these designed actuators on soft pneumatic growing robots to\nshow the benefits of simultaneous growth and shape change.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.03512,regular,pre_llm,2020,6,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'MRFMap: Online Probabilistic 3D Mapping using Forward Ray Sensor Models\n\n  Traditional dense volumetric representations for robotic mapping make\nsimplifying assumptions about sensor noise characteristics due to computational\nconstraints. We present a framework that, unlike conventional occupancy grid\nmaps, explicitly models the sensor ray formation for a depth sensor via a\nMarkov Random Field and performs loopy belief propagation to infer the marginal\nprobability of occupancy at each voxel in a map. By explicitly reasoning about\nocclusions our approach models the correlations between adjacent voxels in the\nmap. Further, by incorporating learnt sensor noise characteristics we perform\naccurate inference even with noisy sensor data without ad-hoc definitions of\nsensor uncertainty. We propose a new metric for evaluating probabilistic\nvolumetric maps and demonstrate the higher fidelity of our approach on\nsimulated as well as real-world datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.16066,regular,pre_llm,2020,6,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Using an Automated Heterogeneous Robotic System for Radiation Surveys\n\n  During missions involving radiation exposure, unmanned robotic platforms may\nembody a valuable tool, especially thanks to their capability of replacing\nhuman operators in certain tasks to eliminate the health risks associated with\nsuch an environment. Moreover, rapid development of the technology allows us to\nincrease the automation rate, making the human operator generally less\nimportant within the entire process. This article presents a multi-robotic\nsystem designed for highly automated radiation mapping and source localization.\nOur approach includes a three-phase procedure comprising sequential deployment\nof two diverse platforms, namely, an unmanned aircraft system (UAS) and an\nunmanned ground vehicle (UGV), to perform aerial photogrammetry, aerial\nradiation mapping, and terrestrial radiation mapping. The central idea is to\nproduce a sparse dose rate map of the entire study site via the UAS and,\nsubsequently, to perform detailed UGV-based mapping in limited\nradiation-contaminated regions. To accomplish these tasks, we designed numerous\nmethods and data processing algorithms to facilitate, for example, digital\nelevation model (DEM)-based terrain following for the UAS, automatic selection\nof the regions of interest, obstacle map-based UGV trajectory planning, and\nsource localization. The overall usability of the multi-robotic system was\ndemonstrated by means of a one-day, authentic experiment, namely, a fictitious\ncar accident including the loss of several radiation sources. The ability of\nthe system to localize radiation hotspots and individual sources has been\nverified.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.00962,regular,pre_llm,2020,6,"{'ai_likelihood': 1.5232298109266494e-06, 'text': ""Off The Beaten Sidewalk: Pedestrian Prediction In Shared Spaces For\n  Autonomous Vehicles\n\n  Pedestrians and drivers interact closely in a wide range of environments.\nAutonomous vehicles (AVs) correspondingly face the need to predict pedestrians'\nfuture trajectories in these same environments. Traditional model-based\nprediction methods have been limited to making predictions in highly structured\nscenes with signalized intersections, marked crosswalks, or curbs. Deep\nlearning methods have instead leveraged datasets to learn predictive features\nthat generalize across scenes, at the cost of model interpretability. This\npaper aims to achieve both widely applicable and interpretable predictions by\nproposing a risk-based attention mechanism to learn when pedestrians yield, and\na model of vehicle influence to learn how yielding affects motion. A novel\nprobabilistic method, Off the Sidewalk Predictions (OSP), uses these to achieve\naccurate predictions in both shared spaces and traditional scenes. Experiments\non urban datasets demonstrate that the realtime method achieves\nstate-of-the-art performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2006.15466,regular,pre_llm,2020,6,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Trust Aware Emergency Response for A Resilient Human-Swarm Cooperative\n  System\n\n  A human-swarm cooperative system, which mixes multiple robots and a human\nsupervisor to form a heterogeneous team, is widely used for emergent scenarios\nsuch as criminal tracking in social security and victim assistance in a natural\ndisaster. These emergent scenarios require a cooperative team to quickly\nterminate the current task and transit the system to a new task, bringing\ndifficulty in motion planning. Moreover, due to the immediate task transitions,\nuncertainty from both physical systems and prior tasks is accumulated to\ndecrease swarm performance, causing robot failures and influencing the\ncooperation effectiveness between the human and the robot swarm. Therefore,\ngiven the quick-transition requirements and the introduced uncertainty, it is\nchallenging for a human-swarm system to respond to emergent tasks, compared\nwith executing normal tasks where a gradual transition between tasks is\nallowed. Human trust reveals the behavior expectations of others and is used to\nadjust unsatisfactory behaviors for better cooperation. Inspired by human\ntrust, in this paper, a trust-aware reflective control (Trust-R) is developed\nto dynamically calibrate human-swarm cooperation. Trust-R, based on a weighted\nmean subsequence reduced algorithm (WMSR) and human trust modeling, helps a\nswarm to self-reflect its performance from the perspective of human trust; then\nproactively correct its faulty behaviors in an early stage before a human\nintervenes. One typical task scenario {emergency response} was designed in the\nreal-gravity simulation environment, and a human user study with 145 volunteers\nwas conducted. Trust-R's effectiveness in correcting faulty behaviors in\nemergency response was validated by the improved swarm performance and\nincreased trust scores.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.01483,regular,pre_llm,2020,7,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'A decentralized framework for simultaneous calibration, localization and\n  mapping with multiple LiDARs\n\n  LiDAR is playing a more and more essential role in autonomous driving\nvehicles for objection detection, self localization and mapping. A single LiDAR\nfrequently suffers from hardware failure (e.g., temporary loss of connection)\ndue to the harsh vehicle environment (e.g., temperature, vibration, etc.), or\nperformance degradation due to the lack of sufficient geometry features,\nespecially for solid-state LiDARs with small field of view (FoV). To improve\nthe system robustness and performance in self-localization and mapping, we\ndevelop a decentralized framework for simultaneous calibration, localization\nand mapping with multiple LiDARs. Our proposed framework is based on an\nextended Kalman filter (EKF), but is specially formulated for decentralized\nimplementation. Such an implementation could potentially distribute the\nintensive computation among smaller computing devices or resources dedicated\nfor each LiDAR and remove the single point of failure problem. Then this\ndecentralized formulation is implemented on an unmanned ground vehicle (UGV)\ncarrying 5 low-cost LiDARs and moving at $1.3m/s$ in urban environments.\nExperiment results show that the proposed method can successfully and\nsimultaneously estimate the vehicle state (i.e., pose and velocity) and all\nLiDAR extrinsic parameters. The localization accuracy is up to 0.2% on the two\ndatasets we collected. To share our findings and to make contributions to the\ncommunity, meanwhile enable the readers to verify our work, we will release all\nour source codes and hardware design blueprint on our Github.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.13065,regular,pre_llm,2020,7,"{'ai_likelihood': 0.0, 'text': 'Multi-UAV Coverage Path Planning for the Inspection of Large and Complex\n  Structures\n\n  We present a multi-UAV Coverage Path Planning (CPP) framework for the\ninspection of large-scale, complex 3D structures. In the proposed\nsampling-based coverage path planning method, we formulate the multi-UAV\ninspection applications as a multi-agent coverage path planning problem. By\ncombining two NP-hard problems: Set Covering Problem (SCP) and Vehicle Routing\nProblem (VRP), a Set-Covering Vehicle Routing Problem (SC-VRP) is formulated\nand subsequently solved by a modified Biased Random Key Genetic Algorithm\n(BRKGA) with novel, efficient encoding strategies and local improvement\nheuristics. We test our proposed method for several complex 3D structures with\nthe 3D model extracted from OpenStreetMap. The proposed method outperforms\nprevious methods, by reducing the length of the planned inspection path by up\nto 48%\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.03271,regular,pre_llm,2020,7,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""CMPCC: Corridor-based Model Predictive Contouring Control for Aggressive\n  Drone Flight\n\n  In this paper, we propose an efficient, receding horizon, local adaptive\nlow-level planner as the middle layer between our original planner and\ncontroller. Our method is named as corridor-based model predictive contouring\ncontrol (CMPCC) since it builds upon on MPCC and utilizes the flight corridor\nas hard safety constraints. It optimizes the flight aggressiveness and tracking\naccuracy simultaneously, thus improving our system's robustness by overcoming\nunmeasured disturbances. Our method features its online flight speed\noptimization, strict safety and feasibility, and real-time performance, and\nwill be released as a low-level plugin for a large variety of quadrotor\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.10577,regular,pre_llm,2020,7,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Distributed Motion Control for Multiple Connected Surface Vessels\n\n  We propose a scalable cooperative control approach which coordinates a group\nof rigidly connected autonomous surface vessels to track desired trajectories\nin a planar water environment as a single floating modular structure. Our\napproach leverages the implicit information of the structure's motion for force\nand torque allocation without explicit communication among the robots. In our\nsystem, a leader robot steers the entire group by adjusting its force and\ntorque according to the structure's deviation from the desired trajectory,\nwhile follower robots run distributed consensus-based controllers to match\ntheir inputs to amplify the leader's intent using only onboard sensors as\nfeedback. To cope with the complex and highly coupled system dynamics in the\nwater, the leader robot employs a nonlinear model predictive controller (NMPC),\nwhere we experimentally estimated the dynamics model of the floating modular\nstructure in order to achieve superior performance for leader-following\ncontrol. Our method has a wide range of potential applications in transporting\nhumans and goods in many of today's existing waterways. We conducted trajectory\nand orientation tracking experiments in hardware with three custom-built\nautonomous modular robotic boats, called Roboat, which are capable of holonomic\nmotions and onboard state estimation. Simulation results with up to 65 robots\nalso prove the scalability of our proposed approach.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.09243,regular,pre_llm,2020,7,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Multi-robot Cooperative Object Transportation using Decentralized Deep\n  Reinforcement Learning\n\n  Object transportation could be a challenging problem for a single robot due\nto the oversize and/or overweight issues. A multi-robot system can take the\nadvantage of increased driving power and more flexible configuration to solve\nsuch a problem. However, increased number of individuals also changed the\ndynamics of the system which makes control of a multi-robot system more\ncomplicated. Even worse, if the whole system is sitting on a centralized\ndecision making unit, the data flow could be easily overloaded due to the\nupscaling of the system. In this research, we propose a decentralized control\nscheme on a multi-robot system with each individual equipped with a deep\nQ-network (DQN) controller to perform an oversized object transportation task.\nDQN is a deep reinforcement learning algorithm thus does not require the\nknowledge of system dynamics, instead, it enables the robots to learn\nappropriate control strategies through trial-and-error style interactions\nwithin the task environment. Since analogous controllers are distributed on the\nindividuals, the computational bottleneck is avoided systematically. We\ndemonstrate such a system in a scenario of carrying an oversized rod through a\ndoorway by a two-robot team. The presented multi-robot system learns abstract\nfeatures of the task and cooperative behaviors are observed. The decentralized\nDQN-style controller is showing strong robustness against uncertainties. In\naddition, We propose a universal metric to assess the cooperation\nquantitatively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.11534,regular,pre_llm,2020,7,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Dynamic Task Allocation for Robotic Network Cloud Systems\n\n  Every robotic network cloud system can be seen as a graph with nodes as\nhardware with independent computational processing powers and edges as data\ntransmissions between nodes. When assigning a task to a node we may change\nseveral values corresponding to the node such as distance to other nodes, the\ntime to complete all of its tasks, the energy level of the node, energy\nconsumed while performing all of its tasks, geometrical position, communication\nwith other nodes, and so on. These values can be seen as fingerprints for the\ncurrent state of the node which can be evaluated as a subspace of a hyperspace.\nWe proposed a theoretical model describing how assigning tasks to a node will\nchange the subspace of the hyperspace, and from that, we show how to obtain the\noptimal task allocation. We described the communication instability between\nnodes and the capability of nodes as subspaces of a hyperspace. We translate\ntask scheduling to nodes as finding the maximum volume of the hyperspace.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.00872,regular,pre_llm,2020,7,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Design of Extra Robotic Legs for Augmenting Human Payload Capabilities\n  by Exploiting Singularity and Torque Redistribution\n\n  We present the design of a new robotic human augmentation system that will\nassist the operator in carrying a heavy payload, reaching and maintaining\ndifficult postures, and ultimately better performing their job. The Extra\nRobotic Legs (XRL) system is worn by the operator and consists of two\narticulated robotic legs that move with the operator to bear a heavy payload.\nThe design was driven by a need to increase the effectiveness of hazardous\nmaterial emergency response personnel who are encumbered by their personal\nprotective equipment (PPE). The legs will ultimately walk, climb stairs, crouch\ndown, and crawl with the operator while eliminating all external PPE loads on\nthe operator. The forces involved in the most extreme loading cases were\nanalyzed to find an effective strategy for reducing actuator loads. The\nanalysis reveals that the maximum torque is exerted during the transition from\nthe crawling to standing mode of motion. Peak torques are significantly reduced\nby leveraging redundancy in force application resulting from a closed-loop\nkinematic chain formed by a particular posture of the XRL. The actuators, power\nsystems, and transmission elements were designed from the results of these\nanalyses. Using differential mechanisms to combine the inputs of multiple\nactuators into a single degree of freedom, the gear reductions needed to bear\nthe heavy loads could be kept at a minimum, enabling high bandwidth force\ncontrol due to the near-direct-drive transmission. A prototype was fabricated\nutilizing the insights gained from these analyses and initial tests indicate\nthe feasibility of the XRL system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.04047,regular,pre_llm,2020,7,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Design, Control, and Applications of a Soft Robotic Arm\n\n  This paper presents the design, control, and applications of a multi-segment\nsoft robotic arm. In order to design a soft arm with large load capacity,\nseveral design principles are proposed by analyzing two kinds of buckling\nissues, under which we present a novel structure named Honeycomb Pneumatic\nNetworks (HPN). Parameter optimization method, based on finite element method\n(FEM), is proposed to optimize HPN Arm design parameters. Through a quick\nfabrication process, several prototypes with different performance are made,\none of which can achieve the transverse load capacity of 3 kg under 3 bar\npressure. Next, considering different internal and external conditions, we\ndevelop three controllers according to different model precision. Specifically,\nbased on accurate model, an open-loop controller is realized by combining\npiece-wise constant curvature (PCC) modeling method and machine learning\nmethod. Based on inaccurate model, a feedback controller, using estimated\nJacobian, is realized in 3D space. A model-free controller, using reinforcement\nlearning to learn a control policy rather than a model, is realized in 2D\nplane, with minimal training data. Then, these three control methods are\ncompared on a same experiment platform to explore the applicability of\ndifferent methods under different conditions. Lastly, we figure out that soft\narm can greatly simplify the perception, planning, and control of interaction\ntasks through its compliance, which is its main advantage over the rigid arm.\nThrough plentiful experiments in three interaction application scenarios,\nhuman-robot interaction, free space interaction task, and confined space\ninteraction task, we demonstrate the potential application prospect of the soft\narm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.00109,regular,pre_llm,2020,7,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""Characterization of Assistive Robot Arm Teleoperation: A Preliminary\n  Study to Inform Shared Control\n\n  Assistive robotic devices can increase the independence of individuals with\nmotor impairments. However, each person is unique in their level of injury,\npreferences, and skills, which moreover can change over time. Further, the\namount of assistance required can vary throughout the day due to pain or\nfatigue, or over longer periods due to rehabilitation, debilitating conditions,\nor aging. Therefore, in order to become an effective team member, the assistive\nmachine should be able to learn from and adapt to the human user. To do so, we\nneed to be able to characterize the user's control commands to determine when\nand how autonomy should change to best assist the user. We perform a 20 person\npilot study in order to establish a set of meaningful performance measures\nwhich can be used to characterize the user's control signals and as cues for\nthe autonomy to modify the level and amount of assistance. Our study includes 8\nspinal cord injured and 12 uninjured individuals. The results unveil a set of\nobjective, runtime-computable metrics that are correlated with user-perceived\ntask difficulty, and thus could be used by an autonomy system when deciding\nwhether assistance is required. The results further show that metrics which\nevaluate the user interaction with the robotic device, robot execution, and the\nperceived task difficulty show differences among spinal cord injured and\nuninjured groups, and are affected by the type of control interface used. The\nresults will be used to develop an adaptable, user-centered, and individually\ncustomized shared-control algorithms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.00798,regular,pre_llm,2020,7,"{'ai_likelihood': 1.0563267601860894e-05, 'text': 'Deliberate Exploration Supports Navigation in Unfamiliar Worlds\n\n  To perform tasks well in a new domain, one must first know something about\nit. This paper reports on a robot controller for navigation through unfamiliar\nindoor worlds. Based on spatial affordances, it integrates planning with\nreactive heuristics. Before it addresses specific targets, however, the system\ndeliberately explores for high-level connectivity and captures that data in a\ncognitive spatial model. Despite limited exploration time, planning in the\nresultant model is faster and better supports successful travel in a\nchallenging, realistic space.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.12497,regular,pre_llm,2020,7,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Advanced Mapping Robot and High-Resolution Dataset\n\n  This paper presents a fully hardware synchronized mapping robot with support\nfor a hardware synchronized external tracking system, for super-precise timing\nand localization. Nine high-resolution cameras and two 32-beam 3D Lidars were\nused along with a professional, static 3D scanner for ground truth map\ncollection. With all the sensors calibrated on the mapping robot, three\ndatasets are collected to evaluate the performance of mapping algorithms within\na room and between rooms. Based on these datasets we generate maps and\ntrajectory data, which is then fed into evaluation algorithms. We provide the\ndatasets for download and the mapping and evaluation procedures are made in a\nvery easily reproducible manner for maximum comparability. We have also\nconducted a survey on available robotics-related datasets and compiled a big\ntable with those datasets and a number of properties of them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.1002,review,pre_llm,2020,7,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'On Randomized Searching for Multi-robot Coordination\n\n  In this chapter, we propose a novel approach for solving the coordination of\na fleet of mobile robots, which consists of finding a set of collision-free\ntrajectories for individual robots in the fleet. This problem is studied for\nseveral decades, and many approaches have been introduced. However, only a\nsmall minority is applicable in practice because of their properties - small\ncomputational requirement, producing solutions near-optimum, and completeness.\nThe approach we present is based on a multi-robot variant of Rapidly Exploring\nRandom Tree algorithm (RRT) for discrete environments and significantly\nimproves its performance. Although the solutions generated by the approach are\nslightly worse than one of the best state-of-the-art algorithms presented in\n[23], it solves problems where ter Morses algorithm fails.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.04499,regular,pre_llm,2020,7,"{'ai_likelihood': 3.7749608357747397e-06, 'text': 'Robotic Grasping using Deep Reinforcement Learning\n\n  In this work, we present a deep reinforcement learning based method to solve\nthe problem of robotic grasping using visio-motor feedback. The use of a deep\nlearning based approach reduces the complexity caused by the use of\nhand-designed features. Our method uses an off-policy reinforcement learning\nframework to learn the grasping policy. We use the double deep Q-learning\nframework along with a novel Grasp-Q-Network to output grasp probabilities used\nto learn grasps that maximize the pick success. We propose a visual servoing\nmechanism that uses a multi-view camera setup that observes the scene which\ncontains the objects of interest. We performed experiments using a Baxter\nGazebo simulated environment as well as on the actual robot. The results show\nthat our proposed method outperforms the baseline Q-learning framework and\nincreases grasping accuracy by adapting a multi-view model in comparison to a\nsingle-view model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.02907,regular,pre_llm,2020,7,"{'ai_likelihood': 1.1556678348117404e-05, 'text': 'Including Image-based Perception in Disturbance Observer for Warehouse\n  Drones\n\n  Grasping and releasing objects would cause oscillations to delivery drones in\nthe warehouse. To reduce such undesired oscillations, this paper treats the\nto-be-delivered object as an unknown external disturbance and presents an\nimage-based disturbance observer (DOB) to estimate and reject such disturbance.\nDifferent from the existing DOB technique that can only compensate for the\ndisturbance after the oscillations happen, the proposed image-based one\nincorporates image-based disturbance prediction into the control loop to\nfurther improve the performance of the DOB. The proposed image-based DOB\nconsists of two parts. The first one is deep-learning-based disturbance\nprediction. By taking an image of the to-be-delivered object, a sequential\ndisturbance signal is predicted in advance using a connected pre-trained\nconvolutional neural network (CNN) and a long short-term memory (LSTM) network.\nThe second part is a conventional DOB in the feedback loop with a feedforward\ncorrection, which utilizes the deep learning prediction to generate a learning\nsignal. Numerical studies are performed to validate the proposed image-based\nDOB regarding oscillation reduction for delivery drones during the grasping and\nreleasing periods of the objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.09633,regular,pre_llm,2020,7,"{'ai_likelihood': 1.304679446750217e-05, 'text': ""Motion Planning for Heterogeneous Unmanned Systems under Partial\n  Observation from UAV\n\n  For heterogeneous unmanned systems composed of unmanned aerial vehicles\n(UAVs) and unmanned ground vehicles (UGVs), using UAVs serve as eyes to assist\nUGVs in motion planning is a promising research direction due to the UAVs' vast\nview scope. However, due to UAVs flight altitude limitations, it may be\nimpossible to observe the global map, and motion planning in the local map is a\nPOMDP (Partially Observable Markov Decision Process) problem. This paper\nproposes a motion planning algorithm for heterogeneous unmanned system under\npartial observation from UAV without reconstruction of global maps, which\nconsists of two parts designed for perception and decision-making,\nrespectively. For the perception part, we propose the Grid Map Generation\nNetwork (GMGN), which is used to perceive scenes from UAV's perspective and\nclassify the pathways and obstacles. For the decision-making part, we propose\nthe Motion Command Generation Network (MCGN). Due to the addition of memory\nmechanism, MCGN has planning and reasoning abilities under partial observation\nfrom UAVs. We evaluate our proposed algorithm by comparing with baseline\nalgorithms. The results show that our method effectively plans the motion of\nheterogeneous unmanned systems and achieves a relatively high success rate.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.10743,regular,pre_llm,2020,7,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection\n  and Tracking\n\n  Dynamic obstacle avoidance is one crucial component for compliant navigation\nin crowded environments. In this paper we present a system for accurate and\nreliable detection and tracking of dynamic objects using noisy point cloud data\ngenerated by stereo cameras. Our solution is real-time capable and specifically\ndesigned for the deployment on computationally-constrained unmanned ground\nvehicles. The proposed approach identifies individual objects in the robot's\nsurroundings and classifies them as either static or dynamic. The dynamic\nobjects are labeled as either a person or a generic dynamic object. We then\nestimate their velocities to generate a 2D occupancy grid that is suitable for\nperforming obstacle avoidance. We evaluate the system in indoor and outdoor\nscenarios and achieve real-time performance on a consumer-grade computer. On\nour test-dataset, we reach a MOTP of $0.07 \\pm 0.07m$, and a MOTA of $85.3\\%$\nfor the detection and tracking of dynamic objects. We reach a precision of\n$96.9\\%$ for the detection of static objects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.09581,regular,pre_llm,2020,7,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Fast Adaptable Mobile Robot Navigation in Dynamic Environment\n\n  Autonomous navigation in dynamic environment heavily depends on the\nenvironment and its topology. Prior knowledge of the environment is not usually\naccurate as the environment keeps evolving in time. Since robot is continuously\nevaluating the environment as it proceeds, deciding the optimal way to traverse\nthe environment to get to the goal, computationally efficient yet\nmathematically adaptive navigation algorithms are needed. In this paper, a\nnavigation scheme for mobile robot, capable of dealing with time variant\nenvironment is proposed. This approach consists of a global planner (A*) and\nlocal planner (VFH) to assure an optimal and collision-free robot motion. The\nalgorithm is tested both in simulation and experimentation in different\nenvironments that are known to result in failures in VFH and ROS navigation\nstack, for comparison purposes. Overall, the algorithm enables the robot to get\nto the goal faster and also produces a smoother path while doing so.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.10396,regular,pre_llm,2020,7,"{'ai_likelihood': 5.033281114366319e-06, 'text': ""Design and Implementation of a Maxi-Sized Mobile Robot (Karo) for Rescue\n  Missions\n\n  Rescue robots are expected to carry out reconnaissance and dexterity\noperations in unknown environments comprising unstructured obstacles. Although\na wide variety of designs and implementations have been presented within the\nfield of rescue robotics, embedding all mobility, dexterity, and reconnaissance\ncapabilities in a single robot remains a challenging problem. This paper\nexplains the design and implementation of Karo, a mobile robot that exhibits a\nhigh degree of mobility at the side of maintaining required dexterity and\nexploration capabilities for urban search and rescue (USAR) missions. We first\nelicit the system requirements of a standard rescue robot from the frameworks\nof Rescue Robot League (RRL) of RoboCup and then, propose the conceptual design\nof Karo by drafting a locomotion and manipulation system. Considering that,\nthis work presents comprehensive design processes along with detail mechanical\ndesign of the robot's platform and its 7-DOF manipulator. Further, we present\nthe design and implementation of the command and control system by discussing\nthe robot's power system, sensors, and hardware systems. In conjunction with\nthis, we elucidate the way that Karo's software system and human-robot\ninterface are implemented and employed. Furthermore, we undertake extensive\nevaluations of Karo's field performance to investigate whether the principal\nobjective of this work has been satisfied. We demonstrate that Karo has\neffectively accomplished assigned standardized rescue operations by evaluating\nall aspects of its capabilities in both RRL's test suites and training suites\nof a fire department. Finally, the comprehensiveness of Karo's capabilities has\nbeen verified by drawing quantitative comparisons between Karo's performance\nand other leading robots participating in RRL.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.11898,regular,pre_llm,2020,7,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial\n  and Multi-Map SLAM\n\n  This paper presents ORB-SLAM3, the first system able to perform visual,\nvisual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras,\nusing pin-hole and fisheye lens models. The first main novelty is a\nfeature-based tightly-integrated visual-inertial SLAM system that fully relies\non Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization\nphase. The result is a system that operates robustly in real-time, in small and\nlarge, indoor and outdoor environments, and is 2 to 5 times more accurate than\nprevious approaches. The second main novelty is a multiple map system that\nrelies on a new place recognition method with improved recall. Thanks to it,\nORB-SLAM3 is able to survive to long periods of poor visual information: when\nit gets lost, it starts a new map that will be seamlessly merged with previous\nmaps when revisiting mapped areas. Compared with visual odometry systems that\nonly use information from the last few seconds, ORB-SLAM3 is the first system\nable to reuse in all the algorithm stages all previous information. This allows\nto include in bundle adjustment co-visible keyframes, that provide high\nparallax observations boosting accuracy, even if they are widely separated in\ntime or if they come from a previous mapping session. Our experiments show\nthat, in all sensor configurations, ORB-SLAM3 is as robust as the best systems\navailable in the literature, and significantly more accurate. Notably, our\nstereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone\nand 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting\nrepresentative of AR/VR scenarios. For the benefit of the community we make\npublic the source code.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2007.05616,regular,pre_llm,2020,7,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'NaviGAN: A Generative Approach for Socially Compliant Navigation\n\n  Robots navigating in human crowds need to optimize their paths not only for\ntheir task performance but also for their compliance to social norms. One of\nthe key challenges in this context is the lack of standard metrics for\nevaluating and optimizing a socially compliant behavior. Existing works in\nsocial navigation can be grouped according to the differences in their\noptimization objectives. For instance, the reinforcement learning approaches\ntend to optimize on the \\textit{comfort} aspect of the socially compliant\nnavigation, whereas the inverse reinforcement learning approaches are designed\nto achieve \\textit{natural} behavior. In this paper, we propose NaviGAN, a\ngenerative navigation algorithm that jointly optimizes both of the\n\\textit{comfort} and \\textit{naturalness} aspects. Our approach is designed as\nan adversarial training framework that can learn to generate a navigation path\nthat is both optimized for achieving a goal and for complying with latent\nsocial rules. A set of experiments has been carried out on multiple datasets to\ndemonstrate the strengths of the proposed approach quantitatively. We also\nperform extensive experiments using a physical robot in a real-world\nenvironment to qualitatively evaluate the trained social navigation behavior.\nThe video recordings of the robot experiments can be found in the link:\nhttps://youtu.be/61blDymjCpw.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.00715,regular,pre_llm,2020,8,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World\n  Reinforcement Learning\n\n  We present Learning to Drive (L2D), a low-cost benchmark for real-world\nreinforcement learning (RL). L2D involves a simple and reproducible\nexperimental setup where an RL agent has to learn to drive a Donkey car around\nthree miniature tracks, given only monocular image observations and speed of\nthe car. The agent has to learn to drive from disengagements, which occurs when\nit drives off the track. We present and open-source our training pipeline,\nwhich makes it straightforward to apply any existing RL algorithm to the task\nof autonomous driving with a Donkey car. We test imitation learning,\nstate-of-the-art model-free, and model-based algorithms on the proposed L2D\nbenchmark. Our results show that existing RL algorithms can learn to drive the\ncar from scratch in less than five minutes of interaction. We demonstrate that\nRL algorithms can learn from sparse and noisy disengagement to drive even\nfaster than imitation learning and a human operator.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.11466,regular,pre_llm,2020,8,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Self-Supervised Goal-Conditioned Pick and Place\n\n  Robots have the capability to collect large amounts of data autonomously by\ninteracting with objects in the world. However, it is often not obvious\n\\emph{how} to learning from autonomously collected data without human-labeled\nsupervision. In this work we learn pixel-wise object representations from\nunsupervised pick and place data that generalize to new objects. We introduce a\nnovel framework for using these representations in order to predict where to\npick and where to place in order to match a goal image. Finally, we demonstrate\nthe utility of our approach in a simulated grasping environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.01655,regular,pre_llm,2020,8,"{'ai_likelihood': 5.033281114366319e-06, 'text': 'Deep Visual Odometry with Adaptive Memory\n\n  We propose a novel deep visual odometry (VO) method that considers global\ninformation by selecting memory and refining poses. Existing learning-based\nmethods take the VO task as a pure tracking problem via recovering camera poses\nfrom image snippets, leading to severe error accumulation. Global information\nis crucial for alleviating accumulated errors. However, it is challenging to\neffectively preserve such information for end-to-end systems. To deal with this\nchallenge, we design an adaptive memory module, which progressively and\nadaptively saves the information from local to global in a neural analogue of\nmemory, enabling our system to process long-term dependency. Benefiting from\nglobal information in the memory, previous results are further refined by an\nadditional refining module. With the guidance of previous outputs, we adopt a\nspatial-temporal attention to select features for each view based on the\nco-visibility in feature domain. Specifically, our architecture consisting of\nTracking, Remembering and Refining modules works beyond tracking. Experiments\non the KITTI and TUM-RGBD datasets demonstrate that our approach outperforms\nstate-of-the-art methods by large margins and produces competitive results\nagainst classic approaches in regular scenes. Moreover, our model achieves\noutstanding performance in challenging scenarios such as texture-less regions\nand abrupt motions, where classic algorithms tend to fail.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.01281,regular,pre_llm,2020,8,"{'ai_likelihood': 0.0, 'text': 'Stochastic Grounded Action Transformation for Robot Learning in\n  Simulation\n\n  Robot control policies learned in simulation do not often transfer well to\nthe real world. Many existing solutions to this sim-to-real problem, such as\nthe Grounded Action Transformation (GAT) algorithm, seek to correct for or\nground these differences by matching the simulator to the real world. However,\nthe efficacy of these approaches is limited if they do not explicitly account\nfor stochasticity in the target environment. In this work, we analyze the\nproblems associated with grounding a deterministic simulator in a stochastic\nreal world environment, and we present examples where GAT fails to transfer a\ngood policy due to stochastic transitions in the target domain. In response, we\nintroduce the Stochastic Grounded Action Transformation(SGAT) algorithm,which\nmodels this stochasticity when grounding the simulator. We find experimentally\nfor both simulated and physical target domains that SGAT can find policies that\nare robust to stochasticity in the target domain\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.13661,regular,pre_llm,2020,8,"{'ai_likelihood': 6.622738308376736e-06, 'text': 'Footstep Planning with Encoded Linear Temporal Logic Specifications\n\n  This article presents an approach to encode Linear Temporal Logic (LTL)\nSpecifications into a Mixed Integer Quadratically Constrained Quadratic Program\n(MIQCQP) footstep planner. We propose that the integration of LTL\nspecifications into the planner not only facilitates safe and desirable\nlocomotion between obstacle-free regions, but also provides a rich language for\nhigh-level reasoning in contact planning. Simulations of the footstep planner\nin a 2D environment satisfying encoded LTL specifications demonstrate the\nresults of this research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.00639,regular,pre_llm,2020,8,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'An Electrocommunication System Using FSK Modulation and Deep Learning\n  Based Demodulation for Underwater Robots\n\n  Underwater communication is extremely challenging for small underwater robots\nwhich typically have stringent power and size constraints. In our previous\nwork, we developed an artificial electrocommunication system which could be an\nalternative for the communication of small underwater robots. This paper\nfurther presents a new electrocommunication system that utilizes Binary\nFrequency Shift Keying (2FSK) modulation and deep-learning-based demodulation\nfor underwater robots. We first derive an underwater electrocommunication model\nthat covers both the near-field area and a large transition area outside of the\nnear-field area. 2FSK modulation is adopted to improve the anti-interference\nability of the electric signal. A deep learning algorithm is used to demodulate\nthe electric signal by the receiver. Simulations and experiments show that with\nthe same testing condition, the new communication system outperforms the\nprevious system in both the communication distance and the data transmitting\nrate. In specific, the newly developed communication system achieves stable\ncommunication within the distance of 10 m at a data transfer rate of 5 Kbps\nwith a power consumption of less than 0.1 W. The substantial increase in\ncommunication distance further improves the possibility of electrocommunication\nin underwater robotics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.09679,regular,pre_llm,2020,8,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Towards Resilient Autonomous Navigation of Drones\n\n  Robots and particularly drones are especially useful in exploring extreme\nenvironments that pose hazards to humans. To ensure safe operations in these\nsituations, usually perceptually degraded and without good GNSS, it is critical\nto have a reliable and robust state estimation solution. The main body of\nliterature in robot state estimation focuses on developing complex algorithms\nfavoring accuracy. Typically, these approaches rely on a strong underlying\nassumption: the main estimation engine will not fail during operation. In\ncontrast, we propose an architecture that pursues robustness in state\nestimation by considering redundancy and heterogeneity in both sensing and\nestimation algorithms. The architecture is designed to expect and detect\nfailures and adapt the behavior of the system to ensure safety. To this end, we\npresent HeRO (Heterogeneous Redundant Odometry): a stack of estimation\nalgorithms running in parallel supervised by a resiliency logic. This logic\ncarries out three main functions: a) perform confidence tests both in data\nquality and algorithm health; b) re-initialize those algorithms that might be\nmalfunctioning; c) generate a smooth state estimate by multiplexing the inputs\nbased on their quality. The state and quality estimates are used by the\nguidance and control modules to adapt the mobility behaviors of the system. The\nvalidation and utility of the approach are shown with real experiments on a\nflying robot for the use case of autonomous exploration of subterranean\nenvironments, with particular results from the STIX event of the DARPA\nSubterranean Challenge.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.06703,review,pre_llm,2020,8,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Description and Technical specification of Cybernetic Transportation\n  Systems: an urban transportation concept\n\n  The Cybernetic Transportation Systems (CTS) is an urban mobility concept\nbased on two ideas: the car sharing and the automation of dedicated systems\nwith door-to-door capabilities. In the last decade, many European projects have\nbeen developed in this context, where some of the most important are:\nCybercars, Cybercars2, CyberMove, CyberC3 and CityMobil. Different companies\nhave developed a first fleet of CTSs in collaboration with research centers\naround Europe, Asia and America. Considering these previous works, the FP7\nproject CityMobil2 is on progress since 2012. Its goal is to solve some of the\nlimitations found so far, including the definition of the legal framework for\nautonomous vehicles on urban environment. This work describes the different\nimprovements, adaptation and instrumentation of the CTS prototypes involved in\nEuropean cities. Results show tests in our facilities at INRIA-Rocquencourt\n(France) and the first showcase at Le\\'on (Spain)\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.08893,regular,pre_llm,2020,8,"{'ai_likelihood': 7.119443681504991e-06, 'text': 'Switching Model Predictive Control for Online Structural Reformations of\n  a Foldable Quadrotor\n\n  The aim of this article is the formulation of a switching model predictive\ncontrol framework for the case of a foldable quadrotor with the ability to\nretain the overall control quality during online structural reformations. The\nmajority of the related scientific publications consider fixed morphology of\nthe aerial vehicles. Recent advances in mechatronics have brought novel\nconsiderations for generalized aerial robotic designs with the ability to alter\ntheir morphology in order to adapt to their environment, thus enhancing their\ncapabilities. Simulation results are provided to prove the efficacy of the\nselected control scheme.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.03826,regular,pre_llm,2020,8,"{'ai_likelihood': 8.17908181084527e-06, 'text': 'Contact-Rich Trajectory Generation in Confined Environments Using\n  Iterative Convex Optimization\n\n  Applying intelligent robot arms in dynamic uncertain environments (i.e.,\nflexible production lines) remains challenging, which requires efficient\nalgorithms for real time trajectory generation. The motion planning problem for\nrobot trajectory generation is highly nonlinear and nonconvex, which usually\ncomes with collision avoidance constraints, robot kinematics and dynamics\nconstraints, and task constraints (e.g., following a Cartesian trajectory\ndefined on a surface and maintain the contact). The nonlinear and nonconvex\nplanning problem is computationally expensive to solve, which limits the\napplication of robot arms in the real world. In this paper, for redundant robot\narm planning problems with complex constraints, we present a motion planning\nmethod using iterative convex optimization that can efficiently handle the\nconstraints and generate optimal trajectories in real time. The proposed\nplanner guarantees the satisfaction of the contact-rich task constraints and\navoids collision in confined environments. Extensive experiments on trajectory\ngeneration for weld grinding are performed to demonstrate the effectiveness of\nthe proposed method and its applicability in advanced robotic manufacturing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.12864,regular,pre_llm,2020,8,"{'ai_likelihood': 3.2782554626464844e-06, 'text': 'Vacuum Driven Auxetic Switching Structure and Its Application on a\n  Gripper and Quadruped\n\n  The properties and applications of auxetics have been widely explored in the\npast years. Through proper utilization of auxetic structures, designs with\nunprecedented mechanical and structural behaviors can be produced. Taking\nadvantage of this, we present the development of novel and lowcost 3D\nstructures inspired by a simple auxetic unit. The core part, which we call the\nbody in this paper, is a 3D realization of 2D rotating squares. This body\nstructure was formed by joining four similar structures through softer material\nat the vertices. A monolithic structure of this kind is accomplished through a\ncustom-built multi-material 3D printer. The model works in a way that, when\ntorque is applied along the face of the rotational squares, they tend to bend\nat the vertex of the softer material, and due to the connected-ness of the\ndesign, a proper opening and closing motion is achieved. To demonstrate the\npotential of this part as an important component for robots, two applications\nare presented: a soft gripper and a crawling robot. Vacuum-driven actuators\nmove both the applications. The proposed gripper combines the benefits of two\ntypes of grippers whose fingers are placed parallel and equally spaced to each\nother, in a single design. This gripper is adaptable to the size of the object\nand can grasp objects with large and small cross-sections alike. A novel\nbending actuator, which is made of soft material and bends in curvature when\nvacuumed, provides the grasping nature of the gripper. Crawling robots, in\naddition to their versatile nature, provide a better interaction with humans.\nThe designed crawling robot employs negative pressure-driven actuators to\nhighlight linear and turning locomotion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.05777,regular,pre_llm,2020,8,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'A Tendon-driven Robot Gripper with Passively Switchable Underactuated\n  Surface and its Physics Simulation Based Parameter Optimization\n\n  In this paper, we propose a single-actuator gripper that can lift thin\nobjects lying on a flat surface, in addition to the ability as a standard\nparallel gripper. The key is a crawler on the fingertip, which is underactuated\ntogether with other finger joints and switched with a passive and spring-loaded\nmechanism. While the idea of crawling finger is not a new one, this paper\ncontributes to realize the crawling without additional motor. The gripper can\npassively change the mode from the parallel approach mode to the pull-in mode,\nthen finally to the power grasp mode, according to the grasping state. To\noptimize the highly underactuated system, we take a combination of black-box\noptimization and physics simulation of the whole grasp process. We show that\nthis simulation-based approach can effectively consider the precontact motion,\nin-hand manipulation, power grasp stability, and even failure mode, which is\ndifficult for the static-equilibrium-analysis-based approaches. In the last\npart of the paper, we demonstrate that a prototype gripper with the proposed\nstructure and design parameters optimized under the proposed process\nsuccessfully power-grasped a thin sheet, a softcover book, and a cylinder lying\non a flat surface.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.00644,regular,pre_llm,2020,8,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized\n  Gaussian process map reconstruction\n\n  This paper presents a 3D lidar SLAM system based on improved regionalized\nGaussian process (GP) map reconstruction to provide both low-drift state\nestimation and mapping in real-time for robotics applications. We utilize\nspatial GP regression to model the environment. This tool enables us to recover\nsurfaces including those in sparsely scanned areas and obtain uniform samples\nwith uncertainty. Those properties facilitate robust data association and map\nupdating in our scan-to-map registration scheme, especially when working with\nsparse range data. Compared with previous GP-SLAM, this work overcomes the\nprohibitive computational complexity of GP and redesigns the registration\nstrategy to meet the accuracy requirements in 3D scenarios. For large-scale\ntasks, a two-thread framework is employed to suppress the drift further. Aerial\nand ground-based experiments demonstrate that our method allows robust odometry\nand precise mapping in real-time. It also outperforms the state-of-the-art\nlidar SLAM systems in our tests with light-weight sensors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.13474,regular,pre_llm,2020,8,"{'ai_likelihood': 3.2120280795627172e-06, 'text': 'A Cost-Effective Person-Following System for Assistive Unmanned Vehicles\n  with Deep Learning at the Edge\n\n  The vital statistics of the last century highlight a sharp increment of the\naverage age of the world population with a consequent growth of the number of\nolder people. Service robotics applications have the potentiality to provide\nsystems and tools to support the autonomous and self-sufficient older adults in\ntheir houses in everyday life, thereby avoiding the task of monitoring them\nwith third parties. In this context, we propose a cost-effective modular\nsolution to detect and follow a person in an indoor, domestic environment. We\nexploited the latest advancements in deep learning optimization techniques, and\nwe compared different neural network accelerators to provide a robust and\nflexible person-following system at the edge. Our proposed cost-effective and\npower-efficient solution is fully-integrable with pre-existing navigation\nstacks and creates the foundations for the development of fully-autonomous and\nself-contained service robotics applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.08265,regular,pre_llm,2020,8,"{'ai_likelihood': 6.9207615322536894e-06, 'text': 'Problem of robotic precision cutting of the geometrically complex shape\n  from an irregular honeycomb grid\n\n  The article considers solving the problem of precision cutting of honeycomb\nblocks. The urgency of using arbitrary shapes application cutting from\nhoney-comb blocks made of modern composite materials is substantiated. The\nproblem is to obtain a cut of the given shape from honeycomb blocks. The\ncomplexity of this problem is in the irregular pattern of honeycomb blocks and\nthe presence of double edges, which forces an operator to scan each block\nbefore cutting. It is necessary to take into account such restrictions as the\nplace and angle of the cut and size of the knife, its angle when cutting and\nthe geometry of cells. For this problem solving, a robotic complex has been\ndeveloped. It includes a device for scanning the geometry of a honeycomb block,\nsoftware for cutting automation and a cutting device itself. The software takes\ninto account all restrictions on the choice of the location and angle of the\noperating mechanism. It helps to obtain the highest quality cut and a cut shape\nwith the best strength characteristics. An actu-ating device has been developed\nand implemented for both scanning and cutting of honeycomb blocks directly. The\nnecessary tests were carried out on real alu-minum honeycomb blocks. Some\ntechnical solutions are used in the cutting de-vice to improve the quality of\ncutting honeycomb blocks. The tests have shown the effectiveness of the\nproposed complex. Robotic planar cutting made it possi-ble to obtain precise\ncutting with a high degree of repeatability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.06585,regular,pre_llm,2020,8,"{'ai_likelihood': 3.4769376118977866e-06, 'text': ""COVID-Robot: Monitoring Social Distancing Constraints in Crowded\n  Scenarios\n\n  Maintaining social distancing norms between humans has become an\nindispensable precaution to slow down the transmission of COVID-19. We present\na novel method to automatically detect pairs of humans in a crowded scenario\nwho are not adhering to the social distance constraint, i.e. about 6 feet of\nspace between them. Our approach makes no assumption about the crowd density or\npedestrian walking directions. We use a mobile robot with commodity sensors,\nnamely an RGB-D camera and a 2-D lidar to perform collision-free navigation in\na crowd and estimate the distance between all detected individuals in the\ncamera's field of view. In addition, we also equip the robot with a thermal\ncamera that wirelessly transmits thermal images to a security/healthcare\npersonnel who monitors if any individual exhibits a higher than normal\ntemperature. In indoor scenarios, our mobile robot can also be combined with\nstatic mounted CCTV cameras to further improve the performance in terms of\nnumber of social distancing breaches detected, accurately pursuing walking\npedestrians etc. We highlight the performance benefits of our approach in\ndifferent static and dynamic indoor scenarios.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.11568,regular,pre_llm,2020,8,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Systematic Analysis of the Sensor Coverage of Automated Vehicles Using\n  Phenomenological Sensor Models\n\n  The objective of this paper is to propose a systematic analysis of the sensor\ncoverage of automated vehicles. Due to an unlimited number of possible traffic\nsituations, a selection of scenarios to be tested must be applied in the safety\nassessment of automated vehicles. This paper describes how phenomenological\nsensor models can be used to identify system-specific relevant scenarios. In\nautomated driving, the following sensors are predominantly used: camera,\nultrasonic, \\radar and \\lidarohne. Based on the literature, phenomenological\nmodels have been developed for the four sensor types, which take into account\nphenomena such as environmental influences, sensor properties and the type of\nobject to be detected. These phenomenological models have a significantly\nhigher reliability than simple ideal sensor models and require lower computing\ncosts than realistic physical sensor models, which represents an optimal\ncompromise for systematic investigations of sensor coverage. The simulations\nshowed significant differences between different system configurations and thus\nsupport the system-specific selection of relevant scenarios for the safety\nassessment of automated vehicles.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.09427,regular,pre_llm,2020,8,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Combining Control Barrier Functions and Behavior Trees for Multi-Agent\n  Underwater Coverage Missions\n\n  Robot missions typically involve a number of desired objectives, such as\navoiding collisions, staying connected to other robots, gathering information\nusing sensors and returning to the charging station before the battery runs\nout. Some of these objectives need to be taken into account at the same time,\nsuch as avoiding collisions and staying connected, while others are focused\nupon during different parts of the executions, such as returning to the\ncharging station and connectivity maintenance.\n  In this paper, we show how Control Barrier Functions(CBFs) and Behavior\nTrees(BTs) can be combined in a principled manner to achieve both types of task\ncompositions, with performance guarantees in terms of mission completion. We\nillustrate our method with a simulated underwater coverage mission.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.04627,regular,pre_llm,2020,8,"{'ai_likelihood': 4.404120975070529e-06, 'text': 'A Comparison of Humanoid Robot Simulators: A Quantitative Approach\n\n  Research on humanoid robotic systems involves a considerable amount of\ncomputational resources, not only for the involved design but also for its\ndevelopment and subsequent implementation. For robotic systems to be\nimplemented in real-world scenarios, in several situations, it is preferred to\ndevelop and test them under controlled environments in order to reduce the risk\nof errors and unexpected behavior. In this regard, a more accessible and\nefficient alternative is to implement the environment using robotic simulation\ntools. This paper presents a quantitative comparison of Gazebo, Webots, and\nV-REP, three simulators widely used by the research community to develop\nrobotic systems. To compare the performance of these three simulators, elements\nsuch as CPU, memory footprint, and disk access are used to measure and compare\nthem to each other. In order to measure the use of resources, each simulator\nexecutes 20 times a robotic scenario composed by a NAO robot that must navigate\nto a goal position avoiding a specific obstacle. In general terms, our results\nshow that Webots is the simulator with the lowest use of resources, followed by\nV-REP, which has advantages over Gazebo, mainly because of the CPU use.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2008.12725,regular,pre_llm,2020,8,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'iviz: A ROS Visualization App for Mobile Devices\n\n  In this work, we introduce iviz, a mobile application for visualizing ROS\ndata. In the last few years, the popularity of ROS has grown enormously, making\nit the standard platform for open source robotic programming. A key reason for\nthis success is the availability of polished, general-purpose modules for many\ntasks, such as localization, mapping, path planning, and quite importantly,\ndata visualization. However, the availability of the latter is generally\nrestricted to PCs with the Linux operating system. Thus, users that want to see\nwhat is happening in the system with a smartphone or a tablet are stuck with\nsolutions such as screen mirroring or using web browser versions of rviz, which\nare difficult to interact with from a mobile interface. More importantly, this\nmakes newer visualization modalities such as Augmented Reality impossible. Our\napplication iviz, based on the Unity engine, addresses these issues by\nproviding a visualization platform designed from scratch to be usable in mobile\nplatforms, such as iOS, Android, and UWP, and including native support for\nAugmented Reality for all three platforms. If desired, it can also be used in a\nPC with Linux, Windows, or macOS without any changes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.14551,regular,pre_llm,2020,9,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Explainable Deep Reinforcement Learning for UAV Autonomous Navigation\n\n  Autonomous navigation in unknown complex environment is still a hard problem,\nespecially for small Unmanned Aerial Vehicles (UAVs) with limited computation\nresources. In this paper, a neural network-based reactive controller is\nproposed for a quadrotor to fly autonomously in unknown outdoor environment.\nThe navigation controller makes use of only current sensor data to generate the\ncontrol signal without any optimization or configuration space searching, which\nreduces both memory and computation requirement. The navigation problem is\nmodelled as a Markov Decision Process (MDP) and solved using deep reinforcement\nlearning (DRL) method. Specifically, to get better understanding of the trained\nnetwork, some model explanation methods are proposed. Based on the feature\nattribution, each decision making result during flight is explained using both\nvisual and texture explanation. Moreover, some global analysis are also\nprovided for experts to evaluate and improve the trained neural network. The\nsimulation results illustrated the proposed method can make useful and\nreasonable explanation for the trained model, which is beneficial for both\nnon-expert users and controller designer. Finally, the real world tests shown\nthe proposed controller can navigate the quadrotor to goal position\nsuccessfully and the reactive controller performs much faster than some\nconventional approach under the same computation resource.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.10484,review,pre_llm,2020,9,"{'ai_likelihood': 5.9604644775390625e-06, 'text': ""Asymptotically Optimal Sampling-Based Motion Planning Methods\n\n  Motion planning is a fundamental problem in autonomous robotics that requires\nfinding a path to a specified goal that avoids obstacles and takes into account\na robot's limitations and constraints. It is often desirable for this path to\nalso optimize a cost function, such as path length.\n  Formal path-quality guarantees for continuously valued search spaces are an\nactive area of research interest. Recent results have proven that some\nsampling-based planning methods probabilistically converge toward the optimal\nsolution as computational effort approaches infinity. This survey summarizes\nthe assumptions behind these popular asymptotically optimal techniques and\nprovides an introduction to the significant ongoing research on this topic.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.1017,regular,pre_llm,2020,9,"{'ai_likelihood': 1.9040372636583117e-05, 'text': 'Time-of-Flight LiDAR-based Precise Mapping\n\n  Last two decades, the problem of robotic mapping has made a lot of progress\nin the research community. However, since the data provided by the sensor still\ncontains noise, how to obtain an accurate map is still an open problem. In this\nnote, we analyze the problem from the perspective of mathematical analysis and\npropose a probabilistic map update method based on multiple explorations. The\nproposed method can help us estimate the number of rounds of robot exploration,\nwhich is meaningful for the hardware and time costs of the task.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.05631,regular,pre_llm,2020,9,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Autonomous Hybrid Ground/Aerial Mobility in Unknown Environments\n\n  Hybrid ground and aerial vehicles can possess distinct advantages over\nground-only or flight-only designs in terms of energy savings and increased\nmobility. In this work we outline our unified framework for controls, planning,\nand autonomy of hybrid ground/air vehicles. Our contribution is three-fold: 1)\nWe develop a control scheme for the control of passive two-wheeled hybrid\nground/aerial vehicles. 2) We present a unified planner for both rolling and\nflying by leveraging differential flatness mappings. 3) We conduct experiments\nleveraging mapping and global planning for hybrid mobility in unknown\nenvironments, showing that hybrid mobility uses up to five times less energy\nthan flying only.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.03139,regular,pre_llm,2020,9,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Receding Horizon Task and Motion Planning in Changing Environments\n\n  Complex manipulation tasks require careful integration of symbolic reasoning\nand motion planning. This problem, commonly referred to as Task and Motion\nPlanning (TAMP), is even more challenging if the workspace is non-static, e.g.\ndue to human interventions and perceived with noisy non-ideal sensors. This\nwork proposes an online approximated TAMP method that combines a geometric\nreasoning module and a motion planner with a standard task planner in a\nreceding horizon fashion. Our approach iteratively solves a reduced planning\nproblem over a receding window of a limited number of future actions during the\nimplementation of the actions. Thus, only the first action of the horizon is\nactually scheduled at each iteration, then the window is moved forward, and the\nproblem is solved again. This procedure allows to naturally take into account\npotential changes in the scene while ensuring good runtime performance. We\nvalidate our approach within extensive experiments in a simulated environment.\nWe showed that our approach is able to deal with unexpected changes in the\nenvironment while ensuring comparable performance with respect to other recent\nTAMP approaches in solving traditional static benchmarks. We release with this\npaper the open-source implementation of our method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.11465,regular,pre_llm,2020,9,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Model Identification and Control of a Low-Cost Wheeled Mobile Robot\n  Using Differentiable Physics\n\n  We present the design of a low-cost wheeled mobile robot, and an analytical\nmodel for predicting its motion under the influence of motor torques and\nfriction forces. Using our proposed model, we show how to analytically compute\nthe gradient of an appropriate loss function, that measures the deviation\nbetween predicted motion trajectories and real-world trajectories, which are\nestimated using Apriltags and an overhead camera. These analytical gradients\nallow us to automatically infer the unknown friction coefficients, by\nminimizing the loss function using gradient descent. Motion trajectories that\nare predicted by the optimized model are in excellent agreement with their\nreal-world counterparts. Experiments show that our proposed approach is\ncomputationally superior to existing black-box system identification methods\nand other data-driven techniques, and also requires very few real-world samples\nfor accurate trajectory prediction. The proposed approach combines the data\nefficiency of analytical models based on first principles, with the flexibility\nof data-driven methods, which makes it appropriate for low-cost robots. Using\nthe learned model and our gradient-based optimization approach, we show how to\nautomatically compute motor control signals for driving the robot along\npre-specified curves.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.05891,regular,pre_llm,2020,9,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'MPC-Based Hierarchical Task Space Control of Underactuated and\n  Constrained Robots for Execution of Multiple Tasks\n\n  This paper proposes an MPC-based controller to efficiently execute multiple\nhierarchical tasks for underactuated and constrained robotic systems. Existing\ntask-space controllers or whole-body controllers solve instantaneous\noptimization problems given task trajectories and the robot plant dynamics.\nHowever, the task-space control method we propose here relies on the prediction\nof future state trajectories and the corresponding costs-to-go terms over a\nfinite time-horizon for computing control commands. We employ acceleration\nenergy error as the performance index for the optimization problem and extend\nit over the finite-time horizon of our MPC. Our approach employs quadratically\nconstrained quadratic programming, which includes quadratic constraints to\nhandle multiple hierarchical tasks, and is computationally more efficient than\nnonlinear MPC-based approaches that rely on nonlinear programming. We validate\nour approach using numerical simulations of a new type of robot manipulator\nsystem, which contains underactuated and constrained mechanical structures.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.13475,regular,pre_llm,2020,9,"{'ai_likelihood': 3.8080745273166235e-06, 'text': 'Enhancing Continuous Control of Mobile Robots for End-to-End Visual\n  Active Tracking\n\n  In the last decades, visual target tracking has been one of the primary\nresearch interests of the Robotics research community. The recent advances in\nDeep Learning technologies have made the exploitation of visual tracking\napproaches effective and possible in a wide variety of applications, ranging\nfrom automotive to surveillance and human assistance. However, the majority of\nthe existing works focus exclusively on passive visual tracking, i.e., tracking\nelements in sequences of images by assuming that no actions can be taken to\nadapt the camera position to the motion of the tracked entity. On the contrary,\nin this work, we address visual active tracking, in which the tracker has to\nactively search for and track a specified target. Current State-of-the-Art\napproaches use Deep Reinforcement Learning (DRL) techniques to address the\nproblem in an end-to-end manner. However, two main problems arise: i) most of\nthe contributions focus only on discrete action spaces and the ones that\nconsider continuous control do not achieve the same level of performance; and\nii) if not properly tuned, DRL models can be challenging to train, resulting in\na considerably slow learning progress and poor final performance. To address\nthese challenges, we propose a novel DRL-based visual active tracking system\nthat provides continuous action policies. To accelerate training and improve\nthe overall performance, we introduce additional objective functions and a\nHeuristic Trajectory Generator (HTG) to facilitate learning. Through an\nextensive experimentation, we show that our method can reach and surpass other\nState-of-the-Art approaches performances, and demonstrate that, even if trained\nexclusively in simulation, it can successfully perform visual active tracking\neven in real scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.01612,regular,pre_llm,2020,9,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Evaluation of a Skill-based Control Architecture for a Visual\n  Inspection-oriented Aerial Platform\n\n  The periodic inspection of vessels is a fundamental task to ensure their\nintegrity and avoid maritime accidents. Currently, these inspections represent\na high cost for the ship owner, in addition to the danger that this kind of\nhostile environment entails for the surveyors. In these situations, robotic\nplatforms turn out to be useful not only for safety reasons, but also to reduce\nvessel downtimes and simplify the inspection procedures. Under this context, in\nthis paper we report on the evaluation of a new control architecture devised to\ndrive an aerial platform during these inspection procedures. The control\narchitecture, based on an extensive use of behaviour-based high-level control,\nimplements visual inspection-oriented functionalities, while releases the\noperator from the complexities of inspection flights and ensures the integrity\nof the platform. Apart from the control software, the full system comprises a\nmulti-rotor platform equipped with a suitable set of sensors to permit\nteleporting the surveyor to the areas that need inspection. The paper provides\nan extensive set of testing results in different scenarios, under different\noperational conditions and over real vessels, in order to demonstrate the\nsuitability of the platform for this kind of tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.08824,regular,pre_llm,2020,9,"{'ai_likelihood': 5.4637591044108076e-06, 'text': ""Pedestrian Motion Tracking by Using Inertial Sensors on the Smartphone\n\n  Inertial Measurement Unit (IMU) has long been a dream for stable and reliable\nmotion estimation, especially in indoor environments where GPS strength limits.\nIn this paper, we propose a novel method for position and orientation\nestimation of a moving object only from a sequence of IMU signals collected\nfrom the phone. Our main observation is that human motion is monotonous and\nperiodic. We adopt the Extended Kalman Filter and use the learning-based method\nto dynamically update the measurement noise of the filter. Our pedestrian\nmotion tracking system intends to accurately estimate planar position,\nvelocity, heading direction without restricting the phone's daily use. The\nmethod is not only tested on the self-collected signals, but also provides\naccurate position and velocity estimations on the public RIDI dataset, i.e.,\nthe absolute transmit error is 1.28m for a 59-second sequence.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.09857,regular,pre_llm,2020,9,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Heterogeneous Fixed-wing Aerial Vehicles for Resilient Coverage of an\n  Area\n\n  This paper presents a distributed approach to provide persistent coverage of\nan arbitrarily shaped area using heterogeneous coverage of fixed-wing unmanned\naerial vehicles (UAVs), and to recover from simultaneous failures of multiple\nUAVs. The proposed approach discusses level-homogeneous deployment and\nmaintenance of a homogeneous fleet of fixed-wing UAVs given the boundary\ninformation and the minimum loitering radius. The UAVs are deployed at\ndifferent altitude levels to provide heterogeneous coverage and sensing. We use\nan efficient square packing method to deploy the UAVs, given the minimum loiter\nradius and the area boundary. The UAVs loiter over the circles inscribed over\nthese packing squares in a synchronized motion to fulfill the full coverage\nobjective. An top-down hierarchy of the square packing, where each outer square\n(super-square) is partitioned into four equal-sized inner squares (sub-square),\nis exploited to introduce resilience in the deployed UAV-network. For a failed\nsub-square UAV, a replacement neighbor is chosen considering the effective\ncoverage and deployed to the corresponding super-square at a higher altitude to\nrecover full coverage, trading-off with the quality of coverage of the\nsub-area. This is a distributed approach as all the decision making is done\nwithin close range of the loss region, and it can be scaled and adapted to\nvarious large scale area and UAV configurations. Simulation results have been\npresented to illustrate and verify the applicability of the approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.0477,regular,pre_llm,2020,9,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Defining Adaptive Proxemic Zones for Activity-aware Navigation\n\n  Many of the tasks that a service robot can perform at home involve navigation\nskills. In a real world scenario, the navigation system should consider\nindividuals beyond just objects, theses days it is necessary to offer\nparticular and dynamic representation in the scenario in order to enhance the\nHRI experience. In this paper, we use the proxemic theory to do this\nrepresentation. The proxemic zones are not static. The culture or the context\ninfluences them and, if we have this influence into account, we can increase\nhumans' comfort. Moreover, there are collaborative tasks in which these zones\ntake different shapes to allow the task's best performance. This research\ndevelops a layer, the social layer, to represent and distribute the proxemics\nzones' information in a standard way, through a cost map and using it to\nperform a social navigate task. We have evaluated these components in a\nsimulated scenario, performing different collaborative and human-robot\ninteraction tasks and reducing the personal area invasion in a 32\\%. The\nmaterial developed during this research can be found in a public repository, as\nwell as instructions to facilitate the reproducibility of the results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.07207,regular,pre_llm,2020,9,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Autonomous Navigation in Unknown Environments with Sparse Bayesian\n  Kernel-based Occupancy Mapping\n\n  This paper focuses on online occupancy mapping and real-time collision\nchecking onboard an autonomous robot navigating in a large unknown environment.\nCommonly used voxel and octree map representations can be easily maintained in\na small environment but have increasing memory requirements as the environment\ngrows. We propose a fundamentally different approach for occupancy mapping, in\nwhich the boundary between occupied and free space is viewed as the decision\nboundary of a machine learning classifier. This work generalizes a kernel\nperceptron model which maintains a very sparse set of support vectors to\nrepresent the environment boundaries efficiently. We develop a probabilistic\nformulation based on Relevance Vector Machines, allowing robustness to\nmeasurement noise and probabilistic occupancy classification, supporting\nautonomous navigation. We provide an online training algorithm, updating the\nsparse Bayesian map incrementally from streaming range data, and an efficient\ncollision-checking method for general curves, representing potential robot\ntrajectories. The effectiveness of our mapping and collision checking\nalgorithms is evaluated in tasks requiring autonomous robot navigation and\nactive mapping in unknown environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.05085,regular,pre_llm,2020,9,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Keypoints into the Future: Self-Supervised Correspondence in Model-Based\n  Reinforcement Learning\n\n  Predictive models have been at the core of many robotic systems, from\nquadrotors to walking robots. However, it has been challenging to develop and\napply such models to practical robotic manipulation due to high-dimensional\nsensory observations such as images. Previous approaches to learning models in\nthe context of robotic manipulation have either learned whole image dynamics or\nused autoencoders to learn dynamics in a low-dimensional latent state. In this\nwork, we introduce model-based prediction with self-supervised visual\ncorrespondence learning, and show that not only is this indeed possible, but\ndemonstrate that these types of predictive models show compelling performance\nimprovements over alternative methods for vision-based RL with autoencoder-type\nvision training. Through simulation experiments, we demonstrate that our models\nprovide better generalization precision, particularly in 3D scenes, scenes\ninvolving occlusion, and in category-generalization. Additionally, we validate\nthat our method effectively transfers to the real world through hardware\nexperiments. Videos and supplementary materials available at\nhttps://sites.google.com/view/keypointsintothefuture\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.02041,regular,pre_llm,2020,9,"{'ai_likelihood': 4.404120975070529e-06, 'text': ""AIR-Act2Act: Human-human interaction dataset for teaching non-verbal\n  social behaviors to robots\n\n  To better interact with users, a social robot should understand the users'\nbehavior, infer the intention, and respond appropriately. Machine learning is\none way of implementing robot intelligence. It provides the ability to\nautomatically learn and improve from experience instead of explicitly telling\nthe robot what to do. Social skills can also be learned through watching\nhuman-human interaction videos. However, human-human interaction datasets are\nrelatively scarce to learn interactions that occur in various situations.\nMoreover, we aim to use service robots in the elderly-care domain; however,\nthere has been no interaction dataset collected for this domain. For this\nreason, we introduce a human-human interaction dataset for teaching non-verbal\nsocial behaviors to robots. It is the only interaction dataset that elderly\npeople have participated in as performers. We recruited 100 elderly people and\ntwo college students to perform 10 interactions in an indoor environment. The\nentire dataset has 5,000 interaction samples, each of which contains depth\nmaps, body indexes and 3D skeletal data that are captured with three Microsoft\nKinect v2 cameras. In addition, we provide the joint angles of a humanoid NAO\nrobot which are converted from the human behavior that robots need to learn.\nThe dataset and useful python scripts are available for download at\nhttps://github.com/ai4r/AIR-Act2Act. It can be used to not only teach social\nskills to robots but also benchmark action recognition algorithms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.12662,regular,pre_llm,2020,9,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry\n\n  This work proposes a novel SLAM framework for stereo and visual inertial\nodometry estimation. It builds an efficient and robust parametrization of\nco-planar points and lines which leverages specific geometric constraints to\nimprove camera pose optimization in terms of both efficiency and accuracy.\n%reduce the size of the Hessian matrix in the optimization. The pipeline\nconsists of extracting 2D points and lines, predicting planar regions and\nfiltering the outliers via RANSAC. Our parametrization scheme then represents\nco-planar points and lines as their 2D image coordinates and parameters of\nplanes. We demonstrate the effectiveness of the proposed method by comparing it\nto traditional parametrizations in a novel Monte-Carlo simulation set. Further,\nthe whole stereo SLAM and VIO system is compared with state-of-the-art methods\non the public real-world dataset EuRoC. Our method shows better results in\nterms of accuracy and efficiency than the state-of-the-art. The code is\nreleased at https://github.com/LiXin97/Co-Planar-Parametrization.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.14089,regular,pre_llm,2020,9,"{'ai_likelihood': 1.1622905731201172e-05, 'text': ""Four-Arm Collaboration: Two Dual-Arm Robots Work Together to Maneuver\n  Tethered Tools\n\n  In this paper, we present a planner for a master dual-arm robot to manipulate\ntethered tools with an assistant dual-arm robot's help. The assistant robot\nprovides assistance to the master robot by manipulating the tool cable and\navoiding collisions. The provided assistance allows the master robot to perform\ntool placements on the robot workspace table to regrasp the tool, which would\ntypically fail since the tool cable tension may change the tool positions. It\nalso allows the master robot to perform tool handovers, which would normally\ncause entanglements or collisions with the cable and the environment without\nthe assistance. Simulations and real-world experiments are performed to\nvalidate the proposed planner.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.1396,review,pre_llm,2020,9,"{'ai_likelihood': 2.2517310248480903e-06, 'text': 'Reality-assisted evolution of soft robots through large-scale physical\n  experimentation: a review\n\n  In this review we introduce the framework of reality-assisted evolution to\nsummarize a growing trend towards combining model-based and model-free\napproaches to improve the design of physically embodied soft robots. In silico,\ndata-driven models build, adapt and improve representations of the target\nsystem using real-world experimental data. By simulating huge numbers of\nvirtual robots using these data-driven models, optimization algorithms can\nilluminate multiple design candidates for transference to the real world. In\nreality, large-scale physical experimentation facilitates the fabrication,\ntesting and analysis of multiple candidate designs. Automated assembly and\nreconfigurable modular systems enable significantly higher numbers of\nreal-world design evaluations than previously possible. Large volumes of\nground-truth data gathered via physical experimentation can be returned to the\nvirtual environment to improve data-driven models and guide optimization.\nGrounding the design process in physical experimentation ensures the complexity\nof virtual robot designs does not outpace the model limitations or available\nfabrication technologies. We outline key developments in the design of\nphysically embodied soft robots under the framework of reality-assisted\nevolution.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.11726,regular,pre_llm,2020,9,"{'ai_likelihood': 3.907415601942274e-06, 'text': ""Evaluation of an indoor localization system for a mobile robot\n\n  Although indoor localization has been a wide researched topic, obtained\nresults may not fit the requirements that some domains need. Most approaches\nare not able to precisely localize a fast moving object even with a complex\ninstallation, which makes their implementation in the automated driving domain\ncomplicated. In this publication, common technologies were analyzed and a\ncommercial product, called Marvelmind Indoor GPS, was chosen for our use case\nin which both ultrasound and radio frequency communications are used. The\nevaluation is given in a first moment on small indoor scenarios with static and\nmoving objects. Further tests were done on wider areas, where the system is\nintegrated within our Robotics Operating System (ROS)-based self-developed\n'Smart PhysIcal Demonstration and evaluation Robot (SPIDER)' and the results of\nthese outdoor tests are compared with the obtained localization by the\ninstalled GPS on the robot. Finally, the next steps to improve the results in\nfurther developments are discussed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2009.05345,regular,pre_llm,2020,9,"{'ai_likelihood': 3.1458006964789497e-06, 'text': ""A Toolkit to Generate Social Navigation Datasets\n\n  Social navigation datasets are necessary to assess social navigation\nalgorithms and train machine learning algorithms. Most of the currently\navailable datasets target pedestrians' movements as a pattern to be replicated\nby robots. It can be argued that one of the main reasons for this to happen is\nthat compiling datasets where real robots are manually controlled, as they\nwould be expected to behave when moving, is a very resource-intensive task.\nAnother aspect that is often missing in datasets is symbolic information that\ncould be relevant, such as human activities, relationships or interactions.\nUnfortunately, the available datasets targeting robots and supporting symbolic\ninformation are restricted to static scenes. This paper argues that simulation\ncan be used to gather social navigation data in an effective and cost-efficient\nway and presents a toolkit for this purpose. A use case studying the\napplication of graph neural networks to create learned control policies using\nsupervised learning is presented as an example of how it can be used.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.00779,regular,pre_llm,2020,10,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Planning a Sequence of Base Positions for a Mobile Manipulator to\n  Perform Multiple Pick-and-Place Tasks\n\n  In this paper, we present a planner that plans a sequence of base positions\nfor a mobile manipulator to efficiently and robustly collect objects stored in\ndistinct trays. We achieve high efficiency by exploring the common areas where\na mobile manipulator can grasp objects stored in multiple trays simultaneously\nand move the mobile manipulator to the common areas to reduce the time needed\nfor moving the mobile base. We ensure robustness by optimizing the base\nposition with the best clearance to positioning uncertainty so that a mobile\nmanipulator can complete the task even if there is a certain deviation from the\nplanned base positions. Besides, considering different styles of object\nplacement in the tray, we analyze feasible schemes for dynamically updating the\nbase positions based on either the remaining objects or the target objects to\nbe picked in one round of the tasks. In the experiment part, we examine our\nplanner on various scenarios, including different object placement: (1)\nRegularly placed toy objects; (2) Randomly placed industrial parts; and\ndifferent schemes for online execution: (1) Apply globally static base\npositions; (2) Dynamically update the base positions. The experiment results\ndemonstrate the efficiency, robustness and feasibility of the proposed method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.09859,regular,pre_llm,2020,10,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'A Unified Approach for Autonomous Volumetric Exploration of Large Scale\n  Environments under Severe Odometry Drift\n\n  Exploration is a fundamental problem in robot autonomy. A major limitation,\nhowever, is that during exploration robots oftentimes have to rely on on-board\nsystems alone for state estimation, accumulating significant drift over time in\nlarge environments. Drift can be detrimental to robot safety and exploration\nperformance. In this work, a submap-based, multi-layer approach for both\nmapping and planning is proposed to enable safe and efficient volumetric\nexploration of large scale environments despite odometry drift. The central\nidea of our approach combines local (temporally and spatially) and global\nmapping to guarantee safety and efficiency. Similarly, our planning approach\nleverages the presented map to compute global volumetric frontiers in a\nchanging global map and utilizes the nature of exploration dealing with partial\ninformation for efficient local and global planning. The presented system is\nthoroughly evaluated and shown to outperform state of the art methods even\nunder drift-free conditions. Our system, termed GLoca}, will be made available\nopen source.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.11675,regular,pre_llm,2020,10,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Optimization-Based Visual-Inertial SLAM Tightly Coupled with Raw GNSS\n  Measurements\n\n  Unlike loose coupling approaches and the EKF-based approaches in the\nliterature, we propose an optimization-based visual-inertial SLAM tightly\ncoupled with raw Global Navigation Satellite System (GNSS) measurements, a\nfirst attempt of this kind in the literature to our knowledge. More\nspecifically, reprojection error, IMU pre-integration error and raw GNSS\nmeasurement error are jointly minimized within a sliding window, in which the\nasynchronism between images and raw GNSS measurements is accounted for. In\naddition, issues such as marginalization, noisy measurements removal, as well\nas tackling vulnerable situations are also addressed. Experimental results on\npublic dataset in complex urban scenes show that our proposed approach\noutperforms state-of-the-art visual-inertial SLAM, GNSS single point\npositioning, as well as a loose coupling approach, including scenes mainly\ncontaining low-rise buildings and those containing urban canyons.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.04401,regular,pre_llm,2020,10,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Lyapunov-Stable Orientation Estimator for Humanoid Robots\n\n  In this paper, we present an observation scheme, with proven Lyapunov\nstability, for estimating a humanoid's floating base orientation. The idea is\nto use velocity aided attitude estimation, which requires to know the velocity\nof the system. This velocity can be obtained by taking into account the\nkinematic data provided by contact information with the environment and using\nthe IMU and joint encoders. We demonstrate how this operation can be used in\nthe case of a fixed or a moving contact, allowing it to be employed for\nlocomotion. We show how to use this velocity estimation within a selected\ntwo-stage state tilt estimator: (i) the first which has a global and quick\nconvergence (ii) and the second which has smooth and robust dynamics. We\nprovide new specific proofs of almost global Lyapunov asymptotic stability and\nlocal exponential convergence for this observer. Finally, we assess its\nperformance by employing a comparative simulation and by using it within a\nclosed-loop stabilization scheme for HRP-5P and HRP-2KAI robots performing\nwhole-body kinematic tasks and locomotion.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.12326,regular,pre_llm,2020,10,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'Robust Footstep Planning and LQR Control for Dynamic Quadrupedal\n  Locomotion\n\n  In this paper, we aim to improve the robustness of dynamic quadrupedal\nlocomotion through two aspects: 1) fast model predictive foothold planning, and\n2) applying LQR to projected inverse dynamic control for robust motion\ntracking. In our proposed planning and control framework, foothold plans are\nupdated at 400 Hz considering the current robot state and an LQR controller\ngenerates optimal feedback gains for motion tracking. The LQR optimal gain\nmatrix with non-zero off-diagonal elements leverages the coupling of dynamics\nto compensate for system underactuation. Meanwhile, the projected inverse\ndynamic control complements the LQR to satisfy inequality constraints. In\naddition to these contributions, we show robustness of our control framework to\nunmodeled adaptive feet. Experiments on the quadruped ANYmal demonstrate the\neffectiveness of the proposed method for robust dynamic locomotion given\nexternal disturbances and environmental uncertainties.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.14537,review,pre_llm,2020,10,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'The State of Industrial Robotics: Emerging Technologies, Challenges, and\n  Key Research Directions\n\n  Robotics and related technologies are central to the ongoing digitization and\nadvancement of manufacturing. In recent years, a variety of strategic\ninitiatives around the world including ""Industry 4.0"", introduced in Germany in\n2011 have aimed to improve and connect manufacturing technologies in order to\noptimize production processes. In this work, we study the changing\ntechnological landscape of robotics and ""internet-of-things"" (IoT)-based\nconnective technologies over the last 7-10 years in the wake of Industry 4.0.\nWe interviewed key players within the European robotics ecosystem, including\nrobotics manufacturers and integrators, original equipment manufacturers\n(OEMs), and applied industrial research institutions and synthesize our\nfindings in this paper. We first detail the state-of-the-art robotics and IoT\ntechnologies we observed and that the companies discussed during our\ninterviews. We then describe the processes the companies follow when deciding\nwhether and how to integrate new technologies, the challenges they face when\nintegrating these technologies, and some immediate future technological avenues\nthey are exploring in robotics and IoT. Finally, based on our findings, we\nhighlight key research directions for the robotics community that can enable\nimproved capabilities in the context of manufacturing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.07929,regular,pre_llm,2020,10,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Multi-Resolution 3D Mapping with Explicit Free Space Representation for\n  Fast and Accurate Mobile Robot Motion Planning\n\n  With the aim of bridging the gap between high quality reconstruction and\nmobile robot motion planning, we propose an efficient system that leverages the\nconcept of adaptive-resolution volumetric mapping, which naturally integrates\nwith the hierarchical decomposition of space in an octree data structure.\nInstead of a Truncated Signed Distance Function (TSDF), we adopt mapping of\noccupancy probabilities in log-odds representation, which allows to represent\nboth surfaces, as well as the entire free, i.e. observed space, as opposed to\nunobserved space. We introduce a method for choosing resolution -- on the fly\n-- in real-time by means of a multi-scale max-min pooling of the input depth\nimage. The notion of explicit free space mapping paired with the spatial\nhierarchy in the data structure, as well as map resolution, allows for\ncollision queries, as needed for robot motion planning, at unprecedented speed.\nWe quantitatively evaluate mapping accuracy, memory, runtime performance, and\nplanning performance showing improvements over the state of the art,\nparticularly in cases requiring high resolution maps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.06752,regular,pre_llm,2020,10,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Mechanical Design Improvement of a Passive Device to Assist Eating in\n  People Living with Movement Disorders\n\n  Many people living with neurological disorders, such as cerebral palsy,\nstroke, muscular dystrophy or dystonia experience upper limb impairments\n(muscle spasticity, loss of selective motor control, muscle weakness or\ntremors) and have difficulty to eat independently. The general goal of this\nproject is to develop a new device to assist with eating, aimed at stabilizing\nthe movement of people who have movement disorders. A first iteration of the\ndevice was validated with children living with cerebral palsy and showed\npromising results. This validation however pointed out important drawbacks.\nThis paper presents an iteration of the design which includes a new mechanism\nreducing the required arm elevation, improving safety through a compliant\nutensil attachment, and improving damping and other static balancing factors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.08506,regular,pre_llm,2020,10,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Direct Policy Optimization using Deterministic Sampling and Collocation\n\n  We present an approach for approximately solving discrete-time stochastic\noptimal-control problems by combining direct trajectory optimization,\ndeterministic sampling, and policy optimization. Our feedback motion-planning\nalgorithm uses a quasi-Newton method to simultaneously optimize a reference\ntrajectory, a set of deterministically chosen sample trajectories, and a\nparameterized policy. We demonstrate that this approach exactly recovers LQR\npolicies in the case of linear dynamics, quadratic objective, and Gaussian\ndisturbances. We also demonstrate the algorithm on several nonlinear,\nunderactuated robotic systems to highlight its performance and ability to\nhandle control limits, safely avoid obstacles, and generate robust plans in the\npresence of unmodeled dynamics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.1207,regular,pre_llm,2020,10,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Dynamics and Domain Randomized Gait Modulation with Bezier Curves for\n  Sim-to-Real Legged Locomotion\n\n  We present a sim-to-real framework that uses dynamics and domain randomized\noffline reinforcement learning to enhance open-loop gaits for legged robots,\nallowing them to traverse uneven terrain without sensing foot impacts. Our\napproach, D$^2$-Randomized Gait Modulation with Bezier Curves (D$^2$-GMBC),\nuses augmented random search with randomized dynamics and terrain to train, in\nsimulation, a policy that modifies the parameters and output of an open-loop\nBezier curve gait generator for quadrupedal robots. The policy, using only\ninertial measurements, enables the robot to traverse unknown rough terrain,\neven when the robot's physical parameters do not match the open-loop model.\n  We compare the resulting policy to hand-tuned Bezier Curve gaits and to\npolicies trained without randomization, both in simulation and on a real\nquadrupedal robot. With D$^2$-GMBC, across a variety of experiments on\nunobserved and unknown uneven terrain, the robot walks significantly farther\nthan with either hand-tuned gaits or gaits learned without domain\nrandomization. Additionally, using D$^2$-GMBC, the robot can walk laterally and\nrotate while on the rough terrain, even though it was trained only for forward\nwalking.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.06082,regular,pre_llm,2020,10,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'Preliminary Development of a Wearable Device to Help Children with\n  Unilateral Cerebral Palsy Increase Their Consciousness of Their Upper\n  Extremity\n\n  Children with unilateral cerebral palsy have movement impairments that are\npredominant to one of their upper extremities (UE) and are prone to a\nphenomenon named ""developmental disregard"", which is characterized by the\nneglect of their most affected UE because of their altered perception or\nconsciousness of this limb. This can cause them not to use their most affected\nhand to its full capacity in their day-to-day life. This paper presents a\nprototype of a wearable technology with the appearance of a smartwatch, which\ndelivers haptic feedback to remind children with unilateral cerebral palsy to\nuse their most affected limb, and which increase sensory afferents to possibly\ninfluence brain plasticity. The prototype consists of an accelerometer, a\nvibration motor and a microcontroller with an algorithm that detects movement\nof the limb. After a given period of inactivity, the watch starts vibrating to\nalert the user.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.13393,regular,pre_llm,2020,10,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Exoway: an exoskeleton on actuated wheels\n\n  In this short work we present a low cost exoskeleton with actuated wheels\nthat allows movements as well as skating-like steps. The simple structure and\nthe actuated wheels allows to minimize the use of motors for locomotion. The\nstructure is stabilized by an active control system that balances the structure\nand permit to be maneuvered by the driver whose commands are acquired by a\ndedicated hardware interface.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00072,regular,pre_llm,2020,10,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Learning Stable Normalizing-Flow Control for Robotic Manipulation\n\n  Reinforcement Learning (RL) of robotic manipulation skills, despite its\nimpressive successes, stands to benefit from incorporating domain knowledge\nfrom control theory. One of the most important properties that is of interest\nis control stability. Ideally, one would like to achieve stability guarantees\nwhile staying within the framework of state-of-the-art deep RL algorithms. Such\na solution does not exist in general, especially one that scales to complex\nmanipulation tasks. We contribute towards closing this gap by introducing\n$\\textit{normalizing-flow}$ control structure, that can be deployed in any\nlatest deep RL algorithms. While stable exploration is not guaranteed, our\nmethod is designed to ultimately produce deterministic controllers with\nprovable stability. In addition to demonstrating our method on challenging\ncontact-rich manipulation tasks, we also show that it is possible to achieve\nconsiderable exploration efficiency--reduced state space coverage and actuation\nefforts--without losing learning efficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.08184,regular,pre_llm,2020,10,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'PRIMAL2: Pathfinding via Reinforcement and Imitation Multi-Agent\n  Learning -- Lifelong\n\n  Multi-agent path finding (MAPF) is an indispensable component of large-scale\nrobot deployments in numerous domains ranging from airport management to\nwarehouse automation. In particular, this work addresses lifelong MAPF (LMAPF)\n- an online variant of the problem where agents are immediately assigned a new\ngoal upon reaching their current one - in dense and highly structured\nenvironments, typical of real-world warehouse operations. Effectively solving\nLMAPF in such environments requires expensive coordination between agents as\nwell as frequent replanning abilities, a daunting task for existing coupled and\ndecoupled approaches alike. With the purpose of achieving considerable agent\ncoordination without any compromise on reactivity and scalability, we introduce\nPRIMAL2, a distributed reinforcement learning framework for LMAPF where agents\nlearn fully decentralized policies to reactively plan paths online in a\npartially observable world. We extend our previous work, which was effective in\nlow-density sparsely occupied worlds, to highly structured and constrained\nworlds by identifying behaviors and conventions which improve implicit agent\ncoordination, and enable their learning through the construction of a novel\nlocal agent observation and various training aids. We present extensive results\nof PRIMAL2 in both MAPF and LMAPF environments and compare its performance to\nstate-of-the-art planners in terms of makespan and throughput. We show that\nPRIMAL2 significantly surpasses our previous work and performs comparably to\nthese baselines, while allowing real-time re-planning and scaling up to 2048\nagents.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.04931,regular,pre_llm,2020,10,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Origami-based Shape Morphing Fingertip to Enhance Grasping Stability and\n  Dexterity\n\n  Adaptation to various scene configurations and object properties, stability\nand dexterity in robotic grasping manipulation is far from explored. This work\npresents an origami-based shape morphing fingertip design to actively tackle\nthe grasping stability and dexterity problems. The proposed fingertip utilizes\norigami as its skeleton providing degrees of freedom at desired positions and\nmotor-driven four-bar-linkages as its transmission components to achieve a\ncompact size of the fingertip. 3 morphing types that are commonly observed and\nessential in robotic grasping are studied and validated with geometrical\nmodeling. Experiments including grasping an object with convex point contact to\npivot or do pinch grasping, grasped object reorientation, and enveloping\ngrasping with concave fingertip surfaces are implemented to demonstrate the\nadvantages of our fingertip compared to conventional parallel grippers.\nMulti-functionality on enhancing grasping stability and dexterity via active\nadaptation given different grasped objects and manipulation tasks are\njustified. Video is available at youtu.be/jJoJ3xnDdVk/.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.03348,regular,pre_llm,2020,10,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'Drone Positioning for Visible Light Communication with Drone-Mounted LED\n  and Camera\n\n  The world is often stricken by catastrophic disasters. On-demand\ndrone-mounted visible light communication (VLC) networks are suitable for\nmonitoring disaster-stricken areas for leveraging disaster-response operations.\nThe concept of an image sensor-based VLC has also attracted attention in the\nrecent past for establishing stable links using unstably moving drones.\nHowever, existing works did not sufficiently consider the one-to-many image\nsensor-based VLC system. Thus, this paper proposes the concept of a one-to-many\nimage sensor-based VLC between a camera and multiple drone-mounted LED lights\nwith a drone-positioning algorithm to avoid interference among VLC links.\nMultiple drones are deployed on-demand in a disaster-stricken area to monitor\nthe ground and continuously send image data to a camera with image sensor-based\nvisible light communication (VLC) links. The proposed idea is demonstrated with\nthe proof-of-concept (PoC) implemented with drones that are equipped with LED\npanels and a 4K camera. As a result, we confirmed the feasibility of the\nproposed system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.09232,regular,pre_llm,2020,10,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Elastic and Efficient LiDAR Reconstruction for Large-Scale Exploration\n  Tasks\n\n  We present an efficient, elastic 3D LiDAR reconstruction framework which can\nreconstruct up to maximum LiDAR ranges (60 m) at multiple frames per second,\nthus enabling robot exploration in large-scale environments. Our approach only\nrequires a CPU. We focus on three main challenges of large-scale\nreconstruction: integration of long-range LiDAR scans at high frequency, the\ncapacity to deform the reconstruction after loop closures are detected, and\nscalability for long-duration exploration. Our system extends upon a\nstate-of-the-art efficient RGB-D volumetric reconstruction technique, called\nsupereight, to support LiDAR scans and a newly developed submapping technique\nto allow for dynamic correction of the 3D reconstruction. We then introduce a\nnovel pose graph clustering and submap fusion feature to make the proposed\nsystem more scalable for large environments. We evaluate the performance using\ntwo public datasets including outdoor exploration with a handheld device and a\ndrone, and with a mobile robot exploring an underground room network.\nExperimental results demonstrate that our system can reconstruct at 3 Hz with\n60 m sensor range and ~5 cm resolution, while state-of-the-art approaches can\nonly reconstruct to 25 cm resolution or 20 m range at the same frequency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.10841,regular,pre_llm,2020,10,"{'ai_likelihood': 9.834766387939453e-06, 'text': ""RigidFusion: Robot Localisation and Mapping in Environments with Large\n  Dynamic Rigid Objects\n\n  This work presents a novel RGB-D SLAM approach to simultaneously segment,\ntrack and reconstruct the static background and large dynamic rigid objects\nthat can occlude major portions of the camera view. Previous approaches treat\ndynamic parts of a scene as outliers and are thus limited to a small amount of\nchanges in the scene, or rely on prior information for all objects in the scene\nto enable robust camera tracking. Here, we propose to treat all dynamic parts\nas one rigid body and simultaneously segment and track both static and dynamic\ncomponents. We, therefore, enable simultaneous localisation and reconstruction\nof both the static background and rigid dynamic components in environments\nwhere dynamic objects cause large occlusion. We evaluate our approach on\nmultiple challenging scenes with large dynamic occlusion. The evaluation\ndemonstrates that our approach achieves better motion segmentation,\nlocalisation and mapping without requiring prior knowledge of the dynamic\nobject's shape and appearance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.06544,regular,pre_llm,2020,10,"{'ai_likelihood': 9.404288397894965e-06, 'text': 'Real-Time Deep Learning Approach to Visual Servo Control and Grasp\n  Detection for Autonomous Robotic Manipulation\n\n  In order to explore robotic grasping in unstructured and dynamic\nenvironments, this work addresses the visual perception phase involved in the\ntask. This phase involves the processing of visual data to obtain the location\nof the object to be grasped, its pose and the points at which the robot`s\ngrippers must make contact to ensure a stable grasp. For this, the Cornell\nGrasping dataset is used to train a convolutional neural network that, having\nan image of the robot`s workspace, with a certain object, is able to predict a\ngrasp rectangle that symbolizes the position, orientation and opening of the\nrobot`s grippers before its closing. In addition to this network, which runs in\nreal-time, another one is designed to deal with situations in which the object\nmoves in the environment. Therefore, the second network is trained to perform a\nvisual servo control, ensuring that the object remains in the robot`s field of\nview. This network predicts the proportional values of the linear and angular\nvelocities that the camera must have so that the object is always in the image\nprocessed by the grasp network. The dataset used for training was automatically\ngenerated by a Kinova Gen3 manipulator. The robot is also used to evaluate the\napplicability in real-time and obtain practical results from the designed\nalgorithms. Moreover, the offline results obtained through validation sets are\nalso analyzed and discussed regarding their efficiency and processing speed.\nThe developed controller was able to achieve a millimeter accuracy in the final\nposition considering a target object seen for the first time. To the best of\nour knowledge, we have not found in the literature other works that achieve\nsuch precision with a controller learned from scratch. Thus, this work presents\na new system for autonomous robotic manipulation with high processing speed and\nthe ability to generalize to several different objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2010.01215,regular,pre_llm,2020,10,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Efficient Multi-Contact Pattern Generation with Sequential Convex\n  Approximations of the Centroidal Dynamics\n\n  This paper investigates the problem of efficient computation of physically\nconsistent multi-contact behaviors. Recent work showed that under mild\nassumptions, the problem could be decomposed into simpler kinematic and\ncentroidal dynamic optimization problems. Based on this approach, we propose a\ngeneral convex relaxation of the centroidal dynamics leading to two\ncomputationally efficient algorithms based on iterative resolutions of second\norder cone programs. They optimize centroidal trajectories, contact forces and,\nimportantly, the timing of the motions. We include the approach in a\nkino-dynamic optimization method to generate full-body movements. Finally, the\napproach is embedded in a mixed-integer solver to further find dynamically\nconsistent contact sequences. Extensive numerical experiments demonstrate the\ncomputational efficiency of the approach, suggesting that it could be used in a\nfast receding horizon control loop. Executions of the planned motions on\nsimulated humanoids and quadrupeds and on a real quadruped robot further show\nthe quality of the optimized motions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.04515,regular,pre_llm,2020,11,"{'ai_likelihood': 1.0497040218777127e-05, 'text': ""SENSAR: A Visual Tool for Intelligent Robots for Collaborative\n  Human-Robot Interaction\n\n  Establishing common ground between an intelligent robot and a human requires\ncommunication of the robot's intention, behavior, and knowledge to the human to\nbuild trust and assure safety in a shared environment. This paper introduces\nSENSAR (Seeing Everything iN Situ with Augmented Reality), an augmented reality\nrobotic system that enables robots to communicate their sensory and cognitive\ndata in context over the real-world with rendered graphics, allowing a user to\nunderstand, correct, and validate the robot's perception of the world. Our\nsystem aims to support human-robot interaction research by establishing common\nground where the perceptions of the human and the robot align.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05767,regular,pre_llm,2020,11,"{'ai_likelihood': 4.371007283528646e-06, 'text': 'Simulating Autonomous Driving in Massive Mixed Urban Traffic\n\n  Autonomous driving in an unregulated urban crowd is an outstanding challenge,\nespecially, in the presence of many aggressive, high-speed traffic\nparticipants. This paper presents SUMMIT, a high-fidelity simulator that\nfacilitates the development and testing of crowd-driving algorithms. SUMMIT\nsimulates dense, unregulated urban traffic at any worldwide locations as\nsupported by the OpenStreetMap. The core of SUMMIT is a multi-agent motion\nmodel, GAMMA, that models the behaviours of heterogeneous traffic agents, and a\nreal-time POMDP planner, Context-POMDP, that serves as a driving expert. SUMMIT\nis built as an extension of CARLA and inherits from it the physical and visual\nrealism for autonomous driving simulation. SUMMIT supports a wide range of\napplications, including perception, vehicle control or planning, and end-to-end\nlearning. We validate the realism of our motion model using its traffic motion\nprediction accuracy on various real-world data sets. We also provide several\nreal-world benchmark scenarios to show that SUMMIT simulates complex, realistic\ntraffic behaviors, and Context-POMDP drives safely and efficiently in\nchallenging crowd-driving settings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00646,regular,pre_llm,2020,11,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'DRF: A Framework for High-Accuracy Autonomous Driving Vehicle Modeling\n\n  An accurate vehicle dynamic model is the key to bridge the gap between\nsimulation and real road test in autonomous driving. In this paper, we present\na Dynamic model-Residual correction model Framework (DRF) for vehicle dynamic\nmodeling. On top of any existing open-loop dynamic model, this framework builds\na Residual Correction Model (RCM) by integrating deep Neural Networks (NN) with\nSparse Variational Gaussian Process (SVGP) model. RCM takes a sequence of\nvehicle control commands and dynamic status for a certain time duration as\nmodeling inputs, extracts underlying context from this sequence with deep\nencoder networks, and predicts open-loop dynamic model prediction errors. Five\nvehicle dynamic models are derived from DRF via encoder variation. Our\ncontribution is consolidated by experiments on evaluation of absolute\ntrajectory error and similarity between DRF outputs and the ground truth.\nCompared to classic rule-based and learning-based vehicle dynamic models, DRF\naccomplishes as high as 74.12% to 85.02% of absolute trajectory error drop\namong all DRF variations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00712,regular,pre_llm,2020,11,"{'ai_likelihood': 0.0, 'text': 'Grasping in the Dark: Zero-Shot Object Grasping Using Tactile Feedback\n\n  Grasping and manipulating a wide variety of objects is a fundamental skill\nthat would determine the success and wide spread adaptation of robots in homes.\nSeveral end-effector designs for robust manipulation have been proposed but\nthey mostly work when provided with prior information about the objects or\nequipped with external sensors for estimating object shape or size. Such\napproaches are limited to many-shot or unknown objects and are prone to\nestimation errors from external estimation systems. We propose an approach to\ngrasp and manipulate previously unseen or zero-shot objects: the objects\nwithout any prior of their shape, size, material and weight properties, using\nonly feedback from tactile sensors which is contrary to the state-of-the-art.\nSuch an approach provides robust manipulation of objects either when the object\nmodel is not known or when it is estimated incorrectly from an external system.\nOur approach is inspired by the ideology of how animals or humans manipulate\nobjects, i.e., by using feedback from their skin. Our grasping and manipulation\nrevolves around the simple notion that objects slip if not grasped stably. This\nslippage can be detected and counteracted for a robust grasp that is agnostic\nto the type, shape, size, material and weight of the object. At the crux of our\napproach is a novel tactile feedback based controller that detects and\ncompensates for slip during grasp. We successfully evaluate and demonstrate our\nproposed approach on many real world experiments using the Shadow Dexterous\nHand equipped with BioTac SP tactile sensors for different object shapes,\nsizes, weights and materials. We obtain an overall success rate of 73.5%\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.04987,regular,pre_llm,2020,11,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'CircuitBot: Learning to Survive with Robotic Circuit Drawing\n\n  Robots with the ability to actively acquire power from surroundings will be\ngreatly beneficial for long-term autonomy, and to survive in dynamic, uncertain\nenvironments. In this work, a scenario is presented where a robot has limited\nenergy, and the only way to survive is to access the energy from a power\nsource. With no cables or wires available, the robot learns to construct an\nelectrical path and avoid potential obstacles during the connection. We present\nthis robot, capable of drawing connected circuit patterns with graphene-based\nconductive ink. A state-of-the-art Mix-Variable Bayesian Optimization is\nadopted to optimize the placement of conductive shapes to maximize the power\nthis robot receives. Our results show that, within a small number of trials,\nthe robot learns to build parallel circuits to maximize the voltage received\nand avoid obstacles which steal energy from the robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.04183,regular,pre_llm,2020,11,"{'ai_likelihood': 1.105997297498915e-05, 'text': 'EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System\n  in Cluttered Environments\n\n  This paper presents a decentralized and asynchronous systematic solution for\nmulti-robot autonomous navigation in unknown obstacle-rich scenes using merely\nonboard resources. The planning system is formulated under gradient-based local\nplanning framework, where collision avoidance is achieved by formulating the\ncollision risk as a penalty of a nonlinear optimization problem. In order to\nimprove robustness and escape local minima, we incorporate a lightweight\ntopological trajectory generation method. Then agents generate safe, smooth,\nand dynamically feasible trajectories in only several milliseconds using an\nunreliable trajectory sharing network. Relative localization drift among agents\nis corrected by using agent detection in depth images. Our method is\ndemonstrated in both simulation and real-world experiments. The source code is\nreleased for the reference of the community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.11836,regular,pre_llm,2020,11,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Stochastic Motion Planning under Partial Observability for Mobile Robots\n  with Continuous Range Measurements\n\n  In this paper, we address the problem of stochastic motion planning under\npartial observability, more specifically, how to navigate a mobile robot\nequipped with continuous range sensors such as LIDAR. In contrast to many\nexisting robotic motion planning methods, we explicitly consider the\nuncertainty of the robot state by modeling the system as a POMDP. Recent work\non general purpose POMDP solvers is typically limited to discrete observation\nspaces, and does not readily apply to the proposed problem due to the\ncontinuous measurements from LIDAR. In this work, we build upon an existing\nMonte Carlo Tree Search method, POMCP, and propose a new algorithm POMCP++. Our\nalgorithm can handle continuous observation spaces with a novel measurement\nselection strategy. The POMCP++ algorithm overcomes over-optimism in the value\nestimation of a rollout policy by removing the implicit perfect state\nassumption at the rollout phase. We validate POMCP++ in theory by proving it is\na Monte Carlo Tree Search algorithm. Through comparisons with other methods\nthat can also be applied to the proposed problem, we show that POMCP++ yields\nsignificantly higher success rate and total reward.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01899,regular,pre_llm,2020,11,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Learning Barrier Functions with Memory for Robust Safe Navigation\n\n  Control barrier functions are widely used to enforce safety properties in\nrobot motion planning and control. However, the problem of constructing barrier\nfunctions online and synthesizing safe controllers that can deal with the\nassociated uncertainty has received little attention. This paper investigates\nsafe navigation in unknown environments, using onboard range sensing to\nconstruct control barrier functions online. To represent different objects in\nthe environment, we use the distance measurements to train neural network\napproximations of the signed distance functions incrementally with replay\nmemory. This allows us to formulate a novel robust control barrier safety\nconstraint which takes into account the error in the estimated distance fields\nand its gradient. Our formulation leads to a second-order cone program,\nenabling safe and stable control synthesis in a priori unknown environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.09045,regular,pre_llm,2020,11,"{'ai_likelihood': 1.1424223581949871e-05, 'text': 'Double-Prong ConvLSTM for Spatiotemporal Occupancy Prediction in Dynamic\n  Environments\n\n  Predicting the future occupancy state of an environment is important to\nenable informed decisions for autonomous vehicles. Common challenges in\noccupancy prediction include vanishing dynamic objects and blurred predictions,\nespecially for long prediction horizons. In this work, we propose a\ndouble-prong neural network architecture to predict the spatiotemporal\nevolution of the occupancy state. One prong is dedicated to predicting how the\nstatic environment will be observed by the moving ego vehicle. The other prong\npredicts how the dynamic objects in the environment will move. Experiments\nconducted on the real-world Waymo Open Dataset indicate that the fused output\nof the two prongs is capable of retaining dynamic objects and reducing\nblurriness in the predictions for longer time horizons than baseline models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.05541,regular,pre_llm,2020,11,"{'ai_likelihood': 1.5762117173936633e-05, 'text': ""Learning Agile Locomotion Skills with a Mentor\n\n  Developing agile behaviors for legged robots remains a challenging problem.\nWhile deep reinforcement learning is a promising approach, learning truly agile\nbehaviors typically requires tedious reward shaping and careful curriculum\ndesign. We formulate agile locomotion as a multi-stage learning problem in\nwhich a mentor guides the agent throughout the training. The mentor is\noptimized to place a checkpoint to guide the movement of the robot's center of\nmass while the student (i.e. the robot) learns to reach these checkpoints. Once\nthe student can solve the task, we teach the student to perform the task\nwithout the mentor. We evaluate our proposed learning system with a simulated\nquadruped robot on a course consisting of randomly generated gaps and hurdles.\nOur method significantly outperforms a single-stage RL baseline without a\nmentor, and the quadruped robot can agilely run and jump across gaps and\nobstacles. Finally, we present a detailed analysis of the learned behaviors'\nfeasibility and efficiency.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.08424,regular,pre_llm,2020,11,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Deep Affordance Foresight: Planning Through What Can Be Done in the\n  Future\n\n  Planning in realistic environments requires searching in large planning\nspaces. Affordances are a powerful concept to simplify this search, because\nthey model what actions can be successful in a given situation. However, the\nclassical notion of affordance is not suitable for long horizon planning\nbecause it only informs the robot about the immediate outcome of actions\ninstead of what actions are best for achieving a long-term goal. In this paper,\nwe introduce a new affordance representation that enables the robot to reason\nabout the long-term effects of actions through modeling what actions are\nafforded in the future, thereby informing the robot the best actions to take\nnext to achieve a task goal. Based on the new representation, we develop a\nlearning-to-plan method, Deep Affordance Foresight (DAF), that learns partial\nenvironment models of affordances of parameterized motor skills through\ntrial-and-error. We evaluate DAF on two challenging manipulation domains and\nshow that it can effectively learn to carry out multi-step tasks, share learned\naffordance representations among different tasks, and learn to plan with\nhigh-dimensional image inputs. Additional material is available at\nhttps://sites.google.com/stanford.edu/daf\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01492,regular,pre_llm,2020,11,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Planning for Aerial Robot Teams for Wide-Area Biometric and Phenotypic\n  Data Collection\n\n  This work presents an efficient and implementable solution to the problem of\njoint task allocation and path planning in a multi-UAV platform deployed for\nbiometric data collection in-the-wild. The sensing requirement associated with\nthe task gives rise to an uncanny variant of the traditional vehicle routing\nproblem with coverage/sensing constraints. As is the case in several\nmulti-robot path-planning problems, our problem reduces to an $m$TSP problem.\nIn order to tame the computational challenges associated with the problem, we\npropose a hierarchical solution that decouples the vehicle routing problem from\nthe target allocation problem. As a tangible solution to the allocation\nproblem, we use a clustering-based technique that incorporates temporal\nuncertainty in the cardinality and position of the robots. Finally, we\nimplement the proposed techniques on our multi-quadcopter platforms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.00397,regular,pre_llm,2020,11,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'APPLR: Adaptive Planner Parameter Learning from Reinforcement\n\n  Classical navigation systems typically operate using a fixed set of\nhand-picked parameters (e.g. maximum speed, sampling rate, inflation radius,\netc.) and require heavy expert re-tuning in order to work in new environments.\nTo mitigate this requirement, it has been proposed to learn parameters for\ndifferent contexts in a new environment using human demonstrations collected\nvia teleoperation. However, learning from human demonstration limits deployment\nto the training environment, and limits overall performance to that of a\npotentially-suboptimal demonstrator. In this paper, we introduce APPLR,\nAdaptive Planner Parameter Learning from Reinforcement, which allows existing\nnavigation systems to adapt to new scenarios by using a parameter selection\nscheme discovered via reinforcement learning (RL) in a wide variety of\nsimulation environments. We evaluate APPLR on a robot in both simulated and\nphysical experiments, and show that it can outperform both a fixed set of\nhand-tuned parameters and also a dynamic parameter tuning scheme learned from\nhuman demonstration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01552,regular,pre_llm,2020,11,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Efficient Sampling of Transition Constraints for Motion Planning under\n  Sliding Contacts\n\n  Contact-based motion planning for manipulation, object exploration or\nbalancing often requires finding sequences of fixed and sliding contacts and\nplanning the transition from one contact in the environment to another.\nHowever, most existing algorithms concentrate on the control and learning\naspect of sliding contacts, but do not embed the problem into a principled\nframework to provide guarantees on completeness or optimality. To address this\nproblem, we propose a method to extend constraint-based planning using contact\ntransitions for sliding contacts. Such transitions are elementary operations\nrequired for whole contact sequences. To model sliding contacts, we define a\nsliding contact constraint that permits the robot to slide on the surface of a\nmesh-based object. To exploit transitions between sliding contacts, we develop\na contact transition sampler, which uses three constraint modes: contact with a\nstart surface, no contact and contact with a goal surface. We sample these\ntransition modes uniformly which makes them usable with sampling-based planning\nalgorithms. Our method is evaluated by testing it on manipulator arms of two,\nthree and seven internal degrees of freedom with different objects and various\nsampling-based planning algorithms. This demonstrates that sliding contact\nconstraints could be used as an elementary method for planning long-horizon\ncontact sequences for high-dimensional robotic systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.10743,regular,pre_llm,2020,11,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Semantic-Based VPS for Smartphone Localization in Challenging Urban\n  Environments\n\n  Accurate smartphone-based outdoor localization system in deep urban canyons\nare increasingly needed for various IoT applications such as augmented reality,\nintelligent transportation, etc. The recently developed feature-based visual\npositioning system (VPS) by Google detects edges from smartphone images to\nmatch with pre-surveyed edges in their map database. As smart cities develop,\nthe building information modeling (BIM) becomes widely available, which\nprovides an opportunity for a new semantic-based VPS. This article proposes a\nnovel 3D city model and semantic-based VPS for accurate and robust pose\nestimation in urban canyons where global navigation satellite system (GNSS)\ntends to fail. In the offline stage, a material segmented city model is used to\ngenerate segmented images. In the online stage, an image is taken with a\nsmartphone camera that provides textual information about the surrounding\nenvironment. The approach utilizes computer vision algorithms to rectify and\nhand segment between the different types of material identified in the\nsmartphone image. A semantic-based VPS method is then proposed to match the\nsegmented generated images with the segmented smartphone image. Each generated\nimage holds a pose that contains the latitude, longitude, altitude, yaw, pitch,\nand roll. The candidate with the maximum likelihood is regarded as the precise\npose of the user. The positioning results achieves 2.0m level accuracy in\ncommon high rise along street, 5.5m in foliage dense environment and 15.7m in\nalleyway. A 45% positioning improvement to current state-of-the-art method. The\nestimation of yaw achieves 2.3{\\deg} level accuracy, 8 times the improvement to\nsmartphone IMU.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.13596,regular,pre_llm,2020,11,"{'ai_likelihood': 1.771582497490777e-05, 'text': ""A Mixed Integer Linear Program For Human And Material Resources\n  Optimization In Emergency Department\n\n  The discrepancy between patient demand and the emergency departments (ED)\ncapacity, that mainly depends on human resources and on beds available for\npatients, often lead to ED's overcrowding and to the increase in waiting time.\nIn this paper, we focus on the optimization of the human (medical and\nparamedical staff) and material resources (beds) in the ED of the hospital\ncenter of Troyes, France (CHT). We seek to minimize the total number of waiting\npatients from their arrival to their discharge. We propose a mixed integer\nlinear program solved by a sample average approximation (SAA) approach. The\nprogram has been tested on a set of real data gathered from the ED information\nsystem. Numerical results show that the optimization of human and material\nresources leads to a decrease of total number of waiting patients.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.11951,regular,pre_llm,2020,11,"{'ai_likelihood': 2.228551440768772e-05, 'text': 'Path Planning with Automatic Seam Extraction over Point Cloud Models for\n  Robotic Arc Welding\n\n  This paper presents a point cloud based robotic system for arc welding. Using\nhand gesture controls, the system scans partial point cloud views of workpiece\nand reconstructs them into a complete 3D model by a linear iterative closest\npoint algorithm. Then, a bilateral filter is extended to denoise the workpiece\nmodel and preserve important geometrical information. To extract the welding\nseam from the model, a novel intensity-based algorithm is proposed that detects\nedge points and generates a smooth 6-DOF welding path. The methods are tested\non multiple workpieces with different joint types and poses. Experimental\nresults prove the robustness and efficiency of this robotic system on automatic\npath planning for welding applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.02135,regular,pre_llm,2020,11,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Planning to Chronicle\n\n  An important class of applications entails a robot monitoring, scrutinizing,\nor recording the evolution of an uncertain time-extended process. This sort of\nsituation leads an interesting family of planning problems in which the robot\nis limited in what it sees and must, thus, choose what to pay attention to. The\ndistinguishing characteristic of this setting is that the robot has influence\nover what it captures via its sensors, but exercises no causal authority over\nthe evolving process. As such, the robot's objective is to observe the\nunderlying process and to produce a `chronicle' of occurrent events, subject to\na goal specification of the sorts of event sequences that may be of interest.\nThis paper examines variants of such problems when the robot aims to collect\nsets of observations to meet a rich specification of their sequential\nstructure. We study this class of problems by modeling a stochastic process via\na variant of a hidden Markov model, and specify the event sequences of interest\nas a regular language, developing a vocabulary of `mutators' that enable\nsophisticated requirements to be expressed. Under different suppositions about\nthe information gleaned about the Markov model, we formulate and solve\ndifferent planning problems. The core underlying idea is the construction of a\nproduct between the event model and a specification automaton. The paper\nreports and compares performance metrics by drawing on some small case studies\nanalyzed in depth in simulation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.02662,regular,pre_llm,2020,11,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Generating Large-Scale Trajectories Efficiently using Double\n  Descriptions of Polynomials\n\n  For quadrotor trajectory planning, describing a polynomial trajectory through\ncoefficients and end-derivatives both enjoy their own convenience in energy\nminimization. We name them double descriptions of polynomial trajectories. The\ntransformation between them, causing most of the inefficiency and instability,\nis formally analyzed in this paper. Leveraging its analytic structure, we\ndesign a linear-complexity scheme for both jerk/snap minimization and parameter\ngradient evaluation, which possesses efficiency, stability, flexibility, and\nscalability. With the help of our scheme, generating an energy optimal (minimum\nsnap) trajectory only costs 1 $\\mu s$ per piece at the scale up to 1,000,000\npieces. Moreover, generating large-scale energy-time optimal trajectories is\nalso accelerated by an order of magnitude against conventional methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2011.01872,regular,pre_llm,2020,11,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Predicting Terrain Mechanical Properties in Sight for Planetary Rovers\n  with Semantic Clues\n\n  Non-geometric mobility hazards such as rover slippage and sinkage posing\ngreat challenges to costly planetary missions are closely related to the\nmechanical properties of terrain. In-situ proprioceptive processes for rovers\nto estimate terrain mechanical properties need to experience different slip as\nwell as sinkage and are helpless to untraversed regions. This paper proposes to\npredict terrain mechanical properties with vision in the distance, which\nexpands the sensing range to the whole view and can partly halt potential\nslippage and sinkage hazards in the planning stage. A semantic-based method is\ndesigned to predict bearing and shearing properties of terrain in two stages\nconnected with semantic clues. The former segmentation phase segments terrain\nwith a light-weighted network promising to be applied onboard with competitive\n93% accuracy and high recall rate over 96%, while the latter inference phase\npredicts terrain properties in a quantitative manner based on human-like\ninference principles. The prediction results in several test routes are 12.5%\nand 10.8% in full-scale error and help to plan appropriate strategies to avoid\nsuffering non-geometric hazards.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.04803,regular,pre_llm,2020,12,"{'ai_likelihood': 1.069572236802843e-05, 'text': 'GATSBI: An Online GTSP-Based Algorithm for Targeted Surface Bridge\n  Inspection\n\n  We study the problem of visual surface inspection of a bridge for defects\nusing an Unmanned Aerial Vehicle (UAV). We do not assume that the geometric\nmodel of the bridge is known beforehand. Our planner, termed GATSBI, plans a\npath in a receding horizon fashion to inspect all points on the surface of the\nbridge. The input to GATSBI consists of a 3D occupancy map created online with\nLiDAR scans. Occupied voxels corresponding to the bridge in this map are\nsemantically segmented and used to create a bridge-only occupancy map.\nInspecting a bridge voxel requires the UAV to take images from a desired\nviewing angle and distance. We then create a Generalized Traveling Salesperson\nProblem (GTSP) instance to cluster candidate viewpoints for inspecting the\nbridge voxels and use an off-the-shelf GTSP solver to find the optimal path for\nthe given instance. As the algorithm sees more parts of the environment over\ntime, it replans the path to inspect novel parts of the bridge while avoiding\nobstacles. We evaluate the performance of our algorithm through high-fidelity\nsimulations conducted in AirSim and real-world experiments. We compare the\nperformance of GATSBI with a classical exploration algorithm. Our evaluation\nreveals that targeting the inspection to only the segmented bridge voxels and\nplanning carefully using a GTSP solver leads to a more efficient and thorough\ninspection than the baseline algorithm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.0825,regular,pre_llm,2020,12,"{'ai_likelihood': 1.2814998626708984e-05, 'text': 'Rigid chain in parallel kinematic positioning system\n\n  The article presents an analysis of the trends in the development of\nkinematic structures of modern machine-building technological equipment. The\nprospects of using machines with parallel kinematics in processing, measuring\nand handling equipment, their advantages and disadvantages are demonstrated. It\nis shown that it is inexpedient to use ball screw drives in machines with\nparallel kinematics, performing tasks of low accuracy, but with displacements\nof more than 3000 mm. The rigid chain system of the Serapid firm and the\npossibility of its use in machines with parallel kinematics are considered. A\nschematic solution of a three-coordinate manipulation robot based on parallel\nkinematics with drive mechanisms on rigid chains is proposed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.10593,regular,pre_llm,2020,12,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Wheel-INS2: Multiple MEMS IMU-based Dead Reckoning System for Wheeled\n  Robots with Evaluation of Different IMU Configurations\n\n  A reliable self-contained navigation system is essential for autonomous\nvehicles. Based on our previous study on Wheel-INS \\cite{niu2019}, a\nwheel-mounted inertial measurement unit (Wheel-IMU)-based dead reckoning (DR)\nsystem, in this paper, we propose a multiple IMUs-based DR solution for the\nwheeled robots. The IMUs are mounted at different places of the wheeled\nvehicles to acquire various dynamic information. In particular, at least one\nIMU has to be mounted at the wheel to measure the wheel velocity and take\nadvantages of the rotation modulation. The system is implemented through a\ndistributed extended Kalman filter structure where each subsystem\n(corresponding to each IMU) retains and updates its own states separately. The\nrelative position constraints between the multiple IMUs are exploited to\nfurther limit the error drift and improve the system robustness. Particularly,\nwe present the DR systems using dual Wheel-IMUs, one Wheel-IMU plus one vehicle\nbody-mounted IMU (Body-IMU), and dual Wheel-IMUs plus one Body-IMU as examples\nfor analysis and comparison. Field tests illustrate that the proposed multi-IMU\nDR system outperforms the single Wheel-INS in terms of both positioning and\nheading accuracy. By comparing with the centralized filter, the proposed\ndistributed filter shows unimportant accuracy degradation while holds\nsignificant computation efficiency. Moreover, among the three multi-IMU\nconfigurations, the one Body-IMU plus one Wheel-IMU design obtains the minimum\ndrift rate. The position drift rates of the three configurations are 0.82\\%\n(dual Wheel-IMUs), 0.69\\% (one Body-IMU plus one Wheel-IMU), and 0.73\\% (dual\nWheel-IMUs plus one Body-IMU), respectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.10863,regular,pre_llm,2020,12,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Path Planning and Obstacle Avoidance Scheme for Autonomous Robots using\n  Raspberry Pi\n\n  With the incremental development of robotic platforms to automate the manual\nprocesses, path planning has become a critical domain with or without the\nknowledge of the indoor and outdoor environment. The algorithms can be\nintelligent or pre-structured and should optimally reach the destination\nefficiently. The major challenge in this domain is to find a path which is free\nfrom static obstacles as well as dynamic obstacles. In this paper, a\nmethodology is proposed with the implementation details of the robotic platform\nto cover the critical key points and to arrive at the original key point in a\ndynamic environment. The main computation is happening inside a Raspberry Pi B+\nmodule, and compass, wheel encoders and ultrasonic sensors were used in the\nimplementation for the localization of the robot to relevant key points.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.06413,regular,pre_llm,2020,12,"{'ai_likelihood': 1.986821492513021e-06, 'text': ""A Vision-based Sensing Approach for a Spherical Soft Robotic Arm\n\n  Sensory feedback is essential for the control of soft robotic systems and to\nenable deployment in a variety of different tasks. Proprioception refers to\nsensing the robot's own state and is of crucial importance in order to deploy\nsoft robotic systems outside of laboratory environments, i.e. where no external\nsensing, such as motion capture systems, is available.\n  A vision-based sensing approach for a soft robotic arm made from fabric is\npresented, leveraging the high-resolution sensory feedback provided by cameras.\nNo mechanical interaction between the sensor and the soft structure is required\nand consequently, the compliance of the soft system is preserved. The\nintegration of a camera into an inflatable, fabric-based bellow actuator is\ndiscussed. Three actuators, each featuring an integrated camera, are used to\ncontrol the spherical robotic arm and simultaneously provide sensory feedback\nof the two rotational degrees of freedom. A convolutional neural network\narchitecture predicts the two angles describing the robot's orientation from\nthe camera images. Ground truth data is provided by a motion capture system\nduring the training phase of the supervised learning approach and its\nevaluation thereafter.\n  The camera-based sensing approach is able to provide estimates of the\norientation in real-time with an accuracy of about one degree. The reliability\nof the sensing approach is demonstrated by using the sensory feedback to\ncontrol the orientation of the robotic arm in closed-loop.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.11131,regular,pre_llm,2020,12,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""Weight-Based Exploration for Unmanned Aerial Teams Searching for\n  Multiple Survivors\n\n  During floods, reaching survivors in the shortest possible time is a priority\nfor rescue teams. Given their ability to explore difficult terrain in short\nspans of time, Unmanned Aerial Vehicles (UAVs) have become an increasingly\nvaluable aid to search and rescue operations. Traditionally, UAVs utilize\nexhaustive lawnmower exploration patterns to locate stranded survivors, without\nany information regarding the survivor's whereabouts. In real life disaster\nscenarios however, on-ground observers provide valuable information to the\nrescue effort, such as the survivor's last known location and heading. In\nearlier work, a Weight Based Exploration (WBE) model, which utilizes this\ninformation to generate a prioritized list of waypoints to aid the UAV in its\nsearch mission, was proposed. This approach was shown to be effective for a\nsingle UAV locating a single survivor. In this paper, we extend the WBE model\nto a team of UAVs locating multiple survivors. The model initially partitions\nthe search environment amongst the UAVs using Voronoi cells. The UAVs then\nutilize the WBE model to locate survivors in their partitions. We test this\nmodel with varying survivor locations and headings. We demonstrate the\nscalability of the model developed by testing the model with aerial teams\ncomprising several UAVs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.08892,regular,pre_llm,2020,12,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""E$ \\mathbf{^3} $MoP: Efficient Motion Planning Based on Heuristic-Guided\n  Motion Primitives Pruning and Path Optimization With Sparse-Banded Structure\n\n  To solve the autonomous navigation problem in complex environments, an\nefficient motion planning approach is newly presented in this paper.\nConsidering the challenges from large-scale, partially unknown complex\nenvironments, a three-layer motion planning framework is elaborately designed,\nincluding global path planning, local path optimization, and time-optimal\nvelocity planning. Compared with existing approaches, the novelty of this work\nis twofold: 1) a novel heuristic-guided pruning strategy of motion primitives\nis proposed and fully integrated into the state lattice-based global path\nplanner to further improve the computational efficiency of graph search, and 2)\na new soft-constrained local path optimization approach is proposed, wherein\nthe sparse-banded system structure of the underlying optimization problem is\nfully exploited to efficiently solve the problem. We validate the safety,\nsmoothness, flexibility, and efficiency of our approach in various complex\nsimulation scenarios and challenging real-world tasks. It is shown that the\ncomputational efficiency is improved by 66.21% in the global planning stage and\nthe motion efficiency of the robot is improved by 22.87% compared with the\nrecent quintic B\\'{e}zier curve-based state space sampling approach. We name\nthe proposed motion planning framework E$ \\mathrm{^3} $MoP, where the number 3\nnot only means our approach is a three-layer framework but also means the\nproposed approach is efficient in three stages.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.12438,review,pre_llm,2020,12,"{'ai_likelihood': 8.245309193929037e-06, 'text': 'State of the Art of Adaptive Cruise Control and Stop and Go Systems\n\n  This paper presents the state of the art of Adaptive Cruise Control (ACC) and\nStop and Go systems as well as Intelligent Transportation Systems enhanced with\ninter vehicle communication. The sensors used in these systems and the level of\ntheir current technology are introduced. Simulators related to ACC and Stop and\nGo (S&G) systems are also surveyed and the MEKAR simulator is presented.\nFinally, future trends of ACC and Stop and Go systems and their advantages are\nemphasized.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.14447,regular,pre_llm,2020,12,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""LOCUS: A Multi-Sensor Lidar-Centric Solution for High-Precision Odometry\n  and 3D Mapping in Real-Time\n\n  A reliable odometry source is a prerequisite to enable complex autonomy\nbehaviour in next-generation robots operating in extreme environments. In this\nwork, we present a high-precision lidar odometry system to achieve robust and\nreal-time operation under challenging perceptual conditions. LOCUS (Lidar\nOdometry for Consistent operation in Uncertain Settings), provides an accurate\nmulti-stage scan matching unit equipped with an health-aware sensor integration\nmodule for seamless fusion of additional sensing modalities. We evaluate the\nperformance of the proposed system against state-of-the-art techniques in\nperceptually challenging environments, and demonstrate top-class localization\naccuracy along with substantial improvements in robustness to sensor failures.\nWe then demonstrate real-time performance of LOCUS on various types of robotic\nmobility platforms involved in the autonomous exploration of the Satsop power\nplant in Elma, WA where the proposed system was a key element of the CoSTAR\nteam's solution that won first place in the Urban Circuit of the DARPA\nSubterranean Challenge.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.05946,regular,pre_llm,2020,12,"{'ai_likelihood': 3.741847144232856e-06, 'text': 'Autonomous Cooperative Wall Building by a Team of Unmanned Aerial\n  Vehicles in the MBZIRC 2020 Competition\n\n  This paper presents a system for autonomous cooperative wall building with a\nteam of Unmanned Aerial Vehicles (UAVs). The system was developed for Challenge\n2 of the Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2020. The\nwall-building scenario of Challenge 2 featured an initial stack of bricks and\nwall structure where the individual bricks had to be placed by a team of three\nUAVs. The objective of the task was to maximize collected points for placing\nthe bricks within the restricted construction time while following the\nprescribed wall pattern. The proposed approach uses initial scanning to find a\npriori unknown locations of the bricks and the wall structure. Each UAV is then\nassigned to individual bricks and wall placing locations and further perform\ngrasping and placement using onboard resources only. The developed system\nconsists of methods for scanning a given area, RGB-D detection of bricks and\nwall placement locations, precise grasping and placing of bricks, and\ncoordination of multiple UAVs. The paper describes the overall system,\nindividual components, experimental verification in demanding outdoor\nconditions, the achieved results in the competition, and lessons learned. The\npresented CTU-UPenn-NYU approach achieved the overall best performance among\nall participants to won the MBZIRC competition by collecting the highest number\nof points by correct placement of a high number of bricks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.1049,regular,pre_llm,2020,12,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Perception-Based Temporal Logic Planning in Uncertain Semantic Maps\n\n  This paper addresses a multi-robot planning problem in environments with\npartially unknown semantics. The environment is assumed to have known geometric\nstructure (e.g., walls) and to be occupied by static labeled landmarks with\nuncertain positions and classes. This modeling approach gives rise to an\nuncertain semantic map generated by semantic SLAM algorithms. Our goal is to\ndesign control policies for robots equipped with noisy perception systems so\nthat they can accomplish collaborative tasks captured by global temporal logic\nspecifications. To specify missions that account for environmental and\nperceptual uncertainty, we employ a fragment of Linear Temporal Logic (LTL),\ncalled co-safe LTL, defined over perception-based atomic predicates modeling\nprobabilistic satisfaction requirements. The perception-based LTL planning\nproblem gives rise to an optimal control problem, solved by a novel\nsampling-based algorithm, that generates open-loop control policies that are\nupdated online to adapt to a continuously learned semantic map. We provide\nextensive experiments to demonstrate the efficiency of the proposed planning\narchitecture.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.13032,review,pre_llm,2020,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Mesh Based Analysis of Low Fractal Dimension Reinforcement Learning\n  Policies\n\n  In previous work, using a process we call meshing, the reachable state spaces\nfor various continuous and hybrid systems were approximated as a discrete set\nof states which can then be synthesized into a Markov chain. One of the\napplications for this approach has been to analyze locomotion policies obtained\nby reinforcement learning, in a step towards making empirical guarantees about\nthe stability properties of the resulting system. In a separate line of\nresearch, we introduced a modified reward function for on-policy reinforcement\nlearning algorithms that utilizes a ""fractal dimension"" of rollout\ntrajectories. This reward was shown to encourage policies that induce\nindividual trajectories which can be more compactly represented as a discrete\nmesh. In this work we combine these two threads of research by building meshes\nof the reachable state space of a system subject to disturbances and controlled\nby policies obtained with the modified reward. Our analysis shows that the\nmodified policies do produce much smaller reachable meshes. This shows that\nagents trained with the fractal dimension reward transfer their desirable\nquality of having a more compact state space to a setting with external\ndisturbances. The results also suggest that the previous work using mesh based\ntools to analyze RL policies may be extended to higher dimensional systems or\nto higher resolution meshes than would have otherwise been possible.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.04855,regular,pre_llm,2020,12,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Reconstruction of Backbone Curves for Snake Robots\n\n  Snake robots composed of alternating single-axis pitch and yaw joints have\nmany internal degrees of freedom, which make them capable of versatile\nthree-dimensional locomotion. In motion planning process, snake robot motions\nare often designed kinematically by a chronological sequence of continuous\nbackbone curves that capture desired macroscopic shapes of the robot. However,\nas the geometric arrangement of single-axis rotary joints creates constraints\non the rotations in the robot, it is challenging for the robot to reconstruct\nan arbitrary 3D curve. When the robot configuration does not accurately achieve\nthe desired shapes defined by these backbone curves, the robot can have\nunexpected contacts with the environment, such that the robot does not achieve\nthe desired motion. In this work, we propose a method for snake robots to\nreconstruct desired backbone curves by posing an optimization problem that\nexploits the robot's geometric structure. We verified that our method enables\nfast and accurate curve-configuration conversions through its applications to\ncommonly used 3D gaits. We also demonstrated via robot experiments that 1) our\nmethod results in smooth locomotion on the robot; 2) our method allows the\nrobot to approach the numerically predicted locomotive performance of a\nsequence of continuous backbone curve.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.04511,regular,pre_llm,2020,12,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""Emotive Response to a Hybrid-Face Robot and Translation to Consumer\n  Social Robots\n\n  We introduce the conceptual formulation, design, fabrication, control and\ncommercial translation with IoT connection of a hybrid-face social robot and\nvalidation of human emotional response to its affective interactions. The\nhybrid-face robot integrates a 3D printed faceplate and a digital display to\nsimplify conveyance of complex facial movements while providing the impression\nof three-dimensional depth for natural interaction. We map the space of\npotential emotions of the robot to specific facial feature parameters and\ncharacterise the recognisability of the humanoid hybrid-face robot's archetypal\nfacial expressions. We introduce pupil dilation as an additional degree of\nfreedom for conveyance of emotive states. Human interaction experiments\ndemonstrate the ability to effectively convey emotion from the hybrid-robot\nface to human observers by mapping their neurophysiological\nelectroencephalography (EEG) response to perceived emotional information and\nthrough interviews. Results show main hybrid-face robotic expressions can be\ndiscriminated with recognition rates above 80% and invoke human emotive\nresponse similar to that of actual human faces as measured by the face-specific\nN170 event-related potentials in EEG. The hybrid-face robot concept has been\nmodified, implemented, and released in the commercial IoT robotic platform Miko\n(My Companion), an affective robot with facial and conversational features\ncurrently in use for human-robot interaction in children by Emotix Inc. We\ndemonstrate that human EEG responses to Miko emotions are comparative to\nneurophysiological responses for actual human facial recognition. Finally,\ninterviews show above 90% expression recognition rates in our commercial robot.\nWe conclude that simplified hybrid-face abstraction conveys emotions\neffectively and enhances human-robot interaction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.03555,regular,pre_llm,2020,12,"{'ai_likelihood': 6.225374009874132e-06, 'text': 'Improving Makespan in Dynamic Task Scheduling for Cloud Robotic Systems\n  with Time Window Constraints\n\n  A scheduling method in a robotic network cloud system with minimal makespan\nis beneficial as the system can complete all the tasks assigned to it in the\nfastest way. Robotic network cloud systems can be translated into graphs where\nnodes represent hardware with independent computing power and edges represent\ndata transmissions between nodes. Time window constraints on tasks are a\nnatural way to order tasks. The makespan is the maximum amount of time between\nwhen the first node to receive a task starts executing its first scheduled task\nand when all nodes have completed their last scheduled task. Load balancing\nallocation and scheduling ensures that the time between when the first node\ncompletes its scheduled tasks and when all other nodes complete their scheduled\ntasks is as short as possible. We propose a grid of all tasks to ensure that\nthe time window constraints for tasks are met. We propose grid of all tasks\nbalancing algorithm for distributing and scheduling tasks with minimum\nmakespan. We theoretically prove the correctness of the proposed algorithm and\npresent simulations illustrating the obtained results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.02271,regular,pre_llm,2020,12,"{'ai_likelihood': 6.192260318332249e-06, 'text': 'LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain\n  Environment\n\n  Mobile robots are often tasked with repeatedly navigating through an\nenvironment whose traversability changes over time. These changes may exhibit\nsome hidden structure, which can be learned. Many studies consider reactive\nalgorithms for online planning, however, these algorithms do not take advantage\nof the past executions of the navigation task for future tasks. In this paper,\nwe formalize the problem of minimizing the total expected cost to perform\nmultiple start-to-goal navigation tasks on a roadmap by introducing the Learned\nReactive Planning Problem. We propose a method that captures information from\npast executions to learn a motion policy to handle obstacles that the robot has\nseen before. We propose the LAMP framework, which integrates the generated\nmotion policy with an existing navigation stack. Finally, an extensive set of\nexperiments in simulated and real-world environments show that the proposed\nmethod outperforms the state-of-the-art algorithms by 10% to 40% in terms of\nexpected time to travel from start to goal. We also evaluate the robustness of\nthe proposed method in the presence of localization and mapping errors on a\nreal robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.04832,regular,pre_llm,2020,12,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Proactive Interaction Framework for Intelligent Social Receptionist\n  Robots\n\n  Proactive human-robot interaction (HRI) allows the receptionist robots to\nactively greet people and offer services based on vision, which has been found\nto improve acceptability and customer satisfaction. Existing approaches are\neither based on multi-stage decision processes or based on end-to-end decision\nmodels. However, the rule-based approaches require sedulous expert efforts and\nonly handle minimal pre-defined scenarios. On the other hand, existing works\nwith end-to-end models are limited to very general greetings or few behavior\npatterns (typically less than 10). To address those challenges, we propose a\nnew end-to-end framework, the TransFormer with Visual Tokens for Human-Robot\nInteraction (TFVT-HRI). The proposed framework extracts visual tokens of\nrelative objects from an RGB camera first. To ensure the correct interpretation\nof the scenario, a transformer decision model is then employed to process the\nvisual tokens, which is augmented with the temporal and spatial information. It\npredicts the appropriate action to take in each scenario and identifies the\nright target. Our data is collected from an in-service receptionist robot in an\noffice building, which is then annotated by experts for appropriate proactive\nbehavior. The action set includes 1000+ diverse patterns by combining language,\nemoji expression, and body motions. We compare our model with other SOTA\nend-to-end models on both offline test sets and online user experiments in\nrealistic office building environments to validate this framework. It is\ndemonstrated that the decision model achieves SOTA performance in action\ntriggering and selection, resulting in more humanness and intelligence when\ncompared with the previous reactive reception policies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.15008,regular,pre_llm,2020,12,"{'ai_likelihood': 8.377763960096572e-06, 'text': 'ALVIO: Adaptive Line and Point Feature-based Visual Inertial Odometry\n  for Robust Localization in Indoor Environments\n\n  The amount of texture can be rich or deficient depending on the objects and\nthe structures of the building. The conventional mono visual-initial navigation\nsystem (VINS)-based localization techniques perform well in environments where\nstable features are guaranteed. However, their performance is not assured in a\nchanging indoor environment. As a solution to this, we propose Adaptive Line\nand point feature-based Visual Inertial Odometry (ALVIO) in this paper. ALVIO\nactively exploits the geometrical information of lines that exist in abundance\nin an indoor space. By using a strong line tracker and adaptive selection of\nfeature-based tightly coupled optimization, it is possible to perform robust\nlocalization in a variable texture environment. The structural characteristics\nof ALVIO are as follows: First, the proposed optical flow-based line tracker\nperforms robust line feature tracking and management. By using epipolar\ngeometry and trigonometry, accurate 3D lines are recovered. These 3D lines are\nused to calculate the line re-projection error. Finally, with the\nsensitivity-analysis-based adaptive feature selection in the optimization\nprocess, we can estimate the pose robustly in various indoor environments. We\nvalidate the performance of our system on public datasets and compare it\nagainst other state-of the-art algorithms (S-MSKCF, VINS-Mono). In the proposed\nalgorithm based on point and line feature selection, translation RMSE increased\nby 16.06% compared to VINS-Mono, while total optimization time decreased by up\nto 49.31%. Through this, we proved that it is a useful algorithm as a real-time\npose estimation algorithm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.11951,review,pre_llm,2020,12,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Robotic Process Automation -- A Systematic Literature Review and\n  Assessment Framework\n\n  Robotic Process Automation (RPA) is the automation of rule-based routine\nprocesses to increase efficiency and to reduce costs. Due to the utmost\nimportance of process automation in industry, RPA attracts increasing attention\nin the scientific field as well. This paper presents the state-of-the-art in\nthe RPA field by means of a Systematic Literature Review (SLR). In this SLR, 63\npublications are identified, categorised, and analysed along well-defined\nresearch questions. From the SLR findings, moreover, a framework for\nsystematically analysing, assessing, and comparing existing as well as upcoming\nRPA works is derived. The discovered thematic clusters advise further\ninvestigations in order to develop an even more detailed structural research\napproach for RPA.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2012.1498,regular,pre_llm,2020,12,"{'ai_likelihood': 1.8046961890326606e-05, 'text': 'Perimeter-defense Game between Aerial Defender and Ground Intruder\n\n  We study a variant of pursuit-evasion game in the context of perimeter\ndefense. In this problem, the intruder aims to reach the base plane of a\nhemisphere without being captured by the defender, while the defender tries to\ncapture the intruder. The perimeter-defense game was previously studied under\nthe assumption that the defender moves on a circle. We extend the problem to\nthe case where the defender moves on a hemisphere. To solve this problem, we\nanalyze the strategies based on the breaching point at which the intruder tries\nto reach the target and predict the goal position, defined as optimal breaching\npoint, that is achieved by the optimal strategies on both players. We provide\nthe barrier that divides the state space into defender-winning and\nintruder-winning regions and prove that the optimal strategies for both players\nare to move towards the optimal breaching point. Simulation results are\npresented to demonstrate that the optimality of the game is given as a Nash\nequilibrium.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.12581,regular,pre_llm,2021,1,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Automated decontamination of workspaces using UVC coupled with occupancy\n  detection\n\n  Periodic disinfection of workspaces can reduce SARS-CoV-2 transmission. In\nmany buildings periodic disinfection is performed manually; this has several\ndisadvantages: it is expensive, limited in the number of times it can be done\nover a day, and poses an increased risk to the workers performing the task. To\nsolve these problems, we developed an automated decontamination system that\nuses ultraviolet C (UVC) radiation for disinfection, coupled with occupancy\ndetection for its safe operation. UVC irradiation is a well-established\ntechnology for the deactivation of a wide range of pathogens. Our proposed\nsystem can deactivate pathogens both on surfaces and in the air. The coupling\nwith occupancy detection ensures that occupants are never directly exposed to\nUVC lights and their potential harmful effects. To help the wider community, we\nhave shared our complete work as an open-source repository, to be used under\nGPL v3.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.05325,regular,pre_llm,2021,1,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Learning Kinematic Feasibility for Mobile Manipulation through Deep\n  Reinforcement Learning\n\n  Mobile manipulation tasks remain one of the critical challenges for the\nwidespread adoption of autonomous robots in both service and industrial\nscenarios. While planning approaches are good at generating feasible whole-body\nrobot trajectories, they struggle with dynamic environments as well as the\nincorporation of constraints given by the task and the environment. On the\nother hand, dynamic motion models in the action space struggle with generating\nkinematically feasible trajectories for mobile manipulation actions. We propose\na deep reinforcement learning approach to learn feasible dynamic motions for a\nmobile base while the end-effector follows a trajectory in task space generated\nby an arbitrary system to fulfill the task at hand. This modular formulation\nhas several benefits: it enables us to readily transform a broad range of\nend-effector motions into mobile applications, it allows us to use the\nkinematic feasibility of the end-effector trajectory as a dense reward signal\nand its modular formulation allows it to generalise to unseen end-effector\nmotions at test time. We demonstrate the capabilities of our approach on\nmultiple mobile robot platforms with different kinematic abilities and\ndifferent types of wheeled platforms in extensive simulated as well as\nreal-world experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.0029,regular,pre_llm,2021,1,"{'ai_likelihood': 2.4504131740993925e-06, 'text': ""Robot Adaptation for Generating Consistent Navigational Behaviors over\n  Unstructured Off-Road Terrain\n\n  Terrain adaptation is an essential capability for a ground robot to\neffectively traverse unstructured off-road terrain in real-world field\nenvironments such as forests. However, the expected robot behaviors generated\nby terrain adaptation methods cannot always be executed accurately due to\nsetbacks such as wheel slip and reduced tire pressure. To address this problem,\nwe propose a novel approach for consistent behavior generation that enables the\nground robot's actual behaviors to more accurately match expected behaviors\nwhile adapting to a variety of unstructured off-road terrain. Our approach\nlearns offset behaviors that are used to compensate for the inconsistency\nbetween the actual and expected behaviors without requiring the explicit\nmodeling of various setbacks. Our approach is also able to estimate the\nimportance of the multi-modal features to improve terrain representations for\nbetter adaptation. In addition, we develop an algorithmic solver for our\nformulated regularized optimization problem, which is guaranteed to converge to\nthe global optimal solution. To evaluate the method, we perform extensive\nexperiments using various unstructured off-road terrain in real-world field\nenvironments. Experimental results have validated that our approach enables\nrobots to traverse complex unstructured off-road terrain with more navigational\nbehavior consistency, and it outperforms previous methods, particularly so on\nchallenging terrain.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.06483,regular,pre_llm,2021,1,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'From hand to brain and back: Grip forces deliver insight into the\n  functional plasticity of somatosensory processes\n\n  The human somatosensory cortex is intimately linked to other central brain\nfunctions such as vision, audition, mechanoreception, and motor planning and\ncontrol. These links are established through brain learning, and display a\nconsiderable functional plasticity. This latter fulfills an important adaptive\nrole and ensures, for example, that humans are able to reliably manipulate and\ncontrol objects in the physical world under constantly changing conditions in\ntheir immediate sensory environment. Variations in human grip force are a\ndirect reflection of this specific kind of functional plasticity. Data from\npreliminary experiments where wearable wireless sensor technology (sensor\ngloves) was exploited to measure human grip force variations under varying\nsensory input conditions (eyes open or shut, soft music or hard music during\ngripping) are discussed here to show the extent to which grip force sensing\npermits quantifying somatosensory brain interactions and their functional\nplasticity. Experiments to take this preliminary work further are suggested.\nImplications for robotics, in particular the development of end-effector robots\nfor upper limb movement planning and control, are brought forward.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.0638,regular,pre_llm,2021,1,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Reliable GNSS Localization Against Multiple Faults Using a Particle\n  Filter Framework\n\n  For reliable operation on urban roads, navigation using the Global Navigation\nSatellite System (GNSS) requires both accurately estimating the positioning\ndetail from GNSS pseudorange measurements and determining when the estimated\nposition is safe to use, or available. However, multiple GNSS measurements in\nurban environments contain biases, or faults, due to signal reflection and\nblockage from nearby buildings which are difficult to mitigate for estimating\nthe position and availability. This paper proposes a novel particle\nfilter-based framework that employs a Gaussian Mixture Model (GMM) likelihood\nof GNSS measurements to robustly estimate the position of a navigating vehicle\nunder multiple measurement faults. Using the probability distribution tracked\nby the filter and the designed GMM likelihood, we measure the accuracy and the\nrisk associated with localization and determine the availability of the\nnavigation system at each time instant. Through experiments conducted on\nchallenging simulated and real urban driving scenarios, we show that our method\nachieves small horizontal positioning errors compared to existing filter-based\nstate estimation techniques when multiple GNSS measurements contain faults.\nFurthermore, we verify using several simulations that our method determines\nsystem availability with smaller probability of false alarms and integrity risk\nthan the existing particle filter-based integrity monitoring approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.00205,regular,pre_llm,2021,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'A self-supervised learning-based 6-DOF grasp planning method for\n  manipulator\n\n  To realize a robust robotic grasping system for unknown objects in an\nunstructured environment, large amounts of grasp data and 3D model data for the\nobject are required, the sizes of which directly affect the rate of successful\ngrasps. To reduce the time cost of data acquisition and labeling and increase\nthe rate of successful grasps, we developed a self-supervised learning\nmechanism to control grasp tasks performed by manipulators. First, a\nmanipulator automatically collects the point cloud for the objects from\nmultiple perspectives to increase the efficiency of data acquisition. The\ncomplete point cloud for the objects is obtained by utilizing the hand-eye\nvision of the manipulator, and the TSDF algorithm. Then, the point cloud data\nfor the objects is used to generate a series of six-degrees-of-freedom grasp\nposes, and the force-closure decision algorithm is used to add the grasp\nquality label to each grasp pose to realize the automatic labeling of grasp\ndata. Finally, the point cloud in the gripper closing area corresponding to\neach grasp pose is obtained; it is then used to train the grasp-quality\nclassification model for the manipulator. The results of data acquisition\nexperiments demonstrate that the proposed method allows high-quality data to be\nobtained. The simulated results prove the effectiveness of the proposed\ngrasp-data acquisition method. The results of performing actual grasping\nexperiments demonstrate that the proposed self-supervised learning method can\nincrease the rate of successful grasps for the manipulator.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.08183,regular,pre_llm,2021,1,"{'ai_likelihood': 1.72522332933214e-05, 'text': ""Mask-GD Segmentation Based Robotic Grasp Detection\n\n  The reliability of grasp detection for target objects in complex scenes is a\nchallenging task and a critical problem that needs to be solved urgently in\npractical application. At present, the grasp detection location comes from\nsearching the feature space of the whole image. However, the cluttered\nbackground information in the image impairs the accuracy of grasping detection.\nIn this paper, a robotic grasp detection algorithm named MASK-GD is proposed,\nwhich provides a feasible solution to this problem. MASK is a segmented image\nthat only contains the pixels of the target object. MASK-GD for grasp detection\nonly uses MASK features rather than the features of the entire image in the\nscene. It has two stages: the first stage is to provide the MASK of the target\nobject as the input image, and the second stage is a grasp detector based on\nthe MASK feature. Experimental results demonstrate that MASK-GD's performance\nis comparable with state-of-the-art grasp detection algorithms on Cornell\nDatasets and Jacquard Dataset. In the meantime, MASK-GD performs much better in\ncomplex scenes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.00741,regular,pre_llm,2021,1,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'SmartArm: Suturing Feasibility of a Surgical Robotic System on a\n  Neonatal Chest Model\n\n  Commercially available surgical-robot technology currently addresses many\nsurgical scenarios for adult patients. This same technology cannot be used to\nthe benefit of neonate patients given the considerably smaller workspace.\nMedically relevant procedures regarding neonate patients include minimally\ninvasive surgery to repair congenital esophagus disorders, which entail the\nsuturing of the fragile esophagus within the narrow neonate cavity. In this\nwork, we explore the use of the SmartArm robotic system in a feasibility study\nusing a neonate chest and esophagus model. We show that a medically\ninexperienced operator can perform two-throw knots inside the neonate chest\nmodel using the robotic system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.07679,regular,pre_llm,2021,1,"{'ai_likelihood': 3.675619761149089e-06, 'text': 'The Six Hug Commandments: Design and Evaluation of a Human-Sized Hugging\n  Robot with Visual and Haptic Perception\n\n  Receiving a hug is one of the best ways to feel socially supported, and the\nlack of social touch can have severe negative effects on an individual\'s\nwell-being. Based on previous research both within and outside of HRI, we\npropose six tenets (""commandments"") of natural and enjoyable robotic hugging: a\nhugging robot should be soft, be warm, be human sized, visually perceive its\nuser, adjust its embrace to the user\'s size and position, and reliably release\nwhen the user wants to end the hug. Prior work validated the first two tenets,\nand the final four are new. We followed all six tenets to create a new robotic\nplatform, HuggieBot 2.0, that has a soft, warm, inflated body (HuggieChest) and\nuses visual and haptic sensing to deliver closed-loop hugging. We first\nverified the outward appeal of this platform in comparison to the previous\nPR2-based HuggieBot 1.0 via an online video-watching study involving 117 users.\nWe then conducted an in-person experiment in which 32 users each exchanged\neight hugs with HuggieBot 2.0, experiencing all combinations of visual hug\ninitiation, haptic sizing, and haptic releasing. The results show that adding\nhaptic reactivity definitively improves user perception a hugging robot,\nlargely verifying our four new tenets and illuminating several interesting\nopportunities for further improvement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.0327,regular,pre_llm,2021,1,"{'ai_likelihood': 1.609325408935547e-05, 'text': 'Investigation by Driving Simulation of Tractor Overturning Accidents\n  Caused by Steering Instability\n\n  Overturning tractors are the leading cause of fatalities on farms. Steering\ninstability contributes significantly to the tractor overturning. This study\ninvestigated tractor overturning accidents caused by the steering instability\nusing a driving simulator. The general commercial driving simulator CarSim\n(Mechanical Simulation Cooperation, MI, USA) was used. Tractor operations on\nsteep passage slopes were simulated to mimic conditions present for a real\naccident case reported in Japan. Simulations were performed on roads with and\nwithout slopes. The tractor overturned only when on the road with the steep\nslope. The decrease in the vertical force on the front wheel caused the\nsteering instability and the tractor to overturn. The steering instability\ncaused understeer which prevents the operator from being able to control the\ntractor properly. Subsequently, the tractor overturned in the simulation. The\ntractor driving simulator was capable of reproducing the steering instability\nwhich can lead to the overturning accident.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.0366,regular,pre_llm,2021,1,"{'ai_likelihood': 3.344482845730252e-06, 'text': ""Aligning Robot's Behaviours and Users' Perceptions Through Participatory\n  Prototyping\n\n  Robots are increasingly being deployed in public spaces. However, the general\npopulation rarely has the opportunity to nominate what they would prefer or\nexpect a robot to do in these contexts. Since most people have little or no\nexperience interacting with a robot, it is not surprising that robots deployed\nin the real world may fail to gain acceptance or engage their intended users.\nTo address this issue, we examine users' understanding of robots in public\nspaces and their expectations of appropriate uses of robots in these spaces.\nFurthermore, we investigate how these perceptions and expectations change as\nusers engage and interact with a robot. To support this goal, we conducted a\nparticipatory design workshop in which participants were actively involved in\nthe prototyping and testing of a robot's behaviours in simulation and on the\nphysical robot. Our work highlights how social and interaction contexts\ninfluence users' perception of robots in public spaces and how users' design\nand understanding of what are appropriate robot behaviors shifts as they\nobserve the enactment of their designs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.11597,regular,pre_llm,2021,1,"{'ai_likelihood': 7.251898447672526e-06, 'text': 'Dexterous Manipulation Primitives for the Real Robot Challenge\n\n  This report describes our approach for Phase 3 of the Real Robot Challenge.\nTo solve cuboid manipulation tasks of varying difficulty, we decompose each\ntask into the following primitives: moving the fingers to the cuboid to grasp\nit, turning it on the table to minimize orientation error, and re-positioning\nit to the goal position. We use model-based trajectory optimization and control\nto plan and execute these primitives. These grasping, turning, and\nre-positioning primitives are sequenced with a state-machine that determines\nwhich primitive to execute given the current object state and goal. Our method\nshows robust performance over multiple runs with randomized initial and goal\npositions. With this approach, our team placed second in the challenge, under\nthe anonymous name ""sombertortoise"" on the leaderboard. Example runs of our\nmethod solving each of the four levels can be seen in this video\n(https://www.youtube.com/watch?v=I65Kwu9PGmg&list=PLt9QxrtaftrHGXcp4Oh8-s_OnQnBnLtei&index=1).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.02965,regular,pre_llm,2021,1,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Geometry Aware NMPC Scheme for Morphing Quadrotor Navigation in\n  Restricted Entrances\n\n  Geometry-morphing Micro Aerial Vehicles (MAVs) are gaining more and more\nattention lately, since their ability to modify their geometric morphology\nin-flight increases their versatility, while expanding their application range.\nIn this novel research field, most of the works focus on the platform design\nand on the low-level control part for maintaining stability after the\ndeformation. Nevertheless, another aspect of geometry morphing MAVs is the\nassociation of the deformation with respect to the shape and structure of the\nenvironment. In this article, we propose a novel Nonlinear Model Predictive\nControl (NMPC) structure that modifies the morphology of a quadrotor based on\nthe environmental entrances geometrical shape. The proposed method considers\nrestricted entrances as a constraint in the NMPC and modifies the arm\nconfiguration of the MAV to provide a collision free path from the initial\nposition to the desired goal, while passing through the entrance. To the\nauthors' best knowledge, this work is the first to connect the in-flight\nmorphology with the characteristics of environmental shapes. Multiple\nsimulation results depict the performance and efficiency of the proposed scheme\nin scenarios where the quadrotor is commanded to pass through restricted areas.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.03834,regular,pre_llm,2021,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Closing the Planning-Learning Loop with Application to Autonomous\n  Driving\n\n  Real-time planning under uncertainty is critical for robots operating in\ncomplex dynamic environments. Consider, for example, an autonomous robot\nvehicle driving in dense, unregulated urban traffic of cars, motorcycles,\nbuses, etc. The robot vehicle has to plan in both short and long terms, in\norder to interact with many traffic participants with uncertain intentions and\ndrive effectively. Planning explicitly over a long time horizon, however,\nincurs prohibitive computational costs and is impractical under real-time\nconstraints. To achieve real-time performance for large-scale planning, this\nwork introduces a new algorithm Learning from Tree Search for Driving\n(LeTS-Drive), which integrates planning and learning in a closed loop, and\napplies it to autonomous driving in crowded urban traffic in simulation.\nSpecifically, LeTS-Drive learns a policy and its value function from data\nprovided by an online planner, which searches a sparsely-sampled belief tree;\nthe online planner in turn uses the learned policy and value functions as\nheuristics to scale up its run-time performance for real-time robot control.\nThese two steps are repeated to form a closed loop so that the planner and the\nlearner inform each other and improve in synchrony. The algorithm learns on its\nown in a self-supervised manner, without human effort on explicit data\nlabeling. Experimental results demonstrate that LeTS-Drive outperforms either\nplanning or learning alone, as well as open-loop integration of planning and\nlearning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.0108,regular,pre_llm,2021,1,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""A Continuum Manipulator for Open-Source Surgical Robotics Research and\n  Shared Development\n\n  Many have explored the application of continuum robot manipulators for\nminimally invasive surgery, and have successfully demonstrated the advantages\ntheir flexible design provides -- with some solutions having reached\ncommercialisation and clinical practice. However, the usual high complexity and\nclosed-nature of such designs has traditionally restricted the shared\ndevelopment of continuum robots across the research area, thus impacting\nfurther progress and the solution of open challenges. In order to close this\ngap, this paper introduces ENDO, an open-source 3-segment continuum robot\nmanipulator with control and actuation mechanism, whose focus is on simplicity,\naffordability, and accessibility. This robotic system is fabricated from low\ncost off-the-shelf components and rapid prototyping methods, and its\ninformation for implementation (and that of future iterations), including CAD\nfiles and source code, is available to the public on the Open Source Medical\nRobots initiative's repository on GitHub\n(https://github.com/OpenSourceMedicalRobots), with the control library also\navailable directly from Arduino. Herein, we present details of the robot design\nand control, validate functionality by experimentally evaluating its workspace,\nand discuss possible paths for future development.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01132,regular,pre_llm,2021,1,"{'ai_likelihood': 1.2185838487413195e-05, 'text': 'Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter\n\n  General robot grasping in clutter requires the ability to synthesize grasps\nthat work for previously unseen objects and that are also robust to physical\ninteractions, such as collisions with other objects in the scene. In this work,\nwe design and train a network that predicts 6 DOF grasps from 3D scene\ninformation gathered from an on-board sensor such as a wrist-mounted depth\ncamera. Our proposed Volumetric Grasping Network (VGN) accepts a Truncated\nSigned Distance Function (TSDF) representation of the scene and directly\noutputs the predicted grasp quality and the associated gripper orientation and\nopening width for each voxel in the queried 3D volume. We show that our\napproach can plan grasps in only 10 ms and is able to clear 92% of the objects\nin real-world clutter removal experiments without the need for explicit\ncollision checking. The real-time capability opens up the possibility for\nclosed-loop grasp planning, allowing robots to handle disturbances, recover\nfrom errors and provide increased robustness. Code is available at\nhttps://github.com/ethz-asl/vgn.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.03624,regular,pre_llm,2021,1,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""Compliant Fins for Locomotion in Granular Media\n\n  In this paper, we present an approach to study the behavior of compliant\nplates in granular media and optimize the performance of a robot that utilizes\nthis technique for mobility. From previous work and fundamental tests on thin\nplate force generation inside granular media, we introduce an origami-inspired\nmechanism with non-linear compliance in the joints that can be used in granular\npropulsion. This concept utilizes one-sided joint limits to create an\nasymmetric gait cycle that avoids more complicated alternatives often found in\nother swimming/digging robots. To analyze its locomotion as well as its shape\nand propulsive force, we utilize granular Resistive Force Theory (RFT) as a\nstarting point. Adding compliance to this theory enables us to predict the\ntime-based evolution of compliant plates when they are dragged and rotated. It\nalso permits more rational design of swimming robots where fin design variables\nmay be optimized against the characteristics of the granular medium. This is\ndone using a Python-based dynamic simulation library to model the deformation\nof the plates and optimize aspects of the robot's gait. Finally, we prototype\nand test robot with a gait optimized using the modelling techniques mentioned\nabove.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.07994,regular,pre_llm,2021,1,"{'ai_likelihood': 2.8477774726019966e-06, 'text': ""Distributed Motion Coordination Using Convex Feasible Set Based Model\n  Predictive Control\n\n  The implementation of optimization-based motion coordination approaches in\nreal world multi-agent systems remains challenging due to their high\ncomputational complexity and potential deadlocks. This paper presents a\ndistributed model predictive control (MPC) approach based on convex feasible\nset (CFS) algorithm for multi-vehicle motion coordination in autonomous\ndriving. By using CFS to convexify the collision avoidance constraints,\ncollision-free trajectories can be computed in real time. We analyze the\npotential deadlocks and show that a deadlock can be resolved by changing\nvehicles' desired speeds. The MPC structure ensures that our algorithm is\nrobust to low-level tracking errors. The proposed distributed method has been\ntested in multiple challenging multi-vehicle environments, including\nunstructured road, intersection, crossing, platoon formation, merging, and\novertaking scenarios. The numerical results and comparison with other\napproaches (including a centralized MPC and reciprocal velocity obstacles) show\nthat the proposed method is computationally efficient and robust, and avoids\ndeadlocks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.01364,review,pre_llm,2021,1,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Run-Time Monitoring of Machine Learning for Robotic Perception: A Survey\n  of Emerging Trends\n\n  As deep learning continues to dominate all state-of-the-art computer vision\ntasks, it is increasingly becoming an essential building block for robotic\nperception. This raises important questions concerning the safety and\nreliability of learning-based perception systems. There is an established field\nthat studies safety certification and convergence guarantees of complex\nsoftware systems at design-time. However, the unknown future deployment\nenvironments of an autonomous system and the complexity of learning-based\nperception make the generalization of design-time verification to run-time\nproblematic. In the face of this challenge, more attention is starting to focus\non run-time monitoring of performance and reliability of perception systems\nwith several trends emerging in the literature. This paper attempts to identify\nthese trends and summarise the various approaches to the topic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2101.10113,regular,pre_llm,2021,1,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'ROS-NetSim: A Framework for the Integration of Robotic and Network\n  Simulators\n\n  Multi-agent systems play an important role in modern robotics. Due to the\nnature of these systems, coordination among agents via communication is\nfrequently necessary. Indeed, Perception-Action-Communication (PAC) loops, or\nPerception-Action loops closed over a communication channel, are a critical\ncomponent of multi-robot systems. However, we lack appropriate tools for\nsimulating PAC loops. To that end, in this paper, we introduce ROS-NetSim, a\nROS package that acts as an interface between robotic and network simulators.\nWith ROS-NetSim, we can attain high-fidelity representations of both robotic\nand network interactions by accurately simulating the PAC loop. Our proposed\napproach is lightweight, modular and adaptive. Furthermore, it can be used with\nmany available network and physics simulators by making use of our proposed\ninterface. In summary, ROS-NetSim is (i) Transparent to the ROS target\napplication, (ii) Agnostic to the specific network and physics simulator being\nused, and (iii) Tunable in fidelity and complexity. As part of our\ncontribution, we have made available an open-source implementation of\nROS-NetSim to the community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12026,regular,pre_llm,2021,2,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Towards Optimized Distributed Multi-Robot Printing: An Algorithmic\n  Approach\n\n  This paper presents a distributed multi-robot printing method which utilizes\nan optimization approach to decompose and allocate a printing task to a group\nof mobile robots. The motivation for this problem is to minimize the printing\ntime of the robots by using an appropriate task decomposition algorithm. We\npresent one such algorithm which decomposes an image into rasterized geodesic\ncells before allocating them to the robots for printing. In addition to this,\nwe also present the design of a numerically controlled holonomic robot capable\nof spraying ink on smooth surfaces. Further, we use this robot to\nexperimentally verify the results of this paper.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.07459,regular,pre_llm,2021,2,"{'ai_likelihood': 1.231829325358073e-05, 'text': 'Minimum Jerk Trajectory Generation for Straight and Curved Movements:\n  Mathematical Analysis\n\n  In this chapter, the mathematical analysis of the minimum jerk trajectory\n(MJT) generation is performed. In this study, the position and the velocity of\nthe minimum jerk trajectory as a function of time is presented in two cases:\nthe first one is the unconstrained point-to-point movements of the human hand,\nwhereas the second case is the curved point-to-point movements of the human\nhand. Simulation study is carried out with some examples and MATLAB is used for\nthis simulation. The results prove that the minimum jerk trajectory is the\nsmoothest possible movement of the human hand.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.04036,review,pre_llm,2021,2,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'Simultaneous Localization and Mapping Related Datasets: A Comprehensive\n  Survey\n\n  Due to the complicated procedure and costly hardware, Simultaneous\nLocalization and Mapping (SLAM) has been heavily dependent on public datasets\nfor drill and evaluation, leading to many impressive demos and good benchmark\nscores. However, with a huge contrast, SLAM is still struggling on the way\ntowards mature deployment, which sounds a warning: some of the datasets are\noverexposed, causing biased usage and evaluation. This raises the problem on\nhow to comprehensively access the existing datasets and correctly select them.\nMoreover, limitations do exist in current datasets, then how to build new ones\nand which directions to go? Nevertheless, a comprehensive survey which can\ntackle the above issues does not exist yet, while urgently demanded by the\ncommunity. To fill the gap, this paper strives to cover a range of cohesive\ntopics about SLAM related datasets, including general collection methodology\nand fundamental characteristic dimensions, SLAM related tasks taxonomy and\ndatasets categorization, introduction of state-of-the-arts, overview and\ncomparison of existing datasets, review of evaluation criteria, and analyses\nand discussions about current limitations and future directions, looking\nforward to not only guiding the dataset selection, but also promoting the\ndataset research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.0711,regular,pre_llm,2021,2,"{'ai_likelihood': 1.4901161193847656e-05, 'text': 'Point-line-based RGB-D SLAM and Bundle Adjustment Uncertainty Analysis\n\n  Most of the state-of-the-art indirect visual SLAM methods are based on the\nsparse point features. However, it is hard to find enough reliable point\nfeatures for state estimation in the case of low-textured scenes. Line features\nare abundant in urban and indoor scenes. Recent studies have shown that the\ncombination of point and line features can provide better accuracy despite the\ndecrease in computational efficiency. In this paper, measurements of point and\nline features are extracted from RGB-D data to create map features, and points\non a line are treated as keypoints. We propose an extended approach to make\nmore use of line observation information. And we prove that, in the local\nbundle adjustment, the estimation uncertainty of keyframe poses can be reduced\nwhen considering more landmarks with independent measurements in the\noptimization process. Experimental results on two public RGB-D datasets\ndemonstrate that the proposed method has better robustness and accuracy in\nchallenging environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03567,regular,pre_llm,2021,2,"{'ai_likelihood': 1.6027026706271703e-05, 'text': 'Standard and Event Cameras Fusion for Dense Mapping\n\n  Event cameras are a kind of bio-inspired sensors that generate data when the\nbrightness changes, which are of low-latency and high dynamic range (HDR).\nHowever, due to the nature of the sparse event stream, event-based mapping can\nonly obtain sparse or semi-dense edge 3D maps. By contrast, standard cameras\nprovide complete frames. To leverage the complementarity of event-based and\nstandard frame-based cameras, we propose a fusion strategy for dense mapping in\nthis paper. We first generate an edge map from events, and then fill the map\nusing frames to obtain the dense depth map. We propose ""filling score"" to\nevaluate the quality of filled results and show that our strategy can increase\nthe number of existing semi-dense 3D map.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.11662,regular,pre_llm,2021,2,"{'ai_likelihood': 6.390942467583551e-06, 'text': 'Design and Integration of a Drone based Passive Manipulator for\n  Capturing Flying Targets\n\n  In this paper, we present a novel passive single Degree-of-Freedom (DoF)\nmanipulator design and its integration on an autonomous drone to capture a\nmoving target. The end-effector is designed to be passive, to disengage the\nmoving target from a flying UAV and capture it efficiently in the presence of\ndisturbances, with minimal energy usage. It is also designed to handle target\nsway and the effect of downwash. The passive manipulator is integrated with the\ndrone through a single Degree of Freedom (DoF) arm, and experiments are carried\nout in an outdoor environment. The rack-and-pinion mechanism incorporated for\nthis manipulator ensures safety by extending the manipulator beyond the body of\nthe drone to capture the target. The autonomous capturing experiments are\nconducted using a red ball hanging from a stationary drone and subsequently\nfrom a moving drone. The experiments show that the manipulator captures the\ntarget with a success rate of 70\\% even under environmental/measurement\nuncertainties and errors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03923,regular,pre_llm,2021,2,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""DroneTrap: Drone Catching in Midair by Soft Robotic Hand with\n  Color-Based Force Detection and Hand Gesture Recognition\n\n  The paper proposes a novel concept of docking drones to make this process as\nsafe and fast as possible. The idea behind the project is that a robot with a\nsoft gripper grasps the drone in midair. The human operator navigates the\nrobotic arm with the ML-based gesture recognition interface. The 3-finger robot\nhand with soft fingers is equipped with touch sensors, making it possible to\nachieve safe drone catching and avoid inadvertent damage to the drone's\npropellers and motors. Additionally, the soft hand is featured with a unique\ncolor-based force estimation technology based on a computer vision (CV) system.\nMoreover, the visual color-changing system makes it easier for the human\noperator to interpret the applied forces.\n  Without any additional programming, the operator has full real-time control\nof the robot's motion and task execution by wearing a mocap glove with gesture\nrecognition, which was developed and applied for the high-level control of\nDroneTrap. The experimental results revealed that the developed color-based\nforce estimation can be applied for rigid object capturing with high precision\n(95.3\\%). The proposed technology can potentially revolutionize the landing and\ndeployment of drones for parcel delivery on uneven ground, structure\nmaintenance and inspection, risque operations, and etc.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.04148,review,pre_llm,2021,2,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Deep Reinforcement Learning for the Control of Robotic Manipulation: A\n  Focussed Mini-Review\n\n  Deep learning has provided new ways of manipulating, processing and analyzing\ndata. It sometimes may achieve results comparable to, or surpassing human\nexpert performance, and has become a source of inspiration in the era of\nartificial intelligence. Another subfield of machine learning named\nreinforcement learning, tries to find an optimal behavior strategy through\ninteractions with the environment. Combining deep learning and reinforcement\nlearning permits resolving critical issues relative to the dimensionality and\nscalability of data in tasks with sparse reward signals, such as robotic\nmanipulation and control tasks, that neither method permits resolving when\napplied on its own. In this paper, we present recent significant progress of\ndeep reinforcement learning algorithms, which try to tackle the problems for\nthe application in the domain of robotic manipulation control, such as sample\nefficiency and generalization. Despite these continuous improvements,\ncurrently, the challenges of learning robust and versatile manipulation skills\nfor robots with deep reinforcement learning are still far from being resolved\nfor real world applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12891,regular,pre_llm,2021,2,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""CPG-ACTOR: Reinforcement Learning for Central Pattern Generators\n\n  Central Pattern Generators (CPGs) have several properties desirable for\nlocomotion: they generate smooth trajectories, are robust to perturbations and\nare simple to implement. Although conceptually promising, we argue that the\nfull potential of CPGs has so far been limited by insufficient sensory-feedback\ninformation. This paper proposes a new methodology that allows tuning CPG\ncontrollers through gradient-based optimization in a Reinforcement Learning\n(RL) setting. To the best of our knowledge, this is the first time CPGs have\nbeen trained in conjunction with a MultilayerPerceptron (MLP) network in a\nDeep-RL context. In particular, we show how CPGs can directly be integrated as\nthe Actor in an Actor-Critic formulation. Additionally, we demonstrate how this\nchange permits us to integrate highly non-linear feedback directly from sensory\nperception to reshape the oscillators' dynamics. Our results on a locomotion\ntask using a single-leg hopper demonstrate that explicitly using the CPG as the\nActor rather than as part of the environment results in a significant increase\nin the reward gained over time (6x more) compared with previous approaches.\nFurthermore, we show that our method without feedback reproduces results\nsimilar to prior work with feedback. Finally, we demonstrate how our\nclosed-loop CPG progressively improves the hopping behaviour for longer\ntraining epochs relying only on basic reward functions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.05773,regular,pre_llm,2021,2,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Data-Driven MPC for Quadrotors\n\n  Aerodynamic forces render accurate high-speed trajectory tracking with\nquadrotors extremely challenging. These complex aerodynamic effects become a\nsignificant disturbance at high speeds, introducing large positional tracking\nerrors, and are extremely difficult to model. To fly at high speeds, feedback\ncontrol must be able to account for these aerodynamic effects in real-time.\nThis necessitates a modelling procedure that is both accurate and efficient to\nevaluate. Therefore, we present an approach to model aerodynamic effects using\nGaussian Processes, which we incorporate into a Model Predictive Controller to\nachieve efficient and precise real-time feedback control, leading to up to 70%\nreduction in trajectory tracking error at high speeds. We verify our method by\nextensive comparison to a state-of-the-art linear drag model in synthetic and\nreal-world experiments at speeds of up to 14m/s and accelerations beyond 4g.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.124,regular,pre_llm,2021,2,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'R2LIVE: A Robust, Real-time, LiDAR-Inertial-Visual tightly-coupled state\n  Estimator and mapping\n\n  In this letter, we propose a robust, real-time tightly-coupled multi-sensor\nfusion framework, which fuses measurement from LiDAR, inertial sensor, and\nvisual camera to achieve robust and accurate state estimation. Our proposed\nframework is composed of two parts: the filter-based odometry and factor graph\noptimization. To guarantee real-time performance, we estimate the state within\nthe framework of error-state iterated Kalman-filter, and further improve the\noverall precision with our factor graph optimization. Taking advantage of\nmeasurement from all individual sensors, our algorithm is robust enough to\nvarious visual failure, LiDAR-degenerated scenarios, and is able to run in\nreal-time on an on-board computation platform, as shown by extensive\nexperiments conducted in indoor, outdoor, and mixed environment of different\nscale. Moreover, the results show that our proposed framework can improve the\naccuracy of state-of-the-art LiDAR-inertial or visual-inertial odometry. To\nshare our findings and to make contributions to the community, we open source\nour codes on our Github.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03804,regular,pre_llm,2021,2,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Kalman Filters on Differentiable Manifolds\n\n  Kalman filter is presumably one of the most important and extensively used\nfiltering techniques in modern control systems. Yet, nearly all current\nvariants of Kalman filters are formulated in the Euclidean space\n$\\mathbb{R}^n$, while many real-world systems (e.g., robotic systems) are\nreally evolving on manifolds. In this paper, we propose a method to develop\nKalman filters for such on-manifold systems. Utilizing $\\boxplus$, $\\boxminus$\noperations and further defining an oplus operation on the respective manifold,\nwe propose a canonical representation of the on-manifold system. Such a\ncanonical form enables us to separate the manifold constraints from the system\nbehaviors in each step of the Kalman filter, ultimately leading to a generic\nand symbolic Kalman filter framework that are naturally evolving on the\nmanifold. Furthermore, the on-manifold Kalman filter is implemented as a\ntoolkit in $C$++ packages which enables users to implement an on-manifold\nKalman filter just like the normal one in $\\mathbb{R}^n$: the user needs only\nto provide the system-specific descriptions, and then call the respective\nfilter steps (e.g., predict, update) without dealing with any of the manifold\nconstraints. The existing implementation supports full iterated Kalman\nfiltering for systems on any manifold composed of $\\mathbb{R}^n$, $SO(3)$ and\n$\\mathbb{S}^2$, and is extendable to other types of manifold when necessary.\nThe proposed symbolic Kalman filter and the developed toolkit are verified by\nimplementing a tightly-coupled lidar-inertial navigation system. Results show\nthat the developed toolkit leads to superior filtering performances and\ncomputation efficiency comparable to hand-engineered counterparts. Finally, the\ntoolkit is opened sourced at https://github.com/hku-mars/IKFoM to assist\npractitioners to quickly deploy an on-manifold Kalman filter.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10313,regular,pre_llm,2021,2,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Mesh Manifold based Riemannian Motion Planning for Omnidirectional Micro\n  Aerial Vehicles\n\n  This paper presents a novel on-line path planning method that enables aerial\nrobots to interact with surfaces. We present a solution to the problem of\nfinding trajectories that drive a robot towards a surface and move along it.\nTriangular meshes are used as a surface map representation that is free of\nfixed discretization and allows for very large workspaces. We propose to\nleverage planar parametrization methods to obtain a lower-dimensional\ntopologically equivalent representation of the original surface. Furthermore,\nwe interpret the original surface and its lower-dimensional representation as\nmanifold approximations that allow the use of Riemannian Motion Policies\n(RMPs), resulting in an efficient, versatile, and elegant motion generation\nframework. We compare against several Rapidly-exploring Random Tree (RRT)\nplanners, a customized CHOMP variant, and the discrete geodesic algorithm.\nUsing extensive simulations on real-world data we show that the proposed\nplanner can reliably plan high-quality near-optimal trajectories at minimal\ncomputational cost. The accompanying multimedia attachment demonstrates\nfeasibility on a real OMAV. The obtained paths show less than 10% deviation\nfrom the theoretical optimum while facilitating reactive re-planning at kHz\nrefresh rates, enabling flying robots to perform motion planning for\ninteraction with complex surfaces.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03643,regular,pre_llm,2021,2,"{'ai_likelihood': 1.4536910586886936e-05, 'text': 'A surgical dataset from the da Vinci Research Kit for task automation\n  and recognition\n\n  The use of datasets is getting more relevance in surgical robotics since they\ncan be used to recognise and automate tasks. Also, this allows to use common\ndatasets to compare different algorithms and methods. The objective of this\nwork is to provide a complete dataset of three common training surgical tasks\nthat surgeons perform to improve their skills. For this purpose, 12 subjects\nteleoperated the da Vinci Research Kit to perform these tasks. The obtained\ndataset includes all the kinematics and dynamics information provided by the da\nVinci robot (both master and slave side) together with the associated video\nfrom the camera. All the information has been carefully timestamped and\nprovided in a readable csv format. A MATLAB interface integrated with ROS for\nusing and replicating the data is also provided.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12649,regular,pre_llm,2021,2,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""Adapting legacy robotic machinery to industry 4: a ciot experiment\n  version 1\n\n  This paper presents an experimental adaptation of a non-collaborative robot\narm to collaborate with the environment, as one step towards adapting legacy\nrobotic machinery to fit in industry 4.0 requirements. A cloud-based internet\nof things (CIoT) service is employed to connect, supervise and control a\nrobotic arm's motion using the added wireless sensing devices to the\nenvironment. A programmable automation controller (PAC) unit, connected to the\nrobot arm receives the most recent changes and updates the motion of the robot\narm. The experimental results show that the proposed non-expensive service is\ntractable and adaptable to higher level for machine to machine collaboration.\nThe proposed approach in this paper has industrial and educational\napplications. In the proposed approach, the CIoT technology is added as a\ntechnology interface between the sensors to the environment and the robotic\narm. The proposed approach is versatile and fits to variety of applications to\nmeet the flexible requirements of industry 4.0. The proposed approach has been\nimplemented in an experiment using MECA 500 robot arm and AMAX 5580\nprogrammable automation controller and ultrasonic proximity wireless sensor.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.08442,regular,pre_llm,2021,2,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'SCAPE: Learning Stiffness Control from Augmented Position Control\n  Experiences\n\n  We introduce a sample-efficient method for learning state-dependent stiffness\ncontrol policies for dexterous manipulation. The ability to control stiffness\nfacilitates safe and reliable manipulation by providing compliance and\nrobustness to uncertainties. Most current reinforcement learning approaches to\nachieve robotic manipulation have exclusively focused on position control,\noften due to the difficulty of learning high-dimensional stiffness control\npolicies. This difficulty can be partially mitigated via policy guidance such\nas imitation learning. However, expert stiffness control demonstrations are\noften expensive or infeasible to record. Therefore, we present an approach to\nlearn Stiffness Control from Augmented Position control Experiences (SCAPE)\nthat bypasses this difficulty by transforming position control demonstrations\ninto approximate, suboptimal stiffness control demonstrations. Then, the\nsuboptimality of the augmented demonstrations is addressed by using\ncomplementary techniques that help the agent safely learn from both the\ndemonstrations and reinforcement learning. By using simulation tools and\nexperiments on a robotic testbed, we show that the proposed approach\nefficiently learns safe manipulation policies and outperforms learned position\ncontrol policies and several other baseline learning algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.10808,regular,pre_llm,2021,2,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'ikd-Tree: An Incremental K-D Tree for Robotic Applications\n\n  This paper proposes an efficient data structure, ikd-Tree, for dynamic space\npartition. The ikd-Tree incrementally updates a k-d tree with new coming points\nonly, leading to much lower computation time than existing static k-d trees.\nBesides point-wise operations, the ikd-Tree supports several features such as\nbox-wise operations and down-sampling that are practically useful in robotic\napplications. In parallel to the incremental operations (i.e., insert,\nre-insert, and delete), ikd-Tree actively monitors the tree structure and\npartially re-balances the tree, which enables efficient nearest point search in\nlater stages. The ikd-Tree is carefully engineered and supports multi-thread\nparallel computing to maximize the overall efficiency. We validate the ikd-Tree\nin both theory and practical experiments. On theory level, a complete time\ncomplexity analysis is presented to prove the high efficiency. On experiment\nlevel, the ikd-Tree is tested on both randomized datasets and real-world LiDAR\npoint data in LiDAR-inertial odometry and mapping application. In all tests,\nikd-Tree consumes only 4% of the running time in a static k-d tree.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.03318,regular,pre_llm,2021,2,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Towards integrated tactile sensorimotor control in anthropomorphic soft\n  robotic hands\n\n  In this work, we report on the integrated sensorimotor control of the\nPisa/IIT SoftHand, an anthropomorphic soft robot hand designed around the\nprinciple of adaptive synergies, with the BRL tactile fingertip (TacTip), a\nsoft biomimetic optical tactile sensor based on the human sense of touch. Our\nfocus is how a sense of touch can be used to control an anthropomorphic hand\nwith one degree of actuation, based on an integration that respects the hand's\nmechanical functionality. We consider: (i) closed-loop tactile control to\nestablish a light contact on an unknown held object, based on the structural\nsimilarity with an undeformed tactile image; and (ii) controlling the estimated\npose of an edge feature of a held object, using a convolutional neural network\napproach developed for controlling other sensors in the TacTip family. Overall,\nthis gives a foundation to endow soft robotic hands with human-like touch, with\nimplications for autonomous grasping, manipulation, human-robot interaction and\nprosthetics. Supplemental video: https://youtu.be/ndsxj659bkQ\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2102.12667,regular,pre_llm,2021,2,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Learning Inverse Kinodynamics for Accurate High-Speed Off-Road\n  Navigation on Unstructured Terrain\n\n  This paper presents a learning-based approach to consider the effect of\nunobservable world states in kinodynamic motion planning in order to enable\naccurate high-speed off-road navigation on unstructured terrain. Existing\nkinodynamic motion planners either operate in structured and homogeneous\nenvironments and thus do not need to explicitly account for terrain-vehicle\ninteraction, or assume a set of discrete terrain classes. However, when\noperating on unstructured terrain, especially at high speeds, even small\nvariations in the environment will be magnified and cause inaccurate plan\nexecution. In this paper, to capture the complex kinodynamic model and\nmathematically unknown world state, we learn a kinodynamic planner in a\ndata-driven manner with onboard inertial observations. Our approach is tested\non a physical robot in different indoor and outdoor environments, enables fast\nand accurate off-road navigation, and outperforms environment-independent\nalternatives, demonstrating 52.4% to 86.9% improvement in terms of plan\nexecution success rate while traveling at high speeds.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.00452,regular,pre_llm,2021,2,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'EKMP: Generalized Imitation Learning with Adaptation, Nonlinear Hard\n  Constraints and Obstacle Avoidance\n\n  As a user-friendly and straightforward solution for robot trajectory\ngeneration, imitation learning has been viewed as a vital direction in the\ncontext of robot skill learning. In contrast to unconstrained imitation\nlearning which ignores possible internal and external constraints arising from\nenvironments and robot kinematics/dynamics, recent works on constrained\nimitation learning allow for transferring human skills to unstructured\nscenarios, further enlarging the application domain of imitation learning.\nWhile various constraints have been studied, e.g., joint limits, obstacle\navoidance and plane constraints, the problem of nonlinear hard constraints has\nnot been well-addressed. In this paper, we propose extended kernelized movement\nprimitives (EKMP) to cope with most of the key problems in imitation learning,\nincluding nonlinear hard constraints. Specifically, EKMP is capable of learning\nthe probabilistic features of multiple demonstrations, adapting the learned\nskills towards arbitrary desired points in terms of joint position and\nvelocity, avoiding obstacles at the level of robot links, as well as satisfying\narbitrary linear and nonlinear, equality and inequality hard constraints.\nBesides, the connections between EKMP and state-of-the-art motion planning\napproaches are discussed. Several evaluations including the planning of joint\ntrajectories for a 7-DoF robotic arm are provided to verify the effectiveness\nof our framework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.01434,regular,pre_llm,2021,3,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Learning Robotic Manipulation Tasks via Task Progress based Gaussian\n  Reward and Loss Adjusted Exploration\n\n  Multi-step manipulation tasks in unstructured environments are extremely\nchallenging for a robot to learn. Such tasks interlace high-level reasoning\nthat consists of the expected states that can be attained to achieve an overall\ntask and low-level reasoning that decides what actions will yield these states.\nWe propose a model-free deep reinforcement learning method to learn multi-step\nmanipulation tasks. We introduce a Robotic Manipulation Network (RoManNet),\nwhich is a vision-based model architecture, to learn the action-value functions\nand predict manipulation action candidates. We define a Task Progress based\nGaussian (TPG) reward function that computes the reward based on actions that\nlead to successful motion primitives and progress towards the overall task\ngoal. To balance the ratio of exploration/exploitation, we introduce a Loss\nAdjusted Exploration (LAE) policy that determines actions from the action\ncandidates according to the Boltzmann distribution of loss estimates. We\ndemonstrate the effectiveness of our approach by training RoManNet to learn\nseveral challenging multi-step robotic manipulation tasks in both simulation\nand real-world. Experimental results show that our method outperforms the\nexisting methods and achieves state-of-the-art performance in terms of success\nrate and action efficiency. The ablation studies show that TPG and LAE are\nespecially beneficial for tasks like multiple block stacking. Code is available\nat: https://github.com/skumra/romannet\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0466,regular,pre_llm,2021,3,"{'ai_likelihood': 3.741847144232856e-06, 'text': ""A Versatile Co-Design Approach For Dynamic Legged Robots\n\n  We present a versatile framework for the computational co-design of legged\nrobots and dynamic maneuvers. Current state-of-the-art approaches are typically\nbased on random sampling or concurrent optimization. We propose a novel bilevel\noptimization approach that exploits the derivatives of the motion planning\nsub-problem (i.e., the lower level). These motion-planning derivatives allow us\nto incorporate arbitrary design constraints and costs in an general-purpose\nnonlinear program (i.e., the upper level). Our approach allows for the use of\nany differentiable motion planner in the lower level and also allows for an\nupper level that captures arbitrary design constraints and costs. It\nefficiently optimizes the robot's morphology, payload distribution and actuator\nparameters while considering its full dynamics, joint limits and physical\nconstraints such as friction cones. We demonstrate these capabilities by\ndesigning quadruped robots that jump and trot. We show that our method is able\nto design a more energy-efficient Solo robot for these tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.01104,regular,pre_llm,2021,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Contact-Implicit Trajectory Optimization for Dynamic Object Manipulation\n\n  We present a reformulation of a contact-implicit optimization (CIO) approach\nthat computes optimal trajectories for rigid-body systems in contact-rich\nsettings. A hard-contact model is assumed, and the unilateral constraints are\nimposed in the form of complementarity conditions. Newton's impact law is\nadopted for enhanced physical correctness. The optimal control problem is\nformulated as a multi-staged program through a multiple-shooting scheme. This\nproblem structure is exploited within the FORCES Pro framework to retrieve\noptimal motion plans, contact sequences and control inputs with increased\ncomputational efficiency. We investigate our method on a variety of dynamic\nobject manipulation tasks, performed by a six degrees of freedom robot. The\ndynamic feasibility of the optimal trajectories, as well as the repeatability\nand accuracy of the task-satisfaction are verified through simulations and real\nhardware experiments on one of the manipulation problems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.01464,regular,pre_llm,2021,3,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'NavTuner: Learning a Scene-Sensitive Family of Navigation Policies\n\n  The advent of deep learning has inspired research into end-to-end learning\nfor a variety of problem domains in robotics. For navigation, the resulting\nmethods may not have the generalization properties desired let alone match the\nperformance of traditional methods. Instead of learning a navigation policy, we\nexplore learning an adaptive policy in the parameter space of an existing\nnavigation module. Having adaptive parameters provides the navigation module\nwith a family of policies that can be dynamically reconfigured based on the\nlocal scene structure, and addresses the common assertion in machine learning\nthat engineered solutions are inflexible. Of the methods tested, reinforcement\nlearning (RL) is shown to provide a significant performance boost to a modern\nnavigation method through reduced sensitivity of its success rate to\nenvironmental clutter. The outcomes indicate that RL as a meta-policy learner,\nor dynamic parameter tuner, effectively robustifies algorithms sensitive to\nexternal, measurable nuisance factors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.08003,regular,pre_llm,2021,3,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Design of a vision based range bearing and heading system for robot\n  swarms\n\n  An essential problem of swarm robotics is how members of the swarm knows the\npositions of other robots. The main aim of this research is to develop a\ncost-effective and simple vision-based system to detect the range, bearing, and\nheading of the robots inside a swarm using a multi-purpose passive landmark. A\nsmall Zumo robot equipped with Raspberry Pi, PiCamera is utilized for the\nimplementation of the algorithm, and different kinds of multipurpose passive\nlandmarks with nonsymmetrical patterns, which give reliable information about\nthe range, bearing and heading in a single unit, are designed. By comparing the\nrecorded features obtained from image analysis of the landmark through\nsystematical experimentation and the actual measurements, correlations are\nobtained, and algorithms converting those features into range, bearing and\nheading are designed. The reliability and accuracy of algorithms are tested and\nerrors are found within an acceptable range.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.06713,regular,pre_llm,2021,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Have I been here before? Learning to Close the Loop with LiDAR Data in\n  Graph-Based SLAM\n\n  This work presents an extension of graph-based SLAM methods to exploit the\npotential of 3D laser scans for loop detection. Every high-dimensional point\ncloud is replaced by a compact global descriptor, whereby a trained detector\ndecides whether a loop exists. Searching for loops is performed locally in a\nvariable space to consider the odometry drift. Since closing a wrong loop has\nfatal consequences, an extensive verification is performed before acceptance.\nThe proposed algorithm is implemented as an extension of the widely used\nstate-of-the-art library RTAB-Map, and several experiments show the\nimprovement: During SLAM with a mobile service robot in changing indoor and\noutdoor campus environments, our approach improves RTAB-Map regarding total\nnumber of closed loops. Especially in the presence of significant environmental\nchanges, which typically lead to failure, localization becomes possible by our\nextension. Experiments with a car in traffic (KITTI benchmark) show the general\napplicability of our approach. These results are comparable to the\nstate-of-the-art LiDAR method LOAM. The developed ROS package is freely\navailable.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.07588,regular,pre_llm,2021,3,"{'ai_likelihood': 1.7649597591824004e-05, 'text': 'RLSS: Real-time Multi-Robot Trajectory Replanning using Linear Spatial\n  Separations\n\n  Trajectory replanning is a critical problem for multi-robot teams navigating\ndynamic environments. We present RLSS (Replanning using Linear Spatial\nSeparations): a real-time trajectory replanning algorithm for cooperative\nmulti-robot teams that uses linear spatial separations to enforce safety. Our\nalgorithm handles the dynamic limits of the robots explicitly, is completely\ndistributed, and is robust to environment changes, robot failures, and\ntrajectory tracking errors. It requires no communication between robots and\nrelies instead on local relative measurements only. We demonstrate that the\nalgorithm works in real-time both in simulations and in experiments using\nphysical robots. We compare our algorithm to a state-of-the-art online\ntrajectory generation algorithm based on model predictive control, and show\nthat our algorithm results in significantly fewer collisions in highly\nconstrained environments, and effectively avoids deadlocks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.01882,regular,pre_llm,2021,3,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Exploring Imitation Learning for Autonomous Driving with Feedback\n  Synthesizer and Differentiable Rasterization\n\n  We present a learning-based planner that aims to robustly drive a vehicle by\nmimicking human drivers' driving behavior. We leverage a mid-to-mid approach\nthat allows us to manipulate the input to our imitation learning network\nfreely. With that in mind, we propose a novel feedback synthesizer for data\naugmentation. It allows our agent to gain more driving experience in various\npreviously unseen environments that are likely to encounter, thus improving\noverall performance. This is in contrast to prior works that rely purely on\nrandom synthesizers. Furthermore, rather than completely commit to imitating,\nwe introduce task losses that penalize undesirable behaviors, such as\ncollision, off-road, and so on. Unlike prior works, this is done by introducing\na differentiable vehicle rasterizer that directly converts the waypoints output\nby the network into images. This effectively avoids the usage of heavyweight\nConvLSTM networks, therefore, yields a faster model inference time. About the\nnetwork architecture, we exploit an attention mechanism that allows the network\nto reason critical objects in the scene and produce better interpretable\nattention heatmaps. To further enhance the safety and robustness of the\nnetwork, we add an optional optimization-based post-processing planner\nimproving the driving comfort. We comprehensively validate our method's\neffectiveness in different scenarios that are specifically created for\nevaluating self-driving vehicles. Results demonstrate that our learning-based\nplanner achieves high intelligence and can handle complex situations. Detailed\nablation and visualization analysis are included to further demonstrate each of\nour proposed modules' effectiveness in our method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.11522,regular,pre_llm,2021,3,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Multi-directional Bicycle Robot for Steel Structure Inspection\n\n  This paper presents a novel design of a multi-directional bicycle robot,\nwhich targets inspecting general ferromagnetic structures including\ncomplex-shaped structures. The locomotion concept is based on arranging two\nmagnetic wheels in a bicycle-like configuration with two independent steering\nactuators. This configuration allows the robot to possess multi-directional\nmobility. An additional free joint helps the robot naturally adapt to non-flat\nand complex surfaces of steel structures. The robot has the biggest advantage\nto be mechanically simple with high mobility. Besides, the robot is equipped\nwith sensing tools for structure health monitoring. We demonstrate the\ndeployment of our robot to perform steel rust detection on steel bridges. The\nfinal inspection results are visualized as 3D models of the bridges together\nwith marked locations of detected rusty areas.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.1003,regular,pre_llm,2021,3,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'AutoDRIVE Simulator: A Simulator for Scaled Autonomous Vehicle Research\n  and Education\n\n  AutoDRIVE is envisioned to be an integrated research and education platform\nfor scaled autonomous vehicles and related applications. This work is a\nstepping-stone towards achieving the greater goal of realizing such a platform.\nParticularly, this work introduces the AutoDRIVE Simulator, a high-fidelity\nsimulator for scaled autonomous vehicles. The proposed simulation ecosystem is\ndeveloped atop the Unity game engine, and exploits its features in order to\nsimulate realistic system dynamics and render photorealistic graphics. It\ncomprises of a scaled vehicle model equipped with a comprehensive sensor suite\nfor redundant perception, a set of actuators for constrained motion control and\na fully functional lighting system for illumination and signaling. It also\nprovides a modular environment development kit, which comprises of various\nenvironment modules that aid in reconfigurable construction of the scene.\nAdditionally, the simulator features a communication bridge in order to extend\nan interface to the autonomous driving software stack developed independently\nby the users. This work describes some of the prominent components of this\nsimulation system along with some key features that it has to offer in order to\naccelerate education and research aimed at autonomous driving.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.04859,regular,pre_llm,2021,3,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Exploiting Spherical Projections To Generate Human-Like Wrist Pointing\n  Movements\n\n  The mechanism behind the generation of human movements is of great interest\nin many fields (e.g. robotics and neuroscience) to improve therapies and\ntechnologies. Optimal Feedback Control (OFC) and Passive Motion Paradigm (PMP)\nare currently two leading theories capable of effectively producing human-like\nmotions, but they require solving nonlinear inverse problems to find a\nsolution. The main benefit of using PMP is the possibility of generating\npath-independent movements consistent with the stereotypical behaviour observed\nin humans, while the equivalent OFC formulation is path-dependent. Our results\ndemonstrate how the path-independent behaviour observed for the wrist pointing\ntask can be explained by spherical projections of the planar tasks. The\ncombination of the projections with the fractal impedance controller eliminates\nthe nonlinear inverse problem, which reduces the computational cost compared to\nprevious methodologies. The motion exploits a recently proposed PMP\narchitecture that replaces the nonlinear inverse optimisation with a nonlinear\nanisotropic stiffness impedance profile generated by the Fractal Impedance\nController, reducing the computational cost and not requiring a task-dependent\noptimisation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.01598,regular,pre_llm,2021,3,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Spatial Attention Point Network for Deep-learning-based Robust\n  Autonomous Robot Motion Generation\n\n  Deep learning provides a powerful framework for automated acquisition of\ncomplex robotic motions. However, despite a certain degree of generalization,\nthe need for vast amounts of training data depending on the work-object\nposition is an obstacle to industrial applications. Therefore, a robot\nmotion-generation model that can respond to a variety of work-object positions\nwith a small amount of training data is necessary. In this paper, we propose a\nmethod robust to changes in object position by automatically extracting spatial\nattention points in the image for the robot task and generating motions on the\nbasis of their positions. We demonstrate our method with an LBR iiwa 7R1400\nrobot arm on a picking task and a pick-and-place task at various positions in\nvarious situations. In each task, the spatial attention points are obtained for\nthe work objects that are important to the task. Our method is robust to\nchanges in object position. Further, it is robust to changes in background,\nlighting, and obstacles that are not important to the task because it only\nfocuses on positions that are important to the task.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.14111,regular,pre_llm,2021,3,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Spatial and Temporal Splitting Heuristics for Multi-Robot Motion\n  Planning\n\n  In this work, we systematically examine the application of spatio-temporal\nsplitting heuristics to the Multi-Robot Motion Planning (MRMP) problem in a\ngraph-theoretic setting: a problem known to be NP-hard to optimally solve.\nFollowing the divide-and-conquer principle, we design multiple spatial and\ntemporal splitting schemes that can be applied to any existing MRMP algorithm,\nincluding integer programming solvers and Enhanced Conflict Based Search, in an\northogonal manner. The combination of a good baseline MRMP algorithm with a\nproper splitting heuristic proves highly effective, allowing the resolution of\nproblems 10+ times than what is possible previously, as corroborated by\nextensive numerical evaluations. Notably, spatial partition of problem fusing\nwith the temporal splitting heuristic and the enhanced conflict based search\n(ECBS) algorithm increases the scalability of ECBS on large and challenging DAO\nmaps by 5--15 folds with negligible impact on solution optimality.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.17098,regular,pre_llm,2021,3,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Ergodic imitation: Learning from what to do and what not to do\n\n  With growing access to versatile robotics, it is beneficial for end users to\nbe able to teach robots tasks without needing to code a control policy. One\npossibility is to teach the robot through successful task executions. However,\nnear-optimal demonstrations of a task can be difficult to provide and even\nsuccessful demonstrations can fail to capture task aspects key to robust skill\nreplication. Here, we propose a learning from demonstration (LfD) approach that\nenables learning of robust task definitions without the need for near-optimal\ndemonstrations. We present a novel algorithmic framework for learning tasks\nbased on the ergodic metric -- a measure of information content in motion.\nMoreover, we make use of negative demonstrations -- demonstrations of what not\nto do -- and show that they can help compensate for imperfect demonstrations,\nreduce the number of demonstrations needed, and highlight crucial task elements\nimproving robot performance. In a proof-of-concept example of cart-pole\ninversion, we show that negative demonstrations alone can be sufficient to\nsuccessfully learn and recreate a skill. Through a human subject study with 24\nparticipants, we show that consistently more information about a task can be\ncaptured from combined positive and negative (posneg) demonstrations than from\nthe same amount of just positive demonstrations. Finally, we demonstrate our\nlearning approach on simulated tasks of target reaching and table cleaning with\na 7-DoF Franka arm. Our results point towards a future with robust,\ndata-efficient LfD for novice users.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.0807,regular,pre_llm,2021,3,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Learning robust driving policies without online exploration\n\n  We propose a multi-time-scale predictive representation learning method to\nefficiently learn robust driving policies in an offline manner that generalize\nwell to novel road geometries, and damaged and distracting lane conditions\nwhich are not covered in the offline training data. We show that our proposed\nrepresentation learning method can be applied easily in an offline (batch)\nreinforcement learning setting demonstrating the ability to generalize well and\nefficiently under novel conditions compared to standard batch RL methods. Our\nproposed method utilizes training data collected entirely offline in the\nreal-world which removes the need of intensive online explorations that impede\napplying deep reinforcement learning on real-world robot training. Various\nexperiments were conducted in both simulator and real-world scenarios for the\npurpose of evaluation and analysis of our proposed claims.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.07298,regular,pre_llm,2021,3,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Augmented Environment Representations with Complete Object Models\n\n  While 2D occupancy maps commonly used in mobile robotics enable safe\nnavigation in indoor environments, in order for robots to understand and\ninteract with their environment and its inhabitants representing 3D geometry\nand semantic environment information is required. Semantic information is\ncrucial in effective interpretation of the meanings humans attribute to\ndifferent parts of a space, while 3D geometry is important for safety and\nhigh-level understanding. We propose a pipeline that can generate a multi-layer\nrepresentation of indoor environments for robotic applications. The proposed\nrepresentation includes 3D metric-semantic layers, a 2D occupancy layer, and an\nobject instance layer where known objects are replaced with an approximate\nmodel obtained through a novel model-matching approach. The metric-semantic\nlayer and the object instance layer are combined to form an augmented\nrepresentation of the environment. Experiments show that the proposed shape\nmatching method outperforms a state-of-the-art deep learning method when tasked\nto complete unseen parts of objects in the scene. The pipeline performance\ntranslates well from simulation to real world as shown by F1-score analysis,\nwith semantic segmentation accuracy using Mask R-CNN acting as the major\nbottleneck. Finally, we also demonstrate on a real robotic platform how the\nmulti-layer map can be used to improve navigation safety.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.12883,regular,pre_llm,2021,3,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'Deep Reinforcement Learning for Mapless Navigation of a Hybrid Aerial\n  Underwater Vehicle with Medium Transition\n\n  Since the application of Deep Q-Learning to the continuous action domain in\nAtari-like games, Deep Reinforcement Learning (Deep-RL) techniques for motion\ncontrol have been qualitatively enhanced. Nowadays, modern Deep-RL can be\nsuccessfully applied to solve a wide range of complex decision-making tasks for\nmany types of vehicles. Based on this context, in this paper, we propose the\nuse of Deep-RL to perform autonomous mapless navigation for Hybrid Unmanned\nAerial Underwater Vehicles (HUAUVs), robots that can operate in both, air or\nwater media. We developed two approaches, one deterministic and the other\nstochastic. Our system uses the relative localization of the vehicle and simple\nsparse range data to train the network. We compared our approaches with a\ntraditional geometric tracking controller for mapless navigation. Based on\nexperimental results, we can conclude that Deep-RL-based approaches can be\nsuccessfully used to perform mapless navigation and obstacle avoidance for\nHUAUVs. Our vehicle accomplished the navigation in two scenarios, being capable\nto achieve the desired target through both environments, and even outperforming\nthe geometric-based tracking controller on the obstacle-avoidance capability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.15241,regular,pre_llm,2021,3,"{'ai_likelihood': 9.271833631727431e-07, 'text': ""Online Flocking Control of UAVs with Mean-Field Approximation\n\n  This work presents a novel, inference-based approach to the distributed and\ncooperative flocking control of aerial robot swarms. The proposed method stems\nfrom the Unmanned Aerial Vehicle (UAV) dynamics by limiting the latent set to\nthe robots' feasible action space, thus preventing any unattainable control\ninputs from being produced and leading to smooth flocking behavior. By modeling\nthe inter-agent relationships using a pairwise energy function, we show that\ninteracting robot swarms constitute a Markov Random Field. Our algorithm builds\non the Mean-Field Approximation and incorporates the collective behavioral\nrules: cohesion, separation, and velocity alignment. We follow a distributed\ncontrol scheme and show that our method can control a swarm of UAVs to a\nformation and velocity consensus with real-time collision avoidance. We\nvalidate the proposed method with physical UAVs and high-fidelity simulation\nexperiments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.01655,regular,pre_llm,2021,3,"{'ai_likelihood': 1.7119778527153863e-05, 'text': ""Run Your Visual-Inertial Odometry on NVIDIA Jetson: Benchmark Tests on a\n  Micro Aerial Vehicle\n\n  This paper presents benchmark tests of various visual(-inertial) odometry\nalgorithms on NVIDIA Jetson platforms. The compared algorithms include mono and\nstereo, covering Visual Odometry (VO) and Visual-Inertial Odometry (VIO):\nVINS-Mono, VINS-Fusion, Kimera, ALVIO, Stereo-MSCKF, ORB-SLAM2 stereo, and\nROVIO. As these methods are mainly used for unmanned aerial vehicles (UAVs),\nthey must perform well in situations where the size of the processing board and\nweight is limited. Jetson boards released by NVIDIA satisfy these constraints\nas they have a sufficiently powerful central processing unit (CPU) and graphics\nprocessing unit (GPU) for image processing. However, in existing studies, the\nperformance of Jetson boards as a processing platform for executing VO/VIO has\nnot been compared extensively in terms of the usage of computing resources and\naccuracy. Therefore, this study compares representative VO/VIO algorithms on\nseveral NVIDIA Jetson platforms, namely NVIDIA Jetson TX2, Xavier NX, and AGX\nXavier, and introduces a novel dataset 'KAIST VIO dataset' for UAVs. Including\npure rotations, the dataset has several geometric trajectories that are harsh\nto visual(-inertial) state estimation. The evaluation is performed in terms of\nthe accuracy of estimated odometry, CPU usage, and memory usage on various\nJetson boards, algorithms, and trajectories. We present the {results of the}\ncomprehensive benchmark test and release the dataset for the computer vision\nand robotics applications.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2103.04374,regular,pre_llm,2021,3,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Learning When to Quit: Meta-Reasoning for Motion Planning\n\n  Anytime motion planners are widely used in robotics. However, the\nrelationship between their solution quality and computation time is not well\nunderstood, and thus, determining when to quit planning and start execution is\nunclear. In this paper, we address the problem of deciding when to stop\ndeliberation under bounded computational capacity, so called meta-reasoning,\nfor anytime motion planning. We propose data-driven learning methods,\nmodel-based and model-free meta-reasoning, that are applicable to different\nenvironment distributions and agnostic to the choice of anytime motion\nplanners. As a part of the framework, we design a convolutional neural\nnetwork-based optimal solution predictor that predicts the optimal path length\nfrom a given 2D workspace image. We empirically evaluate the performance of the\nproposed methods in simulation in comparison with baselines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.14285,regular,pre_llm,2021,4,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Hybrid tracker based optimal path tracking system for complex road\n  environments for autonomous driving\n\n  Path tracking system plays a key technology in autonomous driving. The system\nshould be driven accurately along the lane and be careful not to cause any\ninconvenience to passengers. To address such tasks, this paper proposes hybrid\ntracker based optimal path tracking system. By applying a deep learning based\nlane detection algorithm and a designated fast lane fitting algorithm, this\npaper developed a lane processing algorithm that shows a match rate with actual\nlanes with minimal computational cost. In addition, three modified path\ntracking algorithms were designed using the GPS based path or the vision based\npath. In the driving system, a match rate for the correct ideal path does not\nnecessarily represent driving stability. This paper proposes hybrid tracker\nbased optimal path tracking system by applying the concept of an observer that\nselects the optimal tracker appropriately in complex road environments. The\ndriving stability has been studied in complex road environments such as\nstraight road with multiple 3-way junctions, roundabouts, intersections, and\ntunnels. Consequently, the proposed system experimentally showed the high\nperformance with consistent driving comfort by maintaining the vehicle within\nthe lanes accurately even in the presence of high complexity of road\nconditions. Code will be available in https://github.com/DGIST-ARTIV.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.02402,regular,pre_llm,2021,4,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'General Robot Dynamics Learning and Gen2Real\n\n  Acquiring dynamics is an essential topic in robot learning, but up-to-date\nmethods, such as dynamics randomization, need to restart to check nominal\nparameters, generate simulation data, and train networks whenever they face\ndifferent robots. To improve it, we novelly investigate general robot dynamics,\nits inverse models, and Gen2Real, which means transferring to reality. Our\nmotivations are to build a model that learns the intrinsic dynamics of various\nrobots and lower the threshold of dynamics learning by enabling an amateur to\nobtain robot models without being trapped in details. This paper achieves the\n""generality"" by randomizing dynamics parameters, topology configurations, and\nmodel dimensions, which in sequence cover the property, the connection, and the\nnumber of robot links. A structure modified from GPT is applied to access the\npre-training model of general dynamics. We also study various inverse models of\ndynamics to facilitate different applications. We step further to investigate a\nnew concept, ""Gen2Real"", to transfer simulated, general models to physical,\nspecific robots. Simulation and experiment results demonstrate the validity of\nthe proposed models and method.\\footnote{ These authors contribute equally.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.14184,regular,pre_llm,2021,4,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Uncertainty-aware deep learning for robot touch: Application to Bayesian\n  tactile servo control\n\n  This work investigates uncertainty-aware deep learning (DL) in tactile\nrobotics based on a general framework introduced recently for robot vision. For\na test scenario, we consider optical tactile sensing in combination with DL to\nestimate the edge pose as a feedback signal to servo around various 2D test\nobjects. We demonstrate that uncertainty-aware DL can improve the pose\nestimation over deterministic DL methods. The system estimates the uncertainty\nassociated with each prediction, which is used along with temporal coherency to\nimprove the predictions via a Kalman filter, and hence improve the tactile\nservo control. The robot is able to robustly follow all of the presented\ncontour shapes to reduce not only the error by a factor of two but also smooth\nthe trajectory from the undesired noisy behaviour caused by previous\ndeterministic networks. In our view, as the field of tactile robotics matures\nin its use of DL, the estimation of uncertainty will become a key component in\nthe control of physically interactive tasks in complex environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.11907,regular,pre_llm,2021,4,"{'ai_likelihood': 8.079740736219619e-06, 'text': 'CFNet: LiDAR-Camera Registration Using Calibration Flow Network\n\n  As an essential procedure of data fusion, LiDAR-camera calibration is\ncritical for autonomous vehicles and robot navigation. Most calibration methods\nrely on hand-crafted features and require significant amounts of extracted\nfeatures or specific calibration targets. With the development of deep learning\n(DL) techniques, some attempts take advantage of convolutional neural networks\n(CNNs) to regress the 6 degrees of freedom (DOF) extrinsic parameters.\nNevertheless, the performance of these DL-based methods is reported to be worse\nthan the non-DL methods. This paper proposed an online LiDAR-camera extrinsic\ncalibration algorithm that combines the DL and the geometry methods. We define\na two-channel image named calibration flow to illustrate the deviation from the\ninitial projection to the ground truth. EPnP algorithm within the RANdom SAmple\nConsensus (RANSAC) scheme is applied to estimate the extrinsic parameters with\n2D-3D correspondences constructed by the calibration flow. Experiments on KITTI\ndatasets demonstrate that our proposed method is superior to the\nstate-of-the-art methods. Furthermore, we propose a semantic initialization\nalgorithm with the introduction of instance centroids (ICs). The code will be\npublicly available at https://github.com/LvXudong-HIT/CFNet.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.02108,regular,pre_llm,2021,4,"{'ai_likelihood': 4.437234666612413e-06, 'text': 'Control of a Tail-Sitter VTOL UAV Based on Recurrent Neural Networks\n\n  Tail-sitter vertical takeoff and landing (VTOL) unmanned aerial vehicles\n(UAVs) have the capability of hovering and performing efficient level flight\nwith compact mechanical structures. We present a unified controller design for\nsuch UAVs, based on recurrent neural networks. An advantage of this design\nmethod is that the various flight modes (i.e., hovering, transition and level\nflight) of a VTOL UAV are controlled in a unified manner, as opposed to\ntreating them separately and in the runtime switching one from another. The\nproposed controller consists of an outer-loop position controller and an\ninner-loop attitude controller. The inner-loop controller is composed of a\nproportional attitude controller and a loop-shaping linear angular rate\ncontroller. For the outer-loop controller, we propose a nonlinear solver to\ncompute the desired attitude and thrust, based on the UAV dynamics and an\naerodynamic model, in addition to a cascaded PID controller for the position\nand velocity tracking. We employ a recurrent neural network (RNN) to\napproximate the behavior of the nonlinear solver, which suffers from high\ncomputational complexity. The proposed RNN has negligible approximation errors,\nand can be implemented in real-time (e.g., 50 Hz). Moreover, the RNN generates\nmuch smoother outputs than the nonlinear solver. We provide an analysis of the\nstability and robustness of the overall closed-loop system. Simulation and\nexperiments are also presented to demonstrate the effectiveness of the proposed\nmethod.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.0783,regular,pre_llm,2021,4,"{'ai_likelihood': 9.834766387939453e-06, 'text': 'Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in\n  Autonomous Vehicles\n\n  We present Pylot, a platform for autonomous vehicle (AV) research and\ndevelopment, built with the goal to allow researchers to study the effects of\nthe latency and accuracy of their models and algorithms on the end-to-end\ndriving behavior of an AV. This is achieved through a modular structure enabled\nby our high-performance dataflow system that represents AV software pipeline\ncomponents (object detectors, motion planners, etc.) as a dataflow graph of\noperators which communicate on data streams using timestamped messages. Pylot\nreadily interfaces with popular AV simulators like CARLA, and is easily\ndeployable to real-world vehicles with minimal code changes.\n  To reduce the burden of developing an entire pipeline for evaluating a single\ncomponent, Pylot provides several state-of-the-art reference implementations\nfor the various components of an AV pipeline. Using these reference\nimplementations, a Pylot-based AV pipeline is able to drive a real vehicle, and\nattains a high score on the CARLA Autonomous Driving Challenge. We also present\nseveral case studies enabled by Pylot, including evidence of a need for\ncontext-dependent components, and per-component time allocation. Pylot is open\nsource, with the code available at https://github.com/erdos-project/pylot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.12183,regular,pre_llm,2021,4,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'An Interval Branch-and-Bound-Based Inverse Kinemetics Algorithm Towards\n  Global Optimal Redundancy Resolution\n\n  The general inverse kinematics (IK) problem of a manipulator, namely that of\nacquiring the self-motion manifold (SMM) of all admissible joint angles for a\ndesired end-effector pose, plays a vital role in robotics modeling, planning\nand control. To efficiently solve the generalized IK, this paper proposes an\ninterval branch-and-bound-based approach, which is augmented with a fast\nnumerical IK-solver-enabled search heuristics. In comparison to independent\nsolutions generated by sampling based methods, our approach generates patches\nof neighboring solutions to provide richer information of the inherent geometry\nof the SMM for optimal planning and other applications. It can also be utilized\nin an anytime fashion to obtain solutions with sub-optimal resolution for\napplications within a limited period. The performance of our approach is\nverified by numerical experiments on both non-redundant and redundant\nmanipulators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.11827,regular,pre_llm,2021,4,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Semi-Autonomous Planning and Visualization in Virtual Reality\n\n  Virtual reality (VR) interfaces for robots provide a three-dimensional (3D)\nview of the robot in its environment, which allows people to better plan\ncomplex robot movements in tight or cluttered spaces. In our prior work, we\ncreated a VR interface to allow for the teleoperation of a humanoid robot. As\ndetailed in this paper, we have now focused on a human-in-the-loop planner\nwhere the operator can send higher level manipulation and navigation goals in\nVR through functional waypoints, visualize the results of a robot planner in\nthe 3D virtual space, and then deny, alter or confirm the plan to send to the\nrobot. In addition, we have adapted our interface to also work for a mobile\nmanipulation robot in addition to the humanoid robot. For a video demonstration\nplease see the accompanying video at https://youtu.be/wEHZug_fxrA.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.00279,regular,pre_llm,2021,4,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Efficient Set-Based Approaches for the Reliable Computation of Robot\n  Capabilities\n\n  To reliably model real robot characteristics, interval linear systems of\nequations allow to describe families of problems that consider sets of values.\nThis allows to easily account for typical complexities such as sets of joint\nstates and design parameter uncertainties. Inner approximations of the\nsolutions to the interval linear systems can be used to describe the common\ncapabilities of a robotic manipulator corresponding to the considered sets of\nvalues. In this work, several classes of problems are considered. For each\nclass, reliable and efficient polytope, n-cube, and n-ball inner approximations\nare presented. The interval approaches usually proposed are inefficient because\nthey are too computationally heavy for certain applications, such as control.\nWe propose efficient new inner approximation theorems for the considered\nclasses of problems. This allows for usage with real-time applications as well\nas rapid analysis of potential designs. Several applications are presented for\na redundant planar manipulator including locally evaluating the manipulator's\nvelocity, acceleration, and static force capabilities, and evaluating its\nfuture acceleration capabilities over a given time horizon.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.03203,regular,pre_llm,2021,4,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Predictive 3D Sonar Mapping of Underwater Environments via\n  Object-specific Bayesian Inference\n\n  Recent work has achieved dense 3D reconstruction with wide-aperture imaging\nsonar using a stereo pair of orthogonally oriented sonars. This allows each\nsonar to observe a spatial dimension that the other is missing, without\nrequiring any prior assumptions about scene geometry. However, this is achieved\nonly in a small region with overlapping fields-of-view, leaving large regions\nof sonar image observations with an unknown elevation angle. Our work aims to\nachieve large-scale 3D reconstruction more efficiently using this sensor\narrangement. We propose dividing the world into semantic classes to exploit the\npresence of repeating structures in the subsea environment. We use a Bayesian\ninference framework to build an understanding of each object class's geometry\nwhen 3D information is available from the orthogonal sonar fusion system, and\nwhen the elevation angle of our returns is unknown, our framework is used to\ninfer unknown 3D structure. We quantitatively validate our method in a\nsimulation and use data collected from a real outdoor littoral environment to\ndemonstrate the efficacy of our framework in the field. Video attachment:\nhttps://www.youtube.com/watch?v=WouCrY9eK4o&t=75s\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.09306,review,pre_llm,2021,4,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Ethical Challenges in the Human-Robot Interaction Field\n\n  The aim of this extended abstract is to introduce five examples of ethical\nissues in HRI that could have potential ethical implications, particularly on\nHRI participants. We consider these examples important to discuss in order to\nreach a consensus on how to handle them. Due to space limitations, this list is\nfar from exhaustive and we hope that it can lead to a wider discussion that\nstimulates HRI researchers to think ethically. Previous work has shown a trend\nof underreporting ethical conduct in the HRI field; in this extended abstract\nwe consider some of the ethical issues that could arise in HRI research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.01316,regular,pre_llm,2021,4,"{'ai_likelihood': 1.8676122029622398e-05, 'text': 'Towards Real-time Semantic RGB-D SLAM in Dynamic Environments\n\n  Most of the existing visual SLAM methods heavily rely on a static world\nassumption and easily fail in dynamic environments. Some recent works eliminate\nthe influence of dynamic objects by introducing deep learning-based semantic\ninformation to SLAM systems. However such methods suffer from high\ncomputational cost and cannot handle unknown objects. In this paper, we propose\na real-time semantic RGB-D SLAM system for dynamic environments that is capable\nof detecting both known and unknown moving objects. To reduce the computational\ncost, we only perform semantic segmentation on keyframes to remove known\ndynamic objects, and maintain a static map for robust camera tracking.\nFurthermore, we propose an efficient geometry module to detect unknown moving\nobjects by clustering the depth image into a few regions and identifying the\ndynamic regions via their reprojection errors. The proposed method is evaluated\non public datasets and real-world conditions. To the best of our knowledge, it\nis one of the first semantic RGB-D SLAM systems that run in real-time on a\nlow-power embedded platform and provide high localization accuracy in dynamic\nenvironments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.04563,regular,pre_llm,2021,4,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Context-Aware Task Handling in Resource-Constrained Robots with\n  Virtualization\n\n  Intelligent mobile robots are critical in several scenarios. However, as\ntheir computational resources are limited, mobile robots struggle to handle\nseveral tasks concurrently and yet guaranteeing real-timeliness. To address\nthis challenge and improve the real-timeliness of critical tasks under resource\nconstraints, we propose a fast context-aware task handling technique. To\neffectively handling tasks in real-time, our proposed context-aware technique\ncomprises of three main ingredients: (i) a dynamic time-sharing mechanism,\ncoupled with (ii) an event-driven task scheduling using reactive programming\nparadigm to mindfully use the limited resources; and, (iii) a lightweight\nvirtualized execution to easily integrate functionalities and their\ndependencies. We showcase our technique on a Raspberry-Pi-based robot with a\nvariety of tasks such as Simultaneous localization and mapping (SLAM), sign\ndetection, and speech recognition with a 42% speedup in total execution time\ncompared to the common Linux scheduler.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.07246,regular,pre_llm,2021,4,"{'ai_likelihood': 2.1192762586805556e-06, 'text': ""Human-in-the-Loop Deep Reinforcement Learning with Application to\n  Autonomous Driving\n\n  Due to the limited smartness and abilities of machine intelligence, currently\nautonomous vehicles are still unable to handle all kinds of situations and\ncompletely replace drivers. Because humans exhibit strong robustness and\nadaptability in complex driving scenarios, it is of great importance to\nintroduce humans into the training loop of artificial intelligence, leveraging\nhuman intelligence to further advance machine learning algorithms. In this\nstudy, a real-time human-guidance-based deep reinforcement learning (Hug-DRL)\nmethod is developed for policy training of autonomous driving. Leveraging a\nnewly designed control transfer mechanism between human and automation, human\nis able to intervene and correct the agent's unreasonable actions in real time\nwhen necessary during the model training process. Based on this\nhuman-in-the-loop guidance mechanism, an improved actor-critic architecture\nwith modified policy and value networks is developed. The fast convergence of\nthe proposed Hug-DRL allows real-time human guidance actions to be fused into\nthe agent's training loop, further improving the efficiency and performance of\ndeep reinforcement learning. The developed method is validated by\nhuman-in-the-loop experiments with 40 subjects and compared with other\nstate-of-the-art learning approaches. The results suggest that the proposed\nmethod can effectively enhance the training efficiency and performance of the\ndeep reinforcement learning algorithm under human guidance, without imposing\nspecific requirements on participant expertise and experience.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.01834,regular,pre_llm,2021,4,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Skyeye Team at MBZIRC 2020: A team of aerial and ground robots for\n  GPS-denied autonomous fire extinguishing in an urban building scenario\n\n  The paper presents a framework for fire extinguishing in an urban scenario by\na team of aerial and ground robots. The system was developed to address\nChallenge 3 of the 2020Mohamed Bin Zayed International Robotics Challenge\n(MBZIRC). The challenge required to autonomously detect, locate and extinguish\nfires on different floors of a building, as well as in its surroundings. The\nmulti-robot system developed consists of a heterogeneous robot team of up to\nthree Unmanned Aerial Vehicles (UAV) and one Unmanned Ground Vehicle (UGV). We\ndescribe the main hardware and software components for UAV and UGVplatforms and\nalso present the main algorithmic components of the system: a 3D LIDAR-based\nmapping and localization module able to work in GPS-denied scenarios; a global\nplanner and a fast local re-planning system for robot navigation;\ninfrared-based perception and robot actuation control for fire extinguishing;\nand a mission executive and coordination module based on Behavior Trees. The\npaper finally describes the results obtained during the competition, where the\nsystem worked fully autonomously and scored in all the trials performed. The\npresented system ended in 7th position out of 20 teams in the Challenge3\ncompetition and in 5th position (out of 17 teams) in the Challenge 3 entry to\nthe Grand Finale (Grand Challenge) of MBZIRC 2020 competition.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.03155,regular,pre_llm,2021,4,"{'ai_likelihood': 4.76837158203125e-06, 'text': ""Human-robot collaborative object transfer using human motion prediction\n  based on Cartesian pose Dynamic Movement Primitives\n\n  In this work, the problem of human-robot collaborative object transfer to\nunknown target poses is addressed. The desired pattern of the end-effector pose\ntrajectory to a known target pose is encoded using DMPs (Dynamic Movement\nPrimitives). During transportation of the object to new unknown targets, a\nDMP-based reference model and an EKF (Extended Kalman Filter) for estimating\nthe target pose and time duration of the human's intended motion is proposed. A\nstability analysis of the overall scheme is provided. Experiments using a Kuka\nLWR4+ robot equipped with an ATI sensor at its end-effector validate its\nefficacy with respect to the required human effort and compare it with an\nadmittance control scheme.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.10674,regular,pre_llm,2021,4,"{'ai_likelihood': 5.761782328287761e-06, 'text': 'Hierarchical Cross-Modal Agent for Robotics Vision-and-Language\n  Navigation\n\n  Deep Learning has revolutionized our ability to solve complex problems such\nas Vision-and-Language Navigation (VLN). This task requires the agent to\nnavigate to a goal purely based on visual sensory inputs given natural language\ninstructions. However, prior works formulate the problem as a navigation graph\nwith a discrete action space. In this work, we lift the agent off the\nnavigation graph and propose a more complex VLN setting in continuous 3D\nreconstructed environments. Our proposed setting, Robo-VLN, more closely mimics\nthe challenges of real world navigation. Robo-VLN tasks have longer trajectory\nlengths, continuous action spaces, and challenges such as obstacles. We provide\na suite of baselines inspired by state-of-the-art works in discrete VLN and\nshow that they are less effective at this task. We further propose that\ndecomposing the task into specialized high- and low-level policies can more\neffectively tackle this task. With extensive experiments, we show that by using\nlayered decision making, modularized training, and decoupling reasoning and\nimitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms\nexisting baselines in all key metrics and sets a new benchmark for Robo-VLN.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.14106,regular,pre_llm,2021,4,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Vehicular Teamwork: Collaborative localization of Autonomous Vehicles\n\n  This paper develops a distributed collaborative localization algorithm based\non an extended kalman filter. This algorithm incorporates Ultra-Wideband (UWB)\nmeasurements for vehicle to vehicle ranging, and shows improvements in\nlocalization accuracy where GPS typically falls short. The algorithm was first\ntested in a newly created open-source simulation environment that emulates\nvarious numbers of vehicles and sensors while simultaneously testing multiple\nlocalization algorithms. Predicted error distributions for various algorithms\nare quickly producible using the Monte-Carlo method and optimization techniques\nwithin MatLab. The simulation results were validated experimentally in an\noutdoor, urban environment. Improvements of localization accuracy over a\ntypical extended kalman filter ranged from 2.9% to 9.3% over 180 meter test\nruns. When GPS was denied, these improvements increased up to 83.3% over a\nstandard kalman filter. In both simulation and experimentally, the DCL\nalgorithm was shown to be a good approximation of a full state filter, while\nreducing required communication between vehicles. These results are promising\nin showing the efficacy of adding UWB ranging sensors to cars for collaborative\nand landmark localization, especially in GPS-denied environments. In the\nfuture, additional moving vehicles with additional tags will be tested in other\nchallenging GPS denied environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.12158,regular,pre_llm,2021,4,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Computing a Task-Dependent Grasp Metric Using Second Order Cone Programs\n\n  Evaluating a grasp generated by a set of hand-object contact locations is a\nkey component of many grasp planning algorithms. In this paper, we present a\nnovel second order cone program (SOCP) based optimization formulation for\nevaluating a grasps' ability to apply wrenches to generate a linear motion\nalong a given direction and/or an angular motion about the given direction. Our\nquality measure can be computed efficiently, since the SOCP is a convex\noptimization problem, which can be solved optimally with interior point\nmethods. A key feature of our approach is that we can consider the effect of\ncontact wrenches from any contact of the object with the environment. This is\ndifferent from the extant literature where only the effect of finger-object\ncontacts is considered. Exploiting the environmental contact is useful in many\nmanipulation scenarios either to enhance the dexterity of simple hands or\nimprove the payload capability of the manipulator. In contrast to most existing\napproaches, our approach also takes into account the practical constraint that\nthe maximum contact force that can be applied at a finger-object contact can be\ndifferent for each contact. We can also include the effect of external forces\nlike gravity, as well as the joint torque constraints of the\nfingers/manipulators. Furthermore, for a given motion path as a constant screw\nmotion or a sequence of constant screw motions, we can discretize the path and\ncompute a global grasp metric to accomplish the whole task with a chosen set of\nfinger-object contact locations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2104.03532,regular,pre_llm,2021,4,"{'ai_likelihood': 9.569856855604385e-06, 'text': 'An Equivariant Filter for Visual Inertial Odometry\n\n  Visual Inertial Odometry (VIO) is of great interest due the ubiquity of\ndevices equipped with both a monocular camera and Inertial Measurement Unit\n(IMU). Methods based on the extended Kalman Filter remain popular in VIO due to\ntheir low memory requirements, CPU usage, and processing time when compared to\noptimisation-based methods. In this paper, we analyse the VIO problem from a\ngeometric perspective and propose a novel formulation on a smooth quotient\nmanifold where the equivalence relationship is the well-known invariance of VIO\nto choice of reference frame. We propose a novel Lie group that acts\ntransitively on this manifold and is compatible with the visual measurements.\nThis structure allows for the application of Equivariant Filter (EqF) design\nleading to a novel filter for the VIO problem. Combined with a very simple\nvision processing front-end, the proposed filter demonstrates state-of-the-art\nperformance on the EuRoC dataset compared to other EKF-based VIO algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.03917,regular,pre_llm,2021,5,"{'ai_likelihood': 3.046459621853299e-06, 'text': ""Combining Time-Dependent Force Perturbations in Robot-Assisted Surgery\n  Training\n\n  Teleoperated robot-assisted minimally-invasive surgery (RAMIS) offers many\nadvantages over open surgery. However, there are still no guidelines for\ntraining skills in RAMIS. Motor learning theories have the potential to improve\nthe design of RAMIS training but they are based on simple movements that do not\nresemble the complex movements required in surgery. To fill this gap, we\ndesigned an experiment to investigate the effect of time-dependent force\nperturbations on the learning of a pattern-cutting surgical task. Thirty\nparticipants took part in the experiment: (1) a control group that trained\nwithout perturbations, and (2) a 1Hz group that trained with 1Hz periodic force\nperturbations that pushed each participant's hand inwards and outwards in the\nradial direction. We monitored their learning using four objective metrics and\nfound that participants in the 1Hz group learned how to overcome the\nperturbations and improved their performances during training without impairing\ntheir performances after the perturbations were removed. Our results present an\nimportant step toward understanding the effect of adding perturbations to RAMIS\ntraining protocols and improving RAMIS training for the benefit of surgeons and\npatients.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.03721,regular,pre_llm,2021,5,"{'ai_likelihood': 6.059805552164714e-06, 'text': 'Team Orienteering Coverage Planning with Uncertain Reward\n\n  Many municipalities and large organizations have fleets of vehicles that need\nto be coordinated for tasks such as garbage collection or infrastructure\ninspection. Motivated by this need, this paper focuses on the common subproblem\nin which a team of vehicles needs to plan coordinated routes to patrol an area\nover iterations while minimizing temporally and spatially dependent costs. In\nparticular, at a specific location (e.g., a vertex on a graph), we assume the\ncost grows linearly in expectation with an unknown rate, and the cost is reset\nto zero whenever any vehicle visits the vertex (representing the robot\nservicing the vertex). We formulate this problem in graph terminology and call\nit Team Orienteering Coverage Planning with Uncertain Reward (TOCPUR). We\npropose to solve TOCPUR by simultaneously estimating the accumulated cost at\nevery vertex on the graph and solving a novel variant of the Team Orienteering\nProblem (TOP) iteratively, which we call the Team Orienteering Coverage Problem\n(TOCP). We provide the first mixed integer programming formulation for the\nTOCP, as a significant adaptation of the original TOP. We introduce a new\nbenchmark consisting of hundreds of randomly generated graphs for comparing\ndifferent methods. We show the proposed solution outperforms both the exact TOP\nsolution and a greedy algorithm. In addition, we provide a demo of our method\non a team of three physical robots in a real-world environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.06896,review,pre_llm,2021,5,"{'ai_likelihood': 4.304779900444879e-06, 'text': ""Towards Sensor Data Abstraction of Autonomous Vehicle Perception Systems\n\nFull-stack autonomous driving perception modules usually consist of data-driven models based on multiple sensor modalities. However, these models might be biased to the sensor setup used for data acquisition. This bias can seriously impair the perception models' transferability to new sensor setups, which continuously occur due to the market's competitive nature. We envision sensor data abstraction as an interface between sensor data and machine learning applications for highly automated vehicles (HAD).\n  For this purpose, we review the primary sensor modalities, camera, lidar, and radar, published in autonomous-driving related datasets, examine single sensor abstraction and abstraction of sensor setups, and identify critical paths towards an abstraction of sensor data from multiple perception configurations."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.0068,regular,pre_llm,2021,5,"{'ai_likelihood': 4.265043470594618e-05, 'text': ""Viko: An Adaptive Gecko Gripper with Vision-based Tactile Sensor\n\n  Monitoring the state of contact is essential for robotic devices, especially\ngrippers that implement gecko-inspired adhesives where intimate contact is\ncrucial for a firm attachment. However, due to the lack of deformable sensors,\nfew have demonstrated tactile sensing for gecko grippers. We present Viko, an\nadaptive gecko gripper that utilizes vision-based tactile sensors to monitor\ncontact state. The sensor provides high-resolution real-time measurements of\ncontact area and shear force. Moreover, the sensor is adaptive, low-cost, and\ncompact. We integrated gecko-inspired adhesives into the sensor surface without\nimpeding its adaptiveness and performance. Using a robotic arm, we evaluate the\nperformance of the gripper by a series of grasping test. The gripper has a\nmaximum payload of 8N even at a low fingertip pitch angle of 30 degrees. We\nalso showcase the gripper's ability to adjust fingertip pose for better contact\nusing sensor feedback. Further, everyday object picking is presented as a\ndemonstration of the gripper's adaptiveness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.13838,regular,pre_llm,2021,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""It's your turn! -- A collaborative human-robot pick-and-place scenario\n  in a virtual industrial setting\n\n  In human-robot collaborative interaction scenarios, nonverbal communication\nplays an important role. Both, signals sent by a human collaborator need to be\nidentified and interpreted by the robotic system, and the signals sent by the\nrobot need to be identified and interpreted by the human. In this paper, we\nfocus on the latter. We implemented on an industrial robot in a VR environment\nnonverbal behavior signalling the user that it is now their turn to proceed\nwith a pick-and-place task. The signals were presented in four different test\nconditions: no signal, robot arm gesture, light signal, combination of robot\narm gesture and light signal. Test conditions were presented to the\nparticipants in two rounds. The qualitative analysis was conducted with focus\non (i) potential signals in human behaviour indicating why some participants\nimmediately took over from the robot whereas others needed more time to\nexplore, (ii) human reactions after the nonverbal signal of the robot, and\n(iii) whether participants showed different behaviours in the different test\nconditions. We could not identify potential signals why some participants were\nimmediately successful and others not. There was a bandwidth of behaviors after\nthe robot stopped working, e.g. participants rearranged the objects, looked at\nthe robot or the object, or gestured the robot to proceed. We found evidence\nthat robot deictic gestures were helpful for the human to correctly interpret\nwhat to do next. Moreover, there was a strong tendency that humans interpreted\nthe light signal projected on the robot's gripper as a request to give the\nobject in focus to the robot. Whereas a robot's pointing gesture at the object\nwas a strong trigger for the humans to look at the object.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.13409,regular,pre_llm,2021,5,"{'ai_likelihood': 3.940529293484158e-06, 'text': 'Foresight Social-aware Reinforcement Learning for Robot Navigation\n\n  When robots handle navigation tasks while avoiding collisions, they perform\nin crowded and complex environments not as good as in stable and homogeneous\nenvironments. This often results in a low success rate and poor efficiency.\nTherefore, we propose a novel Foresight Social-aware Reinforcement Learning\n(FSRL) framework for mobile robots to achieve collision-free navigation.\nCompared to previous learning-based methods, our approach is foresighted. It\nnot only considers the current human-robot interaction to avoid an immediate\ncollision, but also estimates upcoming social interactions to still keep\ndistance in the future. Furthermore, an efficiency constraint is introduced in\nour approach that significantly reduces navigation time. Comparative\nexperiments are performed to verify the effectiveness and efficiency of our\nproposed method under more realistic and challenging simulated environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14152,regular,pre_llm,2021,5,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Radar Odometry Combining Probabilistic Estimation and Unsupervised\n  Feature Learning\n\n  This paper presents a radar odometry method that combines probabilistic\ntrajectory estimation and deep learned features without needing groundtruth\npose information. The feature network is trained unsupervised, using only the\non-board radar data. With its theoretical foundation based on a data likelihood\nobjective, our method leverages a deep network for processing rich radar data,\nand a non-differentiable classic estimator for probabilistic inference. We\nprovide extensive experimental results on both the publicly available Oxford\nRadar RobotCar Dataset and an additional 100 km of driving collected in an\nurban setting. Our sliding-window implementation of radar odometry outperforms\nmost hand-crafted methods and approaches the current state of the art without\nrequiring a groundtruth trajectory for training. We also demonstrate the\neffectiveness of radar odometry under adverse weather conditions. Code for this\nproject can be found at: https://github.com/utiasASRL/hero_radar_odometry\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.10842,regular,pre_llm,2021,5,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Toolbox Spotter: A Computer Vision System for Real World Situational\n  Awareness in Heavy Industries\n\n  The majority of fatalities and traumatic injuries in heavy industries involve\nmobile plant and vehicles, often resulting from a lapse of attention or\ncommunication. Existing approaches to hazard identification include the use of\nhuman spotters, passive reversing cameras, non-differentiating proximity\nsensors and tag based systems. These approaches either suffer from problems of\nworker attention or require the use of additional devices on all workers and\nobstacles. Whilst computer vision detection systems have previously been\ndeployed in structured applications such as manufacturing and on-road vehicles,\nthere does not yet exist a robust and portable solution for use in unstructured\nenvironments like construction that effectively communicates risks to relevant\nworkers. To address these limitations, our solution, the Toolbox Spotter (TBS),\nacts to improve worker safety and reduce preventable incidents by employing an\nembedded robotic perception and distributed HMI alert system to augment both\ndetection and communication of hazards in safety critical environments. In this\npaper we outline the TBS safety system and evaluate its performance based on\ndata from real world implementations, demonstrating the suitability of the\nToolbox Spotter for applications in heavy industries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.00389,review,pre_llm,2021,5,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Multi-Robot Coordination and Planning in Uncertain and Adversarial\n  Environments\n\n  Deploying a team of robots that can carefully coordinate their actions can\nmake the entire system robust to individual failures. In this report, we review\nrecent algorithmic development in making multi-robot systems robust to\nenvironmental uncertainties, failures, and adversarial attacks.\n  We find the following three trends in the recent research in the area of\nmulti-robot coordination: (1) resilient coordination to either withstand\nfailures and/or attack or recover from failures/attacks; (2) risk-aware\ncoordination to manage the trade-off risk and reward, where the risk stems due\nto environmental uncertainty; (3) Graph Neural Networks based coordination to\nlearn decentralized multi-robot coordination policies. These algorithms have\nbeen applied to tasks such as formation control, task assignment and\nscheduling, search and planning, and informative data collection.\n  In order for multi-robot systems to become practical, we need coordination\nalgorithms that can scale to large teams of robots dealing with dynamically\nchanging, failure-prone, contested, and uncertain environments. There has been\nsignificant recent research on multi-robot coordination that has contributed\nresilient and risk-aware algorithms to deal with these issues and reduce the\ngap between theory and practice. Learning-based approaches have been seen to be\npromising, especially since they can learn who, when, and how to communicate\nfor effective coordination. However, these algorithms have also been shown to\nbe vulnerable to adversarial attacks, and as such developing learning-based\ncoordination strategies that are resilient to such attacks and robust to\nuncertainties is an important open area of research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.14499,review,pre_llm,2021,5,"{'ai_likelihood': 3.7087334526909726e-06, 'text': 'Perceived Safety in Physical Human Robot Interaction -- A Survey\n\n  This review paper focuses on different aspects of perceived safety for a\nnumber of autonomous physical systems. This is a major aspect of robotics\nresearch, as more and more applications allow human and autonomous systems to\nshare their space, with crucial implications both on safety and on its\nperception. The alternative terms used to express related concepts (e.g.,\npsychological safety, trust, comfort, stress, fear, and anxiety) are listed and\nexplained. Then, the available methods to assess perceived safety (i.e.,\nquestionnaires, physiological measurements, behavioral assessment, and direct\ninput devices) are described. Six categories of autonomous systems are\nconsidered (industrial manipulators, mobile robots, mobile manipulators,\nhumanoid robots, drones, and autonomous vehicles), providing an overview of the\nmain themes related to perceived safety in the specific domain, a description\nof selected works, and an analysis of how motion and characteristics of the\nsystem influence the perception of safety. The survey also discusses\nexperimental duration and location of the reviewed papers as well as identified\ntrends over time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.05161,regular,pre_llm,2021,5,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Towards Long-Distance Inspection for In-pipe Robots in Water\n  Distribution Systems with Smart Navigation\n\n  Incident in water distribution systems (WDS) cause water loss and water\ncontamination that requires the utility managers to assess the condition of\npipelines in a timely manner. However, pipelines are long and access to all\nparts of it is a chal-lenging task; current in-pipe robots have the limitations\nof short-distance inspection and inability to operate in-service networks. In\nthis work, we improve the design of our previously developed in-pipe robot and\nanalyze the effect of line pressure and relative velocity on the robot during\noperation with computational fluid dynamics (CFD) simulations. An extreme\nscenario for robot operation is defined and we estimate the minimum inspection\ndistance for the robot with one turn of battery charge that is 5400m. A\nmulti-phase motion controller is proposed that ensures reliable motion at\nstraight and non-straight configurations of pipeline and also stabilized\nconfiguration with zero velocity at junctions. We also propose a localization\nand navigation method based on particle filtering and combine it with the\nproposed multi-phase motion controller. In this method, the map of the\noperation is provided to the robot and the robot localizes itself based on the\nmap and the particle filtering method. Furthermore, the robot navigates\ndifferent configurations of pipelines by switching between different phases of\nthe motion controller algorithm that is performed by the particle filter\nalgorithm. The experiment and simulation results show that the robot along with\nthe navigation shows a promising solution towards long-distance inspection of\npipelines by in-pipe robots.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.11743,regular,pre_llm,2021,5,"{'ai_likelihood': 8.510218726264106e-06, 'text': 'Avoiding Dense and Dynamic Obstacles in Enclosed Spaces: Application to\n  Moving in Crowds\n\n  This paper presents a closed-form approach to constrain a flow within a given\nvolume and around objects. The flow is guaranteed to converge and to stop at a\nsingle fixed point. We show that the obstacle avoidance problem can be inverted\nto enforce that the flow remains enclosed within a volume defined by a\npolygonal surface. We formally guarantee that such a flow will never contact\nthe boundaries of the enclosing volume and obstacles, and will asymptotically\nconverge towards an attractor. We further create smooth motion fields around\nobstacles with edges (e.g. tables). Both obstacles and enclosures may be\ntime-varying, i.e. moving, expanding and shrinking. The technique enables a\nrobot to navigate within an enclosed corridor while avoiding static and moving\nobstacles. It was applied on an autonomous robot (QOLO) in a static complex\nindoor environment, and also tested in simulations with dense crowds. The final\nproof of concept was performed in an outdoor environment in Lausanne. The\nQOLO-robot successfully traversed a marketplace in the center of town in\npresence of a diverse crowd with a non-uniform motion pattern.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.03242,regular,pre_llm,2021,5,"{'ai_likelihood': 4.006756676567926e-06, 'text': ""Sobi: An Interactive Social Service Robot for Long-Term Autonomy in Open\n  Environments\n\n  Long-term autonomy in service robotics is a current research topic,\nespecially for dynamic, large-scale environments that change over time. We\npresent Sobi, a mobile service robot developed as an interactive guide for open\nenvironments, such as public places with indoor and outdoor areas. The robot\nwill serve as a platform for environmental modeling and human-robot\ninteraction. Its main hardware and software components, which we freely license\nas a documented open source project, are presented. Another key focus is Sobi's\nmonitoring system for long-term autonomy, which restores system components in a\ntargeted manner in order to extend the total system lifetime without unplanned\nintervention. We demonstrate first results of the long-term autonomous\ncapabilities in a 16-day indoor deployment, in which the robot patrols a total\nof 66.6 km with an average of 5.5 hours of travel time per weekday, charging\nautonomously in between. In a user study with 12 participants, we evaluate the\nappearance and usability of the user interface, which allows users to\ninteractively query information about the environment and directions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.12244,regular,pre_llm,2021,5,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'DiSECt: A Differentiable Simulation Engine for Autonomous Robotic\n  Cutting\n\n  Robotic cutting of soft materials is critical for applications such as food\nprocessing, household automation, and surgical manipulation. As in other areas\nof robotics, simulators can facilitate controller verification, policy\nlearning, and dataset generation. Moreover, differentiable simulators can\nenable gradient-based optimization, which is invaluable for calibrating\nsimulation parameters and optimizing controllers. In this work, we present\nDiSECt: the first differentiable simulator for cutting soft materials. The\nsimulator augments the finite element method (FEM) with a continuous contact\nmodel based on signed distance fields (SDF), as well as a continuous damage\nmodel that inserts springs on opposite sides of the cutting plane and allows\nthem to weaken until zero stiffness, enabling crack formation. Through various\nexperiments, we evaluate the performance of the simulator. We first show that\nthe simulator can be calibrated to match resultant forces and deformation\nfields from a state-of-the-art commercial solver and real-world cutting\ndatasets, with generality across cutting velocities and object instances. We\nthen show that Bayesian inference can be performed efficiently by leveraging\nthe differentiability of the simulator, estimating posteriors over hundreds of\nparameters in a fraction of the time of derivative-free methods. Finally, we\nillustrate that control parameters in the simulation can be optimized to\nminimize cutting forces via lateral slicing motions.\n  We publish videos and additional results on our project website at\nhttps://diff-cutting-sim.github.io.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.08958,regular,pre_llm,2021,5,"{'ai_likelihood': 4.76837158203125e-06, 'text': ""Active Visual SLAM with Independently Rotating Camera\n\n  In active Visual-SLAM (V-SLAM), a robot relies on the information retrieved\nby its cameras to control its own movements for autonomous mapping of the\nenvironment. Cameras are usually statically linked to the robot's body,\nlimiting the extra degrees of freedom for visual information acquisition. In\nthis work, we overcome the aforementioned problem by introducing and leveraging\nan independently rotating camera on the robot base. This enables us to\ncontinuously control the heading of the camera, obtaining the desired optimal\norientation for active V-SLAM, without rotating the robot itself. However, this\nadditional degree of freedom introduces additional estimation uncertainties,\nwhich need to be accounted for. We do this by extending our robot's state\nestimate to include the camera state and jointly estimate the uncertainties. We\ndevelop our method based on a state-of-the-art active V-SLAM approach for\nomnidirectional robots and evaluate it through rigorous simulation and real\nrobot experiments. We obtain more accurate maps, with lower energy consumption,\nwhile maintaining the benefits of the active approach with respect to the\nbaseline. We also demonstrate how our method easily generalizes to other\nnon-omnidirectional robotic platforms, which was a limitation of the previous\napproach. Code and implementation details are provided as open-source.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.03813,regular,pre_llm,2021,5,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Adaptive and Risk-Aware Target Tracking with Heterogeneous Robot Teams\n\n  We consider a scenario where a team of robots with heterogeneous sensors must\ntrack a set of hostile targets which induce sensory failures on the robots. In\nparticular, the likelihood of failures depends on the proximity between the\ntargets and the robots. We propose a control framework that implicitly\naddresses the competing objectives of performance maximization and sensor\npreservation (which impacts the future performance of the team). Our framework\nconsists of a predictive component -- which accounts for the risk of being\ndetected by the target, and a reactive component -- which maximizes the\nperformance of the team regardless of the failures that have already occurred.\nBased on a measure of the abundance of sensors in the team, our framework can\ngenerate aggressive and risk-averse robot configurations to track the targets.\nCrucially, the heterogeneous sensing capabilities of the robots are explicitly\nconsidered in each step, allowing for a more expressive risk-performance\ntrade-off. Simulated experiments with induced sensor failures demonstrate the\nefficacy of the proposed approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.05366,regular,pre_llm,2021,5,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Rearrangement on Lattices with Pick-n-Swaps: Optimality Structures and\n  Efficient Algorithms\n\n  We study a class of rearrangement problems under a novel pick-n-swap\nprehensile manipulation model, in which a robotic manipulator, capable of\ncarrying an item and making item swaps, is tasked to sort items stored in\nlattices of variable dimensions in a time-optimal manner. We systematically\nanalyze the intrinsic optimality structure, which is fairly rich and\nintriguing, under different levels of item distinguishability (fully labeled,\nwhere each item has a unique label, or partially labeled, where multiple items\nmay be of the same type) and different lattice dimensions. Focusing on the most\npractical setting of one and two dimensions, we develop low polynomial time\ncycle-following-based algorithms that optimally perform rearrangements on 1D\nlattices under both fully- and partially-labeled settings. On the other hand,\nwe show that rearrangement on 2D and higher-dimensional lattices become\ncomputationally intractable to optimally solve. Despite their NP-hardness, we\nprove that efficient cycle-following-based algorithms remain optimal in the\nasymptotic sense for 2D fully- and partially-labeled settings, in expectation,\nusing the interesting fact that random permutations induce only a small number\nof cycles. We further improve these algorithms to provide $1.x$-optimality when\nthe number of items is small. Simulation studies corroborate the effectiveness\nof our algorithms. The implementation of the algorithms from the paper can be\nfound at github.com/arc-l/lattice-rearrangement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.05484,regular,pre_llm,2021,5,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Learning a Skill-sequence-dependent Policy for Long-horizon Manipulation\n  Tasks\n\n  In recent years, the robotics community has made substantial progress in\nrobotic manipulation using deep reinforcement learning (RL). Effectively\nlearning of long-horizon tasks remains a challenging topic. Typical RL-based\nmethods approximate long-horizon tasks as Markov decision processes and only\nconsider current observation (images or other sensor information) as input\nstate. However, such approximation ignores the fact that skill-sequence also\nplays a crucial role in long-horizon tasks. In this paper, we take both the\nobservation and skill sequences into account and propose a\nskill-sequence-dependent hierarchical policy for solving a typical long-horizon\ntask. The proposed policy consists of a high-level skill policy (utilizing\nskill sequences) and a low-level parameter policy (responding to observation)\nwith corresponding training methods, which makes the learning much more\nsample-efficient. Experiments in simulation demonstrate that our approach\nsuccessfully solves a long-horizon task and is significantly faster than\nProximal Policy Optimization (PPO) and the task schema methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.12983,regular,pre_llm,2021,5,"{'ai_likelihood': 4.7021441989474825e-06, 'text': 'Correlation Filter of 2D Laser Scans For Indoor Environment\n\n  Modern laser SLAM (simultaneous localization and mapping) and structure from\nmotion algorithms face the problem of processing redundant data. Even if a\nsensor does not move, it still continues to capture scans that should be\nprocessed. This paper presents the novel filter that allows dropping 2D scans\nthat bring no new information to the system. Experiments on MIT and TUM\ndatasets show that it is possible to drop more than half of the scans. Moreover\nthepaper describes the formulas that enable filter adaptation to a particular\nrobot with known speed and characteristics of lidar. In addition, the indoor\ncorridor detector is introduced that also can be applied to any specific shape\nof a corridor and sensor.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2105.00546,regular,pre_llm,2021,5,"{'ai_likelihood': 2.3974312676323785e-05, 'text': 'GODSAC*: Graph Optimized DSAC* for Robot Relocalization\n\n  Deep learning based camera pose estimation from monocular camera images has\nseen a recent uptake in Visual SLAM research. Even though such pose estimation\napproaches have excellent results in small confined areas like offices and\napartment buildings, they tend to do poorly when applied to larger areas like\noutdoor settings, mainly because of the scarcity of distinctive features. We\npropose GODSAC* as a camera pose estimation approach that augments pose\npredictions from a trained neural network with noisy odometry data through the\noptimization of a pose graph. GODSAC* outperforms the state-of-the-art\napproaches in pose estimation accuracy, as we demonstrate in our experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07182,regular,pre_llm,2021,6,"{'ai_likelihood': 5.364418029785156e-06, 'text': ""Deploying COTS Legged Robot Platforms into a Heterogeneous Robot Team\n\n  The recent availability of commercial-off-the-shelf (COTS) legged robot\nplatforms have opened up new opportunities in deploying legged systems into\ndifferent scenarios. While the main advantage of legged robots is their ability\nto traverse unstructured terrain, there are still large gaps between what robot\nplatforms can achieve and their animal counterparts. Therefore, when deploying\nas part of a heterogeneous robot team of different platforms, it is beneficial\nto understand the different scenarios where a legged platform would perform\nbetter than a wheeled, tracked or aerial platform. Two COTS quadruped robots,\nGhost Robotics' Vision 60 and Boston Dynamics' Spot, were deployed into a\nheterogeneous team. A description of some of the challenges faced while\nintegrating the platforms, as well as some experiments in traversing different\nterrains are provided to give insight into the real-world deployment of legged\nrobots.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.00534,regular,pre_llm,2021,6,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'DeepWalk: Omnidirectional Bipedal Gait by Deep Reinforcement Learning\n\n  Bipedal walking is one of the most difficult but exciting challenges in\nrobotics. The difficulties arise from the complexity of high-dimensional\ndynamics, sensing and actuation limitations combined with real-time and\ncomputational constraints. Deep Reinforcement Learning (DRL) holds the promise\nto address these issues by fully exploiting the robot dynamics with minimal\ncraftsmanship. In this paper, we propose a novel DRL approach that enables an\nagent to learn omnidirectional locomotion for humanoid (bipedal) robots.\nNotably, the locomotion behaviors are accomplished by a single control policy\n(a single neural network). We achieve this by introducing a new curriculum\nlearning method that gradually increases the task difficulty by scheduling\ntarget velocities. In addition, our method does not require reference motions\nwhich facilities its application to robots with different kinematics, and\nreduces the overall complexity. Finally, different strategies for sim-to-real\ntransfer are presented which allow us to transfer the learned policy to a real\nhumanoid robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07161,regular,pre_llm,2021,6,"{'ai_likelihood': 2.682209014892578e-06, 'text': ""Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent\n  Trajectory Prediction\n\n  Simultaneous trajectory prediction for multiple heterogeneous traffic\nparticipants is essential for the safe and efficient operation of connected\nautomated vehicles under complex driving situations in the real world. The\nmulti-agent prediction task is challenging, as the motions of traffic\nparticipants are affected by many factors, including their individual dynamics,\ntheir interactions with surrounding agents, the traffic infrastructures, and\nthe number and modalities of the target agents. To further advance the\ntrajectory prediction techniques, in this work we propose a three-channel\nframework together with a novel Heterogeneous Edge-enhanced graph ATtention\nnetwork (HEAT), which is able to deal with the heterogeneity of the target\nagents and traffic participants involved. Specifically, the agent's dynamics\nare extracted from their historical states using type-specific encoders. The\ninter-agent interactions are represented with a directed edge-featured\nheterogeneous graph, and then interaction features are extracted using the\nproposed HEAT network. Besides, the map features are shared across all agents\nby introducing a selective gate mechanism. And finally, the trajectories of\nmulti-agent are executed simultaneously. Validations using both urban and\nhighway driving datasets show that the proposed model can realize simultaneous\ntrajectory predictions for multiple agents under complex traffic situations,\nand achieve state-of-the-art performance with respect to prediction accuracy,\ndemonstrating its feasibility and effectiveness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.14832,regular,pre_llm,2021,6,"{'ai_likelihood': 1.3907750447591147e-06, 'text': ""The role of reciprocity in human-robot social influence\n\n  Humans are constantly influenced by others' behavior and opinions. Of\nimportance, social influence among humans is shaped by reciprocity: we follow\nmore the advice of someone who has been taking into consideration our opinions.\nIn the current work, we investigate whether reciprocal social influence can\nemerge while interacting with a social humanoid robot. In a joint task, a human\nparticipant and a humanoid robot made perceptual estimates and then could\novertly modify them after observing the partner's judgment. Results show that\nendowing the robot with the ability to express and modulate its own level of\nsusceptibility to the human's judgments represented a double-edged sword. On\nthe one hand, participants lost confidence in the robot's competence when the\nrobot was following their advice; on the other hand, participants were\nunwilling to disclose their lack of confidence to the susceptible robot,\nsuggesting the emergence of reciprocal mechanisms of social influence\nsupporting human-robot collaboration.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07127,regular,pre_llm,2021,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Transition Motion Planning for Multi-Limbed Vertical Climbing Robots\n  Using Complementarity Constraints\n\n  In order to achieve autonomous vertical wall climbing, the transition phase\nfrom the ground to the wall requires extra consideration inevitably. This paper\nfocuses on the contact sequence planner to transition between flat terrain and\nvertical surfaces for multi-limbed climbing robots. To overcome the transition\nphase, it requires planning both multi-contact and contact wrenches\nsimultaneously which makes it difficult. Instead of using a predetermined\ncontact sequence, we consider various motions on different environment setups\nvia modeling contact constraints and limb switchability as complementarity\nconditions. Two safety factors for toe sliding and motor over-torque are the\nmain tuning parameters for different contact sequences. By solving as a\nnonlinear program (NLP), we can generate several feasible sequences of foot\nplacements and contact forces to avoid failure cases. We verified feasibility\nwith demonstrations on the hardware SiLVIA, a six-legged robot capable of\nvertically climbing between two walls by bracing itself in-between using only\nfriction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.00898,regular,pre_llm,2021,6,"{'ai_likelihood': 8.145968119303387e-06, 'text': 'Trajectory Optimization for Manipulation of Deformable Objects: Assembly\n  of Belt Drive Units\n\n  This paper presents a novel trajectory optimization formulation to solve the\nrobotic assembly of the belt drive unit. Robotic manipulations involving\ncontacts and deformable objects are challenging in both dynamic modeling and\ntrajectory planning. For modeling, variations in the belt tension and contact\nforces between the belt and the pulley could dramatically change the system\ndynamics. For trajectory planning, it is computationally expensive to plan\ntrajectories for such hybrid dynamical systems as it usually requires planning\nfor discrete modes separately. In this work, we formulate the belt drive unit\nassembly task as a trajectory optimization problem with complementarity\nconstraints to avoid explicitly imposing contact mode sequences. The problem is\nsolved as a mathematical program with complementarity constraints (MPCC) to\nobtain feasible and efficient assembly trajectories. We validate the proposed\nmethod both in simulations with a physics engine and in real-world experiments\nwith a robotic manipulator.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.02535,regular,pre_llm,2021,6,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Flying with Cartographer: Adapting the Cartographer 3D Graph SLAM Stack\n  for UAV Navigation\n\n  This paper describes an application of the Cartographer graph SLAM stack as a\npose sensor in a UAV feedback control loop, with certain application-specific\nchanges in the SLAM stack such as smoothing of the optimized pose. Pose\nestimation is performed by fusing 3D LiDAR/IMU-based proprioception with GPS\nposition measurements by means of pose graph optimisation. Moreover, partial\nenvironment maps built from the LiDAR data (submaps) within the Cartographer\nSLAM stack are marshalled into OctoMap, an Octree-based voxel map\nimplementation. The OctoMap is further used for navigation tasks such as path\nplanning and obstacle avoidance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.09508,regular,pre_llm,2021,6,"{'ai_likelihood': 6.755193074544271e-06, 'text': ""KIT Bus: A Shuttle Model for CARLA Simulator\n\n  With the continuous development of science and technology, self-driving\nvehicles will surely change the nature of transportation and realize the\nautomotive industry's transformation in the future. Compared with self-driving\ncars, self-driving buses are more efficient in carrying passengers and more\nenvironmentally friendly in terms of energy consumption. Therefore, it is\nspeculated that in the future, self-driving buses will become more and more\nimportant. As a simulator for autonomous driving research, the CARLA simulator\ncan help people accumulate experience in autonomous driving technology faster\nand safer. However, a shortcoming is that there is no modern bus model in the\nCARLA simulator. Consequently, people cannot simulate autonomous driving on\nbuses or the scenarios interacting with buses. Therefore, we built a bus model\nin 3ds Max software and imported it into the CARLA to fill this gap. Our model,\nnamely KIT bus, is proven to work in the CARLA by testing it with the autopilot\nsimulation. The video demo is shown on our Youtube.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.09257,regular,pre_llm,2021,6,"{'ai_likelihood': 7.781717512342666e-06, 'text': 'Learning Robot Exploration Strategy with 4D Point-Clouds-like\n  Information as Observations\n\n  Being able to explore unknown environments is a requirement for fully\nautonomous robots. Many learning-based methods have been proposed to learn an\nexploration strategy. In the frontier-based exploration, learning algorithms\ntend to learn the optimal or near-optimal frontier to explore. Most of these\nmethods represent the environments as fixed size images and take these as\ninputs to neural networks. However, the size of environments is usually\nunknown, which makes these methods fail to generalize to real world scenarios.\nTo address this issue, we present a novel state representation method based on\n4D point-clouds-like information, including the locations, frontier, and\ndistance information. We also design a neural network that can process these 4D\npoint-clouds-like information and generate the estimated value for each\nfrontier. Then this neural network is trained using the typical reinforcement\nlearning framework. We test the performance of our proposed method by comparing\nit with other five methods and test its scalability on a map that is much\nlarger than maps in the training set. The experiment results demonstrate that\nour proposed method needs shorter average traveling distances to explore whole\nenvironments and can be adopted in maps with arbitrarily sizes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.06617,regular,pre_llm,2021,6,"{'ai_likelihood': 1.215272479587131e-05, 'text': 'Inexact Loops in Robotics Problems\n\n  Loops are pervasive in robotics problems, appearing in mapping and\nlocalization, where one is interested in finding loop closure constraints to\nbetter approximate robot poses or other estimated quantities, as well as\nplanning and prediction, where one is interested in the homotopy classes of the\nspace through which a robot is moving. We generalize the standard topological\ndefinition of a loop to cases where a trajectory passes close to itself, but\ndoesn\'t necessarily touch, giving a definition that is more practical for real\nrobotics problems. This relaxation leads to new and useful properties of\ninexact loops, such as their ability to be partitioned into topologically\nconnected sets closely matching the concept of a ""loop closure"", and the\nexistence of simple and nonsimple loops. Building from these ideas, we\nintroduce several ways to measure properties and quantities of inexact loops on\na trajectory, such as the trajectory\'s ""loop area"" and ""loop density"", and use\nthem to compare strategies for sampling representative inexact loops to build\nconstraints in mapping and localization problems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.06783,regular,pre_llm,2021,6,"{'ai_likelihood': 1.0000334845648871e-05, 'text': ""Lvio-Fusion: A Self-adaptive Multi-sensor Fusion SLAM Framework Using\n  Actor-critic Method\n\n  State estimation with sensors is essential for mobile robots. Due to\ndifferent performance of sensors in different environments, how to fuse\nmeasurements of various sensors is a problem. In this paper, we propose a\ntightly coupled multi-sensor fusion framework, Lvio-Fusion, which fuses stereo\ncamera, Lidar, IMU, and GPS based on the graph optimization. Especially for\nurban traffic scenes, we introduce a segmented global pose graph optimization\nwith GPS and loop-closure, which can eliminate accumulated drifts.\nAdditionally, we creatively use a actor-critic method in reinforcement learning\nto adaptively adjust sensors' weight. After training, actor-critic agent can\nprovide the system better and dynamic sensors' weight. We evaluate the\nperformance of our system on public datasets and compare it with other\nstate-of-the-art methods, which shows that the proposed method achieves high\nestimation accuracy and robustness to various environments. And our\nimplementations are open source and highly scalable.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.03923,regular,pre_llm,2021,6,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Acoustic Power Management by Swarms of Microscopic Robots\n\n  Microscopic robots in the body could harvest energy from ultrasound to\nprovide on-board control of autonomous behaviors such as measuring and\ncommunicating diagnostic information and precisely delivering drugs. This paper\nevaluates the acoustic power available to micron-size robots that collect\nenergy using pistons. Acoustic attenuation and viscous drag on the pistons are\nthe major limitations on the available power. Frequencies around 100kHz can\ndeliver hundreds of picowatts to a robot in low-attenuation tissue within about\n10cm of transducers on the skin, but much less in high-attenuation tissue such\nas a lung. However, applications of microscopic robots could involve such large\nnumbers that the robots significantly increase attenuation, thereby reducing\npower for robots deep in the body. This paper describes how robots can\ncollectively manage where and when they harvest energy to mitigate this\nattenuation so that a swarm of a few hundred billion robots can provide tens of\npicowatts to each robot, on average.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.08129,review,pre_llm,2021,6,"{'ai_likelihood': 7.682376437717014e-06, 'text': 'Human movement augmentation and how to make it a reality\n\n  Augmenting the body with artificial limbs controlled concurrently to the\nnatural limbs has long appeared in science fiction, but recent technological\nand neuroscientific advances have begun to make this vision possible. By\nallowing individuals to achieve otherwise impossible actions, this movement\naugmentation could revolutionize medical and industrial applications and\nprofoundly change the way humans interact with their environment. Here, we\nconstruct a movement augmentation taxonomy through what is augmented and how it\nis achieved. With this framework, we analyze augmentation that extends the\nnumber of degrees-of-freedom, discuss critical features of effective\naugmentation such as physiological control signals, sensory feedback and\nlearning, and propose a vision for the field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.02737,regular,pre_llm,2021,6,"{'ai_likelihood': 2.2881560855441625e-05, 'text': ""Negotiation-Aware Reachability-Based Safety Verification for\n  AutonomousDriving in Interactive Scenarios\n\n  Safety assurance is a critical yet challenging aspect when developing\nself-driving technologies. Hamilton-Jacobi backward-reachability analysis is a\nformal verification tool for verifying the safety of dynamic systems in the\npresence of disturbances. However, the standard approach is too conservative to\nbe applied to self-driving applications due to its worst-case assumption on\nhumans' behaviors (i.e., guard against worst-case outcomes). In this work, we\nintegrate a learning-based prediction algorithm and a game-theoretic human\nbehavioral model to online update the conservativeness of backward-reachability\nanalysis. We evaluate our approach using real driving data. The results show\nthat, with reasonable assumptions on human behaviors, our approach can\neffectively reduce the conservativeness of the standard approach without\nsacrificing its safety verification ability.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.00805,regular,pre_llm,2021,6,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Lattices of sensors reconsidered when less information is preferred\n\n  To treat sensing limitations (with uncertainty in both conflation of\ninformation and noise) we model sensors as covers. This leads to a semilattice\norganization of abstract sensors that is appropriate even when additional\ninformation is problematic (e.g., for tasks involving privacy considerations).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.03648,regular,pre_llm,2021,6,"{'ai_likelihood': 3.662374284532335e-05, 'text': 'Cost-effective Mapping of Mobile Robot Based on the Fusion of UWB and\n  Short-range 2D LiDAR\n\n  Environment mapping is an essential prerequisite for mobile robots to perform\ndifferent tasks such as navigation and mission planning. With the availability\nof low-cost 2D LiDARs, there are increasing applications of such 2D LiDARs in\nindustrial environments. However, environment mapping in an unknown and\nfeature-less environment with such low-cost 2D LiDARs remains a challenge. The\nchallenge mainly originates from the short-range of LiDARs and complexities in\nperforming scan matching in these environments. In order to resolve these\nshortcomings, we propose to fuse the ultra-wideband (UWB) with 2D LiDARs to\nimprove the mapping quality of a mobile robot. The optimization-based approach\nis utilized for the fusion of UWB ranging information and odometry to first\noptimize the trajectory. Then the LiDAR-based loop closures are incorporated to\nimprove the accuracy of the trajectory estimation. Finally, the optimized\ntrajectory is combined with the LiDAR scans to produce the occupancy map of the\nenvironment. The performance of the proposed approach is evaluated in an indoor\nfeature-less environment with a size of 20m*20m. Obtained results show that the\nmapping error of the proposed scheme is 85.5% less than that of the\nconventional GMapping algorithm with short-range LiDAR (for example Hokuyo\nURG-04LX in our experiment with a maximum range of 5.6m).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07125,regular,pre_llm,2021,6,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Variational Policy Search using Sparse Gaussian Process Priors for\n  Learning Multimodal Optimal Actions\n\n  Policy search reinforcement learning has been drawing much attention as a\nmethod of learning a robot control policy. In particular, policy search using\nsuch non-parametric policies as Gaussian process regression can learn optimal\nactions with high-dimensional and redundant sensors as input. However, previous\nmethods implicitly assume that the optimal action becomes unique for each\nstate. This assumption can severely limit such practical applications as robot\nmanipulations since designing a reward function that appears in only one\noptimal action for complex tasks is difficult. The previous methods might have\ncaused critical performance deterioration because the typical non-parametric\npolicies cannot capture the optimal actions due to their unimodality. We\npropose novel approaches in non-parametric policy searches with multiple\noptimal actions and offer two different algorithms commonly based on a sparse\nGaussian process prior and variational Bayesian inference. The following are\nthe key ideas: 1) multimodality for capturing multiple optimal actions and 2)\nmode-seeking for capturing one optimal action by ignoring the others. First, we\npropose a multimodal sparse Gaussian process policy search that uses multiple\noverlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian\nprocess policy search that uses the student-t distribution for a likelihood\nfunction. The effectiveness of those algorithms is demonstrated through\napplications to object manipulation tasks with multiple optimal actions in\nsimulations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.07011,regular,pre_llm,2021,6,"{'ai_likelihood': 2.3179584079318578e-05, 'text': 'Underwater Soft Robotic Hand with Multi-Source Coupling Bio-Inspired\n  Soft Palm and Six Fingers Driven by Water Hydraulic\n\n  A new fluid-driven soft robot hand in this study uses the idea of the bionics\nand has the anthropomorphic form, which is oriented to the flexible grasp\nfunction. The soft robot hand is composed of a new kind of multi-freedom soft\nfinger and soft palm, which realizes the characteristic grasping function of\nforehand and backhand. Combined with the fine fluid control system, the soft\nhand can realize flexible grasping under high pressure, so as to realize\nflexible grasping operation for different types of target objects in the\nunderwater environment. The soft robot hand was controlled based on water\nhydraulic platform, Finally, the soft robot hand and the fine fluid control\nsystem were connected to form the underwater soft robot hand experiment\nplatform.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.01842,regular,pre_llm,2021,6,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""The dynamic effect of mechanical losses of transmissions on the equation\n  of motion of legged robots\n\n  Industrial manipulators do not collapse under their own weight when powered\noff due to the friction in their joints. Although these mechanism are effective\nfor stiff position control of pick-and-place, they are inappropriate for legged\nrobots that must rapidly regulate compliant interactions with the environment.\nHowever, no metric exists to quantify the robot's performance degradation due\nto mechanical losses in the actuators and transmissions. This paper provides a\nfundamental formulation that uses the mechanical efficiency of transmissions to\nquantify the effect of power losses in the mechanical transmissions on the\ndynamics of a whole robotic system. We quantitatively demonstrate the intuitive\nfact that the apparent inertia of the robots increase in the presence of joint\nfriction. We also show that robots that employ high gear ratio and low\nefficiency transmissions can statically sustain more substantial external\nloads. We expect that the framework presented here will provide the fundamental\ntools for designing the next generation of legged robots that can effectively\ninteract with the world.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2106.09219,regular,pre_llm,2021,6,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Decentralised Intelligence, Surveillance, and Reconnaissance in Unknown\n  Environments with Heterogeneous Multi-Robot Systems\n\n  We present the design and implementation of a decentralised, heterogeneous\nmulti-robot system for performing intelligence, surveillance and reconnaissance\n(ISR) in an unknown environment. The team consists of functionally specialised\nrobots that gather information and others that perform a mission-specific task,\nand is coordinated to achieve simultaneous exploration and exploitation in the\nunknown environment. We present a practical implementation of such a system,\nincluding decentralised inter-robot localisation, mapping, data fusion and\ncoordination. The system is demonstrated in an efficient distributed\nsimulation. We also describe an UAS platform for hardware experiments, and the\nongoing progress.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.01507,regular,pre_llm,2021,7,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Mission-level Robustness with Rapidly-deployed, Autonomous Aerial\n  Vehicles by Carnegie Mellon Team Tartan at MBZIRC 2020\n\n  For robotic systems to succeed in high risk, real-world situations, they have\nto be quickly deployable and robust to environmental changes, under-performing\nhardware, and mission subtask failures. These robots are often designed to\nconsider a single sequence of mission events, with complex algorithms lowering\nindividual subtask failure rates under some critical constraints. Our approach\nutilizes common techniques in vision and control, and encodes robustness into\nmission structure through outcome monitoring and recovery strategies. In\naddition, our system infrastructure enables rapid deployment and requires no\ncentral communication. This report also includes lessons in rapid field robotic\ndevelopment and testing. We developed and evaluated our systems through\nreal-robot experiments at an outdoor test site in Pittsburgh, Pennsylvania,\nUSA, as well as in the 2020 Mohamed Bin Zayed International Robotics Challenge.\nAll competition trials were completed in fully autonomous mode without RTK-GPS.\nOur system placed fourth in Challenge 2 and seventh in the Grand Challenge,\nwith notable achievements such as popping five balloons (Challenge 1),\nsuccessfully picking and placing a block (Challenge 2), and dispensing the most\nwater onto an outdoor, real fire with an autonomous UAV (Challenge 3).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.06116,regular,pre_llm,2021,7,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'A Novel Dual Quaternion Based Dynamic Motion Primitives for Acrobatic\n  Flight\n\n  The realization of motion description is a challenging work for fixed-wing\nUnmanned Aerial Vehicle (UAV) acrobatic flight, due to the inherent coupling\nproblem in ranslational-rotational motion. This paper aims to develop a novel\nmaneuver description method through the idea of imitation learning, and there\nare two main contributions of our work: 1) A dual quaternion based dynamic\nmotion primitives (DQ-DMP) is proposed and the state equations of the position\nand attitude can be combined without loss of accuracy. 2) An online\nhardware-inthe-loop (HITL) training system is established. Based on the DQDMP\nmethod, the geometric features of the demonstrated maneuver can be obtained in\nreal-time, and the stability of the DQ-DMP is theoretically proved. The\nsimulation results illustrate the superiority of the proposed method compared\nto the traditional position/attitude decoupling method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.07712,regular,pre_llm,2021,7,"{'ai_likelihood': 2.185503641764323e-06, 'text': ""LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping\n\n  Long-term 3D map management is a fundamental capability required by a robot\nto reliably navigate in the non-stationary real-world. This paper develops\nopen-source, modular, and readily available LiDAR-based lifelong mapping for\nurban sites. This is achieved by dividing the problem into successive\nsubproblems: multi-session SLAM (MSS), high/low dynamic change detection, and\npositive/negative change management. The proposed method leverages MSS and\nhandles potential trajectory error; thus, good initial alignment is not\nrequired for change detection. Our change management scheme preserves efficacy\nin both memory and computation costs, providing automatic object segregation\nfrom a large-scale point cloud map. We verify the framework's reliability and\napplicability even under permanent year-level variation, through extensive\nreal-world experiments with multiple temporal gaps (from day to year).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.02632,regular,pre_llm,2021,7,"{'ai_likelihood': 6.788306766086155e-06, 'text': 'Best Axes Composition: Multiple Gyroscopes IMU Sensor Fusion to Reduce\n  Systematic Error\n\n  In this paper, we propose an algorithm to combine multiple cheap Inertial\nMeasurement Unit (IMU) sensors to calculate 3D-orientations accurately. Our\napproach takes into account the inherent and non-negligible systematic error in\nthe gyroscope model and provides a solution based on the error observed during\nprevious instants of time. Our algorithm, the Best Axes Composition (BAC),\nchooses dynamically the most fitted axes among IMUs to improve the estimation\nperformance. We compare our approach with a probabilistic Multiple IMU (MIMU)\napproach, and we validate our algorithm in our collected dataset. As a result,\nit only takes as few as 2 IMUs to significantly improve accuracy, while other\nMIMU approaches need a higher number of sensors to achieve the same results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.12285,regular,pre_llm,2021,7,"{'ai_likelihood': 2.7484363979763457e-05, 'text': 'OpenFish: Biomimetic Design of a Soft Robotic Fish for High Speed\n  Locomotion\n\n  We present OpenFish: an open source soft robotic fish which is optimized for\nspeed and efficiency. The soft robotic fish uses a combination of an active and\npassive tail segment to accurately mimic the thunniform swimming mode. Through\nthe implementation of a novel propulsion system that is capable of achieving\nhigher oscillation frequencies with a more sinusoidal waveform, the open source\nsoft robotic fish achieves a top speed of $0.85~\\mathrm{m/s}$. Hereby, it\noutperforms the previously reported fastest soft robotic fish by $27\\%$.\nBesides the propulsion system, the optimization of the fish morphology played a\ncrucial role in achieving this speed. In this work, a detailed description of\nthe design, construction and customization of the soft robotic fish is\npresented. Hereby, we hope this open source design will accelerate future\nresearch and developments in soft robotic fish.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.14662,regular,pre_llm,2021,7,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Marine Locomotion: A Tethered UAV$-$Buoy System with Surge Velocity\n  Control\n\n  Unmanned aerial vehicles (UAVs) are reaching offshore. In this work, we\nformulate the novel problem of a marine locomotive quadrotor UAV, which\nmanipulates the surge velocity of a floating buoy by means of a cable. The\nproposed robotic system can have a variety of novel applications for UAVs where\ntheir high speed and maneuverability, as well as their ease of deployment and\nwide field of vision, give them a superior advantage. In addition, the major\nlimitation of limited flight time of quadrotor UAVs is typically addressed\nthrough an umbilical power cable, which naturally integrates with the proposed\nsystem. A detailed high-fidelity dynamic model is presented for the buoy, UAV,\nand water environment. In addition, a stable control system design is proposed\nto manipulate the surge velocity of the buoy within certain constraints that\nkeep the buoy in contact with the water surface. Polar coordinates are used in\nthe controller design process since they outperform traditional Cartesian-based\nvelocity controllers when it comes to ensuring correlated effects on the\ntracking performance, where each control channel independently affects one\ncontrol parameter. The system model and controller design are validated in\nnumerical simulation under different wave scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00324,regular,pre_llm,2021,7,"{'ai_likelihood': 5.563100179036459e-06, 'text': 'Robotic Template Library\n\n  Robotic Template Library (RTL) is a set of tools for dealing with geometry\nand point cloud processing, especially in robotic applications. The software\npackage covers basic objects such as vectors, line segments, quaternions, rigid\ntransformations, etc., however, its main contribution lies in the more advanced\nmodules: The segmentation module for batch or stream clustering of point\nclouds, the fast vectorization module for approximation of continuous point\nclouds by geometric objects of higher grade and the LaTeX export module\nenabling automated generation of high-quality visual outputs. It is a\nheader-only library written in C++17, uses the Eigen library as a linear\nalgebra back-end, and is designed with high computational performance in mind.\nRTL can be used in all robotic tasks such as motion planning, map building,\nobject recognition and many others, but the point cloud processing utilities\nare general enough to be employed in any field touching object reconstruction\nand computer vision applications as well.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.0385,regular,pre_llm,2021,7,"{'ai_likelihood': 9.271833631727431e-07, 'text': ""Navigate-and-Seek: a Robotics Framework for People Localization in\n  Agricultural Environments\n\n  The agricultural domain offers a working environment where many human\nlaborers are nowadays employed to maintain or harvest crops, with huge\npotential for productivity gains through the introduction of robotic\nautomation. Detecting and localizing humans reliably and accurately in such an\nenvironment, however, is a prerequisite to many services offered by fleets of\nmobile robots collaborating with human workers. Consequently, in this paper, we\nexpand on the concept of a topological particle filter (TPF) to accurately and\nindividually localize and track workers in a farm environment, integrating\ninformation from heterogeneous sensors and combining local active sensing\n(exploiting a robot's onboard sensing employing a Next-Best-Sense planning\napproach) and global localization (using affordable IoT GNSS devices). We\nvalidate the proposed approach in topologies created for the deployment of\nrobotics fleets to support fruit pickers in a real farm environment. By\ncombining multi-sensor observations on the topological level complemented by\nactive perception through the NBS approach, we show that we can improve the\naccuracy of picker localization in comparison to prior work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.10972,regular,pre_llm,2021,7,"{'ai_likelihood': 4.437234666612413e-06, 'text': 'Automatic Construction of Lane-level HD Maps for Urban Scenes\n\n  High definition (HD) maps have demonstrated their essential roles in enabling\nfull autonomy, especially in complex urban scenarios. As a crucial layer of the\nHD map, lane-level maps are particularly useful: they contain geometrical and\ntopological information for both lanes and intersections. However, large scale\nconstruction of HD maps is limited by tedious human labeling and high\nmaintenance costs, especially for urban scenarios with complicated road\nstructures and irregular markings. This paper proposes an approach based on\nsemantic-particle filter to tackle the automatic lane-level mapping problem in\nurban scenes. The map skeleton is firstly structured as a directed cyclic graph\nfrom online mapping database OpenStreetMap. Our proposed method then performs\nsemantic segmentation on 2D front-view images from ego vehicles and explores\nthe lane semantics on a birds-eye-view domain with true topographical\nprojection. Exploiting OpenStreetMap, we further infer lane topology and\nreference trajectory at intersections with the aforementioned lane semantics.\nThe proposed algorithm has been tested in densely urbanized areas, and the\nresults demonstrate accurate and robust reconstruction of the lane-level HD\nmap.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.06484,regular,pre_llm,2021,7,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Robust and Recursively Feasible Real-Time Trajectory Planning in Unknown\n  Environments\n\n  Motion planners for mobile robots in unknown environments face the challenge\nof simultaneously maintaining both robustness against unmodeled uncertainties\nand persistent feasibility of the trajectory-finding problem. That is, while\ndealing with uncertainties, a motion planner must update its trajectory,\nadapting to the newly revealed environment in real-time; failing to do so may\ninvolve unsafe circumstances. Many existing planning algorithms guarantee these\nby maintaining the clearance needed to perform an emergency brake, which is\nitself a robust and persistently feasible maneuver. However, such maneuvers are\nnot applicable for systems in which braking is impossible or risky, such as\nfixed-wing aircraft. To that end, we propose a real-time robust planner that\nrecursively guarantees persistent feasibility without any need of braking. The\nplanner ensures robustness against bounded uncertainties and persistent\nfeasibility by constructing a loop of sequentially composed funnels, starting\nfrom the receding horizon local trajectory's forward reachable set. We\nimplement the proposed algorithm for a robotic car tracking a speed-fixed\nreference trajectory. The experiment results show that the proposed algorithm\ncan be run at faster than 16 Hz, while successfully keeping the system away\nfrom entering any dead-end, to maintain safety and feasibility.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13055,regular,pre_llm,2021,7,"{'ai_likelihood': 8.775128258599176e-06, 'text': 'Thrust Direction Control of an Underactuated Oscillating Swimming Robot\n\n  The Modboat is an autonomous surface robot that turns the oscillation of a\nsingle motor into a controlled paddling motion through passive flippers.\nInertial control methods developed in prior work can successfully drive the\nModboat along trajectories and enable docking to neighboring modules, but have\na non-constant cycle time and cannot react to dynamic environments. In this\nwork we present a thrust direction control method for the Modboat that\nsignificantly improves the time-response of the system and increases the\naccuracy with which it can be controlled. We experimentally demonstrate that\nthis method can be used to perform more compact maneuvers than prior methods or\ncomparable robots can. We also present an extension to the controller that\nsolves the reaction wheel problem of unbounded actuator velocity, and show that\nit further improves performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.00822,regular,pre_llm,2021,7,"{'ai_likelihood': 2.327892515394423e-05, 'text': 'F-LOAM: Fast LiDAR Odometry And Mapping\n\n  Simultaneous Localization and Mapping (SLAM) has wide robotic applications\nsuch as autonomous driving and unmanned aerial vehicles. Both computational\nefficiency and localization accuracy are of great importance towards a good\nSLAM system. Existing works on LiDAR based SLAM often formulate the problem as\ntwo modules: scan-to-scan match and scan-to-map refinement. Both modules are\nsolved by iterative calculation which are computationally expensive. In this\npaper, we propose a general solution that aims to provide a computationally\nefficient and accurate framework for LiDAR based SLAM. Specifically, we adopt a\nnon-iterative two-stage distortion compensation method to reduce the\ncomputational cost. For each scan input, the edge and planar features are\nextracted and matched to a local edge map and a local plane map separately,\nwhere the local smoothness is also considered for iterative pose optimization.\nThorough experiments are performed to evaluate its performance in challenging\nscenarios, including localization for a warehouse Automated Guided Vehicle\n(AGV) and a public dataset on autonomous driving. The proposed method achieves\na competitive localization accuracy with a processing rate of more than 10 Hz\nin the public dataset evaluation, which provides a good trade-off between\nperformance and computational cost for practical applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.11777,regular,pre_llm,2021,7,"{'ai_likelihood': 1.3907750447591146e-05, 'text': 'Reinforcement Learning Compensated Extended Kalman Filter for Attitude\n  Estimation\n\n  Inertial measurement units are widely used in different fields to estimate\nthe attitude. Many algorithms have been proposed to improve estimation\nperformance. However, most of them still suffer from 1) inaccurate initial\nestimation, 2) inaccurate initial filter gain, and 3) non-Gaussian process\nand/or measurement noise. In this paper, we leverage reinforcement learning to\ncompensate for the classical extended Kalman filter estimation, i.e., to learn\nthe filter gain from the sensor measurements. We also analyse the convergence\nof the estimate error. The effectiveness of the proposed algorithm is validated\non both simulated data and real data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.14631,regular,pre_llm,2021,7,"{'ai_likelihood': 9.17249255710178e-06, 'text': ""Critical ride comfort detection for automated vehicles\n\n  In a future connected vehicle environment, an optimized route and motion\nplanning should not only fulfill efficiency and safety constraints but also\nminimize vehicle motions and oscillations, causing poor ride comfort perceived\nby passengers. This work provides a framework for a large-scale and\ncost-efficient evaluation to address AV's ride comfort and allow the comparison\nof different comfort assessment strategies. The proposed tool also gives\ninsights to comfort data, allowing for the development of novel algorithms,\nguidelines, or motion planning systems incorporating passenger comfort. A\nvehicle-road simulation framework utilizable to assess the most common ride\ncomfort determination strategies based on vehicle dynamics data is presented.\nThe developed methodology encompasses a road surface model, a non-linear\nvehicle model optimization, and Monte Carlo simulations to allow for an\naccurate and cost-efficient generation of virtual chassis acceleration data.\nRide comfort is determined by applying a commonly used threshold method and an\nanalysis based on ISO 2631. The two methods are compared against comfort\nclassifications based on empirical measurements of the International Roughness\nIndex (IRI). A case study with three road sites in Austria demonstrates the\nframework's practical application with real data and achieves high-resolution\nride comfort classifications. The results highlight that ISO 2631 comfort\nestimates are most similar to IRI classifications and that the thresholding\nprocedure detects preventable situations but also over- or underestimates ride\ncomfort. Hence, the work shows the potential risk of negative ride comfort of\nAVs using simple threshold values and stresses the importance of a robust\ncomfort evaluation method for enhancing AVs' path and motion planning with\nmaximal ride comfort.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.02715,regular,pre_llm,2021,7,"{'ai_likelihood': 9.073151482476129e-06, 'text': ""Geometrical Postural Optimisation of 7-DoF Limb-Like Manipulators\n\n  Robots are moving towards applications in less structured environments, but\ntheir model-based controllers are challenged by the tasks' complexity and\nintrinsic environmental unpredictability. Studying biological motor control can\nprovide insights into overcoming these limitations due to the high dexterity\nand stability observable in humans and animals. This work presents a\ngeometrical solution to the postural optimisation of 7-DoF limbs-like\nmechanisms, which are robust to singularities and computationally efficient.\nThe theoretical formulation identified two separate decoupled optimisation\nstrategies. The shoulder and elbow strategy align the plane of motion with the\nexpected plane of motion and guarantee the reachability of the end-posture. The\nwrist strategy ensures the end-effector orientation, which is essential to\nretain manipulability when nearing a singular configuration. The numerical\nresults confirmed the theoretical observations and allowed us to identify the\neffect of different grasp strategies on system manipulability. The geometrical\nmethod was numerically tested in thousands of configurations proving to be both\nrobust and accurate. The tested scenarios include left and right arm postures,\nsingular configurations, and walking scenarios. The proposed geometrical\napproach can find application in developing efficient and robust interaction\ncontrollers that could be applied in computational neuroscience and robotics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.14697,regular,pre_llm,2021,7,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'A Novel Approach to Model the Kinematics of Human Fingers Based on an\n  Elliptic Multi-Joint Configuration\n\n  In this paper, we present a novel kinematic model of the human phalanges\nbased on the elliptical motion of their joints. The presence of the soft\nelastic tissues and the general anatomical structure of the hand joints highly\naffect the relative movement of the bones. Commonly used assumption of circular\ntrajectories simplifies the designing process but leads to divergence with the\nactual hand behavior. The advantages of the proposed model are demonstrated\nthrough the comparison with the conventional revolute joint model. Conducted\nsimulations and experiments validate designed forward and inverse kinematic\nalgorithms. Obtained results show a high performance of the model in mimicking\nthe human fingertip motion trajectory.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.12492,regular,pre_llm,2021,7,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""SpectGRASP: Robotic Grasping by Spectral Correlation\n\n  This paper presents a spectral correlation-based method (SpectGRASP) for\nrobotic grasping of arbitrarily shaped, unknown objects. Given a point cloud of\nan object, SpectGRASP extracts contact points on the object's surface matching\nthe hand configuration. It neither requires offline training nor a-priori\nobject models. We propose a novel Binary Extended Gaussian Image (BEGI), which\nrepresents the point cloud surface normals of both object and robot fingers as\nsignals on a 2-sphere. Spherical harmonics are then used to estimate the\ncorrelation between fingers and object BEGIs. The resulting spectral\ncorrelation density function provides a similarity measure of gripper and\nobject surface normals. This is highly efficient in that it is simultaneously\nevaluated at all possible finger rotations in SO(3). A set of contact points\nare then extracted for each finger using rotations with high correlation\nvalues. We then use our previous work, Local Contact Moment (LoCoMo) similarity\nmetric, to sequentially rank the generated grasps such that the one with\nmaximum likelihood is executed. We evaluate the performance of SpectGRASP by\nconducting experiments with a 7-axis robot fitted with a parallel-jaw gripper,\nin a physics simulation environment. Obtained results indicate that the method\nnot only can grasp individual objects, but also can successfully clear randomly\norganized groups of objects. The SpectGRASP method also outperforms the closest\nstate-of-the-art method in terms of grasp generation time and grasp-efficiency.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.11467,regular,pre_llm,2021,7,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Spatio-Temporal Lattice Planning Using Optimal Motion Primitives\n\n  Lattice-based planning techniques simplify the motion planning problem for\nautonomous vehicles by limiting available motions to a pre-computed set of\nprimitives. These primitives are then combined online to generate more complex\nmaneuvers. A set of motion primitives t-span a lattice if, given a real number\nt at least 1, any configuration in the lattice can be reached via a sequence of\nmotion primitives whose cost is no more than a factor of t from optimal.\nComputing a minimal t-spanning set balances a trade-off between computed motion\nquality and motion planning performance. In this work, we formulate this\nproblem for an arbitrary lattice as a mixed integer linear program. We also\npropose an A*-based algorithm to solve the motion planning problem using these\nprimitives. Finally, we present an algorithm that removes the excessive\noscillations from planned motions -- a common problem in lattice-based\nplanning. Our method is validated for autonomous driving in both parking lot\nand highway scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.13717,regular,pre_llm,2021,7,"{'ai_likelihood': 3.7186675601535375e-05, 'text': 'Maximize the Foot Clearance for a Hopping Robotic Leg Considering Motor\n  Saturation\n\n  A hopping leg, no matter in legged animals or humans, usually behaves like a\nspring during the periodic hopping. Hopping like a spring is efficient and\nwithout the requirement of complicated control algorithms. Position and force\ncontrol are two main methods to realize such a spring-like behaviour. The\nposition control usually consumes the torque resources to ensure the position\naccuracy and compensate the tracking errors. In comparison, the force control\nstrategy is able to maintain a high elasticity. Currently, the position and\nforce control both leads to the discount of motor saturation ratio as well as\nthe bandwidth of the control system, and thus attenuates the performance of the\nactuator. To augment the performance, this letter proposes a motor saturation\nstrategy based on the force control to maximize the output torque of the\nactuator and realize the continuous hopping motion with natural dynamics. The\nproposed strategy is able to maximize the saturation ratio of motor and thus\nmaximize the foot clearance of the single leg. The dynamics of the two-mass\nmodel is utilized to increase the force bandwidth and the performance of the\nactuator. A single leg with two degrees of freedom is designed as the\nexperiment platform. The actuator consists of a powerful electric motor, a\nharmonic gear and encoder. The effectiveness of this method is verified through\nsimulations and experiments using a robotic leg actuated by powerful high\nreduction ratio actuators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2107.04314,regular,pre_llm,2021,7,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Dynamic Modeling of Bucket-Soil Interactions Using Koopman-DFL Lifting\n  Linearization for Model Predictive Contouring Control of Autonomous\n  Excavators\n\n  A lifting-linearization method based on the Koopman operator and Dual Faceted\nLinearization is applied to the control of a robotic excavator. In excavation,\na bucket interacts with the surrounding soil in a highly nonlinear and complex\nmanner. Here, we propose to represent the nonlinear bucket-soil dynamics with a\nset of linear state equations in a higher-dimensional space. The space of\nindependent state variables is augmented by adding variables associated with\nnonlinear elements involved in the bucket-soil dynamics. These include\nnonlinear resistive forces and moment acting on the bucket from the soil, and\nthe effective inertia of the bucket that varies as the soil is captured into\nthe bucket. Variables associated with these nonlinear resistive and inertia\nelements are treated as additional state variables, and their time evolution is\nrepresented as another set of linear differential equations. The lifted linear\ndynamic model is then applied to Model Predictive Contouring Control, where a\ncost functional is minimized as a convex optimization problem thanks to the\nlinear dynamics in the lifted space. The lifted linear model is tuned based on\na data-driven method by using a soil dynamics simulator. Simulation experiments\nverify the effectiveness of the proposed lifting linearization compared to its\ncounterpart.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.09801,regular,pre_llm,2021,8,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'APPLE: Adaptive Planner Parameter Learning from Evaluative Feedback\n\n  Classical autonomous navigation systems can control robots in a\ncollision-free manner, oftentimes with verifiable safety and explainability.\nWhen facing new environments, however, fine-tuning of the system parameters by\nan expert is typically required before the system can navigate as expected. To\nalleviate this requirement, the recently-proposed Adaptive Planner Parameter\nLearning paradigm allows robots to \\emph{learn} how to dynamically adjust\nplanner parameters using a teleoperated demonstration or corrective\ninterventions from non-expert users. However, these interaction modalities\nrequire users to take full control of the moving robot, which requires the\nusers to be familiar with robot teleoperation. As an alternative, we introduce\n\\textsc{apple}, Adaptive Planner Parameter Learning from \\emph{Evaluative\nFeedback} (real-time, scalar-valued assessments of behavior), which represents\na less-demanding modality of interaction. Simulated and physical experiments\nshow \\textsc{apple} can achieve better performance compared to the planner with\nstatic default parameters and even yield improvement over learned parameters\nfrom richer interaction modalities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.02028,regular,pre_llm,2021,8,"{'ai_likelihood': 7.847944895426433e-06, 'text': ""Incorporating Learnt Local and Global Embeddings into Monocular Visual\n  SLAM\n\n  Traditional approaches for Visual Simultaneous Localization and Mapping\n(VSLAM) rely on low-level vision information for state estimation, such as\nhandcrafted local features or the image gradient. While significant progress\nhas been made through this track, under more challenging configuration for\nmonocular VSLAM, e.g., varying illumination, the performance of\nstate-of-the-art systems generally degrades. As a consequence, robustness and\naccuracy for monocular VSLAM are still widely concerned. This paper presents a\nmonocular VSLAM system that fully exploits learnt features for better state\nestimation. The proposed system leverages both learnt local features and global\nembeddings at different modules of the system: direct camera pose estimation,\ninter-frame feature association, and loop closure detection. With a\nprobabilistic explanation of keypoint prediction, we formulate the camera pose\ntracking in a direct manner and parameterize local features with uncertainty\ntaken into account. To alleviate the quantization effect, we adapt the mapping\nmodule to generate 3D landmarks better to guarantee the system's robustness.\nDetecting temporal loop closure via deep global embeddings further improves the\nrobustness and accuracy of the proposed system. The proposed system is\nextensively evaluated on public datasets (Tsukuba, EuRoC, and KITTI), and\ncompared against the state-of-the-art methods. The competitive performance of\ncamera pose estimation confirms the effectiveness of our method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.01481,regular,pre_llm,2021,8,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Impact Mitigation for Dynamic Legged Robots with Steel Wire Transmission\n  Using Nonlinear Active Compliance Control\n\n  Impact mitigation is crucial to the stable locomotion of legged robots,\nespecially in high-speed dynamic locomotion. This paper presents a leg\nlocomotion system including the nonlinear active compliance control and the\nactive impedance control for the steel wire transmission-based legged robot.\nThe developed control system enables high-speed dynamic locomotion with\nexcellent impact mitigation and leg position tracking performance, where three\nstrategies are applied. a) The feed-forward controller is designed according to\nthe linear motor-leg model with the information of Coulomb friction and viscous\nfriction. b) Steel wire transmission model-based compensation guarantees ideal\nvirtual spring compliance characteristics. c) Nonlinear active compliance\ncontrol and active impedance control ensure better impact mitigation\nperformance than linear scheme and guarantee position tracking performance. The\nproposed control system is verified on a real robot named SCIT Dog and the\nexperiment demonstrates the ideal impact mitigation ability in high-speed\ndynamic locomotion without any passive spring mechanism.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.08789,regular,pre_llm,2021,8,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Resilient and consistent multirobot cooperative localization with\n  covariance intersection\n\n  Cooperative localization is fundamental to autonomous multirobot systems, but\nmost algorithms couple inter-robot communication with observation, making these\nalgorithms susceptible to failures in both communication and observation steps.\nTo enhance the resilience of multirobot cooperative localization algorithms in\na distributed system, we use covariance intersection to formalize a\nlocalization algorithm with an explicit communication update and ensure\nestimation consistency at the same time. We investigate the covariance\nboundedness criterion of our algorithm with respect to communication and\nobservation graphs, demonstrating provable localization performance under even\nsparse communications topologies. We substantiate the resilience of our\nalgorithm as well as the boundedness analysis through experiments on simulated\nand benchmark physical data against varying communications connectivity and\nfailure metrics. Especially when inter-robot communication is entirely blocked\nor partially unavailable, we demonstrate that our method is less affected and\nmaintains desired performance compared to existing cooperative localization\nalgorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.04021,regular,pre_llm,2021,8,"{'ai_likelihood': 5.629327562120226e-06, 'text': 'Unknown Object Segmentation through Domain Adaptation\n\n  The ability to segment unknown objects in cluttered scenes has a profound\nimpact on robot grasping. The rise of deep learning has greatly transformed the\npipeline of robotic grasping from model-based approach to data-driven stream,\nwhich generally requires a large scale of grasping data either collected in\nsimulation or from real-world examples. In this paper, we proposed a\nsim-to-real framework to transfer the object segmentation model learned in\nsimulation to the real-world. First, data samples are collected in simulation,\nincluding RGB, 6D pose, and point cloud. Second, we also present a GAN-based\nunknown object segmentation method through domain adaptation, which consists of\nan image translation module and an image segmentation module. The image\ntranslation module is used to shorten the reality gap and the segmentation\nmodule is responsible for the segmentation mask generation. We used the above\nmethod to perform segmentation experiments on unknown objects in a bin-picking\nscenario. Finally, the experimental result shows that the segmentation model\nlearned in simulation can be used for real-world data segmentation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.07085,regular,pre_llm,2021,8,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""Smart Pointers and Shared Memory Synchronisation for Efficient\n  Inter-process Communication in ROS on an Autonomous Vehicle\n\n  Despite the stringent requirements of a real-time system, the reliance of the\nRobot Operating System (ROS) on the loopback network interface imposes a\nconsiderable overhead on the transport of high bandwidth data, while the\nnodelet package, which is an efficient mechanism for intra-process\ncommunication, does not address the problem of efficient local inter-process\ncommunication (IPC). To remedy this, we propose a novel integration into ROS of\nsmart pointers and synchronisation primitives stored in shared memory. These\nobey the same semantics and, more importantly, exhibit the same performance as\ntheir C++ standard library counterparts, making them preferable to other local\nIPC mechanisms. We present a series of benchmarks for our mechanism - which we\ncall LOT (Low Overhead Transport) - and use them to assess its performance on\nrealistic data loads based on Five's Autonomous Vehicle (AV) system, and extend\nour analysis to the case where multiple ROS nodes are running in Docker\ncontainers. We find that our mechanism performs up to two orders of magnitude\nbetter than the standard IPC via local loopback. Finally, we apply\nindustry-standard profiling techniques to explore the hotspots of code running\nin both user and kernel space, comparing our implementation against\nalternatives.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05047,regular,pre_llm,2021,8,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Road Mapping and Localization using Sparse Semantic Visual Features\n\n  We present a novel method for visual mapping and localization for autonomous\nvehicles, by extracting, modeling, and optimizing semantic road elements.\nSpecifically, our method integrates cascaded deep models to detect standardized\nroad elements instead of traditional point features, to seek for improved pose\naccuracy and map representation compactness. To utilize the structural\nfeatures, we model road lights and signs by their representative deep keypoints\nfor skeleton and boundary, and parameterize lanes via piecewise cubic splines.\nBased on the road semantic features, we build a complete pipeline for mapping\nand localization, which includes a) image processing front-end, b) sensor\nfusion strategies, and c) optimization backend. Experiments on public datasets\nand our testing platform have demonstrated the effectiveness and advantages of\nour method by outperforming traditional approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.11253,regular,pre_llm,2021,8,"{'ai_likelihood': 4.635916815863716e-06, 'text': 'On Reciprocally Rotating Magnetic Actuation of a Robotic Capsule in\n  Unknown Tubular Environments\n\n  Active wireless capsule endoscopy (WCE) based on simultaneous magnetic\nactuation and localization (SMAL) techniques holds great promise for improving\ndiagnostic accuracy, reducing examination time and relieving operator burden.\nTo date, the rotating magnetic actuation methods have been constrained to use a\ncontinuously rotating permanent magnet. In this paper, we first propose the\nreciprocally rotating magnetic actuation (RRMA) approach for active WCE to\nenhance patient safety. We first show how to generate a desired reciprocally\nrotating magnetic field for capsule actuation, and provide a theoretical\nanalysis of the potential risk of causing volvulus due to the capsule motion.\nThen, an RRMA-based SMAL workflow is presented to automatically propel a\ncapsule in an unknown tubular environment. We validate the effectiveness of our\nmethod in real-world experiments to automatically propel a robotic capsule in\nan ex-vivo pig colon. The experiment results show that our approach can achieve\nefficient and robust propulsion of the capsule with an average moving speed of\n$2.48 mm/s$ in the pig colon, and demonstrate the potential of using RRMA to\nenhance patient safety, reduce the inspection time, and improve the clinical\nacceptance of this technology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05218,regular,pre_llm,2021,8,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Estimation and Navigation Methods with Limited Information for\n  Autonomous Urban Driving\n\n  Urban environments offer a challenging scenario for autonomous driving.\nGlobally localizing information, such as a GPS signal, can be unreliable due to\nsignal shadowing and multipath errors. Detailed a priori maps of the\nenvironment with sufficient information for autonomous navigation typically\nrequire driving the area multiple times to collect large amounts of data,\nsubstantial post-processing on that data to obtain the map, and then\nmaintaining updates on the map as the environment changes. This dissertation\naddresses the issue of autonomous driving in an urban environment by\ninvestigating algorithms and an architecture to enable fully functional\nautonomous driving with limited information.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06886,regular,pre_llm,2021,8,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Decentralized Multi-AGV Task Allocation based on Multi-Agent\n  Reinforcement Learning with Information Potential Field Rewards\n\n  Automated Guided Vehicles (AGVs) have been widely used for material handling\nin flexible shop floors. Each product requires various raw materials to\ncomplete the assembly in production process. AGVs are used to realize the\nautomatic handling of raw materials in different locations. Efficient AGVs task\nallocation strategy can reduce transportation costs and improve distribution\nefficiency. However, the traditional centralized approaches make high demands\non the control center's computing power and real-time capability. In this\npaper, we present decentralized solutions to achieve flexible and\nself-organized AGVs task allocation. In particular, we propose two improved\nmulti-agent reinforcement learning algorithms, MADDPG-IPF (Information\nPotential Field) and BiCNet-IPF, to realize the coordination among AGVs\nadapting to different scenarios. To address the reward-sparsity issue, we\npropose a reward shaping strategy based on information potential field, which\nprovides stepwise rewards and implicitly guides the AGVs to different material\ntargets. We conduct experiments under different settings (3 AGVs and 6 AGVs),\nand the experiment results indicate that, compared with baseline methods, our\nwork obtains up to 47\\% task response improvement and 22\\% training iterations\nreduction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.05505,regular,pre_llm,2021,8,"{'ai_likelihood': 1.5629662407769098e-05, 'text': 'Agile Formation Control of Drone Flocking Enhanced with Active\n  Vision-based Relative Localization\n\n  The vision-based relative localization can provide effective feedback for the\ncooperation of aerial swarm and has been widely investigated in previous works.\nHowever, the limited field of view (FOV) inherently restricts its performance.\nTo cope with this issue, this letter proposes a novel distributed active\nvision-based relative localization framework and apply it to formation control\nin aerial swarms. Inspired by bird flocks in nature, we devise graph-based\nattention planning (GAP) to improve the observation quality of the active\nvision in the swarm. Then active detection results are fused with onboard\nmeasurements from Ultra-WideBand (UWB) and visual-inertial odometry (VIO) to\nobtain real-time relative positions, which further improve the formation\ncontrol performance of the swarm. Simulations and experiments demonstrate that\nthe proposed active vision system outperforms the fixed vision system in terms\nof estimation and formation accuracy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.04567,regular,pre_llm,2021,8,"{'ai_likelihood': 0.99267578125, 'text': ""Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance\n  Control\n\n  In recent years, the need for robots to transition from isolated industrial\ntasks to shared environments, including human-robot collaboration and\nteleoperation, has become increasingly evident. Building on the foundation of\nFractal Impedance Control (FIC) introduced in our previous work, this paper\npresents a novel extension to dual-arm tele-cooperation, leveraging the\nnon-linear stiffness and passivity of FIC to adapt to diverse cooperative\nscenarios. Unlike traditional impedance controllers, our approach ensures\nstability without relying on energy tanks, as demonstrated in our prior\nresearch. In this paper, we further extend the FIC framework to bimanual\noperations, allowing for stable and smooth switching between different dynamic\ntasks without gain tuning. We also introduce a telemanipulation architecture\nthat offers higher transparency and dexterity, addressing the challenges of\nsignal latency and low-bandwidth communication. Through extensive experiments,\nwe validate the robustness of our method and the results confirm the advantages\nof the FIC approach over traditional impedance controllers, showcasing its\npotential for applications in planetary exploration and other scenarios\nrequiring dexterous telemanipulation. This paper's contributions include the\nseamless integration of FIC into multi-arm systems, the ability to perform\nrobust interactions in highly variable environments, and the provision of a\ncomprehensive comparison with competing approaches, thereby significantly\nenhancing the robustness and adaptability of robotic systems.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00033926963806152344, 'GPT4': 0.98583984375, 'CLAUDE': 5.745887756347656e-05, 'GOOGLE': 0.0125579833984375, 'OPENAI_O_SERIES': 0.0009899139404296875, 'DEEPSEEK': 2.682209014892578e-05, 'GROK': 1.245737075805664e-05, 'NOVA': 2.9802322387695312e-06, 'OTHER': 0.00013148784637451172, 'HUMAN': 0.0001913309097290039}}"
2108.00753,regular,pre_llm,2021,8,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Non-linear stiffness modeling of multi-link compliant serial manipulator\n  composed of multiple tensegrity segments\n\n  The paper focuses on the stiffness modeling of a new type of compliant\nmanipulator and its non-linear behavior while interacting with the environment.\nThe manipulator under study is a serial mechanical structure composed of\ndualtriangle segments. The main attention is paid to the initial straight\nconfiguration which may suddenly change its shape under the loading. It was\ndiscovered that under the external loading such manipulator may have six\nequilibrium configurations but only two of them are stable. In the neighborhood\nof these configurations, the manipulator behavior was analyzed using the\nVirtual Joint Method (VJM). This approach allowed us to propose an analytical\ntechnique for computing a critical force causing the buckling and evaluate the\nmanipulator shape under the loading. A relevant simulation study confirmed the\nvalidity of the developed technique and its advantages in non-linear stiffness\nanalysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.12869,regular,pre_llm,2021,8,"{'ai_likelihood': 2.384185791015625e-06, 'text': ""Flying Through a Narrow Gap Using End-to-end Deep Reinforcement Learning\n  Augmented with Curriculum Learning and Sim2Real\n\n  Traversing through a tilted narrow gap is previously an intractable task for\nreinforcement learning mainly due to two challenges. First, searching feasible\ntrajectories is not trivial because the goal behind the gap is difficult to\nreach. Second, the error tolerance after Sim2Real is low due to the relatively\nhigh speed in comparison to the gap's narrow dimensions. This problem is\naggravated by the intractability of collecting real-world data due to the risk\nof collision damage. In this paper, we propose an end-to-end reinforcement\nlearning framework that solves this task successfully by addressing both\nproblems. To search for dynamically feasible flight trajectories, we use\ncurriculum learning to guide the agent towards the sparse reward behind the\nobstacle. To tackle the Sim2Real problem, we propose a Sim2Real framework that\ncan transfer control commands to a real quadrotor without using real flight\ndata. To the best of our knowledge, our paper is the first work that\naccomplishes successful gap traversing task purely using deep reinforcement\nlearning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.13105,regular,pre_llm,2021,8,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'COMPRA: A COMPact Reactive Autonomy framework for subterranean MAV based\n  search-and-rescue operations\n\n  This work establishes COMPRA, a compact and reactive autonomy framework for\nfast deployment of Micro Aerial Vehicles (MAVs) in subterranean\nSearch-and-Rescue (SAR) missions. A COMPRA-enabled MAV is able to autonomously\nexplore previously unknown areas while specific mission criteria are considered\ne.g. an object of interest is identified and localized, the remaining useful\nbattery life, the overall desired exploration mission duration. The proposed\narchitecture follows a low-complexity algorithmic design to facilitate fully\non-board computations, including nonlinear control, state-estimation,\nnavigation, exploration behavior and object localization capabilities. The\nframework is mainly structured around a reactive local avoidance planner, based\non enhanced Potential Field concepts and using instantaneous 3D pointclouds, as\nwell as a computationally efficient heading regulation technique, based on\ndepth images from an instantaneous camera stream. Those techniques decouple the\ncollision-free path generation from the dependency of a global map and are\ncapable of handling imprecise localization occasions. Field experimental\nverification of the overall architecture is performed in relevant unknown\nGlobal Positioning System (GPS)-denied environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.04792,regular,pre_llm,2021,8,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Learning Autonomous Mobility Using Real Demonstration Data\n\n  This work proposed an efficient learning-based framework to learn feedback\ncontrol policies from human teleoperated demonstrations, which achieved\nobstacle negotiation, staircase traversal, slipping control and parcel delivery\nfor a tracked robot. Due to uncertainties in real-world scenarios, eg obstacle\nand slippage, closed-loop feedback control plays an important role in improving\nrobustness and resilience, but the control laws are difficult to program\nmanually for achieving autonomous behaviours. We formulated an architecture\nbased on a long-short-term-memory (LSTM) neural network, which effectively\nlearn reactive control policies from human demonstrations. Using datasets from\na few real demonstrations, our algorithm can directly learn successful\npolicies, including obstacle-negotiation, stair-climbing and delivery, fall\nrecovery and corrective control of slippage. We proposed decomposition of\ncomplex robot actions to reduce the difficulty of learning the long-term\ndependencies. Furthermore, we proposed a method to efficiently handle\nnon-optimal demos and to learn new skills, since collecting enough\ndemonstration can be time-consuming and sometimes very difficult on a real\nrobotic system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.06728,regular,pre_llm,2021,8,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Learning Dynamical System for Grasping Motion\n\n  Dynamical System has been widely used for encoding trajectories from human\ndemonstration, which has the inherent adaptability to dynamically changing\nenvironments and robustness to perturbations. In this paper we propose a\nframework to learn a dynamical system that couples position and orientation\nbased on a diffeomorphism. Different from other methods, it can realise the\nsynchronization between positon and orientation during the whole trajectory.\nOnline grasping experiments are carried out to prove its effectiveness and\nonline adaptability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.02976,regular,pre_llm,2021,8,"{'ai_likelihood': 6.092919243706598e-06, 'text': 'On Bundle Adjustment for Multiview PointCloud Registration\n\n  Multiview registration is used to estimate Rigid Body Transformations (RBTs)\nfrom multiple frames and reconstruct a scene with corresponding scans. Despite\nthe success of pairwise registration and pose synchronization, the concept of\nBundle Adjustment (BA) has been proven to better maintain global consistency.\nSo in this work, we make the multiview point-cloud registration more tractable\nfrom a different perspective in resolving range-based BA. Based on this\nanalysis, we propose an objective function that takes both measurement noises\nand computational cost into account. For the feature parameter update, instead\nof calculating the global distribution parameters from the raw measurements, we\naggregate the local distributions upon the pose update at each iteration. The\ncomputational cost of feature update is then only dependent on the number of\nscans. Finally, we develop a multiview registration system using voxel-based\nquantization that can be applied in real-world scenarios. The experimental\nresults demonstrate our superiority over the baselines in terms of both\naccuracy and speed. Moreover, the results also show that our average\npositioning errors achieve the centimeter level.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.07265,regular,pre_llm,2021,8,"{'ai_likelihood': 4.470348358154297e-06, 'text': 'The Integrated Probabilistic Data Association Filter Adapted to Lie\n  Groups\n\n  The Integrated Probabilistic Data Association Filter (IPDAF) is a target\ntracking algorithm based on the Probabilistic Data Association Filter that\ncalculates a statistical measure that indicates if an estimated representation\nof the target properly represents the target or is generated from\nnon-target-originated measurements. The main contribution of this paper is to\nadapt the IPDAF to constant velocity target models that evolve on connected,\nunimodular Lie groups, and where the measurements are also defined on a Lie\ngroup. We present an example where the methods developed in the paper are\napplied to the problem of tracking a ground vehicle on the special Euclidean\ngroup SE(2).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2108.12007,regular,pre_llm,2021,8,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Dual-arm Coordinated Manipulation for Object Twisting with Human\n  Intelligence\n\n  Robotic dual-arm twisting is a common but very challenging task in both\nindustrial production and daily services, as it often requires dexterous\ncollaboration, a large scale of end-effector rotating, and good adaptivity for\nobject manipulation. Meanwhile, safety and efficiency are preliminary concerns\nfor robotic dual-arm coordinated manipulation. Thus, the normally adopted fully\nautomated task execution approaches based on environmental perception and\nmotion planning techniques are still inadequate and problematic for the arduous\ntwisting tasks. To this end, this paper presents a novel strategy of the\ndual-arm coordinated control for twisting manipulation based on the combination\nof optimized motion planning for one arm and real-time telecontrol with human\nintelligence for the other. The analysis and simulation results showed it can\nachieve collision and singularity free for dual arms with enhanced dexterity,\nsafety, and efficiency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13661,review,pre_llm,2021,9,"{'ai_likelihood': 6.4240561591254345e-06, 'text': ""Literature Review on Endoscopic Robotic Systems in Ear and Sinus Surgery\n\n  In otolaryngologic surgery, endoscopy is increasingly used to provide a\nbetter view of hard-to-reach areas and to promote minimally invasive surgery.\nHowever, the need to manipulate the endoscope limits the surgeon's ability to\noperate with only one instrument at a time. Currently, several robotic systems\nare being developed, demonstrating the value of robotic assistance in\nmicrosurgery. The aim of this literature review is to present and classify\ncurrent robotic systems that are used for otological and endonasal\napplications. For these solutions, an analysis of the functionalities in\nrelation to the surgeon's needs will be carried out in order to produce a set\nof specifications for the creation of new robots.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.04677,regular,pre_llm,2021,9,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Optimizing Space Utilization for More Effective Multi-Robot Path\n  Planning\n\n  We perform a systematic exploration of the principle of Space Utilization\nOptimization (SUO) as a heuristic for planning better individual paths in a\ndecoupled multi-robot path planner, with applications to both one-shot and\nlife-long multi-robot path planning problems. We show that the decentralized\nheuristic set, SU-I, preserves single path optimality and significantly reduces\ncongestion that naturally happens when many paths are planned without\ncoordination. Integration of SU-I into complete planners brings dramatic\nreductions in computation time due to the significantly reduced number of\nconflicts and leads to sizable solution optimality gains in diverse evaluation\nscenarios with medium and large maps, for both one-shot and life-long problem\nsettings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.10488,regular,pre_llm,2021,9,"{'ai_likelihood': 6.669097476535373e-05, 'text': ""A Model-free Deep Reinforcement Learning Approach To Maneuver A\n  Quadrotor Despite Single Rotor Failure\n\n  Ability to recover from faults and continue mission is desirable for many\nquadrotor applications. The quadrotor's rotor may fail while performing a\nmission and it is essential to develop recovery strategies so that the vehicle\nis not damaged. In this paper, we develop a model-free deep reinforcement\nlearning approach for a quadrotor to recover from a single rotor failure. The\napproach is based on Soft-actor-critic that enables the vehicle to hover, land,\nand perform complex maneuvers. Simulation results are presented to validate the\nproposed approach using a custom simulator. The results show that the proposed\napproach achieves hover, landing, and path following in 2D and 3D. We also show\nthat the proposed approach is robust to wind disturbances.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.07764,regular,pre_llm,2021,9,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Meeting-Merging-Mission: A Multi-robot Coordinate Framework for\n  Large-Scale Communication-Limited Exploration\n\n  This letter presents a complete framework Meeting-Merging-Mission for\nmulti-robot exploration under communication restriction. Considering\ncommunication is limited in both bandwidth and range in the real world, we\npropose a lightweight environment presentation method and an efficient\ncooperative exploration strategy. For lower bandwidth, each robot utilizes\nspecific polytopes to maintains free space and super frontier information (SFI)\nas the source for exploration decision-making. To reduce repeated exploration,\nwe develop a mission-based protocol that drives robots to share collected\ninformation in stable rendezvous. We also design a complete path planning\nscheme for both centralized and decentralized cases. To validate that our\nframework is practical and generic, we present an extensive benchmark and\ndeploy our system into multi-UGV and multi-UAV platforms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.1354,regular,pre_llm,2021,9,"{'ai_likelihood': 5.265076955159506e-06, 'text': 'Comparison of Information-Gain Criteria for Action Selection\n\n  Accurate object pose estimation using multi-modal perception such as visual\nand tactile sensing have been used for autonomous robotic manipulators in\nliterature. Due to variation in density of visual and tactile data, a novel\nprobabilistic Bayesian filter-based approach termed translation-invariant\nQuaternion filter (TIQF) is proposed for pose estimation using point cloud\nregistration. Active tactile data collection is preferred by reasoning over\nmultiple potential actions for maximal expected information gain as tactile\ndata collection is time consuming. In this paper, we empirically evaluate\nvarious information gain criteria for action selection in the context of object\npose estimation. We demonstrate the adaptability and effectiveness of our\nproposed TIQF pose estimation approach with various information gain criteria.\nWe find similar performance in terms of pose accuracy with sparse measurements\n(<15 points) across all the selected criteria. Furthermore, we explore the use\nof uncommon information theoretic criteria in the robotics domain for action\nselection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.12928,regular,pre_llm,2021,9,"{'ai_likelihood': 1.367595460679796e-05, 'text': 'A Biologically Inspired Global Localization System for Mobile Robots\n  Using LiDAR Sensor\n\n  Localization in the environment is an essential navigational capability for\nanimals and mobile robots. In the indoor environment, the global localization\nproblem remains challenging to be perfectly solved with probabilistic methods.\nHowever, animals are able to instinctively localize themselves with much less\neffort. Therefore, it is intriguing and promising to seek biological\ninspiration from animals. In this paper, we present a biologically-inspired\nglobal localization system using a LiDAR sensor that utilizes a hippocampal\nmodel and a landmark-based re-localization approach. The experiment results\nshow that the proposed method is competitive to Monte Carlo Localization, and\nthe results demonstrate the high accuracy, applicability, and reliability of\nthe proposed biologically-inspired localization system in different\nlocalization scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.06446,regular,pre_llm,2021,9,"{'ai_likelihood': 0.00011258655124240452, 'text': ""Multi-modal Motion Prediction with Transformer-based Neural Network for\n  Autonomous Driving\n\n  Predicting the behaviors of other agents on the road is critical for\nautonomous driving to ensure safety and efficiency. However, the challenging\npart is how to represent the social interactions between agents and output\ndifferent possible trajectories with interpretability. In this paper, we\nintroduce a neural prediction framework based on the Transformer structure to\nmodel the relationship among the interacting agents and extract the attention\nof the target agent on the map waypoints. Specifically, we organize the\ninteracting agents into a graph and utilize the multi-head attention\nTransformer encoder to extract the relations between them. To address the\nmulti-modality of motion prediction, we propose a multi-modal attention\nTransformer encoder, which modifies the multi-head attention mechanism to\nmulti-modal attention, and each predicted trajectory is conditioned on an\nindependent attention mode. The proposed model is validated on the Argoverse\nmotion forecasting dataset and shows state-of-the-art prediction accuracy while\nmaintaining a small model size and a simple training process. We also\ndemonstrate that the multi-modal attention module can automatically identify\ndifferent modes of the target agent's attention on the map, which improves the\ninterpretability of the model.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13145,regular,pre_llm,2021,9,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Non-prehensile Planar Manipulation via Trajectory Optimization with\n  Complementarity Constraints\n\n  Contact adaption is an essential capability when manipulating objects. Two\nkey contact modes of non-prehensile manipulation are sticking and sliding. This\npaper presents a Trajectory Optimization (TO) method formulated as a\nMathematical Program with Complementarity Constraints (MPCC), which is able to\nswitch between these two modes. We show that this formulation can be applicable\nto both planning and Model Predictive Control (MPC) for planar manipulation\ntasks. We numerically compare: (i) our planner against a mixed integer\nalternative, showing that the MPCC planer converges faster, scales better with\nrespect to time horizon, and can handle environments with obstacles; (ii) our\ncontroller against a state-of-the-art mixed integer approach, showing that the\nMPCC controller achieves better tracking and more consistent computation times.\nAdditionally, we experimentally validate both our planner and controller with\nthe KUKA LWR robot on a range of planar manipulation tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.08265,regular,pre_llm,2021,9,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Stability Analysis of Planar Probabilistic Piecewise Constant Derivative\n  Systems\n\n  In this paper, we study the probabilistic stability analysis of a subclass of\nstochastic hybrid systems, called the Planar Probabilistic Piecewise Constant\nDerivative Systems (Planar PPCD), where the continuous dynamics is\ndeterministic, constant rate and planar, the discrete switching between the\nmodes is probabilistic and happens at boundary of the invariant regions, and\nthe continuous states are not reset during switching. These aptly model\npiecewise linear behaviors of planar robots. Our main result is an exact\nalgorithm for deciding absolute and almost sure stability of Planar PPCD under\nsome mild assumptions on mutual reachability between the states and the\npresence of non-zero probability self-loops. Our main idea is to reduce the\nstability problems on planar PPCD into corresponding problems on Discrete Time\nMarkov Chains with edge weights.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.07133,regular,pre_llm,2021,9,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Combining Context Awareness and Planning to Learn Behavior Trees from\n  Demonstration\n\n  Fast changing tasks in unpredictable, collaborative environments are typical\nfor medium-small companies, where robotised applications are increasing. Thus,\nrobot programs should be generated in short time with small effort, and the\nrobot able to react dynamically to the environment. To address this we propose\na method that combines context awareness and planning to learn Behavior Trees\n(BTs), a reactive policy representation that is becoming more popular in\nrobotics and has been used successfully in many collaborative scenarios.\nContext awareness allows to infer from the demonstration the frames in which\nactions are executed and to capture relevant aspects of the task, while a\nplanner is used to automatically generate the BT from the sequence of actions\nfrom the demonstration. The learned BT is shown to solve non-trivial\nmanipulation tasks where learning the context is fundamental to achieve the\ngoal. Moreover, we collected non-expert demonstrations to study the\nperformances of the algorithm in industrial scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.02301,regular,pre_llm,2021,9,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Task-Level Authoring for Remote Robot Teleoperation\n\n  Remote teleoperation of robots can broaden the reach of domain specialists\nacross a wide range of industries such as home maintenance, health care, light\nmanufacturing, and construction. However, current direct control methods are\nimpractical, and existing tools for programming robot remotely have focused on\nusers with significant robotic experience. Extending robot remote programming\nto end users, i.e., users who are experts in a domain but novices in robotics,\nrequires tools that balance the rich features necessary for complex\nteleoperation tasks with ease of use. The primary challenge to usability is\nthat novice users are unable to specify complete and robust task plans to allow\na robot to perform duties autonomously, particularly in highly variable\nenvironments. Our solution is to allow operators to specify shorter sequences\nof high-level commands, which we call task-level authoring, to create periods\nof variable robot autonomy. This approach allows inexperienced users to create\nrobot behaviors in uncertain environments by interleaving exploration,\nspecification of behaviors, and execution as separate steps. End users are able\nto break down the specification of tasks and adapt to the current needs of the\ninteraction and environments, combining the reactivity of direct control to\nasynchronous operation. In this paper, we describe a prototype system\ncontextualized in light manufacturing and its empirical validation in a user\nstudy where 18 participants with some programming experience were able to\nperform a variety of complex telemanipulation tasks with little training. Our\nresults show that our approach allowed users to create flexible periods of\nautonomy and solve rich manipulation tasks. Furthermore, participants\nsignificantly preferred our system over comparative more direct interfaces,\ndemonstrating the potential of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.14828,regular,pre_llm,2021,9,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Uncertainty Estimation of Dense Optical-Flow for Robust Visual\n  Navigation\n\n  This paper presents a novel dense optical-flow algorithm to solve the\nmonocular simultaneous localization and mapping (SLAM) problem for ground or\naerial robots. Dense optical flow can effectively provide the ego-motion of the\nvehicle while enabling collision avoidance with the potential obstacles.\nExisting work has not fully utilized the uncertainty of the optical flow -- at\nmost an isotropic Gaussian density model. We estimate the full uncertainty of\nthe optical flow and propose a new eight-point algorithm based on the\nstatistical Mahalanobis distance. Combined with the pose-graph optimization,\nthe proposed method demonstrates enhanced robustness and accuracy for the\npublic autonomous car dataset (KITTI) and aerial monocular dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.13382,regular,pre_llm,2021,9,"{'ai_likelihood': 9.338061014811199e-06, 'text': 'Bimanual Telemanipulation with Force and Haptic Feedback and Predictive\n  Limit Avoidance\n\n  Robotic teleoperation is a key technology for a wide variety of applications.\nIt allows sending robots instead of humans in remote, possibly dangerous\nlocations while still using the human brain with its enormous knowledge and\ncreativity, especially for solving unexpected problems. A main challenge in\nteleoperation consists of providing enough feedback to the human operator for\nsituation awareness and thus create full immersion, as well as offering the\noperator suitable control interfaces to achieve efficient and robust task\nfulfillment. We present a bimanual telemanipulation system consisting of an\nanthropomorphic avatar robot and an operator station providing force and haptic\nfeedback to the human operator. The avatar arms are controlled in Cartesian\nspace with a 1:1 mapping of the operator movements. The measured forces and\ntorques on the avatar side are haptically displayed directly to the operator.\nWe developed a predictive avatar model for limit avoidance which runs on the\noperator side, ensuring low latency. Only off-the-shelf components were used to\nbuild the system. It is evaluated in lab experiments and by untrained operators\nin a small user study.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.05297,regular,pre_llm,2021,9,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'A Right Invariant Extended Kalman Filter for Object based SLAM\n\n  With the recent advance of deep learning based object recognition and\nestimation, it is possible to consider object level SLAM where the pose of each\nobject is estimated in the SLAM process. In this paper, based on a novel Lie\ngroup structure, a right invariant extended Kalman filter (RI-EKF) for object\nbased SLAM is proposed. The observability analysis shows that the proposed\nalgorithm automatically maintains the correct unobservable subspace, while\nstandard EKF (Std-EKF) based SLAM algorithm does not. This results in a better\nconsistency for the proposed algorithm comparing to Std-EKF. Finally,\nsimulations and real world experiments validate not only the consistency and\naccuracy of the proposed algorithm, but also the practicability of the proposed\nRI-EKF for object based SLAM problem. The MATLAB code of the algorithm is made\npublicly available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.07995,regular,pre_llm,2021,9,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'Towards Defensive Autonomous Driving: Collecting and Probing Driving\n  Demonstrations of Mixed Qualities\n\n  Designing or learning an autonomous driving policy is undoubtedly a\nchallenging task as the policy has to maintain its safety in all corner cases.\nIn order to secure safety in autonomous driving, the ability to detect\nhazardous situations, which can be seen as an out-of-distribution (OOD)\ndetection problem, becomes crucial. However, most conventional datasets only\nprovide expert driving demonstrations, although some non-expert or uncommon\ndriving behavior data are needed to implement a safety guaranteed autonomous\ndriving platform. To this end, we present a novel dataset called the R3 Driving\nDataset, composed of driving data with different qualities. The dataset\ncategorizes abnormal driving behaviors into eight categories and 369 different\ndetailed situations. The situations include dangerous lane changes and\nnear-collision situations. To further enlighten how these abnormal driving\nbehaviors can be detected, we utilize different uncertainty estimation and\nanomaly detection methods to the proposed dataset. From the results of the\nproposed experiment, it can be inferred that by using both uncertainty\nestimation and anomaly detection, most of the abnormal cases in the proposed\ndataset can be discriminated. The dataset of this paper can be downloaded from\nhttps://rllab-snu.github.io/projects/R3-Driving-Dataset/doc.html.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.08228,regular,pre_llm,2021,9,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""ROOAD: RELLIS Off-road Odometry Analysis Dataset\n\n  The development and implementation of visual-inertial odometry (VIO) has\nfocused on structured environments, but interest in localization in off-road\nenvironments is growing. In this paper, we present the RELLIS Off-road Odometry\nAnalysis Dataset (ROOAD) which provides high-quality, time-synchronized\noff-road monocular visual-inertial data sequences to further the development of\nrelated research. We evaluated the dataset on two state-of-the-art VIO\nalgorithms, (1) Open-VINS and (2) VINS-Fusion. Our findings indicate that both\nalgorithms perform 2 to 30 times worse on the ROOAD dataset compared to their\nperformance in structured environments. Furthermore, OpenVINS has better\ntracking stability and real-time performance than VINS-Fusion in the off-road\nenvironment, while VINS-Fusion outperformed OpenVINS in tracking accuracy in\nseveral data sequences. Since the camera-IMU calibration tool from Kalibr\ntoolkit is used extensively in this work, we have included several calibration\ndata sequences. Our hand measurements show Kalibr's tool achieved +/-1 degree\nfor orientation error and +/-1 mm at best (x- and y-axis) and +/-10 mm (z-axis)\nat worse for position error in the camera frame between the camera and IMU.\nThis novel dataset provides a new set of scenarios for researchers to design\nand test their localization algorithms on, as well as critical insights in the\ncurrent performance of VIO in off-road environments.\n  ROOAD Dataset: github.com/unmannedlab/ROOAD\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.00662,regular,pre_llm,2021,9,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Quori: A Community-Informed Design of a Socially Interactive Humanoid\n  Robot\n\n  Hardware platforms for socially interactive robotics can be limited by cost\nor lack of functionality. This paper presents the overall system -- design,\nhardware, and software -- for Quori, a novel, affordable, socially interactive\nhumanoid robot platform for facilitating non-contact human-robot interaction\n(HRI) research. The design of the system is motivated by feedback sampled from\nthe HRI research community. The overall design maintains a balance of\naffordability and functionality. Initial Quori testing and a six-month\ndeployment are presented. Ten Quori platforms have been awarded to a diverse\ngroup of researchers from across the United States to facilitate HRI research\nto build a community database from a common platform.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.031,regular,pre_llm,2021,9,"{'ai_likelihood': 7.9141722785102e-06, 'text': 'Optimal Stroke Learning with Policy Gradient Approach for Robotic Table\n  Tennis\n\n  Learning to play table tennis is a challenging task for robots, as a wide\nvariety of strokes required. Recent advances have shown that deep Reinforcement\nLearning (RL) is able to successfully learn the optimal actions in a simulated\nenvironment. However, the applicability of RL in real scenarios remains limited\ndue to the high exploration effort. In this work, we propose a realistic\nsimulation environment in which multiple models are built for the dynamics of\nthe ball and the kinematics of the robot. Instead of training an end-to-end RL\nmodel, a novel policy gradient approach with TD3 backbone is proposed to learn\nthe racket strokes based on the predicted state of the ball at the hitting\ntime. In the experiments, we show that the proposed approach significantly\noutperforms the existing RL methods in simulation. Furthermore, to cross the\ndomain from simulation to reality, we adopt an efficient retraining method and\ntest it in three real scenarios. The resulting success rate is 98% and the\ndistance error is around 24.9 cm. The total training time is about 1.5 hours.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.06979,regular,pre_llm,2021,9,"{'ai_likelihood': 2.9802322387695312e-06, 'text': ""CORNET 2.0: A Co-Simulation Middleware for Robot Networks\n\n  We present a networked co-simulation framework for multi-robot systems\napplications. We require a simulation framework that captures both physical\ninteractions and communications aspects to effectively design such complex\nsystems. This is necessary to co-design the multi-robots' autonomy logic and\nthe communication protocols. The proposed framework extends existing tools to\nsimulate the robot's autonomy and network-related aspects. We have used Gazebo\nwith ROS/ROS2 to develop the autonomy logic for robots and mininet-WiFi as the\nnetwork simulator to capture the cyber-physical systems properties of the\nmulti-robot system. This framework addresses the need to seamlessly integrate\nthe two simulation environments by synchronizing mobility and time, allowing\nfor easy migration of the algorithms to real platforms. The framework supports\ncontainer-based virtualization and extends a generic robotic framework by\ndecoupling the data plane and control plane.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2109.04918,regular,pre_llm,2021,9,"{'ai_likelihood': 2.814663781060113e-06, 'text': 'Estimation and Adaption of Indoor Ego Airflow Disturbance with\n  Application to Quadrotor Trajectory Planning\n\n  It is ubiquitously accepted that during the autonomous navigation of the\nquadrotors, one of the most widely adopted unmanned aerial vehicles (UAVs),\nsafety always has the highest priority. However, it is observed that the ego\nairflow disturbance can be a significant adverse factor during flights, causing\npotential safety issues, especially in narrow and confined indoor environments.\nTherefore, we propose a novel method to estimate and adapt indoor ego airflow\ndisturbance of quadrotors, meanwhile applying it to trajectory planning.\nFirstly, the hover experiments for different quadrotors are conducted against\nthe proximity effects. Then with the collected acceleration variance, the\ndisturbances are modeled for the quadrotors according to the proposed\nformulation. The disturbance model is also verified under hover conditions in\ndifferent reconstructed complex environments. Furthermore, the approximation of\nHamilton-Jacobi reachability analysis is performed according to the estimated\ndisturbances to facilitate the safe trajectory planning, which consists of\nkinodynamic path search as well as B-spline trajectory optimization. The whole\nplanning framework is validated on multiple quadrotor platforms in different\nindoor environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.14817,regular,pre_llm,2021,10,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Similarity-Aware Skill Reproduction based on Multi-Representational\n  Learning from Demonstration\n\n  Learning from Demonstration (LfD) algorithms enable humans to teach new\nskills to robots through demonstrations. The learned skills can be robustly\nreproduced from the identical or near boundary conditions (e.g., initial\npoint). However, when generalizing a learned skill over boundary conditions\nwith higher variance, the similarity of the reproductions changes from one\nboundary condition to another, and a single LfD representation cannot preserve\na consistent similarity across a generalization region. We propose a novel\nsimilarity-aware framework including multiple LfD representations and a\nsimilarity metric that can improve skill generalization by finding\nreproductions with the highest similarity values for a given boundary\ncondition. Given a demonstration of the skill, our framework constructs a\nsimilarity region around a point of interest (e.g., initial point) by\nevaluating individual LfD representations using the similarity metric. Any\npoint within this volume corresponds to a representation that reproduces the\nskill with the greatest similarity. We validate our multi-representational\nframework in three simulated and four sets of real-world experiments using a\nphysical 6-DOF robot. We also evaluate 11 different similarity metrics and\ncategorize them according to their biases in 286 simulated experiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.14928,regular,pre_llm,2021,10,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Learning Actions for Drift-Free Navigation in Highly Dynamic Scenes\n\n  We embark on a hitherto unreported problem of an autonomous robot\n(self-driving car) navigating in dynamic scenes in a manner that reduces its\nlocalization error and eventual cumulative drift or Absolute Trajectory Error,\nwhich is pronounced in such dynamic scenes. With the hugely popular Velodyne-16\n3D LIDAR as the main sensing modality, and the accurate LIDAR-based\nLocalization and Mapping algorithm, LOAM, as the state estimation framework, we\nshow that in the absence of a navigation policy, drift rapidly accumulates in\nthe presence of moving objects. To overcome this, we learn actions that lead to\ndrift-minimized navigation through a suitable set of reward and penalty\nfunctions. We use Proximal Policy Optimization, a class of Deep Reinforcement\nLearning methods, to learn the actions that result in drift-minimized\ntrajectories. We show by extensive comparisons on a variety of synthetic, yet\nphoto-realistic scenes made available through the CARLA Simulator the superior\nperformance of the proposed framework vis-a-vis methods that do not adopt such\npolicies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.00489,regular,pre_llm,2021,10,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Integrated Modular Solution for Task Oriented Manipulator Configuration\n  Design\n\n  Modular and reconfigurable robotic systems have been designed to provide a\ncustomized solution for the non-repetitive tasks to be performed in a\nconstrained environment. Customized solutions are normally extracted from\ntask-based optimization of the possible manipulator configurations but the\nsolution are not integrated, for providing the modular compositions directly.\nIn this work, in the first phase, a strategy of finding unconventional optimal\nconfigurations with minimal number of degrees-of-freedom are discussed based\nupon the prescribed working locations and the cluttered environment. Then, in\nthe second phase, design of the modular and reconfigurable architecture is\npresented which can adapt these unconventional robotic parameters. Rather than\ngenerating and evolving the modular compositions, a strategy is presented\nthrough which the unconventional optimal configurations can be mapped directly\nto the modular compositions. The generated modular composition is validated\nusing Robot Operating System for the motion planning between the prescribed\nworking locations in a given cluttered environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.06553,regular,pre_llm,2021,10,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Spatial-temporal Transformers for EEG Emotion Recognition\n\n  Electroencephalography (EEG) is a popular and effective tool for emotion\nrecognition. However, the propagation mechanisms of EEG in the human brain and\nits intrinsic correlation with emotions are still obscure to researchers. This\nwork proposes four variant transformer frameworks~(spatial attention, temporal\nattention, sequential spatial-temporal attention and simultaneous\nspatial-temporal attention) for EEG emotion recognition to explore the\nrelationship between emotion and spatial-temporal EEG features. Specifically,\nspatial attention and temporal attention are to learn the topological structure\ninformation and time-varying EEG characteristics for emotion recognition\nrespectively. Sequential spatial-temporal attention does the spatial attention\nwithin a one-second segment and temporal attention within one sample\nsequentially to explore the influence degree of emotional stimulation on EEG\nsignals of diverse EEG electrodes in the same temporal segment. The\nsimultaneous spatial-temporal attention, whose spatial and temporal attention\nare performed simultaneously, is used to model the relationship between\ndifferent spatial features in different time segments. The experimental results\ndemonstrate that simultaneous spatial-temporal attention leads to the best\nemotion recognition accuracy among the design choices, indicating modeling the\ncorrelation of spatial and temporal features of EEG signals is significant to\nemotion recognition.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.0946,regular,pre_llm,2021,10,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'FAR Planner: Fast, Attemptable Route Planner using Dynamic Visibility\n  Update\n\n  The problem of path planning in unknown environments remains a challenging\nproblem - as the environment is gradually observed during the navigation, the\nunderlying planner has to update the environment representation and replan,\npromptly and constantly, to account for the new observations. In this paper, we\npresent a visibility graph-based planning framework capable of dealing with\nnavigation tasks in both known and unknown environments. The planner employs a\npolygonal representation of the environment and constructs the representation\nby extracting edge points around obstacles to form enclosed polygons. With\nthat, the method dynamically updates a global visibility graph using a\ntwo-layered data structure, expanding the visibility edges along with the\nnavigation and removing edges that become occluded by newly observed obstacles.\nWhen navigating in unknown environments, the method is attemptable in\ndiscovering a way to the goal by picking up the environment layout on the fly,\nupdating the visibility graph, and fast re-planning corresponding to the newly\nobserved environment. We evaluate the method in simulated and real-world\nsettings. The method shows the capability to attempt and navigate through\nunknown environments, reducing the travel time by up to 12-47% from\nsearch-based methods: A*, D* Lite, and more than 24-35% than sampling-based\nmethods: RRT*, BIT*, and SPARS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.1294,regular,pre_llm,2021,10,"{'ai_likelihood': 1.1920928955078125e-06, 'text': ""CoboGuider: Haptic Potential Fields for Safe Human-Robot Interaction\n\n  Modern industry still relies on manual manufacturing operations and safe\nhuman-robot interaction is of great interest nowadays. Speed and Separation\nMonitoring (SSM) allows close and efficient collaborative scenarios by\nmaintaining a protective separation distance during robot operation. The paper\nfocuses on a novel approach to strengthen the SSM safety requirements by\nintroducing haptic feedback to a robotic cell worker. Tactile stimuli provide\nearly warning of dangerous movements and proximity to the robot, based on the\nhuman reaction time and instantaneous velocities of robot and operator. A\npreliminary experiment was performed to identify the reaction time of\nparticipants when they are exposed to tactile stimuli in a collaborative\nenvironment with controlled conditions. In a second experiment, we evaluated\nour approach into a study case where human worker and cobot performed\ncollaborative planetary gear assembly. Results show that the applied approach\nincreased the average minimum distance between the robot's end-effector and\nhand by 44% compared to the operator relying only on the visual feedback.\nMoreover, the participants without the haptic support have failed several times\nto maintain the protective separation distance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.08639,regular,pre_llm,2021,10,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Partial Hierarchical Pose Graph Optimization for SLAM\n\n  In this paper we consider a hierarchical pose graph optimization (HPGO) for\nSimultaneous Localization and Mapping (SLAM). We propose a fast incremental\nprocedure for building hierarchy levels in pose graphs. We study the properties\nof this procedure and show that our solution delivers high execution speed,\nhigh reduction rate and good flexibility. We propose a way to do partial\nhierarchical optimization and compare it to other optimization modes. We show\nthat given a comparatively large amount of poses, partial HPGO gives a 10x\nspeed up comparing to the original optimization, not sacrificing the quality.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.01743,regular,pre_llm,2021,10,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Design and Characterization of a 3D-printed Pneumatically-driven\n  Bistable Valve with Tunable Characteristics\n\n  Although research studies in pneumatic soft robots develop rapidly, most\npneumatic actuators are still controlled by rigid valves and conventional\nelectronics. The existence of these rigid, electronic components sacrifices the\ncompliance and adaptability of soft robots.} Current electronics-free valve\ndesigns based on soft materials are facing challenges in behaviour consistency,\ndesign flexibility, and fabrication complexity. Taking advantages of soft\nmaterial 3D printing, this paper presents a new design of a bi-stable pneumatic\nvalve, which utilises two soft, pneumatically-driven, and\nsymmetrically-oriented conical shells with structural bistability to stabilise\nand regulate the airflow. The critical pressure required to operate the valve\ncan be adjusted by changing the design features of the soft bi-stable\nstructure. Multi-material printing simplifies the valve fabrication, enhances\nthe flexibility in design feature optimisations, and improves the system\nrepeatability. In this work, both a theoretical model and physical experiments\nare introduced to examine the relationships between the critical operating\npressure and the key design features. Results with valve characteristic tuning\nvia material stiffness changing show better effectiveness compared to the\nchange of geometry design features (demonstrated largest tunable critical\npressure range from 15.3 to 65.2 kPa and fastest response time $\\leq$ 1.8 s.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.00067,regular,pre_llm,2021,10,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Multi-Objective Autonomous Exploration on Real-Time Continuous Occupancy\n  Maps\n\n  Autonomous exploration in unknown environments using mobile robots is the\npillar of many robotic applications. Existing exploration frameworks either\nselect the nearest geometric frontier or the nearest information-theoretic\nfrontier. However, just because a frontier itself is informative does not\nnecessarily mean that the robot will be in an informative area after reaching\nthat frontier. To fill this gap, we propose to use a multi-objective variant of\nMonte-Carlo tree search that provides a non-myopic Pareto optimal action\nsequence leading the robot to a frontier with the greatest extent of unknown\narea uncovering. We also adopted Bayesian Hilbert Map (BHM) for continuous\noccupancy mapping and made it more applicable to real-time tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.14516,regular,pre_llm,2021,10,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Self-Contained Kinematic Calibration of a Novel Whole-Body Artificial\n  Skin for Human-Robot Collaboration\n\n  In this paper, we present an accelerometer-based kinematic calibration\nalgorithm to accurately estimate the pose of multiple sensor units distributed\nalong a robot body. Our approach is self-contained, can be used on any robot\nprovided with a Denavit-Hartenberg kinematic model, and on any skin equipped\nwith Inertial Measurement Units (IMUs). To validate the proposed method, we\nfirst conduct extensive experimentation in simulation and demonstrate a sub-cm\npositional error from ground truth data --an improvement of six times with\nrespect to prior work; subsequently, we then perform a real-world evaluation on\na seven degrees-of-freedom collaborative platform. For this purpose, we\nadditionally introduce a novel design for a stand-alone artificial skin\nequipped with an IMU for use with the proposed algorithm and a proximity sensor\nfor sensing distance to nearby objects. In conclusion, in this work, we\ndemonstrate seamless integration between a novel hardware design, an accurate\ncalibration method, and preliminary work on applications: the high positional\naccuracy effectively enables to locate distributed proximity data and allows\nfor a distributed avoidance controller to safely avoid obstacles and people\nwithout the need of additional sensing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.02904,regular,pre_llm,2021,10,"{'ai_likelihood': 0.0, 'text': 'CCO-VOXEL: Chance Constrained Optimization over Uncertain Voxel-Grid\n  Representation for Safe Trajectory Planning\n\n  We present CCO-VOXEL: the very first chance-constrained optimization (CCO)\nalgorithm that can compute trajectory plans with probabilistic safety\nguarantees in real-time directly on the voxel-grid representation of the world.\nCCO-VOXEL maps the distribution over the distance to the closest obstacle to a\ndistribution over collision-constraint violation and computes an optimal\ntrajectory that minimizes the violation probability. Importantly, unlike\nexisting works, we never assume the nature of the sensor uncertainty or the\nprobability distribution of the resulting collision-constraint violations. We\nleverage the notion of Hilbert Space embedding of distributions and Maximum\nMean Discrepancy (MMD) to compute a tractable surrogate for the original\nchance-constrained optimization problem and employ a combination of A* based\ngraph-search and Cross-Entropy Method for obtaining its minimum. We show\ntangible performance gain in terms of collision avoidance and trajectory\nsmoothness as a consequence of our probabilistic formulation vis a vis\nstate-of-the-art planning methods that do not account for such nonparametric\nnoise. Finally, we also show how a combination of low-dimensional feature\nembedding and pre-caching of Kernel Matrices of MMD allows us to achieve\nreal-time performance in simulations as well as in implementations on on-board\ncommodity hardware that controls the quadrotor flight\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.00541,regular,pre_llm,2021,10,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Validating Robotics Simulators on Real-World Impacts\n\n  A realistic simulation environment is an essential tool in every roboticist's\ntoolkit, with uses ranging from planning and control to training policies with\nreinforcement learning. Despite the centrality of simulation in modern\nrobotics, little work has been done to compare the performance of robotics\nsimulators against real-world data, especially for scenarios involving dynamic\nmotions with high speed impact events. Handling dynamic contact is the\ncomputational bottleneck for most simulations, and thus the modeling and\nalgorithmic choices surrounding impacts and friction form the largest\ndistinctions between popular tools. Here, we evaluate the ability of several\nsimulators to reproduce real-world trajectories involving impacts. Using\nexperimental data, we identify system-specific contact parameters of popular\nsimulators Drake, MuJoCo, and Bullet, analyzing the effects of modeling choices\naround these parameters. For the simple example of a cube tossed onto a table,\nsimulators capture inelastic impacts well while failing to capture elastic\nimpacts. For the higher-dimensional case of a Cassie biped landing from a jump,\nthe simulators capture the bulk motion well but the accuracy is limited by\nnumerous model differences between the real robot and the simulators.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.10436,review,pre_llm,2021,10,"{'ai_likelihood': 2.5166405571831598e-05, 'text': 'A Survey on Deep-Learning Approaches for Vehicle Trajectory Prediction\n  in Autonomous Driving\n\n  With the rapid development of machine learning, autonomous driving has become\na hot issue, making urgent demands for more intelligent perception and planning\nsystems. Self-driving cars can avoid traffic crashes with precisely predicted\nfuture trajectories of surrounding vehicles. In this work, we review and\ncategorize existing learning-based trajectory forecasting methods from\nperspectives of representation, modeling, and learning. Moreover, we make our\nimplementation of Target-driveN Trajectory Prediction publicly available at\nhttps://github.com/Henry1iu/TNT-Trajectory-Predition, demonstrating its\noutstanding performance whereas its original codes are withheld. Enlightenment\nis expected for researchers seeking to improve trajectory prediction\nperformance based on the achievement we have made.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.03119,regular,pre_llm,2021,10,"{'ai_likelihood': 1.4073318905300564e-05, 'text': 'Adaptive Safety Margin Estimation for Safe Real-Time Replanning under\n  Time-Varying Disturbance\n\n  Safe navigation in real-time is challenging because engineers need to work\nwith uncertain vehicle dynamics, variable external disturbances, and imperfect\ncontrollers. A common safety strategy is to inflate obstacles by hand-defined\nmargins. However, arbitrary static margins often fail in more dynamic\nscenarios, and using worst-case assumptions is overly conservative for most\nsettings where disturbances over time. In this work, we propose a middle\nground: safety margins that adapt on-the-fly. In an offline phase, we use Monte\nCarlo simulations to pre-compute a library of safety margins for multiple\nlevels of disturbance uncertainties. Then, at runtime, our system estimates the\ncurrent disturbance level to query the associated safety margins that best\ntrades off safety and performance. We validate our approach with extensive\nsimulated and real-world flight tests. We show that our adaptive method\nsignificantly outperforms static margins, allowing the vehicle to operate up to\n1.5 times faster than worst-case static margins while maintaining safety.\nVideo: https://youtu.be/SHzKHSUjdUU\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.11516,regular,pre_llm,2021,10,"{'ai_likelihood': 4.106097751193576e-06, 'text': ""Contact Anticipation for Physical Human-Robot Interaction with Robotic\n  Manipulators using Onboard Proximity Sensors\n\n  In this paper, we present a framework that unites obstacle avoidance and\ndeliberate physical interaction for robotic manipulators. As humans and robots\nbegin to coexist in work and household environments, pure collision avoidance\nis insufficient, as human-robot contact is inevitable and, in some situations,\ndesired. Our work enables manipulators to anticipate, detect, and act on\ncontact. To achieve this, we allow limited deviation from the robot's original\ntrajectory through velocity reduction and motion restrictions. Then, if contact\noccurs, a robot can detect it and maneuver based on a novel dynamic contact\nthresholding algorithm. The core contribution of this work is dynamic contact\nthresholding, which allows a manipulator with onboard proximity sensors to\ntrack nearby objects and reduce contact forces in anticipation of a collision.\nOur framework elicits natural behavior during physical human-robot interaction.\nWe evaluate our system on a variety of scenarios using the Franka Emika Panda\nrobot arm; collectively, our results demonstrate that our contribution is not\nonly able to avoid and react on contact, but also anticipate it.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.06195,regular,pre_llm,2021,10,"{'ai_likelihood': 2.430544959174262e-05, 'text': ""Planning Sensing Sequences for Subsurface 3D Tumor Mapping\n\n  Surgical automation has the potential to enable increased precision and\nreduce the per-patient workload of overburdened human surgeons. An effective\nautomation system must be able to sense and map subsurface anatomy, such as\ntumors, efficiently and accurately. In this work, we present a method that\nplans a sequence of sensing actions to map the 3D geometry of subsurface\ntumors. We leverage a sequential Bayesian Hilbert map to create a 3D\nprobabilistic occupancy model that represents the likelihood that any given\npoint in the anatomy is occupied by a tumor, conditioned on sensor readings. We\niteratively update the map, utilizing Bayesian optimization to determine\nsensing poses that explore unsensed regions of anatomy and exploit the\nknowledge gained by previous sensing actions. We demonstrate our method's\nefficiency and accuracy in three anatomical scenarios including a liver tumor\nscenario generated from a real patient's CT scan. The results show that our\nproposed method significantly outperforms comparison methods in terms of\nefficiency while detecting subsurface tumors with high accuracy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.12618,regular,pre_llm,2021,10,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Learning Insertion Primitives with Discrete-Continuous Hybrid Action\n  Space for Robotic Assembly Tasks\n\n  This paper introduces a discrete-continuous action space to learn insertion\nprimitives for robotic assembly tasks. Primitive is a sequence of elementary\nactions with certain exit conditions, such as ""pushing down the peg until\ncontact"". Since the primitive is an abstraction of robot control commands and\nencodes human prior knowledge, it reduces the exploration difficulty and yields\nbetter learning efficiency. In this paper, we learn robot assembly skills via\nprimitives. Specifically, we formulate insertion primitives as parameterized\nactions: hybrid actions consisting of discrete primitive types and continuous\nprimitive parameters. Compared with the previous work using a set of\ndiscretized parameters for each primitive, the agent in our method can freely\nchoose primitive parameters from a continuous space, which is more flexible and\nefficient. To learn these insertion primitives, we propose Twin-Smoothed\nMulti-pass Deep Q-Network (TS-MP-DQN), an advanced version of MP-DQN with twin\nQ-network to reduce the Q-value over-estimation. Extensive experiments are\nconducted in the simulation and real world for validation. From experiment\nresults, our approach achieves higher success rates than three baselines:\nMP-DQN with parameterized actions, primitives with discrete parameters, and\ncontinuous velocity control. Furthermore, learned primitives are robust to\nsim-to-real transfer and can generalize to challenging assembly tasks such as\ntight round peg-hole and complex shaped electric connectors with promising\nsuccess rates. Experiment videos are available at\nhttps://msc.berkeley.edu/research/insertion-primitives.html.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.11652,regular,pre_llm,2021,10,"{'ai_likelihood': 1.4007091522216797e-05, 'text': 'Action Planning for Packing Long Linear Elastic Objects into Compact\n  Boxes with Bimanual Robotic Manipulation\n\n  In this paper, we propose a new action planning approach to automatically\npack long linear elastic objects into common-size boxes with a bimanual robotic\nsystem. For that, we developed a hybrid geometric model to handle large-scale\nocclusions combining an online vision-based method and an offline reference\ntemplate. Then, a reference point generator is introduced to automatically plan\nthe reference poses for the predesigned action primitives. Finally, an action\nplanner integrates these components enabling the execution of high-level\nbehaviors and the accomplishment of packing manipulation tasks. To validate the\nproposed approach, we conducted a detailed experimental study with multiple\ntypes and lengths of objects and packing boxes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.09745,regular,pre_llm,2021,10,"{'ai_likelihood': 1.5331639183892144e-05, 'text': 'Path Planning for Optimal Coverage of Areas with Nonuniform Importance\n\n  Coverage of an inaccessible or challenging region with potential health and\nsafety hazards, such as in a volcanic region, is difficult yet crucial from\nscientific and meteorological perspectives. Areas contained within the region\noften provide valuable information of varying importance. We present an\nalgorithm to optimally cover a volcanic region in Hawai`i with an unmanned\naerial vehicle (UAV). The target region is assigned with a nonuniform coverage\nimportance score distribution. For a specified battery capacity of the UAV, the\noptimization problem seeks the path that maximizes the total coverage area and\nthe accumulated importance score while penalizing the revisiting of the same\narea. Trajectories are generated offline for the UAV based on the available\npower and coverage information map. The optimal trajectory minimizes the\nunspent battery power while enforcing that the UAV returns to its starting\nlocation. This multi-objective optimization problem is solved by using\nsequential quadratic programming. The details of the competitive optimization\nproblem are discussed along with the analysis and simulation results to\ndemonstrate the applicability of the proposed algorithm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2110.01749,regular,pre_llm,2021,10,"{'ai_likelihood': 1.2252065870496963e-05, 'text': 'Set-theoretic Localization for Mobile Robots with Infrastructure-based\n  Sensing\n\n  In this paper, we introduce a set-theoretic approach for mobile robot\nlocalization with infrastructure-based sensing. The proposed method computes\nsets that over-bound the robot body and orientation under an assumption of\nknown noise bounds on the sensor and robot motion model. We establish\ntheoretical properties and computational approaches for this set-theoretic\nlocalization approach and illustrate its application to an automated valet\nparking example in simulations and to omnidirectional robot localization\nproblems in real-world experiments. We demonstrate that the set-theoretic\nlocalization method can perform robustly against uncertainty set initialization\nand sensor noises compared to the FastSLAM.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.10948,regular,pre_llm,2021,11,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Hybrid Imitative Planning with Geometric and Predictive Costs in\n  Off-road Environments\n\n  Geometric methods for solving open-world off-road navigation tasks, by\nlearning occupancy and metric maps, provide good generalization but can be\nbrittle in outdoor environments that violate their assumptions (e.g., tall\ngrass). Learning-based methods can directly learn collision-free behavior from\nraw observations, but are difficult to integrate with standard geometry-based\npipelines. This creates an unfortunate conflict -- either use learning and lose\nout on well-understood geometric navigational components, or do not use it, in\nfavor of extensively hand-tuned geometry-based cost maps. In this work, we\nreject this dichotomy by designing the learning and non-learning-based\ncomponents in a way such that they can be effectively combined in a\nself-supervised manner. Both components contribute to a planning criterion: the\nlearned component contributes predicted traversability as rewards, while the\ngeometric component contributes obstacle cost information. We instantiate and\ncomparatively evaluate our system in both in-distribution and\nout-of-distribution environments, showing that this approach inherits\ncomplementary gains from the learned and geometric components and significantly\noutperforms either of them. Videos of our results are hosted at\nhttps://sites.google.com/view/hybrid-imitative-planning\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.14972,regular,pre_llm,2021,11,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Design and Control of a Recovery System for Legged Robots\n\n  This paper describes the design and control of a support and recovery system\nfor use with planar legged robots. The system operates in three modes. First,\nit can be operated in a fully transparent mode where no forces are applied to\nthe robot. In this mode, the system follows the robot closely to be able to\nquickly catch the robot if needed. Second, it can provide a vertical supportive\nforce to assist a robot during operation. Third, it can catch the robot and\npull it away from the ground after a failure to avoid falls and the associated\ndamages. In this mode, the system automatically resets the robot after a trial\nallowing for multiple consecutive trials to be run without manual intervention.\nThe supportive forces are applied to the robot through an actuated cable and\npulley system that uses series elastic actuation with a unidirectional spring\nto enable truly transparent operation. The nonlinear nature of this system\nnecessitates careful design of controllers to ensure predictable, safe\nbehaviors. In this paper we introduce the mechatronic design of the recovery\nsystem, develop suitable controllers, and evaluate the system's performance on\nthe bipedal robot RAMone.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02999,regular,pre_llm,2021,11,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Dynamic Mirror Descent based Model Predictive Control for Accelerating\n  Robot Learning\n\n  Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL\nalgorithms with model-based (Mb)-RL approaches to get the best from both:\nasymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired\nby these works, we propose a hierarchical framework that integrates online\nlearning for the Mb-trajectory optimization with off-policy methods for the\nMf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent\nbased Model Predictive Control (DMD-MPC) is used as the inner loop Mb-RL to\nobtain an optimal sequence of actions. These actions are in turn used to\nsignificantly accelerate the outer loop Mf-RL. We show that our formulation is\ngeneric for a broad class of MPC-based policies and objectives, and includes\nsome of the well-known Mb-Mf approaches. We finally introduce a new algorithm:\nMirror-Descent Model Predictive RL (M-DeMoRL), which uses Cross-Entropy Method\n(CEM) with elite fractions for the inner loop. Our experiments show faster\nconvergence of the proposed hierarchical approach on benchmark MuJoCo tasks. We\nalso demonstrate hardware training for trajectory tracking in a 2R leg and\nhardware transfer for robust walking in a quadruped. We show that the\ninner-loop Mb-RL significantly decreases the number of training iterations\nrequired in the real system, thereby validating the proposed approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.1236,regular,pre_llm,2021,11,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Fault-Tolerant Perception for Automated Driving A Lightweight Monitoring\n  Approach\n\n  While the most visible part of the safety verification process of automated\nvehicles concerns the planning and control system, it is often overlooked that\nsafety of the latter crucially depends on the fault-tolerance of the preceding\nenvironment perception. Modern perception systems feature complex and often\nmachine-learning-based components with various failure modes that can\njeopardize the overall safety. At the same time, a verification by for example\nredundant execution is not always feasible due to resource constraints. In this\npaper, we address the need for feasible and efficient perception monitors and\npropose a lightweight approach that helps to protect the integrity of the\nperception system while keeping the additional compute overhead minimal. In\ncontrast to existing solutions, the monitor is realized by a well-balanced\ncombination of sensor checks -- here using LiDAR information -- and\nplausibility checks on the object motion history. It is designed to detect\nrelevant errors in the distance and velocity of objects in the environment of\nthe automated vehicle. In conjunction with an appropriate planning system, such\na monitor can help to make safe automated driving feasible.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.01376,regular,pre_llm,2021,11,"{'ai_likelihood': 6.821420457628038e-06, 'text': 'SEED: Series Elastic End Effectors in 6D for Visuotactile Tool Use\n\n  We propose the framework of Series Elastic End Effectors in 6D (SEED), which\ncombines a spatially compliant element with visuotactile sensing to grasp and\nmanipulate tools in the wild. Our framework generalizes the benefits of series\nelasticity to 6-dof, while providing an abstraction of control using\nvisuotactile sensing. We propose an algorithm for relative pose estimation from\nvisuotactile sensing, and a spatial hybrid force-position controller capable of\nachieving stable force interaction with the environment. We demonstrate the\neffectiveness of our framework on tools that require regulation of spatial\nforces. Video link: https://youtu.be/2-YuIfspDrk\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.06454,regular,pre_llm,2021,11,"{'ai_likelihood': 1.3543499840630426e-05, 'text': 'Towards Transferring Human Preferences from Canonical to Actual Assembly\n  Tasks\n\n  To assist human users according to their individual preference in assembly\ntasks, robots typically require user demonstrations in the given task. However,\nproviding demonstrations in actual assembly tasks can be tedious and\ntime-consuming. Our thesis is that we can learn user preferences in assembly\ntasks from demonstrations in a representative canonical task. Inspired by\nprevious work in economy of human movement, we propose to represent user\npreferences as a linear function of abstract task-agnostic features, such as\nmovement and physical and mental effort required by the user. For each user, we\nlearn their preference from demonstrations in a canonical task and use the\nlearned preference to anticipate their actions in the actual assembly task\nwithout any user demonstrations in the actual task. We evaluate our proposed\nmethod in a model-airplane assembly study and show that preferences can be\neffectively transferred from canonical to actual assembly tasks, enabling\nrobots to anticipate user actions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.07438,regular,pre_llm,2021,11,"{'ai_likelihood': 3.7087334526909726e-06, 'text': 'Methods for Combining and Representing Non-Contextual Autonomy Scores\n  for Unmanned Aerial Systems\n\n  Measuring an overall autonomy score for a robotic system requires the\ncombination of a set of relevant aspects and features of the system that might\nbe measured in different units, qualitative, and/or discordant. In this paper,\nwe build upon an existing non-contextual autonomy framework that measures and\ncombines the Autonomy Level and the Component Performance of a system as\noverall autonomy score. We examine several methods of combining features,\nshowing how some methods find different rankings of the same data, and we\nemploy the weighted product method to resolve this issue. Furthermore, we\nintroduce the non-contextual autonomy coordinate and represent the overall\nautonomy of a system with an autonomy distance. We apply our method to a set of\nseven Unmanned Aerial Systems (UAS) and obtain their absolute autonomy score as\nwell as their relative score with respect to the best system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.13981,regular,pre_llm,2021,11,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Kilometer-scale autonomous navigation in subarctic forests: challenges\n  and lessons learned\n\n  Challenges inherent to autonomous wintertime navigation in forests include\nlack of reliable a Global Navigation Satellite System (GNSS) signal, low\nfeature contrast, high illumination variations and changing environment. This\ntype of off-road environment is an extreme case of situations autonomous cars\ncould encounter in northern regions. Thus, it is important to understand the\nimpact of this harsh environment on autonomous navigation systems. To this end,\nwe present a field report analyzing teach-and-repeat navigation in a subarctic\nforest while subject to fluctuating weather, including light and heavy snow,\nrain and drizzle. First, we describe the system, which relies on point cloud\nregistration to localize a mobile robot through a boreal forest, while\nsimultaneously building a map. We experimentally evaluate this system in over\n18.8 km of autonomous navigation in the teach-and-repeat mode. Over 14 repeat\nruns, only four manual interventions were required, three of which were due to\nlocalization failure and another one caused by battery power outage. We show\nthat dense vegetation perturbs the GNSS signal, rendering it unsuitable for\nnavigation in forest trails. Furthermore, we highlight the increased\nuncertainty related to localizing using point cloud registration in forest\ntrails. We demonstrate that it is not snow precipitation, but snow\naccumulation, that affects our system's ability to localize within the\nenvironment. Finally, we expose some challenges and lessons learned from our\nfield campaign to support better experimental work in winter conditions. Our\ndataset is available online.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.04814,regular,pre_llm,2021,11,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Planar Robot Casting with Real2Sim2Real Self-Supervised Learning\n\n  This paper introduces the task of {\\em Planar Robot Casting (PRC)}: where one\nplanar motion of a robot arm holding one end of a cable causes the other end to\nslide across the plane toward a desired target. PRC allows the cable to reach\npoints beyond the robot workspace and has applications for cable management in\nhomes, warehouses, and factories. To efficiently learn a PRC policy for a given\ncable, we propose Real2Sim2Real, a self-supervised framework that automatically\ncollects physical trajectory examples to tune parameters of a dynamics\nsimulator using Differential Evolution, generates many simulated examples, and\nthen learns a policy using a weighted combination of simulated and physical\ndata. We evaluate Real2Sim2Real with three simulators, Isaac Gym-segmented,\nIsaac Gym-hybrid, and PyBullet, two function approximators, Gaussian Processes\nand Neural Networks (NNs), and three cables with differing stiffness, torsion,\nand friction. Results with 240 physical trials suggest that the PRC policies\ncan attain median error distance (as % of cable length) ranging from 8% to 14%,\noutperforming baselines and policies trained on only real or only simulated\nexamples. Code, data, and videos are available at\nhttps://tinyurl.com/robotcast.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.02046,regular,pre_llm,2021,11,"{'ai_likelihood': 1.6788641611735027e-05, 'text': 'An adaptive recursive sliding mode attitude control for tiltrotor UAV in\n  flight mode transition based on super-twisting extended state observer\n\n  With the characteristics of vertical take-off and landing and long endurance,\ntiltrotor has attracted considerable attention in recent decades for its\npotential applications in civil and scientific research. However, the problems\nof strong couplings, nonlinear characteristics and mismatched disturbances\ninevitably exist in the tiltrotor, which bring great challenges to the\ncontroller design in transition mode. In this paper, we combined a\nsuper-twisting extended state observer (STESO) with an adaptive recursive\nsliding mode control (ARSMC) together to design a tiltrotor aircraft attitude\nsystem controller in transition mode using STESO-ARSMC (SAC). Firstly, the six\ndegrees of freedom (DOF) nonlinear mathematical model of tiltrotor is\nestablished. Secondly, the states and disturbances are estimated by the STES\nobserver. Thirdly, ARSM controller is designed to achieve finite time\nconvergence. The Lyapunov function is used to testify the convergence of the\ntiltrotor UAV system. The new aspect is that the assessments of the states are\nincorporated into the control rules to adjust for disruptions. When compared to\nprior techniques, the control system proposed in this work can considerably\nenhance anti-disturbance performance. Finally, simulation tests are used to\ndemonstrate the efficacy of the suggested technique.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08873,regular,pre_llm,2021,11,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Adaptive Lookahead Pure-Pursuit for Autonomous Racing\n\n  This paper presents an adaptive lookahead pure-pursuit lateral controller for\noptimizing racing metrics such as lap time, average lap speed, and deviation\nfrom a reference trajectory in an autonomous racing scenario. We propose a\ngreedy algorithm to compute and assign optimal lookahead distances for the\npure-pursuit controller for each waypoint on a reference trajectory for\nimproving the race metrics. We use a ROS based autonomous racing simulator to\nevaluate the adaptive pure-pursuit algorithm and compare our method with\nseveral other pure-pursuit based lateral controllers. We also demonstrate our\napproach on a scaled real testbed using a F1/10 autonomous racecar. Our method\nresults in a significant improvement (20%) in the racing metrics for an\nautonomous racecar.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08097,regular,pre_llm,2021,11,"{'ai_likelihood': 0.0, 'text': 'Virtual Reality for Synergistic Surgical Training and Data Generation\n\n  Surgical simulators not only allow planning and training of complex\nprocedures, but also offer the ability to generate structured data for\nalgorithm development, which may be applied in image-guided computer assisted\ninterventions. While there have been efforts on either developing training\nplatforms for surgeons or data generation engines, these two features, to our\nknowledge, have not been offered together. We present our developments of a\ncost-effective and synergistic framework, named Asynchronous Multibody\nFramework Plus (AMBF+), which generates data for downstream algorithm\ndevelopment simultaneously with users practicing their surgical skills. AMBF+\noffers stereoscopic display on a virtual reality (VR) device and haptic\nfeedback for immersive surgical simulation. It can also generate diverse data\nsuch as object poses and segmentation maps. AMBF+ is designed with a flexible\nplugin setup which allows for unobtrusive extension for simulation of different\nsurgical procedures. We show one use case of AMBF+ as a virtual drilling\nsimulator for lateral skull-base surgery, where users can actively modify the\npatient anatomy using a virtual surgical drill. We further demonstrate how the\ndata generated can be used for validating and training downstream computer\nvision algorithms\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.12793,regular,pre_llm,2021,11,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Bacteria-inspired robotic propulsion from bundling of soft helical\n  filaments at low Reynolds number\n\n  The bundling of flagella is known to create a ""run"" phase, where the bacteria\nmoves in a nearly straight line rather than making changes in direction.\nHistorically, mechanical explanations for the bundling phenomenon intrigued\nmany researchers, and significant advances were made in physical models and\nexperimental methods. Contributing to the field of research, we present a\nbacteria-inspired centimeter-scale soft robotic hardware platform and a\ncomputational framework for a physically plausible simulation model of the\nmulti-flagellated robot under low Reynolds number (~0.1). The fluid-structure\ninteraction simulation couples the Discrete Elastic Rods algorithm with the\nmethod of Regularized Stokeslet Segments. Contact between two flagella is\nhandled by a penalty-based method. We present a comparison between our\nexperimental and simulation results and verify that the simulation tool can\ncapture the essential physics of this problem. Preliminary findings on\nrobustness to buckling provided by the bundling phenomenon and the efficiency\nof a multi-flagellated soft robot are compared with the single-flagellated\ncounterparts. Observations were made on the coupling between geometry and\nelasticity, which manifests itself in the propulsion of the robot by nonlinear\ndependency on the rotational speed of the flagella.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.14314,regular,pre_llm,2021,11,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Braking and Body Angles Control of an Insect-Computer Hybrid Robot by\n  Electrical Stimulation of Beetle Flight Muscle in Free Flight\n\n  While engineers put lots of effort, resources, and time in building insect\nscale micro aerial vehicles (MAVs) that fly like insects, insects themselves\nare the real masters of flight. What if we would use living insect as platform\nfor MAV instead? Here, we reported a flight control via electrical stimulation\nof a flight muscle of an insect-computer hybrid robot, which is the interface\nof a mountable wireless backpack controller and a living beetle. The beetle\nuses indirect flight muscles to drive wing flapping and three major direct\nflight muscles (basalar, subalar and third axilliary (3Ax) muscles) to control\nthe kinematics of the wings for flight maneuver. While turning control was\nalready achieved by stimulating basalar and 3Ax muscles, electrical stimulation\nof subalar muscles resulted in braking and elevation control in flight. We also\ndemonstrated around 20 degrees of contralateral yaw and roll by stimulating\nindividual subalar muscle. Stimulating both subalar muscles lead to an increase\nof 20 degrees in pitch and decelerate the flight by 1.5 m/s2 as well as an\ninduce an elevation of 2 m/s2.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08248,regular,pre_llm,2021,11,"{'ai_likelihood': 0.00011834833357069228, 'text': ""Active Vapor-Based Robotic Wiper\n\n  This paper presents a method for estimating normals of mirrors and\ntransparent objects challenging for cameras to recognize. We propose spraying\nwater vapor onto mirror or transparent surfaces to create a diffuse reflective\nsurface. Using an ultrasonic humidifier on a robotic arm, we apply water vapor\nto the target object's surface, forming a cross-shaped misted area. This\ncreates partially diffuse reflective surfaces, enabling the camera to detect\nthe target object's surface. Adjusting the gripper-mounted camera viewpoint\nmaximizes the extracted misted area's appearance in the image, allowing normal\nestimation of the target surface. Experiments show the method's effectiveness,\nwith RMSEs of azimuth estimation for mirrors and transparent glass at\napproximately 4.2 and 5.8 degrees, respectively. Our robot experiments\ndemonstrated that our robotic wiper can perform contact-force-regulated wiping\nmotions to clean a transparent window, akin to human performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.13815,regular,pre_llm,2021,11,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'GATER: Learning Grasp-Action-Target Embeddings and Relations for\n  Task-Specific Grasping\n\n  Intelligent service robots require the ability to perform a variety of tasks\nin dynamic environments. Despite the significant progress in robotic grasping,\nit is still a challenge for robots to decide grasping position when given\ndifferent tasks in unstructured real life environments. In order to overcome\nthis challenge, creating a proper knowledge representation framework is the\nkey. Unlike the previous work, in this paper, task is defined as a triplet\nincluding grasping tool, desired action and target object. Our proposed\nalgorithm GATER (Grasp--Action--Target Embeddings and Relations) models the\nrelationship among grasping tools--action--target objects in embedding space.\nTo validate our method, a novel dataset is created for task-specific grasping.\nGATER is trained on the new dataset and achieve task-specific grasping\ninference with 94.6\\% success rate. Finally, the effectiveness of GATER\nalgorithm is tested on a real service robot platform. GATER algorithm has its\npotential in human behavior prediction and human-robot interaction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.06271,regular,pre_llm,2021,11,"{'ai_likelihood': 3.443823920355903e-06, 'text': 'Multi-Resolution Elevation Mapping and Safe Landing Site Detection with\n  Applications to Planetary Rotorcraft\n\n  In this paper, we propose a resource-efficient approach to provide an\nautonomous UAV with an on-board perception method to detect safe, hazard-free\nlanding sites during flights over complex 3D terrain. We aggregate 3D\nmeasurements acquired from a sequence of monocular images by a\nStructure-from-Motion approach into a local, robot-centric, multi-resolution\nelevation map of the overflown terrain, which fuses depth measurements\naccording to their lateral surface resolution (pixel-footprint) in a\nprobabilistic framework based on the concept of dynamic Level of Detail. Map\naggregation only requires depth maps and the associated poses, which are\nobtained from an onboard Visual Odometry algorithm. An efficient landing site\ndetection method then exploits the features of the underlying multi-resolution\nmap to detect safe landing sites based on slope, roughness, and quality of the\nreconstructed terrain surface. The evaluation of the performance of the mapping\nand landing site detection modules are analyzed independently and jointly in\nsimulated and real-world experiments in order to establish the efficacy of the\nproposed approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.03088,regular,pre_llm,2021,11,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Learning to Manipulate Tools by Aligning Simulation to Video\n  Demonstration\n\n  A seamless integration of robots into human environments requires robots to\nlearn how to use existing human tools. Current approaches for learning tool\nmanipulation skills mostly rely on expert demonstrations provided in the target\nrobot environment, for example, by manually guiding the robot manipulator or by\nteleoperation. In this work, we introduce an automated approach that replaces\nan expert demonstration with a Youtube video for learning a tool manipulation\nstrategy. The main contributions are twofold. First, we design an alignment\nprocedure that aligns the simulated environment with the real-world scene\nobserved in the video. This is formulated as an optimization problem that finds\na spatial alignment of the tool trajectory to maximize the sparse goal reward\ngiven by the environment. Second, we describe an imitation learning approach\nthat focuses on the trajectory of the tool rather than the motion of the human.\nFor this we combine reinforcement learning with an optimization procedure to\nfind a control policy and the placement of the robot based on the tool motion\nin the aligned environment. We demonstrate the proposed approach on spade,\nscythe and hammer tools in simulation, and show the effectiveness of the\ntrained policy for the spade on a real Franka Emika Panda robot demonstration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.08971,regular,pre_llm,2021,11,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'On the modification of the SPARUS II AUV for close range imaging survey\n  platform\n\n  Based on the need for high resolution underwater visual surveys, this study\npresents the adaptation of an existing SPARUS II autonomous underwater vehicle\n(AUV) into an entirely hovering AUV fully capable of performing autonomous,\nclose range imaging survey missions. This paper focuses on the enhancement of\nthe AUVs maneuvering capability (enabling improved maneuvering control),\nimplementation of an state of the art thruster allocation algorithm (allowing\noptimal thrusters allocation and thrusters redundancy), and the development of\nan upgraded path following controller to facilitate precise and delicate\nmotions necessary for high resolution imaging missions. To facilitate the\nvehicles adaptation, a dynamic model is developed. The calibration process of\nthe dynamic model coefficients initially obtained using well accepted formulas,\nby computational fluid dynamics and in real sea experiments is presented. The\nin house development of a pressure resistant imaging system is also presented.\nThis system which includes a stereo camera and high power lightning strobes was\ndeveloped and fitted as a dedicated AUV payload. Finally, the performance of\nthe platform is demonstrated in an actual seabed visual survey mission.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2111.03712,regular,pre_llm,2021,11,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Using Monocular Vision and Human Body Priors for AUVs to Autonomously\n  Approach Divers\n\n  Direct communication between humans and autonomous underwater vehicles (AUVs)\nis a relatively underexplored area in human-robot interaction (HRI) research,\nalthough many tasks (\\eg surveillance, inspection, and search-and-rescue)\nrequire close diver-robot collaboration. Many core functionalities in this\ndomain are in need of further study to improve robotic capabilities for ease of\ninteraction. One of these is the challenge of autonomous robots approaching and\npositioning themselves relative to divers to initiate and facilitate\ninteractions. Suboptimal AUV positioning can lead to poor quality interaction\nand lead to excessive cognitive and physical load for divers. In this paper, we\nintroduce a novel method for AUVs to autonomously navigate and achieve\ndiver-relative positioning to begin interaction. Our method is based only on\nmonocular vision, requires no global localization, and is computationally\nefficient. We present our algorithm along with an implementation of said\nalgorithm on board both a simulated and physical AUV, performing extensive\nevaluations in the form of closed-water tests in a controlled pool. Analysis of\nour results show that the proposed monocular vision-based algorithm performs\nreliably and efficiently operating entirely on-board the AUV.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02356,regular,pre_llm,2021,12,"{'ai_likelihood': 0.0, 'text': ""A Bio-inspired Modular System for Humanoid Posture Control\n\n  Bio-inspired sensorimotor control systems may be appealing to roboticists who\ntry to solve problems of multiDOF humanoids and human-robot interactions. This\npaper presents a simple posture control concept from neuroscience, called\ndisturbance estimation and compensation, DEC concept [1]. It provides\nhuman-like mechanical compliance due to low loop gain, tolerance of time\ndelays, and automatic adjustment to changes in external disturbance scenarios.\nIts outstanding feature is that it uses feedback of multisensory disturbance\nestimates rather than 'raw' sensory signals for disturbance compensation. After\nproof-of-principle tests in 1 and 2 DOF posture control robots, we present here\na generalized DEC control module for multi-DOF robots. In the control layout,\none DEC module controls one DOF (modular control architecture). Modules of\nneighboring joints are synergistically interconnected using vestibular\ninformation in combination with joint angle and torque signals. These sensory\ninterconnections allow each module to control the kinematics of the more distal\nlinks as if they were a single link. This modular design makes the complexity\nof the robot control scale linearly with the DOFs and error robustness high\ncompared to monolithic control architectures. The presented concept uses\nMatlab/Simulink (The MathWorks, Natick, USA) for both, model simulation and\nrobot control and will be available as open library\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.09075,regular,pre_llm,2021,12,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'A minimalistic stochastic dynamics model of cluttered obstacle traversal\n\n  Robots are still poor at traversing cluttered large obstacles required for\nimportant applications like search and rescue. By contrast, animals are\nexcellent at doing so, often using direct physical interaction with obstacles\nrather than avoiding them. Here, towards understanding the dynamics of\ncluttered obstacle traversal, we developed a minimalistic stochastic dynamics\nsimulation inspired by our recent study of insects traversing grass-like beams.\nThe 2-D model system consists of a forward self-propelled circular locomotor\ntranslating on a frictionless level plane with a lateral random force and\ninteracting with two adjacent horizontal beams that form a gate. We found that\ntraversal probability increases monotonically with propulsive force, but first\nincreases then decreases with random force magnitude. For asymmetric beams with\ndifferent stiffness, traversal is more likely towards the side of the less\nstiff beam. These observations are in accord with those expected from a\npotential energy landscape approach. Furthermore, we extended the single gate\nin a lattice configuration to form a large cluttered obstacle field. A Markov\nchain Monte Carlo method was applied to predict traversal in the large field,\nusing the input-output probability map obtained from single gate simulations.\nThis method achieved high accuracy in predicting the statistical distribution\nof the final location of the body within the obstacle field, while saving\ncomputation time by a factor of 10^5 over our dynamic simulation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02057,regular,pre_llm,2021,12,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Snake Robot Gait Decomposition and Gait Parameter Optimization\n\n  This paper proposes Gait Decomposition (G.D), a method of mathematically\ndecomposing snake movements, and Gait Parameter Gradient (GPG), a method of\noptimizing decomposed gait parameters. G.D is a method that can express the\nsnake gait mathematically and concisely from generating movement using the\ncurve function to the motor control order when generating movement of snake\nrobot. Through this method, the gait of the snake robot can be intuitively\nclassified into a matrix, as well as flexibly adjusting the parameters of the\ncurve function required for gait generation. This can solve the problem that\nparameter tuning, which is the reason why it is difficult for a snake robot to\npractical use, is difficult. Therefore, if this G.D is applied to snake robots,\nvarious gaits can be generated with a few of parameters, so snake robots can be\nused in many fields. We also implemented the GPG algorithm to optimize the gait\ncurve function as well as define the gait of the snake robot through G.D.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.03685,regular,pre_llm,2021,12,"{'ai_likelihood': 3.940529293484158e-06, 'text': 'A low-cost wave-solar powered Unmanned Surface Vehicle\n\n  This paper presents a prototype of a low-cost Unmanned Surface Vehicle (USV)\nthat is operated by wave and solar energy which can be used to minimize the\ncost of ocean data collection. The current prototype is a compact USV, with a\nlength of 1.2m that can be deployed and recovered by two persons. The design\nincludes an electrically operated winch that can be used to retract and lower\nthe underwater unit. Several elements of the design make use of additive\nmanufacturing and inexpensive materials. The vehicle can be controlled using\nradio frequency (RF) and a satellite communication, through a custom developed\nweb application. Both the surface and underwater units were optimized with\nregard to drag, lift, weight, and price by using recommendation of previous\nresearch work and advanced materials. The USV could be used in water condition\nmonitoring by measuring several parameters, such as dissolved oxygen, salinity,\ntemperature, and pH.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.09988,regular,pre_llm,2021,12,"{'ai_likelihood': 7.0532162984212245e-06, 'text': 'Smooth Model Predictive Path Integral Control without Smoothing\n\nWe present a sampling-based control approach that can generate smooth actions for general nonlinear systems without external smoothing algorithms. Model Predictive Path Integral (MPPI) control has been utilized in numerous robotic applications due to its appealing characteristics to solve non-convex optimization problems. However, the stochastic nature of sampling-based methods can cause significant chattering in the resulting commands. Chattering becomes more prominent in cases where the environment changes rapidly, possibly even causing the MPPI to diverge. To address this issue, we propose a method that seamlessly combines MPPI with an input-lifting strategy. In addition, we introduce a new action cost to smooth control sequence during trajectory rollouts while preserving the information theoretic interpretation of MPPI, which was derived from non-affine dynamics. We validate our method in two nonlinear control tasks with neural network dynamics: a pendulum swing-up task and a challenging autonomous driving task. The experimental results demonstrate that our method outperforms the MPPI baselines with additionally applied smoothing algorithms.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.10285,regular,pre_llm,2021,12,"{'ai_likelihood': 5.695554945203993e-06, 'text': ""Distributed Adaptive and Resilient Control of Multi-Robot Systems with\n  Limited Field of View Interactions\n\n  In this paper, we consider two coupled problems for distributed multi-robot\nsystems (MRSs) coordinating with limited field of view (FOV) sensors: adaptive\ntuning of interaction gains and rejection of sensor attacks. First, a typical\nshortcoming of distributed control frameworks (e.g., potential fields) is that\nthe overall system behavior is highly sensitive to the gain assigned to\nrelative interactions. Second, MRSs with limited FOV sensors can be more\nsusceptible to sensor attacks aimed at their FOVs, and therefore must be\nresilient to such attacks. Based on these shortcomings, we propose a\ncomprehensive solution that combines efforts in adaptive gain tuning and attack\nresilience to the problem of topology control for MRSs with limited FOVs.\nSpecifically, we first derive an adaptive gain tuning scheme based on\nsatisfying nominal pairwise interactions, which yields a dynamic balancing of\ninteraction strengths in a robot's neighborhood. We then model additive sensor\nand actuator attacks (or faults) and derive H infinity control protocols by\nemploying a static output-feedback technique, guaranteeing bounded L2 gains of\nthe error induced by the attack (fault) signals. Finally, simulation results\nusing ROS Gazebo are provided to support our theoretical findings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.14225,regular,pre_llm,2021,12,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Robotic Hand Rehabilitation System\n\n  Rehabilitation exercises are essential to ensure speedy recovery of stroke\npatients. An automated system to assist the patient in performing a\nrehabilitation exercise repeatedly is designed. The design process is presented\nin this report.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08426,regular,pre_llm,2021,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Contact simulation of a 2D Bipedal Robot kicking a ball\n\n  This report describes an approach for simulating multi-body contacts of\nactively-controlled systems. In this work, we focus on the controls and contact\nsimulation of a 2-dimensional bipedal robot kicking a circular ball.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.08106,regular,pre_llm,2021,12,"{'ai_likelihood': 7.106198204888238e-05, 'text': 'Enhance Connectivity of Promising Regions for Sampling-based Path\n  Planning\n\n  Sampling-based path planning algorithms usually implement uniform sampling\nmethods to search the state space. However, uniform sampling may lead to\nunnecessary exploration in many scenarios, such as the environment with a few\ndead ends. Our previous work proposes to use the promising region to guide the\nsampling process to address the issue. However, the predicted promising regions\nare often disconnected, which means they cannot connect the start and goal\nstate, resulting in a lack of probabilistic completeness. This work focuses on\nenhancing the connectivity of predicted promising regions. Our proposed method\nregresses the connectivity probability of the edges in the x and y directions.\nIn addition, it calculates the weight of the promising edges in loss to guide\nthe neural network to pay more attention to the connectivity of the promising\nregions. We conduct a series of simulation experiments, and the results show\nthat the connectivity of promising regions improves significantly. Furthermore,\nwe analyze the effect of connectivity on sampling-based path planning\nalgorithms and conclude that connectivity plays an essential role in\nmaintaining algorithm performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.02162,regular,pre_llm,2021,12,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'A Low-cost Robot with Autonomous Recharge and Navigation for Weed\n  Control in Fields with Narrow Row Spacing\n\n  Modern herbicide application in agricultural settings typically relies on\neither large scale sprayers that dispense herbicide over crops and weeds alike\nor portable sprayers that require labor intensive manual operation. The former\nmethod results in overuse of herbicide and reduction in crop yield while the\nlatter is often untenable in large scale operations. This paper presents the\nfirst fully autonomous robot for weed management for row crops capable of\ncomputer vision based navigation, weed detection, complete field coverage, and\nautomatic recharge for under \\$400. The target application is autonomous\ninter-row weed control in crop fields, e.g. flax and canola, where the spacing\nbetween croplines is as small as one foot. The proposed robot is small enough\nto pass between croplines at all stages of plant growth while detecting weeds\nand spraying herbicide. A recharging system incorporates newly designed robotic\nhardware, a ramp, a robotic charging arm, and a mobile charging station. An\nintegrated vision algorithm is employed to assist with charger alignment\neffectively. Combined, they enable the robot to work continuously in the field\nwithout access to electricity. In addition, a color-based contour algorithm\ncombined with preprocessing techniques is applied for robust navigation relying\non the input from the onboard monocular camera. Incorporating such compact\nrobots into farms could help automate weed control, even during late stages of\ngrowth, and reduce herbicide use by targeting weeds with precision. The robotic\nplatform is field-tested in the flaxseed fields of North Dakota.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.01126,regular,pre_llm,2021,12,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Situation-Aware Environment Perception Using a Multi-Layer Attention Map\n\n  Within the field of automated driving, a clear trend in environment\nperception tends towards more sensors, higher redundancy, and overall increase\nin computational power. This is mainly driven by the paradigm to perceive the\nentire environment as best as possible at all times. However, due to the\nongoing rise in functional complexity, compromises have to be considered to\nensure real-time capabilities of the perception system.\n  In this work, we introduce a concept for situation-aware environment\nperception to control the resource allocation towards processing relevant areas\nwithin the data as well as towards employing only a subset of functional\nmodules for environment perception, if sufficient for the current driving task.\nSpecifically, we propose to evaluate the context of an automated vehicle to\nderive a multi-layer attention map (MLAM) that defines relevant areas. Using\nthis MLAM, the optimum of active functional modules is dynamically configured\nand intra-module processing of only relevant data is enforced.\n  We outline the feasibility of application of our concept using real-world\ndata in a straight-forward implementation for our system at hand. While\nretaining overall functionality, we achieve a reduction of accumulated\nprocessing time of 59%.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.10303,regular,pre_llm,2021,12,"{'ai_likelihood': 8.90758302476671e-06, 'text': ""Towards Computational Awareness in Autonomous Robots: An Empirical Study\n  of Computational Kernels\n\n  The potential impact of autonomous robots on everyday life is evident in\nemerging applications such as precision agriculture, search and rescue, and\ninfrastructure inspection. However, such applications necessitate operation in\nunknown and unstructured environments with a broad and sophisticated set of\nobjectives, all under strict computation and power limitations. We therefore\nargue that the computational kernels enabling robotic autonomy must be\nscheduled and optimized to guarantee timely and correct behavior, while\nallowing for reconfiguration of scheduling parameters at run time. In this\npaper, we consider a necessary first step towards this goal of computational\nawareness in autonomous robots: an empirical study of a base set of\ncomputational kernels from the resource management perspective. Specifically,\nwe conduct a data-driven study of the timing, power, and memory performance of\nkernels for localization and mapping, path planning, task allocation, depth\nestimation, and optical flow, across three embedded computing platforms. We\nprofile and analyze these kernels to provide insight into scheduling and\ndynamic resource management for computation-aware autonomous robots. Notably,\nour results show that there is a correlation of kernel performance with a\nrobot's operational environment, justifying the notion of computation-aware\nrobots and why our work is a crucial step towards this goal.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.04063,regular,pre_llm,2021,12,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Semantic OcTree Mapping and Shannon Mutual Information Computation for\n  Robot Exploration\n\n  Autonomous robot operation in unstructured and unknown environments requires\nefficient techniques for mapping and exploration using streaming range and\nvisual observations. Information-based exploration techniques, such as\nCauchy-Schwarz quadratic mutual information (CSQMI) and fast Shannon mutual\ninformation (FSMI), have successfully achieved active binary occupancy mapping\nwith range measurements. However, as we envision robots performing complex\ntasks specified with semantically meaningful concepts, it is necessary to\ncapture semantics in the measurements, map representation, and exploration\nobjective. This work presents Semantic octree mapping and Shannon Mutual\nInformation (SSMI) computation for robot exploration. We develop a Bayesian\nmulti-class mapping algorithm based on an octree data structure, where each\nvoxel maintains a categorical distribution over semantic classes. We derive a\nclosed-form efficiently-computable lower bound of the Shannon mutual\ninformation between a multi-class octomap and a set of range-category\nmeasurements using semantic run-length encoding of the sensor rays. The bound\nallows rapid evaluation of many potential robot trajectories for autonomous\nexploration and mapping. We compare our method against state-of-the-art\nexploration techniques and apply it in a variety of simulated and real-world\nexperiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.0189,regular,pre_llm,2021,12,"{'ai_likelihood': 9.967221154106988e-06, 'text': 'Fast Direct Stereo Visual SLAM\n\n  We propose a novel approach for fast and accurate stereo visual Simultaneous\nLocalization and Mapping (SLAM) independent of feature detection and matching.\nWe extend monocular Direct Sparse Odometry (DSO) to a stereo system by\noptimizing the scale of the 3D points to minimize photometric error for the\nstereo configuration, which yields a computationally efficient and robust\nmethod compared to conventional stereo matching. We further extend it to a full\nSLAM system with loop closure to reduce accumulated errors. With the assumption\nof forward camera motion, we imitate a LiDAR scan using the 3D points obtained\nfrom the visual odometry and adapt a LiDAR descriptor for place recognition to\nfacilitate more efficient detection of loop closures. Afterward, we estimate\nthe relative pose using direct alignment by minimizing the photometric error\nfor potential loop closures. Optionally, further improvement over direct\nalignment is achieved by using the Iterative Closest Point (ICP) algorithm.\nLastly, we optimize a pose graph to improve SLAM accuracy globally. By avoiding\nfeature detection or matching in our SLAM system, we ensure high computational\nefficiency and robustness. Thorough experimental validations on public datasets\ndemonstrate its effectiveness compared to the state-of-the-art approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.10578,regular,pre_llm,2021,12,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'From Robot Self-Localization to Global-Localization: An RSSI Based\n  Approach\n\n  Localization is a crucial task for autonomous mobile robots in order to\nsuccessfully move to goal locations in their environment. Usually, this is done\nin a robot-centric manner, where the robot maintains a map with its body in the\ncenter. In swarm robotics applications, where a group of robots needs to\ncoordinate in order to achieve their common goals, robot-centric localization\nwill not suffice as each member of the swarm has its own frame of reference.\nOne way to deal with this problem is to create, maintain and share a common map\n(global coordinate system), among the members of the swarm. This paper presents\nan approach to global localization for a group of robots in unknown, GPS and\nlandmark free environments that extends the localization scheme of the LadyBug\nalgorithm. The main idea relies on members of the swarm staying still and\nacting as beacons, emitting electromagnetic signals. These stationary robots\nform a global frame of reference and the rest of the group localize themselves\nin it using the Received Signal Strength Indicator (RSSI). The proposed method\nis evaluated, and the results obtained from the experiments are promising.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.05563,regular,pre_llm,2021,12,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'D*+: A Risk Aware Platform Agnostic Heterogeneous Path Planner\n\n  This article establishes the novel D$^*_+$, a risk-aware and\nplatform-agnostic heterogeneous global path planner for robotic navigation in\ncomplex environments. The proposed planner addresses a fundamental bottleneck\nof occupancy-based path planners related to their dependency on accurate and\ndense maps. More specifically, their performance is highly affected by poorly\nreconstructed or sparse areas (e.g. holes in the walls or ceilings) leading to\nfaulty generated paths outside the physical boundaries of the 3-dimensional\nspace. As it will be presented, D$^*_+$ addresses this challenge with three\nnovel contributions, integrated into one solution, namely: a) the proximity\nrisk, b) the modeling of the unknown space, and c) the map updates. By adding a\nrisk layer to spaces that are closer to the occupied ones, some holes are\nfilled, and thus the problematic short-cutting through them to the final goal\nis prevented. The novel established D$^*_+$ also provides safety marginals to\nthe walls and other obstacles, a property that results in paths that do not cut\nthe corners that could potentially disrupt the platform operation. D$^*_+$ has\nalso the capability to model the unknown space as risk-free areas that should\nkeep the paths inside, e.g in a tunnel environment, and thus heavily reducing\nthe risk of larger shortcuts through openings in the walls. D$^*_+$ is also\nintroducing a dynamic map handling capability that continuously updates with\nthe latest information acquired during the map building process, allowing the\nplanner to use constant map growth and resolve cases of planning over outdated\nsparser map reconstructions...\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.00779,regular,pre_llm,2021,12,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Effects of Interfaces on Human-Robot Trust: Specifying and Visualizing\n  Physical Zones\n\n  In this paper we investigate the influence interfaces and feedback have on\nhuman-robot trust levels when operating in a shared physical space. The task we\nuse is specifying a ""no-go"" region for a robot in an indoor environment. We\nevaluate three styles of interface (physical, AR, and map-based) and four\nfeedback mechanisms (no feedback, robot drives around the space, an AR ""fence"",\nand the region marked on the map). Our evaluation looks at both usability and\ntrust. Specifically, if the participant trusts that the robot ""knows"" where the\nno-go region is and their confidence in the robot\'s ability to avoid that\nregion. We use both self-reported and indirect measures of trust and usability.\nOur key findings are: 1) interfaces and feedback do influence levels of trust;\n2) the participants largely preferred a mixed interface-feedback pair, where\nthe modality for the interface differed from the feedback.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.09487,regular,pre_llm,2021,12,"{'ai_likelihood': 1.3245476616753472e-06, 'text': ""A controller for reaching and unveiling a partially occluded object of\n  interest with an eye-in-hand robot\n\n  In this work, a control scheme for approaching and unveiling a partially\noccluded object of interest is proposed.The control scheme is based only on the\nclassified point cloud obtained by the in-hand camera attached to the robot's\nend effector. It is shown that the proposed controller reaches in the vicinity\nof the object progressively unveiling the neighborhood of each visible point of\nthe object of interest. It can therefore potentially achieve the complete\nunveiling of the object. The proposed control scheme is evaluated through\nsimulations and experiments with a UR5e robot with an in-hand RealSense camera\non a mock-up vine setup for unveiling the stem of a grape.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.06474,regular,pre_llm,2021,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Aerial Chasing of a Dynamic Target in Complex Environments\n\n  Rapidly generating an optimal chasing motion of a drone to follow a dynamic\ntarget among obstacles is challenging due to numerical issues rising from\nmultiple conflicting objectives and non-convex constraints. This study proposes\nto resolve the difficulties with a fast and reliable pipeline that incorporates\n1) a target movement forecaster and 2) a chasing planner. They are based on a\nsample-and-check approach that consists of the generation of high-quality\ncandidate primitives and the feasibility tests with a light computation load.\nWe forecast the movement of the target by selecting an optimal prediction among\na set of candidates built from past observations. Based on the prediction, we\nconstruct a set of prospective chasing trajectories which reduce the high-order\nderivatives, while maintaining the desired relative distance from the predicted\ntarget movement. Then, the candidate trajectories are tested on safety of the\nchaser and visibility toward the target without loose approximation of the\nconstraints. The proposed algorithm is thoroughly evaluated in challenging\nscenarios involving dynamic obstacles. Also, the overall process from the\ntarget recognition to the chasing motion planning is implemented fully onboard\non a drone, demonstrating real-world applicability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2112.00291,regular,pre_llm,2021,12,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Bumblebee: A Path Towards Fully Autonomous Robotic Vine Pruning\n\n  Dormant season grapevine pruning requires skilled seasonal workers during the\nwinter season which are becoming less available. As workers hasten to prune\nmore vines in less time amid to the short-term seasonal hiring culture and low\nwages, vines are often pruned inconsistently leading to imbalanced grapevines.\nIn addition to this, currently existing mechanical methods cannot selectively\nprune grapevines and manual follow-up operations are often required that\nfurther increase production cost. In this paper, we present the design and\nfield evaluation of a rugged, and fully autonomous robot for end-to-end pruning\nof dormant season grapevines. The proposed design incorporates novel camera\nsystems, a kinematically redundant manipulator, a ground robot, and novel\nalgorithms in the perception system. The presented research prototype robot\nsystem was able to spur prune a row of vines from both sides completely in 213\nsec/vine with a total pruning accuracy of 87%. Initial field tests of the\nautonomous system in a commercial vineyard have shown significant variability\nreduction in dormant season pruning when compared to mechanical pre-pruning\ntrials. The design approach, system components, lessons learned, future\nenhancements as well as a brief economic analysis are described in the\nmanuscript.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.02718,regular,pre_llm,2022,1,"{'ai_likelihood': 9.271833631727431e-07, 'text': ""Multi-Vehicle Control in Roundabouts using Decentralized Game-Theoretic\n  Planning\n\n  Safe navigation in dense, urban driving environments remains an open problem\nand an active area of research. Unlike typical predict-then-plan approaches,\ngame-theoretic planning considers how one vehicle's plan will affect the\nactions of another. Recent work has demonstrated significant improvements in\nthe time required to find local Nash equilibria in general-sum games with\nnonlinear objectives and constraints. When applied trivially to driving, these\nworks assume all vehicles in a scene play a game together, which can result in\nintractable computation times for dense traffic. We formulate a decentralized\napproach to game-theoretic planning by assuming that agents only play games\nwithin their observational vicinity, which we believe to be a more reasonable\nassumption for human driving. Games are played in parallel for all strongly\nconnected components of an interaction graph, significantly reducing the number\nof players and constraints in each game, and therefore the time required for\nplanning. We demonstrate that our approach can achieve collision-free,\nefficient driving in urban environments by comparing performance against an\nadaptation of the Intelligent Driver Model and centralized game-theoretic\nplanning when navigating roundabouts in the INTERACTION dataset. Our\nimplementation is available at http://github.com/sisl/DecNashPlanning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08046,regular,pre_llm,2022,1,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'DFBVS: Deep Feature-Based Visual Servo\n\n  Classical Visual Servoing (VS) rely on handcrafted visual features, which\nlimit their generalizability. Recently, a number of approaches, some based on\nDeep Neural Networks, have been proposed to overcome this limitation by\ncomparing directly the entire target and current camera images. However, by\ngetting rid of the visual features altogether, those approaches require the\ntarget and current images to be essentially similar, which precludes the\ngeneralization to unknown, cluttered, scenes. Here we propose to perform VS\nbased on visual features as in classical VS approaches but, contrary to the\nlatter, we leverage recent breakthroughs in Deep Learning to automatically\nextract and match the visual features. By doing so, our approach enjoys the\nadvantages from both worlds: (i) because our approach is based on visual\nfeatures, it is able to steer the robot towards the object of interest even in\npresence of significant distraction in the background; (ii) because the\nfeatures are automatically extracted and matched, our approach can easily and\nautomatically generalize to unseen objects and scenes. In addition, we propose\nto use a render engine to synthesize the target image, which offers a further\nlevel of generalization. We demonstrate these advantages in a robotic grasping\ntask, where the robot is able to steer, with high accuracy, towards the object\nto grasp, based simply on an image of the object rendered from the camera view\ncorresponding to the desired robot grasping pose.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.04762,review,pre_llm,2022,1,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Autonomous soft hand grasping -- Literature review\n\n  Autonomous grasping remains challenging as unlike humans, robots do not\npossess a sophisticated sensing nor delicate interaction capability with the\nreal environment. Among other efforts that tried to close the gap between them,\nanthropomorphic robotic hands is the most prominent direction. However, exactly\nfollowing human hand design might be unnecessary as it will significantly\nincrease the mechanical complexity and hence make it less economically\nfeasible. Recently, soft robotic hands, a new trend has emerged, aiming to make\nthe design adequately complex and affordable while requiring much less effort\nto control. In the first part of this article, we will lay out several\nprominent designs in this direction and their applications in real world\nscenarios. Having a suitable hardware simplified the complexity of software\ndesigning. However, manually controlling the hand for one task requires a\nsignificantly large amount of time and effort and doing it repeatedly is\nunsurprisingly tedious. Therefore, in the second part, we will show some recent\ntechniques for soft hand autonomous control. We start by briefly discussing the\nanalytic methods that mainly exploit the hand dynamic information. Then,\ndata-driven approaches will be our main focus. It is the trending research\ntopic for soft hand grasping in recent years as it has shown a high performance\nwhen dealing with a large number of various objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.1336,regular,pre_llm,2022,1,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Hydra: A Real-time Spatial Perception System for 3D Scene Graph\n  Construction and Optimization\n\n  3D scene graphs have recently emerged as a powerful high-level representation\nof 3D environments. A 3D scene graph describes the environment as a layered\ngraph where nodes represent spatial concepts at multiple levels of abstraction\nand edges represent relations between concepts. While 3D scene graphs can serve\nas an advanced ""mental model"" for robots, how to build such a rich\nrepresentation in real-time is still uncharted territory. This paper describes\na real-time Spatial Perception System, a suite of algorithms to build a 3D\nscene graph from sensor data in real-time. Our first contribution is to develop\nreal-time algorithms to incrementally construct the layers of a scene graph as\nthe robot explores the environment; these algorithms build a local Euclidean\nSigned Distance Function (ESDF) around the current robot location, extract a\ntopological map of places from the ESDF, and then segment the places into rooms\nusing an approach inspired by community-detection techniques. Our second\ncontribution is to investigate loop closure detection and optimization in 3D\nscene graphs. We show that 3D scene graphs allow defining hierarchical\ndescriptors for loop closure detection; our descriptors capture statistics\nacross layers in the scene graph, ranging from low-level visual appearance to\nsummary statistics about objects and places. We then propose the first\nalgorithm to optimize a 3D scene graph in response to loop closures; our\napproach relies on embedded deformation graphs to simultaneously correct all\nlayers of the scene graph. We implement the proposed Spatial Perception System\ninto a architecture named Hydra, that combines fast early and mid-level\nperception processes with slower high-level perception. We evaluate Hydra on\nsimulated and real data and show it is able to reconstruct 3D scene graphs with\nan accuracy comparable with batch offline methods despite running online.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.01537,regular,pre_llm,2022,1,"{'ai_likelihood': 5.496872795952691e-06, 'text': 'Few-shot Domain Adaptation for IMU Denoising\n\n  Different application scenarios will cause IMU to exhibit different error\ncharacteristics which will cause trouble to robot application. However, most\ndata processing methods need to be designed for specific scenario. To solve\nthis problem, we propose a few-shot domain adaptation method. In this work, a\ndomain adaptation framework is considered for denoising the IMU, a\nreconstitution loss is designed to improve domain adaptability. In addition, in\norder to further improve the adaptability in the case of limited data, a\nfew-shot training strategy is adopted. In the experiment, we quantify our\nmethod on two datasets (EuRoC and TUM-VI) and two real robots (car and\nquadruped robot) with three different precision IMUs. According to the\nexperimental results, the adaptability of our framework is verified by t-SNE.\nIn orientation results, our proposed method shows the great denoising\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.05753,regular,pre_llm,2022,1,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Parameter Identification and Motion Control for Articulated Rigid Body\n  Robots Using Differentiable Position-based Dynamics\n\n  Simulation modeling of robots, objects, and environments is the backbone for\nall model-based control and learning. It is leveraged broadly across dynamic\nprogramming and model-predictive control, as well as data generation for\nimitation, transfer, and reinforcement learning. In addition to fidelity, key\nfeatures of models in these control and learning contexts are speed, stability,\nand native differentiability. However, many popular simulation platforms for\nrobotics today lack at least one of the features above. More recently,\nposition-based dynamics (PBD) has become a very popular simulation tool for\nmodeling complex scenes of rigid and non-rigid object interactions, due to its\nspeed and stability, and is starting to gain significant interest in robotics\nfor its potential use in model-based control and learning. Thus, in this paper,\nwe present a mathematical formulation for coupling position-based dynamics\n(PBD) simulation and optimal robot design, model-based motion control and\nsystem identification. Our framework breaks down PBD definitions and\nderivations for various types of joint-based articulated rigid bodies. We\npresent a back-propagation method with automatic differentiation, which can\nintegrate both positional and angular geometric constraints. Our framework can\ncritically provide the native gradient information and perform gradient-based\noptimization tasks. We also propose articulated joint model representations and\nsimulation workflow for our differentiable framework. We demonstrate the\ncapability of the framework in efficient optimal robot design, accurate\ntrajectory torque estimation and supporting spring stiffness estimation, where\nwe achieve minor errors. We also implement impedance control in real robots to\ndemonstrate the potential of our differentiable framework in human-in-the-loop\napplications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.04012,regular,pre_llm,2022,1,"{'ai_likelihood': 8.940696716308594e-07, 'text': ""Decentralized Probabilistic Multi-Robot Collision Avoidance Using\n  Buffered Uncertainty-Aware Voronoi Cells\n\n  In this paper, we present a decentralized and communication-free collision\navoidance approach for multi-robot systems that accounts for both robot\nlocalization and sensing uncertainties. The approach relies on the computation\nof an uncertainty-aware safe region for each robot to navigate among other\nrobots and static obstacles in the environment, under the assumption of\nGaussian-distributed uncertainty. In particular, at each time step, we\nconstruct a chance-constrained buffered uncertainty-aware Voronoi cell (B-UAVC)\nfor each robot given a specified collision probability threshold. Probabilistic\ncollision avoidance is achieved by constraining the motion of each robot to be\nwithin its corresponding B-UAVC, i.e. the collision probability between the\nrobots and obstacles remains below the specified threshold. The proposed\napproach is decentralized, communication-free, scalable with the number of\nrobots and robust to robots' localization and sensing uncertainties. We applied\nthe approach to single-integrator, double-integrator, differential-drive\nrobots, and robots with general nonlinear dynamics. Extensive simulations and\nexperiments with a team of ground vehicles, quadrotors, and heterogeneous robot\nteams are performed to analyze and validate the proposed approach.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.11916,regular,pre_llm,2022,1,"{'ai_likelihood': 4.933940039740669e-06, 'text': 'Constraint-based Formation of Drone Swarms\n\n  Drone swarms are required for the simultaneous delivery of multiple packages.\nWe demonstrate a multi-stop drone swarm-based delivery in a smart city. We\nleverage formation flying to conserve energy and increase the flight range of a\ndrone swarm. An adaptive formation is presented in which a swarm adjusts to\nextrinsic constraints and changes the formation pattern in-flight. We utilize\nthe existing building rooftops in a city and build a line-of-sight skyway\nnetwork to safely operate the swarms. We use a heuristic-based A* algorithm to\nroute a drone swarm in a skyway network.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.04314,regular,pre_llm,2022,1,"{'ai_likelihood': 9.205606248643664e-06, 'text': 'Configuration Space Decomposition for Scalable Proxy Collision Checking\n  in Robot Planning and Control\n\n  Real-time robot motion planning in complex high-dimensional environments\nremains an open problem. Motion planning algorithms, and their underlying\ncollision checkers, are crucial to any robot control stack. Collision checking\ntakes up a large portion of the computational time in robot motion planning.\nExisting collision checkers make trade-offs between speed and accuracy and\nscale poorly to high-dimensional, complex environments. We present a novel\nspace decomposition method using K-Means clustering in the Forward Kinematics\nspace to accelerate proxy collision checking. We train individual configuration\nspace models using Fastron, a kernel perceptron algorithm, on these decomposed\nsubspaces, yielding compact yet highly accurate models that can be queried\nrapidly and scale better to more complex environments. We demonstrate this new\nmethod, called Decomposed Fast Perceptron (D-Fastron), on the 7-DOF Baxter\nrobot producing on average 29x faster collision checks and up to 9.8x faster\nmotion planning compared to state-of-the-art geometric collision checkers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.0029,regular,pre_llm,2022,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Develop of a Pneumatic Force Sensor Prototype\n\n  One of the difficulties of applying a SEA force control is the complexity\nthat exists when implementing its three main components, which must work\ntogether one after the other. To facilitate the implementation of a force\ncontrol by SEA, in this thesis the pneumatic force sensor is developed. A\npneumatic force sensor differs from other force sensors in that it can work as\na force sensor and as an elastic element. These features facilitate the\nimplementation of force control by SEA, by reducing the number of components\nrequired. On the other hand, the pneumatic force sensor has reduced proportions\nto facilitate its installation in manipulator robots and biomechatronic\nprostheses. The first step that was made for the development of the pneumatic\nforce sensor was the construction of the mathematical model of the sensor, to\nlater use the MATLAB / Simulink software to simulate it. With the data obtained\nfrom the simulation of the mathematical model, the CAD model and the sensor\nplanes were developed in SolidWorks software. Subsequently, the prototype of\nthe pneumatic force sensor was built based on the plans made in the SolidWorks\nsoftware. Once the stage of construction of the pneumatic force sensor was\ncompleted, the calibration and classification of the force sensor was carried\nout based on the UNE-EN ISO 376 standard, and the experimental tests were\ncarried out to validate the sensor. Once the classification of the pneumatic\nforce sensor was obtained, the results of the simulation of the mathematical\nmodel were compared with the results of the experimental test. vi In the\ncomparison, it was possible to show a graphic coherence in the results\nobtained, validating the pneumatic force sensor system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.04501,regular,pre_llm,2022,1,"{'ai_likelihood': 8.079740736219619e-06, 'text': 'Automatic Labeling to Generate Training Data for Online LiDAR-based\n  Moving Object Segmentation\n\n  Understanding the scene is key for autonomously navigating vehicles and the\nability to segment the surroundings online into moving and non-moving objects\nis a central ingredient for this task. Often, deep learning-based methods are\nused to perform moving object segmentation (MOS). The performance of these\nnetworks, however, strongly depends on the diversity and amount of labeled\ntraining data, information that may be costly to obtain. In this paper, we\npropose an automatic data labeling pipeline for 3D LiDAR data to save the\nextensive manual labeling effort and to improve the performance of existing\nlearning-based MOS systems by automatically generating labeled training data.\nOur proposed approach achieves this by processing the data offline in batches.\nIt first exploits an occupancy-based dynamic object removal to detect possible\ndynamic objects coarsely. Second, it extracts segments among the proposals and\ntracks them using a Kalman filter. Based on the tracked trajectories, it labels\nthe actually moving objects such as driving cars and pedestrians as moving. In\ncontrast, the non-moving objects, e.g., parked cars, lamps, roads, or\nbuildings, are labeled as static. We show that this approach allows us to label\nLiDAR data highly effectively and compare our results to those of other label\ngeneration methods. We also train a deep neural network with our auto-generated\nlabels and achieve similar performance compared to the one trained with manual\nlabels on the same data, and an even better performance when using additional\ndatasets with labels generated by our approach. Furthermore, we evaluate our\nmethod on multiple datasets using different sensors and our experiments\nindicate that our method can generate labels in diverse environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.09975,regular,pre_llm,2022,1,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Learning Task-Parameterized Skills from Few Demonstrations\n\n  Moving away from repetitive tasks, robots nowadays demand versatile skills\nthat adapt to different situations. Task-parameterized learning improves the\ngeneralization of motion policies by encoding relevant contextual information\nin the task parameters, hence enabling flexible task executions. However,\ntraining such a policy often requires collecting multiple demonstrations in\ndifferent situations. To comprehensively create different situations is\nnon-trivial thus renders the method less applicable to real-world problems.\nTherefore, training with fewer demonstrations/situations is desirable. This\npaper presents a novel concept to augment the original training dataset with\nsynthetic data for policy improvements, thus allows learning task-parameterized\nskills with few demonstrations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.01561,regular,pre_llm,2022,1,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Multi-layer VI-GNSS Global Positioning Framework with Numerical Solution\n  aided MAP Initialization\n\n  Motivated by the goal of achieving long-term drift-free camera pose\nestimation in complex scenarios, we propose a global positioning framework\nfusing visual, inertial and Global Navigation Satellite System (GNSS)\nmeasurements in multiple layers. Different from previous loosely- and tightly-\ncoupled methods, the proposed multi-layer fusion allows us to delicately\ncorrect the drift of visual odometry and keep reliable positioning while GNSS\ndegrades. In particular, local motion estimation is conducted in the\ninner-layer, solving the problem of scale drift and inaccurate bias estimation\nin visual odometry by fusing the velocity of GNSS, pre-integration of Inertial\nMeasurement Unit (IMU) and camera measurement in a tightly-coupled way. The\nglobal localization is achieved in the outer-layer, where the local motion is\nfurther fused with GNSS position and course in a long-term period in a\nloosely-coupled way. Furthermore, a dedicated initialization method is proposed\nto guarantee fast and accurate estimation for all state variables and\nparameters. We give exhaustive tests of the proposed framework on indoor and\noutdoor public datasets. The mean localization error is reduced up to 63%, with\na promotion of 69% in initialization accuracy compared with state-of-the-art\nworks. We have applied the algorithm to Augmented Reality (AR) navigation,\ncrowd sourcing high-precision map update and other large-scale applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.0431,regular,pre_llm,2022,1,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Coverage Path Planning for Robotic Quality Inspection with Control on\n  Measurement Uncertainty\n\n  The optical scanning gauges mounted on the robots are commonly used in\nquality inspection, such as verifying the dimensional specification of sheet\nstructures. Coverage path planning (CPP) significantly influences the accuracy\nand efficiency of robotic quality inspection. Traditional CPP strategies focus\non minimizing the number of viewpoints or traveling distance of robots under\nthe condition of full coverage inspection. The measurement uncertainty when\ncollecting the scanning data is less considered in the free-form surface\ninspection. To address this problem, a novel CPP method with the optimal\nviewpoint sampling strategy is proposed to incorporate the measurement\nuncertainty of key measurement points (MPs) into free-form surface inspection.\nAt first, the feasible ranges of measurement uncertainty are calculated based\non the tolerance specifications of the MPs. The initial feasible viewpoint set\nis generated considering the measurement uncertainty and the visibility of MPs.\nThen, the inspection cost function is built to evaluate the number of selected\nviewpoints and the average measurement uncertainty in the field of views (FOVs)\nof all the selected viewpoints. Afterward, an enhanced rapidly-exploring random\ntree (RRT*) algorithm is proposed for viewpoint sampling using the inspection\ncost function and CPP optimization. Case studies, including simulation tests\nand inspection experiments, have been conducted to evaluate the effectiveness\nof the proposed method. Results show that the scanning precision of key MPs is\nsignificantly improved compared with the benchmark method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.09212,regular,pre_llm,2022,1,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Large-Dimensional Multibody Dynamics Simulation Using Contact\n  Nodalization and Diagonalization\n\n  We propose a novel multibody dynamics simulation framework that can\nefficiently deal with large-dimensionality and complementarity multi-contact\nconditions. Typical contact simulation approaches perform contact impulse-level\nfixed-point iteration (IL-FPI), which has high time-complexity from large-size\nmatrix inversion and multiplication, as well as susceptibility to\nill-conditioned contact situations. To circumvent this, we propose a novel\nframework based on velocity-level fixed-point iteration (VL-FPI), which, by\nutilizing a certain surrogate dynamics and contact nodalization (with virtual\nnodes), can achieve not only inter-contact decoupling but also their inter-axes\ndecoupling (i.e., contact diagonalization). This then enables us to\none-shot/parallel-solve the contact problem during each VL-FPI iteration-loop,\nwhile the surrogate dynamics structure allows us to circumvent large-size/dense\nmatrix inversion/multiplication, thereby, significantly speeding up the\nsimulation time with improved convergence property. We theoretically show that\nthe solution of our framework is consistent with that of the original problem\nand, further, elucidate mathematical conditions for the convergence of our\nproposed solver. Performance and properties of our proposed simulation\nframework are also demonstrated and experimentally-validated for various\nlarge-dimensional/multi-contact scenarios including deformable objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.03147,regular,pre_llm,2022,1,"{'ai_likelihood': 3.096130159166124e-05, 'text': ""The development of a portable elbow exoskeleton with a Twisted Strings\n  Actuator to assist patients with upper limb inhabitation\n\n  Over the years, the number of exoskeleton devices utilized for upper-limb\nrehabilitation has increased dramatically, each with its own set of pros and\ncons. Most exoskeletons are not portable, limiting their utility to daily use\nfor house patients. Additionally, the huge size of some grounded exoskeletons\nconsumes space while maintaining a sophisticated structure and require more\nexpensive materials. In other words, to maintain affordability, the device's\nstructure must be simple. Thus, in this work, a portable elbow exoskeleton is\ndeveloped using SolidWorks to incorporate a Twisted Strings Actuator (TSA) to\naid in upper-limb rehabilitation and to provide an alternative for those with\ncompromised limbs to recuperate. Experiments are conducted to identify the\noptimal value for building a more flexible elbow exoskeleton prototype by\nanalyzing stress, strain conditions, torque, forces, and strings. Preliminary\ncomputational findings reveal that for the proposed intended prototype, a\nstring length of.033 m and a torque value ranging from 1.5 Nm to 3 Nm are\noptimal.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.03163,regular,pre_llm,2022,1,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Continuous-Curvature Target Tree Algorithm for Path Planning in Complex\n  Parking Environments\n\n  Rapidly-exploring random tree (RRT) has been applied for autonomous parking\ndue to quickly solving high-dimensional motion planning and easily reflecting\nconstraints. However, planning time increases by the low probability of\nextending toward narrow parking spots without collisions. To reduce the\nplanning time, the target tree algorithm was proposed, substituting a parking\ngoal in RRT with a set (target tree) of backward parking paths. However, it\nconsists of circular and straight paths, and an autonomous vehicle cannot park\naccurately because of curvature-discontinuity. Moreover, the planning time\nincreases in complex environments; backward paths can be blocked by obstacles.\nTherefore, this paper introduces the continuous-curvature target tree algorithm\nfor complex parking environments. First, a target tree includes clothoid paths\nto address such curvature-discontinuity. Second, to reduce the planning time\nfurther, a cost function is defined to construct a target tree that considers\nobstacles. Integrated with optimal-variant RRT and searching for the shortest\npath among the reached backward paths, the proposed algorithm obtains a\nnear-optimal path as the sampling time increases. Experiment results in real\nenvironments show that the vehicle more accurately parks, and\ncontinuous-curvature paths are obtained more quickly and with higher success\nrates than those acquired with other sampling-based algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.08976,regular,pre_llm,2022,1,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Sub-1.5 Time-Optimal Multi-Robot Path Planning on Grids in Polynomial\n  Time\n\n  Graph-based multi-robot path planning (MRPP) is NP-hard to optimally solve.\nIn this work, we propose the first low polynomial-time algorithm for MRPP\nachieving 1--1.5 asymptotic optimality guarantees on makespan for random\ninstances under very high robot density, with high probability. The dual\nguarantee on computational efficiency and solution optimality suggests our\nproposed general method is promising in significantly scaling up multi-robot\napplications for logistics, e.g., at large robotic warehouses.\n  Specifically, on an $m_1\\times m_2$ gird, $m_1 \\ge m_2$, our RTH (Rubik Table\nwith Highways) algorithm computes solutions for routing up to\n$\\frac{m_1m_2}{3}$ robots with uniformly randomly distributed start and goal\nconfigurations with a makespan of $m_1 + 2m_2 + o(m_1)$, with high probability.\nBecause the minimum makespan for such instances is $m_1 + m_2 - o(m_1)$, also\nwith high probability, RTH guarantees $\\frac{m_1+2m_2}{m_1+m_2}$ optimality as\n$m_1 \\to \\infty$ for random instances with up to $\\frac{1}{3}$ robot density,\nwith high probability. $\\frac{m_1+2m_2}{m_1+m_2} \\in (1, 1.5]$. Alongside this\nkey result, we also establish a series of related results supporting even\nhigher robot densities and environments with regularly distributed obstacles,\nwhich directly map to real-world parcel sorting scenarios. Building on the\nbaseline methods with provable guarantees, we have developed effective,\nprincipled heuristics that further improve the computed optimality of the RTH\nalgorithms. In extensive numerical evaluations, RTH and its variants\ndemonstrate exceptional scalability as compared with methods including ECBS and\nDDM, scaling to over $450 \\times 300$ grids with $45,000$ robots, and\nconsistently achieves makespan around $1.5$ optimal or better, as predicted by\nour theoretical analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.05272,regular,pre_llm,2022,1,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Toward Fully Automated Robotic Platform for Remote Auscultation\n\n  Since most developed countries are facing an increase in the number of\npatients per healthcare worker due to a declining birth rate and an aging\npopulation, relatively simple and safe diagnosis tasks may need to be performed\nusing robotics and automation technologies, without specialists and hospitals.\nThis study presents an automated robotic platform for remote auscultation,\nwhich is a highly cost-effective screening tool for detecting abnormal clinical\nsigns. The developed robotic platform is composed of a 6-degree-of-freedom\ncooperative robotic arm, light detection and ranging (LiDAR) camera, and a\nspring-based mechanism holding an electric stethoscope. The platform enables\nautonomous stethoscope positioning based on external body information acquired\nusing the LiDAR camera-based multi-way registration; the platform also ensures\nsafe and flexible contact, maintaining the contact force within a certain range\nthrough the passive mechanism. Our preliminary results confirm that the robotic\nplatform enables estimation of the landing positions required for cardiac\nexaminations based on the depth and landmark information of the body surface.\nIt also handles the stethoscope while maintaining the contact force without\nrelying on the push-in displacement by the robotic arm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2201.10633,regular,pre_llm,2022,1,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Informative Path Planning to Estimate Quantiles for Environmental\n  Analysis\n\n  Scientists interested in studying natural phenomena often take physical\nspecimens from locations in the environment for later analysis. These analysis\nlocations are typically specified by expert heuristics. Instead, we propose to\nchoose locations for scientific analysis by using a robot to perform an\ninformative path planning survey. The survey results in a list of locations\nthat correspond to the quantile values of the phenomenon of interest. We\ndevelop a robot planner using novel objective functions to improve the\nestimates of the quantile values over time and an approach to find locations\nwhich correspond to the quantile values. We test our approach in four different\nenvironments using previously collected aquatic data and validate it in a field\ntrial. Our proposed approach to estimate quantiles has a 10.2% mean reduction\nin median error when compared to a baseline approach which attempts to maximize\nspatial coverage. Additionally, when localizing these values in the\nenvironment, we see a 15.7% mean reduction in median error when using\ncross-entropy with our loss function compared to a baseline.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.09221,regular,pre_llm,2022,2,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Motion Macro Programming on Assistive Robotic Manipulators: Three Skill\n  Types for Everyday Tasks\n\n  Assistive robotic manipulators are becoming increasingly important for people\nwith disabilities. Teleoperating the manipulator in mundane tasks is part of\ntheir daily lives. Instead of steering the robot through all actions, applying\nself-recorded motion macros could greatly facilitate repetitive tasks. Dynamic\nMovement Primitives (DMP) are a powerful method for skill learning via\nteleoperation. For this use case, however, they need simple heuristics to\nspecify where to start, stop, and parameterize a skill without a background in\ncomputer science and academic sensor setups for autonomous perception. To\nachieve this goal, this paper provides the concept of local, global, and hybrid\nskills that form a modular basis for composing single-handed tasks of daily\nliving. These skills are specified implicitly and can easily be programmed by\nusers themselves, requiring only their basic robotic manipulator. The paper\ncontributes all details for robot-agnostic implementations. Experiments\nvalidate the developed methods for exemplary tasks, such as scratching an itchy\nspot, sorting objects on a desk, and feeding a piggy bank with coins. The paper\nis accompanied by an open-source implementation at\nhttps://github.com/fzi-forschungszentrum-informatik/ArNe\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.11792,regular,pre_llm,2022,2,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Let's Handle It: Generalizable Manipulation of Articulated Objects\n\n  In this project we present a framework for building generalizable\nmanipulation controller policies that map from raw input point clouds and\nsegmentation masks to joint velocities. We took a traditional robotics\napproach, using point cloud processing, end-effector trajectory calculation,\ninverse kinematics, closed-loop position controllers, and behavior trees. We\ndemonstrate our framework on four manipulation skills on common household\nobjects that comprise the SAPIEN ManiSkill Manipulation challenge.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.06723,regular,pre_llm,2022,2,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'An Experimental Study of Wind Resistance and Power Consumption in MAVs\n  with a Low-Speed Multi-Fan Wind System\n\n  This paper discusses a low-cost, open-source and open-hardware design and\nperformance evaluation of a low-speed, multi-fan wind system dedicated to micro\nair vehicle (MAV) testing. In addition, a set of experiments with a flapping\nwing MAV and rotorcraft is presented, demonstrating the capabilities of the\nsystem and the properties of these different types of drones in response to\nvarious types of wind. We performed two sets of experiments where a MAV is\nflying into the wake of the fan system, gathering data about states, battery\nvoltage and current. Firstly, we focus on steady wind conditions with wind\nspeeds ranging from 0.5 m/s to 3.4 m/s. During the second set of experiments,\nwe introduce wind gusts, by periodically modulating the wind speed from 1.3 m/s\nto 3.4 m/s with wind gust oscillations of 0.5 Hz, 0.25 Hz and 0.125 Hz. The\n""Flapper"" flapping wing MAV requires much larger pitch angles to counter wind\nthan the ""CrazyFlie"" quadrotor. This is due to the Flapper\'s larger wing\nsurface. In forward flight, its wings do provide extra lift, considerably\nreducing the power consumption. In contrast, the CrazyFlie\'s power consumption\nstays more constant for different wind speeds. The experiments with the varying\nwind show a quicker gust response by the CrazyFlie compared with the Flapper\ndrone, but both their responses could be further improved. We expect that the\nproposed wind gust system will provide a useful tool to the community to\nachieve such improvements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.00788,regular,pre_llm,2022,2,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Modular Multi-Rotors: From Quadrotors to Fully-Actuated Aerial Vehicles\n\nTraditional aerial vehicles are constrained to perform specific tasks due to their adhoc designs. Based on modularity, we propose a versatile robot, H-ModQuad, that can adapt to different tasks by increasing its load capacity and actuated degrees of freedom. It is composed of cuboid modules propelled by quadrotors with tilted rotors. We present two families of module designs that bring scalable and versatile actuation to the aerial systems. By configuring multiple modules, H-ModQuad can increase its payload capacity and change its actuated degrees of freedom from 4 to 5 and 6. By modeling the actuation capability of H-ModQuad using actuation ellipsoids and wrench polytopes, we find the body frame of a vehicle that maximizes its thrusting efficiency. We also compare the vehicle capabilities against formally defined task requirements. We present the dynamics of H-ModQuad and integrate control strategies despite the vehicle design. The design and model are validated with experiments using actual robots, showing that H-ModQuad vehicles with different configurations provide different actuation properties.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.13401,regular,pre_llm,2022,2,"{'ai_likelihood': 7.0532162984212245e-06, 'text': ""MOCA-S: A Sensitive Mobile Collaborative Robotic Assistant exploiting\n  Low-Cost Capacitive Tactile Cover and Whole-Body Control\n\n  Safety is one of the most fundamental aspects of robotics, especially when it\ncomes to collaborative robots (cobots) that are expected to physically interact\nwith humans. Although a large body of literature has focused on safety-related\naspects for fixed-based cobots, a low effort has been put into developing\ncollaborative mobile manipulators. In response to this need, this work presents\nMOCA-S, i.e., Sensitive Mobile Collaborative Robotic Assistant, that integrates\na low-cost, capacitive tactile cover to measure interaction forces applied to\nthe robot base. The tactile cover comprises a set of 11 capacitive large-area\ntactile sensors distributed as a 1-D tactile array around the base.\nCharacterization of the tactile sensors with different materials is included.\nMoreover, two expanded whole-body controllers that exploit the platform's\ntactile cover and the loco-manipulation features are proposed. These\ncontrollers are tested in two experiments, demonstrating the potential of\nMOCA-S for safe physical Human-Robot Interaction (pHRI). Finally, an experiment\nis carried out in which an undesired collision occurs between MOCA-S and a\nhuman during a loco-manipulation task. The results demonstrate the intrinsic\nsafety of MOCA-S and the proposed controllers, suggesting a new step towards\ncreating safe mobile manipulators.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.10036,regular,pre_llm,2022,2,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Guided Visual Attention Model Based on Interactions Between Top-down and\n  Bottom-up Information for Robot Pose Prediction\n\n  Deep robot vision models are widely used for recognizing objects from camera\nimages, but shows poor performance when detecting objects at untrained\npositions. Although such problem can be alleviated by training with large\ndatasets, the dataset collection cost cannot be ignored. Existing visual\nattention models tackled the problem by employing a data efficient structure\nwhich learns to extract task relevant image areas. However, since the models\ncannot modify attention targets after training, it is difficult to apply to\ndynamically changing tasks. This paper proposed a novel Key-Query-Value\nformulated visual attention model. This model is capable of switching attention\ntargets by externally modifying the Query representations, namely top-down\nattention. The proposed model is experimented on a simulator and a real-world\nenvironment. The model was compared to existing end-to-end robot vision models\nin the simulator experiments, showing higher performance and data efficiency.\nIn the real-world robot experiments, the model showed high precision along with\nits scalability and extendibility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.10002,regular,pre_llm,2022,2,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Vision-based Autonomous Driving for Unstructured Environments Using\n  Imitation Learning\n\n  Unstructured environments are difficult for autonomous driving. This is\nbecause various unknown obstacles are lied in drivable space without lanes, and\nits width and curvature change widely. In such complex environments, searching\nfor a path in real-time is difficult. Also, inaccurate localization data reduce\nthe path tracking accuracy, increasing the risk of collision. Instead of\nsearching and tracking the path, an alternative approach has been proposed that\nreactively avoids obstacles in real-time. Some methods are available for\ntracking global path while avoiding obstacles using the candidate paths and the\nartificial potential field. However, these methods require heuristics to find\nspecific parameters for handling various complex environments. In addition, it\nis difficult to track the global path accurately in practice because of\ninaccurate localization data. If the drivable space is not accurately\nrecognized (i.e., noisy state), the vehicle may not smoothly drive or may\ncollide with obstacles. In this study, a method in which the vehicle drives\ntoward drivable space only using a vision-based occupancy grid map is proposed.\nThe proposed method uses imitation learning, where a deep neural network is\ntrained with expert driving data. The network can learn driving patterns suited\nfor various complex and noisy situations because these situations are contained\nin the training data. Experiments with a vehicle in actual parking lots\ndemonstrated the limitations of general model-based methods and the\neffectiveness of the proposed imitation learning method.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.03582,review,pre_llm,2022,2,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Simulators for Mobile Social Robots:State-of-the-Art and Challenges\n\n  The future robots are expected to work in a shared physical space with humans\n[1], however, the presence of humans leads to a dynamic environment that is\nchallenging for mobile robots to navigate. The path planning algorithms\ndesigned to navigate a collision free path in complex human environments are\noften tested in real environments due to the lack of simulation frameworks.\nThis paper identifies key requirements for an ideal simulator for this task,\nevaluates existing simulation frameworks and most importantly, it identifies\nthe challenges and limitations of the existing simulation techniques. First and\nforemost, we recognize that the simulators needed for the purpose of testing\nmobile robots designed for human environments are unique as they must model\nrealistic pedestrian behavior in addition to the modelling of mobile robots.\nOur study finds that Pedsim_ros [2] and a more recent SocNavBench framework [3]\nare the only two 3D simulation frameworks that meet most of the key\nrequirements defined in our paper. In summary, we identify the need for\ndeveloping more simulators that offer an ability to create realistic 3D\npedestrian rich virtual environments along with the flexibility of designing\ncomplex robots and their sensor models from scratch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.04213,regular,pre_llm,2022,2,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Stein Particle Filter for Nonlinear, Non-Gaussian State Estimation\n\n  Estimation of a dynamical system's latent state subject to sensor noise and\nmodel inaccuracies remains a critical yet difficult problem in robotics. While\nKalman filters provide the optimal solution in the least squared sense for\nlinear and Gaussian noise problems, the general nonlinear and non-Gaussian\nnoise case is significantly more complicated, typically relying on sampling\nstrategies that are limited to low-dimensional state spaces. In this paper we\ndevise a general inference procedure for filtering of nonlinear, non-Gaussian\ndynamical systems that exploits the differentiability of both the update and\nprediction models to scale to higher dimensional spaces. Our method, Stein\nparticle filter, can be seen as a deterministic flow of particles, embedded in\na reproducing kernel Hilbert space, from an initial state to the desirable\nposterior. The particles evolve jointly to conform to a posterior approximation\nwhile interacting with each other through a repulsive force. We evaluate the\nmethod in simulation and in complex localization tasks while comparing it to\nsequential Monte Carlo solutions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.02215,review,pre_llm,2022,2,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'A Survey on Safety-Critical Driving Scenario Generation -- A\n  Methodological Perspective\n\n  Autonomous driving systems have witnessed a significant development during\nthe past years thanks to the advance in machine learning-enabled sensing and\ndecision-making algorithms. One critical challenge for their massive deployment\nin the real world is their safety evaluation. Most existing driving systems are\nstill trained and evaluated on naturalistic scenarios collected from daily life\nor heuristically-generated adversarial ones. However, the large population of\ncars, in general, leads to an extremely low collision rate, indicating that the\nsafety-critical scenarios are rare in the collected real-world data. Thus,\nmethods to artificially generate scenarios become crucial to measure the risk\nand reduce the cost. In this survey, we focus on the algorithms of\nsafety-critical scenario generation in autonomous driving. We first provide a\ncomprehensive taxonomy of existing algorithms by dividing them into three\ncategories: data-driven generation, adversarial generation, and knowledge-based\ngeneration. Then, we discuss useful tools for scenario generation, including\nsimulation platforms and packages. Finally, we extend our discussion to five\nmain challenges of current works -- fidelity, efficiency, diversity,\ntransferability, controllability -- and research opportunities lighted up by\nthese challenges.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.11336,regular,pre_llm,2022,2,"{'ai_likelihood': 5.135933558146159e-05, 'text': ""Trajectory planning in Dynamics Environment : Application for Haptic\n  Perception in Safe HumanRobot Interaction\n\n  In a human-robot interaction system, the most important thing to consider is\nthe safety of the user. This must be guaranteed in order to implement a\nreliable system. The main objective of this paper is to generate a safe motion\nscheme that takes into account the obstacles present in a virtual reality (VR)\nenvironment. The work is developed using the MoveIt software in ROS to control\nan industrial robot UR5. Thanks to this, we will be able to set up the planning\ngroup, which is realized by the UR5 robot with a 6-sided prop and the base of\nthe manipulator, in order to plan feasible trajectories that it will be able to\nexecute in the environment. The latter is based on the interior of a vehicle,\ncontaining a person (which would be the user in this case) for which the\nconfiguration will also be made to be taken into account in the system. To do\nthis, we first investigated the software's capabilities and options for path\nplanning, as well as the different ways to execute the movements. We also\ncompared the different trajectory planning algorithms that the software is\ncapable of using in order to determine which one is best suited for the task.\nFinally, we proposed different mobility schemes to be executed by the robot\ndepending on the situation it is facing. The first one is used when the robot\nhas to plan trajectories in a safe space, where the only obstacle to avoid is\nthe user's workspace. The second one is used when the robot has to interact\nwith the user, where a dummy model represents the user's position as a function\nof time, which is the one to be avoided.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.02179,regular,pre_llm,2022,2,"{'ai_likelihood': 1.1589792039659289e-05, 'text': 'DelTact: A Vision-based Tactile Sensor Using Dense Color Pattern\n\n  Tactile sensing is an essential perception for robots to complete dexterous\ntasks. As a promising tactile sensing technique, vision-based tactile sensors\nhave been developed to improve robot performance in manipulation and grasping.\nHere we propose a new design of a vision-based tactile sensor, DelTact. The\nsensor uses a modular hardware architecture for compactness whilst maintaining\na contact measurement of full resolution (798*586) and large area (675mm2).\nMoreover, it adopts an improved dense random color pattern based on the\nprevious version to achieve high accuracy of contact deformation tracking. In\nparticular, we optimize the color pattern generation process and select the\nappropriate pattern for coordinating with a dense optical flow algorithm under\na real-world experimental sensory setting. The optical flow obtained from the\nraw image is processed to determine shape and force distribution on the contact\nsurface. We also demonstrate the method to extract contact shape and force\ndistribution from the raw images. Experimental results demonstrate that the\nsensor is capable of providing tactile measurements with low error and high\nfrequency (40Hz).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.02927,regular,pre_llm,2022,2,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Reinforcement Learning for Shared Autonomy Drone Landings\n\n  Novice pilots find it difficult to operate and land unmanned aerial vehicles\n(UAVs), due to the complex UAV dynamics, challenges in depth perception, lack\nof expertise with the control interface and additional disturbances from the\nground effect. Therefore we propose a shared autonomy approach to assist pilots\nin safely landing a UAV under conditions where depth perception is difficult\nand safe landing zones are limited. Our approach comprises of two modules: a\nperception module that encodes information onto a compressed latent\nrepresentation using two RGB-D cameras and a policy module that is trained with\nthe reinforcement learning algorithm TD3 to discern the pilot's intent and to\nprovide control inputs that augment the user's input to safely land the UAV.\nThe policy module is trained in simulation using a population of simulated\nusers. Simulated users are sampled from a parametric model with four\nparameters, which model a pilot's tendency to conform to the assistant,\nproficiency, aggressiveness and speed. We conduct a user study (n = 28) where\nhuman participants were tasked with landing a physical UAV on one of several\nplatforms under challenging viewing conditions. The assistant, trained with\nonly simulated user data, improved task success rate from 51.4% to 98.2%\ndespite being unaware of the human participants' goal or the structure of the\nenvironment a priori. With the proposed assistant, regardless of prior piloting\nexperience, participants performed with a proficiency greater than the most\nexperienced unassisted participants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.12729,regular,pre_llm,2022,2,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'The Uncertainty Aware Salted Kalman Filter: State Estimation for Hybrid\n  Systems with Uncertain Guards\n\n  In this paper we present a method for updating robotic state belief through\ncontact with uncertain surfaces and apply this update to a Kalman filter for\nmore accurate state estimation. Examining how guard surface uncertainty affects\nthe time spent in each mode, we derive a guard saltation matrix - which maps\nperturbations prior to hybrid events to perturbations after - accounting for\nadditional variation in the resulting state. Additionally, we propose the use\nof parameterized reset functions - capturing how unknown parameters change how\nstates are mapped from one mode to the next - the Jacobian of which accounts\nfor the additional uncertainty in the resulting state. The accuracy of these\nmappings is shown by simulating sampled distributions through uncertain\ntransition events and comparing the resulting covariances. Finally, we\nintegrate these additional terms into the ""uncertainty aware Salted Kalman\nFilter"", uaSKF, and show a peak reduction in average estimation error by 24-60%\non a variety of test conditions and systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.12796,regular,pre_llm,2022,2,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Hybrid Robotic Grasping with a Soft Multimodal Gripper and a Deep\n  Multistage Learning Scheme\n\n  Grasping has long been considered an important and practical task in robotic\nmanipulation. Yet achieving robust and efficient grasps of diverse objects is\nchallenging, since it involves gripper design, perception, control and\nlearning, etc. Recent learning-based approaches have shown excellent\nperformance in grasping a variety of novel objects. However, these methods\neither are typically limited to one single grasping mode, or else more end\neffectors are needed to grasp various objects. In addition, gripper design and\nlearning methods are commonly developed separately, which may not adequately\nexplore the ability of a multimodal gripper. In this paper, we present a deep\nreinforcement learning (DRL) framework to achieve multistage hybrid robotic\ngrasping with a new soft multimodal gripper. A soft gripper with three grasping\nmodes (i.e., enveloping, sucking, and enveloping_then_sucking) can both deal\nwith objects of different shapes and grasp more than one object simultaneously.\nWe propose a novel hybrid grasping method integrated with the multimodal\ngripper to optimize the number of grasping actions. We evaluate the DRL\nframework under different scenarios (i.e., with different ratios of objects of\ntwo grasp types). The proposed algorithm is shown to reduce the number of\ngrasping actions (i.e., enlarge the grasping efficiency, with maximum values of\n161% in simulations and 154% in real-world experiments) compared to single\ngrasping modes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.09936,regular,pre_llm,2022,2,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Adaptive Safe Merging Control for Heterogeneous Autonomous Vehicles\n  using Parametric Control Barrier Functions\n\n  With the increasing emphasis on the safe autonomy for robots, model-based\nsafe control approaches such as Control Barrier Functions have been extensively\nstudied to ensure guaranteed safety during inter-robot interactions. In this\npaper, we introduce the Parametric Control Barrier Function (Parametric-CBF), a\nnovel variant of the traditional Control Barrier Function to extend its\nexpressivity in describing different safe behaviors among heterogeneous robots.\nInstead of assuming cooperative and homogeneous robots using the same safe\ncontrollers, the ego robot is able to model the neighboring robots' underlying\nsafe controllers through different Parametric-CBFs with observed data. Given\nlearned parametric-CBF and proved forward invariance, it provides greater\nflexibility for the ego robot to better coordinate with other heterogeneous\nrobots with improved efficiency while enjoying formally provable safety\nguarantees. We demonstrate the usage of Parametric-CBF in behavior prediction\nand adaptive safe control in the ramp merging scenario from the applications of\nautonomous driving. Compared to traditional CBF, Parametric-CBF has the\nadvantage of capturing varying drivers' characteristics given richer\ndescription of robot behavior in the context of safe control. Numerical\nsimulations are given to validate the effectiveness of the proposed method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.02207,regular,pre_llm,2022,2,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Active Visuo-Tactile Interactive Robotic Perception for Accurate Object\n  Pose Estimation in Dense Clutter\n\n  This work presents a novel active visuo-tactile based framework for robotic\nsystems to accurately estimate pose of objects in dense cluttered environments.\nThe scene representation is derived using a novel declutter graph (DG) which\ndescribes the relationship among objects in the scene for decluttering by\nleveraging semantic segmentation and grasp affordances networks. The graph\nformulation allows robots to efficiently declutter the workspace by\nautonomously selecting the next best object to remove and the optimal action\n(prehensile or non-prehensile) to perform. Furthermore, we propose a novel\ntranslation-invariant Quaternion filter (TIQF) for active vision and active\ntactile based pose estimation. Both active visual and active tactile points are\nselected by maximizing the expected information gain. We evaluate our proposed\nframework on a system with two robots coordinating on randomized scenes of\ndense cluttered objects and perform ablation studies with static vision and\nactive vision based estimation prior and post decluttering as baselines. Our\nproposed active visuo-tactile interactive perception framework shows upto 36%\nimprovement in pose accuracy compared to the active vision baseline.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.15788,regular,pre_llm,2022,2,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'COMPASS: Contrastive Multimodal Pretraining for Autonomous Systems\n\n  Learning representations that generalize across tasks and domains is\nchallenging yet necessary for autonomous systems. Although task-driven\napproaches are appealing, designing models specific to each application can be\ndifficult in the face of limited data, especially when dealing with highly\nvariable multimodal input spaces arising from different tasks in different\nenvironments.We introduce the first general-purpose pretraining pipeline,\nCOntrastive Multimodal Pretraining for AutonomouS Systems (COMPASS), to\novercome the limitations of task-specific models and existing pretraining\napproaches. COMPASS constructs a multimodal graph by considering the essential\ninformation for autonomous systems and the properties of different modalities.\nThrough this graph, multimodal signals are connected and mapped into two\nfactorized spatio-temporal latent spaces: a ""motion pattern space"" and a\n""current state space."" By learning from multimodal correspondences in each\nlatent space, COMPASS creates state representations that models necessary\ninformation such as temporal dynamics, geometry, and semantics. We pretrain\nCOMPASS on a large-scale multimodal simulation dataset TartanAir\n\\cite{tartanair2020iros} and evaluate it on drone navigation, vehicle racing,\nand visual odometry tasks. The experiments indicate that COMPASS can tackle all\nthree scenarios and can also generalize to unseen environments and real-world\ndata.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.01385,regular,pre_llm,2022,2,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Technical Report: A Hierarchical Deliberative-Reactive System\n  Architecture for Task and Motion Planning in Partially Known Environments\n\n  We describe a task and motion planning architecture for highly dynamic\nsystems that combines a domain-independent sampling-based deliberative planning\nalgorithm with a global reactive planner. We leverage the recent development of\na reactive, vector field planner that provides guarantees of reachability to\nlarge regions of the environment even in the face of unknown or unforeseen\nobstacles. The reachability guarantees can be formalized using contracts that\nallow a deliberative planner to reason purely in terms of those contracts and\nsynthesize a plan by choosing a sequence of reactive behaviors and their target\nconfigurations, without evaluating specific motion plans between targets. This\nreduces both the search depth at which plans will be found, and the number of\nsamples required to ensure a plan exists, while crucially preserving\ncorrectness guarantees. The result is reduced computational cost of\nsynthesizing plans, and increased robustness of generated plans to actuator\nnoise, model misspecification, or unknown obstacles. Simulation studies show\nthat our hierarchical planning and execution architecture can solve complex\nnavigation and rearrangement tasks, even when faced with narrow passageways or\nincomplete world information.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2202.13708,regular,pre_llm,2022,2,"{'ai_likelihood': 1.2020270029703777e-05, 'text': ""Joint Camera Intrinsic and LiDAR-Camera Extrinsic Calibration\n\n  Sensor-based environmental perception is a crucial step for autonomous\ndriving systems, for which an accurate calibration between multiple sensors\nplays a critical role. For the calibration of LiDAR and camera, the existing\nmethod is generally to calibrate the intrinsic of the camera first and then\ncalibrate the extrinsic of the LiDAR and camera. If the camera's intrinsic is\nnot calibrated correctly in the first stage, it isn't easy to calibrate the\nLiDAR-camera extrinsic accurately. Due to the complex internal structure of the\ncamera and the lack of an effective quantitative evaluation method for the\ncamera's intrinsic calibration, in the actual calibration, the accuracy of\nextrinsic parameter calibration is often reduced due to the tiny error of the\ncamera's intrinsic parameters. To this end, we propose a novel target-based\njoint calibration method of the camera intrinsic and LiDAR-camera extrinsic\nparameters. Firstly, we design a novel calibration board pattern, adding four\ncircular holes around the checkerboard for locating the LiDAR pose.\nSubsequently, a cost function defined under the reprojection constraints of the\ncheckerboard and circular holes features is designed to solve the camera's\nintrinsic parameters, distortion factor, and LiDAR-camera extrinsic parameter.\nIn the end, quantitative and qualitative experiments are conducted in actual\nand simulated environments, and the result shows the proposed method can\nachieve accuracy and robustness performance. The open-source code is available\nat https://github.com/OpenCalib/JointCalib.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.04021,regular,pre_llm,2022,3,"{'ai_likelihood': 4.549821217854818e-05, 'text': 'EXOSMOOTH: Test of Innovative EXOskeleton Control for SMOOTH Assistance,\n  With and Without Ankle Actuation\n\n  This work presents a description of the EXOSMOOTH project, oriented to the\nbenchmarking of lower limb exoskeletons performance. In the field of assisted\nwalking by powered lower limb exoskeletons, the EXOSMOOTH project proposes an\nexperiment that targets two scientific questions. The first question is related\nto the effectiveness of a novel control strategy for smooth assistance. Current\nassist strategies are based on controllers that switch the assistance level\nbased on the gait segmentation provided by a finite state machine. The proposed\nstrategy aims at managing phase transitions to provide a smoother assistance to\nthe user, thus increasing the device transparency and comfort for the user. The\nsecond question is the role of the actuation at the ankle joint in assisted\nwalking. Many novel exoskeletons devised for industrial applications do not\nfeature an actuated ankle joint. In the EXOSMOOTH project, the ankle joint\nactuation will be one experimental factor to have a direct assessment of the\nrole of an actuated joint in assisted walking. Preliminary results of 15\nhealthy subjects walking at different speeds while wearing a lower limb\nexoskeleton supported the rationale behind this question: having an actuated\nankle joint could potentially reduce the torques applied by the user by a\nmaximum value of 85 Nm. The two aforementioned questions will be investigated\nin a protocol that includes walking on a treadmill and on flat ground, with or\nwithout slope, and with a load applied on the back. In addition, the\ninteraction forces measured at the exoskeleton harnesses will be used to assess\nthe comfort of the user and the effectiveness of the control strategy to\nimprove transparency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.01403,regular,pre_llm,2022,3,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Modeling of an On-Orbit Maintenance Robotic Arm Test-Bed\n\n  This paper focuses on the development of a ground-based test-bed to analyze\nthe complexities of contact dynamics between multibody systems in space. The\ntest-bed consists of an air-bearing platform equipped with a 7\ndegrees-of-freedom (one degree per revolute joint) robotic arm which acts as\nthe servicing satellite. The dynamics of the manipulator on the platform is\nmodeled as an aid for the analysis and design of stabilizing control algorithms\nsuited for autonomous on-orbit servicing missions.\n  The dynamics are represented analytically using a recursive Newton-Euler\nmultibody method with D-H parameters derived from the physical properties of\nthe arm and platform. In addition, Product of Exponential (PoE) method is also\nemployed to serve as a comparison with the D-H parameters approach. Finally, an\nindependent numerical simulation created with the SimScape modeling environment\nis also presented as a means of verifying the accuracy of the recursive model\nand the PoE approach. The results from both models and SimScape are then\nvalidated through comparison with internal measurement data taken from the\nrobotic arm itself.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.15052,regular,pre_llm,2022,3,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Learning Minimum-Time Flight in Cluttered Environments\n\n  We tackle the problem of minimum-time flight for a quadrotor through a\nsequence of waypoints in the presence of obstacles while exploiting the full\nquadrotor dynamics. Early works relied on simplified dynamics or polynomial\ntrajectory representations that did not exploit the full actuator potential of\nthe quadrotor, and, thus, resulted in suboptimal solutions. Recent works can\nplan minimum-time trajectories; yet, the trajectories are executed with control\nmethods that do not account for obstacles. Thus, a successful execution of such\ntrajectories is prone to errors due to model mismatch and in-flight\ndisturbances. To this end, we leverage deep reinforcement learning and\nclassical topological path planning to train robust neural-network controllers\nfor minimum-time quadrotor flight in cluttered environments. The resulting\nneural network controller demonstrates substantially better performance of up\nto 19\\% over state-of-the-art methods. More importantly, the learned policy\nsolves the planning and control problem simultaneously online to account for\ndisturbances, thus achieving much higher robustness. As such, the presented\nmethod achieves 100% success rate of flying minimum-time policies without\ncollision, while traditional planning and control approaches achieve only 40%.\nThe proposed method is validated in both simulation and the real world, with\nquadrotor speeds of up to 42km/h and accelerations of 3.6g.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.08098,regular,pre_llm,2022,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""RB2: Robotic Manipulation Benchmarking with a Twist\n\n  Benchmarks offer a scientific way to compare algorithms using objective\nperformance metrics. Good benchmarks have two features: (a) they should be\nwidely useful for many research groups; (b) and they should produce\nreproducible findings. In robotic manipulation research, there is a trade-off\nbetween reproducibility and broad accessibility. If the benchmark is kept\nrestrictive (fixed hardware, objects), the numbers are reproducible but the\nsetup becomes less general. On the other hand, a benchmark could be a loose set\nof protocols (e.g. object sets) but the underlying variation in setups make the\nresults non-reproducible. In this paper, we re-imagine benchmarking for robotic\nmanipulation as state-of-the-art algorithmic implementations, alongside the\nusual set of tasks and experimental protocols. The added baseline\nimplementations will provide a way to easily recreate SOTA numbers in a new\nlocal robotic setup, thus providing credible relative rankings between existing\napproaches and new work. However, these local rankings could vary between\ndifferent setups. To resolve this issue, we build a mechanism for pooling\nexperimental data between labs, and thus we establish a single global ranking\nfor existing (and proposed) SOTA algorithms. Our benchmark, called\nRanking-Based Robotics Benchmark (RB2), is evaluated on tasks that are inspired\nfrom clinically validated Southampton Hand Assessment Procedures. Our benchmark\nwas run across two different labs and reveals several surprising findings. For\nexample, extremely simple baselines like open-loop behavior cloning, outperform\nmore complicated models (e.g. closed loop, RNN, Offline-RL, etc.) that are\npreferred by the field. We hope our fellow researchers will use RB2 to improve\ntheir research's quality and rigor.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.03516,regular,pre_llm,2022,3,"{'ai_likelihood': 2.1192762586805556e-06, 'text': 'A Large Force Haptic Interface with Modular Linear Actuators\n\n  This paper presents a haptic interface with modular linear actuators which\ncan address limitations of conventional devices based on rotatory joints. The\nproposed haptic interface is composed of parallel linear actuators that provide\nhigh backdrivability and small inertia. The performance of the haptic interface\nis compared with the conventional mechanisms in terms of force capability,\nreflected inertia, and structural stiffness. High stiffness, large range of\nmotion with high force capability are achieved with the proposed mechanism,\nwhich are in trade-off relationships in traditional haptic interfaces. The\ndevice can apply up to 83 N continuously, which is three times larger than most\nhaptic devices. The theoretical minimum haptic force density and the stiffness\nof the proposed mechanism were 1.3 to 1.9 times and 37 times of conventional\nmechanisms in a similar condition, respectively. The system is also scalable\nbecause its structural stiffness only depends on the timing belt stiffness,\nwhile that of conventional haptic interfaces is inversely proportional to the\ncube of structural lengths. The modular actuator design enables change of\ndegrees freedom (DOFs) for different applications. The proposed haptic\ninterface was tested by the interaction experiment with a virtual environment\nwith rigid walls.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.01061,regular,pre_llm,2022,3,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Real-Time Trajectory Planning for Aerial Perching\n\n  This paper presents a novel trajectory planning method for aerial perching.\nCompared with the existing work, the terminal states and the trajectory\ndurations can be adjusted adaptively, instead of being determined in advance.\nFurthermore, our planner is able to minimize the tangential relative speed on\nthe premise of safety and dynamic feasibility. This feature is especially\nnotable on micro aerial robots with low maneuverability or scenarios where the\nspace is not enough. Moreover, we design a flexible transformation strategy to\neliminate terminal constraints along with reducing optimization variables.\nBesides, we take precise SE(3) motion planning into account to ensure that the\ndrone would not touch the landing platform until the last moment. The proposed\nmethod is validated onboard by a palm-sized micro aerial robot with quite\nlimited thrust and moment (thrust-to-weight ratio 1.7) perching on a mobile\ninclined surface. Sufficient experimental results show that our planner\ngenerates an optimal trajectory within 20ms, and replans with warm start in\n2ms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.09155,regular,pre_llm,2022,3,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'AdaSplats: Adaptive Splatting of Point Clouds for Accurate 3D Modeling\n  and Real-time High-Fidelity LiDAR Simulation\n\n  LiDAR sensors provide rich 3D information about their surrounding{s} and are\nbecoming increasingly important for autonomous vehicles tasks such as\n{localization}, semantic segmentation, object detection, and tracking.\n{Simulation} accelerates the testing, validation, and deployment of autonomous\nvehicles while {also} reducing cost and eliminating the risks of testing in\nreal-world scenarios. We address the problem of high-fidelity LiDAR simulation\nand present a pipeline that leverages real-world point clouds acquired by\nmobile mapping systems. Point-based geometry representations, more specifically\nsplats {(2D oriented disks with normals)}, have proven their ability to\naccurately model the underlying surface in large point clouds{, mainly with\nuniform density}. We introduce an adaptive splat generation method that\naccurately models the underlying 3D geometry {to handle real-world point clouds\nwith variable densities}, especially for thin structures. Moreover, we\nintroduce a {fast} LiDAR {sensor} simulator, {working} in the splatted model,\n{that leverages} the GPU parallel architecture with an acceleration structure\nwhile focusing on efficiently handling large point clouds. We test our LiDAR\nsimulation in real-world conditions, showing qualitative and quantitative\nresults compared to basic splatting and meshing techniques, demonstrating the\ninterest of our modeling technique.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.1618,regular,pre_llm,2022,3,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Millimeter-Wave Sensing for Avoidance of High-Risk Ground Conditions for\n  Mobile Robots\n\n  Mobile robot autonomy has made significant advances in recent years, with\nnavigation algorithms well developed and used commercially in certain\nwell-defined environments, such as warehouses. The common link in usage\nscenarios is that the environments in which the robots are utilized have a high\ndegree of certainty. Operating environments are often designed to be robot\nfriendly, for example augmented reality markers are strategically placed and\nthe ground is typically smooth, level, and clear of debris. For robots to be\nuseful in a wider range of environments, especially environments that are not\nsanitized for their use, robots must be able to handle uncertainty. This\nrequires a robot to incorporate new sensors and sources of information, and to\nbe able to use this information to make decisions regarding navigation and the\noverall mission. When using autonomous mobile robots in unstructured and poorly\ndefined environments, such as a natural disaster site or in a rural\nenvironment, ground condition is of critical importance and is a common cause\nof failure. Examples include loss of traction due to high levels of ground\nwater, hidden cavities, or material boundary failures. To evaluate a\nnon-contact sensing method to mitigate these risks, Frequency Modulated\nContinuous Wave (FMCW) radar is integrated with an Unmanned Ground Vehicle\n(UGV), representing a novel application of FMCW to detect new measurands for\nRobotic Autonomous Systems (RAS) navigation, informing on terrain integrity and\nadding to the state-of-the-art in sensing for optimized autonomous path\nplanning. In this paper, the FMCW is first evaluated in a desktop setting to\ndetermine its performance in anticipated ground conditions. The FMCW is then\nfixed to a UGV and the sensor system is tested and validated in a\nrepresentative environment containing regions with significant levels of ground\nwater saturation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.02176,regular,pre_llm,2022,3,"{'ai_likelihood': 5.4637591044108076e-06, 'text': 'ST-RRT*: Asymptotically-Optimal Bidirectional Motion Planning through\n  Space-Time\n\n  We present a motion planner for planning through space-time with dynamic\nobstacles, velocity constraints, and unknown arrival time. Our algorithm,\nSpace-Time RRT* (ST-RRT*), is a probabilistically complete, bidirectional\nmotion planning algorithm, which is asymptotically optimal with respect to the\nshortest arrival time. We experimentally evaluate ST-RRT* in both abstract (2D\ndisk, 8D disk in cluttered spaces, and on a narrow passage problem), and\nsimulated robotic path planning problems (sequential planning of 8DoF mobile\nrobots, and 7DoF robotic arms). The proposed planner outperforms RRT-Connect\nand RRT* on both initial solution time, and attained final solution cost. The\ncode for ST-RRT* is available in the Open Motion Planning Library (OMPL).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.00356,regular,pre_llm,2022,3,"{'ai_likelihood': 3.245141771104601e-06, 'text': ""Indoor Localization for Quadrotors using Invisible Projected Tags\n\n  Augmented reality (AR) technology has been introduced into the robotics field\nto narrow the visual gap between indoor and outdoor environments. However,\nwithout signals from satellite navigation systems, flight experiments in these\nindoor AR scenarios need other accurate localization approaches. This work\nproposes a real-time centimeter-level indoor localization method based on\npsycho-visually invisible projected tags (IPT), requiring a projector as the\nsender and quadrotors with high-speed cameras as the receiver. The method\nincludes a modulation process for the sender, as well as demodulation and pose\nestimation steps for the receiver, where screen-camera communication technology\nis applied to hide fiducial tags using human vision property. Experiments have\ndemonstrated that IPT can achieve accuracy within ten centimeters and a speed\nof about ten FPS. Compared with other localization methods for AR robotics\nplatforms, IPT is affordable by using only a projector and high-speed cameras\nas hardware consumption and convenient by omitting a coordinate alignment step.\nTo the authors' best knowledge, this is the first time screen-camera\ncommunication is utilized for AR robot localization.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.0729,regular,pre_llm,2022,3,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'GradTac: Spatio-Temporal Gradient Based Tactile Sensing\n\n  Tactile sensing for robotics is achieved through a variety of mechanisms,\nincluding magnetic, optical-tactile, and conductive fluid. Currently, the\nfluid-based sensors have struck the right balance of anthropomorphic sizes and\nshapes and accuracy of tactile response measurement. However, this design is\nplagued by a low Signal to Noise Ratio (SNR) due to the fluid based sensing\nmechanism ""damping"" the measurement values that are hard to model. To this end,\nwe present a spatio-temporal gradient representation on the data obtained from\nfluid-based tactile sensors, which is inspired from neuromorphic principles of\nevent based sensing. We present a novel algorithm (GradTac) that converts\ndiscrete data points from spatial tactile sensors into spatio-temporal surfaces\nand tracks tactile contours across these surfaces. Processing the tactile data\nusing the proposed spatio-temporal domain is robust, makes it less susceptible\nto the inherent noise from the fluid based sensors, and allows accurate\ntracking of regions of touch as compared to using the raw data. We successfully\nevaluate and demonstrate the efficacy of GradTac on many real-world experiments\nperformed using the Shadow Dexterous Hand, equipped with the BioTac SP sensors.\nSpecifically, we use it for tracking tactile input across the sensor\'s surface,\nmeasuring relative forces, detecting linear and rotational slip, and for edge\ntracking. We also release an accompanying task-agnostic dataset for the BioTac\nSP, which we hope will provide a resource to compare and quantify various novel\napproaches, and motivate further research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.09844,regular,pre_llm,2022,3,"{'ai_likelihood': 2.053048875596788e-06, 'text': 'Reinforcement Learning based Voice Interaction to Clear Path for Robots\n  in Elevator Environment\n\n  Efficient use of the space in an elevator is very necessary for a service\nrobot, due to the need for reducing the amount of time caused by waiting for\nthe next elevator. To provide a solution for this, we propose a hybrid approach\nthat combines reinforcement learning (RL) with voice interaction for robot\nnavigation in the scene of entering the elevator. RL provides robots with a\nhigh exploration ability to find a new clear path to enter the elevator\ncompared to traditional navigation methods such as Optimal Reciprocal Collision\nAvoidance (ORCA). The proposed method allows the robot to take an active clear\npath action towards the elevator whilst a crowd of people stands at the\nentrance of the elevator wherein there are still lots of space. This is done by\nembedding a clear path action (voice prompt) into the RL framework, and the\nproposed navigation policy helps the robot to finish tasks efficiently and\nsafely. Our model approach provides a great improvement in the success rate and\nreward of entering the elevator compared to state-of-the-art navigation\npolicies without active clear path operation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.10498,regular,pre_llm,2022,3,"{'ai_likelihood': 3.940529293484158e-06, 'text': 'Generating Task-specific Robotic Grasps\n\n  This paper describes a method for generating robot grasps by jointly\nconsidering stability and other task and object-specific constraints. We\nintroduce a three-level representation that is acquired for each object class\nfrom a small number of exemplars of objects, tasks, and relevant grasps. The\nrepresentation encodes task-specific knowledge for each object class as a\nrelationship between a keypoint skeleton and suitable grasp points that is\npreserved despite intra-class variations in scale and orientation. The learned\nmodels are queried at run time by a simple sampling-based method to guide the\ngeneration of grasps that balance task and stability constraints. We ground and\nevaluate our method in the context of a Franka Emika Panda robot assisting a\nhuman in picking tabletop objects for which the robot does not have prior CAD\nmodels. Experimental results demonstrate that in comparison with a baseline\nmethod that only focuses on stability, our method is able to provide suitable\ngrasps for different tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.09622,regular,pre_llm,2022,3,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Modeling of a Quadruped Robot with Spine Joints and Full-Dynamics\n  Simulation Environment Construction\n\n  This paper presents modeling and simulation of a spined quadruped robot.\nExtended literature survey is employed and spine joints researches of the\nquadruped robots are classified. Most of the researchers execute simplified\nquadruped robot models in their simulations. This survey reveals the need for\nthe full-body spined quadruped simulation environment. First, the kinematics\nand dynamics modeling of the active spined quadruped robot is obtained. Since\nquadruped robots are floating-base robots, all derivations are performed with\nrespect to an inertial frame. The motion equations are acquired by the\nLagrangian approach. The simulation environment is constructed in the\nMATLAB/Simulink platform, considering its rich library, powerful solvers, and\nsuitable and resilient environment in integrating controllers. The computation\nspeed of the simulation environment is increased by using optimized MATLAB\nfunctions. Precise and accurate contact model is utilized in the simulation\nenvironment. We foreseen that the provided full-dynamics simulation environment\nwill be helpful for further spine joint studies on the quadruped robot field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.16932,regular,pre_llm,2022,3,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Probabilistic Map Matching for Robust Inertial Navigation Aiding\n\n  Robust aiding of inertial navigation systems in GNSS-denied environments is\ncritical for the removal of accumulated navigation error caused by the drift\nand bias inherent in inertial sensors. One way to perform such an aiding uses\nmatching of geophysical measurements, such as gravimetry, gravity gradiometry\nor magnetometry, with a known geo-referenced map. Although simple in concept,\nthis map matching procedure is challenging: the measurements themselves are\nnoisy; their associated spatial location is uncertain; and the measurements may\nmatch multiple points within the map (i.e. non-unique solution). In this paper,\nwe propose a probabilistic multiple hypotheses tracker to solve the map\nmatching problem and allow robust inertial navigation aiding. Our approach\naddresses the problem both locally, via probabilistic data association, and\ntemporally by incorporating the underlying platform kinematic constraints into\nthe tracker. The map matching output is then integrated into the navigation\nsystem using an unscented Kalman filter. Additionally, we present a statistical\nmeasure of local map information density -- the map feature variability -- and\nuse it to weight the output covariance of the proposed algorithm. The\neffectiveness and robustness of the proposed algorithm are demonstrated using a\nnavigation scenario involving gravitational map matching.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.12057,review,pre_llm,2022,3,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Cat-inspired Gaits for A Tilt-rotor -- from Symmetrical to Asymmetrical\n\n  Among the tilt-rotors (quadrotors) developed in the last decades, Rylls model\nwith eight inputs (four magnitudes of the thrusts and four tilting angles)\nattracted great attention. Typical feedback linearization maneuvers all the\neight inputs with a united control rule to stabilize this tilt-rotor. Instead\nof assigning the tilting angles by the control rule, the recent research\npredetermined the tilting angles and left the magnitudes of the thrusts the\nonly control signals. These tilting angles are designed to mimic the cat-trot\ngait, avoiding the singular decoupling matrix feedback linearization. To\ncomplete the discussions of the cat-gaits inspired tilt-rotor gaits, this\nresearch addresses the analyses on the rest of the common cat gaits, walk, run,\ntransverse gallop, and rotary gallop. It is found that the singular decoupling\nmatrix exist in walk gait and rotary gallop. Further modifications are\nconducted to these two gaits to accommodate the application of feedback\nlinearization. The modified gaits with different periods are then applied to\nthe tilt-rotor in tracking experiments, in which the references are uniform\nrectilinear motion and uniform circular motion. All the experiments are\nsimulated in Simulink, MATLAB. The result shows that.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.00459,regular,pre_llm,2022,3,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Fast-MbyM: Leveraging Translational Invariance of the Fourier Transform\n  for Efficient and Accurate Radar Odometry\n\n  Masking By Moving (MByM), provides robust and accurate radar odometry\nmeasurements through an exhaustive correlative search across discretised pose\ncandidates. However, this dense search creates a significant computational\nbottleneck which hinders real-time performance when high-end GPUs are not\navailable. Utilising the translational invariance of the Fourier Transform, in\nour approach, f-MByM, we decouple the search for angle and translation. By\nmaintaining end-to-end differentiability a neural network is used to mask scans\nand trained by supervising pose prediction directly. Training faster and with\nless memory, utilising a decoupled search allows f-MByM to achieve significant\nrun-time performance improvements on a CPU (168%) and to run in real-time on\nembedded devices, in stark contrast to MByM. Throughout, our approach remains\naccurate and competitive with the best radar odometry variants available in the\nliterature -- achieving an end-point drift of 2.01% in translation and\n6.3deg/km on the Oxford Radar RobotCar Dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.03919,regular,pre_llm,2022,3,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'A Hierarchical Approach to Active Pose Estimation\n\n  Creating mobile robots which are able to find and manipulate objects in large\nenvironments is an active topic of research. These robots not only need to be\ncapable of searching for specific objects but also to estimate their poses\noften relying on environment observations, which is even more difficult in the\npresence of occlusions. Therefore, to tackle this problem we propose a simple\nhierarchical approach to estimate the pose of a desired object. An Active\nVisual Search module operating with RGB images first obtains a rough estimation\nof the object 2D pose, followed by a more computationally expensive Active Pose\nEstimation module using point cloud data. We empirically show that processing\nimage features to obtain a richer observation speeds up the search and pose\nestimation computations, in comparison to a binary decision that indicates\nwhether the object is or not in the current image.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.07929,regular,pre_llm,2022,3,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Dynamical Modeling and Control of Soft Robots with Non-constant\n  Curvature Deformation\n\n  The Piecewise Constant Curvature (PCC) model is the most widely used soft\nrobotic modeling and control. However, the PCC fails to accurately describe the\ndeformation of the soft robots when executing dynamic tasks or interacting with\nthe environment. This paper presents a simple threedimensional (3D) modeling\nmethod for a multi-segment soft robotic manipulator with non-constant curvature\ndeformation. We devise kinematic and dynamical models for soft manipulators by\nmodeling each segment of the manipulator as two stretchable links connected by\na universal joint. Based on that, we present two controllers for dynamic\ntrajectory tracking in confguration space and pose control in task space,\nrespectively. Model accuracy is demonstrated with simulations and experimental\ndata. The controllers are implemented on a four-segment soft robotic\nmanipulator and validated in dynamic motions and pose control with unknown\nloads. The experimental results show that the dynamic controller enables a\nstable reference trajectory tracking at speeds up to 7m/s.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2203.02882,regular,pre_llm,2022,3,"{'ai_likelihood': 7.450580596923828e-06, 'text': 'RGB-D SLAM in Indoor Planar Environments with Multiple Large Dynamic\n  Objects\n\n  This work presents a novel dense RGB-D SLAM approach for dynamic planar\nenvironments that enables simultaneous multi-object tracking, camera\nlocalisation and background reconstruction. Previous dynamic SLAM methods\neither rely on semantic segmentation to directly detect dynamic objects; or\nassume that dynamic objects occupy a smaller proportion of the camera view than\nthe static background and can, therefore, be removed as outliers. Our approach,\nhowever, enables dense SLAM when the camera view is largely occluded by\nmultiple dynamic objects with the aid of camera motion prior. The dynamic\nplanar objects are separated by their different rigid motions and tracked\nindependently. The remaining dynamic non-planar areas are removed as outliers\nand not mapped into the background. The evaluation demonstrates that our\napproach outperforms the state-of-the-art methods in terms of localisation,\nmapping, dynamic segmentation and object tracking. We also demonstrate its\nrobustness to large drift in the camera motion prior.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.01312,regular,pre_llm,2022,4,"{'ai_likelihood': 1.5795230865478516e-05, 'text': 'Stacked Tensegrity Mechanism for Medical Application\n\n  In this article a multi-segmented planar tensegrity mechanism was presented.\nThis mechanism has a three-segment structure with each segment residing on top\nof another. The size of the segments may decrease proportionally from base to\ntop, resulting in a tapered shape from base to tip like an elephant trunk. The\nsystem was mechanically formulated as having linear springs and cables\nfunctioning as actuators. The singularities, as well as the stability of the\nparallel mechanism, were analyzed by using the principle of minimum energy.\nOptimization was also done to obtain the greatest angular deflection for a\nsegment according to a ratio between the size of the base and the moving\nplatform of the robotic system. The result of this work is a family of\nmechanisms that can generate the same workspace for different stability\nproperties.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.07268,regular,pre_llm,2022,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Visual Pressure Estimation and Control for Soft Robotic Grippers\n\n  Soft robotic grippers facilitate contact-rich manipulation, including robust\ngrasping of varied objects. Yet the beneficial compliance of a soft gripper\nalso results in significant deformation that can make precision manipulation\nchallenging. We present visual pressure estimation & control (VPEC), a method\nthat infers pressure applied by a soft gripper using an RGB image from an\nexternal camera. We provide results for visual pressure inference when a\npneumatic gripper and a tendon-actuated gripper make contact with a flat\nsurface. We also show that VPEC enables precision manipulation via closed-loop\ncontrol of inferred pressure images. In our evaluation, a mobile manipulator\n(Stretch RE1 from Hello Robot) uses visual servoing to make contact at a\ndesired pressure; follow a spatial pressure trajectory; and grasp small\nlow-profile objects, including a microSD card, a penny, and a pill. Overall,\nour results show that visual estimates of applied pressure can enable a soft\ngripper to perform precision manipulation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.07015,regular,pre_llm,2022,4,"{'ai_likelihood': 6.291601392957899e-06, 'text': ""NASA/GSFC's Flight Software Core Flight System Implementation For A\n  Lunar Surface Imaging Mission\n\n  The interest in returning to the Moon for research and exploration has\nincreased as new tipping point technologies are providing the possibility to do\nso. One of these initiatives is the Artemis program by NASA, which plans to\nreturn humans by 2024 to the lunar surface and study water deposits on the\nsurface. This program will also serve as a practice run to plan the logistics\nof sending humans to explore Mars. To return humans safely to the Moon,\nmultiple technological advances and diverse knowledge about the nature of the\nlunar surface are needed. This paper will discuss the design and implementation\nof the flight software of EagleCam, a CubeSat camera system based on the free\nopen-source core Flight System (cFS) architecture developed by NASA's Goddard\nSpace Flight Center. EagleCam is a payload transported to the Moon by the\nCommercial Lunar Payload Services Nova-C lander developed by Intuitive\nMachines. The camera system will capture the first third-person view of a\nspacecraft performing a Moon landing and collect other scientific data such as\nplume interaction with the surface. The complete system is composed of the\nCubeSat and the deployer that will eject it. This will be the first time WiFi\nprotocol is used on the Moon to establish a local communication network.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.11923,regular,pre_llm,2022,4,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Sparse-Dense Motion Modelling and Tracking for Manipulation without\n  Prior Object Models\n\n  This work presents an approach for modelling and tracking previously unseen\nobjects for robotic grasping tasks. Using the motion of objects in a scene, our\napproach segments rigid entities from the scene and continuously tracks them to\ncreate a dense and sparse model of the object and the environment. While the\ndense tracking enables interaction with these models, the sparse tracking makes\nthis robust against fast movements and allows to redetect already modelled\nobjects.\n  The evaluation on a dual-arm grasping task demonstrates that our approach 1)\nenables a robot to detect new objects online without a prior model and to grasp\nthese objects using only a simple parameterisable geometric representation, and\n2) is much more robust compared to the state of the art methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.10826,regular,pre_llm,2022,4,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'A Fully-autonomous Framework of Unmanned Surface Vehicles in Maritime\n  Environments using Gaussian Process Motion Planning\n\n  Unmanned surface vehicles (USVs) are of increasing importance to a growing\nnumber of sectors in the maritime industry, including offshore exploration,\nmarine transportation and defence operations. A major factor in the growth in\nuse and deployment of USVs is the increased operational flexibility that is\noffered through use of autonomous navigation systems that generate optimised\ntrajectories. Unlike path planning in terrestrial environments, planning in the\nmaritime environment is more demanding as there is need to assure mitigating\naction is taken against the significant, random and often unpredictable\nenvironmental influences from winds and ocean currents. With the focus of these\nnecessary requirements as the main basis of motivation, this paper proposes a\nnovel motion planner, denoted as GPMP2*, extending the application scope of the\nfundamental GP-based motion planner, GPMP2, into complex maritime environments.\nAn interpolation strategy based on Monte-Carlo stochasticity has been\ninnovatively added to GPMP2* to produce a new algorithm named GPMP2* with\nMonte-Carlo stochasticity (MC-GPMP2*), which can increase the diversity of the\npaths generated. In parallel with algorithm design, a ROS based\nfully-autonomous framework for an advanced unmanned surface vehicle, the WAM-V\n20 USV, has been proposed. The practicability of the proposed motion planner as\nwell as the fully-autonomous framework have been functionally validated in a\nsimulated inspection missions for an offshore wind farm in ROS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.12124,regular,pre_llm,2022,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Memory based neural networks for end-to-end autonomous driving\n\n  Recent works in end-to-end control for autonomous driving have investigated\nthe use of vision-based exteroceptive perception. Inspired by such results, we\npropose a new end-to-end memory-based neural architecture for robot steering\nand throttle control. We describe and compare this architecture with previous\napproaches using fundamental error metrics (MAE, MSE) and several external\nmetrics based on their performance on simulated test circuits. The presented\nwork demonstrates the advantages of using internal memory for better\ngeneralization capabilities of the model and allowing it to drive in a broader\namount of circuits/situations. We analyze the algorithm in a wide range of\nenvironments and conclude that the proposed pipeline is robust to varying\ncamera configurations. All the present work, including datasets, network models\narchitectures, weights, simulator, and comparison software, is open source and\neasy to replicate and extend.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.06237,regular,pre_llm,2022,4,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Ada-Detector: Adaptive Frontier Detector for Rapid Exploration\n\n  In this paper, we propose an efficient frontier detector method based on\nadaptive Rapidly-exploring Random Tree (RRT) for autonomous robot exploration.\nRobots can achieve real-time incremental frontier detection when they are\nexploring unknown environments. First, our detector adaptively adjusts the\nsampling space of RRT by sensing the surrounding environment structure. The\nadaptive sampling space can greatly improve the successful sampling rate of RRT\n(the ratio of the number of samples successfully added to the RRT tree to the\nnumber of sampling attempts) according to the environment structure and control\nthe expansion bias of the RRT. Second, by generating non-uniform distributed\nsamples, our method also solves the over-sampling problem of RRT in the sliding\nwindows, where uniform random sampling causes over-sampling in the overlap area\nbetween two adjacent sliding windows. In this way, our detector is more\ninclined to sample in the latest explored area, which improves the efficiency\nof frontier detection and achieves incremental detection. We validated our\nmethod in three simulated benchmark scenarios. The experimental comparison\nshows that we reduce the frontier detection runtime by about 40% compared with\nthe SOTA method, DSV Planner.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.04932,regular,pre_llm,2022,4,"{'ai_likelihood': 8.775128258599176e-06, 'text': 'Optimized SC-F-LOAM: Optimized Fast LiDAR Odometry and Mapping Using\n  Scan Context\n\n  LiDAR odometry can achieve accurate vehicle pose estimation for short driving\nrange or in small-scale environments, but for long driving range or in\nlarge-scale environments, the accuracy deteriorates as a result of cumulative\nestimation errors. This drawback necessitates the inclusion of loop closure\ndetection in a SLAM framework to suppress the adverse effects of cumulative\nerrors. To improve the accuracy of pose estimation, we propose a new\nLiDAR-based SLAM method which uses F-LOAM as LiDAR odometry, Scan Context for\nloop closure detection, and GTSAM for global optimization. In our approach, an\nadaptive distance threshold (instead of a fixed threshold) is employed for loop\nclosure detection, which achieves more accurate loop closure detection results.\nBesides, a feature-based matching method is used in our approach to compute\nvehicle pose transformations between loop closure point cloud pairs, instead of\nusing the raw point cloud obtained by the LiDAR sensor, which significantly\nreduces the computation time. The KITTI dataset is used for verifications of\nour method, and the experimental results demonstrate that the proposed method\noutperforms typical LiDAR odometry/SLAM methods in the literature. Our code is\nmade publicly available for the benefit of the community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.00184,regular,pre_llm,2022,4,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Nondeterminism subject to output commitment in combinatorial filters\n\n  We study a class of filters -- discrete finite-state transition systems\nemployed as incremental stream transducers -- that have application to\nrobotics: e.g., to model combinatorial estimators and also as concise encodings\nof feedback plans/policies. The present paper examines their minimization\nproblem under some new assumptions. Compared to strictly deterministic filters,\nallowing nondeterminism supplies opportunities for compression via re-use of\nstates. But this paper suggests that the classic automata-theoretic concept of\nnondeterminism, though it affords said opportunities for reduction in state\ncomplexity, is problematic in many robotics settings. Instead, we argue for a\nnew constrained type of nondeterminism that preserves input-output behavior for\ncircumstances when, as for robots, causation forbids 'rewinding' of the world.\nWe identify problem instances where compression under this constrained form of\nnondeterminism results in improvements over all deterministic filters. In this\nnew setting, we examine computational complexity questions for the problem of\nreducing the state complexity of some given input filter. A hardness result for\ngeneral deterministic input filters is presented, as well as for checking\nspecific, narrower requirements, and some special cases. These results show\nthat this class of nondeterminism gives problems of the same complexity class\nas classical nondeterminism, and the narrower questions help give a more\nnuanced understanding of the source of this complexity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.09753,regular,pre_llm,2022,4,"{'ai_likelihood': 1.4073318905300564e-05, 'text': 'Path Planning Algorithms for Robotic Aquaculture Monitoring\n\n  Aerial drones have great potential to monitor large areas quickly and\nefficiently. Aquaculture is an industry that requires continuous water quality\ndata to successfully grow and harvest fish. The Hybrid Aerial Underwater\nRobotic System (HAUCS) is designed to collect water quality data of aquaculture\nponds to reduce labor costs for farmers. The routing of drones to cover each\nfish pond on an aquaculture farm can be reduced to the Vehicle Routing Problem.\nA dataset is created to simulate the distribution of ponds on a farm and is\nused to assess the HAUCS Path Planning Algorithm (HPP). Its performance is\ncompared with the Google Linear Optimization Package (GLOP) and a Graph\nAttention Model (AM) for routing problems. GLOP is the most efficient solver\nfor 50 to 200 ponds at the expense of long run times, while HPP outperforms the\nother methods in solution quality and run time for instances larger than 200\nponds.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.02693,regular,pre_llm,2022,4,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Exploration with Global Consistency Using Real-Time Re-integration and\n  Active Loop Closure\n\n  Despite recent progress of robotic exploration, most methods assume that\ndrift-free localization is available, which is problematic in reality and\ncauses severe distortion of the reconstructed map. In this work, we present a\nsystematic exploration mapping and planning framework that deals with drifted\nlocalization, allowing efficient and globally consistent reconstruction. A\nreal-time re-integration-based mapping approach along with a frame pruning\nmechanism is proposed, which rectifies map distortion effectively when drifted\nlocalization is corrected upon detecting loop-closure. Besides, an exploration\nplanning method considering historical viewpoints is presented to enable active\nloop closing, which promotes a higher opportunity to correct localization\nerrors and further improves the mapping quality. We evaluate both the mapping\nand planning methods as well as the entire system comprehensively in simulation\nand real-world experiments, showing their effectiveness in practice. The\nimplementation of the proposed method will be made open-source for the benefit\nof the robotics community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.12906,regular,pre_llm,2022,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Towards A COLREGs Compliant Autonomous Surface Vessel in a Constrained\n  Channel\n\n  In this paper, we look at the role of autonomous navigation in the maritime\ndomain. Specifically, we examine how an Autonomous Surface Vessel(ASV) can\nachieve obstacle avoidance based on the Convention on the International\nRegulations for Preventing Collisions at Sea (1972), or COLREGs, in real-world\nenvironments. Our ASV is equipped with a broadband marine radar, an Inertial\nNavigation System (INS), and uses official Electronic Navigational Charts\n(ENC). These sensors are used to provide situational awareness and, in series\nof well-defined steps, we can exclude land objects from the radar data, extract\ntracks associated with moving vessels within range of the radar, and then use a\nKalman Filter to track and predict the motion of other moving vessels in the\nvicinity. A Constant Velocity model for the Kalman Filter allows us to solve\nthe data association to build a consistent model between successive radar\nscans. We account for multiple COLREGs situations based on the predicted\nrelative motion. Finally, an efficient path planning algorithm is presented to\nfind a path and publish waypoints to perform real-time COLREGs compliant\nautonomous navigation within highly constrained environments. We demonstrate\nthe results of our framework with operational results collected over the course\nof a 3.4 nautical mile mission on the Charles River in Boston in which the ASV\nencountered and successfully navigated multiple scenarios and encounters with\nother moving vessels at close quarters.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.02371,regular,pre_llm,2022,4,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Optical Proximity Sensing for Pose Estimation During In-Hand\n  Manipulation\n\n  During in-hand manipulation, robots must be able to continuously estimate the\npose of the object in order to generate appropriate control actions. The\nperformance of algorithms for pose estimation hinges on the robot's sensors\nbeing able to detect discriminative geometric object features, but previous\nsensing modalities are unable to make such measurements robustly. The robot's\nfingers can occlude the view of environment- or robot-mounted image sensors,\nand tactile sensors can only measure at the local areas of contact. Motivated\nby fingertip-embedded proximity sensors' robustness to occlusion and ability to\nmeasure beyond the local areas of contact, we present the first evaluation of\nproximity sensor based pose estimation for in-hand manipulation. We develop a\nnovel two-fingered hand with fingertip-embedded optical time-of-flight\nproximity sensors as a testbed for pose estimation during planar in-hand\nmanipulation. Here, the in-hand manipulation task consists of the robot moving\na cylindrical object from one end of its workspace to the other. We\ndemonstrate, with statistical significance, that proximity-sensor based pose\nestimation via particle filtering during in-hand manipulation: a) exhibits 50%\nlower average pose error than a tactile-sensor based baseline; b) empowers a\nmodel predictive controller to achieve 30% lower final positioning error\ncompared to when using tactile-sensor based pose estimates.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.01861,regular,pre_llm,2022,4,"{'ai_likelihood': 2.914004855685764e-06, 'text': 'Four-dimensional Gait Surfaces for A Tilt-rotor -- Two Color Map Theorem\n\n  This article presents the four-dimensional surfaces which instruct the gait\nplan for a tilt-rotor. The previous gaits analyzed in the tilt-rotor research\nare inspired by animals; no theoretical base backs the robustness of these\ngaits. This research deduces the gaits by diminishing the effect of the\nattitude of the tilt-rotor for the first time. Four-dimensional gait surfaces\nare subsequently found, on which the gaits are expected to be robust to the\nattitude. These surfaces provide the region where the gait is suggested to be\nplanned. However, a discontinuous region may hinder the gait plan process while\nutilizing the proposal gait surfaces. A Two Color Map Theorem is then\nestablished to guarantee the continuity of each gait designed. The robustness\nof the typical gaits obeying the Two Color Map Theorem and on the gait surface\nis demonstrated by comparing the singular curve in attitude with the gaits not\non the gait surface. The result shows that the acceptable attitudes enlarge for\nthe gaits on the gait surface.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.07939,regular,pre_llm,2022,4,"{'ai_likelihood': 6.192260318332249e-06, 'text': 'Long-Horizon Motion Planning via Sampling and Segmented Trajectory\n  Optimization\n\n  This paper presents a hybrid robot motion planner that generates long-horizon\nmotion plans for robot navigation in environments with obstacles. We propose a\nhybrid planner, RRT* with segmented trajectory optimization (RRT*-sOpt), which\ncombines the merits of sampling-based planning, optimization-based planning,\nand trajectory splitting to quickly plan for a collision-free and\ndynamically-feasible motion plan. When generating a plan, the RRT* layer\nquickly samples a semi-optimal path and sets it as an initial reference path.\nThen, the sOpt layer splits the reference path and performs optimization on\neach segment. It then splits the new trajectory again and repeats the process\nuntil the whole trajectory converges. We also propose to reduce the number of\nsegments before convergence with the aim of further reducing computation time.\nSimulation results show that RRT*-sOpt benefits from the hybrid structure with\ntrajectory splitting and performs robustly in various robot platforms and\nscenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.12357,regular,pre_llm,2022,4,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Direct 3D Printing of Soft Fluidic Actuators with Graded Porosity\n\n  New additive manufacturing methods are needed to realize more complex soft\nrobots. One example is soft fluidic robotics, which exploits fluidic power and\nstiffness gradients. Porous structures are an interesting type for this\napproach, as they are flexible and allow for fluid transport. Within this work,\nthe Infill-Foam (InFoam) is proposed to print structures with graded porosity\nby liquid rope coiling (LRC). By exploiting LRC, the InFoam method could\nexploit the repeatable coiling patterns to print structures. To this end, only\nthe characterization of the relation between nozzle height and coil radius and\nthe extruded length were necessary (at a fixed temperature). Then by adjusting\nthe nozzle height and/or extrusion speed the porosity of the printed structure\ncould be set. The InFoam method was demonstrated by printing porous structures\nusing styrene-ethylene-butylene-styrene (SEBS) with porosities ranging from\n46\\% to 89\\%. In compression tests, the cubes showed large changes in modulus\n(more than 200 times), density (-89\\% compared to bulk), and energy\ndissipation. The InFoam method combined coiling and normal plotting to realize\na large range of porosity gradients. This grading was exploited to realize\nrectangular structures with varying deformation patterns, which included\ntwisting, contraction, and bending. Furthermore, the InFoam method was shown to\nbe capable of programming the behavior of bending actuators by varying the\nporosity. Both the output force and stroke showed correlations similar to those\nof the cubes. Thus, the InFoam method can fabricate and program the mechanical\nbehavior of a soft fluidic (porous) actuator by grading porosity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.06218,regular,pre_llm,2022,4,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'A Novel Quadratic Interpolated Beetle Antennae Search for Manipulator\n  Calibration\n\n  Over the past decades, industrial manipulators play a vital role in in\nvarious fields, like aircraft manufacturing and automobile manufacturing.\nHowever, an industrial manipulator without calibration suffers from its low\nabsolute positioning accuracy, which extensively restricts its application in\nhigh-precision intelligent manufacture. Recent manipulator calibration methods\nare developed to address this issue, while they frequently encounter long-tail\nconvergence and low calibration accuracy. To address this thorny issue, this\nwork proposes a novel manipulator calibration method incorporating an extended\nKalman filter with a Quadratic Interpolated Beetle Antennae Search algorithm.\nThis paper has three-fold ideas: a) proposing a new Quadratic Interpolated\nBeetle Antennae Search algorithm to deal with the issue of local optimum and\nlow convergence rate in a Beetle Antennae Search algorithm; b) adopting an\nextended Kalman filter algorithm to suppress non-Gaussian noises and c)\ndeveloping a new manipulator calibration method incorporating an extended\nKalman filter with a Quadratic Interpolated Beetle Antennae Search algorithm to\ncalibrating a manipulator. Extensively experimental results on an ABB IRB120\nindustrial manipulator demonstrate that the proposed method achieves much\nhigher calibration accuracy than several state-of-the-art calibration methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.09617,regular,pre_llm,2022,4,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'CALI: Coarse-to-Fine ALIgnments Based Unsupervised Domain Adaptation of\n  Traversability Prediction for Deployable Autonomous Navigation\n\n  Traversability prediction is a fundamental perception capability for\nautonomous navigation. The diversity of data in different domains imposes\nsignificant gaps to the prediction performance of the perception model. In this\nwork, we make efforts to reduce the gaps by proposing a novel coarse-to-fine\nunsupervised domain adaptation (UDA) model - CALI. Our aim is to transfer the\nperception model with high data efficiency, eliminate the prohibitively\nexpensive data labeling, and improve the generalization capability during the\nadaptation from easy-to-obtain source domains to various challenging target\ndomains. We prove that a combination of a coarse alignment and a fine alignment\ncan be beneficial to each other and further design a first-coarse-then-fine\nalignment process. This proposed work bridges theoretical analyses and\nalgorithm designs, leading to an efficient UDA model with easy and stable\ntraining. We show the advantages of our proposed model over multiple baselines\nin several challenging domain adaptation setups. To further validate the\neffectiveness of our model, we then combine our perception model with a visual\nplanner to build a navigation system and show the high reliability of our model\nin complex natural environments where no labeled data is available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.03028,regular,pre_llm,2022,4,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Software Testing, AI and Robotics (STAIR) Learning Lab\n\n  In this paper we presented the Software Testing, AI and Robotics (STAIR)\nLearning Lab. STAIR is an initiative started at the University of Innsbruck to\nbring robotics, Artificial Intelligence (AI) and software testing into schools.\nIn the lab physical and virtual learning units are developed in parallel and in\nsync with each other. Its core learning approach is based the develop of both a\nphysical and simulated robotics environment. In both environments AI scenarios\n(like traffic sign recognition) are deployed and tested. We present and focus\non our newly designed MiniBot that are both built on hardware which was\ndesigned for educational and research purposes as well as the simulation\nenvironment. Additionally, we describe first learning design concepts and a\nshowcase scenario (i.e., AI-based traffic sign recognition) with different\nexercises which can easily be extended.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2204.12173,regular,pre_llm,2022,4,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Map-based Visual-Inertial Localization: Consistency and Complexity\n\n  Drift-free localization is essential for autonomous vehicles. In this paper,\nwe address the problem by proposing a filter-based framework, which integrates\nthe visual-inertial odometry and the measurements of the features in the\npre-built map. In this framework, the transformation between the odometry frame\nand the map frame is augmented into the state and estimated on the fly.\nBesides, we maintain only the keyframe poses in the map and employ Schmidt\nextended Kalman filter to update the state partially, so that the uncertainty\nof the map information can be consistently considered with low computational\ncost. Moreover, we theoretically demonstrate that the ever-changing\nlinearization points of the estimated state can introduce spurious information\nto the augmented system and make the original four-dimensional unobservable\nsubspace vanish, leading to inconsistent estimation in practice. To relieve\nthis problem, we employ first-estimate Jacobian (FEJ) to maintain the correct\nobservability properties of the augmented system. Furthermore, we introduce an\nobservability-constrained updating method to compensate for the significant\naccumulated error after the long-term absence (can be 3 minutes and 1 km) of\nmap-based measurements. Through simulations, the consistent estimation of our\nproposed algorithm is validated. Through real-world experiments, we demonstrate\nthat our proposed algorithm runs successfully on four kinds of datasets with\nthe lower computational cost (20% time-saving) and the better estimation\naccuracy (45% trajectory error reduction) compared with the baseline algorithm\nVINS-Fusion, whereas VINS-Fusion fails to give bounded localization performance\non three of four datasets because of its inconsistent estimation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.04847,regular,pre_llm,2022,5,"{'ai_likelihood': 0.000625716315375434, 'text': 'Multi-Tree Guided Efficient Robot Motion Planning\n\n  Motion Planning is necessary for robots to complete different tasks.\nRapidly-exploring Random Tree (RRT) and its variants have been widely used in\nrobot motion planning due to their fast search in state space. However, they\nperform not well in many complex environments since the motion planning needs\nto simultaneously consider the geometry constraints and differential\nconstraints. In this article, we propose a novel robot motion planning\nalgorithm that utilizes multi-tree to guide the exploration and exploitation.\nThe proposed algorithm maintains more than two trees to search the state space\nat first. Each tree will explore the local environment. The tree starts from\nthe root will gradually collect information from other trees and grow towards\nthe goal state. This simultaneous exploration and exploitation method can\nquickly find a feasible trajectory. We compare the proposed algorithm with\nother popular motion planning algorithms. The experiment results demonstrate\nthat our algorithm achieves the best performance on different evaluation\nmetrics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.15112,regular,pre_llm,2022,5,"{'ai_likelihood': 0.00010934140947129992, 'text': 'Robotic grasp detection based on Transformer\n\n  Grasp detection in a cluttered environment is still a great challenge for\nrobots. Currently, the Transformer mechanism has been successfully applied to\nvisual tasks, and its excellent ability of global context information\nextraction provides a feasible way to improve the performance of robotic grasp\ndetection in cluttered scenes. However, the insufficient inductive bias ability\nof the original Transformer model requires large-scale datasets training, which\nis difficult to obtain for grasp detection. In this paper, we propose a grasp\ndetection model based on encoder-decoder structure. The encoder uses a\nTransformer network to extract global context information. The decoder uses a\nfully convolutional neural network to improve the inductive bias capability of\nthe model and combine features extracted by the encoder to predict the final\ngrasp configuration. Experiments on the VMRD dataset demonstrate that our model\nperforms much better in overlapping object scenes. Meanwhile, on the Cornell\nGrasp dataset, our approach achieves an accuracy of 98.1%, which is comparable\nwith state-of-the-art algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.06747,regular,pre_llm,2022,5,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Augmented Reality Appendages for Robots: Design Considerations and\n  Recommendations for Maximizing Social and Functional Perception\n\n  In order to address the limitations of gestural capabilities in physical\nrobots, researchers in Virtual, Augmented, Mixed Reality Human-Robot\nInteraction (VAM-HRI) have been using augmented-reality visualizations that\nincrease robot expressivity and improve user perception (e.g., social\npresence). While a multitude of virtual robot deictic gestures (e.g., pointing\nto an object) have been implemented to improve interactions within VAM-HRI,\nsuch systems are often reported to have tradeoffs between functional and social\nuser perceptions of robots, creating a need for a unified approach that\nconsiders both attributes. We performed a literature analysis that selected\nfactors that were noted to significantly influence either user perception or\ntask efficiency and propose a set of design considerations and recommendations\nthat address those factors by combining anthropomorphic and non-anthropomorphic\nvirtual gestures based on the motivation of the interaction, visibility of the\ntarget and robot, salience of the target, and distance between the target and\nrobot. The proposed recommendations provide the VAM-HRI community with starting\npoints for selecting appropriate gesture types for a multitude of interaction\ncontexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10564,regular,pre_llm,2022,5,"{'ai_likelihood': 5.099508497450087e-06, 'text': ""Shared-Control Robotic Manipulation in Virtual Reality\n\n  In this paper, we present the implementation details of a Virtual Reality\n(VR)-based teleoperation interface for moving a robotic manipulator. We propose\nan iterative human-in-the-loop design where the user sets the next task-space\nwaypoint for the robot's end effector and executes the action on the physical\nrobot before setting the next waypoints. Information from the robot's\nsurroundings is provided to the user in two forms: as a point cloud in 3D space\nand a video stream projected on a virtual wall. The feasibility of the selected\nend effector pose is communicated to the user by the color of the virtual end\neffector. The interface is demonstrated to successfully work for a pick and\nplace scenario, however, our trials showed that the fluency of the interaction\nand the autonomy level of the system can be increased.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.01823,regular,pre_llm,2022,5,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose\n  Estimation\n\n  We propose a keypoint-based object-level SLAM framework that can provide\nglobally consistent 6DoF pose estimates for symmetric and asymmetric objects\nalike. To the best of our knowledge, our system is among the first to utilize\nthe camera pose information from SLAM to provide prior knowledge for tracking\nkeypoints on symmetric objects -- ensuring that new measurements are consistent\nwith the current 3D scene. Moreover, our semantic keypoint network is trained\nto predict the Gaussian covariance for the keypoints that captures the true\nerror of the prediction, and thus is not only useful as a weight for the\nresiduals in the system's optimization problems, but also as a means to detect\nharmful statistical outliers without choosing a manual threshold. Experiments\nshow that our method provides competitive performance to the state of the art\nin 6DoF object pose estimation, and at a real-time speed. Our code, pre-trained\nmodels, and keypoint labels are available https://github.com/rpng/suo_slam.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.13242,regular,pre_llm,2022,5,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'GNSS-Denied Semi Direct Visual Navigation for Autonomous UAVs Aided by\n  PI-Inspired Inertial Priors\n\n  This article proposes a method to diminish the pose (position plus attitude)\ndrift experienced by an SVO (Semi-Direct Visual Odometry) based visual\nnavigation system installed onboard a UAV (Unmanned Air Vehicle) by\nsupplementing its pose estimation non linear optimizations with priors based on\nthe outputs of a GNSS (Global Navigation Satellite System) Denied inertial\nnavigation system. The method is inspired in a PI (Proportional Integral)\ncontrol system, in which the attitude, altitude, and rate of climb inertial\noutputs act as targets to ensure that the visual estimations do not deviate far\nfrom their inertial counterparts. The resulting IA-VNS (Inertially Assisted\nVisual Navigation System) achieves major reductions in the horizontal position\ndrift inherent to the GNSS-Denied navigation of autonomous fixed wing low SWaP\n(Size, Weight, and Power) UAVs. Additionally, the IA-VNS can be considered as a\nvirtual incremental position (ground velocity) sensor capable of providing\nobservations to the inertial filter. Stochastic high fidelity Monte Carlo\nsimulations of two representative scenarios involving the loss of GNSS signals\nare employed to evaluate the results and to analyze their sensitivity to the\nterrain type overflown by the aircraft as well as to the quality of the onboard\nsensors on which the priors are based. The author releases the C ++\nimplementation of both the navigation algorithms and the high fidelity\nsimulation as open-source software.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.11719,regular,pre_llm,2022,5,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Safe, Occlusion-Aware Manipulation for Online Object Reconstruction in\n  Confined Spaces\n\n  Recent work in robotic manipulation focuses on object retrieval in cluttered\nspaces under occlusion. Nevertheless, the majority of efforts lack an analysis\nof conditions for the completeness of the approaches or the methods apply only\nwhen objects can be removed from the workspace. This work formulates the\ngeneral, occlusion-aware manipulation task, and focuses on safe object\nreconstruction in a confined space with in-place rearrangement. It proposes a\nframework that ensures safety with completeness guarantees. Furthermore, an\nalgorithm, which is an instantiation of this abstract framework for monotone\ninstances is developed and evaluated empirically by comparing against a random\nand a greedy baseline on randomly generated experiments in simulation. Even for\ncluttered scenes with realistic objects, the proposed algorithm significantly\noutperforms the baselines and maintains a high success rate across experimental\nconditions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.0514,regular,pre_llm,2022,5,"{'ai_likelihood': 2.615981631808811e-06, 'text': ""RotorTM: A Flexible Simulator for Aerial Transportation and Manipulation\n\n  Low-cost autonomous Micro Aerial Vehicles (MAVs) have the potential to help\nhumans by simplifying and speeding up complex tasks that require their\ninteraction with the environment, such as construction, package delivery, and\nsearch and rescue. These systems, composed of single or multiple vehicles, can\nbe endowed with passive connection mechanisms such as rigid links or cables to\nperform transportation and manipulation tasks. However, they are inherently\ncomplex since they are often underactuated and evolve in nonlinear manifold\nconfiguration spaces. In addition, the complexity of systems with\ncable-suspended load is further increased by the hybrid dynamics depending on\nthe cables' varying tension conditions. This paper presents the first aerial\ntransportation and manipulation simulator incorporating different payloads and\npassive connection mechanisms with full system dynamics, planning, and control\nalgorithms. Furthermore, it includes a novel general model accounting for the\ntransient hybrid dynamics for aerial systems with cable-suspended load to\nclosely mimic real-world systems. The availability of a flexible and intuitive\ninterface further contributes to its usability and versatility. Comparisons\nbetween simulations and real-world experiments with different vehicles'\nconfigurations show the fidelity of the simulator results with respect to\nreal-world settings. The experiments also show the simulator's benefit for the\nrapid prototyping and transitioning of aerial transportation and manipulation\nsystems to real-world deployment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.01791,regular,pre_llm,2022,5,"{'ai_likelihood': 1.9073486328125e-05, 'text': ""TartanDrive: A Large-Scale Dataset for Learning Off-Road Dynamics Models\n\n  We present TartanDrive, a large scale dataset for learning dynamics models\nfor off-road driving. We collected a dataset of roughly 200,000 off-road\ndriving interactions on a modified Yamaha Viking ATV with seven unique sensing\nmodalities in diverse terrains. To the authors' knowledge, this is the largest\nreal-world multi-modal off-road driving dataset, both in terms of number of\ninteractions and sensing modalities. We also benchmark several state-of-the-art\nmethods for model-based reinforcement learning from high-dimensional\nobservations on this dataset. We find that extending these models to\nmulti-modality leads to significant performance on off-road dynamics\nprediction, especially in more challenging terrains. We also identify some\nshortcomings with current neural network architectures for the off-road driving\ntask. Our dataset is available at https://github.com/castacks/tartan_drive.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.08454,regular,pre_llm,2022,5,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Dynamic Optimization Fabrics for Motion Generation\n\n  Optimization fabrics are a geometric approach to real-time local motion\ngeneration, where motions are designed by the composition of several\ndifferential equations that exhibit a desired motion behavior. We generalize\nthis framework to dynamic scenarios and non-holonomic robots and prove that\nfundamental properties can be conserved. We show that convergence to desired\ntrajectories and avoidance of moving obstacles can be guaranteed using simple\nconstruction rules of the components. Additionally, we present the first\nquantitative comparisons between optimization fabrics and model predictive\ncontrol and show that optimization fabrics can generate similar trajectories\nwith better scalability, and thus, much higher replanning frequency (up to 500\nHz with a 7 degrees of freedom robotic arm). Finally, we present empirical\nresults on several robots, including a non-holonomic mobile manipulator with 10\ndegrees of freedom and avoidance of a moving human, supporting the theoretical\nfindings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.04169,regular,pre_llm,2022,5,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Multi-Fingered In-Hand Manipulation with Various Object Properties Using\n  Graph Convolutional Networks and Distributed Tactile Sensors\n\n  Multi-fingered hands could be used to achieve many dexterous manipulation\ntasks, similarly to humans, and tactile sensing could enhance the manipulation\nstability for a variety of objects. However, tactile sensors on multi-fingered\nhands have a variety of sizes and shapes. Convolutional neural networks (CNN)\ncan be useful for processing tactile information, but the information from\nmulti-fingered hands needs an arbitrary pre-processing, as CNNs require a\nrectangularly shaped input, which may lead to unstable results. Therefore, how\nto process such complex shaped tactile information and utilize it for achieving\nmanipulation skills is still an open issue. This paper presents a control\nmethod based on a graph convolutional network (GCN) which extracts geodesical\nfeatures from the tactile data with complicated sensor alignments. Moreover,\nobject property labels are provided to the GCN to adjust in-hand manipulation\nmotions. Distributed tri-axial tactile sensors are mounted on the fingertips,\nfinger phalanges and palm of an Allegro hand, resulting in 1152 tactile\nmeasurements. Training data is collected with a data-glove to transfer human\ndexterous manipulation directly to the robot hand. The GCN achieved high\nsuccess rates for in-hand manipulation. We also confirmed that fragile objects\nwere deformed less when correct object labels were provided to the GCN. When\nvisualizing the activation of the GCN with a PCA, we verified that the network\nacquired geodesical features. Our method achieved stable manipulation even when\nan experimenter pulled a grasped object and for untrained objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10277,regular,pre_llm,2022,5,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Loco-Manipulation Planning for Legged Robots: Offline and Online\n  Strategies\n\n  The deployment of robots within realistic environments requires the\ncapability to plan and refine the loco-manipulation trajectories on the fly to\navoid unexpected interactions with a dynamic environment. This extended\nabstract provides a pipeline to offline plan a configuration space global\ntrajectory based on a randomized strategy, and to online locally refine it\ndepending on any change of the dynamic environment and the robot state. The\noffline planner directly plans in the contact space, and additionally seeks for\nwhole-body feasible configurations compliant with the sampled contact states.\nThe planned trajectory, made by a discrete set of contacts and configurations,\ncan be seen as a graph and it can be online refined during the execution of the\nglobal trajectory. The online refinement is carried out by a graph optimization\nplanner exploiting visual information. It locally acts on the global initial\nplan to account for possible changes in the environment. While the offline\nplanner is a concluded work, tested on the humanoid COMAN+, the online local\nplanner is still a work-in-progress which has been tested on a reduced model of\nthe CENTAURO robot to avoid dynamic and static obstacles interfering with a\nwheeled motion task. Both the COMAN+ and the CENTAURO robots have been designed\nat the Italian Institute of Technology (IIT).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.03192,regular,pre_llm,2022,5,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Controlling Robot Swarm Aggregation through a Minority of Informed\n  Robots\n\n  Self-organized aggregation is a well studied behavior in swarm robotics as it\nis the pre-condition for the development of more advanced group-level\nresponses. In this paper, we investigate the design of decentralized algorithms\nfor a swarm of heterogeneous robots that self-aggregate over distinct target\nsites. A previous study has shown that including as part of the swarm a number\nof informed robots can steer the dynamic of the aggregation process to a\ndesirable distribution of the swarm between the available aggregation sites. We\nhave replicated the results of the previous study using a simplified approach:\nwe removed constraints related to the communication protocol of the robots and\nsimplified the control mechanisms regulating the transitions between states of\nthe probabilistic controller. The results show that the performances obtained\nwith the previous, more complex, controller can be replicated with our\nsimplified approach which offers clear advantages in terms of portability to\nthe physical robots and in terms of flexibility. That is, our simplified\napproach can generate self-organized aggregation responses in a larger set of\noperating conditions than what can be achieved with the complex controller.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.02523,regular,pre_llm,2022,5,"{'ai_likelihood': 4.404120975070529e-06, 'text': 'Parallel Parking: Optimal Entry and Minimum Slot Dimensions\n\n  The problem of path planning for automated parking is usually presented as\nfinding a collision-free path from initial to goal positions, where three out\nof four parking slot edges represent obstacles. We rethink the path planning\nproblem for parallel parking by decomposing it into two independent parts. The\ntopic of this paper is finding optimal parking slot entry positions. Path\nplanning from initial to entry position is out of scope here. We show the\nrelation between entry positions, parking slot dimensions, and the number of\nbackward-forward direction changes. This information can be used as an input to\noptimize other parts of the automated parking process.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.08701,regular,pre_llm,2022,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Extrinsic Calibration of LiDAR, IMU and Camera\n\n  In this work we present a novel method to jointly calibrate a sensor suite\nconsisting a 3D-LiDAR, Inertial Measurement Unit (IMU) and Camera under an\nExtended Kalman Filter (EKF) framework. We exploit pairwise constraints between\nthe 3 sensor pairs to perform EKF update and experimentally demonstrate the\nsuperior performance obtained with joint calibration as against individual\nsensor pair calibration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.14099,regular,pre_llm,2022,5,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'BURG-Toolkit: Robot Grasping Experiments in Simulation and the Real\n  World\n\n  This paper presents BURG-Toolkit, a set of open-source tools for Benchmarking\nand Understanding Robotic Grasping. Our tools allow researchers to: (1) create\nvirtual scenes for generating training data and performing grasping in\nsimulation; (2) recreate the scene by arranging the corresponding objects\naccurately in the physical world for real robot experiments, supporting an\nanalysis of the sim-to-real gap; and (3) share the scenes with other\nresearchers to foster comparability and reproducibility of experimental\nresults. We explain how to use our tools by describing some potential use\ncases. We further provide proof-of-concept experimental results quantifying the\nsim-to-real gap for robot grasping in some example scenes. The tools are\navailable at: https://mrudorfer.github.io/burg-toolkit/\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.01267,regular,pre_llm,2022,5,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'PropEM-L: Radio Propagation Environment Modeling and Learning for\n  Communication-Aware Multi-Robot Exploration\n\n  Multi-robot exploration of complex, unknown environments benefits from the\ncollaboration and cooperation offered by inter-robot communication. Accurate\nradio signal strength prediction enables communication-aware exploration.\nModels which ignore the effect of the environment on signal propagation or rely\non a priori maps suffer in unknown, communication-restricted (e.g.\nsubterranean) environments. In this work, we present Propagation Environment\nModeling and Learning (PropEM-L), a framework which leverages real-time\nsensor-derived 3D geometric representations of an environment to extract\ninformation about line of sight between radios and attenuating walls/obstacles\nin order to accurately predict received signal strength (RSS). Our data-driven\napproach combines the strengths of well-known models of signal propagation\nphenomena (e.g. shadowing, reflection, diffraction) and machine learning, and\ncan adapt online to new environments. We demonstrate the performance of\nPropEM-L on a six-robot team in a communication-restricted environment with\nsubway-like, mine-like, and cave-like characteristics, constructed for the 2021\nDARPA Subterranean Challenge. Our findings indicate that PropEM-L can improve\nsignal strength prediction accuracy by up to 44% over a log-distance path loss\nmodel.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.00816,regular,pre_llm,2022,5,"{'ai_likelihood': 1.447068320380317e-05, 'text': 'Semantic localization on BIM-generated maps using a 3D LiDAR sensor\n\n  Conventional sensor-based localization relies on high-precision maps, which\nare generally built using specialized mapping techniques involving high labor\nand computational costs. In the architectural, engineering and construction\nindustry, Building Information Models (BIM) are available and can provide\ninformative descriptions of environments. This paper explores an effective way\nto localize a mobile 3D LiDAR sensor on BIM-generated maps considering both\ngeometric and semantic properties. First, original BIM elements are converted\nto semantically augmented point cloud maps using categories and locations.\nAfter that, a coarse-to-fine semantic localization is performed to align laser\npoints to the map based on iterative closest point registration. The\nexperimental results show that the semantic localization can track the pose\nsuccessfully with only one LiDAR sensor, thus demonstrating the feasibility of\nthe proposed mapping-free localization framework. The results also show that\nusing semantic information can help reduce localization errors on BIM-generated\nmaps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.10841,regular,pre_llm,2022,5,"{'ai_likelihood': 7.152557373046875e-06, 'text': 'Robust Modeling and Controls for Racing on the Edge\n\n  Race cars are routinely driven to the edge of their handling limits in\ndynamic scenarios well above 200mph. Similar challenges are posed in autonomous\nracing, where a software stack, instead of a human driver, interacts within a\nmulti-agent environment. For an Autonomous Racing Vehicle (ARV), operating at\nthe edge of handling limits and acting safely in these dynamic environments is\nstill an unsolved problem. In this paper, we present a baseline controls stack\nfor an ARV capable of operating safely up to 140mph. Additionally, limitations\nin the current approach are discussed to highlight the need for improved\ndynamics modeling and learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2205.09973,regular,pre_llm,2022,5,"{'ai_likelihood': 8.775128258599176e-06, 'text': ""An In-Pipe Inspection Robot With Sensorless Underactuated Magnets and\n  Omnidirectional Tracks: Design and Implementation\n\n  This paper presents the plan of an in-pipe climbing robot that works\nutilizing an astute transmission part to investigate complex relationship of\nlines. Standard wheeled/proceeded in-pipe climbing robots are inclined to slip\nand take while investigating in pipe turns. The instrument helps in\naccomplishing the main inevitable result of getting out slip and drag in the\nrobot tracks during advancement. The proposed transmission appreciates the\npractical furthest reaches of the standard two-yield transmission, which is\ndeveloped the basic time for a transmission with three results. The instrument\nconclusively changes the track speeds of the robot considering the powers\napplied on each track inside the line relationship, by getting out the\nessential for any remarkable control. The amusement of the robot crossing in\nthe line network in various orientation and in pipe-turns without slip shows\nthe proposed game plan's adequacy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.00607,regular,pre_llm,2022,6,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'Performance Study of Low Inertia Magnetorheological Actuators for\n  Kinesthetic Haptic Devices\n\n  A challenge to high quality virtual reality (VR) simulations is the\ndevelopment of high-fidelity haptic devices that can render a wide range of\nimpedances at both low and high frequencies. To this end, a thorough analytical\nand experimental assessment of the performance of magnetorheological (MR)\nactuators is performed and compared to electric motor (EM) actuation. A 2\ndegrees-of-freedom dynamic model of a kinesthetic haptic device is used to\nconduct the analytical study comparing the rendering area, rendering bandwidth,\ngearing and scaling of both technologies. Simulation predictions are\ncorroborated by experimental validation over a wide range of operating\nconditions. Results show that, for a same output force, MR actuators can render\na bandwidth over 52.9% higher than electric motors due to their low inertia.\nUnlike electric motors, the performance of MR actuators for use in haptic\ndevices are not limited by their output inertia but by their viscous damping,\nwhich must be carefully addressed at the design stage.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.07201,regular,pre_llm,2022,6,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'An autonomous robot for pruning modern, planar fruit trees\n\n  Dormant pruning of fruit trees is an important task for maintaining tree\nhealth and ensuring high-quality fruit. Due to decreasing labor availability,\npruning is a prime candidate for robotic automation. However, pruning also\nrepresents a uniquely difficult problem for robots, requiring robust systems\nfor perception, pruning point determination, and manipulation that must operate\nunder variable lighting conditions and in complex, highly unstructured\nenvironments. In this paper, we introduce a system for pruning sweet cherry\ntrees (in a planar tree architecture called an upright fruiting offshoot\nconfiguration) that integrates various subsystems from our previous work on\nperception and manipulation. The resulting system is capable of operating\ncompletely autonomously and requires minimal control of the environment. We\nvalidate the performance of our system through field trials in a sweet cherry\norchard, ultimately achieving a cutting success rate of 58%. Though not fully\nrobust and requiring improvements in throughput, our system is the first to\noperate on fruit trees and represents a useful base platform to be improved in\nthe future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.00373,regular,pre_llm,2022,6,"{'ai_likelihood': 9.43740208943685e-06, 'text': ""A Flexible and Robust Vision Trap for Automated Part Feeder Design\n\n  Fast, robust, and flexible part feeding is essential for enabling automation\nof low volume, high variance assembly tasks. An actuated vision-based solution\non a traditional vibratory feeder, referred to here as a vision trap, should in\nprinciple be able to meet these demands for a wide range of parts. However, in\npractice, the flexibility of such a trap is limited as an expert is needed to\nboth identify manageable tasks and to configure the vision system. We propose a\nnovel approach to vision trap design in which the identification of manageable\ntasks is automatic and the configuration of these tasks can be delegated to an\nautomated feeder design system. We show that the trap's capabilities can be\nformalized in such a way that it integrates seamlessly into the ecosystem of\nautomated feeder design. Our results on six canonical parts show great promise\nfor autonomous configuration of feeder systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.06537,regular,pre_llm,2022,6,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'A software toolkit and hardware platform for investigating and comparing\n  robot autonomy algorithms in simulation and reality\n\n  We describe a software framework and a hardware platform used in tandem for\nthe design and analysis of robot autonomy algorithms in simulation and reality.\nThe software, which is open source, containerized, and operating system (OS)\nindependent, has three main components: a ROS 2 interface to a C++ vehicle\nsimulation framework (Chrono), which provides high-fidelity wheeled/tracked\nvehicle and sensor simulation; a basic ROS 2-based autonomy stack for algorithm\ndesign and testing; and, a development ecosystem which enables visualization,\nand hardware-in-the-loop experimentation in perception, state estimation, path\nplanning, and controls. The accompanying hardware platform is a 1/6th scale\nvehicle augmented with reconfigurable mountings for computing, sensing, and\ntracking. Its purpose is to allow algorithms and sensor configurations to be\nphysically tested and improved. Since this vehicle platform has a digital twin\nwithin the simulation environment, one can test and compare the same algorithms\nand autonomy stack in simulation and reality. This platform has been built with\nan eye towards characterizing and managing the simulation-to-reality gap.\nHerein, we describe how this platform is set up, deployed, and used to improve\nautonomy for mobility applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.00528,regular,pre_llm,2022,6,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Collaborative Bimanual Manipulation Using Optimal Motion Adaptation and\n  Interaction Control\n\n  This work developed collaborative bimanual manipulation for reliable and safe\nhuman-robot collaboration, which allows remote and local human operators to\nwork interactively for bimanual tasks. We proposed an optimal motion adaptation\nto retarget arbitrary commands from multiple human operators into feasible\ncontrol references. The collaborative manipulation framework has three main\nmodules: (1) contact force modulation for compliant physical interactions with\nobjects via admittance control; (2) task-space sequential equilibrium and\ninverse kinematics optimization, which adapts interactive commands from\nmultiple operators to feasible motions by satisfying the task constraints and\nphysical limits of the robots; and (3) an interaction controller adopted from\nthe fractal impedance control, which is robust to time delay and stable to\nsuperimpose multiple control efforts for generating desired joint torques and\ncontrolling the dual-arm robots. Extensive experiments demonstrated the\ncapability of the collaborative bimanual framework, including (1) dual-arm\nteleoperation that adapts arbitrary infeasible commands that violate joint\ntorque limits into continuous operations within safe boundaries, compared to\nfailures without the proposed optimization; (2) robust maneuver of a stack of\nobjects via physical interactions in presence of model inaccuracy; (3)\ncollaborative multi-operator part assembly, and teleoperated industrial\nconnector insertion, which validate the guaranteed stability of reliable\nhuman-robot co-manipulation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.01683,regular,pre_llm,2022,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'FishGym: A High-Performance Physics-based Simulation Framework for\n  Underwater Robot Learning\n\n  Bionic underwater robots have demonstrated their superiority in many\napplications. Yet, training their intelligence for a variety of tasks that\nmimic the behavior of underwater creatures poses a number of challenges in\npractice, mainly due to lack of a large amount of available training data as\nwell as the high cost in real physical environment. Alternatively, simulation\nhas been considered as a viable and important tool for acquiring datasets in\ndifferent environments, but it mostly targeted rigid and soft body systems.\nThere is currently dearth of work for more complex fluid systems interacting\nwith immersed solids that can be efficiently and accurately simulated for robot\ntraining purposes. In this paper, we propose a new platform called ""FishGym"",\nwhich can be used to train fish-like underwater robots. The framework consists\nof a robotic fish modeling module using articulated body with skinning, a\nGPU-based high-performance localized two-way coupled fluid-structure\ninteraction simulation module that handles both finite and infinitely large\ndomains, as well as a reinforcement learning module. We leveraged existing\ntraining methods with adaptations to underwater fish-like robots and obtained\nlearned control policies for multiple benchmark tasks. The training results are\ndemonstrated with reasonable motion trajectories, with comparisons and analyses\nto empirical models as well as known real fish swimming behaviors to highlight\nthe advantages of the proposed platform.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.01517,regular,pre_llm,2022,6,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'GraphDistNet: A Graph-based Collision-distance Estimator for\n  Gradient-based Trajectory Optimization\n\n  Trajectory optimization (TO) aims to find a sequence of valid states while\nminimizing costs. However, its fine validation process is often costly due to\ncomputationally expensive collision searches, otherwise coarse searches lower\nthe safety of the system losing a precise solution. To resolve the issues, we\nintroduce a new collision-distance estimator, GraphDistNet, that can precisely\nencode the structural information between two geometries by leveraging edge\nfeature-based convolutional operations, and also efficiently predict a batch of\ncollision distances and gradients through 25,000 random environments with a\nmaximum of 20 unforeseen objects. Further, we show the adoption of attention\nmechanism enables our method to be easily generalized in unforeseen complex\ngeometries toward TO. Our evaluation show GraphDistNet outperforms\nstate-of-the-art baseline methods in both simulated and real world tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.11789,regular,pre_llm,2022,6,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Probabilistically Resilient Multi-Robot Informative Path Planning\n\n  In this paper, we solve a multi-robot informative path planning (MIPP) task\nunder the influence of uncertain communication and adversarial attackers. The\ngoal is to create a multi-robot system that can learn and unify its knowledge\nof an unknown environment despite the presence of corrupted robots sharing\nmalicious information. We use a Gaussian Process (GP) to model our unknown\nenvironment and define informativeness using the metric of mutual information.\nThe objectives of our MIPP task is to maximize the amount of information\ncollected by the team while maximizing the probability of resilience to attack.\nUnfortunately, these objectives are at odds especially when exploring large\nenvironments which necessitates disconnections between robots. As a result, we\nimpose a probabilistic communication constraint that allows robots to meet\nintermittently and resiliently share information, and then act to maximize\ncollected information during all other times. To solve our problem, we select\nmeeting locations with the highest probability of resilience and use a\nsequential greedy algorithm to optimize paths for robots to explore. Finally,\nwe show the validity of our results by comparing the learning ability of\nwell-behaving robots applying resilient vs. non-resilient MIPP algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.04533,regular,pre_llm,2022,6,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'DogTouch: CNN-based Recognition of Surface Textures by Quadruped Robot\n  with High Density Tactile Sensors\n\n  The ability to perform locomotion in various terrains is critical for legged\nrobots. However, the robot has to have a better understanding of the surface it\nis walking on to perform robust locomotion on different terrains. Animals and\nhumans are able to recognize the surface with the help of the tactile sensation\non their feet. Although, the foot tactile sensation for legged robots has not\nbeen much explored. This paper presents research on a novel quadruped robot\nDogTouch with tactile sensing feet (TSF). TSF allows the recognition of\ndifferent surface textures utilizing a tactile sensor and a convolutional\nneural network (CNN). The experimental results show a sufficient validation\naccuracy of 74.37\\% for our trained CNN-based model, with the highest\nrecognition for line patterns of 90\\%. In the future, we plan to improve the\nprediction model by presenting surface samples with the various depths of\npatterns and applying advanced Deep Learning and Shallow learning models for\nsurface recognition.\n  Additionally, we propose a novel approach to navigation of quadruped and\nlegged robots. We can arrange the tactile paving textured surface (similar that\nused for blind or visually impaired people). Thus, DogTouch will be capable of\nlocomotion in unknown environment by just recognizing the specific tactile\npatterns which will indicate the straight path, left or right turn, pedestrian\ncrossing, road, and etc. That will allow robust navigation regardless of\nlighting condition. Future quadruped robots equipped with visual and tactile\nperception system will be able to safely and intelligently navigate and\ninteract in the unstructured indoor and outdoor environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.14867,regular,pre_llm,2022,6,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Pre-stressed Bi-stable Hair Clip Mechanism for Faster Swimming Robots\n\n  Structural instability is a hazard that leads to catastrophic failure and is\ngenerally avoided through special designs. A trend, however, has emerged over\nthe past decades pointing to the harnessing of mechanisms with instability.\nInspired by the snapping of a hair clip, we are finessing the unique\ncharacteristics of the lateral-torsional buckling of beams and the snap-through\nof pre-buckled dome-like thin-wall structures in a new field: the in-plane\nprestressed mechanism. Analyses reveal how the 2D-3D assembly of an in-plane\nprestressed actuator (IPA) is achieved and how the post-buckling energy\nlandscape is pictured. Combining them with soft robotics, we show that the\ninclusion of a bistable IPA can enormously enhance the performance of an\nunderwater fish robot as well as inspire a finger-like soft gripper.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.11977,regular,pre_llm,2022,6,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Hierarchical Planning with Annotated Skeleton Guidance\n\n  We present a hierarchical skeleton-guided motion planning algorithm to guide\nmobile robots. A good skeleton maps the connectivity of the subspace of c-space\ncontaining significant degrees of freedom and is able to guide the planner to\nfind the desired solutions fast. However, sometimes the skeleton does not\nclosely represent the free c-space, which often misleads current\nskeleton-guided planners. The hierarchical skeleton-guided planning strategy\ngradually relaxes its reliance on the workspace skeleton as C space is sampled,\nthereby incrementally returning a sub-optimal path, a feature that is not\nguaranteed in the standard skeleton-guided algorithm. Experimental comparisons\nto the standard skeleton-guided planners and other lazy planning strategies\nshow significant improvement in roadmap construction run time while maintaining\npath quality for multi-query problems in cluttered environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.12728,regular,pre_llm,2022,6,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Learning Preconditions of Hybrid Force-Velocity Controllers for\n  Contact-Rich Manipulation\n\n  Robots need to manipulate objects in constrained environments like shelves\nand cabinets when assisting humans in everyday settings like homes and offices.\nThese constraints make manipulation difficult by reducing grasp accessibility,\nso robots need to use non-prehensile strategies that leverage\nobject-environment contacts to perform manipulation tasks. To tackle the\nchallenge of planning and controlling contact-rich behaviors in such settings,\nthis work uses Hybrid Force-Velocity Controllers (HFVCs) as the skill\nrepresentation and plans skill sequences with learned preconditions. While\nHFVCs naturally enable robust and compliant contact-rich behaviors, solvers\nthat synthesize them have traditionally relied on precise object models and\nclosed-loop feedback on object pose, which are difficult to obtain in\nconstrained environments due to occlusions. We first relax HFVCs' need for\nprecise models and feedback with our HFVC synthesis framework, then learn a\npoint-cloud-based precondition function to classify where HFVC executions will\nstill be successful despite modeling inaccuracies. Finally, we use the learned\nprecondition in a search-based task planner to complete contact-rich\nmanipulation tasks in a shelf domain. Our method achieves a task success rate\nof $73.2\\%$, outperforming the $51.5\\%$ achieved by a baseline without the\nlearned precondition. While the precondition function is trained in simulation,\nit can also transfer to a real-world setup without further fine-tuning. See\nsupplementary materials and videos at\nhttps://sites.google.com/view/constrained-manipulation/\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.08075,regular,pre_llm,2022,6,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Exploring Collaborative Game Play with Robots to Encourage Good Hand\n  Hygiene Practises among Children\n\n  This paper presents the design, implementation, and evaluation of a novel\ncollaborative educational game titled ""Land of Hands"", involving children and a\ncustomized social robot that we designed (HakshE). Through this gaming\nplatform, we aim to teach proper hand hygiene practises to children and explore\nthe extent of interactions that take place between a pro-social robot and\nchildren in such a setting. We blended gamification with Computers as Social\nActors (CASA) paradigm to model the robot as a social actor or a fellow player\nin the game. The game was developed using Godot\'s 2D engine and Alice 3. In\nthis study, 32 participants played the game online through a video\nteleconferencing platform Zoom. To understand the influence a pro-social\nrobot\'s nudges has on children\'s interactions, we split our study into two\nconditions: With-Nudges and Without-Nudges. Detailed analysis of rubrics and\nvideo analyses of children\'s interactions show that our platform helped\nchildren learn good hand hygiene practises. We also found that using a\npro-social robot creates enjoyable interactions and greater social engagement\nbetween the children and the robot although learning itself wasn\'t influenced\nby the pro-sociality of the robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.08783,regular,pre_llm,2022,6,"{'ai_likelihood': 1.2550089094373916e-05, 'text': 'A Human-Centric Method for Generating Causal Explanations in Natural\n  Language for Autonomous Vehicle Motion Planning\n\n  Inscrutable AI systems are difficult to trust, especially if they operate in\nsafety-critical settings like autonomous driving. Therefore, there is a need to\nbuild transparent and queryable systems to increase trust levels. We propose a\ntransparent, human-centric explanation generation method for autonomous vehicle\nmotion planning and prediction based on an existing white-box system called\nIGP2. Our method integrates Bayesian networks with context-free generative\nrules and can give causal natural language explanations for the high-level\ndriving behaviour of autonomous vehicles. Preliminary testing on simulated\nscenarios shows that our method captures the causes behind the actions of\nautonomous vehicles and generates intelligible explanations with varying\ncomplexity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.15242,regular,pre_llm,2022,6,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Secure Heterogeneous Multi-Robot Collaboration and Docking with\n  Hyperledger Fabric Blockchain\n\n  In recent years, multi-robot systems have received increasing attention from\nboth industry and academia. Besides the need of accurate and robust estimation\nof relative localization, security and trust in the system are essential to\nenable wider adoption. In this paper, we propose a framework using Hyperledger\nFabric for multi-robot collaboration in industrial applications. We rely on\nblockchain identities for the interaction of ground and aerial robots, and use\nsmart contracts for collaborative decision making. The use of ultra-wideband\n(UWB) localization for both autonomous navigation and robot collaboration\nextends our previous work in Fabric-based fleet management. We focus on an\ninventory management application which uses a ground robot and an aerial robot\nto inspect a warehouse-like environment and store information about the found\nobjects in the blockchain. We measure the impact of adding the blockchain\nlayer, analyze the transaction commit latency and compare the resource\nutilization of blockchain-related processes to the already running data\nprocessing modules.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.14071,regular,pre_llm,2022,6,"{'ai_likelihood': 4.933940039740669e-06, 'text': 'R2: Heuristic Bug-Based Any-angle Path-Planning using Lazy Searches\n\n  R2 is a novel online any-angle path planner that uses heuristic bug-based or\nray casting approaches to find optimal paths in 2D maps with non-convex,\npolygonal obstacles. R2 is competitive to traditional free-space planners,\nfinding paths quickly if queries have direct line-of-sight. On large sparse\nmaps with few obstacle contours, which are likely to occur in practice, R2\noutperforms free-space planners, and can be much faster than state-of-the-art\nfree-space expansion planner Anya. On maps with many contours, Anya performs\nfaster than R2. R2 is built on RayScan, introducing lazy-searches and a\nsource-pledge counter to find successors optimistically on contiguous contours.\nThe novel approach bypasses most successors on jagged contours to reduce\nexpensive line-of-sight checks, therefore requiring no pre-processing to be a\ncompetitive online any-angle planner.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.06556,regular,pre_llm,2022,6,"{'ai_likelihood': 1.7881393432617188e-06, 'text': ""F3 Hand: A Versatile Robot Hand Inspired by Human Thumb and Index\n  Fingers\n\n  It is challenging to grasp numerous objects with varying sizes and shapes\nwith a single robot hand. To address this, we propose a new robot hand called\nthe 'F3 hand' inspired by the complex movements of human index finger and\nthumb. The F3 hand attempts to realize complex human-like grasping movements by\ncombining a parallel motion finger and a rotational motion finger with an\nadaptive function. In order to confirm the performance of our hand, we attached\nit to a mobile manipulator - the Toyota Human Support Robot (HSR) and conducted\ngrasping experiments. In our results, we show that it is able to grasp all YCB\nobjects (82 in total), including washers with outer diameters as small as\n6.4mm. We also built a system for intuitive operation with a 3D mouse and grasp\nan additional 24 objects, including small toothpicks and paper clips and large\npitchers and cracker boxes. The F3 hand is able to achieve a 98% success rate\nin grasping even under imprecise control and positional offsets. Furthermore,\nowing to the finger's adaptive function, we demonstrate characteristics of the\nF3 hand that facilitate the grasping of soft objects such as strawberries in a\ndesirable posture.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.06112,regular,pre_llm,2022,6,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Vision-State Fusion: Improving Deep Neural Networks for Autonomous\n  Robotics\n\n  Vision-based deep learning perception fulfills a paramount role in robotics,\nfacilitating solutions to many challenging scenarios, such as acrobatic\nmaneuvers of autonomous unmanned aerial vehicles (UAVs) and robot-assisted\nhigh-precision surgery. Control-oriented end-to-end perception approaches,\nwhich directly output control variables for the robot, commonly take advantage\nof the robot's state estimation as an auxiliary input. When intermediate\noutputs are estimated and fed to a lower-level controller, i.e. mediated\napproaches, the robot's state is commonly used as an input only for egocentric\ntasks, which estimate physical properties of the robot itself. In this work, we\npropose to apply a similar approach for the first time -- to the best of our\nknowledge -- to non-egocentric mediated tasks, where the estimated outputs\nrefer to an external subject. We prove how our general methodology improves the\nregression performance of deep convolutional neural networks (CNNs) on a broad\nclass of non-egocentric 3D pose estimation problems, with minimal computational\ncost. By analyzing three highly-different use cases, spanning from grasping\nwith a robotic arm to following a human subject with a pocket-sized UAV, our\nresults consistently improve the R\\textsuperscript{2} regression metric, up to\n+0.51, compared to their stateless baselines. Finally, we validate the in-field\nperformance of a closed-loop autonomous cm-scale UAV on the human pose\nestimation task. Our results show a significant reduction, i.e., 24\\% on\naverage, on the mean absolute error of our stateful CNN, compared to a\nState-of-the-Art stateless counterpart.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.11626,regular,pre_llm,2022,6,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Model-Based Disturbance Estimation for a Fiber-Reinforced Soft\n  Manipulator using Orientation Sensing\n\n  For soft robots to work effectively in human-centered environments, they need\nto be able to estimate their state and external interactions based on\n(proprioceptive) sensors. Estimating disturbances allows a soft robot to\nperform desirable force control. Even in the case of rigid manipulators, force\nestimation at the end-effector is seen as a non-trivial problem. And indeed,\nother current approaches to address this challenge have shortcomings that\nprevent their general application. They are often based on simplified soft\ndynamic models, such as the ones relying on a piece-wise constant curvature\n(PCC) approximation or matched rigid-body models that do not represent enough\ndetails of the problem. Thus, the applications needed for complex human-robot\ninteraction can not be built. Finite element methods (FEM) allow for\npredictions of soft robot dynamics in a more generic fashion. Here, using the\nsoft robot modeling capabilities of the framework SOFA, we build a detailed FEM\nmodel of a multi-segment soft continuum robotic arm composed of compliant\ndeformable materials and fiber-reinforced pressurized actuation chambers with a\nmodel for sensors that provide orientation output. This model is used to\nestablish a state observer for the manipulator. Model parameters were\ncalibrated to match imperfections of the manual fabrication process using\nphysical experiments. We then solve a quadratic programming inverse dynamics\nproblem to compute the components of external force that explain the pose\nerror. Our experiments show an average force estimation error of around 1.2%.\nAs the methods proposed are generic, these results are encouraging for the task\nof building soft robots exhibiting complex, reactive, sensor-based behavior\nthat can be deployed in human-centered environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2206.03292,regular,pre_llm,2022,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Learning-Based Motion Planning with Mixture Density Networks\n\n  The trade-off between computation time and path optimality is a key\nconsideration in motion planning algorithms. While classical sampling based\nalgorithms fall short of computational efficiency in high dimensional planning,\nlearning based methods have shown great potential in achieving time efficient\nand optimal motion planning. The SOTA learning based motion planning algorithms\nutilize paths generated by sampling based methods as expert supervision data\nand train networks via regression techniques. However, these methods often\noverlook the important multimodal property of the optimal paths in the training\nset, making them incapable of finding good paths in some scenarios. In this\npaper, we propose a Multimodal Neuron Planner (MNP) based on the mixture\ndensity networks that explicitly takes into account the multimodality of the\ntraining data and simultaneously achieves time efficiency and path optimality.\nFor environments represented by a point cloud, MNP first efficiently compresses\nthe point cloud into a latent vector by encoding networks that are suitable for\nprocessing point clouds. We then design multimodal planning networks which\nenables MNP to learn and predict multiple optimal solutions. Simulation results\nshow that our method outperforms SOTA learning based method MPNet and advanced\nsampling based methods IRRT* and BIT*.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.04163,regular,pre_llm,2022,7,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Optimizing Bipedal Maneuvers of Single Rigid-Body Models for\n  Reinforcement Learning\n\n  In this work, we propose a method to generate reduced-order model reference\ntrajectories for general classes of highly dynamic maneuvers for bipedal robots\nfor use in sim-to-real reinforcement learning. Our approach is to utilize a\nsingle rigid-body model (SRBM) to optimize libraries of trajectories offline to\nbe used as expert references in the reward function of a learned policy. This\nmethod translates the model's dynamically rich rotational and translational\nbehaviour to a full-order robot model and successfully transfers to real\nhardware. The SRBM's simplicity allows for fast iteration and refinement of\nbehaviors, while the robustness of learning-based controllers allows for highly\ndynamic motions to be transferred to hardware. % Within this work we introduce\na set of transferability constraints that amend the SRBM dynamics to actual\nbipedal robot hardware, our framework for creating optimal trajectories for\ndynamic stepping, turning maneuvers and jumps as well as our approach to\nintegrating reference trajectories to a reinforcement learning policy. Within\nthis work we introduce a set of transferability constraints that amend the SRBM\ndynamics to actual bipedal robot hardware, our framework for creating optimal\ntrajectories for a variety of highly dynamic maneuvers as well as our approach\nto integrating reference trajectories for a high-speed running reinforcement\nlearning policy. We validate our methods on the bipedal robot Cassie on which\nwe were successfully able to demonstrate highly dynamic grounded running gaits\nup to 3.0 m/s.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.08765,regular,pre_llm,2022,7,"{'ai_likelihood': 6.722079383002388e-06, 'text': 'A Novel Design and Evaluation of a Dactylus-Equipped Quadruped Robot for\n  Mobile Manipulation\n\n  Quadruped robots are usually equipped with additional arms for manipulation,\nnegatively impacting price and weight. On the other hand, the requirements of\nlegged locomotion mean that the legs of such robots often possess the needed\ntorque and precision to perform manipulation. In this paper, we present a novel\ndesign for a small-scale quadruped robot equipped with two leg-mounted\nmanipulators inspired by crustacean chelipeds and knuckle-walker forelimbs. By\nmaking use of the actuators already present in the legs, we can achieve\nmanipulation using only 3 additional motors per limb. The design enables the\nuse of small and inexpensive actuators relative to the leg motors, further\nreducing cost and weight. The moment of inertia impact on the leg is small\nthanks to an integrated cable/pulley system. As we show in a suite of\ntele-operation experiments, the robot is capable of performing single- and\ndual-limb manipulation, as well as transitioning between manipulation modes.\nThe proposed design performs similarly to an additional arm while weighing and\ncosting 5 times less per manipulator and enabling the completion of tasks\nrequiring 2 manipulators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.10821,regular,pre_llm,2022,7,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real\n  Transfer in Navigation\n\n  If we want to train robots in simulation before deploying them in reality, it\nseems natural and almost self-evident to presume that reducing the sim2real gap\ninvolves creating simulators of increasing fidelity (since reality is what it\nis). We challenge this assumption and present a contrary hypothesis -- sim2real\ntransfer of robots may be improved with lower (not higher) fidelity simulation.\nWe conduct a systematic large-scale evaluation of this hypothesis on the\nproblem of visual navigation -- in the real world, and on 2 different\nsimulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot).\nOur results show that, contrary to expectation, adding fidelity does not help\nwith learning; performance is poor due to slow simulation speed (preventing\nlarge-scale learning) and overfitting to inaccuracies in simulation physics.\nInstead, building simple models of the robot motion using real-world data can\nimprove learning and generalization.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00202,regular,pre_llm,2022,7,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'DiffPills: Differentiable Collision Detection for Capsules and Padded\n  Polygons\n\n  Collision detection plays an important role in simulation, control, and\nlearning for robotic systems. However, no existing method is differentiable\nwith respect to the configurations of the objects, greatly limiting the sort of\nalgorithms that can be built on top of collision detection. In this work, we\npropose a set of differentiable collision detection algorithms between capsules\nand padded polygons by formulating these problems as differentiable convex\nquadratic programs. The resulting algorithms are able to return a proximity\nvalue indicating if a collision has taken place, as well as the closest points\nbetween objects, all of which are differentiable. As a result, they can be used\nreliably within other gradient-based optimization methods, including trajectory\noptimization, state estimation, and reinforcement learning methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.09909,regular,pre_llm,2022,7,"{'ai_likelihood': 3.7749608357747397e-06, 'text': 'SLAMER: Simultaneous Localization and Map-Assisted Environment\n  Recognition\n\n  This paper presents a simultaneous localization and map-assisted environment\nrecognition (SLAMER) method. Mobile robots usually have an environment map and\nenvironment information can be assigned to the map. Important information for\nmobile robots such as no entry zone can be predicted if localization has\nsucceeded since relative pose of them can be known. However, this prediction is\nfailed when localization does not work. Uncertainty of pose estimate must be\nconsidered for robustly using the map information. In addition, robots have\nexternal sensors and environment information can be recognized using the\nsensors. This on-line recognition of course contains uncertainty; however, it\nhas to be fused with the map information for robust environment recognition\nsince the map also contains uncertainty owing to over time. SLAMER can\nsimultaneously cope with these uncertainties and achieves accurate localization\nand environment recognition. In this paper, we demonstrate LiDAR-based\nimplementation of SLAMER in two cases. In the first case, we use the\nSemanticKITTI dataset and show that SLAMER achieves accurate estimate more than\ntraditional methods. In the second case, we use an indoor mobile robot and show\nthat unmeasurable environmental objects such as open doors and no entry lines\ncan be recognized.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.11773,regular,pre_llm,2022,7,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""N-LIMB: Neural Limb Optimization for Efficient Morphological Design\n\n  A robot's ability to complete a task is heavily dependent on its physical\ndesign. However, identifying an optimal physical design and its corresponding\ncontrol policy is inherently challenging. The freedom to choose the number of\nlinks, their type, and how they are connected results in a combinatorial design\nspace, and the evaluation of any design in that space requires deriving its\noptimal controller. In this work, we present N-LIMB, an efficient approach to\noptimizing the design and control of a robot over large sets of morphologies.\nCentral to our framework is a universal, design-conditioned control policy\ncapable of controlling a diverse sets of designs. This policy greatly improves\nthe sample efficiency of our approach by allowing the transfer of experience\nacross designs and reducing the cost to evaluate new designs. We train this\npolicy to maximize expected return over a distribution of designs, which is\nsimultaneously updated towards higher performing designs under the universal\npolicy. In this way, our approach converges towards a design distribution\npeaked around high-performing designs and a controller that is effectively\nfine-tuned for those designs. We demonstrate the potential of our approach on a\nseries of locomotion tasks across varying terrains and show the discovery novel\nand high-performing design-control pairs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03395,regular,pre_llm,2022,7,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""Unified Learning from Demonstrations, Corrections, and Preferences\n  during Physical Human-Robot Interaction\n\n  Humans can leverage physical interaction to teach robot arms. This physical\ninteraction takes multiple forms depending on the task, the user, and what the\nrobot has learned so far. State-of-the-art approaches focus on learning from a\nsingle modality, or combine multiple interaction types by assuming that the\nrobot has prior information about the human's intended task. By contrast, in\nthis paper we introduce an algorithmic formalism that unites learning from\ndemonstrations, corrections, and preferences. Our approach makes no assumptions\nabout the tasks the human wants to teach the robot; instead, we learn a reward\nmodel from scratch by comparing the human's inputs to nearby alternatives. We\nfirst derive a loss function that trains an ensemble of reward models to match\nthe human's demonstrations, corrections, and preferences. The type and order of\nfeedback is up to the human teacher: we enable the robot to collect this\nfeedback passively or actively. We then apply constrained optimization to\nconvert our learned reward into a desired robot trajectory. Through simulations\nand a user study we demonstrate that our proposed approach more accurately\nlearns manipulation tasks from physical human interaction than existing\nbaselines, particularly when the robot is faced with new or unexpected\nobjectives. Videos of our user study are available at:\nhttps://youtu.be/FSUJsTYvEKU\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.10454,regular,pre_llm,2022,7,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Temporal and Spatial Online Integrated Calibration for Camera and LiDAR\n\n  While camera and LiDAR are widely used in most of the assisted and autonomous\ndriving systems, only a few works have been proposed to associate the temporal\nsynchronization and extrinsic calibration for camera and LiDAR which are\ndedicated to online sensors data fusion. The temporal and spatial calibration\ntechnologies are facing the challenges of lack of relevance and real-time. In\nthis paper, we introduce the pose estimation model and environmental robust\nline features extraction to improve the relevance of data fusion and instant\nonline ability of correction. Dynamic targets eliminating aims to seek optimal\npolicy considering the correspondence of point cloud matching between adjacent\nmoments. The searching optimization process aims to provide accurate parameters\nwith both computation accuracy and efficiency. To demonstrate the benefits of\nthis method, we evaluate it on the KITTI benchmark with ground truth value. In\nonline experiments, our approach improves the accuracy by 38.5\\% than the soft\nsynchronization method in temporal calibration. While in spatial calibration,\nour approach automatically corrects disturbance errors within 0.4 second and\nachieves an accuracy of 0.3-degree. This work can promote the research and\napplication of sensor fusion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.05774,regular,pre_llm,2022,7,"{'ai_likelihood': 8.410877651638455e-06, 'text': 'A solvable walking model for a two-legged robot\n\n  We present a solvable biped walking model based on an inverted pendulum with\ntwo massless articulated legs capable of walking on uneven floors and inclined\nplanes. The stride of the two-legged robot results from the pendular motion of\na standing leg and the articulated motion of a trailing leg. Gaiting is\npossible due to the alternating role of the legs, the standing and the trailing\nleg, and the conservation of energy of the pendular motion. The motion on\nuneven surfaces and inclined planes is possible by imposing the same maximal\nopening angle between the two legs in the transition between strides and the\nadaptability of the time of each stride. This model is solvable in closed form\nand is reversible in time, modelling the different types of biped motion.\nSeveral optimisation results for the speed of gaiting as a function of the\nrobot parameters have been derived.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.08214,regular,pre_llm,2022,7,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""FEJ-VIRO: A Consistent First-Estimate Jacobian Visual-Inertial-Ranging\n  Odometry\n\n  In recent years, Visual-Inertial Odometry (VIO) has achieved many significant\nprogresses. However, VIO methods suffer from localization drift over long\ntrajectories. In this paper, we propose a First-Estimates Jacobian\nVisual-Inertial-Ranging Odometry (FEJ-VIRO) to reduce the localization drifts\nof VIO by incorporating ultra-wideband (UWB) ranging measurements into the VIO\nframework \\textit{consistently}. Considering that the initial positions of UWB\nanchors are usually unavailable, we propose a long-short window structure to\ninitialize the UWB anchors' positions as well as the covariance for state\naugmentation. After initialization, the FEJ-VIRO estimates the UWB anchors'\npositions simultaneously along with the robot poses. We further analyze the\nobservability of the visual-inertial-ranging estimators and proved that there\nare \\textit{four} unobservable directions in the ideal case, while one of them\nvanishes in the actual case due to the gain of spurious information. Based on\nthese analyses, we leverage the FEJ technique to enforce the unobservable\ndirections, hence reducing inconsistency of the estimator. Finally, we validate\nour analysis and evaluate the proposed FEJ-VIRO with both simulation and\nreal-world experiments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00246,regular,pre_llm,2022,7,"{'ai_likelihood': 2.788172827826606e-05, 'text': 'Point Cloud Change Detection With Stereo V-SLAM:Dataset, Metrics and\n  Baseline\n\n  Localization and navigation are basic robotic tasks requiring an accurate and\nup-to-date map to finish these tasks, with crowdsourced data to detect map\nchanges posing an appealing solution. Collecting and processing crowdsourced\ndata requires low-cost sensors and algorithms, but existing methods rely on\nexpensive sensors or computationally expensive algorithms. Additionally, there\nis no existing dataset to evaluate point cloud change detection. Thus, this\npaper proposes a novel framework using low-cost sensors like stereo cameras and\nIMU to detect changes in a point cloud map. Moreover, we create a dataset and\nthe corresponding metrics to evaluate point cloud change detection with the\nhelp of the high-fidelity simulator Unreal Engine 4. Experiments show that our\nvisualbased framework can effectively detect the changes in our dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.01796,regular,pre_llm,2022,7,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Manipulator Differential Kinematics: Part 1: Kinematics, Velocity, and\n  Applications\n\n  Manipulator kinematics is concerned with the motion of each link within a\nmanipulator without considering mass or force. In this article, which is the\nfirst in a two-part tutorial, we provide an introduction to modelling\nmanipulator kinematics using the elementary transform sequence (ETS). Then we\nformulate the first-order differential kinematics, which leads to the\nmanipulator Jacobian, which is the basis for velocity control and inverse\nkinematics. We describe essential classical techniques which rely on the\nmanipulator Jacobian before exhibiting some contemporary applications. Part II\nof this tutorial provides a formulation of second and higher-order differential\nkinematics, introduces the manipulator Hessian, and illustrates advanced\ntechniques, some of which improve the performance of techniques demonstrated in\nPart I.\n  We have provided Jupyter Notebooks to accompany each section within this\ntutorial. The Notebooks are written in Python code and use the Robotics Toolbox\nfor Python, and the Swift Simulator to provide examples and implementations of\nalgorithms. While not absolutely essential, for the most engaging and\ninformative experience, we recommend working through the Jupyter Notebooks\nwhile reading this article. The Notebooks and setup instructions can be\naccessed at https://github.com/jhavl/dkt.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03034,regular,pre_llm,2022,7,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Energy-based Legged Robots Terrain Traversability Modeling via Deep\n  Inverse Reinforcement Learning\n\n  This work reports on developing a deep inverse reinforcement learning method\nfor legged robots terrain traversability modeling that incorporates both\nexteroceptive and proprioceptive sensory data. Existing works use\nrobot-agnostic exteroceptive environmental features or handcrafted kinematic\nfeatures; instead, we propose to also learn robot-specific inertial features\nfrom proprioceptive sensory data for reward approximation in a single deep\nneural network. Incorporating the inertial features can improve the model\nfidelity and provide a reward that depends on the robot's state during\ndeployment. We train the reward network using the Maximum Entropy Deep Inverse\nReinforcement Learning (MEDIRL) algorithm and propose simultaneously minimizing\na trajectory ranking loss to deal with the suboptimality of legged robot\ndemonstrations. The demonstrated trajectories are ranked by locomotion energy\nconsumption, in order to learn an energy-aware reward function and a more\nenergy-efficient policy than demonstration. We evaluate our method using a\ndataset collected by an MIT Mini-Cheetah robot and a Mini-Cheetah simulator.\nThe code is publicly available at\nhttps://github.com/ganlumomo/minicheetah-traversability-irl.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00692,regular,pre_llm,2022,7,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Humanoid Self-Collision Avoidance Using Whole-Body Control with Control\n  Barrier Functions\n\n  This work combines control barrier functions (CBFs) with a whole-body\ncontroller to enable self-collision avoidance for the MIT Humanoid. Existing\nreactive controllers for self-collision avoidance cannot guarantee\ncollision-free trajectories as they do not leverage the robot's full dynamics,\nthus compromising kinematic feasibility. In comparison, the proposed CBF-WBC\ncontroller can reason about the robot's underactuated dynamics in real-time to\nguarantee collision-free motions. The effectiveness of this approach is\nvalidated in simulation. First, a simple hand-reaching experiment shows that\nthe CBF-WBC enables the robot's hand to deviate from an infeasible reference\ntrajectory to avoid self-collisions. Second, the CBF-WBC is combined with a\nlinear model predictive controller (LMPC) designed for dynamic locomotion, and\nthe CBF-WBC is used to track the LMPC predictions. Walking experiments show\nthat adding CBFs avoids leg self-collisions when the footstep location or swing\ntrajectory provided by the high-level planner are infeasible for the real\nrobot, and generates feasible arm motions that improve disturbance recovery.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.00911,regular,pre_llm,2022,7,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Learning Switching Criteria for Sim2Real Transfer of Robotic Fabric\n  Manipulation Policies\n\n  Simulation-to-reality transfer has emerged as a popular and highly successful\nmethod to train robotic control policies for a wide variety of tasks. However,\nit is often challenging to determine when policies trained in simulation are\nready to be transferred to the physical world. Deploying policies that have\nbeen trained with very little simulation data can result in unreliable and\ndangerous behaviors on physical hardware. On the other hand, excessive training\nin simulation can cause policies to overfit to the visual appearance and\ndynamics of the simulator. In this work, we study strategies to automatically\ndetermine when policies trained in simulation can be reliably transferred to a\nphysical robot. We specifically study these ideas in the context of robotic\nfabric manipulation, in which successful sim2real transfer is especially\nchallenging due to the difficulties of precisely modeling the dynamics and\nvisual appearance of fabric. Results in a fabric smoothing task suggest that\nour switching criteria correlate well with performance in real. In particular,\nour confidence-based switching criteria achieve average final fabric coverage\nof 87.2-93.7% within 55-60% of the total training budget. See\nhttps://tinyurl.com/lsc-case for code and supplemental materials.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.08312,regular,pre_llm,2022,7,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'A Fast, Autonomous, Bipedal Walking Behavior over Rapid Regions\n\n  In trying to build humanoid robots that perform useful tasks in a world built\nfor humans, we address the problem of autonomous locomotion. Humanoid robot\nplanning and control algorithms for walking over rough terrain are becoming\nincreasingly capable. At the same time, commercially available depth cameras\nhave been getting more accurate and GPU computing has become a primary tool in\nAI research. In this paper, we present a newly constructed behavior control\nsystem for achieving fast, autonomous, bipedal walking, without pauses or\ndeliberation. We achieve this using a recently published rapid planar regions\nperception algorithm, a height map based body path planner, an A* footstep\nplanner, and a momentum-based walking controller. We put these elements\ntogether to form a behavior control system supported by modern software\ndevelopment practices and simulation tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.10929,regular,pre_llm,2022,7,"{'ai_likelihood': 6.457169850667318e-06, 'text': ""Static Hovering Realization for Multirotor Aerial Vehicles with Tiltable\n  Propellers\n\n  This paper presents a theoretical study on the ability of multi-rotor aerial\nvehicles (MRAVs) with tiltable propellers to achieve and sustain static\nhovering at different orientations. To analyze the ability of MRAVs with\ntiltable propellers to achieve static hovering, a novel linear map between the\nplatform's control inputs and applied forces and moments is introduced. The\nrelation between the introduced map and the platform's ability to hover at\ndifferent orientations is developed. Correspondingly, the conditions for MRAVs\nwith tiltable propellers to realize and sustain static hovering are detailed. A\nnumerical metric is then introduced, which reflects the ability of MRAVs to\nsustain static hovering at different orientations. A subclass of MRAVs with\ntiltable propellers is defined as the Critically Statically Hoverable platforms\n(CSH), where CSH platforms are MRAVs that cannot sustain static hovering with\nfixed propellers, but can achieve static hovering with tilting propellers.\nFinally, extensive simulations are conducted to test and validate the above\nfindings, and to demonstrate the effect of the proposed numerical metric on the\nplatform's dynamics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.03435,regular,pre_llm,2022,7,"{'ai_likelihood': 3.410710228814019e-06, 'text': ""Sociable and Ergonomic Human-Robot Collaboration through Action\n  Recognition and Augmented Hierarchical Quadratic Programming\n\n  The recognition of actions performed by humans and the anticipation of their\nintentions are important enablers to yield sociable and successful\ncollaboration in human-robot teams. Meanwhile, robots should have the capacity\nto deal with multiple objectives and constraints, arising from the\ncollaborative task or the human. In this regard, we propose vision techniques\nto perform human action recognition and image classification, which are\nintegrated into an Augmented Hierarchical Quadratic Programming (AHQP) scheme\nto hierarchically optimize the robot's reactive behavior and human ergonomics.\nThe proposed framework allows one to intuitively command the robot in space\nwhile a task is being executed. The experiments confirm increased human\nergonomics and usability, which are fundamental parameters for reducing\nmusculoskeletal diseases and increasing trust in automation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.02735,regular,pre_llm,2022,7,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Polynomial Time Near-Time-Optimal Multi-Robot Path Planning in Three\n  Dimensions with Applications to Large-Scale UAV Coordination\n\n  For enabling efficient, large-scale coordination of unmanned aerial vehicles\n(UAVs) under the labeled setting, in this work, we develop the first polynomial\ntime algorithm for the reconfiguration of many moving bodies in\nthree-dimensional spaces, with provable $1.x$ asymptotic makespan optimality\nguarantee under high robot density. More precisely, on an $m_1\\times m_2 \\times\nm_3$ grid, $m_1\\ge m_2\\ge m_3$, our method computes solutions for routing up to\n$\\frac{m_1m_2m_3}{3}$ uniquely labeled robots with uniformly randomly\ndistributed start and goal configurations within a makespan of $m_1 + 2m_2\n+2m_3+o(m_1)$, with high probability. Because the makespan lower bound for such\ninstances is $m_1 + m_2+m_3 - o(m_1)$, also with high probability, as $m_1 \\to\n\\infty$, $\\frac{m_1+2m_2+2m_3}{m_1+m_2+m_3}$ optimality guarantee is achieved.\n$\\frac{m_1+2m_2+2m_3}{m_1+m_2+m_3} \\in (1, \\frac{5}{3}]$, yielding $1.x$\noptimality. In contrast, it is well-known that multi-robot path planning is\nNP-hard to optimally solve. In numerical evaluations, our method readily scales\nto support the motion planning of over $100,000$ robots in 3D while\nsimultaneously achieving $1.x$ optimality. We demonstrate the application of\nour method in coordinating many quadcopters in both simulation and hardware\nexperiments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2207.13647,regular,pre_llm,2022,7,"{'ai_likelihood': 1.655684577094184e-06, 'text': ""NAUTS: Negotiation for Adaptation to Unstructured Terrain Surfaces\n\n  When robots operate in real-world off-road environments with unstructured\nterrains, the ability to adapt their navigational policy is critical for\neffective and safe navigation. However, off-road terrains introduce several\nchallenges to robot navigation, including dynamic obstacles and terrain\nuncertainty, leading to inefficient traversal or navigation failures. To\naddress these challenges, we introduce a novel approach for adaptation by\nnegotiation that enables a ground robot to adjust its navigational behaviors\nthrough a negotiation process. Our approach first learns prediction models for\nvarious navigational policies to function as a terrain-aware joint local\ncontroller and planner. Then, through a new negotiation process, our approach\nlearns from various policies' interactions with the environment to agree on the\noptimal combination of policies in an online fashion to adapt robot navigation\nto unstructured off-road terrains on the fly. Additionally, we implement a new\noptimization algorithm that offers the optimal solution for robot negotiation\nin real-time during execution. Experimental results have validated that our\nmethod for adaptation by negotiation outperforms previous methods for robot\nnavigation, especially over unseen and uncertain dynamic terrains.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.00731,regular,pre_llm,2022,8,"{'ai_likelihood': 5.927350785997179e-06, 'text': ""Planar Modeling and Sim-to-Real of a Tethered Multimaterial Soft Swimmer\n  Driven by Peano-HASELs\n\n  Soft robotics has the potential to revolutionize robotic locomotion, in\nparticular, soft robotic swimmers offer a minimally invasive and adaptive\nsolution to explore and preserve our oceans. Unfortunately, current soft\nrobotic swimmers are vastly inferior to evolved biological swimmers, especially\nin terms of controllability, efficiency, maneuverability, and longevity.\nAdditionally, the tedious iterative fabrication and empirical testing required\nto design soft robots has hindered their optimization. In this work, we tackle\nthis challenge by providing an efficient and straightforward pipeline for\ndesigning and fabricating soft robotic swimmers equipped with electrostatic\nactuation. We streamline the process to allow for rapid additive manufacturing,\nand show how a differentiable simulation can be used to match a simplified\nmodel to the real deformation of a robotic swimmer. We perform several\nexperiments with the fabricated swimmer by varying the voltage and actuation\nfrequency of the swimmer's antagonistic muscles. We show how the voltage and\nfrequency vary the locomotion speed of the swimmer while moving in liquid oil\nand observe a clear optimum in forward swimming speed. The differentiable\nsimulation model we propose has various downstream applications, such as\ncontrol and shape optimization of the swimmer; optimization results can be\ndirectly mapped back to the real robot through our sim-to-real matching.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.09861,regular,pre_llm,2022,8,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'The Single Robot Line Coverage Problem: Theory, Algorithms, and\n  Experiments\n\n  Line coverage is the task of servicing a given set of one-dimensional\nfeatures in an environment. It is important for the inspection of linear\ninfrastructure such as road networks, power lines, and oil and gas pipelines.\nThis paper addresses the single robot line coverage problem for aerial and\nground robots by modeling it as an optimization problem on a graph. The problem\nbelongs to the broad class of arc routing problems and is closely related to\nthe rural postman problem (RPP) on asymmetric graphs. The paper presents an\ninteger linear programming formulation with proofs of correctness. Using the\nminimum cost flow problem, we develop approximation algorithms with guarantees\non the solution quality. These guarantees also improve the existing results for\nthe asymmetric RPP. The main algorithm partitions the problem into three cases\nbased on the structure of the required graph, i.e., the graph induced by the\nfeatures that require servicing. We evaluate our algorithms on road networks\nfrom the 50 most populous cities in the world, consisting of up to 730 road\nsegments. The algorithms, augmented with improvement heuristics, run within 3s\nand generate solutions that are within 10% of the optimum. We experimentally\ndemonstrate our algorithms with commercial UAVs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.02772,regular,pre_llm,2022,8,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Decentralized Risk-Aware Tracking of Multiple Targets\n\n  We consider the setting where a team of robots is tasked with tracking\nmultiple targets with the following property: approaching the targets enables\nmore accurate target position estimation, but also increases the risk of sensor\nfailures. Therefore, it is essential to address the trade-off between tracking\nquality maximization and risk minimization. In our previous work, a centralized\ncontroller is developed to plan motions for all the robots -- however, this is\nnot a scalable approach. Here, we present a decentralized and risk-aware\nmulti-target tracking framework, in which each robot plans its motion trading\noff tracking accuracy maximization and aversion to risk, while only relying on\nits own information and information exchanged with its neighbors. We use the\ncontrol barrier function to guarantee network connectivity throughout the\ntracking process. Extensive numerical experiments demonstrate that our system\ncan achieve similar tracking accuracy and risk-awareness to its centralized\ncounterpart.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.10834,regular,pre_llm,2022,8,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Real-Time Sonar Fusion for Layered Navigation Controller\n\n  Navigation in varied and dynamic indoor environments remains a complex task\nfor autonomous mobile platforms. Especially when conditions worsen, typical\nsensor modalities may fail to operate optimally and subsequently provide inapt\ninput for safe navigation control. In this study, we present an approach for\nthe navigation of a dynamic indoor environment with a mobile platform with a\nsingle or several sonar sensors using a layered control system. These sensors\ncan operate in conditions such as rain, fog, dust, or dirt. The different\ncontrol layers, such as collision avoidance and corridor following behavior,\nare activated based on acoustic flow queues in the fusion of the sonar images.\nThe novelty of this work is allowing these sensors to be freely positioned on\nthe mobile platform and providing the framework for designing the optimal\nnavigational outcome based on a zoning system around the mobile platform.\nPresented in this paper is the acoustic flow model used, as well as the design\nof the layered controller. Next to validation in simulation, an implementation\nis presented and validated in a real office environment using a real mobile\nplatform with one, two, or three sonar sensors in real time with 2D navigation.\nMultiple sensor layouts were validated in both the simulation and real\nexperiments to demonstrate that the modular approach for the controller and\nsensor fusion works optimally. The results of this work show stable and safe\nnavigation of indoor environments with dynamic objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.02312,regular,pre_llm,2022,8,"{'ai_likelihood': 7.450580596923828e-06, 'text': 'Rearrangement-Based Manipulation via Kinodynamic Planning and Dynamic\n  Planning Horizons\n\n  Robot manipulation in cluttered environments often requires complex and\nsequential rearrangement of multiple objects in order to achieve the desired\nreconfiguration of the target objects. Due to the sophisticated physical\ninteractions involved in such scenarios, rearrangement-based manipulation is\nstill limited to a small range of tasks and is especially vulnerable to\nphysical uncertainties and perception noise. This paper presents a planning\nframework that leverages the efficiency of sampling-based planning approaches,\nand closes the manipulation loop by dynamically controlling the planning\nhorizon. Our approach interleaves planning and execution to progressively\napproach the manipulation goal while correcting any errors or path deviations\nalong the process. Meanwhile, our framework allows the definition of\nmanipulation goals without requiring explicit goal configurations, enabling the\nrobot to flexibly interact with all objects to facilitate the manipulation of\nthe target ones. With extensive experiments both in simulation and on a real\nrobot, we evaluate our framework on three manipulation tasks in cluttered\nenvironments: grasping, relocating, and sorting. In comparison with two\nbaseline approaches, we show that our framework can significantly improve\nplanning efficiency, robustness against physical uncertainties, and task\nsuccess rate under limited time budgets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.12997,regular,pre_llm,2022,8,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Learning to SLAM on the Fly in Unknown Environments: A Continual\n  Learning Approach for Drones in Visually Ambiguous Scenes\n\n  Learning to safely navigate in unknown environments is an important task for\nautonomous drones used in surveillance and rescue operations. In recent years,\na number of learning-based Simultaneous Localisation and Mapping (SLAM) systems\nrelying on deep neural networks (DNNs) have been proposed for applications\nwhere conventional feature descriptors do not perform well. However, such\nlearning-based SLAM systems rely on DNN feature encoders trained offline in\ntypical deep learning settings. This makes them less suited for drones deployed\nin environments unseen during training, where continual adaptation is\nparamount. In this paper, we present a new method for learning to SLAM on the\nfly in unknown environments, by modulating a low-complexity Dictionary Learning\nand Sparse Coding (DLSC) pipeline with a newly proposed Quadratic Bayesian\nSurprise (QBS) factor. We experimentally validate our approach with data\ncollected by a drone in a challenging warehouse scenario, where the high number\nof ambiguous scenes makes visual disambiguation hard.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.1225,regular,pre_llm,2022,8,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Grasp'D: Differentiable Contact-rich Grasp Synthesis for Multi-fingered\n  Hands\n\n  The study of hand-object interaction requires generating viable grasp poses\nfor high-dimensional multi-finger models, often relying on analytic grasp\nsynthesis which tends to produce brittle and unnatural results. This paper\npresents Grasp'D, an approach for grasp synthesis with a differentiable contact\nsimulation from both known models as well as visual inputs. We use\ngradient-based methods as an alternative to sampling-based grasp synthesis,\nwhich fails without simplifying assumptions, such as pre-specified contact\nlocations and eigengrasps. Such assumptions limit grasp discovery and, in\nparticular, exclude high-contact power grasps. In contrast, our\nsimulation-based approach allows for stable, efficient, physically realistic,\nhigh-contact grasp synthesis, even for gripper morphologies with high-degrees\nof freedom. We identify and address challenges in making grasp simulation\namenable to gradient-based optimization, such as non-smooth object surface\ngeometry, contact sparsity, and a rugged optimization landscape. Grasp'D\ncompares favorably to analytic grasp synthesis on human and robotic hand\nmodels, and resultant grasps achieve over 4x denser contact, leading to\nsignificantly higher grasp stability. Video and code available at\nhttps://graspd-eccv22.github.io/.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.11865,regular,pre_llm,2022,8,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'FusionPortable: A Multi-Sensor Campus-Scene Dataset for Evaluation of\n  Localization and Mapping Accuracy on Diverse Platforms\n\n  Combining multiple sensors enables a robot to maximize its perceptual\nawareness of environments and enhance its robustness to external disturbance,\ncrucial to robotic navigation. This paper proposes the FusionPortable\nbenchmark, a complete multi-sensor dataset with a diverse set of sequences for\nmobile robots. This paper presents three contributions. We first advance a\nportable and versatile multi-sensor suite that offers rich sensory\nmeasurements: 10Hz LiDAR point clouds, 20Hz stereo frame images, high-rate and\nasynchronous events from stereo event cameras, 200Hz inertial readings from an\nIMU, and 10Hz GPS signal. Sensors are already temporally synchronized in\nhardware. This device is lightweight, self-contained, and has plug-and-play\nsupport for mobile robots. Second, we construct a dataset by collecting 17\nsequences that cover a variety of environments on the campus by exploiting\nmultiple robot platforms for data collection. Some sequences are challenging to\nexisting SLAM algorithms. Third, we provide ground truth for the decouple\nlocalization and mapping performance evaluation. We additionally evaluate\nstate-of-the-art SLAM approaches and identify their limitations. The dataset,\nconsisting of raw sensor easurements, ground truth, calibration data, and\nevaluated algorithms, will be released:\nhttps://ram-lab.com/file/site/multi-sensor-dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.05095,regular,pre_llm,2022,8,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Robotics in Snow and Ice\n\n  Definition: The terms ""robotics in snow and ice"" refers to robotic systems\nbeing studied, developed, and used in areas where water can be found in its\nsolid state. This specialized branch of field robotics investigates the impact\nof extreme conditions related to cold environments on autonomous vehicles.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.1316,regular,pre_llm,2022,8,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'An Efficient Spatial-Temporal Trajectory Planner for Autonomous Vehicles\n  in Unstructured Environments\n\n  As a core part of autonomous driving systems, motion planning has received\nextensive attention from academia and industry. However, real-time trajectory\nplanning capable of spatial-temporal joint optimization is challenged by\nnonholonomic dynamics, particularly in the presence of unstructured\nenvironments and dynamic obstacles. To bridge the gap, we propose a real-time\ntrajectory optimization method that can generate a high-quality whole-body\ntrajectory under arbitrary environmental constraints. By leveraging the\ndifferential flatness property of car-like robots, we simplify the trajectory\nrepresentation and analytically formulate the planning problem while\nmaintaining the feasibility of the nonholonomic dynamics. Moreover, we achieve\nefficient obstacle avoidance with a safe driving corridor for unmodelled\nobstacles and signed distance approximations for dynamic moving objects. We\npresent comprehensive benchmarks with State-of-the-Art methods, demonstrating\nthe significance of the proposed method in terms of efficiency and trajectory\nquality. Real-world experiments verify the practicality of our algorithm. We\nwill release our codes for the research community\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.10299,regular,pre_llm,2022,8,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Passive and Active Acoustic Sensing for Soft Pneumatic Actuators\n\n  We propose a sensorization method for soft pneumatic actuators that uses an\nembedded microphone and speaker to measure different actuator properties. The\nphysical state of the actuator determines the specific modulation of sound as\nit travels through the structure. Using simple machine learning, we create a\ncomputational sensor that infers the corresponding state from sound recordings.\nWe demonstrate the acoustic sensor on a soft pneumatic continuum actuator and\nuse it to measure contact locations, contact forces, object materials, actuator\ninflation, and actuator temperature. We show that the sensor is reliable\n(average classification rate for six contact locations of 93%), precise (mean\nspatial accuracy of 3.7 mm), and robust against common disturbances like\nbackground noise. Finally, we compare different sounds and learning methods and\nachieve best results with 20 ms of white noise and a support vector classifier\nas the sensor model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.01372,regular,pre_llm,2022,8,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'A Riemannian Take on Human Motion Analysis and Retargeting\n\n  Dynamic motions of humans and robots are widely driven by posture-dependent\nnonlinear interactions between their degrees of freedom. However, these\ndynamical effects remain mostly overlooked when studying the mechanisms of\nhuman movement generation. Inspired by recent works, we hypothesize that human\nmotions are planned as sequences of geodesic synergies, and thus correspond to\ncoordinated joint movements achieved with piecewise minimum energy. The\nunderlying computational model is built on Riemannian geometry to account for\nthe inertial characteristics of the body. Through the analysis of various human\narm motions, we find that our model segments motions into geodesic synergies,\nand successfully predicts observed arm postures, hand trajectories, as well as\ntheir respective velocity profiles. Moreover, we show that our analysis can\nfurther be exploited to transfer arm motions to robots by reproducing\nindividual human synergies as geodesic paths in the robot configuration space.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.01273,regular,pre_llm,2022,8,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Industry 4.0 Asset Administration Shell (AAS): Interoperable Skill-Based\n  Service-Robots\n\n  This paper describes our use of Industry 4.0 Asset Administration Shells\n(AASs) in the context of service robots. We use AASs with software components\nof service robots and with complete service robot systems. The AAS for a\nsoftware component serves as a standardized digital data sheet. It helps sysem\nbuilders at design time in finding and selecting software components that match\nsystem-level requirements of the systems to be built. The AAS for a system\ncomprises a data sheet for the system and furthermore collects at runtime\noperational data and it allows for skill-level commanding of the service robot.\nAASs are generated and filled as part of our model-driven development and\ncomposition workflow for service robotics. AASs can serve as a key enabler for\na standardized integration and interaction with service robots.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.0902,regular,pre_llm,2022,8,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Challenges in Applying Robotics to Retail Store Management\n\n  An autonomous retail store management system entails inventory tracking,\nstore monitoring, and anomaly correction. Recent attempts at autonomous retail\nstore management have faced challenges primarily in perception for anomaly\ndetection, as well as new challenges arising in mobile manipulation for\nexecuting anomaly correction. Advances in each of these areas along with system\nintegration are necessary for a scalable solution in this domain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.09743,regular,pre_llm,2022,8,"{'ai_likelihood': 5.298190646701389e-06, 'text': ""Where Shall I Touch? Vision-Guided Tactile Poking for Transparent Object\n  Grasping\n\n  Picking up transparent objects is still a challenging task for robots. The\nvisual properties of transparent objects such as reflection and refraction make\nthe current grasping methods that rely on camera sensing fail to detect and\nlocalise them. However, humans can handle the transparent object well by first\nobserving its coarse profile and then poking an area of interest to get a fine\nprofile for grasping. Inspired by this, we propose a novel framework of\nvision-guided tactile poking for transparent objects grasping. In the proposed\nframework, a segmentation network is first used to predict the horizontal upper\nregions named as poking regions, where the robot can poke the object to obtain\na good tactile reading while leading to minimal disturbance to the object's\nstate. A poke is then performed with a high-resolution GelSight tactile sensor.\nGiven the local profiles improved with the tactile reading, a heuristic grasp\nis planned for grasping the transparent object. To mitigate the limitations of\nreal-world data collection and labelling for transparent objects, a large-scale\nrealistic synthetic dataset was constructed. Extensive experiments demonstrate\nthat our proposed segmentation network can predict the potential poking region\nwith a high mean Average Precision (mAP) of 0.360, and the vision-guided\ntactile poking can enhance the grasping success rate significantly from 38.9%\nto 85.2%. Thanks to its simplicity, our proposed approach could also be adopted\nby other force or tactile sensors and could be used for grasping of other\nchallenging objects. All the materials used in this paper are available at\nhttps://sites.google.com/view/tactilepoking.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.06331,regular,pre_llm,2022,8,"{'ai_likelihood': 1.3377931382921008e-05, 'text': 'A Linear and Exact Algorithm for Whole-Body Collision Evaluation via\n  Scale Optimization\n\n  Collision evaluation is of vital importance in various applications. However,\nexisting methods are either cumbersome to calculate or have a gap with the\nactual value. In this paper, we propose a zero-gap whole-body collision\nevaluation which can be formulated as a low dimensional linear program. This\nevaluation can be solved analytically in O(m) computational time, where m is\nthe total number of the linear inequalities in this linear program. Moreover,\nthe proposed method is efficient in obtaining its gradient, making it easy to\napply to optimization-based applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.0482,regular,pre_llm,2022,8,"{'ai_likelihood': 3.013345930311415e-06, 'text': 'Rapid Development of a Mobile Robot Simulation Environment\n\n  Robotics simulation provides many advantages during the development of an\nintelligent ground vehicle (IGV) such as testing the software components in\nvarying scenarios without requiring a complete physical robot. This paper\ndiscusses a 3D simulation environment created using rapid application\ndevelopment and the Unity game engine to enable testing during a mobile\nrobotics competition. Our experience shows that the simulation environment\ncontributed greatly to the development of software for the competition. The\nsimulator also contributed to the hardware development of the robot.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.13695,regular,pre_llm,2022,8,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'A Data-Centric Approach For Dual-Arm Robotic Garment Flattening\n\n  Due to the high dimensionality of object states, a garment flattening\npipeline requires recognising the configurations of garments for a robot to\nproduce/select manipulation plans to flatten garments. In this paper, we\npropose a data-centric approach to identify known configurations of garments\nbased on a known configuration network (KCNet) trained on depth images that\ncapture the known configurations of garments and prior knowledge of garment\nshapes. In this paper, we propose a data-centric approach to identify the known\nconfigurations of garments based on a known configuration network (KCNet)\ntrained on the depth images that capture the known configurations of garments\nand prior knowledge of garment shapes. The known configurations of garments are\nthe configurations of garments when a robot hangs garments in the middle of the\nair. We found that it is possible to achieve 92\\% accuracy if we let the robot\nrecognise the common hanging configurations (the known configurations) of\ngarments. We also demonstrate an effective robot garment flattening pipeline\nwith our proposed approach on a dual-arm Baxter robot. The robot achieved an\naverage operating time of 221.6 seconds and successfully manipulated garments\nof five different shapes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.09355,regular,pre_llm,2022,8,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'ArUco Maker based localization and Node graph approach to mapping\n\n  This paper explores a method of localization and navigation of indoor mobile\nrobots using a node graph of landmarks that are based on fiducial markers. The\nuse of ArUco markers and their 2D orientation with respect to the camera of the\nrobot and the distance to the markers from the camera is used to calculate the\nrelative position of the robot as well as the relative positions of other\nmarkers. The proposed method combines aspects of beacon-based navigation and\nSimultaneous Localization and Mapping based navigation. The implementation of\nthis method uses a depth camera to obtain the distance to the marker. After\ncalculating the required orientation of the marker, it relies on odometry\ncalculations for tracking the position after localization with respect to the\nmarker. Using the odometry and the relative position of one marker, the robot\nis then localized with respect to another marker. The relative positions and\norientation of the two markers are then calculated. The markers are represented\nas nodes and the relative distances and orientations are represented as edges\nconnecting the nodes and a node graph can be generated that represents a map\nfor the robot. The method was tested on a wheeled humanoid robot with the\nobjective of having it autonomously navigate to a charging station inside a\nroom. This objective was successfully achieved and the limitations and future\nimprovements are briefly discussed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2208.11538,regular,pre_llm,2022,8,"{'ai_likelihood': 1.1854701571994357e-05, 'text': 'Visual Servoing in Orchard Settings\n\n  We present a general framework for accurate positioning of sensors and end\neffectors in farm settings using a camera mounted on a robotic manipulator. Our\nmain contribution is a visual servoing approach based on a new and robust\nfeature tracking algorithm. Results from field experiments performed at an\napple orchard demonstrate that our approach converges to a given termination\ncriterion even under environmental influences such as strong winds, varying\nillumination conditions and partial occlusion of the target object. Further, we\nshow experimentally that the system converges to the desired view for a wide\nrange of initial conditions. This approach opens possibilities for new\napplications such as automated fruit inspection, fruit picking or precise\npesticide application.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.09294,regular,pre_llm,2022,9,"{'ai_likelihood': 1.457002427842882e-06, 'text': ""Collaborative Human-Robot Exploration via Implicit Coordination\n\n  This paper develops a methodology for collaborative human-robot exploration\nthat leverages implicit coordination. Most autonomous single- and multi-robot\nexploration systems require a remote operator to provide explicit guidance to\nthe robotic team. Few works consider how to embed the human partner alongside\nrobots to provide guidance in the field. A remaining challenge for\ncollaborative human-robot exploration is efficient communication of goals from\nthe human to the robot. In this paper we develop a methodology that implicitly\ncommunicates a region of interest from a helmet-mounted depth camera on the\nhuman's head to the robot and an information gain-based exploration objective\nthat biases motion planning within the viewpoint provided by the human. The\nresult is an aerial system that safely accesses regions of interest that may\nnot be immediately viewable or reachable by the human. The approach is\nevaluated in simulation and with hardware experiments in a motion capture\narena. Videos of the simulation and hardware experiments are available at:\nhttps://youtu.be/7jgkBpVFIoE.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.1071,regular,pre_llm,2022,9,"{'ai_likelihood': 9.139378865559896e-06, 'text': 'Visual Localization and Mapping in Dynamic and Changing Environments\n\n  The real-world deployment of fully autonomous mobile robots depends on a\nrobust SLAM (Simultaneous Localization and Mapping) system, capable of handling\ndynamic environments, where objects are moving in front of the robot, and\nchanging environments, where objects are moved or replaced after the robot has\nalready mapped the scene. This paper presents Changing-SLAM, a method for\nrobust Visual SLAM in both dynamic and changing environments. This is achieved\nby using a Bayesian filter combined with a long-term data association\nalgorithm. Also, it employs an efficient algorithm for dynamic keypoints\nfiltering based on object detection that correctly identify features inside the\nbounding box that are not dynamic, preventing a depletion of features that\ncould cause lost tracks. Furthermore, a new dataset was developed with RGB-D\ndata especially designed for the evaluation of changing environments on an\nobject level, called PUC-USP dataset. Six sequences were created using a mobile\nrobot, an RGB-D camera and a motion capture system. The sequences were designed\nto capture different scenarios that could lead to a tracking failure or a map\ncorruption. To the best of our knowledge, Changing-SLAM is the first Visual\nSLAM system that is robust to both dynamic and changing environments, not\nassuming a given camera pose or a known map, being also able to operate in real\ntime. The proposed method was evaluated using benchmark datasets and compared\nwith other state-of-the-art methods, proving to be highly accurate.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.13041,regular,pre_llm,2022,9,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Efficient Concurrent Design of the Morphology of Unmanned Aerial Systems\n  and their Collective-Search Behavior\n\n  The collective operation of robots, such as unmanned aerial vehicles (UAVs)\noperating as a team or swarm, is affected by their individual capabilities,\nwhich in turn is dependent on their physical design, aka morphology. However,\nwith the exception of a few (albeit ad hoc) evolutionary robotics methods,\nthere has been very little work on understanding the interplay of morphology\nand collective behavior. There is especially a lack of computational frameworks\nto concurrently search for the robot morphology and the hyper-parameters of\ntheir behavior model that jointly optimize the collective (team) performance.\nTo address this gap, this paper proposes a new co-design framework. Here the\nexploding computational cost of an otherwise nested morphology/behavior\nco-design is effectively alleviated through the novel concept of ``talent""\nmetrics; while also allowing significantly better solutions compared to the\ntypically sub-optimal sequential morphology$\\to$behavior design approach. This\nframework comprises four major steps: talent metrics selection, talent Pareto\nexploration (a multi-objective morphology optimization process), behavior\noptimization, and morphology finalization. This co-design concept is\ndemonstrated by applying it to design UAVs that operate as a team to localize\nsignal sources, e.g., in victim search and hazard localization. Here, the\ncollective behavior is driven by a recently reported batch Bayesian search\nalgorithm called Bayes-Swarm. Our case studies show that the outcome of\nco-design provides significantly higher success rates in signal source\nlocalization compared to a baseline design, across a variety of signal\nenvironments and teams with 6 to 15 UAVs. Moreover, this co-design process\nprovides two orders of magnitude reduction in computing time compared to a\nprojected nested design approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.13137,regular,pre_llm,2022,9,"{'ai_likelihood': 2.4437904357910156e-05, 'text': ""Using Unmanned Aerial Systems (UAS) for Assessing and Monitoring Fall\n  Hazard Prevention Systems in High-rise Building Projects\n\n  This study develops a framework for unmanned aerial systems (UASs) to monitor\nfall hazard prevention systems near unprotected edges and openings in high-rise\nbuilding projects. A three-step machine-learning-based framework was developed\nand tested to detect guardrail posts from the images captured by UAS. First, a\nguardrail detector was trained to localize the candidate locations of posts\nsupporting the guardrail. Since images were used in this process collected from\nan actual job site, several false detections were identified. Therefore,\nadditional constraints were introduced in the following steps to filter out\nfalse detections. Second, the research team applied a horizontal line detector\nto the image to properly detect floors and remove the detections that were not\nclose to the floors. Finally, since the guardrail posts are installed with\napproximately normal distribution between each post, the space between them was\nestimated and used to find the most likely distance between the two posts. The\nresearch team used various combinations of the developed approaches to monitor\nguardrail systems in the captured images from a high-rise building project.\nComparing the precision and recall metrics indicated that the cascade\nclassifier achieves better performance with floor detection and guardrail\nspacing estimation. The research outcomes illustrate that the proposed\nguardrail recognition system can improve the assessment of guardrails and\nfacilitate the safety engineer's task of identifying fall hazards in high-rise\nbuilding projects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.07515,regular,pre_llm,2022,9,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'A Novel Design and Improvement of 15-Bar Assembly Tensegrity Robotics\n  Structure\n\n  While the ultimate goal is to produce a tensegrity more than 6 struts, e.g. a\n15-bar tensegrity, past experience has demonstrated that we must first develop\nan innovative system that will facilitate the assembly of a general n-bar\ntensegrity. To be successful, we believe the development of the new assembly\nmethodology must encompass not only the design of the clamping system but also\nthe design of the tensegrity itself, including the struts, the springs and the\nspring-to-strut connectors. We therefore propose to develop the 15-bar in two\nphases: Phase I will be the development of an innovative assembly method, and\nPhase II will focus on the design and manufacture of a 15-bar tensegrity, with\na new strut design probably being part of this. Longer term goals will be aimed\nat repackaging the wireless electronics on the new struts and adding encoders\nto control the phase of the motors shafts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.09171,regular,pre_llm,2022,9,"{'ai_likelihood': 3.7087334526909726e-06, 'text': 'HyperDog: An Open-Source Quadruped Robot Platform Based on ROS2 and\n  micro-ROS\n\n  Nowadays, design and development of legged quadruped robots is a quite active\narea of scientific research. In fact, the legged robots have become popular due\nto their capabilities to adapt to harsh terrains and diverse environmental\nconditions in comparison to other mobile robots. With the higher demand for\nlegged robot experiments, more researches and engineers need an affordable and\nquick way of locomotion algorithm development. In this paper, we present a new\nopen source quadruped robot HyperDog platform, which features 12 RC servo\nmotors, onboard NVIDIA Jetson nano computer and STM32F4 Discovery board.\nHyperDog is an open-source platform for quadruped robotic software development,\nwhich is based on Robot Operating System 2 (ROS2) and micro-ROS. Moreover, the\nHyperDog is a quadrupedal robotic dog entirely built from 3D printed parts and\ncarbon fiber, which allows the robot to have light weight and good strength.\nThe idea of this work is to demonstrate an affordable and customizable way of\nrobot development and provide researches and engineers with the legged robot\nplatform, where different algorithms can be tested and validated in simulation\nand real environment. The developed project with code is available on GitHub\n(https://github.com/NDHANA94/hyperdog_ros2).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.14009,regular,pre_llm,2022,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Carrying the uncarriable: a deformation-agnostic and human-cooperative\n  framework for unwieldy objects using multiple robots\n\n  This manuscript introduces an object deformability-agnostic framework for\nco-carrying tasks that are shared between a person and multiple robots. Our\napproach allows the full control of the co-carrying trajectories by the person\nwhile sharing the load with multiple robots depending on the size and the\nweight of the object. This is achieved by merging the haptic information\ntransferred through the object and the human motion information obtained from a\nmotion capture system. One important advantage of the framework is that no\nstrict internal communication is required between the robots, regardless of the\nobject size and deformation characteristics. We validate the framework with two\nchallenging real-world scenarios: co-transportation of a wooden rigid closet\nand a bulky box on top of forklift moving straps, with the latter\ncharacterizing deformable objects. In order to evaluate the generalizability of\nthe proposed framework, a heterogenous team of two mobile manipulators that\nconsist of an Omni-directional mobile base and a collaborative robotic arm with\ndifferent DoFs is chosen for the experiments. The qualitative comparison\nbetween our controller and the baseline controller (i.e., an admittance\ncontroller) during these experiments demonstrated the effectiveness of the\nproposed framework especially when co-carrying deformable objects. Furthermore,\nwe believe that the performance of our framework during the experiment with the\nlifting straps offers a promising solution for the co-transportation of bulky\nand ungraspable objects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.12365,regular,pre_llm,2022,9,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""Deep Convolutional Neural Network and Transfer Learning for Locomotion\n  Intent Prediction\n\n  Powered prosthetic legs must anticipate the user's intent when switching\nbetween different locomotion modes (e.g., level walking, stair ascent/descent,\nramp ascent/descent). Numerous data-driven classification techniques have\ndemonstrated promising results for predicting user intent, but the performance\nof these intent prediction models on novel subjects remains undesirable. In\nother domains (e.g., image classification), transfer learning has improved\nclassification accuracy by using previously learned features from a large\ndataset (i.e., pre-trained models) and then transferring this learned model to\na new task where a smaller dataset is available. In this paper, we develop a\ndeep convolutional neural network with intra-subject (subject-dependent) and\ninter-subject (subject-independent) validations based on a human locomotion\ndataset. We then apply transfer learning for the subject-independent model\nusing a small portion (10%) of the data from the left-out subject. We compare\nthe performance of these three models. Our results indicate that the transfer\nlearning (TL) model outperforms the subject-independent (IND) model and is\ncomparable to the subject-dependent (DEP) model (DEP Error: 0.74 $\\pm$ 0.002%,\nIND Error: 11.59 $\\pm$ 0.076%, TL Error: 3.57 $\\pm$ 0.02% with 10% data).\nMoreover, as expected, transfer learning accuracy increases with the\navailability of more data from the left-out subject. We also evaluate the\nperformance of the intent prediction system in various sensor configurations\nthat may be available in a prosthetic leg application. Our results suggest that\na thigh IMU on the the prosthesis is sufficient to predict locomotion intent in\npractice.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.06545,regular,pre_llm,2022,9,"{'ai_likelihood': 1.0364585452609592e-05, 'text': ""Tac2Structure: Object Surface Reconstruction Only through Multi Times\n  Touch\n\n  Inspired by humans' ability to perceive the surface texture of unfamiliar\nobjects without relying on vision, the sense of touch can play a crucial role\nin robots exploring the environment, particularly in scenes where vision is\ndifficult to apply, or occlusion is inevitable. Existing tactile surface\nreconstruction methods rely on external sensors or have strong prior\nassumptions, making the operation complex and limiting their application\nscenarios. This paper presents a framework for low-drift surface reconstruction\nthrough multiple tactile measurements, Tac2Structure. Compared with existing\nalgorithms, the proposed method uses only a new vision-based tactile sensor\nwithout relying on external devices. Aiming at the difficulty that\nreconstruction accuracy is easily affected by the pressure at contact, we\npropose a correction algorithm to adapt it. The proposed method also reduces\nthe accumulative errors that occur easily during global object surface\nreconstruction. Multi-frame tactile measurements can accurately reconstruct\nobject surfaces by jointly using the point cloud registration algorithm,\nloop-closure detection algorithm based on deep learning, and pose graph\noptimization algorithm. Experiments verify that Tac2Structure can achieve\nmillimeter-level accuracy in reconstructing the surface of objects, providing\naccurate tactile information for the robot to perceive the surrounding\nenvironment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.09447,regular,pre_llm,2022,9,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Decentralized Deadlock-free Trajectory Planning for Quadrotor Swarm in\n  Obstacle-rich Environments -- Extended version\n\n  This paper presents a decentralized multi-agent trajectory planning (MATP)\nalgorithm that guarantees to generate a safe, deadlock-free trajectory in an\nobstacle-rich environment under a limited communication range. The proposed\nalgorithm utilizes a grid-based multi-agent path planning (MAPP) algorithm for\ndeadlock resolution, and we introduce the subgoal optimization method to make\nthe agent converge to the waypoint generated from the MAPP without deadlock. In\naddition, the proposed algorithm ensures the feasibility of the optimization\nproblem and collision avoidance by adopting a linear safe corridor (LSC). We\nverify that the proposed algorithm does not cause a deadlock in both random\nforests and dense mazes regardless of communication range, and it outperforms\nour previous work in flight time and distance. We validate the proposed\nalgorithm through a hardware demonstration with ten quadrotors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.04805,regular,pre_llm,2022,9,"{'ai_likelihood': 1.655684577094184e-06, 'text': ""Real-Time Heuristic Framework for Safe Landing of UAVs in Dynamic\n  Scenarios\n\n  The world we live in is full of technology and with each passing day the\nadvancement and usage of UAVs increases efficiently. As a result of the many\napplication scenarios, there are some missions where the UAVs are vulnerable to\nexternal disruptions, such as a ground station's loss of connectivity, security\nmissions, safety concerns, and delivery-related missions. Therefore, depending\non the scenario, this could affect the operations and result in the safe\nlanding of UAVs. Hence, this paper presents a heuristic approach towards safe\nlanding of multi-rotor UAVs in the dynamic environments. The aim of this\napproach is to detect safe potential landing zones - PLZ, and find out the best\none to land in. The PLZ is initially, detected by processing an image through\nthe canny edge algorithm, and then the diameter-area estimation is applied for\neach region with minimal edges. The spots that have a higher area than the\nvehicle's clearance are labeled as safe PLZ. Onto the second phase of this\napproach, the velocities of dynamic obstacles that are moving towards the PLZs\nare calculated and their time to reach the zones are taken into consideration.\nThe ETA of the UAV is calculated and during the descending of UAV, the dynamic\nobstacle avoidance is executed. The approach tested on the real-world\nenvironments have shown better results from existing work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08578,regular,pre_llm,2022,9,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Data-driven Loop Closure Detection in Bathymetric Point Clouds for\n  Underwater SLAM\n\n  Simultaneous localization and mapping (SLAM) frameworks for autonomous\nnavigation rely on robust data association to identify loop closures for\nback-end trajectory optimization. In the case of autonomous underwater vehicles\n(AUVs) equipped with multibeam echosounders (MBES), data association is\nparticularly challenging due to the scarcity of identifiable landmarks in the\nseabed, the large drift in dead-reckoning navigation estimates to which AUVs\nare prone and the low resolution characteristic of MBES data. Deep learning\nsolutions to loop closure detection have shown excellent performance on data\nfrom more structured environments. However, their transfer to the seabed domain\nis not immediate and efforts to port them are hindered by the lack of\nbathymetric datasets. Thus, in this paper we propose a neural network\narchitecture aimed to showcase the potential of adapting such techniques to\ncorrespondence matching in bathymetric data. We train our framework on real\nbathymetry from an AUV mission and evaluate its performance on the tasks of\nloop closure detection and coarse point cloud alignment. Finally, we show its\npotential against a more traditional method and release both its implementation\nand the dataset used.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08602,regular,pre_llm,2022,9,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'ASAP: Adaptive Transmission Scheme for Online Processing of Event-based\n  Algorithms\n\n  Online event-based perception techniques on board robots navigating in\ncomplex, unstructured, and dynamic environments can suffer unpredictable\nchanges in the incoming event rates and their processing times, which can cause\ncomputational overflow or loss of responsiveness. This paper presents ASAP: a\nnovel event handling framework that dynamically adapts the transmission of\nevents to the processing algorithm, keeping the system responsiveness and\npreventing overflows. ASAP is composed of two adaptive mechanisms. The first\none prevents event processing overflows by discarding an adaptive percentage of\nthe incoming events. The second mechanism dynamically adapts the size of the\nevent packages to reduce the delay between event generation and processing.\nASAP has guaranteed convergence and is flexible to the processing algorithm. It\nhas been validated on board a quadrotor and an ornithopter robot in challenging\nconditions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.07753,regular,pre_llm,2022,9,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Code as Policies: Language Model Programs for Embodied Control\n\n  Large language models (LLMs) trained on code completion have been shown to be\ncapable of synthesizing simple Python programs from docstrings [1]. We find\nthat these code-writing LLMs can be re-purposed to write robot policy code,\ngiven natural language commands. Specifically, policy code can express\nfunctions or feedback loops that process perception outputs (e.g.,from object\ndetectors [2], [3]) and parameterize control primitive APIs. When provided as\ninput several example language commands (formatted as comments) followed by\ncorresponding policy code (via few-shot prompting), LLMs can take in new\ncommands and autonomously re-compose API calls to generate new policy code\nrespectively. By chaining classic logic structures and referencing third-party\nlibraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way\ncan write robot policies that (i) exhibit spatial-geometric reasoning, (ii)\ngeneralize to new instructions, and (iii) prescribe precise values (e.g.,\nvelocities) to ambiguous descriptions (""faster"") depending on context (i.e.,\nbehavioral commonsense). This paper presents code as policies: a robot-centric\nformulation of language model generated programs (LMPs) that can represent\nreactive policies (e.g., impedance controllers), as well as waypoint-based\npolicies (vision-based pick and place, trajectory-based control), demonstrated\nacross multiple real robot platforms. Central to our approach is prompting\nhierarchical code-gen (recursively defining undefined functions), which can\nwrite more complex code and also improves state-of-the-art to solve 39.8% of\nproblems on the HumanEval [1] benchmark. Code and videos are available at\nhttps://code-as-policies.github.io\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.05649,regular,pre_llm,2022,9,"{'ai_likelihood': 8.940696716308594e-07, 'text': ""Social-PatteRNN: Socially-Aware Trajectory Prediction Guided by Motion\n  Patterns\n\n  As robots across domains start collaborating with humans in shared\nenvironments, algorithms that enable them to reason over human intent are\nimportant to achieve safe interplay. In our work, we study human intent through\nthe problem of predicting trajectories in dynamic environments. We explore\ndomains where navigation guidelines are relatively strictly defined but not\nclearly marked in their physical environments. We hypothesize that within these\ndomains, agents tend to exhibit short-term motion patterns that reveal context\ninformation related to the agent's general direction, intermediate goals and\nrules of motion, e.g., social behavior. From this intuition, we propose\nSocial-PatteRNN, an algorithm for recurrent, multi-modal trajectory prediction\nthat exploits motion patterns to encode the aforesaid contexts. Our approach\nguides long-term trajectory prediction by learning to predict short-term motion\npatterns. It then extracts sub-goal information from the patterns and\naggregates it as social context. We assess our approach across three domains:\nhumans crowds, humans in sports and manned aircraft in terminal airspace,\nachieving state-of-the-art performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.14417,regular,pre_llm,2022,9,"{'ai_likelihood': 3.013345930311415e-06, 'text': ""Multi-Robot Coordination and Cooperation with Task Precedence\n  Relationships\n\n  We propose a new formulation for the multi-robot task planning and allocation\nproblem that incorporates (a) precedence relationships between tasks; (b)\ncoordination for tasks allowing multiple robots to achieve increased\nefficiency; and (c) cooperation through the formation of robot coalitions for\ntasks that cannot be performed by individual robots alone. In our formulation,\nthe tasks and the relationships between the tasks are specified by a task\ngraph. We define a set of reward functions over the task graph's nodes and\nedges. These functions model the effect of robot coalition size on the task\nperformance, and incorporate the influence of one task's performance on a\ndependent task. Solving this problem optimally is NP-hard. However, using the\ntask graph formulation allows us to leverage min-cost network flow approaches\nto obtain approximate solutions efficiently. Additionally, we explore a mixed\ninteger programming approach, which gives optimal solutions for small instances\nof the problem but is computationally expensive. We also develop a greedy\nheuristic algorithm as a baseline. Our modeling and solution approaches result\nin task plans that leverage task precedence relationships and robot\ncoordination and cooperation to achieve high mission performance, even in large\nmissions with many agents.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.10452,regular,pre_llm,2022,9,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Robust Bipedal Locomotion: Leveraging Saltation Matrices for Gait\n  Optimization\n\n  The ability to generate robust walking gaits on bipedal robots is key to\ntheir successful realization on hardware. To this end, this work extends the\nmethod of Hybrid Zero Dynamics (HZD) -- which traditionally only accounts for\nlocomotive stability via periodicity constraints under perfect impact events --\nthrough the inclusion of the saltation matrix with a view toward synthesizing\nrobust walking gaits. By jointly minimizing the norm of the extended saltation\nmatrix and the torque of the robot directly in the gait generation process, we\ndemonstrate that the synthesized gaits are more robust than gaits generated\nwith either term alone; these results are shown in simulation and on hardware\nfor the AMBER-3M planar biped and the Atalante lower-body exoskeleton (both\nwith and without a human subject). The end result is experimental validation\nthat combining saltation matrices with HZD methods produces more robust bipedal\nwalking in practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.10244,regular,pre_llm,2022,9,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Condition-based Design of Variable Impedance Controllers from User\n  Demonstrations\n\n  This paper presents an approach to ensure conditions on Variable Impedance\nControllers through the off-line tuning of the parameters involved in its\ndescription. In particular, we prove its application to term modulations\ndefined by a Learning from Demonstration technique. This is performed through\nthe assessment of conditions regarding safety and performance, which encompass\nheuristics and constraints in the form of Linear Matrix Inequalities. Latter\nones allow to define a convex optimisation problem to analyse their fulfilment,\nand require a polytopic description of the VIC, in this case, obtained from its\nformulation as a discrete-time Linear Parameter Varying system. With respect to\nthe current state-of-art, this approach only limits the term definition\nobtained by the Learning from Demonstration technique to be continuous and\nfunction of exogenous signals, i.e. external variables to the robot. Therefore,\nusing a solution-search method, the most suitable set of parameters according\nto assessment criteria can be obtained. Using a 7-DoF Kinova Gen3 manipulator,\nvalidation and comparison against solutions with relaxed conditions are\nperformed. The method is applied to generate Variable Impedance Controllers for\na pulley belt looping task, inspired by the Assembly Challenge for Industrial\nRobotics in World Robot Summit 2018, to reduce the exerted force with respect\nto a standard (constant) Impedance Controller. Additionally, method agility is\nevaluated on the generation of controllers for one-off modifications of the\nnominal belt looping task setup without new demonstrations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.08608,regular,pre_llm,2022,9,"{'ai_likelihood': 2.9802322387695312e-06, 'text': 'HGI-SLAM: Loop Closure With Human and Geometric Importance Features\n\n  We present Human and Geometric Importance SLAM (HGI-SLAM), a novel approach\nto loop closure using salient and geometric features. Loop closure is a key\nelement of SLAM, with many established methods for this problem. However,\ncurrent methods are narrow, using either geometric or salient based features.\nWe merge their successes into a model that outperforms both types of methods\nalone. Our method utilizes inexpensive monocular cameras and does not depend on\ndepth sensors nor Lidar. HGI-SLAM utilizes geometric and salient features,\nprocesses them into descriptors, and optimizes them for a bag of words\nalgorithm. By using a concurrent thread and combing our loop closure detection\nwith ORB-SLAM2, our system is a complete SLAM framework. We present extensive\nevaluations of HGI loop detection and HGI-SLAM on the KITTI and EuRoC datasets.\nWe also provide a qualitative analysis of our features. Our method runs in real\ntime, and is robust to large viewpoint changes while staying accurate in\norganic environments. HGI-SLAM is an end-to-end SLAM system that only requires\nmonocular vision and is comparable in performance to state-of-the-art SLAM\nmethods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2209.06331,regular,pre_llm,2022,9,"{'ai_likelihood': 6.02669186062283e-06, 'text': 'Learning Category-Level Manipulation Tasks from Point Clouds with\n  Dynamic Graph CNNs\n\n  This paper presents a new technique for learning category-level manipulation\nfrom raw RGB-D videos of task demonstrations, with no manual labels or\nannotations. Category-level learning aims to acquire skills that can be\ngeneralized to new objects, with geometries and textures that are different\nfrom the ones of the objects used in the demonstrations. We address this\nproblem by first viewing both grasping and manipulation as special cases of\ntool use, where a tool object is moved to a sequence of key-poses defined in a\nframe of reference of a target object. Tool and target objects, along with\ntheir key-poses, are predicted using a dynamic graph convolutional neural\nnetwork that takes as input an automatically segmented depth and color image of\nthe entire scene. Empirical results on object manipulation tasks with a real\nrobotic arm show that the proposed network can efficiently learn from real\nvisual demonstrations to perform the tasks on novel objects within the same\ncategory, and outperforms alternative approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.086,regular,pre_llm,2022,10,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'Heterogeneous Full-body Control of a Mobile Manipulator with Behavior\n  Trees\n\n  Integrating the heterogeneous controllers of a complex mechanical system,\nsuch as a mobile manipulator, within the same structure and in a modular way is\nstill challenging. In this work we extend our framework based on Behavior Trees\nfor the control of a redundant mechanical system to the problem of commanding\nmore complex systems that involve multiple low-level controllers. This allows\nthe integrated systems to achieve non-trivial goals that require coordination\namong the sub-systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.1421,regular,pre_llm,2022,10,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""MidasTouch: Monte-Carlo inference over distributions across sliding\n  touch\n\n  We present MidasTouch, a tactile perception system for online global\nlocalization of a vision-based touch sensor sliding on an object surface. This\nframework takes in posed tactile images over time, and outputs an evolving\ndistribution of sensor pose on the object's surface, without the need for\nvisual priors. Our key insight is to estimate local surface geometry with\ntactile sensing, learn a compact representation for it, and disambiguate these\nsignals over a long time horizon. The backbone of MidasTouch is a Monte-Carlo\nparticle filter, with a measurement model based on a tactile code network\nlearned from tactile simulation. This network, inspired by LIDAR place\nrecognition, compactly summarizes local surface geometries. These generated\ncodes are efficiently compared against a precomputed tactile codebook\nper-object, to update the pose distribution. We further release the YCB-Slide\ndataset of real-world and simulated forceful sliding interactions between a\nvision-based tactile sensor and standard YCB objects. While single-touch\nlocalization can be inherently ambiguous, we can quickly localize our sensor by\ntraversing salient surface geometries. Project page:\nhttps://suddhu.github.io/midastouch-tactile/\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.09727,regular,pre_llm,2022,10,"{'ai_likelihood': 6.986988915337457e-06, 'text': 'Vision-based GNSS-Free Localization for UAVs in the Wild\n\n  Considering the accelerated development of Unmanned Aerial Vehicles (UAVs)\napplications in both industrial and research scenarios, there is an increasing\nneed for localizing these aerial systems in non-urban environments, using\nGNSS-Free, vision-based methods. Our paper proposes a vision-based localization\nalgorithm that utilizes deep features to compute geographical coordinates of a\nUAV flying in the wild. The method is based on matching salient features of RGB\nphotographs captured by the drone camera and sections of a pre-built map\nconsisting of georeferenced open-source satellite images. Experimental results\nprove that vision-based localization has comparable accuracy with traditional\nGNSS-based methods, which serve as ground truth. Compared to state-of-the-art\nVisual Odometry (VO) approaches, our solution is designed for long-distance,\nhigh-altitude UAV flights. Code and datasets are available at\nhttps://github.com/TIERS/wildnav.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.0328,regular,pre_llm,2022,10,"{'ai_likelihood': 1.7219119601779514e-06, 'text': ""Real-Time Navigation for Bipedal Robots in Dynamic Environments\n\n  The popularity of mobile robots has been steadily growing, with these robots\nbeing increasingly utilized to execute tasks previously completed by human\nworkers. For bipedal robots to see this same success, robust autonomous\nnavigation systems need to be developed that can execute in real-time and\nrespond to dynamic environments. These systems can be divided into three\nstages: perception, planning, and control. A holistic navigation framework for\nbipedal robots must successfully integrate all three components of the\nautonomous navigation problem to enable robust real-world navigation. In this\npaper, we present a real-time navigation framework for bipedal robots in\ndynamic environments. The proposed system addresses all components of the\nnavigation problem: We introduce a depth-based perception system for obstacle\ndetection, mapping, and localization. A two-stage planner is developed to\ngenerate collision-free trajectories robust to unknown and dynamic\nenvironments. And execute trajectories on the Digit bipedal robot's walking\ngait controller. The navigation framework is validated through a series of\nsimulation and hardware experiments that contain unknown environments and\ndynamic obstacles.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.04571,regular,pre_llm,2022,10,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Modular Multi-Copter Structure Control for Cooperative Aerial Cargo\n  Transportation\n\n  The control problem of a multi-copter swarm, mechanically coupled through a\nmodular lattice structure of connecting rods, is considered in this article.\nThe system's structural elasticity is considered in deriving the system's\ndynamics. The devised controller is robust against the induced flexibilities,\nwhile an inherent adaptation scheme allows for the control of asymmetrical\nconfigurations and the transportation of unknown payloads. Certain optimization\nmetrics are introduced for solving the individual agent thrust allocation\nproblem while achieving maximum system flight time, resulting in a\nplatform-independent control implementation. Experimental studies are offered\nto illustrate the efficiency of the suggested controller under typical flight\nconditions, increased rod elasticities and payload transportation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.15082,regular,pre_llm,2022,10,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'A Rapidly-Exploring Random Trees Motion Planning Algorithm for Hybrid\n  Dynamical Systems\n\n  This paper proposes a rapidly-exploring random trees (RRT) algorithm to solve\nthe motion planning problem for hybrid systems. At each iteration, the proposed\nalgorithm, called HyRRT, randomly picks a state sample and extends the search\ntree by flow or jump, which is also chosen randomly when both regimes are\npossible. Through a definition of concatenation of functions defined on hybrid\ntime domains, we show that HyRRT is probabilistically complete, namely, the\nprobability of failing to find a motion plan approaches zero as the number of\niterations of the algorithm increases. This property is guaranteed under mild\nconditions on the data defining the motion plan, which include a relaxation of\nthe usual positive clearance assumption imposed in the literature of classical\nsystems. The motion plan is computed through the solution of two optimization\nproblems, one associated with the flow and the other with the jumps of the\nsystem. The proposed algorithm is applied to a walking robot so as to highlight\nits generality and computational features.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.13956,regular,pre_llm,2022,10,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'HiddenGems: Efficient safety boundary detection with active learning\n\n  Evaluating safety performance in a resource-efficient way is crucial for the\ndevelopment of autonomous systems. Simulation of parameterized scenarios is a\npopular testing strategy but parameter sweeps can be prohibitively expensive.\nTo address this, we propose HiddenGems: a sample-efficient method for\ndiscovering the boundary between compliant and non-compliant behavior via\nactive learning. Given a parameterized scenario, one or more compliance\nmetrics, and a simulation oracle, HiddenGems maps the compliant and\nnon-compliant domains of the scenario. The methodology enables critical test\ncase identification, comparative analysis of different versions of the system\nunder test, as well as verification of design objectives. We evaluate\nHiddenGems on a scenario with a jaywalker crossing in front of an autonomous\nvehicle and obtain compliance boundary estimates for collision, lane keep, and\nacceleration metrics individually and in combination, with 6 times fewer\nsimulations than a parameter sweep. We also show how HiddenGems can be used to\ndetect and rectify a failure mode for an unprotected turn with 86% fewer\nsimulations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.01487,regular,pre_llm,2022,10,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'SwarMan: Anthropomorphic Swarm of Drones Avatar with Body Tracking and\n  Deep Learning-Based Gesture Recognition\n\n  Anthropomorphic robot avatars present a conceptually novel approach to remote\naffective communication, allowing people across the world a wider specter of\nemotional and social exchanges over traditional 2D and 3D image data. However,\nthere are several limitations of current telepresence robots, such as the high\nweight, complexity of the system that prevents its fast deployment, and the\nlimited workspace of the avatars mounted on either static or wheeled mobile\nplatforms.\n  In this paper, we present a novel concept of telecommunication through a\nrobot avatar based on an anthropomorphic swarm of drones; SwarMan. The\ndeveloped system consists of nine nanocopters controlled remotely by the\noperator through a gesture recognition interface. SwarMan allows operators to\ncommunicate by directly following their motions and by recognizing one of the\nprerecorded emotional patterns, thus rendering the captured emotion as\nillumination on the drones. The LSTM MediaPipe network was trained on a\ncollected dataset of 600 short videos with five emotional gestures. The\naccuracy of achieved emotion recognition was 97% on the test dataset.\n  As communication through the swarm avatar significantly changes the visual\nappearance of the operator, we investigated the ability of the users to\nrecognize and respond to emotions performed by the swarm of drones. The\nexperimental results revealed a high consistency between the users in rating\nemotions. Additionally, users indicated low physical demand (2.25 on the Likert\nscale) and were satisfied with their performance (1.38 on the Likert scale)\nwhen communicating by the SwarMan interface.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.15779,regular,pre_llm,2022,10,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Adapting Neural Models with Sequential Monte Carlo Dropout\n\n  The ability to adapt to changing environments and settings is essential for\nrobots acting in dynamic and unstructured environments or working alongside\nhumans with varied abilities or preferences. This work introduces an extremely\nsimple and effective approach to adapting neural models in response to changing\nsettings. We first train a standard network using dropout, which is analogous\nto learning an ensemble of predictive models or distribution over predictions.\nAt run-time, we use a particle filter to maintain a distribution over dropout\nmasks to adapt the neural model to changing settings in an online manner.\nExperimental results show improved performance in control problems requiring\nboth online and look-ahead prediction, and showcase the interpretability of the\ninferred masks in a human behaviour modelling task for drone teleoperation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.02735,regular,pre_llm,2022,10,"{'ai_likelihood': 9.106265174018012e-06, 'text': ""What Should the System Do Next?: Operative Action Captioning for\n  Estimating System Actions\n\n  Such human-assisting systems as robots need to correctly understand the\nsurrounding situation based on observations and output the required support\nactions for humans. Language is one of the important channels to communicate\nwith humans, and the robots are required to have the ability to express their\nunderstanding and action planning results. In this study, we propose a new task\nof operative action captioning that estimates and verbalizes the actions to be\ntaken by the system in a human-assisting domain. We constructed a system that\noutputs a verbal description of a possible operative action that changes the\ncurrent state to the given target state. We collected a dataset consisting of\ntwo images as observations, which express the current state and the state\nchanged by actions, and a caption that describes the actions that change the\ncurrent state to the target state, by crowdsourcing in daily life situations.\nThen we constructed a system that estimates operative action by a caption.\nSince the operative action's caption is expected to contain some state-changing\nactions, we use scene-graph prediction as an auxiliary task because the events\nwritten in the scene graphs correspond to the state changes. Experimental\nresults showed that our system successfully described the operative actions\nthat should be conducted between the current and target states. The auxiliary\ntasks that predict the scene graphs improved the quality of the estimation\nresults.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.12115,regular,pre_llm,2022,10,"{'ai_likelihood': 1.483493381076389e-05, 'text': 'Pedestrian Emergency Braking in Ten Weeks\n\n  In the last decade, research in the field of autonomous vehicles has grown\nimmensely, and there is a wealth of information available for researchers to\nrapidly establish an autonomous vehicle platform for basic maneuvers. In this\npaper, we design, implement, and test, in ten weeks, a PD approach to\nlongitudinal control for pedestrian emergency braking. We also propose a\nlateral controller with a similar design for future testing in lane following.\nUsing widely available tools, we demonstrate the safety of the vehicle in\npedestrian emergency braking scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.05572,regular,pre_llm,2022,10,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Modular Robots: extending the capabilities of one robot\n\n  For a robot to be perfect and enter the everyday life of humans,like\ncomputers did, it needs to move from special-purpose robots to general-purpose.\nSo, the idea of modularity is considered in this project.Thus, any type of task\nthat falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be\nachieved by adding a module to the robot.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.12386,regular,pre_llm,2022,10,"{'ai_likelihood': 9.17249255710178e-06, 'text': 'Learning Feasibility of Factored Nonlinear Programs in Robotic\n  Manipulation Planning\n\n  A factored Nonlinear Program (Factored-NLP) explicitly models the\ndependencies between a set of continuous variables and nonlinear constraints,\nproviding an expressive formulation for relevant robotics problems such as\nmanipulation planning or simultaneous localization and mapping. When the\nproblem is over-constrained or infeasible, a fundamental issue is to detect a\nminimal subset of variables and constraints that are infeasible. Previous\napproaches require solving several nonlinear programs, incrementally adding and\nremoving constraints, and are thus computationally expensive. In this paper, we\npropose a graph neural architecture that predicts which variables and\nconstraints are jointly infeasible. The model is trained with a dataset of\nlabeled subgraphs of Factored-NLPs, and importantly, can make useful\npredictions on larger factored nonlinear programs than the ones seen during\ntraining. We evaluate our approach in robotic manipulation planning, where our\nmodel is able to generalize to longer manipulation sequences involving more\nobjects and robots, and different geometric environments. The experiments show\nthat the learned model accelerates general algorithms for conflict extraction\n(by a factor of 50) and heuristic algorithms that exploit expert knowledge (by\na factor of 4).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.00812,regular,pre_llm,2022,10,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'A Benchmark for Multi-Modal Lidar SLAM with Ground Truth in GNSS-Denied\n  Environments\n\n  Lidar-based simultaneous localization and mapping (SLAM) approaches have\nobtained considerable success in autonomous robotic systems. This is in part\nowing to the high-accuracy of robust SLAM algorithms and the emergence of new\nand lower-cost lidar products. This study benchmarks current state-of-the-art\nlidar SLAM algorithms with a multi-modal lidar sensor setup showcasing diverse\nscanning modalities (spinning and solid-state) and sensing technologies, and\nlidar cameras, mounted on a mobile sensing and computing platform. We extend\nour previous multi-modal multi-lidar dataset with additional sequences and new\nsources of ground truth data. Specifically, we propose a new multi-modal\nmulti-lidar SLAM-assisted and ICP-based sensor fusion method for generating\nground truth maps. With these maps, we then match real-time pointcloud data\nusing a natural distribution transform (NDT) method to obtain the ground truth\nwith full 6 DOF pose estimation. This novel ground truth data leverages\nhigh-resolution spinning and solid-state lidars. We also include new open road\nsequences with GNSS-RTK data and additional indoor sequences with motion\ncapture (MOCAP) ground truth, complementing the previous forest sequences with\nMOCAP data. We perform an analysis of the positioning accuracy achieved with\nten different SLAM algorithm and lidar combinations. We also report the\nresource utilization in four different computational platforms and a total of\nfive settings (Intel and Jetson ARM CPUs). Our experimental results show that\ncurrent state-of-the-art lidar SLAM algorithms perform very differently for\ndifferent types of sensors. More results, code, and the dataset can be found\nat:\n\\href{https://github.com/TIERS/tiers-lidars-dataset-enhanced}{github.com/TIERS/tiers-lidars-dataset-enhanced.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.1329,regular,pre_llm,2022,10,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'If You Are Careful, So Am I! How Robot Communicative Motions Can\n  Influence Human Approach in a Joint Task\n\n  As humans, we have a remarkable capacity for reading the characteristics of\nobjects only by observing how another person carries them. Indeed, how we\nperform our actions naturally embeds information on the item features.\nCollaborative robots can achieve the same ability by modulating the strategy\nused to transport objects with their end-effector. A contribution in this sense\nwould promote spontaneous interactions by making an implicit yet effective\ncommunication channel available. This work investigates if humans correctly\nperceive the implicit information shared by a robotic manipulator through its\nmovements during a dyadic collaboration task. Exploiting a generative approach,\nwe designed robot actions to convey virtual properties of the transported\nobjects, particularly to inform the partner if any caution is required to\nhandle the carried item. We found that carefulness is correctly interpreted\nwhen observed through the robot movements. In the experiment, we used identical\nempty plastic cups; nevertheless, participants approached them differently\ndepending on the attitude shown by the robot: humans change how they reach for\nthe object, being more careful whenever the robot does the same. This emerging\nform of motor contagion is entirely spontaneous and happens even if the task\ndoes not require it.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.12034,review,pre_llm,2022,10,"{'ai_likelihood': 1.2748771243625217e-05, 'text': 'Proceedings of the Dialogue Robot Competition 2022\n\n  The proceedings contain papers on the dialogue systems developed by the\ntwelve teams participating in DRC2022, as well as an overview paper summarizing\nthe competition.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.13992,regular,pre_llm,2022,10,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'SphNet: A Spherical Network for Semantic Pointcloud Segmentation\n\n  Semantic segmentation for robotic systems can enable a wide range of\napplications, from self-driving cars and augmented reality systems to domestic\nrobots. We argue that a spherical representation is a natural one for\negocentric pointclouds. Thus, in this work, we present a novel framework\nexploiting such a representation of LiDAR pointclouds for the task of semantic\nsegmentation. Our approach is based on a spherical convolutional neural network\nthat can seamlessly handle observations from various sensor systems (e.g.,\ndifferent LiDAR systems) and provides an accurate segmentation of the\nenvironment. We operate in two distinct stages: First, we encode the projected\ninput pointclouds to spherical features. Second, we decode and back-project the\nspherical features to achieve an accurate semantic segmentation of the\npointcloud. We evaluate our method with respect to state-of-the-art\nprojection-based semantic segmentation approaches using well-known public\ndatasets. We demonstrate that the spherical representation enables us to\nprovide more accurate segmentation and to have a better generalization to\nsensors with different field-of-view and number of beams than what was seen\nduring training.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.02685,regular,pre_llm,2022,10,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'A Real2Sim2Real Method for Robust Object Grasping with Neural Surface\n  Reconstruction\n\n  Recent 3D-based manipulation methods either directly predict the grasp pose\nusing 3D neural networks, or solve the grasp pose using similar objects\nretrieved from shape databases. However, the former faces generalizability\nchallenges when testing with new robot arms or unseen objects; and the latter\nassumes that similar objects exist in the databases. We hypothesize that recent\n3D modeling methods provides a path towards building digital replica of the\nevaluation scene that affords physical simulation and supports robust\nmanipulation algorithm learning. We propose to reconstruct high-quality meshes\nfrom real-world point clouds using state-of-the-art neural surface\nreconstruction method (the Real2Sim step). Because most simulators take meshes\nfor fast simulation, the reconstructed meshes enable grasp pose labels\ngeneration without human efforts. The generated labels can train grasp network\nthat performs robustly in the real evaluation scene (the Sim2Real step). In\nsynthetic and real experiments, we show that the Real2Sim2Real pipeline\nperforms better than baseline grasp networks trained with a large dataset and a\ngrasp sampling method with retrieval-based reconstruction. The benefit of the\nReal2Sim2Real pipeline comes from 1) decoupling scene modeling and grasp\nsampling into sub-problems, and 2) both sub-problems can be solved with\nsufficiently high quality using recent 3D learning algorithms and mesh-based\nphysical simulation techniques.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.0954,regular,pre_llm,2022,10,"{'ai_likelihood': 2.6490953233506944e-06, 'text': 'Contact-Implicit Planning and Control for Non-Prehensile Manipulation\n  Using State-Triggered Constraints\n\n  We present a contact-implicit planning approach that can generate\ncontact-interaction trajectories for non-prehensile manipulation problems\nwithout tuning or a tailored initial guess and with high success rates. This is\nachieved by leveraging the concept of state-triggered constraints (STCs) to\ncapture the hybrid dynamics induced by discrete contact modes without\nexplicitly reasoning about the combinatorics. STCs enable triggering arbitrary\nconstraints by a strict inequality condition in a continuous way. We first use\nSTCs to develop an automatic contact constraint activation method to minimize\nthe effective constraint space based on the utility of contact candidates for a\ngiven task. Then, we introduce a re-formulation of the Coulomb friction model\nbased on STCs that is more efficient for the discovery of tangential forces\nthan the well-studied complementarity constraints-based approach. Last, we\ninclude the proposed friction model in the planning and control of quasi-static\nplanar pushing. The performance of the STC-based contact activation and\nfriction methods is evaluated by extensive simulation experiments in a dynamic\npushing scenario. The results demonstrate that our methods outperform the\nbaselines based on complementarity constraints with a significant decrease in\nthe planning time and a higher success rate. We then compare the proposed\nquasi-static pushing controller against a mixed-integer programming-based\napproach in simulation and find that our method is computationally more\nefficient and provides a better tracking accuracy, with the added benefit of\nnot requiring an initial control trajectory. Finally, we present hardware\nexperiments demonstrating the usability of our framework in executing complex\ntrajectories in real-time even with a low-accuracy tracking system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2210.11877,regular,pre_llm,2022,10,"{'ai_likelihood': 2.1192762586805556e-06, 'text': ""A Multi-Arm Robotic Platform for Scientific Exploration\n\nThere are a large number of robotic platforms with two or more arms targeting surgical applications. Despite that, very few groups have employed such platforms for scientific exploration. Possible applications of a multi-arm platform in scientific exploration involve the study of the mechanisms of intractable diseases by using organoids, i.e., miniature human organs) The study of organoids requires the preparation of a cranial window, which is done by carefully removing an 8 mm patch of the skull of a mouse. In this work, we present the first prototype of our AI-robot science platform for scientific experimentation, its digital twins, and perform validation experiments under teleoperation. The experiments showcase the dexterity of the platform by performing peg transfer, gauze cutting, mock experiments using eggs, and the world's first four-hand teleoperated drilling for a cranial window. The digital twins and related control software are freely available for noncommercial use at https://AISciencePlatform.github.io."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.05257,regular,pre_llm,2022,11,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Single-Fingered Reconfigurable Robotic Gripper With a Folding Mechanism\n  for Narrow Working Spaces\n\n  This letter proposes a novel single-fingered reconfigurable robotic gripper\nfor grasping objects in narrow working spaces. The finger of the developed\ngripper realizes two configurations, namely, the insertion and grasping modes,\nusing only a single motor. In the insertion mode, the finger assumes a thin\nshape such that it can insert its tip into a narrow space. The grasping mode of\nthe finger is activated through a folding mechanism. Mode switching can be\nachieved in two ways: switching the mode actively by a motor, or combining\npassive rotation of the fingertip through contact with the support surface and\nactive motorized construction of the claw. The latter approach is effective\nwhen it is unclear how much finger insertion is required for a specific task.\nThe structure provides a simple control scheme. The performance of the proposed\nrobotic gripper design and control methodology was experimentally evaluated.\nThe minimum width of the insertion space required to grasp an object is 4 mm (1\nmm, when using a strategy).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.04972,regular,pre_llm,2022,11,"{'ai_likelihood': 2.086162567138672e-05, 'text': 'Hibikino-Musashi@Home 2018 Team Description Paper\n\n  Our team, Hibikino-Musashi@Home (the shortened name is HMA), was founded in\n2010. It is based in the Kitakyushu Science and Research Park, Japan. We have\nparticipated in the RoboCup@Home Japan open competition open platform league\nevery year since 2010. Moreover, we participated in the RoboCup 2017 Nagoya as\nopen platform league and domestic standard platform league teams. Currently,\nthe Hibikino-Musashi@Home team has 20 members from seven different laboratories\nbased in the Kyushu Institute of Technology. In this paper, we introduce the\nactivities of our team and the technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.05263,regular,pre_llm,2022,11,"{'ai_likelihood': 6.722079383002388e-06, 'text': 'Sensing and Control of Friction Mode for Contact Area Variable Surfaces\n  (Friction-variable Surface Structure)\n\n  Robotic hands with soft surfaces can perform stable grasping, but the high\nfriction of the soft surfaces makes it difficult to release objects, or to\nperform operations that require sliding. To solve this issue, we previously\ndeveloped a contact area variable surface (CAVS), whose friction changed\naccording to the load. However, only our fundamental results were previously\npresented, with detailed analyses not provided. In this study, we first\ninvestigated the CAVS friction anisotropy, and demonstrated that the\nlongitudinal direction exhibited a larger ratio of friction change. Next, we\nproposed a sensible CAVS, capable of providing a variable-friction mechanism,\nand tested its sensing and control systems in operations requiring switching\nbetween sliding and stable-grasping modes. Friction sensing was performed using\nan embedded camera, and we developed a gripper using the sensible CAVS,\nconsidering the CAVS friction anisotropy. In CAVS, the low-friction mode\ncorresponds to a small grasping force, while the high-friction mode corresponds\nto a greater grasping force. Therefore, by controlling only the friction mode,\nthe gripper mode can be set to either the sliding or stable-grasping mode.\nBased on this feature, a methodology for controlling the contact mode was\nconstructed. We demonstrated a manipulation involving sliding and stable\ngrasping, and thus verified the efficacy of the developed sensible CAVS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.05243,regular,pre_llm,2022,11,"{'ai_likelihood': 1.7219119601779514e-06, 'text': ""Vision-based navigation and obstacle avoidance via deep reinforcement\n  learning\n\n  Development of navigation algorithms is essential for the successful\ndeployment of robots in rapidly changing hazardous environments for which prior\nknowledge of configuration is often limited or unavailable. Use of traditional\npath-planning algorithms, which are based on localization and require detailed\nobstacle maps with goal locations, is not possible. In this regard,\nvision-based algorithms hold great promise, as visual information can be\nreadily acquired by a robot's onboard sensors and provides a much richer source\nof information from which deep neural networks can extract complex patterns.\nDeep reinforcement learning has been used to achieve vision-based robot\nnavigation. However, the efficacy of these algorithms in environments with\ndynamic obstacles and high variation in the configuration space has not been\nthoroughly investigated. In this paper, we employ a deep Dyna-Q learning\nalgorithm for room evacuation and obstacle avoidance in partially observable\nenvironments based on low-resolution raw image data from an onboard camera. We\nexplore the performance of a robotic agent in environments containing no\nobstacles, convex obstacles, and concave obstacles, both static and dynamic.\nObstacles and the exit are initialized in random positions at the start of each\nepisode of reinforcement learning. Overall, we show that our algorithm and\ntraining approach can generalize learning for collision-free evacuation of\nenvironments with complex obstacle configurations. It is evident that the agent\ncan navigate to a goal location while avoiding multiple static and dynamic\nobstacles, and can escape from a concave obstacle while searching for and\nnavigating to the exit.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.09555,review,pre_llm,2022,11,"{'ai_likelihood': 3.387530644734701e-05, 'text': 'UAV Assisted Data Collection for Internet of Things: A Survey\n\n  Thanks to the advantages of flexible deployment and high mobility, unmanned\naerial vehicles (UAVs) have been widely applied in the areas of disaster\nmanagement, agricultural plant protection, environment monitoring and so on.\nWith the development of UAV and sensor technologies, UAV assisted data\ncollection for Internet of Things (IoT) has attracted increasing attentions. In\nthis article, the scenarios and key technologies of UAV assisted data\ncollection are comprehensively reviewed. First, we present the system model\nincluding the network model and mathematical model of UAV assisted data\ncollection for IoT. Then, we review the key technologies including clustering\nof sensors, UAV data collection mode as well as joint path planning and\nresource allocation. Finally, the open problems are discussed from the\nperspectives of efficient multiple access as well as joint sensing and data\ncollection. This article hopefully provides some guidelines and insights for\nresearchers in the area of UAV assisted data collection for IoT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.06548,regular,pre_llm,2022,11,"{'ai_likelihood': 1.556343502468533e-06, 'text': ""Computationally Light Spectrally Normalized Memory Neuron Network based\n  Estimator for GPS-Denied operation of Micro UAV\n\n  This paper addresses the problem of position estimation in UAVs operating in\na cluttered environment where GPS information is unavailable. A model\nlearning-based approach is proposed that takes in the rotor RPMs and past state\nas input and predicts the one-step-ahead position of the UAV using a novel\nspectral-normalized memory neural network (SN-MNN). The spectral normalization\nguarantees stable and reliable prediction performance. The predicted position\nis transformed to global coordinate frame which is then fused along with the\nodometry of other peripheral sensors like IMU, barometer, compass etc., using\nthe onboard extended Kalman filter to estimate the states of the UAV. The\nexperimental flight data collected from a motion capture facility using a\nmicro-UAV is used to train the SN-MNN. The PX4-ECL library is used to replay\nthe flight data using the proposed algorithm, and the estimated position is\ncompared with actual ground truth data. The proposed algorithm doesn't require\nany additional onboard sensors, and is computationally light. The performance\nof the proposed approach is compared with the current state-of-art GPS-denied\nalgorithms, and it can be seen that the proposed algorithm has the least RMSE\nfor position estimates.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.11089,regular,pre_llm,2022,11,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Multi-Arm Bin-Picking in Real-Time: A Combined Task and Motion Planning\n  Approach\n\n  Automated bin-picking is a prerequisite for fully automated manufacturing and\nwarehouses. To successfully pick an item from an unstructured bin the robot\nneeds to first detect possible grasps for the objects, decide on the object to\nremove and consequently plan and execute a feasible trajectory to retrieve the\nchosen object. Over the last years significant progress has been made towards\nsolving these problems. However, when multiple robot arms are cooperating the\ndecision and planning problems become exponentially harder. We propose an\nintegrated multi-arm bin-picking pipeline (IMAPIP), and demonstrate that it is\nable to reliably pick objects from a bin in real-time using multiple robot\narms. IMAPIP solves the multi-arm bin-picking task first at high-level using a\ngeometry-aware policy integrated in a combined task and motion planning\nframework. We then plan motions consistent with this policy using the BIT*\nalgorithm on the motion planning level. We show that this integrated solution\nenables robot arm cooperation. In our experiments, we show the proposed\ngeometry-aware policy outperforms a baseline by increasing bin-picking time by\n28\\% using two robot arms. The policy is robust to changes in the position of\nthe bin and number of objects. We also show that IMAPIP to successfully scale\nup to four robot arms working in close proximity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01105,regular,pre_llm,2022,11,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Road Markings Segmentation from LIDAR Point Clouds using Reflectivity\n  Information\n\n  Lane detection algorithms are crucial for the development of autonomous\nvehicles technologies. The more extended approach is to use cameras as sensors.\nHowever, LIDAR sensors can cope with weather and light conditions that cameras\ncan not. In this paper, we introduce a method to extract road markings from the\nreflectivity data of a 64-layers LIDAR sensor. First, a plane segmentation\nmethod along with region grow clustering was used to extract the road plane.\nThen we applied an adaptive thresholding based on Otsu s method and finally, we\nfitted line models to filter out the remaining outliers. The algorithm was\ntested on a test track at 60km/h and a highway at 100km/h. Results showed the\nalgorithm was reliable and precise. There was a clear improvement when using\nreflectivity data in comparison to the use of the raw intensity data both of\nthem provided by the LIDAR sensor.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.03014,regular,pre_llm,2022,11,"{'ai_likelihood': 5.563100179036459e-06, 'text': 'HeRoSwarm: Fully-Capable Miniature Swarm Robot Hardware Design With\n  Open-Source ROS Support\n\n  Experiments using large numbers of miniature swarm robots are desirable to\nteach, study, and test multi-robot and swarm intelligence algorithms and their\napplications. To realize the full potential of a swarm robot, it should be\ncapable of not only motion but also sensing, computing, communication, and\npower management modules with multiple options. Current swarm robot platforms\ndeveloped for commercial and academic research purposes lack several of these\ncritical attributes by focusing only on a few of these aspects. Therefore, in\nthis paper, we propose the HeRoSwarm, a fully-capable swarm robot platform with\nopen-source hardware and software support. The proposed robot hardware is a\nlow-cost design with commercial off-the-shelf components that uniquely\nintegrates multiple sensing, communication, and computing modalities with\nvarious power management capabilities into a tiny footprint. Moreover, our\nswarm robot with odometry capability with Robot Operating Systems (ROS) support\nis unique in its kind. This simple yet powerful swarm robot design has been\nextensively verified with different prototyping variants and multi-robot\nexperimental demonstrations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.02597,regular,pre_llm,2022,11,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Autonomous Medical Needle Steering In Vivo\n\n  The use of needles to access sites within organs is fundamental to many\ninterventional medical procedures both for diagnosis and treatment. Safe and\naccurate navigation of a needle through living tissue to an intra-tissue target\nis currently often challenging or infeasible due to the presence of anatomical\nobstacles in the tissue, high levels of uncertainty, and natural tissue motion\n(e.g., due to breathing). Medical robots capable of automating needle-based\nprocedures in vivo have the potential to overcome these challenges and enable\nan enhanced level of patient care and safety. In this paper, we show the first\nmedical robot that autonomously navigates a needle inside living tissue around\nanatomical obstacles to an intra-tissue target. Our system leverages an aiming\ndevice and a laser-patterned highly flexible steerable needle, a type of needle\ncapable of maneuvering along curvilinear trajectories to avoid obstacles. The\nautonomous robot accounts for anatomical obstacles and uncertainty in living\ntissue/needle interaction with replanning and control and accounts for\nrespiratory motion by defining safe insertion time windows during the breathing\ncycle. We apply the system to lung biopsy, which is critical in the diagnosis\nof lung cancer, the leading cause of cancer-related death in the United States.\nWe demonstrate successful performance of our system in multiple in vivo porcine\nstudies and also demonstrate that our approach leveraging autonomous needle\nsteering outperforms a standard manual clinical technique for lung nodule\naccess.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.06762,regular,pre_llm,2022,11,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Adaptive Nonlinear MPC for Trajectory Tracking of An Overactuated\n  Tiltrotor Hexacopter\n\n  Omnidirectional micro aerial vehicles (OMAVs) are more capable of doing\nenvironmentally interactive tasks due to their ability to exert full wrenches\nwhile maintaining stable poses. However, OMAVs often incorporate additional\nactuators and complex mechanical structures to achieve omnidirectionality.\nObtaining precise mathematical models is difficult, and the mismatch between\nthe model and the real physical system is not trivial. The large model-plant\nmismatch significantly degrades overall system performance if a non-adaptive\nmodel predictive controller (MPC) is used. This work presents the\n$\\mathcal{L}_1$-MPC, an adaptive nonlinear model predictive controller for\naccurate 6-DOF trajectory tracking of an overactuated tiltrotor hexacopter in\nthe presence of model uncertainties and external disturbances. The\n$\\mathcal{L}_1$-MPC adopts a cascaded system architecture in which a nominal\nMPC is followed and augmented by an $\\mathcal{L}_1$ adaptive controller. The\nproposed method is evaluated against the non-adaptive MPC, the EKF-MPC, and the\nPID method in both numerical and PX4 software-in-the-loop simulation with\nGazebo. The $\\mathcal{L}_1$-MPC reduces the tracking error by around 90% when\ncompared to a non-adaptive MPC, and the $\\mathcal{L}_1$-MPC has lower tracking\nerrors, higher uncertainty estimation rates, and less tuning requirements over\nthe EKF-MPC. We will make the implementations, including the hardware-verified\nPX4 firmware and Gazebo plugins, open-source at\nhttps://github.com/HITSZ-NRSL/omniHex.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.10716,regular,pre_llm,2022,11,"{'ai_likelihood': 6.490283542209202e-06, 'text': 'MARSIM: A light-weight point-realistic simulator for LiDAR-based UAVs\n\n  The emergence of low-cost, small form factor and light-weight solid-state\nLiDAR sensors have brought new opportunities for autonomous unmanned aerial\nvehicles (UAVs) by advancing navigation safety and computation efficiency. Yet\nthe successful developments of LiDAR-based UAVs must rely on extensive\nsimulations. Existing simulators can hardly perform simulations of real-world\nenvironments due to the requirements of dense mesh maps that are difficult to\nobtain. In this paper, we develop a point-realistic simulator of real-world\nscenes for LiDAR-based UAVs. The key idea is the underlying point rendering\nmethod, where we construct a depth image directly from the point cloud map and\ninterpolate it to obtain realistic LiDAR point measurements. Our developed\nsimulator is able to run on a light-weight computing platform and supports the\nsimulation of LiDARs with different resolution and scanning patterns, dynamic\nobstacles, and multi-UAV systems. Developed in the ROS framework, the simulator\ncan easily communicate with other key modules of an autonomous robot, such as\nperception, state estimation, planning, and control. Finally, the simulator\nprovides 10 high-resolution point cloud maps of various real-world\nenvironments, including forests of different densities, historic building,\noffice, parking garage, and various complex indoor environments. These\nrealistic maps provide diverse testing scenarios for an autonomous UAV.\nEvaluation results show that the developed simulator achieves superior\nperformance in terms of time and memory consumption against Gazebo and that the\nsimulated UAV flights highly match the actual one in real-world environments.\nWe believe such a point-realistic and light-weight simulator is crucial to\nbridge the gap between UAV simulation and experiments and will significantly\nfacilitate the research of LiDAR-based autonomous UAVs in the future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.08206,regular,pre_llm,2022,11,"{'ai_likelihood': 6.291601392957899e-06, 'text': 'Phase Distribution in Probabilistic Movement Primitives, Representing\n  Time Variability for the Recognition and Reproduction of Human Movements\n\n  Probabilistic Movement Primitives (ProMPs) are a widely used representation\nof movements for human-robot interaction. They also facilitate the\nfactorization of temporal and spatial structure of movements. In this work we\ninvestigate a method to temporally align observations so that when learning\nProMPs, information in the spatial structure of the observed motion is\nmaximized while maintaining a smooth phase velocity. We apply the method on\nrecordings of hand trajectories in a two-dimensional reaching task. A system\nfor simultaneous recognition of movement and phase is proposed and performance\nof movement recognition and movement reproduction is discussed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01538,regular,pre_llm,2022,11,"{'ai_likelihood': 0.18120659722222224, 'text': ""$D^2$SLAM: Decentralized and Distributed Collaborative Visual-inertial\n  SLAM System for Aerial Swarm\n\n  Collaborative simultaneous localization and mapping (CSLAM) is essential for\nautonomous aerial swarms, laying the foundation for downstream algorithms such\nas planning and control. To address existing CSLAM systems' limitations in\nrelative localization accuracy, crucial for close-range UAV collaboration, this\npaper introduces $D^2$SLAM-a novel decentralized and distributed CSLAM system.\n$D^2$SLAM innovatively manages near-field estimation for precise relative state\nestimation in proximity and far-field estimation for consistent global\ntrajectories. Its adaptable front-end supports both stereo and omnidirectional\ncameras, catering to various operational needs and overcoming field-of-view\nchallenges in aerial swarms. Experiments demonstrate $D^2$SLAM's effectiveness\nin accurate ego-motion estimation, relative localization, and global\nconsistency. Enhanced by distributed optimization algorithms, $D^2$SLAM\nexhibits remarkable scalability and resilience to network delays, making it\nwell-suited for a wide range of real-world aerial swarm applications. The\nadaptability and proven performance of $D^2$SLAM represent a significant\nadvancement in autonomous aerial swarm technology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.13428,regular,pre_llm,2022,11,"{'ai_likelihood': 1.3344817691379123e-05, 'text': 'Real-Time Marker Localization Learning for GelStereo Tactile Sensing\n\n  Visuotactile sensing technology is becoming more popular in tactile sensing,\nbut the effectiveness of the existing marker detection localization methods\nremains to be further explored. Instead of contour-based blob detection, this\npaper presents a learning-based marker localization network for GelStereo\nvisuotactile sensing called Marknet. Specifically, the Marknet presents a grid\nregression architecture to incorporate the distribution of the GelStereo\nmarkers. Furthermore, a marker rationality evaluator (MRE) is modelled to\nscreen suitable prediction results. The experimental results show that the\nMarknet combined with MRE achieves 93.90% precision for irregular markers in\ncontact areas, which outperforms the traditional contour-based blob detection\nmethod by a large margin of 42.32%. Meanwhile, the proposed learning-based\nmarker localization method can achieve better real-time performance beyond the\nblob detection interface provided by the OpenCV library through GPU\nacceleration, which we believe will lead to considerable perceptual sensitivity\ngains in various robotic manipulation tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.03393,regular,pre_llm,2022,11,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Bayesian Disturbance Injection: Robust Imitation Learning of Flexible\n  Policies for Robot Manipulation\n\n  Humans demonstrate a variety of interesting behavioral characteristics when\nperforming tasks, such as selecting between seemingly equivalent optimal\nactions, performing recovery actions when deviating from the optimal\ntrajectory, or moderating actions in response to sensed risks. However,\nimitation learning, which attempts to teach robots to perform these same tasks\nfrom observations of human demonstrations, often fails to capture such\nbehavior. Specifically, commonly used learning algorithms embody inherent\ncontradictions between the learning assumptions (e.g., single optimal action)\nand actual human behavior (e.g., multiple optimal actions), thereby limiting\nrobot generalizability, applicability, and demonstration feasibility. To\naddress this, this paper proposes designing imitation learning algorithms with\na focus on utilizing human behavioral characteristics, thereby embodying\nprinciples for capturing and exploiting actual demonstrator behavioral\ncharacteristics. This paper presents the first imitation learning framework,\nBayesian Disturbance Injection (BDI), that typifies human behavioral\ncharacteristics by incorporating model flexibility, robustification, and risk\nsensitivity. Bayesian inference is used to learn flexible non-parametric\nmulti-action policies, while simultaneously robustifying policies by injecting\nrisk-sensitive disturbances to induce human recovery action and ensuring\ndemonstration feasibility. Our method is evaluated through risk-sensitive\nsimulations and real-robot experiments (e.g., table-sweep task, shaft-reach\ntask and shaft-insertion task) using the UR5e 6-DOF robotic arm, to demonstrate\nthe improved characterisation of behavior. Results show significant improvement\nin task performance, through improved flexibility, robustness as well as\ndemonstration feasibility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.12849,regular,pre_llm,2022,11,"{'ai_likelihood': 1.0563267601860894e-05, 'text': 'Whole-Body Trajectory Optimization for Robot Multimodal Locomotion\n\n  The general problem of planning feasible trajectories for multimodal robots\nis still an open challenge. This paper presents a whole-body trajectory\noptimisation approach that addresses this challenge by combining methods and\ntools developed for aerial and legged robots. First, robot models that enable\nthe presented whole-body trajectory optimisation framework are presented. The\nkey model is the so-called robot centroidal momentum, the dynamics of which is\ndirectly related to the models of the robot actuation for aerial and\nterrestrial locomotion. Then, the paper presents how these models can be\nemployed in an optimal control problem to generate either terrestrial or aerial\nlocomotion trajectories with a unified approach. The optimisation problem\nconsiders robot kinematics, momentum, thrust forces and their bounds. The\noverall approach is validated using the multimodal robot iRonCub, a flying\nhumanoid robot that expresses a degree of terrestrial and aerial locomotion. To\nsolve the associated optimal trajectory generation problem, we employ ADAM, a\ncustom-made open-source library that implements a collection of algorithms for\ncalculating rigid-body dynamics using CasADi.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.01941,regular,pre_llm,2022,11,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'DyOb-SLAM : Dynamic Object Tracking SLAM System\n\n  Simultaneous Localization & Mapping (SLAM) is the process of building a\nmutual relationship between localization and mapping of the subject in its\nsurrounding environment. With the help of different sensors, various types of\nSLAM systems have developed to deal with the problem of building the\nrelationship between localization and mapping. A limitation in the SLAM process\nis the lack of consideration of dynamic objects in the mapping of the\nenvironment. We propose the Dynamic Object Tracking SLAM (DyOb-SLAM), which is\na Visual SLAM system that can localize and map the surrounding dynamic objects\nin the environment as well as track the dynamic objects in each frame. With the\nhelp of a neural network and a dense optical flow algorithm, dynamic objects\nand static objects in an environment can be differentiated. DyOb-SLAM creates\ntwo separate maps for both static and dynamic contents. For the static\nfeatures, a sparse map is obtained. For the dynamic contents, a trajectory\nglobal map is created as output. As a result, a frame to frame real-time based\ndynamic object tracking system is obtained. With the pose calculation of the\ndynamic objects and camera, DyOb-SLAM can estimate the speed of the dynamic\nobjects with time. The performance of DyOb-SLAM is observed by comparing it\nwith a similar Visual SLAM system, VDO-SLAM and the performance is measured by\ncalculating the camera and object pose errors as well as the object speed\nerror.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.12873,regular,pre_llm,2022,11,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Effects of Sim2Real Image Translation on Lane Keeping Assist System in\n  CARLA Simulator\n\n  Autonomous vehicle simulation has the advantage of testing algorithms in\nvarious environment variables and scenarios without wasting time and resources,\nhowever, there is a visual gap with the real-world. In this paper, we trained\nDCLGAN to realistically convert the image of the CARLA simulator and evaluated\nthe effect of the Sim2Real conversion focusing on the LKAS (Lane Keeping Assist\nSystem) algorithm. In order to avoid the case where the lane is translated\ndistortedly by DCLGAN, we found the optimal training hyperparameter using FSIM\n(feature-similarity). After training, we built a system that connected the\nDCLGAN model with CARLA and AV in real-time. Then, we collected data (e.g.\nimages, GPS) and analyzed them using the following four methods. First, image\nreality was measured with FID, which we verified quantitatively reflects the\nlane characteristics. CARLA images that passed through DCLGAN had smaller FID\nvalues than the original images. Second, lane segmentation accuracy through\nENet-SAD was improved by DCLGAN. Third, in the curved route, the case of using\nDCLGAN drove closer to the center of the lane and had a high success rate.\nLastly, in the straight route, DCLGAN improved lane restoring ability after\ndeviating from the center of the lane as much as in reality.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2211.11209,regular,pre_llm,2022,11,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'A Novel Uncalibrated Visual Servoing Controller Baesd on Model-Free\n  Adaptive Control Method with Neural Network\n\n  Nowadays, with the continuous expansion of application scenarios of robotic\narms, there are more and more scenarios where nonspecialist come into contact\nwith robotic arms. However, in terms of robotic arm visual servoing,\ntraditional Position-based Visual Servoing (PBVS) requires a lot of calibration\nwork, which is challenging for the nonspecialist to cope with. To cope with\nthis situation, Uncalibrated Image-Based Visual Servoing (UIBVS) frees people\nfrom tedious calibration work. This work applied a model-free adaptive control\n(MFAC) method which means that the parameters of controller are updated in real\ntime, bringing better ability of suppression changes of system and environment.\nAn artificial intelligent neural network is applied in designs of controller\nand estimator for hand-eye relationship. The neural network is updated with the\nknowledge of the system input and output information in MFAC method. Inspired\nby ""predictive model"" and ""receding-horizon"" in Model Predictive Control (MPC)\nmethod and introducing similar structures into our algorithm, we realizes the\nuncalibrated visual servoing for both stationary targets and moving\ntrajectories. Simulated experiments with a robotic manipulator will be carried\nout to validate the proposed algorithm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.06746,regular,pre_llm,2022,12,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Pacific Lamprey Inspired Climbing\n\n  Snakes and their bio-inspired robot counterparts have demonstrated locomotion\non a wide range of terrains. However, dynamic vertical climbing is one\nlocomotion strategy that has received little attention in the existing snake\nrobotics literature. We demonstrate a new scansorial gait and robot inspired by\nthe locomotion of the Pacific Lamprey. This new gait allows a robot to steer\nwhile climbing on flat, near-vertical surfaces. A reduced-order model is\ndeveloped and used to explore the relationship between body actuation and\nvertical and lateral motions of the robot. Trident, the new wall climbing\nlamprey-inspired robot, demonstrates dynamic climbing on flat vertical surfaces\nwith a peak net vertical stride displacement of 4.1 cm per step. Actuating at\n1.3 Hz, Trident attains a vertical climbing speed of 4.8 cm/s (0.09 Bl/s) at\nspecific resistance of 8.3. Trident can also traverse laterally at 9 cm/s (0.17\nBl/s). Moreover, Trident is able to make 14\\% longer strides than the Pacific\nLamprey when climbing vertically. The computational and experimental results\ndemonstrate that a lamprey-inspired climbing gait coupled with appropriate\nattachment is a useful climbing strategy for snake robots climbing near\nvertical surfaces with limited push points.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.01678,regular,pre_llm,2022,12,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'FBG-Based Variable-Length Estimation for Shape Sensing of Extensible\n  Soft Robotic Manipulators\n\n  In this paper, we propose a novel variable-length estimation approach for\nshape sensing of extensible soft robots utilizing fiber Bragg gratings (FBGs).\nShape reconstruction from FBG sensors has been increasingly developed for soft\nrobots, while the narrow stretching range of FBG fiber makes it difficult to\nacquire accurate sensing results for extensible robots. Towards this\nlimitation, we newly introduce an FBG-based length sensor by leveraging a rigid\ncurved channel, through which FBGs are allowed to slide within the robot\nfollowing its body extension/compression, hence we can search and match the\nFBGs with specific constant curvature in the fiber to determine the effective\nlength. From the fusion with the above measurements, a model-free filtering\ntechnique is accordingly presented for simultaneous calibration of a\nvariable-length model and temporally continuous length estimation of the robot,\nenabling its accurate shape sensing using solely FBGs. The performances of the\nproposed method have been experimentally evaluated on an extensible soft robot\nequipped with an FBG fiber in both free and unstructured environments. The\nresults concerning dynamic accuracy and robustness of length estimation and\nshape sensing demonstrate the effectiveness of our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.02671,regular,pre_llm,2022,12,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Visibility-Aware Navigation Among Movable Obstacles\n\n  In this paper, we examine the problem of visibility-aware robot navigation\namong movable obstacles (VANAMO). A variant of the well-known NAMO robotic\nplanning problem, VANAMO puts additional visibility constraints on robot motion\nand object movability. This new problem formulation lifts the restrictive\nassumption that the map is fully visible and the object positions are fully\nknown. We provide a formal definition of the VANAMO problem and propose the\nLook and Manipulate Backchaining (LaMB) algorithm for solving such problems.\nLaMB has a simple vision-based API that makes it more easily transferable to\nreal-world robot applications and scales to the large 3D environments. To\nevaluate LaMB, we construct a set of tasks that illustrate the complex\ninterplay between visibility and object movability that can arise in mobile\nbase manipulation problems in unknown environments. We show that LaMB\noutperforms NAMO and visibility-aware motion planning approaches as well as\nsimple combinations of them on complex manipulation problems with partial\nobservability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.01536,regular,pre_llm,2022,12,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'NEPTUNE: Nonentangling Trajectory Planning for Multiple Tethered\n  Unmanned Vehicles\n\n  Despite recent progress on trajectory planning of multiple robots and path\nplanning of a single tethered robot, planning of multiple tethered robots to\nreach their individual targets without entanglements remains a challenging\nproblem. In this paper, we present a complete approach to address this problem.\nFirstly, we propose a multi-robot tether-aware representation of homotopy,\nusing which we can efficiently evaluate the feasibility and safety of a\npotential path in terms of (1) the cable length required to reach a target\nfollowing the path, and (2) the risk of entanglements with the cables of other\nrobots. Then, the proposed representation is applied in a decentralized and\nonline planning framework that includes a graph-based kinodynamic trajectory\nfinder and an optimization-based trajectory refinement, to generate\nentanglement-free, collision-free and dynamically feasible trajectories. The\nefficiency of the proposed homotopy representation is compared against existing\nsingle and multiple tethered robot planning approaches. Simulations with up to\n8 UAVs show the effectiveness of the approach in entanglement prevention and\nits real-time capabilities. Flight experiments using 3 tethered UAVs verify the\npracticality of the presented approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.11402,regular,pre_llm,2022,12,"{'ai_likelihood': 2.914004855685764e-06, 'text': 'Design Considerations of an Unmanned Aerial Vehicle for Aerial Filming\n\n  Filming sport videos from an aerial view has always been a hard and an\nexpensive task to achieve, especially in sports that require a wide open area\nfor its normal development or the ones that put in danger human safety.\nRecently, a new solution arose for aerial filming based on the use of Unmanned\nAerial Vehicles (UAVs), which is substantially cheaper than traditional aerial\nfilming solutions that require conventional aircrafts like helicopters or\ncomplex structures for wide mobility. In this paper, we describe the design\nprocess followed for building a customized UAV suitable for sports aerial\nfilming. The process includes the requirements definition, technical sizing and\nselection of mechanical, hardware and software technologies, as well as the\nwhole integration and operation settings. One of the goals is to develop\ntechnologies allowing to build low cost UAVs and to manage them for a wide\nrange of usage scenarios while achieving high levels of flexibility and\nautomation. This work also shows some technical issues found during the\ndevelopment of the UAV as well as the solutions implemented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.11552,regular,pre_llm,2022,12,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Trajectory Generation and Tracking Control for Aggressive Tail-Sitter\n  Flights\n\n  We address the theoretical and practical problems related to the trajectory\ngeneration and tracking control of tail-sitter UAVs. Theoretically, we focus on\nthe differential flatness property with full exploitation of actual UAV\naerodynamic models, which lays a foundation for generating dynamically feasible\ntrajectory and achieving high-performance tracking control. We have found that\na tail-sitter is differentially flat with accurate aerodynamic models within\nthe entire flight envelope, by specifying coordinate flight condition and\nchoosing the vehicle position as the flat output. This fundamental property\nallows us to fully exploit the high-fidelity aerodynamic models in the\ntrajectory planning and tracking control to achieve accurate tail-sitter\nflights. Particularly, an optimization-based trajectory planner for\ntail-sitters is proposed to design high-quality, smooth trajectories with\nconsideration of kinodynamic constraints, singularity-free constraints and\nactuator saturation. The planned trajectory of flat output is transformed to\nstate trajectory in real-time with consideration of wind in environments. To\ntrack the state trajectory, a global, singularity-free, and\nminimally-parameterized on-manifold MPC is developed, which fully leverages the\naccurate aerodynamic model to achieve high-accuracy trajectory tracking within\nthe whole flight envelope. The effectiveness of the proposed framework is\ndemonstrated through extensive real-world experiments in both indoor and\noutdoor field tests, including agile SE(3) flight through consecutive narrow\nwindows requiring specific attitude and with speed up to 10m/s, typical\ntail-sitter maneuvers (transition, level flight and loiter) with speed up to\n20m/s, and extremely aggressive aerobatic maneuvers (Wingover, Loop, Vertical\nEight and Cuban Eight) with acceleration up to 2.5g.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.03106,regular,pre_llm,2022,12,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Scale-Invariant Specifications for Human-Swarm Systems\n\n  We present a method for controlling a swarm using its spectral decomposition\n-- that is, by describing the set of trajectories of a swarm in terms of a\nspatial distribution throughout the operational domain -- guaranteeing scale\ninvariance with respect to the number of agents both for computation and for\nthe operator tasked with controlling the swarm. We use ergodic control,\ndecentralized across the network, for implementation. In the DARPA OFFSET\nprogram field setting, we test this interface design for the operator using the\nSTOMP interface -- the same interface used by Raytheon BBN throughout the\nduration of the OFFSET program. In these tests, we demonstrate that our\napproach is scale-invariant -- the user specification does not depend on the\nnumber of agents; it is persistent -- the specification remains active until\nthe user specifies a new command; and it is real-time -- the user can interact\nwith and interrupt the swarm at any time. Moreover, we show that the\nspectral/ergodic specification of swarm behavior degrades gracefully as the\nnumber of agents goes down, enabling the operator to maintain the same approach\nas agents become disabled or are added to the network. We demonstrate the\nscale-invariance and dynamic response of our system in a field relevant\nsimulator on a variety of tactical scenarios with up to 50 agents. We also\ndemonstrate the dynamic response of our system in the field with a smaller team\nof agents. Lastly, we make the code for our system available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.10425,review,pre_llm,2022,12,"{'ai_likelihood': 1.6921096377902562e-05, 'text': 'Evaluating Multimodal Interaction of Robots Assisting Older Adults\n\n  We outline our work on evaluating robots that assist older adults by engaging\nwith them through multiple modalities that include physical interaction. Our\nthesis is that to increase the effectiveness of assistive robots: 1) robots\nneed to understand and effect multimodal actions, 2) robots should not only\nreact to the human, they need to take the initiative and lead the task when it\nis necessary. We start by briefly introducing our proposed framework for\nmultimodal interaction and then describe two different experiments with the\nactual robots. In the first experiment, a Baxter robot helps a human find and\nlocate an object using the Multimodal Interaction Manager (MIM) framework. In\nthe second experiment, a NAO robot is used in the same task, however, the roles\nof the robot and the human are reversed. We discuss the evaluation methods that\nwere used in these experiments, including different metrics employed to\ncharacterize the performance of the robot in each case. We conclude by\nproviding our perspective on the challenges and opportunities for the\nevaluation of assistive robots for older adults in realistic settings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.12867,regular,pre_llm,2022,12,"{'ai_likelihood': 7.748603820800781e-06, 'text': 'Differential Flatness of Lifting-Wing Quadcopters Subject to Drag and\n  Lift for Accurate Tracking\n\n  In this paper, we propose an effective unified control law for accurately\ntracking agile trajectories for lifting-wing quadcopters with different\ninstallation angles, which have the capability of vertical takeoff and landing\n(VTOL) as well as high-speed cruise flight. First, we derive a differential\nflatness transform for the lifting-wing dynamics with a nonlinear model under\ncoordinated turn condition. To increase the tracking performance on agile\ntrajectories, the proposed controller incorporates the state and input\nvariables calculated from differential flatness as feedforward. In particular,\nthe jerk, the 3-order derivative of the trajectory, is converted into angular\nvelocity as a feedforward item, which significantly improves the system\nbandwidth. At the same time, feedback and feedforward outputs are combined to\ndeal with external disturbances and model mismatch. The control algorithm has\nbeen thoroughly evaluated in the outdoor flight tests, which show that it can\nachieve accurate trajectory tracking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.05154,regular,pre_llm,2022,12,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Optimal Control for Quadruped Locomotion using LTV MPC\n\n  This paper presents a state-of-the-art optimal controller for quadruped\nlocomotion. The robot dynamics is represented using a single rigid body (SRB)\nmodel. A linear time-varying model predictive controller (LTV MPC) is proposed\nby using linearization schemes. Simulation results show that the LTV MPC can\nexecute various gaits, such as trot and crawl, and is capable of tracking\ndesired reference trajectories even under unknown external disturbances. The\nLTV MPC is implemented as a quadratic program using qpOASES through the CasADi\ninterface at 50 Hz. The proposed MPC can reach up to 1 m/s top speed with an\nacceleration of 0.5 m/s2 executing a trot gait. The implementation is available\nat https:// github.com/AndrewZheng-1011/Quad_ConvexMPC\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.05966,regular,pre_llm,2022,12,"{'ai_likelihood': 5.099508497450087e-06, 'text': 'Comparison between Docker and Kubernetes based Edge Architectures for\n  Enabling Remote Model Predictive Control for Aerial Robots\n\n  Edge computing is becoming more and more popular among researchers who seek\nto take advantage of the edge resources and the minimal time delays, in order\nto run their robotic applications more efficiently. Recently, many edge\narchitectures have been proposed, each of them having their advantages and\ndisadvantages, depending on each application. In this work, we present two\ndifferent edge architectures for controlling the trajectory of an Unmanned\nAerial Vehicle (UAV). The first architecture is based on docker containers and\nthe second one is based on kubernetes, while the main framework for operating\nthe robot is the Robotic Operating System (ROS). The efficiency of the overall\nproposed scheme is being evaluated through extended simulations for comparing\nthe two architectures and the overall results obtained.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.00198,regular,pre_llm,2022,12,"{'ai_likelihood': 5.39753172132704e-06, 'text': 'An Integrated Visual System for Unmanned Aerial Vehicles Tracking and\n  Landing on the Ground Vehicles\n\n  The vision of unmanned aerial vehicles is very significant for UAV-related\napplications such as search and rescue, landing on a moving platform, etc. In\nthis work, we have developed an integrated system for the UAV landing on the\nmoving platform, and the UAV object detection with tracking in the complicated\nenvironment. Firstly, we have proposed a robust LoG-based deep neural network\nfor object detection and tracking, which has great advantages in robustness to\nobject scale and illuminations compared with typical deep network-based\napproaches. Then, we have also improved based on the original Kalman filter and\ndesigned an iterative multi-model-based filter to tackle the problem of unknown\ndynamics in real circumstances of motion estimations. Next, we implemented the\nwhole system and do ROS Gazebo-based testing in two complicated circumstances\nto verify the effectiveness of our design. Finally, we have deployed the\nproposed detection, tracking, and motion estimation strategies into real\napplications to do UAV tracking of a pillar and obstacle avoidance. It is\ndemonstrated that our system shows great accuracy and robustness in real\napplications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.06923,regular,pre_llm,2022,12,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Know What You Don't Know: Consistency in Sliding Window Filtering with\n  Unobservable States Applied to Visual-Inertial SLAM (Extended Version)\n\n  Estimation algorithms, such as the sliding window filter, produce an estimate\nand uncertainty of desired states. This task becomes challenging when the\nproblem involves unobservable states. In these situations, it is critical for\nthe algorithm to ``know what it doesn't know'', meaning that it must maintain\nthe unobservable states as unobservable during algorithm deployment. This\nletter presents general requirements for maintaining consistency in sliding\nwindow filters involving unobservable states. The value of these requirements\nfor designing navigation solutions is experimentally shown within the context\nof visual-inertial SLAM making use of IMU preintegration.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.12523,regular,pre_llm,2022,12,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Lie Group Formulation and Sensitivity Analysis for Shape Sensing of\n  Variable Curvature Continuum Robots with General String Encoder Routing\n\n  This paper considers a combination of actuation tendons and measurement\nstrings to achieve accurate shape sensing and direct kinematics of continuum\nrobots. Assuming general string routing, a methodical Lie group formulation for\nthe shape sensing of these robots is presented. The shape kinematics is\nexpressed using arc-length-dependent curvature distributions parameterized by\nmodal functions, and the Magnus expansion for Lie group integration is used to\nexpress the shape as a product of exponentials. The tendon and string length\nkinematic constraints are solved for the modal coefficients and the\nconfiguration space and body Jacobian are derived. The noise amplification\nindex for the shape reconstruction problem is defined and used for optimizing\nthe string/tendon routing paths, and a planar simulation study shows the\nminimal number of strings/tendons needed for accurate shape reconstruction. A\ntorsionally stiff continuum segment is used for experimental evaluation,\ndemonstrating mean (maximal) end-effector absolute position error of less than\n2% (5%) of total length. Finally, a simulation study of a torsionally compliant\nsegment demonstrates the approach for general deflections and string routings.\nWe believe that the methods of this paper can benefit the design process,\nsensing and control of continuum and soft robots.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.0872,regular,pre_llm,2022,12,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Imitation Learning based Auto-Correction of Extrinsic Parameters for A\n  Mixed-Reality Setup\n\n  In this paper, we discuss an imitation learning based method for reducing the\ncalibration error for a mixed reality system consisting of a vision sensor and\na projector. Unlike a head mounted display, in this setup, augmented\ninformation is available to a human subject via the projection of a scene into\nthe real world. Inherently, the camera and projector need to be calibrated as a\nstereo setup to project accurate information in 3D space. Previous calibration\nprocesses require multiple recording and parameter tuning steps to achieve the\ndesired calibration, which is usually time consuming process. In order to avoid\nsuch tedious calibration, we train a CNN model to iteratively correct the\nextrinsic offset given a QR code and a projected pattern. We discuss the\noverall system setup, data collection for training, and results of the\nauto-correction model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.04298,regular,pre_llm,2022,12,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Real-time Sampling-based Model Predictive Control based on Reverse\n  Kullback-Leibler Divergence and Its Adaptive Acceleration\n\n  Sampling-based model predictive control (MPC) can be applied to versatile\nrobotic systems. However, the real-time control with it is a big challenge due\nto its unstable updates and poor convergence. This paper tackles this challenge\nwith a novel derivation from reverse Kullback-Leibler divergence, which has a\nmode-seeking behavior and is likely to find one of the sub-optimal solutions\nearly. With this derivation, a weighted maximum likelihood estimation with\npositive/negative weights is obtained, solving by mirror descent (MD)\nalgorithm. While the negative weights eliminate unnecessary actions, that\nrequires to develop a practical implementation that avoids the interference\nwith positive/negative updates based on rejection sampling. In addition,\nalthough the convergence of MD can be accelerated with Nesterov's acceleration\nmethod, it is modified for the proposed MPC with a heuristic of a step size\nadaptive to the noise estimated in update amounts. In the real-time\nsimulations, the proposed method can solve more tasks statistically than the\nconventional method and accomplish more complex tasks only with a CPU due to\nthe improved acceleration. In addition, its applicability is also demonstrated\nin a variable impedance control of a force-driven mobile robot.\nhttps://youtu.be/D8bFMzct1XM\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.02231,regular,pre_llm,2022,12,"{'ai_likelihood': 1.0927518208821615e-05, 'text': ""TMSTC*: A Turn-minimizing Algorithm For Multi-robot Coverage Path\n  Planning\n\n  Coverage path planning is a major application for mobile robots, which\nrequires robots to move along a planned path to cover the entire map. For\nlarge-scale tasks, coverage path planning benefits greatly from multiple\nrobots. In this paper, we describe Turn-minimizing Multirobot Spanning Tree\nCoverage Star(TMSTC*), an improved multirobot coverage path planning (mCPP)\nalgorithm based on the MSTC*. Our algorithm partitions the map into minimum\nbricks as tree's branches and thereby transforms the problem into finding the\nmaximum independent set of bipartite graph. We then connect bricks with greedy\nstrategy to form a tree, aiming to reduce the number of turns of corresponding\ncircumnavigating coverage path. Our experimental results show that our approach\nenables multiple robots to make fewer turns and thus complete terrain coverage\ntasks faster than other popular algorithms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.12597,review,pre_llm,2022,12,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'Deep Causal Learning for Robotic Intelligence\n\n  This invited review discusses causal learning in the context of robotic\nintelligence. The paper introduced the psychological findings on causal\nlearning in human cognition, then it introduced the traditional statistical\nsolutions on causal discovery and causal inference. The paper reviewed recent\ndeep causal learning algorithms with a focus on their architectures and the\nbenefits of using deep nets and discussed the gap between deep causal learning\nand the needs of robotic intelligence.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.03545,regular,pre_llm,2022,12,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Smoothly Connected Preemptive Impact Reduction and Contact Impedance\n  Control\n\n  This study proposes novel control methods that lower impact force by\npreemptive movement and smoothly transition to conventional contact impedance\ncontrol. These suggested techniques are for force control-based robots and\nposition/velocity control-based robots, respectively. Strong impact forces have\na negative influence on multiple robotic tasks. Recently, preemptive impact\nreduction techniques that expand conventional contact impedance control by\nusing proximity sensors have been examined. However, a seamless transition from\nimpact reduction to contact impedance control has not yet been accomplished.\nThe proposed methods utilize a serial combined impedance control framework to\nsolve this problem. The preemptive impact reduction feature can be added to the\nalready implemented impedance controller because the parameter design is\ndivided into impact reduction and contact impedance control. There is no\nundesirable contact force during the transition. Furthermore, even though the\npreemptive impact reduction employs a crude optical proximity sensor, the\ninfluence of reflectance is minimized using a virtual viscous force. Analyses\nand real-world experiments confirm these benefits.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2212.06764,regular,pre_llm,2022,12,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Single-Level Differentiable Contact Simulation\n\n  We present a differentiable formulation of rigid-body contact dynamics for\nobjects and robots represented as compositions of convex primitives. Existing\noptimization-based approaches simulating contact between convex primitives rely\non a bilevel formulation that separates collision detection and contact\nsimulation. These approaches are unreliable in realistic contact simulation\nscenarios because isolating the collision detection problem introduces contact\nlocation non-uniqueness. Our approach combines contact simulation and collision\ndetection into a unified single-level optimization problem. This disambiguates\nthe collision detection problem in a physics-informed manner. Compared to\nprevious differentiable simulation approaches, our formulation features\nimproved simulation robustness and a reduction in computational complexity by\nmore than an order of magnitude. We illustrate the contact and collision\ndifferentiability on a robotic manipulation task requiring\noptimization-through-contact. We provide a numerically efficient implementation\nof our formulation in the Julia language called Silico.jl.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
