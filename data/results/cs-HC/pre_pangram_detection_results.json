[
  {
    "arxiv_id":2001.06737,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000057618,
      "text":"Developing and Validating an Interactive Training Tool for Inferring 2D\n  Cross-Sections of Complex 3D Structures\n\n  Understanding 2D cross-sections of 3D structures is a crucial skill in many\ndisciplines, from geology to medical imaging. Cross-section inference in the\ncontext of 3D structures requires a complex set of spatial\/visualization skills\nincluding mental rotation, spatial structure understanding, and viewpoint\nprojection. Prior studies show that experts differ from novices in these, and\nother, skill dimensions. Building on a previously developed model that\nhierarchically characterizes the specific spatial sub-skills needed for this\ntask, we have developed the first domain-agnostic, computer-based training tool\nfor cross-section understanding of complex 3D structures. We demonstrate, in an\nevaluation with 60 participants, that this interactive tool is effective for\nincreasing cross-section inference skills for a variety of structures, from\nsimple primitive ones to more complex biological structures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03012,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Predicting Student Performance in Interactive Online Question Pools\n  Using Mouse Interaction Features\n\n  Modeling student learning and further predicting the performance is a\nwell-established task in online learning and is crucial to personalized\neducation by recommending different learning resources to different students\nbased on their needs. Interactive online question pools (e.g., educational game\nplatforms), an important component of online education, have become\nincreasingly popular in recent years. However, most existing work on student\nperformance prediction targets at online learning platforms with a\nwell-structured curriculum, predefined question order and accurate knowledge\ntags provided by domain experts. It remains unclear how to conduct student\nperformance prediction in interactive online question pools without such\nwell-organized question orders or knowledge tags by experts. In this paper, we\npropose a novel approach to boost student performance prediction in interactive\nonline question pools by further considering student interaction features and\nthe similarity between questions. Specifically, we introduce new features\n(e.g., think time, first attempt, and first drag-and-drop) based on student\nmouse movement trajectories to delineate students' problem-solving details. In\naddition, heterogeneous information network is applied to integrating students'\nhistorical problem-solving information on similar questions, enhancing student\nperformance predictions on a new question. We evaluate the proposed approach on\nthe dataset from a real-world interactive question pool using four typical\nmachine learning models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.06423,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"InChorus: Designing Consistent Multimodal Interactions for Data\n  Visualization on Tablet Devices\n\n  While tablet devices are a promising platform for data visualization,\nsupporting consistent interactions across different types of visualizations on\ntablets remains an open challenge. In this paper, we present multimodal\ninteractions that function consistently across different visualizations,\nsupporting common operations during visual data analysis. By considering\nstandard interface elements (e.g., axes, marks) and grounding our design in a\nset of core concepts including operations, parameters, targets, and\ninstruments, we systematically develop interactions applicable to different\nvisualization types. To exemplify how the proposed interactions collectively\nfacilitate data exploration, we employ them in a tablet-based system, InChorus\nthat supports pen, touch, and speech input. Based on a study with 12\nparticipants performing replication and fact-checking tasks with InChorus, we\ndiscuss how participants adapted to using multimodal input and highlight\nconsiderations for future multimodal visualization systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"GUIComp: A GUI Design Assistant with Real-Time, Multi-Faceted Feedback\n\n  Users may face challenges while designing graphical user interfaces, due to a\nlack of relevant experience and guidance. This paper aims to investigate the\nissues that users with no experience face during the design process, and how to\nresolve them. To this end, we conducted semi-structured interviews, based on\nwhich we built a GUI prototyping assistance tool called GUIComp. This tool can\nbe connected to GUI design software as an extension, and it provides real-time,\nmulti-faceted feedback on a user's current design. Additionally, we conducted\ntwo user studies, in which we asked participants to create mobile GUIs with or\nwithout GUIComp, and requested online workers to assess the created GUIs. The\nexperimental results show that GUIComp facilitated iterative design and the\nparticipants with GUIComp had better a user experience and produced more\nacceptable designs than those who did not.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000006126,
      "text":"Informing the Design of Privacy-Empowering Tools for the Connected Home\n\n  Connected devices in the home represent a potentially grave new privacy\nthreat due to their unfettered access to the most personal spaces in people's\nlives. Prior work has shown that despite concerns about such devices, people\noften lack sufficient awareness, understanding, or means of taking effective\naction. To explore the potential for new tools that support such needs directly\nwe developed Aretha, a privacy assistant technology probe that combines a\nnetwork disaggregator, personal tutor, and firewall, to empower end-users with\nboth the knowledge and mechanisms to control disclosures from their homes. We\ndeployed Aretha in three households over six weeks, with the aim of\nunderstanding how this combination of capabilities might enable users to gain\nawareness of data disclosures by their devices, form educated privacy\npreferences, and to block unwanted data flows. The probe, with its novel\naffordances-and its limitations-prompted users to co-adapt, finding new control\nmechanisms and suggesting new approaches to address the challenge of regaining\nprivacy in the connected home.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.07546,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000068214,
      "text":"Exploring an Application of Virtual Reality for Early Detection of\n  Dementia\n\n  Facing the severe global dementia problem, an exploration was conducted\nadopting the technology of virtual reality (VR). This report lays a technical\nfoundation for further research project \"Early Detection of Dementia Using\nTesting Tools in VR Environment\", which illustrates the process of developing a\nVR application using Unity 3D software on Oculus Go. This preliminary\nexploration is composed of three steps, including 3D virtual scene\nconstruction, VR interaction design and monitoring. The exploration was\nrecorded to provide basic technical guidance and detailed method for subsequent\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03271,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000033445,
      "text":"Du Bois Wrapped Bar Chart: Visualizing categorical data with\n  disproportionate values\n\n  We propose a visualization technique, Du Bois wrapped bar chart, inspired by\nwork of W.E.B Du Bois. Du Bois wrapped bar charts enable better large-to-small\nbar comparison by wrapping large bars over a certain threshold. We first\npresent two crowdsourcing experiments comparing wrapped and standard bar charts\nto evaluate (1) the benefit of wrapped bars in helping participants identify\nand compare values; (2) the characteristics of data most suitable for wrapped\nbars. In the first study (n=98) using real-world datasets, we find that wrapped\nbar charts lead to higher accuracy in identifying and estimating ratios between\nbars. In a follow-up study (n=190) with 13 simulated datasets, we find\nparticipants were consistently more accurate with wrapped bar charts when\ncertain category values are disproportionate as measured by entropy and\nH-spread. Finally, in an in-lab study, we investigate participants' experience\nand strategies, leading to guidelines for when and how to use wrapped bar\ncharts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.11571,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Analysis of the Bergen-Belsen VR\/AR application by means of the Virtual\n  Subjectiveness Model\n\n  We test the usefulness of Virtual Subjectiveness (Par\\'es and Par\\'es, 2006)\nas an analytical model for AMVR projects by means of the evaluation of the\nBergen-Belsen VR\/AR application. This application allows users to retrieve\ngeolocated historical data through 3D architectural reconstructions, while\nexploring the site of the former concentration camp. Having analyzed the\ncontext of development of this application, its interface, mappings and\ninteraction behaviors, and their interrelation in time, we found the Virtual\nSubjectiveness model to be an adequate paradigm to make a structured evaluation\nof the technical and conceptual levels of the chosen VR\/AR application.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.08301,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"How to Support Users in Understanding Intelligent Systems? Structuring\n  the Discussion\n\n  The opaque nature of many intelligent systems violates established usability\nprinciples and thus presents a challenge for human-computer interaction.\nResearch in the field therefore highlights the need for transparency,\nscrutability, intelligibility, interpretability and explainability, among\nothers. While all of these terms carry a vision of supporting users in\nunderstanding intelligent systems, the underlying notions and assumptions about\nusers and their interaction with the system often remain unclear. We review the\nliterature in HCI through the lens of implied user questions to synthesise a\nconceptual framework integrating user mindsets, user involvement, and knowledge\noutcomes to reveal, differentiate and classify current notions in prior work.\nThis framework aims to resolve conceptual ambiguity in the field and enables\nresearchers to clarify their assumptions and become aware of those made in\nprior work. We thus hope to advance and structure the dialogue in the HCI\nresearch community on supporting users in understanding intelligent systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03899,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Wearable Haptics for Remote Social Walking\n\n  Walking is an essential activity for a healthy life, which becomes less\ntiring and more enjoyable if done together. Common difficulties we have in\nperforming sufficient physical exercise, for instance the lack of motivation,\ncan be overcome by exploiting its social aspect. However, our lifestyle\nsometimes makes it very difficult to find time together with others who live\nfar away from us to go for a walk. In this paper we propose a novel system\nenabling people to have a 'remote social walk' by streaming the gait cadence\nbetween two persons walking in different places, increasing the sense of mutual\npresence. Vibrations provided at the users' ankles display the partner's\nsensation perceived during the heel-strike. In order to achieve the\naforementioned goal in a two users experiment, we envisaged a four-step\nincremental validation process: i) a single walker has to adapt the cadence\nwith a virtual reference generated by a software; ii) a single user is tasked\nto follow a predefined time varying gait cadence; iii) a leader-follower\nscenario in which the haptic actuation is mono-directional; iv) a peer-to-peer\ncase with bi-directional haptic communication. Careful experimental validation\nwas conducted involving a total of 50 people, which confirmed the efficacy of\nour system in perceiving the partners' gait cadence in each of the proposed\nscenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.02306,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Examining Potential Usability and Health Beliefs Among Young Adults\n  Using a Conversational Agent for HPV Vaccine Counseling\n\n  The human papillomavirus (HPV) vaccine is the most effective way to prevent\nHPV-related cancers. Integrating provider vaccine counseling is crucial to\nimproving HPV vaccine completion rates. Automating the counseling experience\nthrough a conversational agent could help improve HPV vaccine coverage and\nreduce the burden of vaccine counseling for providers. In a previous study, we\ntested a simulated conversational agent that provided HPV vaccine counseling\nfor parents using the Wizard of OZ protocol. In the current study, we assessed\nthe conversational agent among young college adults (n=24), a population that\nmay have missed the HPV vaccine during their adolescence when vaccination is\nrecommended. We also administered surveys for system and voice usability, and\nfor health beliefs concerning the HPV vaccine. Participants perceived the agent\nto have high usability that is slightly better or equivalent to other voice\ninteractive interfaces, and there is some evidence that the agent impacted\ntheir beliefs concerning the harms, uncertainty, and risk denials for the HPV\nvaccine. Overall, this study demonstrates the potential for conversational\nagents to be an impactful tool for health promotion endeavors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09604,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Factors Influencing Perceived Fairness in Algorithmic Decision-Making:\n  Algorithm Outcomes, Development Procedures, and Individual Differences\n\n  Algorithmic decision-making systems are increasingly used throughout the\npublic and private sectors to make important decisions or assist humans in\nmaking these decisions with real social consequences. While there has been\nsubstantial research in recent years to build fair decision-making algorithms,\nthere has been less research seeking to understand the factors that affect\npeople's perceptions of fairness in these systems, which we argue is also\nimportant for their broader acceptance. In this research, we conduct an online\nexperiment to better understand perceptions of fairness, focusing on three sets\nof factors: algorithm outcomes, algorithm development and deployment\nprocedures, and individual differences. We find that people rate the algorithm\nas more fair when the algorithm predicts in their favor, even surpassing the\nnegative effects of describing algorithms that are very biased against\nparticular demographic groups. We find that this effect is moderated by several\nvariables, including participants' education level, gender, and several aspects\nof the development procedure. Our findings suggest that systems that evaluate\nalgorithmic fairness through users' feedback must consider the possibility of\noutcome favorability bias.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.06579,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Towards a Virtual Reality Home IoT Network Visualizer\n\n  We present an IoT home network visualizer that utilizes virtual reality (VR).\nThis prototype demonstrates the potential that VR has to aid in the\nunderstanding of home IoT networks. This is particularly important due the\nincreased number of household devices now connected to the Internet. This\nprototype is able to function in a standard display or a VR headset. A\nprototype was developed to aid in the understanding of home IoT networks for\nhomeowners.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00732,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Phoenixmap: An Abstract Approach to Visualize 2D Spatial Distributions\n\n  The multidimensional nature of spatial data poses a challenge for\nvisualization. In this paper, we introduce Phoenixmap, a simple abstract\nvisualization method to address the issue of visualizing multiple spatial\ndistributions at once. The Phoenixmap approach starts by identifying the\nenclosed outline of the point collection, then assigns different widths to\noutline segments according to the segments' corresponding inside regions. Thus,\none 2D distribution is represented as an outline with varied thicknesses.\nPhoenixmap is capable of overlaying multiple outlines and comparing them across\ncategories of objects in a 2D space. We chose heatmap as a benchmark spatial\nvisualization method and conducted user studies to compare performances among\nPhoenixmap, heatmap, and dot distribution map. Based on the analysis and\nparticipant feedback, we demonstrate that Phoenixmap 1) allows users to\nperceive and compare spatial distribution data efficiently; 2) frees up\ngraphics space with a concise form that can provide visualization design\npossibilities like overlapping; and 3) provides a good quantitative perceptual\nestimating capability given the proper legends. Finally, we discuss several\npossible applications of Phoenixmap and present one visualization of multiple\nspecies of birds' active regions in a nature preserve.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04352,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000062916,
      "text":"Button Simulation and Design via FDVV Models\n\n  Designing a push-button with desired sensation and performance is challenging\nbecause the mechanical construction must have the right response\ncharacteristics. Physical simulation of a button's force-displacement (FD)\nresponse has been studied to facilitate prototyping; however, the simulations'\nscope and realism have been limited. In this paper, we extend FD modeling to\ninclude vibration (V) and velocity-dependence characteristics (V). The\nresulting FDVV models better capture tactility characteristics of buttons,\nincluding snap. They increase the range of simulated buttons and the perceived\nrealism relative to FD models. The paper also demonstrates methods for\nobtaining these models, editing them, and simulating accordingly. This\nend-to-end approach enables the analysis, prototyping, and optimization of\nbuttons, and supports exploring designs that would be hard to implement\nmechanically.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.06798,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000052651,
      "text":"Creativity on Paid Crowdsourcing Platforms\n\n  General-purpose crowdsourcing platforms are increasingly being harnessed for\ncreative work. The platforms' potential for creative work is clearly\nidentified, but the workers' perspectives on such work have not been\nextensively documented. In this paper, we uncover what the workers have to say\nabout creative work on paid crowdsourcing platforms. Through a quantitative and\nqualitative analysis of a questionnaire launched on two different crowdsourcing\nplatforms, our results revealed clear differences between the workers on the\nplatforms in both preferences and prior experience with creative work. We\nidentify common pitfalls with creative work on crowdsourcing platforms, provide\nrecommendations for requesters of creative work, and discuss the meaning of our\nfindings within the broader scope of creativity-oriented research. To the best\nof our knowledge, we contribute the first extensive worker-oriented study of\ncreative work on paid crowdsourcing platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.02705,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000021259,
      "text":"Understanding the Use of Crisis Informatics Technology among Older\n  Adults\n\n  Mass emergencies increasingly pose significant threats to human life, with a\ndisproportionate burden being incurred by older adults. Research has explored\nhow mobile technology can mitigate the effects of mass emergencies. However,\nless work has examined how mobile technologies support older adults during\nemergencies, considering their unique needs. To address this research gap, we\ninterviewed 16 older adults who had recent experience with an emergency\nevacuation to understand the perceived value of using mobile technology during\nemergencies. We found that there was a lack of awareness and engagement with\nexisting crisis apps. Our findings characterize the ways in which our\nparticipants did and did not feel crisis informatics tools address human\nvalues, including basic needs and esteem needs. We contribute an understanding\nof how older adults used mobile technology during emergencies and their\nperspectives on how well such tools address human values.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00091,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Using Inaudible Audio to Improve Indoor-Localization- and\n  Proximity-Aware Intelligent Applications\n\n  While it is often critical for indoor-location- and proximity-aware\napplications to know whether a user is in a space or not (e.g., a specific room\nor office), a key challenge is that the difference between standing on one side\nor another of a doorway or wall is well within the error range of most RF-based\napproaches. In this work, we address this challenge by augmenting RF-based\nlocalization and proximity detection with active ultrasonic sensing, taking\nadvantage of the limited propagation of sound waves. This simple and\ncost-effective approach can allow, for example, a Bluetooth smart-lock to\ndiscern whether a user is inside or outside their home in order to lock or\nunlock doors automatically. We describe a configurable architecture for our\nsolution and present experiments that validate this approach but also\ndemonstrate that different user behavior and application needs can impact\nsystem configuration decisions. Finally, we describe applications that could\nbenefit from our solution and address privacy concerns.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.01824,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Foveated Haptic Gaze\n\n  As digital worlds become ubiquitous via video games, simulations, virtual and\naugmented reality, people with disabilities who cannot access those worlds are\nbecoming increasingly disenfranchised. More often than not the design of these\nenvironments focuses on vision, making them inaccessible in whole or in part to\npeople with visual impairments. Accessible games and visual aids have been\ndeveloped but their lack of prevalence or unintuitive interfaces make them\nimpractical for daily use. To address this gap, we present Foveated Haptic\nGaze, a method for conveying visual information via haptics that is intuitive\nand designed for interacting with real-time 3-dimensional environments. To\nvalidate our approach we developed a prototype of the system along with a\nsimplified first-person shooter game. Lastly we present encouraging user study\nresults of both sighted and blind participants using our system to play the\ngame with no visual feedback.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03352,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Optimal Sensor Position for a Computer Mouse\n\n  Computer mice have their displacement sensors in various locations (center,\nfront, and rear). However, there has been little research into the effects of\nsensor position or on engineering approaches to exploit it. This paper first\ndiscusses the mechanisms via which sensor position affects mouse movement and\nreports the results from a study of a pointing task in which the sensor\nposition was systematically varied. Placing the sensor in the center turned out\nto be the best compromise: improvements over front and rear were in the 11--14%\nrange for throughput and 20--23% for path deviation. However, users varied in\ntheir personal optima. Accordingly, variable-sensor-position mice are then\npresented, with a demonstration that high accuracy can be achieved with two\nstatic optical sensors. A virtual sensor model is described that allows\nsoftware-side repositioning of the sensor. Individual-specific calibration\nshould yield an added 4% improvement in throughput over the default center\nposition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.04263,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Designing a Holistic At-Home Learning Aid for Autism\n\n  In recent years, much focus has been put on employing technology to make\nnovel behavioural aids for those with autism. Most of these are digital\nadaptations of tools used in standard behavioural therapy to enforce normative\nskills. These digital counterparts are often used outside of both the larger\ntherapeutic context and the real world, in which the learned skills might\napply. To address this, we are designing a system of automatic expression\nrecognition on wearable devices that integrates directly into the families\ndaily social interactions, to give children and their caregivers the tools and\ninformation they need to design their own holistic therapy. In order to develop\na tool that will be truly useful to families, we proactively include children\nwith autism and their families as co-designers in the development process. By\nproviding an app and interface with interchangeable social feedback options, we\naim to produce a framework for therapy that folds into their daily lives,\ntailored to their specific needs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11596,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000077817,
      "text":"An Optimal Control Model of Mouse Pointing Using the LQR\n\n  In this paper we explore the Linear-Quadratic Regulator (LQR) to model\nmovement of the mouse pointer. We propose a model in which users are assumed to\nbehave optimally with respect to a certain cost function. Users try to minimize\nthe distance of the mouse pointer to the target smoothly and with minimal\neffort, by simultaneously minimizing the jerk of the movement. We identify\nparameters of our model from a dataset of reciprocal pointing with the mouse.\nWe compare our model to the classical minimum-jerk and second-order lag models\non data from 12 users with a total of 7702 movements. Our results show that our\napproach explains the data significantly better than either of these previous\nmodels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.1202,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Impact of Visuomotor Feedback on the Embodiment of Virtual Hands\n  Detached from the Body\n\n  It has been shown that mere observation of body discontinuity leads to\ndiminished body ownership. However, the impact of body discontinuity has mainly\nbeen investigated in conditions where participants observe a collocated static\nvirtual body from a first-person perspective. This study explores the influence\nof body discountinuity on the sense of embodiment, when rich visuomotor\ncorrelations between a real and an artificial virtual body are established. In\ntwo experiments, we evaluated body ownership and motor performance, when\nparticipants interacted in virtual reality either using virtual hands connected\nor disconnected from a body. We found that even under the presence of congruent\nvisuomotor feedback, mere observation of body discontinuity resulted in\ndiminished embodiment. Contradictory evidence was found in relation to motor\nperformance, where further research is needed to understand the role of visual\nbody discontinuity in motor tasks. Preliminary findings on physiological\nreactions to a threat were also assessed, indicating that body visual\ndiscontinuity does not differently impact threat-related skin conductance\nresponses. The present results are in accordance with past evidence showing\nthat body discontinuity negatively impacts embodiment. However, further\nresearch is needed to understand the influence of visuomotor feedback and body\nmorphological congruency on motor performance and threat-related physiological\nreactions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00511,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Investigating usability of MSstatsQC software\n\n  MSstatsQC [3] is an open-source software that provides longitudinal system\nsuitability monitoring tools in the form of control charts for proteomic\nexperiments. It includes simultaneous tools for the mean and dispersion of\nsuitability metrics and presents alternative methods of monitoring through\ndifferent tabs that are designed in the interface. This research focuses on\ninvestigating the usability of MSstatsQC software and the interpretability of\nthe designed plots. In this study, we ask 4 test users, from the proteomics\nfield, to complete a series of tasks and questionnaires. The tasks are designed\nto test the usability of the software in terms of importing data files,\nselecting appropriate metrics, guide set, and peptides, and finally creating\ndecision rules (tasks 1 and 3 in appendix). The questionnaires ask about\ninterpretability of the plots including control charts, box plots, heat maps,\nriver plots, and radar plots (tasks 1 and 4 in appendix). The goal of the\nquestions is to determine if the test users understand the plots and can\ninterpret them. Results show limitations in usability and plot\ninterpretability, especially in the data import section. We suggest the\nfollowing modifications. I) providing conspicuous guides close to the window\nrelated to up-loading a datafile as well as providing error messages that\npop-up when the data set has a wrong format II) providing plot descriptions,\nhints to interpret plots, plot titles and appropriate axis labels, and, III)\nNumbering tabs to show the flow of procedures in the software.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.10594,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"On-Orbit Operations Simulator for Workload Measurement during\n  Telerobotic Training\n\n  Training for telerobotic systems often makes heavy use of simulated\nplatforms, which ensure safe operation during the learning process. Outer space\nis one domain in which such a simulated training platform would be useful, as\nOn-Orbit Operations (O3) can be costly, inefficient, or even dangerous if not\nperformed properly. In this paper, we present a new telerobotic training\nsimulator for the Canadarm2 on the International Space Station (ISS), which is\nable to modulate workload through the addition of confounding factors such as\nlatency, obstacles, and time pressure. In addition, multimodal physiological\ndata is collected from subjects as they perform a task from the simulator under\nthese different conditions. As most current workload measures are subjective,\nwe analyse objective measures from the simulator and EEG data that can provide\na reliable measure. ANOVA of task data revealed which simulator-based\nperformance measures could predict the presence of latency and time pressure.\nFurthermore, EEG classification using a Riemannian classifier and\nLeave-One-Subject-Out cross-validation showed promising classification\nperformance and allowed for comparison of different channel configurations and\npreprocessing methods. Additionally, Riemannian distance and beta power of EEG\ndata were investigated as potential cross-trial and continuous workload\nmeasures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03037,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000052982,
      "text":"Above Surface Interaction for Multiscale Navigation in Mobile Virtual\n  Reality\n\n  Virtual Reality enables the exploration of large information spaces. In\nphysically constrained spaces such as airplanes or buses, controller-based or\nmid-air interaction in mobile Virtual Reality can be challenging. Instead, the\ninput space on and above touch-screen enabled devices such as smartphones or\ntablets could be employed for Virtual Reality interaction in those spaces.\n  In this context, we compared an above surface interaction technique with\ntraditional 2D on-surface input for navigating large planar information spaces\nsuch as maps in a controlled user study (n = 20). We find that our proposed\nabove surface interaction technique results in significantly better performance\nand user preference compared to pinch-to-zoom and drag-to-pan when navigating\nplanar information spaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.06308,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"How to democratize Internet of Things devices. A participatory design\n  research\n\n  The global introduction of affordable Internet of Things (IoT) devices offers\nan opportunity to empower a large variety of users with different needs.\nHowever, many off-the-shelf digital products are still not widely adopted by\npeople who are hesitant technology users or by older adults, notwithstanding\nthat the design and user-interaction of these devices is recognized to be\nuser-friendly. In view of the potential of IoT-based devices, how can we reduce\nthe obstacles of a cohort with low digital literacy and technology anxiety and\nenable them to be equal participants in the digitalized world? This article\nshows the method and results achieved in a community-stakeholder workshop,\ndeveloped through the participatory design methodology, aiming at brainstorming\nproblems and scenarios through a focus group and a structured survey. The\nresearch activity focused on understanding factors to increase the usability of\noff-the-shelf IoT devices for hesitant users and identify strategies for\nimproving digital literacy and reducing technology anxiety. A notable result\nwas a series of feedback items pointing to the importance of creating learning\nresources to support individuals with different abilities, age, gender\nexpression, to better adopt off-the-shelf IoT-based solutions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.08956,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Toward An Interdisciplinary Methodology to Solve New (Old)\n  Transportation Problems\n\n  The rising availability of digital traces provides a fertile ground for new\nsolutions to both, new and old problems in cities. Even though a massive data\nset analyzed with Data Science methods may provide a powerful solution to a\nproblem, its adoption by relevant stakeholders is not guaranteed, due to\nadoption blockers such as lack of interpretability and transparency. In this\ncontext, this paper proposes a preliminary methodology toward bridging two\ndisciplines, Data Science and Transportation, to solve urban problems with\nmethods that are suitable for adoption. The methodology is defined by four\nsteps where people from both disciplines go from algorithm and model definition\nto the building of a potentially adoptable solution. As case study, we describe\nhow this methodology was applied to define a model to infer commuting trips\nwith mode of transportation from mobile phone data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.06655,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Concurrent Crossmodal Feedback Assists Target-searching: Displaying\n  Distance Information Through Visual, Auditory and Haptic Modalities\n\n  Humans sense of distance depends on the integration of multi sensory cues.\nThe incoming visual luminance, auditory pitch and tactile vibration could all\ncontribute to the ability of distance judgement. This ability can be enhanced\nif the multimodal cues are associated in a congruent manner, a phenomenon has\nbeen referred to as Crossmodal correspondences. In the context of multi-sensory\ninteraction, whether and how such correspondences influence information\nprocessing with continuous motor engagement, particularly for target searching\nactivities, has rarely been investigated. This paper presents an experimental\nuser study to address this question. We built a target-searching application\nbased on a Table-top, displayed the unimodal and Crossmodal distance cues\nconcurrently responding to peoples searching movement, measured task\nperformance through kinematic evaluation. We find that the Crossmodal display\nan audio display lead to improved searching efficiency and accuracy. More\ninterestingly, this improvement is confirmed by kinematic analysis, which also\nunveiled the underlying movement features that could account for this\nimprovement. We discussed how these findings could shed lights on the design of\nassistive technology and of other multi sensory interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07576,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Presence in VR experiences -- an empirical cost-benefit-analysis\n\n  Virtual reality (VR) is on the edge of getting a mainstream platform for\ngaming, education and product design. The feeling of being present in the\nvirtual world is influenced by many factors and even more intriguing a single\nnegative influence can destroy the illusion that was created with a lot of\neffort by other measures. Therefore, it is crucial to have a balance between\nthe influencing factors, know the importance of the factors and have a good\nestimation of how much effort it takes to bring each factor to a certain level\nof fidelity. This paper collects influencing factors discussed in literature,\nanalyses the immersion of current off-the-shelf VR-solutions and presents\nresults from an empirical study on efforts and benefits from certain aspects\ninfluencing presence in VR experiences. It turns out, that sometimes delivering\nhigh fidelity is easier to achieve than medium fidelity and for other aspects\nit is worthwhile investing more effort to achieve higher fidelity to improve\npresence a lot.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09949,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"Path Outlines: Browsing Path-Based Summaries of Knowledge Graphs\n\n  Knowledge Graphs have become a ubiquitous technology powering search engines,\nrecommender systems, connected objects, corporate knowledge management and Open\nData. They rely on small units of information named triples that can be\ncombined to form higher level statements across datasets following information\nneeds. But data producers face a problem: reconstituting chains of triples has\na high cognitive cost, which hinders them from gaining meaningful overviews of\ntheir own datasets. We introduce path outlines: conceptual objects\ncharacterizing sequences of triples with descriptive statistics. We interview\n11 data producers to evaluate their interest. We present Path Outlines, a tool\nto browse path-based summaries, based on coordinated views with 2 novel\nvisualisations. We compare Path Outlines with the current baseline technique in\nan experiment with 36 participants. We show that it is 3 times faster, leads to\nbetter task completion, less errors, that participants prefer it, and find\ntasks easier with it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07968,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Familiarization tours for first-time users of highly automated cars:\n  Comparing the effects of virtual environments with different levels of\n  interaction fidelity\n\n  Research in aviation and driving has highlighted the importance of training\nas an effective approach to reduce the costs associated with the supervisory\nrole of the human in automated systems. However, only a few studies have\ninvestigated the effect of pre-trip familiarization tours on highly automated\ndriving. In the present study, a driving simulator experiment compared the\neffectiveness of four familiarization groups, control, video, low fidelity\nvirtual reality (VR), and high fidelity VR on automation trust and driving\nperformance in several critical and non-critical transition tasks. The results\nrevealed the positive impact of familiarization tours on trust, takeover, and\nhandback performance at the first time of measurement. Takeover quality only\nimproved when practice was presented in high-fidelity VR. After three times of\nexposure to transition requests, trust and transition performance of all groups\nconverged to those of the high fidelity VR group, demonstrating that: a)\nexperiencing automation failures during the training may reduce costs\nassociated with first failures in highly automated driving; b) the VR tour with\nhigh level of interaction fidelity is superior to other types of\nfamiliarization tour, and c) uneducated and less-educated drivers learn about\nautomation by experiencing it. Knowledge resulting from this research could\nhelp develop cost-effective familiarization tours for highly automated vehicles\nin dealerships and car rental centers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0795,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000063909,
      "text":"Survey on Individual Differences in Visualization\n\n  Developments in data visualization research have enabled visualization\nsystems to achieve great general usability and application across a variety of\ndomains. These advancements have improved not only people's understanding of\ndata, but also the general understanding of people themselves, and how they\ninteract with visualization systems. In particular, researchers have gradually\ncome to recognize the deficiency of having one-size-fits-all visualization\ninterfaces, as well as the significance of individual differences in the use of\ndata visualization systems. Unfortunately, the absence of comprehensive surveys\nof the existing literature impedes the development of this research. In this\npaper, we review the research perspectives, as well as the personality traits\nand cognitive abilities, visualizations, tasks, and measures investigated in\nthe existing literature. We aim to provide a detailed summary of existing\nscholarship, produce evidence-based reviews, and spur future inquiry.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.01116,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"A BCI based Smart Home System Combined with Event-related Potentials and\n  Speech Imagery Task\n\n  Recently, smart home systems based on brain-computer interface (BCI) has\nattracted a wide range of interests in both industry and academia. However, the\ncurrent BCI system has several shortcomings as it produces a comparatively\nlower accuracy for real-time implementations as well as the intuitive paradigm\nfor the users cannot be well established here. Therefore, in this study, we\nproposed a highly intuitive BCI paradigm that combines event-related potential\n(ERP) with the speech-imagery task for the individual target objects. The\ndecoding accuracy of the proposed paradigm was 88.1% (plus or minus 5.90) which\nis a much significant higher performance than a conventional ERP system.\nFurthermore, the amplitude of N700 components was significantly enhanced over\nfrontal regions which are priory evoked by the speech-imagery task. Our results\ncould be utilized to develop a smart home system so that it could be more\nuser-friendly and convenient by means of delivering user's intentions both,\nintuitively and accurately.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0094,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Isness: Using Multi-Person VR to Design Peak Mystical-Type Experiences\n  Comparable to Psychedelics\n\n  Studies combining psychotherapy with psychedelic drugs (PsiDs) have\ndemonstrated positive outcomes that are often associated with PsiDs' ability to\ninduce 'mystical-type' experiences (MTEs) - i.e., subjective experiences whose\ncharacteristics include a sense of connectedness, transcendence, and\nineffability. We suggest that both PsiDs and virtual reality can be situated on\na broader spectrum of psychedelic technologies. To test this hypothesis, we\nused concepts, methods, and analysis strategies from PsiD research to design\nand evaluate 'Isness', a multi-person VR journey where participants experience\nthe collective emergence, fluctuation, and dissipation of their bodies as\nenergetic essences. A study (N=57) analyzing participant responses to a\ncommonly used PsiD experience questionnaire (MEQ30) indicates that Isness\nparticipants had MTEs comparable to those reported in double-blind clinical\nstudies after high doses of psilocybin & LSD. Within a supportive setting and\nconceptual framework, VR phenomenology can create the conditions for MTEs from\nwhich participants derive insight and meaning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.01537,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Academic viewpoints and concerns on CSCW education and training in Latin\n  America\n\n  Computer-Supported Cooperative Work, or simply CSCW, is the research area\nthat studies the design and use of socio-technical technology for supporting\ngroup work. CSCW has a long tradition in interdisciplinary work exploring\ntechnical, social, and theoretical challenges for the design of technologies to\nsupport cooperative and collaborative work and life activities. However, most\nof the research tradition, methods, and theories in the field follow a strong\ntrend grounded in social and cultural aspects from North America and Western\nEurope. Therefore, it is inevitable that some of the underlying, and\nestablished, knowledge in the field will not be directly transferrable or\napplicable to other populations. This paper presents the results of an\ninterview study conducted with Latin American faculty on the feasability,\nviability, and prospect of a curriculum proposal for CSCW Education in Latin\nAmerica: To this end, we conducted nine interviews with faculty currently based\nin six countries of the region, aiming to understand how a CSCW course targeted\nto undergraduate and\/or graduate students in Latin America might be deployed.\nOur findings suggest that there are specific traits that need to be addressed\nin such a course, such as: tailoring foundational CSCW concepts to the\ndiversity of local cultures, motivating the involvement of students by tackling\nrelevant problems to their local communities, and revitalizing CSCW research\nand practice in the continent.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.10702,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"Optimizing User Interface Layouts via Gradient Descent\n\n  Automating parts of the user interface (UI) design process has been a\nlongstanding challenge. We present an automated technique for optimizing the\nlayouts of mobile UIs. Our method uses gradient descent on a neural network\nmodel of task performance with respect to the model's inputs to make layout\nmodifications that result in improved predicted error rates and task completion\ntimes. We start by extending prior work on neural network based performance\nprediction to 2-dimensional mobile UIs with an expanded interaction space. We\nthen apply our method to two UIs, including one that the model had not been\ntrained on, to discover layout alternatives with significantly improved\npredicted performance. Finally, we confirm these predictions experimentally,\nshowing improvements up to 9.2 percent in the optimized layouts. This\ndemonstrates the algorithm's efficacy in improving the task performance of a\nlayout, and its ability to generalize and improve layouts of new interfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00772,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Evaluating Saliency Map Explanations for Convolutional Neural Networks:\n  A User Study\n\n  Convolutional neural networks (CNNs) offer great machine learning performance\nover a range of applications, but their operation is hard to interpret, even\nfor experts. Various explanation algorithms have been proposed to address this\nissue, yet limited research effort has been reported concerning their user\nevaluation. In this paper, we report on an online between-group user study\ndesigned to evaluate the performance of \"saliency maps\" - a popular explanation\nalgorithm for image classification applications of CNNs. Our results indicate\nthat saliency maps produced by the LRP algorithm helped participants to learn\nabout some specific image features the system is sensitive to. However, the\nmaps seem to provide very limited help for participants to anticipate the\nnetwork's output for new images. Drawing on our findings, we highlight\nimplications for design and further research on explainable AI. In particular,\nwe argue the HCI and AI communities should look beyond instance-level\nexplanations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.10971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Role of Intrinsic Motivation in User Interface Design to Enhance Worker\n  Performance in Amazon MTurk\n\n  Biologists and scientists have been tackling the problem of marine life\nmonitoring and fish stock estimation for many years now. Efforts are now\ndirected to move towards non-intrusive methods, by utilizing specially designed\nunderwater robots to collect images of the marine population. Training machine\nlearning algorithms on the images collected, we can now estimate the\npopulation. This in turn helps to impose regulations to control overfishing. To\ntrain these models, however, we need annotated images. Annotation of large sets\nof images collected over a decade is quite challenging. Hence, we resort to\nAmazon Mechanical Turk (MTurk), a crowdsourcing platform, for the image\nannotation task. Although it is fast to get work done in MTurk, the work\nobtained is often of poor quality. This work aims to understand the human\nfactors in designing Human Intelligence Tasks (HITs), from the perspective of\nthe Self-Determination Theory. Applying elements from the theory, we design an\nHIT to increase the competence and motivation of the workers. Within our\nexperimental framework, we find that the new interface significantly improves\nthe accuracy of worker performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11834,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000077155,
      "text":"Understanding How and Why University Students Use Virtual Private\n  Networks\n\n  We study how and why university students chose and use VPNs, and whether they\nare aware of the security and privacy risks that VPNs pose. To answer these\nquestions, we conducted 32 in-person interviews and a survey with 349\nrespondents, all university students in the United States. We find students are\nmostly concerned with access to content and privacy concerns were often\nsecondary. They made tradeoffs to achieve a particular goal, such as using a\nfree commercial VPN that may collect their online activities to access an\nonline service in a geographic area. Many users expected that their VPNs were\ncollecting data about them, although they did not understand how VPNs work. We\nconclude with a discussion of ways to help users make choices about VPNs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.04661,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Further Exploring Communal Technology Use in Smart Homes: Social\n  Expectations\n\n  Device use in smart homes is becoming increasingly communal, requiring\ncohabitants to navigate a complex social and technological context. In this\npaper, we report findings from an exploratory survey grounded in our prior work\non communal technology use in the home [4]. The findings highlight the\nimportance of considering qualities of social relationships and technology in\nunderstanding expectations and intentions of communal technology use. We\npropose a design perspective of social expectations, and we suggest existing\ndesigns can be expanded using already available information such as location,\nand considering additional information, such as levels of trust and\nreliability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03207,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Does Siri Have a Soul? Exploring Voice Assistants Through Shinto Design\n  Fictions\n\n  It can be difficult to critically reflect on technology that has become part\nof everyday rituals and routines. To combat this, speculative and fictional\napproaches have previously been used by HCI to decontextualise the familiar and\nimagine alternatives. In this work we turn to Japanese Shinto narratives as a\nway to defamiliarise voice assistants, inspired by the similarities between how\nassistants appear to 'inhabit' objects similarly to kami. Describing an\nalternate future where assistant presences live inside objects, this approach\nforegrounds some of the phenomenological quirks that can otherwise easily\nbecome lost. Divorced from the reality of daily life, this approach allows us\nto reevaluate some of the common interactions and design patterns that are\ncommon in the virtual assistants of the present.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.00689,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"What Timing for an Automated Vehicle to Make Pedestrians Understand Its\n  Driving Intentions for Improving Their Perception of Safety?\n\n  Although automated driving systems have been used frequently, they are still\nunpopular in society. To increase the popularity of automated vehicles (AVs),\nassisting pedestrians to accurately understand the driving intentions and\nimproving their perception of safety when interacting with AVs are considered\neffective. Therefore, the AV should send information about its driving\nintention to pedestrians when they interact with each other. However, the\nfollowing questions should be answered regarding how the AV sends the\ninformation to them: 1) What timing for an AV to make pedestrians understand\nits driving intentions after being noticed by them? 2) What timing for an AV to\nmake pedestrians feel safe after being noticed by them? Thirteen participants\nwere invited to interact with a manually driven vehicle and an AV in an\nexperiment. The participants' gaze information and a subjective evaluation of\ntheir understanding of the driving intention as well as their perception of\nsafety were collected. By analyzing the participants' gaze duration on the\nvehicle with their subjective evaluations, we found that the AV should enable\nthe pedestrian to accurately understand its driving intention within 0.5~6.5\n[s] and make the pedestrian feel safe within 0.5~8.0 [s] while the pedestrian\nis gazing at it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.00921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Decision Support in the Context of a Complex Decision Situation\n\n  The aim of a clinical decision support tool is to reduce the complexity of\nclinical decisions. However, when decision support tools are poorly implemented\nthey may actually confuse physicians and complicate clinical care. This paper\nargues that information from decision support tools is often removed from the\nclinical context of the targeted decisions. Physicians largely depend on\nclinical context to handle the complexity of their day-to-day decisions.\nClinical context enables them to take into account all ambiguous information\nand patient preferences. Decision support tools that provide analytic\ninformation to physicians, without its context, may then complicate the\ndecision process of physicians. It is likely that the joint forces of\nphysicians and technology will produce better decisions than either of them\nexclusively: after all, they do have different ways of dealing with the\ncomplexity of a decision and are thus complementary. Therefore, the future\nchallenges of decision support do not only reside in the optimization of the\npredictive value of the underlying models and algorithms, but equally in the\neffective communication of information and its context to doctors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.10886,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000068545,
      "text":"Assessment of Empathy in an Affective VR Environment using EEG Signals\n\n  With the advancements in social robotics and virtual avatars, it becomes\nincreasingly important that these agents adapt their behavior to the mood,\nfeelings and personality of their users. One such aspect of the user is\nempathy. Whereas many studies measure empathy through offline measures that are\ncollected after empathic stimulation (e.g. post-hoc questionnaires), the\ncurrent study aimed to measure empathy online, using brain activity collected\nduring the experience. Participants watched an affective 360 video of a child\nexperiencing domestic violence in a virtual reality headset while their EEG\nsignals were recorded. Results showed a significant attenuation of alpha, theta\nand delta asymmetry in the frontal and central areas of the brain. Moreover, a\nsignificant relationship between participants' empathy scores and their frontal\nalpha asymmetry at baseline was found. These results demonstrate specific brain\nactivity alterations when participants are exposed to an affective virtual\nreality environment, with the level of empathy as a personality trait being\nvisible in brain activity during a baseline measurement. These findings suggest\nthe potential of EEG measurements for development of passive brain-computer\ninterfaces that assess the user's affective responses in real-time and\nconsequently adapt the behavior of socially intelligent agents for a\npersonalized interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.06186,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"Developing a Personality Model for Speech-based Conversational Agents\n  Using the Psycholexical Approach\n\n  We present the first systematic analysis of personality dimensions developed\nspecifically to describe the personality of speech-based conversational agents.\nFollowing the psycholexical approach from psychology, we first report on a new\nmulti-method approach to collect potentially descriptive adjectives from 1) a\nfree description task in an online survey (228 unique descriptors), 2) an\ninteraction task in the lab (176 unique descriptors), and 3) a text analysis of\n30,000 online reviews of conversational agents (Alexa, Google Assistant,\nCortana) (383 unique descriptors). We aggregate the results into a set of 349\nadjectives, which are then rated by 744 people in an online survey. A factor\nanalysis reveals that the commonly used Big Five model for human personality\ndoes not adequately describe agent personality. As an initial step to\ndeveloping a personality model, we propose alternative dimensions and discuss\nimplications for the design of agent personalities, personality-aware\npersonalisation, and future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05026,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Super-reflective Data: Speculative Imaginings of a World Where Data\n  Works for People\n\n  It's the year 2020, and every space and place on- and off-line has been\naugmented with digital things that observe, record, transmit, and compute, for\nthe purposes of recording endless data traces of what is happening in the\nworld. Individually, these things (and the invisible services the power them)\nhave reached considerable sophistication in their ability to analyse and\ndissect such observations, turning streams of audio and video into informative\ndata fragments. Yet somehow, individuals as end-users of platforms and services\nhave not seen the full potential of such data. In this speculative paper, we\npropose two hypothetical mini scenarios different from our current digital\nworld. In the former, instead of hoarding it, data controllers turn captured\ndata over to those who need it as quickly as possible, working together to\ncombine, validate, and refine it for maximum usefulness. This simultaneously\naddresses the data fragmentation and privacy problem, by handing over long-term\ndata governance to those that value it the most In the latter, we discuss\nethical dilemmas using the long-term use of such rich data and its tendency to\ncause people to relentlessly optimise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09648,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Reward-Mediated Individual and Altruistic Behavior\n\n  Recent research has taken particular interest in observing the dynamics\nbetween altruistic and individual behavior. This is a commonly approached\nproblem when reasoning about social dilemmas, which have a plethora of real\nworld counterparts in the fields of education, health and economics. Weighing\nhow incentives influence in-game behavior, our study examines individual and\naltruistic interactions, by analyzing the players' strategies and interaction\nmotives when facing different reward attribution strategies. Consequently, a\nmodel for interaction motives is also proposed, with the premise that the\nmotives for interactions can be defined as a continuous space, ranging from\nself-oriented (associated to self-improvement behaviors) to others-oriented\n(associated to extreme altruism behaviors) motives. To evaluate the promotion\nof individual and altruistic behavior, we leverage Message Across, an in-loco\ntwo-player videogame with adaptable reward attribution systems. We conducted\nseveral user tests (N = 66) to verify to what extent individual and altruistic\nreward attribution systems led players to vary their strategies and motives\norientation. Our results indicate that players' strategies and self-reported\norientation of interaction motives varied highly significantly upon the\ndeployment of individual and altruistic reward systems, which leads us to\nbelieve on the suitability of applying an incentive-based strategy to moderate\nthe emergence of individual and altruistic behavior in games.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.12282,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000177821,
      "text":"SpatialRugs: Enhancing Spatial Awareness of Movement in Dense Pixel\n  Visualizations\n\n  Compact visual summaries of spatio-temporal movement data often strive to\nexpress accurate positions of movers. We present SpatialRugs, a technique to\nenhance the spatial awareness of movements in dense pixel visualizations.\nSpatialRugs apply 2D colormaps to visualize location mapped to a juxtaposed\ndisplay. We explore the effect of various colormaps discussing perceptual\nlimitations and introduce a custom color-smoothing method to mitigate distorted\npatterns of collective movement behavior.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09333,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Physiologically Driven Storytelling: Concept and Software Tool\n\n  We put forth Physiologically Driven Storytelling, a new approach to\ninteractive storytelling where narratives adaptively unfold based on the\nreader's physiological state. We first describe a taxonomy framing how\nphysiological signals can be used to drive interactive systems both as input\nand output. We then propose applications to interactive storytelling and\ndescribe the implementation of a software tool to create Physiological\nInteractive Fiction (PIF). The results of an online study (N=140) provided\nguidelines towards augmenting the reading experience. PIF was then evaluated in\na lab study (N=14) to determine how physiological signals can be used to infer\na reader's state. Our results show that breathing, electrodermal activity, and\neye tracking can help differentiate positive from negative tones, and\nmonotonous from exciting events. This work demonstrates how PIF can support\nstorytelling in creating engaging content and experience tailored to the\nreader. Moreover, it opens the space to future physiologically driven systems\nwithin broader application areas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05756,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"Jiskefet, a bookkeeping application for ALICE\n\n  A new bookkeeping system called Jiskefet is being developed for A Large Ion\nCollider Experiment (ALICE) during Long Shutdown 2, to be in production until\nthe end of LHC Run 4 (2029). Jiskefet unifies two functionalities: a)\ngathering, storing and presenting metadata associated with the operations of\nthe ALICE experiment and b) tracking the asynchronous processing of the physics\ndata. It will replace the existing ALICE Electronic Logbook and AliMonitor,\nallowing for a technology refresh and the inclusion of new features based on\nthe experience collected during Run 1 and Run 2. The front end leverages web\ntechnologies much in use nowadays such as TypeScript and NodeJS and is adaptive\nto various clients such as tablets, mobile devices and other screens. The back\nend includes an OpenAPI specification based REST API and a relational database.\nThis paper will describe the organization of the work done by various student\nteams who work on Jiskefet in sequential and parallel semesters and how\ncontinuity is guaranteed by using guidelines on coding, documentation and\ndevelopment. It will also describe the current status of the development, the\ninitial experience in detector stand-alone commissioning setups and the future\nplans.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.11875,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"Creating Personas with Disabilities\n\n  Personas can help raise awareness among stakeholders about users' needs.\nWhile personas are made-up people, they are based on facts gathered from user\nresearch. Personas can also be used to raise awareness of universal design and\naccessibility needs of people with disabilities. We review the current state of\nthe art of the personas and review some research and industry projects that use\nthem. We outline techniques that can be used to create personas with\ndisabilities. This includes advice on how to get more information about\nassistive technology and how to better include people with disabilities in the\npersona creation process. We also describe our use of personas with\ndisabilities in several projects and discuss how it has helped to find\naccessibility issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.06318,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Investigating Error Injection to Enhance the Effectiveness of Mobile\n  Text Entry Studies of Error Behaviour\n\n  During lab studies of text entry methods it is typical to observer very few\nerrors in participants' typing - users tend to type very carefully in labs.\nThis is a problem when investigating methods to support error awareness or\ncorrection as support mechanisms are not tested. We designed a novel evaluation\nmethod based around injection of errors into the users' typing stream and\nreport two user studies on the effectiveness of this technique. Injection\nallowed us to observe a larger number of instances and more diverse types of\nerror correction behaviour than would normally be possible in a single study,\nwithout having a significant impact on key input behaviour characteristics.\nQualitative feedback from both studies suggests that our injection algorithm\nwas successful in creating errors that appeared realistic to participants. The\nuse of error injection shows promise for the investigation of error correction\nbehaviour in text entry studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03158,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"What is \"Intelligent\" in Intelligent User Interfaces? A Meta-Analysis of\n  25 Years of IUI\n\n  This reflection paper takes the 25th IUI conference milestone as an\nopportunity to analyse in detail the understanding of intelligence in the\ncommunity: Despite the focus on intelligent UIs, it has remained elusive what\nexactly renders an interactive system or user interface \"intelligent\", also in\nthe fields of HCI and AI at large. We follow a bottom-up approach to analyse\nthe emergent meaning of intelligence in the IUI community: In particular, we\napply text analysis to extract all occurrences of \"intelligent\" in all IUI\nproceedings. We manually review these with regard to three main questions: 1)\nWhat is deemed intelligent? 2) How (else) is it characterised? and 3) What\ncapabilities are attributed to an intelligent entity? We discuss the\ncommunity's emerging implicit perspective on characteristics of intelligence in\nintelligent user interfaces and conclude with ideas for stating one's own\nunderstanding of intelligence more explicitly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.13934,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000177489,
      "text":"Vibrotactile Feedback for Vertical 2D Space Exploration\n\n  Visually impaired people encounter many challenges in their everyday life,\nespecially when it comes to navigating and representing space. The issue of\nshopping is addressed mostly on the level of navigation and product detection,\nbut conveying clues about the object position to the user is rarely\nimplemented. This work presents a prototype of vibrotactile wristband using\nspatiotemporal patterns to help visually impaired users reach an object in the\n2D plane in front of them. A pilot study on twelve blindfolded sighted subjects\nshowed that discretizing space in a seven by seven targets matrix and conveying\nclues with a discrete pattern on the vertical axis and a continuous pattern on\nthe horizontal axis is an intuitive and effective design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09061,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000063909,
      "text":"EchoLock: Towards Low Effort Mobile User Identification\n\n  User identification plays a pivotal role in how we interact with our mobile\ndevices. Many existing authentication approaches require active input from the\nuser or specialized sensing hardware, and studies on mobile device usage show\nsignificant interest in less inconvenient procedures. In this paper, we propose\nEchoLock, a low effort identification scheme that validates the user by sensing\nhand geometry via commodity microphones and speakers. These acoustic signals\nproduce distinct structure-borne sound reflections when contacting the user's\nhand, which can be used to differentiate between different people based on how\nthey hold their mobile devices. We process these reflections to derive unique\nacoustic features in both the time and frequency domain, which can effectively\nrepresent physiological and behavioral traits, such as hand contours, finger\nsizes, holding strength, and gesture. Furthermore, learning-based algorithms\nare developed to robustly identify the user under various environments and\nconditions. We conduct extensive experiments with 20 participants using\ndifferent hardware setups in key use case scenarios and study various attack\nmodels to demonstrate the performance of our proposed system. Our results show\nthat EchoLock is capable of verifying users with over 90% accuracy, without\nrequiring any active input from the user.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.13731,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Affective Automotive User Interfaces -- Reviewing the State of Emotion\n  Regulation in the Car\n\n  Affective technology offers exciting opportunities to improve road safety by\ncatering to human emotions. Modern car interiors enable the contactless\ndetection of user states, paving the way for a systematic promotion of safe\ndriver behavior through emotion regulation. We review the current literature\nregarding the impact of emotions on driver behavior and analyze the state of\nemotion regulation approaches in the car. We summarize challenges for affective\ninteraction in form of cultural aspects, technological hurdles and\nmethodological considerations, as well as opportunities to improve road safety\nby reinstating drivers into an emotionally balanced state. The purpose of this\nreview is to outline the community's combined knowledge for interested\nresearchers, to provide a focussed introduction for practitioners and to\nidentify future directions for affective interaction in the car.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0361,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Conceptual Model of Visual Analytics for Hands-on Cybersecurity Training\n\n  Hands-on training is an effective way to practice theoretical cybersecurity\nconcepts and increase participants' skills. In this paper, we discuss the\napplication of visual analytics principles to the design, execution, and\nevaluation of training sessions. We propose a conceptual model employing visual\nanalytics that supports the sensemaking activities of users involved in various\nphases of the training life cycle. The model emerged from our long-term\nexperience in designing and organizing diverse hands-on cybersecurity training\nsessions. It provides a classification of visualizations and can be used as a\nframework for developing novel visualization tools supporting phases of the\ntraining life-cycle. We demonstrate the model application on examples covering\ntwo types of cybersecurity training programs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.01092,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"3D Augmented Reality Tangible User Interface using Commodity Hardware\n\n  During the last years, the emerging field of Augmented and Virtual Reality\n(AR-VR) has seen tremendous growth. An interface that has also become very\npopular for the AR systems is the tangible interface or passive-haptic\ninterface. Specifically, an interface where users can manipulate digital\ninformation with input devices that are physical objects. This work presents a\nlow cost Augmented Reality system with a tangible interface that offers\ninteraction between the real and the virtual world. The system estimates in\nreal-time the 3D position of a small colored ball (input device), it maps it to\nthe 3D virtual world and then uses it to control the AR application that runs\nin a mobile device. Using the 3D position of our \"input\" device, it allows us\nto implement more complicated interactivity compared to a 2D input device.\nFinally, we present a simple, fast and robust algorithm that can estimate the\ncorners of a convex quadrangle. The proposed algorithm is suitable for the fast\nregistration of markers and significantly improves performance compared to the\nstate of the art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05249,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Ethical Guidelines for the Construction of Digital Nudges\n\n  Under certain circumstances, humans tend to behave in irrational ways,\nleading to situations in which they make undesirable choices. The concept of\ndigital nudging addresses these limitations of bounded rationality by\nestablishing a libertarian paternalist alternative to nudge users in virtual\nenvironments towards their own preferential choices. Thereby, choice\narchitectures are designed to address biases and heuristics involved in\ncognitive thinking. As research on digital nudging has become increasingly\npopular in the Information Systems community, an increasing necessity for\nethical guidelines has emerged around this concept to safeguard its\nlegitimization in distinction to e.g. persuasion or manipulation. However,\nreflecting on ethical debates regarding digital nudging in academia, we find\nthat current conceptualizations are scare. This is where on the basis of\nexisting literature, we provide a conceptualization of ethical guidelines for\nthe design of digital nudges, and thereby aim to ensure the applicability of\nnudging mechanisms in virtual environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.08198,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Sketch-and-test: picture-centered research with p5.js assisted\n  crowdsourcing\n\n  Relating human judgements to pictures is central to a wide variety of\nscientific disciplines. Pictures are used to evoke and study faculties of the\nhuman mind, while human input is used to label, understand and model pictorial\nrepresentations. Human input is often collected through online crowdsourcing\nexperiments. This paper discusses the usage of crowdsourcing in two major\nbranches of picture-centered research, human and computer vision, and\nidentifies novel directions such as art history and design. We demonstrate that\na wide variety of experiments can be conducted by using p5.js, a library\noriginally intended to facilitate visual creation. We report five complementary\nexperimental paradigms to illustrated the accessibility and versatility of\np5.js: Change blindness, BubbleView, 3D shape perception, Composition, and\nPerspective reconstruction. Results reveal that literature findings can be\nreproduced and novel insights can easily be achieved with the p5.js library.\nThe creative freedom of p5.js combined with low threshold access to\ncrowdsourcing seems like a powerful combination for all picture-centred\nresearch areas: perception, design, art history, communication, and beyond.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000118216,
      "text":"Using Conformity to Probe Interaction Challenges in XR Collaboration\n\n  The concept of a conformity spectrum is introduced to describe the degree to\nwhich virtualization adheres to real world physical characteristics surrounding\nthe user. This is then used to examine interaction challenges when\ncollaborating across different levels of virtuality and conformity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.02481,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"What If Your Car Would Care? Exploring Use Cases For Affective\n  Automotive User Interfaces\n\n  In this paper we present use cases for affective user interfaces (UIs) in\ncars and how they are perceived by potential users in China and Germany.\nEmotion-aware interaction is enabled by the improvement of ubiquitous sensing\nmethods and provides potential benefits for both traffic safety and personal\nwell-being. To promote the adoption of affective interaction at an\ninternational scale, we developed 20 mobile in-car use cases through an\ninter-cultural design approach and evaluated them with 65 drivers in Germany\nand China. Our data shows perceived benefits in specific areas of pragmatic\nquality as well as cultural differences, especially for socially interactive\nuse cases. We also discuss general implications for future affective automotive\nUI. Our results provide a perspective on cultural peculiarities and a concrete\nstarting point for practitioners and researchers working on emotion-aware\ninterfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.13853,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Bali Temple VR: The Virtual Reality based Application for the\n  Digitalization of Balinese Temples\n\n  The aim of this project is the development of Virtual Reality Application in\norder to document one kind of Balinese cultural heritages which are Temples.\nThe Bali Temple VR application will allow users to do the virtual tour and\nexperience the landscape of the temples and all objects inside the temples. The\napplication gives on-site tour guide using virtual reality that allow users\nexperience the visualization of the Balinese culture heritages in this case are\ntemples. The users can walk through the temples and can see the 3D objects of\ntemples and also there is narration of every object inside the temples with\nbackground musics. Right now, the project has completed two temples for virtual\nreality tour guide application. Those temples are Melanting Temples and Pulaki\nTemples. Based on the test results of its functional requirements, this virtual\nreality application has been able to run well as expected. All features that\nhave been developed have been running well. Based on 20 respondents with\nvarious ages and backgrounds, our finding shows that The Bali Temple VR\nApplication attracts people of all ages to use and experience it. They are\neager to use it and hope that there will be more temples that they can\nexperience to visit in this application.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.12923,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Improving Usability of User Centric Decision Making of Multi-Attribute\n  Products on E-commerce Websites\n\n  The high number of products available makes it difficult for a user to find\nthe most suitable products according to their needs. This problem is especially\nexacerbated when the user is trying to optimize multiple attributes during\nproduct selection, e.g. memory size and camera resolution requirements in case\nof smartphones. Previous studies have shown that such users search extensively\nto find a product that best meets their needs. In this paper, we propose an\ninterface that will help users in selecting a multi-attribute product through a\nseries of visualizations. This interface is especially targeted for users that\ndesire to purchase the best possible product according to some criteria. The\ninterface works by allowing the user to progressively shortlist products and\nultimately select the most appropriate product from a very small consideration\nset. We evaluated our proposed interface by conducting a controlled experiment\nthat empirically measures the efficiency, effectiveness and satisfaction of our\nvisualization based interface and a typical e-commerce interface. The results\nshowed that our proposed interface allowed the user to find a desired product\nquickly and correctly, moreover, the subjective opinion of the users also\nfavored our proposed interface.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14505,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000060267,
      "text":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction\n  for Visual Network Exploration and Analysis\n\n  Interaction plays a vital role during visual network exploration as users\nneed to engage with both elements in the view (e.g., nodes, links) and\ninterface controls (e.g., sliders, dropdown menus). Particularly as the size\nand complexity of a network grow, interactive displays supporting multimodal\ninput (e.g., touch, speech, pen, gaze) exhibit the potential to facilitate\nfluid interaction during visual network exploration and analysis. While\nmultimodal interaction with network visualization seems like a promising idea,\nmany open questions remain. For instance, do users actually prefer multimodal\ninput over unimodal input, and if so, why? Does it enable them to interact more\nnaturally, or does having multiple modes of input confuse users? To answer such\nquestions, we conducted a qualitative user study in the context of a network\nvisualization tool, comparing speech- and touch-based unimodal interfaces to a\nmultimodal interface combining the two. Our results confirm that participants\nstrongly prefer multimodal input over unimodal input attributing their\npreference to: 1) the freedom of expression, 2) the complementary nature of\nspeech and touch, and 3) integrated interactions afforded by the combination of\nthe two modalities. We also describe the interaction patterns participants\nemployed to perform common network visualization operations and highlight\nthemes for future multimodal network visualization systems to consider.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.10002,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"A Smartphone App to Support Sedentary Behavior Change by Visualizing\n  Personal Mobility Patterns and Action Planning (SedVis): Development and\n  Pilot Study\n\n  Given the high prevalence of sedentary behavior in daily life, simple yet\npractical solutions for behavior change are needed to avoid detrimental health\neffects. The mobile app SedVis was developed based on the health action process\napproach. The app provides personal mobility pattern visualization (for both\nphysical activity and sedentary behavior) and action planning for sedentary\nbehavior change. The primary aim of the study is to investigate the effect of\nmobility pattern visualization on users' action planning for changing their\nsedentary behavior. The secondary aim is to evaluate user engagement with the\nvisualization and user experience of the app. In a 3-week user study,\nparticipants were allocated to either an active control group (n=8) or an\nintervention group (n=8). In the 1-week baseline period, none of the\nparticipants had access to the functions in the app. In the following 2-week\nintervention period, only the intervention group was given access to the\nvisualizations, whereas both groups were asked to make action plans every day\nand reduce their sedentary behavior. The results suggested that the\nvisualizations in SedVis had no effect on the participants' action planning\naccording to both the NHST and Bayesian statistics. The intervention involving\nvisualizations and action planning in SedVis had a positive effect on reducing\nparticipants' sedentary hours, with weak evidence according to Bayesian\nstatistics, whereas no change in sedentary time was more likely in the active\ncontrol condition. Furthermore, Bayesian analysis weakly suggested that the\nmore frequently the users checked the app, the more likely they were to reduce\ntheir sedentary behavior.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05878,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000043379,
      "text":"In the Eye of the Beholder? Detecting Creativity in Visual Programming\n  Environments\n\n  Visual programming environments are increasingly part of the curriculum in\nschools. Their potential for promoting creative thinking of students is an\nimportant factor in their adoption. However, there does not exist a standard\napproach for detecting creativity in students' programming behavior, and\nanalyzing programs manually requires human expertise and is time consuming.\nThis work provides a computational tool for measuring creativity in visual\nprogramming that combines theory from the literature with data mining\napproaches. It adapts the classical dimensions of creative processes to our\nsetting, as well as considering new aspects such as visual elements of the\nprojects. We apply this approach to the Scratch programming environment,\nmeasuring the creativity score of hundreds of projects. We show that current\nmetrics of computational thinking in Scratch fail to capture important aspects\nof creativity, such as the visual artifacts of projects. Interviews conducted\nwith Scratch teachers validate our approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.12217,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000424849,
      "text":"Gesture controlled environment using sixth sense technology and its\n  implementation in IoT\n\n  This paper proposes an idea of building an interface to merge the existing\ntechnologies like Image processing, Internet of Things, Sixth sense, etc. at\none place to reduce the hardware restrictions imposed on a user and improve the\nresponsiveness of the system. The wearable device comprises of a camera, a\nprojector, and its own gesture-controlled environment having smart tools based\non trending techniques like gesture recognition, color marker detection, and\nspeech recognition. The interface is trained using machine learning. It is also\ninterfaced with an IoT based lab to access the lab controls remotely, enhance\nthe security, and to connect devices present in the lab.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.09204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Supporting Creative Work with Crowd Feedback Systems\n\n  Crowd feedback systems have the potential to support creative workers with\nfeedback from the crowd. In this position paper for the Workshop on Designing\nCrowd-powered Creativity Support Systems (DC2S2) at CHI '19, we present three\ncreativity support tools in which we explore how creative workers can be\nassisted with crowdsourced formative and summative feedback. For each of the\nthree crowd feedback systems, we provide one idea for future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.01451,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"Comparison of a Head-Mounted Display and a Curved Screen in a\n  Multi-Talker Audiovisual Listening Task\n\n  Introduction: Virtual audiovisual technology and its methodology has yet to\nbe established for psychoacoustic research. This study examined the effects of\ndifferent audiovisual conditions on preference when listening to multi-talker\nconversations. The study's goal is to explore and assess audiovisual\ntechnologies in the context of hearing research. Methods: The participants\nlistened to audiovisual conversations between four talkers. Two displays were\ntested and compared: a curved screen (CS) and a head-mounted display (HMD).\nUsing three visual conditions (audio-only, virtual characters and video\nrecordings), three groups of participants were tested: seventeen young\nnormal-hearing, ten older normal-hearing, and ten older hearing-impaired\nlisteners. Results: Open interviews showed that the CS was preferred over the\nHMD for older normal-hearing participants and that video recordings were the\npreferred visual condition. Young and older hearing-impaired participants did\nnot show a preference between the CS and the HMD. Conclusions: CSs and video\nrecordings should be the preferred audiovisual setup of laboratories and\nclinics, although HMDs and virtual characters can be used for hearing research\nwhen necessary and suitable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.0588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"SecureIT using Firebase, Google map and Node.Js\n\n  This paper is about describing the features of a software that was developed\nfor its user safety which we called SecureIT is a android based soft ware using\nAndroid SDK along with Firebase and Google map SDK along with Node.Js. The aim\nof developing this project was to make sure and taking its users safety to a\nnext level. Actually now a days some crime incidents like rapes, fire accidents\nand snatchings are very common and we believe many of those can be prevented if\nvictim got support at the right time. According to the well known daily news\npaper of Bangladesh The Daily Star there were about 1413 women was rapped where\n76 women were dead in 2019. On the same pa-per it also said that in 2018 and\n2017 the number of rapes were 732 and 818 where we can easily get that the\nnumber increases to almost double in 2019. Where we get a point that if those\ngirls get support or get people known about their location at the right time\nthey might get rid of the situation and the number of rapes could be reduced a\nlot because we all know that now a days using mo-bile smartphone is too easy\nfor people. Although leading Chinese mobile phone company Xiaomi introduced a\nnew feature Emergency SOS service in their mobile phones at the end of 2018\nwith a MIUI 10 update but this feature is only limited to the Xiaomi phones and\nit was well advertised as well. This paper will briefly describe all the\nfeatures of our software and its usages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.10428,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000046359,
      "text":"Interweaving Multimodal Interaction with Flexible Unit Visualizations\n  for Data Exploration\n\n  Multimodal interfaces that combine direct manipulation and natural language\nhave shown great promise for data visualization. Such multimodal interfaces\nallow people to stay in the flow of their visual exploration by leveraging the\nstrengths of one modality to complement the weaknesses of others. In this work,\nwe introduce an approach that interweaves multimodal interaction combining\ndirect manipulation and natural language with flexible unit visualizations. We\nemploy the proposed approach in a proof-of-concept system, DataBreeze. Coupling\npen, touch, and speech-based multimodal interaction with flexible unit\nvisualizations, DataBreeze allows people to create and interact with both\nsystematically bound (e.g., scatterplots, unit column charts) and manually\ncustomized views, enabling a novel visual data exploration experience. We\ndescribe our design process along with DataBreeze's interface and interactions,\ndelineating specific aspects of the design that empower the synergistic use of\nmultiple modalities. We also present a preliminary user study with DataBreeze,\nhighlighting the data exploration patterns that participants employed. Finally,\nreflecting on our design process and preliminary user study, we discuss future\nresearch directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.06594,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Finding the Inner Clock: A Chronobiology-based Calendar\n\n  Time and its lack of play a central role in our everyday lives. Despite\nincreasing productivity, many people experience time stress, exhaustion and a\nlonging for time affluence, and at the same time, a fear of not being busy\nenough. All this leads to a neglect of natural time, especially the patterns\nand rhythms created by physiological processes, subsumed under the heading of\nchronobiology. The present paper presents and evaluates a calendar application,\nwhich uses chronobiological knowledge to support people s planning activities.\nParticipants found our calendar to be interesting and engaging. It especially\nmade them think more about their bodies and appropriate times for particular\nactivities. All in all, it supported participants in negotiating. external\ndemands and personal health and wellbeing. This shows that technology does not\nnecessarily has to be neutral or even further current (mal-)practices. Our\ncalendar cares about changing perspectives and thus about enhancing users\nwellbeing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.07198,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Embracing Companion Technologies\n\n  As an increasing number of interactive devices offer human-like assistance,\nthere is a growing need to understand the human experience of interactive\nagents. When interactive artefacts with human-like features become intertwined\nin our everyday experience, we need to make sure that they assume the right\nroles and contribute to our wellbeing. In this theoretical exploration, we\npropose a reframing of our understanding of interactions with everyday\ntechnologies by proposing the metaphor of digital companions. We employ the\ntheory in the philosophy of empathy to propose a framework for understanding\nhow users develop relationships with digital agents. The experiential framework\nfor companion technologies provides connections between the users'\npsychological needs and companion-like features of interactive systems. Our\nwork provides a theoretical basis for rethinking the user experience of\neveryday artefacts with a humanistic mindset and poses future challenges for\nHCI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.02989,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Analyzing 3D Volume Segmentation by Low-level Perceptual Cues,\n  High-level Cognitive Tasks, and Decision-making Processes\n\n  3D volume segmentation is a fundamental task in many scientific and medical\napplications. Producing accurate segmentations efficiently is challenging, in\npart due to low imaging data quality (e.g., noise and low image resolution) and\nambiguity in the data that can only be resolved with higher-level knowledge of\nthe structure. Automatic algorithms do exist, but there are many use cases\nwhere they fail. The gold standard is still manual segmentation or review.\nUnfortunately, even for an expert, manual segmentation is laborious, time\nconsuming, and prone to errors. Existing 3D segmentation tools are often\ndesigned based on the underlying algorithm, and do not take into account human\nmental models, their lower-level perception abilities, and higher-level\ncognitive tasks. Our goal is to analyze manual segmentation using the critical\ndecision method (CDM) in order to gain a better understanding of the low-level\n(perceptual and marking) actions and higher-level decision-making processes\nthat segmenters use. A key challenge we faced is that decision-making consists\nof an accumulated set of low-level visual-spatial decisions that are\ninter-related and difficult to articulate verbally. To address this, we\ndeveloped a novel hybrid protocol which integrates CDM with eye-tracking,\nobservation, and targeted questions. In this paper, we develop and validate\ndata coding schemes for this hybrid data set that discern segmenters' low-level\nactions, higher-level cognitive tasks, overall task structures, and\ndecision-making processes. We successfully detect the visual processing changes\nbased on tasks sequences and micro decisions reflected in the eye-gaze data and\nidentified different segmentation decision strategies utilized by the\nsegmenters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.06435,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000039405,
      "text":"RankBooster: Visual Analysis of Ranking Predictions\n\n  Ranking is a natural and ubiquitous way to facilitate decision-making in\nvarious applications. However, different rankings are often used for the same\nset of entities, with each ranking method placing emphasis on different\nfactors. These factors can also be multi-dimensional in nature, compounding the\nproblem. This complexity can make it challenging for an entity which is being\nranked to understand what they can do to improve their rankings, and to analyze\nthe effect of changes in various factors to their overall rank. In this paper,\nwe present RankBooster, a novel visual analytics system to help users\nconveniently investigate ranking predictions. We take university rankings as an\nexample and focus on helping universities to better explore their rankings,\nwhere they can compare themselves to their rivals in key areas as well as\noverall. Novel visualizations are proposed to enable efficient analysis of\nrankings, including a Scenario Analysis View to show a high-level summary of\ndifferent ranking scenarios, a Relationship View to visualize the influence of\neach attribute on different indicators and a Rival View to compare the ranking\nof a university and those of its rivals. A case study demonstrates the\nusefulness and effectiveness of RankBooster in facilitating the visual analysis\nof ranking predictions and helping users better understand their current\nsituation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.09889,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"WiFE: WiFi and Vision based Intelligent Facial-Gesture Emotion\n  Recognition\n\n  Emotion is an essential part of Artificial Intelligence (AI) and human mental\nhealth. Current emotion recognition research mainly focuses on single modality\n(e.g., facial expression), while human emotion expressions are multi-modal in\nnature. In this paper, we propose a hybrid emotion recognition system\nleveraging two emotion-rich and tightly-coupled modalities, i.e., facial\nexpression and body gesture. However, unbiased and fine-grained facial\nexpression and gesture recognition remain a major problem. To this end, unlike\nour rivals relying on contact or even invasive sensors, we explore the\ncommodity WiFi signal for device-free and contactless gesture recognition,\nwhile adopting a vision-based facial expression. However, there exist two\ndesign challenges, i.e., how to improve the sensitivity of WiFi signals and how\nto process the large-volume, heterogeneous, and non-synchronous data\ncontributed by the two-modalities. For the former, we propose a signal\nsensitivity enhancement method based on the Rician K factor theory; for the\nlatter, we combine CNN and RNN to mine the high-level features of bi-modal\ndata, and perform a score-level fusion for fine-grained recognition. To\nevaluate the proposed method, we build a first-of-its-kind Vision-CSI Emotion\nDatabase (VCED) and conduct extensive experiments. Empirical results show the\nsuperiority of the bi-modality by achieving 83.24\\% recognition accuracy for\nseven emotions, as compared with 66.48% and 66.67% recognition accuracy by\ngesture-only based solution and facial-only based solution, respectively. The\nVCED database download link is https:\/\/github.com\/purpleleaves007\/WIFE-Dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.08382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"From Horseback Riding to Changing the World: UX Competence as a Journey\n\n  In this paper, we explore the notion of competence in UX based on the\nperspective of practitioners. As a result of this exploration, we observed four\ndomains through which we conceptualize a plan of sources of competence that\ndescribes the ways a UX practitioner develop competence. Based on this plane,\nwe present the idea of competence as a journey. A journey whose furthest stage\nimplies an urge towards transforming society and UX practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.13908,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Interactive Rainbow Score: A Visual-centered Multimodal Flute Tutoring\n  System\n\n  Learning to play an instrument is intrinsically multimodal, and we have seen\na trend of applying visual and haptic feedback in music games and\ncomputer-aided music tutoring systems. However, most current systems are still\ndesigned to master individual pieces of music; it is unclear how well the\nlearned skills can be generalized to new pieces. We aim to explore this\nquestion. In this study, we contribute Interactive Rainbow Score, an\ninteractive visual system to boost the learning of sight-playing, the general\nmusical skill to read music and map the visual representations to performance\nmotions. The key design of Interactive Rainbow Score is to associate pitches\n(and the corresponding motions) with colored notation and further strengthen\nsuch association via real-time interactions. Quantitative results show that the\ninteractive feature on average increases the learning efficiency by 31.1%.\nFurther analysis indicates that it is critical to apply the interaction in the\nearly period of learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06301,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"LeviCursor: Dexterous Interaction with a Levitating Object\n\n  We present LeviCursor, a method for interactively moving a physical,\nlevitating particle in 3D with high agility. The levitating object can move\ncontinuously and smoothly in any direction. We optimize the transducer phases\nfor each possible levitation point independently. Using precomputation, our\nsystem can determine the optimal transducer phases within a few microseconds\nand achieves round-trip latencies of 15 ms. Due to our interpolation scheme,\nthe levitated object can be controlled almost instantaneously with\nsub-millimeter accuracy. We present a particle stabilization mechanism which\nensures the levitating particle is always in the main levitation trap. Lastly,\nwe conduct the first Fitts' law-type pointing study with a real 3D cursor,\nwhere participants control the movement of the levitated cursor between two\nphysical targets. The results of the user study demonstrate that using\nLeviCursor, users reach performance comparable to that of a mouse pointer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04058,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Guidelines For Pursuing and Revealing Data Abstractions\n\n  Many data abstraction types, such as networks or set relationships, remain\nunfamiliar to data workers beyond the visualization research community. We\nconduct a survey and series of interviews about how people describe their data,\neither directly or indirectly. We refer to the latter as latent data\nabstractions. We conduct a Grounded Theory analysis that (1) interprets the\nextent to which latent data abstractions exist, (2) reveals the far-reaching\neffects that the interventionist pursuit of such abstractions can have on data\nworkers, (3) describes why and when data workers may resist such explorations,\nand (4) suggests how to take advantage of opportunities and mitigate risks\nthrough transparency about visualization research perspectives and agendas. We\nthen use the themes and codes discovered in the Grounded Theory analysis to\ndevelop guidelines for data abstraction in visualization projects. To continue\nthe discussion, we make our dataset open along with a visual interface for\nfurther exploration.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.10612,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Personal+Context navigation: combining AR and shared displays in network\n  path-following\n\n  Shared displays are well suited to public viewing and collaboration, however\nthey lack personal space to view private information and act without disturbing\nothers. Combining them with Augmented Reality (AR) headsets allows interaction\nwithout altering the context on the shared display. We study a set of such\ninteraction techniques in the context of network navigation, in particular path\nfollowing, an important network analysis task. Applications abound, for example\nplanning private trips on a network map shown on a public display.The proposed\ntechniques allow for hands-free interaction, rendering visual aids inside the\nheadset, in order to help the viewer maintain a connection between the AR\ncursor and the network that is only shown on the shared display. In two\nexperiments on path following, we found that adding persistent connections\nbetween the AR cursor and the network on the shared display works well for high\nprecision tasks, but more transient connections work best for lower precision\ntasks. More broadly, we show that combining personal AR interaction with shared\ndisplays is feasible for network navigation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.00372,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"The Effectiveness of Haptic Properties Under Cognitive Load: An\n  Exploratory Study\n\n  With the rise of wearables, haptic interfaces are increasingly favored to\ncommunicate information in an ambient manner. Despite this expectation,\nexisting guidelines are developed in studies where the participant's focus is\nentirely on the haptic task. In this work, we systematically study the\ncognitive load imposed by properties of a haptic signal. Participants wear a\nhaptic device on their forearm, and are asked to perform a 1-back task. Each\nexperimental condition isolates an individual property of the haptic signal\n(e.g., amplitude, waveform, rhythm) and participants are asked to identify the\ngradient of the data. We evaluate each condition across 16 participants,\nmeasuring participants' response times, error rates, and qualitative and\nquantitative surveys (e.g., NASA TLX). Our results indicate that gender and\nlanguage differences may impact preference for some properties, that\nparticipants prefer properties that can be rapidly identified, and that\namplitude imposes the lowest cognitive load.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Eliciting User Preferences for Personalized Explanations for Video\n  Summaries\n\n  Video summaries or highlights are a compelling alternative for exploring and\ncontextualizing unprecedented amounts of video material. However, the\nsummarization process is commonly automatic, non-transparent and potentially\nbiased towards particular aspects depicted in the original video. Therefore,\nour aim is to help users like archivists or collection managers to quickly\nunderstand which summaries are the most representative for an original video.\nIn this paper, we present empirical results on the utility of different types\nof visual explanations to achieve transparency for end users on how\nrepresentative video summaries are, with respect to the original video. We\nconsider four types of video summary explanations, which use in different ways\nthe concepts extracted from the original video subtitles and the video stream,\nand their prominence. The explanations are generated to meet target user\npreferences and express different dimensions of transparency: concept\nprominence, semantic coverage, distance and quantity of coverage. In two user\nstudies we evaluate the utility of the visual explanations for achieving\ntransparency for end users. Our results show that explanations representing all\nof the dimensions have the highest utility for transparency, and consequently,\nfor understanding the representativeness of video summaries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.05688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000129806,
      "text":"Design of a Privacy-Preserving Data Platform for Collaboration Against\n  Human Trafficking\n\n  Case records on victims of human trafficking are highly sensitive, yet the\nability to share such data is critical to evidence-based practice and policy\ndevelopment across government, business, and civil society. We present new\nmethods to anonymize, publish, and explore such data, implemented as a pipeline\ngenerating three artifacts: (1) synthetic data mitigating the privacy risk that\npublished attribute combinations might be linked to known individuals or\ngroups; (2) aggregate data mitigating the utility risk that synthetic data\nmight misrepresent statistics needed for official reporting; and (3) visual\nanalytics interfaces to both datasets mitigating the accessibility risk that\nprivacy mechanisms or analysis tools might not be understandable and usable by\nall stakeholders. We present our work as a design study motivated by the goal\nof transforming how the world's largest database of identified victims is made\navailable for global collaboration against human trafficking.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.01653,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Equal Area Breaks: A Classification Scheme for Data to Obtain an\n  Evenly-colored Choropleth Map\n\n  An efficient algorithm for computing the choropleth map classification scheme\nknown as equal area breaks or geographical quantiles is introduced. An equal\narea breaks classification aims to obtain a coloring for the map such that the\narea associated with each of the colors is approximately equal. This is meant\nto be an alternative to an approach that assigns an equal number of regions\nwith a particular range of property values to each color, called quantiles,\nwhich could result in the mapped area being dominated by one or a few colors.\nMoreover, it is possible that the other colors are barely discernible. This is\nthe case when some regions are much larger than others (e.g., compare\nSwitzerland with Russia). A number of algorithms of varying computational\ncomplexity are presented to achieve an equal area assignment to regions. They\ninclude a pair of greedy algorithms, as well as an optimal algorithm that is\nbased on dynamic programming. The classification obtained from the optimal\nequal area algorithm is compared with the quantiles and Jenks natural breaks\nalgorithms and found to be superior from a visual standpoint by a user study.\nFinally, a modified approach is presented which enables users to vary the\nextent to which the coloring algorithm satisfies the conflicting goals of equal\narea for each color with that of assigning an equal number of regions to each\ncolor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06039,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Evaluating the Effect of Timeline Shape on Visualization Task\n  Performance\n\n  Timelines are commonly represented on a horizontal line, which is not\nnecessarily the most effective way to visualize temporal event sequences.\nHowever, few experiments have evaluated how timeline shape influences task\nperformance. We present the design and results of a controlled experiment run\non Amazon Mechanical Turk (n=192) in which we evaluate how timeline shape\naffects task completion time, correctness, and user preference. We tested 12\ncombinations of 4 shapes -- horizontal line, vertical line, circle, and spiral\n-- and 3 data types -- recurrent, non-recurrent, and mixed event sequences. We\nfound good evidence that timeline shape meaningfully affects user task\ncompletion time but not correctness and that users have a strong shape\npreference. Building on our results, we present design guidelines for creating\neffective timeline visualizations based on user task and data types. A free\ncopy of this paper, the evaluation stimuli and data, and code are available at\nhttps:\/\/osf.io\/qr5yu\/\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06292,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"HaptiRead: Reading Braille as Mid-Air Haptic Information\n\n  Mid-air haptic interfaces have several advantages - the haptic information is\ndelivered directly to the user, in a manner that is unobtrusive to the\nimmediate environment. They operate at a distance, thus easier to discover;\nthey are more hygienic and allow interaction in 3D. We validate, for the first\ntime, in a preliminary study with sighted and a user study with blind\nparticipants, the use of mid-air haptics for conveying Braille. We tested three\nhaptic stimulation methods, where the haptic feedback was either: a) aligned\ntemporally, with haptic stimulation points presented simultaneously (Constant);\nb) not aligned temporally, presenting each point independently\n(Point-By-Point); or c) a combination of the previous methodologies, where\nfeedback was presented Row-by-Row. The results show that mid-air haptics is a\nviable technology for presenting Braille characters, and the highest average\naccuracy (94% in the preliminary and 88% in the user study) was achieved with\nthe Point-by-Point method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06842,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Quantifying the Impact of Making and Breaking Interface Habits\n\n  The frequency with which people interact with technology means that users may\ndevelop interface habits, i.e. fast, automatic responses to stable interface\ncues. Design guidelines often assume that interface habits are beneficial.\nHowever, we lack quantitative evidence of how the development of habits\nactually affect user performance and an understanding of how changes in the\ninterface design may affect habit development. Our work quantifies the effect\nof habit formation and disruption on user performance in interaction. Through a\nforced choice lab study task (n=19) and in the wild deployment (n=18) of a\nnotificationdialog experiment on smartphones, we show that people become more\naccurate and faster at option selection as they develop an interface habit.\nCrucially this performance gain is entirely eliminated once the habit is\ndisrupted. We discuss reasons for this performance shift and analyse some\ndisadvantages of interface habits, outlining general design patterns on how to\nboth support and disrupt them.Keywords: Interface habits, user behaviour,\nbreaking habit, interaction science, quantitative research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00324,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"An Evaluation of Visualization Methods for Population Statistics Based\n  on Choropleth Maps\n\n  We evaluate several augmentations to the choropleth map to convey additional\ninformation, including glyphs, 3D, cartograms, juxtaposed maps, and shading\nmethods. While choropleth maps are a common method used to represent societal\ndata, with multivariate data they can impede as much as improve understanding.\nIn particular large, low population density regions often dominate the map and\ncan mislead the viewer as to the message conveyed. Our results highlight the\npotential of 3D choropleth maps as well as the low accuracy of choropleth map\ntasks with multivariate data. We also introduce and evaluate popcharts, four\ntechniques designed to show the density of population at a very fine scale on\ntop of choropleth maps. All the data, results, and scripts are available from\nhttps:\/\/osf.io\/8rxwg\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.03504,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Sunny Pointer: Designing a mouse pointer for people with peripheral\n  vision loss\n\n  We present a new mouse cursor designed to facilitate the use of the mouse by\npeople with peripheral vision loss. The pointer consists of a collection of\nconverging straight lines covering the whole screen and following the position\nof the mouse cursor. We measured its positive effects with a group of\nparticipants with peripheral vision loss of different kinds and we found that\nit can reduce by a factor of 7 the time required to complete a targeting task\nusing the mouse. Using eye tracking, we show that this system makes it possible\nto initiate the movement towards the target without having to precisely locate\nthe mouse pointer. Using Fitts' Law, we compare these performances with those\nof full visual field users in order to understand the relation between the\naccuracy of the estimated mouse cursor position and the index of performance\nobtained with our tool.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06011,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Data Comets: Designing a Visualization Tool for Analyzing Autonomous\n  Aerial Vehicle Logs with Grounded Evaluation\n\n  Autonomous unmanned aerial vehicles are complex systems of hardware,\nsoftware, and human input. Understanding this complexity is key to their\ndevelopment and operation. Information visualizations already exist for\nexploring flight logs but comprehensive analyses currently require several\ndisparate and custom tools. This design study helps address the pain points\nfaced by autonomous unmanned aerial vehicle developers and operators. We\ncontribute: a spiral development process model for grounded evaluation\nvisualization development focused on progressively broadening target user\ninvolvement and refining user goals; a demonstration of the model as part of\ndeveloping a deployed and adopted visualization system; a data and task\nabstraction for developers and operators performing post-flight analysis of\nautonomous unmanned aerial vehicle logs; the design and implementation of DATA\nCOMETS, an open-source and web-based interactive visualization tool for\npost-flight log analysis incorporating temporal, geospatial, and multivariate\ndata; and the results of a summative evaluation of the visualization system and\nour abstractions based on in-the-wild usage. A free copy of this paper and\nsource code are available at osf.io\/h4p7g\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.03244,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"DFSeer: A Visual Analytics Approach to Facilitate Model Selection for\n  Demand Forecasting\n\n  Selecting an appropriate model to forecast product demand is critical to the\nmanufacturing industry. However, due to the data complexity, market uncertainty\nand users' demanding requirements for the model, it is challenging for demand\nanalysts to select a proper model. Although existing model selection methods\ncan reduce the manual burden to some extent, they often fail to present model\nperformance details on individual products and reveal the potential risk of the\nselected model. This paper presents DFSeer, an interactive visualization system\nto conduct reliable model selection for demand forecasting based on the\nproducts with similar historical demand. It supports model comparison and\nselection with different levels of details. Besides, it shows the difference in\nmodel performance on similar products to reveal the risk of model selection and\nincrease users' confidence in choosing a forecasting model. Two case studies\nand interviews with domain experts demonstrate the effectiveness and usability\nof DFSeer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.05025,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"Interactive Sensor Dashboard for Smart Manufacturing\n\n  This paper presents development of a smart sensor dashboard for Industry 4.0\nencompassing both 2D and 3D visualization modules. In 2D module, we described\nphysical connections among sensors and visualization modules and rendering data\non 2D screen. A user study was presented where participants answered a few\nquestions using four types of graphs. We analyzed eye gaze patterns in screen,\nnumber of correct answers and response time for all the four graphs. For 3D\nmodule, we developed a VR digital twin for sensor data visualization. A user\nstudy was presented evaluating the effect of different feedback scenarios on\nquantitative and qualitative metrics of interaction in the virtual environment.\nWe compared visual and haptic feedback and a multimodal combination of both\nvisual and haptic feedback for VR environment. We found that haptic feedback\nsignificantly improved quantitative metrics of interaction than a no feedback\ncase whereas a multimodal feedback is significantly improved qualitative\nmetrics of the interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.08916,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Man vs machine: an experimental study of geosteering decision skills\n\n  With the steady growth of the amount of real-time data while drilling,\noperational decision-making is becoming both better informed and more complex.\nTherefore, as no human brain has the capacity to interpret and integrate all\ndecision-relevant information from the data, the adoption of advanced\nalgorithms is required not only for data interpretation but also for decision\noptimization itself. However, the advantages of the automatic decision-making\nare hard to quantify.\n  The main contribution of this paper is an experiment in which we compare the\ndecision skills of geosteering experts with those of an automatic decision\nsupport system in a fully controlled synthetic environment. The implementation\nof the system, hereafter called DSS-1, is presented in our earlier work [Alyaev\net al. \"A decision support system for multi-target geosteering.\" Journal of\nPetroleum Science and Engineering 183 (2019)]. For the current study we have\ndeveloped an easy-to-use web-based platform which can visualize and update\nuncertainties in a 2D geological model. The platform has both user and\napplication interfaces (GUI and API) allowing us to put human participants and\nDSS-1 into a similar environment and conditions.\n  The results of comparing 29 geoscientists with DSS-1 over three experimental\nrounds showed that the automatic algorithm outperformed 28 participants. What's\nmore, no expert has beaten DSS-1 more than once over the three rounds, giving\nit the best comparative rating among the participants.\n  By design DSS-1 performs consistently, that is, identical problem setup is\nguaranteed to yield identical decisions. The study showed that only two experts\nmanaged to demonstrate partial consistency within a tolerance but ended up with\nmuch lower scores.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.0118,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"MAGES 3.0: Tying the knot of medical VR\n\n  In this work, we present MAGES 3.0, a novel Virtual Reality (VR)-based\nauthoring SDK platform for accelerated surgical training and assessment. The\nMAGES Software Development Kit (SDK) allows code-free prototyping of any VR\npsychomotor simulation of medical operations by medical professionals, who\nurgently need a tool to solve the issue of outdated medical training. Our\nplatform encapsulates the following novel algorithmic techniques: a)\ncollaborative networking layer with Geometric Algebra (GA) interpolation engine\nb) supervised machine learning analytics module for real-time recommendations\nand user profiling c) GA deformable cutting and tearing algorithm d) on-the-go\nconfigurable soft body simulation for deformable surfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.13477,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Omnis Pr{\\ae}dictio: Estimating the Full Spectrum of Human Performance\n  with Stroke Gestures\n\n  Designing effective, usable, and widely adoptable stroke gesture commands for\ngraphical user interfaces is a challenging task that traditionally involves\nmultiple iterative rounds of prototyping, implementation, and follow-up user\nstudies and controlled experiments for evaluation, verification, and\nvalidation. An alternative approach is to employ theoretical models of human\nperformance, which can deliver practitioners with insightful information right\nfrom the earliest stages of user interface design. However, very few aspects of\nthe large spectrum of human performance with stroke gesture input have been\ninvestigated and modeled so far, leaving researchers and practitioners of\ngesture-based user interface design with a very narrow range of predictable\nmeasures of human performance, mostly focused on estimating production time, of\nwhich extremely few cases delivered accompanying software tools to assist\nmodeling. We address this problem by introducing \"Omnis Praedictio\" (Omnis for\nshort), a generic technique and companion web tool that provides accurate\nuser-independent estimations of any numerical stroke gesture feature, including\ncustom features specified in code. Our experimental results on three public\ndatasets show that our model estimations correlate on average r > .9 with\ngroundtruth data. Omnis also enables researchers and practitioners to\nunderstand human performance with stroke gestures on many levels and,\nconsequently, raises the bar for human performance models and estimation\ntechniques for stroke gesture input.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.02304,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Resonating Experiences of Self and Others enabled by a Tangible\n  Somaesthetic Design\n\n  Digitalization is penetrating every aspect of everyday life including a\nhuman's heart beating, which can easily be sensed by wearable sensors and\ndisplayed for others to see, feel, and potentially \"bodily resonate\" with.\nPrevious work in studying human interactions and interaction designs with\nphysiological data, such as a heart's pulse rate, have argued that feeding it\nback to the users may, for example support users' mindfulness and\nself-awareness during various everyday activities and ultimately support their\nwellbeing. Inspired by Somaesthetics as a discipline, which focuses on an\nappreciation of the living body's role in all our experiences, we designed and\nexplored mobile tangible heart beat displays, which enable rich forms of bodily\nexperiencing oneself and others in social proximity. In this paper, we first\nreport on the design process of tangible heart displays and then present\nresults of a field study with 30 pairs of participants. Participants were asked\nto use the tangible heart displays during watching movies together and report\ntheir experience in three different heart display conditions (i.e., displaying\ntheir own heart beat, their partner's heart beat, and watching a movie without\na heart display). We found, for example that participants reported significant\neffects in experiencing sensory immersion when they felt their own heart beats\ncompared to the condition without any heart beat display, and that feeling\ntheir partner's heart beats resulted in significant effects on social\nexperience. We refer to resonance theory to discuss the results, highlighting\nthe potential of how ubiquitous technology could utilize physiological data to\nprovide resonance in a modern society facing social acceleration.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00284,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000278155,
      "text":"Strangers in the Room: Unpacking Perceptions of 'Smartness' and Related\n  Ethical Concerns in the Home\n\n  The increasingly widespread use of 'smart' devices has raised multifarious\nethical concerns regarding their use in domestic spaces. Previous work\nexamining such ethical dimensions has typically either involved empirical\nstudies of concerns raised by specific devices and use contexts, or\nalternatively expounded on abstract concepts like autonomy, privacy or trust in\nrelation to 'smart homes' in general. This paper attempts to bridge these\napproaches by asking what features of smart devices users consider as rendering\nthem 'smart' and how these relate to ethical concerns. Through a multimethod\ninvestigation including surveys with smart device users (n=120) and\nsemi-structured interviews (n=15), we identify and describe eight types of\nsmartness and explore how they engender a variety of ethical concerns including\nprivacy, autonomy, and disruption of the social order. We argue that this\nmiddle ground, between concerns arising from particular devices and more\nabstract ethical concepts, can better anticipate potential ethical concerns\nregarding smart devices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.11904,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"The CARP Mobile Sensing Framework -- A Cross-platform, Reactive,\n  Programming Framework and Runtime Environment for Digital Phenotyping\n\n  Mobile sensing - i.e., the ability to unobtrusively collect sensor data from\nbuilt-in phone sensors - has long been a core research topic in Ubicomp. A\nnumber of technological platforms for mobile sensing have been presented over\nthe years and a lot of knowledge on how to facilitate mobile sensing has been\naccumulated. This paper presents the CARP Mobile Sensing (CAMS) framework,\nwhich is a modern cross-platform (Android \/ iOS) software architecture\nproviding a reactive and unified programming model that emphasizes\nextensibility, maintainability, and adaptability. Moreover, the CAMS framework\nsupports sensing from wearable devices such as an electrocardiography (ECG)\nmonitor, and configuring data transformers. The latter allows to transform\ncollected data to a standardized data format and to implement\nprivacy-preserving data transformations. The paper presents the design,\narchitecture, implementation, and evaluation of CAMS, and shows how the\nframework has been used in two real-world mobile sensing and mobile health\n(mHealth) applications. We conclude that CAMS provides a novel cross-platform\napplication programming framework which has proved mature, stable, scalable,\nand flexible in the design of digital phenotyping and mHealth applications\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.03813,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Multimodal Systems: Taxonomy, Methods, and Challenges\n\n  Naturally, humans use multiple modalities to convey information. The\nmodalities are processed both sequentially and in parallel for communication in\nthe human brain, this changes when humans interact with computers. Empowering\ncomputers with the capability to process input multimodally is a major domain\nof investigation in Human-Computer Interaction (HCI). The advancement in\ntechnology (powerful mobile devices, advanced sensors, new ways of output,\netc.) has opened up new gateways for researchers to design systems that allow\nmultimodal interaction. It is a matter of time when the multimodal inputs will\novertake the traditional ways of interactions. The paper provides an\nintroduction to the domain of multimodal systems, explains a brief history,\ndescribes advantages of multimodal systems over unimodal systems, and discusses\nvarious modalities. The input modeling, fusion, and data collection were\ndiscussed. Finally, the challenges in the multimodal systems research were\nlisted. The analysis of the literature showed that multimodal interface systems\nimprove the task completion rate and reduce the errors compared to unimodal\nsystems. The commonly used inputs for multimodal interaction are speech and\ngestures. In the case of multimodal inputs, late integration of input\nmodalities is preferred by researchers because it allows easy update of\nmodalities and corresponding vocabularies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.09521,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"From Ancient Contemplative Practice to the App Store: Designing a\n  Digital Container for Mindfulness\n\n  Hundreds of popular mobile apps today market their ties to mindfulness. What\nactivities do these apps support and what benefits do they claim? How do\nmindfulness teachers, as domain experts, view these apps? We first conduct an\nexploratory review of 370 mindfulness-related apps on Google Play, finding that\nmindfulness is presented primarily as a tool for relaxation and stress\nreduction. We then interviewed 15 U.S. mindfulness teachers from the\ntherapeutic, Buddhist, and Yogic traditions about their perspectives on these\napps. Teachers expressed concern that apps that introduce mindfulness only as a\ntool for relaxation neglect its full potential. We draw upon the experiences of\nthese teachers to suggest design implications for linking mindfulness with\nfurther contemplative practices like the cultivation of compassion. Our\nfindings speak to the importance of coherence in design: that the metaphors and\nmechanisms of a technology align with the underlying principles it follows.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.00762,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"TeleVital: Enhancing the quality of contactless health assessment\n\n  In the midst of rising positive cases of COVID-19, the hospitals face a\nnewfound difficulty to prioritize on their patients and accommodate them.\nMoreover, crowding of patients at hospitals pose a threat to the healthcare\nworkers and other patients at the hospital. With that in mind, a non-contact\nmethod of measuring the necessary vitals such as heart rate, respiratory rate\nand SPO$_2$ will prove highly beneficial for the hospitals to tackle this\nissue. This paper discusses our approach in achieving the non-contact\nmeasurement of vitals with the sole help of a webcam and further our design of\nan e-hospital platform for doctors and patients to attend appointments\nvirtually. The platform also provides the doctor with an option to provide with\nvoice-based prescriptions or digital prescriptions, to simplify the daily,\nexhausting routine of a doctor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.12349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Tactile Perception of Objects by the User's Palm for the Development of\n  Multi-contact Wearable Tactile Displays\n\n  The user's palm plays an important role in object detection and manipulation.\nThe design of a robust multi-contact tactile display must consider the\nsensation and perception of of the stimulated area aiming to deliver the right\nstimuli at the correct location. To the best of our knowledge, there is no\nstudy to obtain the human palm data for this purpose. The objective of this\nwork is to introduce the method to investigate the user's palm sensations\nduring the interaction with objects. An array of fifteen Force Sensitive\nResistors (FSRs) was located at the user's palm to get the area of interaction,\nand the normal force delivered to four different convex surfaces. Experimental\nresults showed the active areas at the palm during the interaction with each of\nthe surfaces at different forces. The obtained results can be applied in the\ndevelopment of multi-contact wearable tactile and haptic displays for the palm,\nand in training a machine-learning algorithm to predict stimuli aiming to\nachieve a highly immersive experience in Virtual Reality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.1011,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Exergames for telerehabilitation\n\n  Recent advancements in technology have improved the connectivity between\nhumans enhancing the transfer of information. Leveraging these technological\nmarvels in the healthcare industry has led to the development of telehealth\nallowing patients and clinicians to receive and administer treatment remotely.\nTelerehabilitation is a subset of telehealth that facilitates remote\nrehabilitation treatment for patients. Providing rehabilitative services to the\naging baby boomer population requires tech-savvy solutions to augment the\ntherapists and clinicians for effective remote monitoring and tele-medicine.\nHence, this thesis develops easy-to-use exergames for low-cost mechatronic\ndevices targeting rehabilitation of post-stroke patients. Specifically, it\ndemonstrates wearable inertial sensors for exergames consisting of an animated\nvirtual coach for providing patients with instructions for performing range of\nmotion exercises. Next, a gaming environment is developed for task-specific\nrehabilitation such as eating. Finally, exergames are developed for\nrehabilitation of pincer grasping. In addition to gamified interfaces providing\nan engaging rehabilitation experience to the user, the data acquired from the\nmechatronic devices facilitate data-driven telerehabilitation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.07294,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Building a navigable fine texture design space\n\n  Friction modulation technology enables the creation of textural effects on\nflat haptic displays. However, an intuitive and manageably small design space\nfor construction of such haptic textures remains an unfulfilled goal for user\ninterface designers. In this paper, we explore perceptually relevant features\nof fine texture for use in texture construction and modification. Beginning\nwith simple sinusoidal patterns of friction force that vary in frequency and\namplitude, we define irregularity as a third building block of a texture\npattern and show it to be a scalable feature distinct from the others using\nmultidimensional scaling. Additionally, subjects' verbal descriptions of this\n3-dimensional design space provide insight into their intuitive interpretation\nof the physical parameter changes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Gamification and engagement of tourists and residents in public\n  transportation exploiting location-based technologies\n\n  Cities are becoming very congested. There is a need to reduce the number of\nprivate cars on the roads, by maximising the potential for local public\ntransport. With the increasing awareness of transport that is sustainable in\nthe sense of environmental impact, but also climate and social, there is the\nneed to create engagement into public transportation. Gamification, which is\nthe use of game elements in non-game contexts, has proven to deliver very\npositive results, by turning regular activities into engaging ones, which are\nfun to perform. We have designed a mobile application, that interacts with\nshort-range wireless communication technologies, inviting people to use public\ntransport. To evaluate the solution, we have created a questionnaire based on\nthe System Usability Scale, but also using usability testing with specific\ntasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.15285,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000114573,
      "text":"Promoting the Research of Health Behavior Change in Chinese HCI\n  Community\n\n  Unhealthy lifestyles largely contribute to many chronic diseases, which makes\nthe research on health behavior change crucial for both individuals and the\nwhole society. As an interdisciplinary research field, health behavior change\nresearch in the HCI community is still in the early stage. This research field\nis notably less developed in Chinese HCI community. In this position paper, we\nwill first illustrate the research of health behavior change in the HCI\ncommunity based on our previous systematic review. According to the unique\nproperties of Chinese society, we will then discuss both the potential\nadvantages and challenges of conducting health behavior change research in\nChina. Lastly, we will briefly introduce the SMARTACT project in Germany to\nprovide a reference for future related research. This paper aims to draw more\nattention to this research field and promote its development in China.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.0909,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000067552,
      "text":"Response of Vulnerable Road Users to Visual Information from Autonomous\n  Vehicles in Shared Spaces\n\n  Completely unmanned autonomous vehicles have been anticipated for a while.\nInitially, these are expected to drive only under certain conditions on some\nroads, and advanced functionality is required to cope with the ever-increasing\nchallenges of safety. To enhance the public's perception of road safety and\ntrust in new vehicular technologies, we investigate in this paper the effect of\nseveral interaction paradigms with vulnerable road users by developing and\napplying algorithms for the automatic analysis of pedestrian body language. We\nassess behavioral patterns and determine the impact of the coexistence of AVs\nand other road users on general road safety in a shared space for VRUs and\nvehicles. Results showed that the implementation of visual communication cues\nfor interacting with VRUs is not necessarily required for a shared space in\nwhich informal traffic rules apply.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.10793,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000094043,
      "text":"Reflection in Game-Based Learning: A Survey of Programming Games\n\n  Reflection is a critical aspect of the learning process. However, educational\ngames tend to focus on supporting learning concepts rather than supporting\nreflection. While reflection occurs in educational games, the educational game\ndesign and research community can benefit from more knowledge of how to\nfacilitate player reflection through game design. In this paper, we examine\neducational programming games and analyze how reflection is currently\nsupported. We find that current approaches prioritize accuracy over the\nindividual learning process and often only support reflection post-gameplay.\nOur analysis identifies common reflective features, and we develop a set of\nopen areas for future work. We discuss these promising directions towards\nengaging the community in developing more mechanics for reflection in\neducational games.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16572,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Mitigating undesirable emergent behavior arising between driver and\n  semi-automated vehicle\n\n  Emergent behavior arising in a joint human-robot system cannot be fully\npredicted based on an understanding of the individual agents. Typically, robot\nbehavior is governed by algorithms that optimize a reward function that should\nquantitatively capture the joint system's goal. Although reward functions can\nbe updated to better match human needs, this is no guarantee that no\nmisalignment with the complex and variable human needs will occur. Algorithms\nmay learn undesirable behavior when interacting with the human and the\nintrinsically unpredictable human-inhabited world, thereby producing further\nmisalignment with human users or bystanders. As a result, humans might behave\ndifferently than anticipated, causing robots to learn differently and\nundesirable behavior to emerge. With this short paper, we state that to design\nfor Human-Robot Interaction that mitigates such undesirable emergent behavior,\nwe need to complement advancements in human-robot interaction algorithms with\nhuman factors knowledge and expertise. More specifically, we advocate a\nthree-pronged approach that we illustrate using a particularly challenging\nexample of safety-critical human-robot interaction: a driver interacting with a\nsemi-automated vehicle. Undesirable emergent behavior should be mitigated by a\ncombination of 1) including driver behavioral mechanisms in the vehicle's\nalgorithms and reward functions, 2) model-based approaches that account for\ninteraction-induced driver behavioral adaptations and 3) driver-centered\ninteraction design that promotes driver engagement with the semi-automated\nvehicle, and the transparent communication of each agent's actions that allows\nmutual support and adaptation. We provide examples from recent empirical work\nin our group, in the hope this proves to be fruitful for discussing emergent\nhuman-robot interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.14291,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"Survey on Visual Analysis of Event Sequence Data\n\n  Event sequence data record series of discrete events in the time order of\noccurrence. They are commonly observed in a variety of applications ranging\nfrom electronic health records to network logs, with the characteristics of\nlarge-scale, high-dimensional, and heterogeneous. This high complexity of event\nsequence data makes it difficult for analysts to manually explore and find\npatterns, resulting in ever-increasing needs for computational and perceptual\naids from visual analytics techniques to extract and communicate insights from\nevent sequence datasets. In this paper, we review the state-of-the-art visual\nanalytics approaches, characterize them with our proposed design space, and\ncategorize them based on analytical tasks and applications. From our review of\nrelevant literature, we have also identified several remaining research\nchallenges and future research opportunities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.13386,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000128481,
      "text":"Gender and Emotion Recognition from Implicit User Behavior Signals\n\n  This work explores the utility of implicit behavioral cues, namely,\nElectroencephalogram (EEG) signals and eye movements for gender recognition\n(GR) and emotion recognition (ER) from psychophysical behavior. Specifically,\nthe examined cues are acquired via low-cost, off-the-shelf sensors. 28 users\n(14 male) recognized emotions from unoccluded (no mask) and partially occluded\n(eye or mouth masked) emotive faces; their EEG responses contained\ngender-specific differences, while their eye movements were characteristic of\nthe perceived facial emotions. Experimental results reveal that (a) reliable GR\nand ER is achievable with EEG and eye features, (b) differential cognitive\nprocessing of negative emotions is observed for females and (c) eye gaze-based\ngender differences manifest under partial face occlusion, as typified by the\neye and mouth mask conditions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.15545,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000062585,
      "text":"Dynamic Difficulty Adjustment via Fast User Adaptation\n\n  Dynamic difficulty adjustment (DDA) is a technology that adapts a game's\nchallenge to match the player's skill. It is a key element in game development\nthat provides continuous motivation and immersion to the player. However,\nconventional DDA methods require tuning in-game parameters to generate the\nlevels for various players. Recent DDA approaches based on deep learning can\nshorten the time-consuming tuning process, but require sufficient user demo\ndata for adaptation. In this paper, we present a fast user adaptation method\nthat can adjust the difficulty of the game for various players using only a\nsmall amount of demo data by applying a meta-learning algorithm. In the video\ngame environment user test (n=9), our proposed DDA method outperformed a\ntypical deep learning-based baseline method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.02737,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000151992,
      "text":"Vehicle Automation Field Test: Impact on Driver Behavior and Trust\n\n  With the growing technological advances in autonomous driving, the transport\nindustry and research community seek to determine the impact that autonomous\nvehicles (AV) will have on consumers, as well as identify the different factors\nthat will influence their use. Most of the research performed so far relies on\nlaboratory-controlled conditions using driving simulators, as they offer a safe\nenvironment for testing advanced driving assistance systems (ADAS). In this\nstudy we analyze the behavior of drivers that are placed in control of an\nautomated vehicle in a real life driving environment. The vehicle is equipped\nwith advanced autonomy, making driver control of the vehicle unnecessary in\nmany scenarios, although a driver take over is possible and sometimes required.\nIn doing so, we aim to determine the impact of such a system on the driver and\ntheir driving performance. To this end road users' behavior from naturalistic\ndriving data is analyzed focusing on awareness and diagnosis of the road\nsituation. Results showed that the road features determined the level of visual\nattention and trust in the automation. They also showed that the activities\nperformed during the automation affected the reaction time to take over the\ncontrol of the vehicle.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.05977,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000077155,
      "text":"Trust-UBA: A Corpus for the Study of the Manifestation of Trust in\n  Speech\n\n  This paper describes a novel protocol for collecting speech data from\nsubjects induced to have different degrees of trust in the skills of a\nconversational agent. The protocol consists of an interactive session where the\nsubject is asked to respond to a series of factual questions with the help of a\nvirtual assistant. In order to induce subjects to either trust or distrust the\nagent's skills, they are first informed that it was previously rated by other\nusers as being either good or bad; subsequently, the agent answers the\nsubjects' questions consistently to its alleged abilities. All interactions are\nspeech-based, with subjects and agents communicating verbally, which allows the\nrecording of speech produced under different trust conditions. We collected a\nspeech corpus in Argentine Spanish using this protocol, which we are currently\nusing to study the feasibility of predicting the degree of trust from speech.\nWe find clear evidence that the protocol effectively succeeded in influencing\nsubjects into the desired mental state of either trusting or distrusting the\nagent's skills, and present preliminary results of a perceptual study of the\ndegree of trust performed by expert listeners. The collected speech dataset\nwill be made publicly available once ready.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.02187,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Lower Limb Rehabilitation in Juvenile Idiopathic Arthritis using Serious\n  Games\n\n  Patients undergoing physical rehabilitation therapy must perform series of\nexercises regularly over a long period of time to improve, or at least not to\nworsen, their condition. Rehabilitation can easily become boring because of the\ntedious repetition of simple exercises, which can also cause mild pain and\ndiscomfort. As a consequence, patients often fail to follow their\nrehabilitation schedule with the required regularity, thus endangering their\nrecovery. In the last decade, video games have become largely popular and the\navailability of advanced input controllers has made them a viable approach to\nmake physical rehabilitation more entertaining while increasing patients\nmotivation. In this paper, we present a framework integrating serious games for\nthe lower-limb rehabilitation of children suffering from Juvenile Idiopathic\nArthritis (JIA). The framework comprises games that implement parts of the\ntherapeutic protocol followed by the young patients and provides modules to\ntune, control, record, and analyze the therapeutic sessions. We present the\nresult of a preliminary validation we performed with patients at the clinic\nunder therapists supervision. The feedback we received has been overall very\npositive both from patients, who enjoyed performing their usual therapy using\nvideo games, and therapists, who liked how the games could keep the children\nengaged and motivated while performing the usual therapeutic routine.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16508,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Learning to Ignore: A Case Study of Organization-Wide Bulk Email\n  Effectiveness\n\n  Bulk email is a primary communication channel within organizations, with\nall-company emails and regular newsletters serving as a mechanism for making\nemployees aware of policies and events. Ineffective communication could result\nin wasted employee time and a lack of compliance or awareness. Previous studies\non organizational emails focused mostly on recipients. However, organizational\nbulk email system is a multi-stakeholder problem including recipients,\ncommunicators, and the organization itself. We studied the effectiveness,\npractice, and assessments of the organizational bulk email system of a large\nuniversity from multi-stakeholders' perspectives. We conducted a qualitative\nstudy with the university's communicators, recipients, and managers. We delved\ninto the organizational bulk email's distributing mechanisms of the\ncommunicators, the reading behaviors of recipients, and the perspectives on\nemails' values of communicators, managers, and recipients. We found that the\norganizational bulk email system as a whole was strained, and communicators are\ncaught in the middle of this multi-stakeholder problem. First, though the\ncommunicators had an interest in preserving the effectiveness of channels in\nreaching employees, they had high-level clients whose interests might outweigh\njudgment about whether a message deserves widespread circulation. Second,\nthough communicators thought they were sending important information,\nrecipients viewed most of the organizational bulk emails as not relevant to\nthem. Third, this disagreement was amplified by the success metric used by\ncommunicators. They viewed their bulk emails as successful if they had a high\nopen rate. But recipients often opened and then rapidly discarded emails\nwithout reading the details. Last, while the communicators in general\nunderstood the challenge, they had a limited set of targeting and feedback\ntools to support their task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.1687,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"The Effect of Robo-taxi User Experience on User Acceptance: Field Test\n  Data Analysis\n\n  With the advancement of self-driving technology, the commercialization of\nRobo-taxi services is just a matter of time. However, there is some skepticism\nregarding whether such taxi services will be successfully accepted by real\ncustomers due to perceived safety-related concerns; therefore, studies focused\non user experience have become more crucial. Although many studies\nstatistically analyze user experience data obtained by surveying individuals'\nperceptions of Robo-taxi or indirectly through simulators, there is a lack of\nresearch that statistically analyzes data obtained directly from actual\nRobo-taxi service experiences. Accordingly, based on the user experience data\nobtained by implementing a Robo-taxi service in the downtown of Seoul and\nDaejeon in South Korea, this study quantitatively analyzes the effect of user\nexperience on user acceptance through structural equation modeling and path\nanalysis. We also obtained balanced and highly valid insights by reanalyzing\nmeaningful causal relationships obtained through statistical models based on\nin-depth interview results. Results revealed that the experience of the\ntraveling stage had the greatest effect on user acceptance, and the cutting\nedge of the service and apprehension of technology were emotions that had a\ngreat effect on user acceptance. Based on these findings, we suggest guidelines\nfor the design and marketing of future Robo-taxi services.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.0438,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"Training design fostering the emergence of new meanings toward\n  unprecedented and critical events\n\n  Our research is part of a technological research program in adult education\nconducted in reference to the \"course of action\" program. Using the\nactivity-sign hypothesis of this programme, training situations are thought as\nopportunities to perturb\/relaunch the participants'dynamics of meaning. Our\ncontribution aims to (i) improve the conceptualization of training situations\nthat are thought as aids to the understanding and transformation of the\nparticipants situations, and (ii) derive cross-cutting design principles that\nallow the enactment of these training situations in different contexts. We rely\non the analysis of training programs aiming either at the management or at the\novercoming of events experienced as unprecedented and critical by the\nindividuals concerned. Depending on the case, these training programs have a\n\"restorative\" aim (resolving of impasse situations), or have a \"preparatory\"\naim (prefiguration of crisis situations). The design principles of these\ntraining programs and their effects are analysed with the conceptual tools\ndeveloped within the course-of-action, integrating two additional dimensions:\nfictional and event-driven. The conditions for the emergence of new meanings\nare described in terms of abduction, either for reparative purposes (resolution\nof deadlock situations) or for preparatory purposes (prefiguration of crisis\nsituations). The contribution to course-of-action program and to research on\nadult education is discussed through the prism of the hypothesis of\nactivity-sign.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04831,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"n-Gage: Predicting in-class Emotional, Behavioural and Cognitive\n  Engagement in the Wild\n\n  The study of student engagement has attracted growing interests to address\nproblems such as low academic performance, disaffection, and high dropout\nrates. Existing approaches to measuring student engagement typically rely on\nsurvey-based instruments. While effective, those approaches are time-consuming\nand labour-intensive. Meanwhile, both the response rate and quality of the\nsurvey are usually poor. As an alternative, in this paper, we investigate\nwhether we can infer and predict engagement at multiple dimensions, just using\nsensors. We hypothesize that multidimensional student engagement can be\ntranslated into physiological responses and activity changes during the class,\nand also be affected by the environmental changes. Therefore, we aim to explore\nthe following questions: Can we measure the multiple dimensions of high school\nstudent's learning engagement including emotional, behavioural and cognitive\nengagement with sensing data in the wild? Can we derive the activity,\nphysiological, and environmental factors contributing to the different\ndimensions of student engagement? If yes, which sensors are the most useful in\ndifferentiating each dimension of the engagement? Then, we conduct an in-situ\nstudy in a high school from 23 students and 6 teachers in 144 classes over 11\ncourses for 4 weeks. We present the n-Gage, a student engagement sensing system\nusing a combination of sensors from wearables and environments to automatically\ndetect student in-class multidimensional learning engagement. Experiment\nresults show that n-Gage can accurately predict multidimensional student\nengagement in real-world scenarios with an average MAE of 0.788 and RMSE of\n0.975 using all the sensors. We also show a set of interesting findings of how\ndifferent factors (e.g., combinations of sensors, school subjects, CO2 level)\naffect each dimension of the student learning engagement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.01862,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"An Analysis of Data Driven, Decision-Making Capabilities of Managers in\n  Banks\n\n  Organizations are adopting data analytics and Business Intelligence (BI)\ntools to gain insights from the past data, forecast future events, and to get\ntimely and reliable information for decision making. While the tools are\nbecoming mature, affordable, and more comfortable to use, it is also essential\nto understand whether the contemporary managers and leaders are ready for\nData-Driven Decision Making (DDDM). We explore the extent the Decision Makers\n(DMs) utilize data and tools, as well as their ability to interpret various\nforms of outputs from tools and to apply those insights to gain competitive\nadvantage. Our methodology was based on a qualitative survey, where we\ninterviewed 12 DMs of six commercial banks in Sri Lanka at the branch,\nregional, and CTO, CIO, and Head of IT levels. We identified that on many\noccasions, DMs' intuition overrules the DDDM due to uncertainty, lack of trust,\nknowledge, and risk-taking. Moreover, it was identified that the quality of\nvisualizations has a significant impact on the use of intuition by overruling\nDDDM. We further provide a set of recommendations on the adoption of BI tools\nand how to overcome the struggles faced while performing DDDM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09809,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Geno: A Developer Tool for Authoring Multimodal Interaction on Existing\n  Web Applications\n\n  Supporting voice commands in applications presents significant benefits to\nusers. However, adding such support to existing GUI-based web apps is\neffort-consuming with a high learning barrier, as shown in our formative study,\ndue to the lack of unified support for creating multimodal interfaces. We\npresent Geno---a developer tool for adding the voice input modality to existing\nweb apps without requiring significant NLP expertise. Geno provides a\nhigh-level workflow for developers to specify functionalities to be supported\nby voice (intents), create language models for detecting intents and the\nrelevant information (parameters) from user utterances, and fulfill the intents\nby either programmatically invoking the corresponding functions or replaying\nGUI actions on the web app. Geno further supports multimodal references to GUI\ncontext in voice commands (e.g. \"move this [event] to next week\" while pointing\nat an event with the cursor). In a study, developers with little NLP expertise\nwere able to add multimodal voice command support for two existing web apps\nusing Geno.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12328,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000085102,
      "text":"Surface Electromyography-controlled Pedestrian Collision Avoidance: A\n  Driving Simulator Study\n\n  Drivers with disabilities such as hemiplegia or unilateral upper limb\namputation restricting steering wheel operation to one arm could encounter the\nchallenge of stabilizing vehicles during pedestrian collision avoidance. An\nsEMG-controlled steering assistance system was developed for these drivers to\nenable rapid steering wheel rotation with only one healthy arm. Test drivers\nwere recruited to use the Myo armband as a sEMG-based interface to perform\npedestrian collision avoidance in a driving simulator. It was hypothesized that\nthe sEMG-based interface would be comparable or superior in vehicle stability\nto manual takeover from automated driving and conventional steering wheel\noperation. The Myo armband interface was significantly superior to manual\ntakeover from automated driving and comparable to manual steering wheel\noperation. The results of the driving simulator trials confirm the feasibility\nof the sEMG-controlled system as a safe alternative that could benefit drivers\nwith the aforesaid disabilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.10884,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"The Role and Potentials of Field User Interaction Data in the Automotive\n  UX Development Lifecycle: An Industry Perspective\n\n  We are interested in the role of field user interaction data in the\ndevelopment of IVIS, the potentials practitioners see in analyzing this data,\nthe concerns they share, and how this compares to companies with digital\nproducts. We conducted interviews with 14 UX professionals, 8 from automotive\nand 6 from digital companies, and analyzed the results by emergent thematic\ncoding. Our key findings indicate that implicit feedback through field user\ninteraction data is currently not evident in the automotive UX development\nprocess. Most decisions regarding the design of IVIS are made based on personal\npreferences and the intuitions of stakeholders. However, the interviewees also\nindicated that user interaction data has the potential to lower the influence\nof guesswork and assumptions in the UX design process and can help to make the\nUX development lifecycle more evidence-based and user-centered.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09262,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"Wearable vibrotactile stimulation for upper extremity rehabilitation in\n  chronic stroke: clinical feasibility trial using the VTS Glove\n\n  Objective: Evaluate the feasibility and potential impacts on hand function\nusing a wearable stimulation device (the VTS Glove) which provides mechanical,\nvibratory input to the affected limb of chronic stroke survivors.\n  Methods: A double-blind, randomized, controlled feasibility study including\nsixteen chronic stroke survivors (mean age: 54; 1-13 years post-stroke) with\ndiminished movement and tactile perception in their affected hand. Participants\nwere given a wearable device to take home and asked to wear it for three hours\ndaily over eight weeks. The device intervention was either (1) the VTS Glove,\nwhich provided vibrotactile stimulation to the hand, or (2) an identical glove\nwith vibration disabled. Participants were equally randomly assigned to each\ncondition. Hand and arm function were measured weekly at home and in local\nphysical therapy clinics.\n  Results: Participants using the VTS Glove showed significantly improved\nSemmes-Weinstein monofilament exam, reduction in Modified Ashworth measures in\nthe fingers, and some increased voluntary finger flexion, elbow and shoulder\nrange of motion.\n  Conclusions: Vibrotactile stimulation applied to the disabled limb may impact\ntactile perception, tone and spasticity, and voluntary range of motion.\nWearable devices allow extended application and study of stimulation methods\noutside of a clinical setting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.15407,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Composition and Configuration Patterns in Multiple-View Visualizations\n\n  Multiple-view visualization (MV) is a layout design technique often employed\nto help users see a large number of data attributes and values in a single\ncohesive representation. Because of its generalizability, the MV design has\nbeen widely adopted by the visualization community to help users examine and\ninteract with large, complex, and high-dimensional data. However, although\nubiquitous, there has been little work to categorize and analyze MVs in order\nto better understand its design space. As a result, there has been little to no\nguideline in how to use the MV design effectively. In this paper, we present an\nin-depth study of how MVs are designed in practice. We focus on two fundamental\nmeasures of multiple-view patterns: composition, which quantifies what view\ntypes and how many are there; and configuration, which characterizes spatial\narrangement of view layouts in the display space. We build a new dataset\ncontaining 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis\npublications 2011 to 2019, and make fine-grained annotations of view types and\nlayouts for these visualization images. From this data we conduct composition\nand configuration analyses using quantitative metrics of term frequency and\nlayout topology. We identify common practices around MVs, including\nrelationship of view types, popular view layouts, and correlation between view\ntypes and layouts. We combine the findings into a MV recommendation system,\nproviding interactive tools to explore the design space, and support\nexample-based design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.08875,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"Security, Availability, and Multiple Information Sources: Exploring\n  Update Behavior of System Administrators\n\n  Experts agree that keeping systems up to date is a powerful security measure.\nPrevious work found that users sometimes explicitly refrain from performing\ntimely updates, e.g., due to bad experiences which has a negative impact on\nend-user security. Another important user group has been investigated less\nextensively: system administrators, who are responsible for keeping complex and\nheterogeneous system landscapes available and secure.\n  In this paper, we sought to understand administrators' behavior, experiences,\nand attitudes regarding updates in a corporate environment. Based on the\nresults of an interview study, we developed an online survey and quantified\ncommon practices and obstacles (e.g., downtime or lack of information about\nupdates). The findings indicate that even experienced administrators struggle\nwith update processes as the consequences of an update are sometimes hard to\nassess. Therefore, we argue that more usable monitoring and update processes\nare essential to guarantee IT security at scale.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.00494,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Content-Aware Automated Parameter Tuning for Approximate Color\n  Transforms\n\n  There are numerous approximate color transforms reported in the literature\nthat aim to reduce display power consumption by imperceptibly changing the\ncolor content of displayed images. To be practical, these techniques need to be\ncontent-aware in picking transformation parameters to preserve perceptual\nquality. This work presents a computationally-efficient method for calculating\na parameter lower bound for approximate color transform parameters based on the\ncontent to be transformed. We conduct a user study with 62 participants and\n6,400 image pair comparisons to derive the proposed solution. We use the user\nstudy results to predict this lower bound reliably with a 1.6% mean squared\nerror by using simple image-color-based heuristics. We show that these\nheuristics have Pearson and Spearman rank correlation coefficients greater than\n0.7 (p<0.01) and that our model generalizes beyond the data from the user\nstudy. The user study results also show that the color transform is able to\nachieve up to 50% power saving with most users reporting negligible visual\nimpairment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12665,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Considerations for Eye Tracking Experiments in Information Retrieval\n\n  In this survey I discuss ophthalmic neurophysiology and the experimental\nconsiderations that must be made to reduce possible noise in an eye-tracking\ndata stream. I also review the history, experiments, technological benefits and\nlimitations of eye-tracking within the information retrieval field. The\nconcepts of aware and adaptive user interfaces are also explored that humbly\nmake an attempt to synthesize work from the fields of industrial engineering\nand psychophysiology with information retrieval.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04361,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Understanding the impact of the alphabetical ordering of names in user\n  interfaces: a gender bias analysis\n\n  Listing people alphabetically on an electronic output device is a traditional\ntechnique, since alphabetical order is easily perceived by users and\nfacilitates access to information. However, this apparently harmless technique,\nespecially when the list is ordered by first name, needs to be used with\ncaution by designers and programmers. We show, via empirical data analysis,\nthat when an interface displays people's first name in alphabetical order in\nseveral pages\/screens, each page\/screen may have imbalances in respect to\ngender of its Top-k individuals.k represents the size of the list of names\nvisualized first, which may be the number of names that fits in a screen page\nof a certain device.The research work was carried out with the analysis of\nactual datasets of names of five different countries. Each dataset has a person\nname and the frequency of adoption of the name in the country.Our analysis\nshows that, even though all countries have exhibit imbalance problems, the\nsamples of individuals with Brazilian and Spanish first names are more prone to\ngender imbalance among their Top-k individuals. These results can be useful for\ndesigners and engineers to construct information systems that avoid gender bias\ninduction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.15824,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Evaluating Semantic Interaction on Word Embeddings via Simulation\n\n  Semantic interaction (SI) attempts to learn the user's cognitive intents as\nthey directly manipulate data projections during sensemaking activity. For text\nanalysis, prior implementations of SI have used common data features, such as\nbag-of-words representations, for machine learning from user interactions.\nInstead, we hypothesize that features derived from deep learning word\nembeddings will enable SI to better capture the user's subtle intents. However,\nevaluating these effects is difficult. SI systems are usually evaluated by a\nhuman-centred qualitative approach, by observing the utility and effectiveness\nof the application for end-users. This approach has drawbacks in terms of\nreplicability, scalability, and objectiveness, which makes it hard to perform\nconvincing contrast experiments between different SI models. To tackle this\nproblem, we explore a quantitative algorithm-centered analysis as a\ncomplementary evaluation approach, by simulating users' interactions and\ncalculating the accuracy of the learned model. We use these methods to compare\nword-embeddings to bag-of-words features for SI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.05286,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"TangToys: Smart Toys that can Communicate and Improve Children's\n  Wellbeing\n\n  Children can find it challenging to communicate their emotions especially\nwhen experiencing mental health challenges. Technological solutions may help\nchildren communicate digitally and receive support from one another as advances\nin networking and sensors enable the real-time transmission of physical\ninteractions. In this work, we pursue the design of multiple tangible user\ninterfaces designed for children containing multiple sensors and feedback\nactuators. Bluetooth is used to provide communication between Tangible Toys\n(TangToys) enabling peer to peer support groups to be developed and allowing\nfeedback to be issued whenever other children are nearby. TangToys can provide\na non-intrusive means for children to communicate their wellbeing through play.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.10614,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Melody: Generating and Visualizing Machine Learning Model Summary to\n  Understand Data and Classifiers Together\n\n  With the increasing sophistication of machine learning models, there are\ngrowing trends of developing model explanation techniques that focus on only\none instance (local explanation) to ensure faithfulness to the original model.\nWhile these techniques provide accurate model interpretability on various data\nprimitive (e.g., tabular, image, or text), a holistic Explainable Artificial\nIntelligence (XAI) experience also requires a global explanation of the model\nand dataset to enable sensemaking in different granularity. Thus, there is a\nvast potential in synergizing the model explanation and visual analytics\napproaches. In this paper, we present MELODY, an interactive algorithm to\nconstruct an optimal global overview of the model and data behavior by\nsummarizing the local explanations using information theory. The result (i.e.,\nan explanation summary) does not require additional learning models,\nrestrictions of data primitives, or the knowledge of machine learning from the\nusers. We also design MELODY UI, an interactive visual analytics system to\ndemonstrate how the explanation summary connects the dots in various XAI tasks\nfrom a global overview to local inspections. We present three usage scenarios\nregarding tabular, image, and text classifications to illustrate how to\ngeneralize model interpretability of different data. Our experiments show that\nour approaches: (1) provides a better explanation summary compared to a\nstraightforward information-theoretic summarization and (2) achieves a\nsignificant speedup in the end-to-end data modeling pipeline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.13048,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Trick the Body Trick the Mind: Avatar representation affects the\n  perception of available action possibilities in Virtual Reality\n\n  In immersive Virtual Reality (VR), your brain can trick you into believing\nthat your virtual hands are your real hands. Manipulating the representation of\nthe body, namely the avatar, is a potentially powerful tool for the design of\ninnovative interactive systems in VR. In this study, we investigated\ninteractive behavior in VR by using the methods of experimental psychology.\nObjects with handles are known to potentiate the afforded action. Participants\ntend to respond faster when the handle is on the same side as the responding\nhand in bi-manual speed response tasks. In the first experiment, we\nsuccessfully replicated this affordance effect in a Virtual Reality (VR)\nsetting. In the second experiment, we showed that the affordance effect was\ninfluenced by the avatar, which was manipulated by two different hand types: 1)\nhand models with full finger tracking that are able to grasp objects, and 2)\ncapsule-shaped -- fingerless -- hand models that are not able to grasp objects.\nWe found that less than 5 minutes of adaptation to an avatar, significantly\naltered the affordance perception. Counter intuitively, action planning was\nsignificantly shorter with the hand model that is not able to grasp. Possibly,\nfewer action possibilities provided an advantage in processing time. The\npresence of a handle speeded up the initiation of the hand movement but slowed\ndown the action completion because of ongoing action planning. The results were\nexamined from a multidisciplinary perspective and the design implications for\nVR applications were discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00058,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"A Bayesian cognition approach for belief updating of correlation\n  judgement through uncertainty visualizations\n\n  Understanding correlation judgement is important to designing effective\nvisualizations of bivariate data. Prior work on correlation perception has not\nconsidered how factors including prior beliefs and uncertainty representation\nimpact such judgements. The present work focuses on the impact of uncertainty\ncommunication when judging bivariate visualizations. Specifically, we model how\nusers update their beliefs about variable relationships after seeing a\nscatterplot with and without uncertainty representation. To model and evaluate\nthe belief updating, we present three studies. Study 1 focuses on a proposed\n''Line + Cone'' visual elicitation method for capturing users' beliefs in an\naccurate and intuitive fashion. The findings reveal that our proposed method of\nbelief solicitation reduces complexity and accurately captures the users'\nuncertainty about a range of bivariate relationships. Study 2 leverages the\n``Line + Cone'' elicitation method to measure belief updating on the\nrelationship between different sets of variables when seeing correlation\nvisualization with and without uncertainty representation. We compare changes\nin users beliefs to the predictions of Bayesian cognitive models which provide\nnormative benchmarks for how users should update their prior beliefs about a\nrelationship in light of observed data. The findings from Study 2 revealed that\none of the visualization conditions with uncertainty communication led to users\nbeing slightly more confident about their judgement compared to visualization\nwithout uncertainty information. Study 3 builds on findings from Study 2 and\nexplores differences in belief update when the bivariate visualization is\ncongruent or incongruent with users' prior belief. Our results highlight the\neffects of incorporating uncertainty representation, and the potential of\nmeasuring belief updating on correlation judgement with Bayesian cognitive\nmodels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.13872,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Modeling the Influence of Visual Density on Cluster Perception in\n  Scatterplots Using Topology\n\n  Scatterplots are used for a variety of visual analytics tasks, including\ncluster identification, and the visual encodings used on a scatterplot play a\ndeciding role on the level of visual separation of clusters. For visualization\ndesigners, optimizing the visual encodings is crucial to maximizing the clarity\nof data. This requires accurately modeling human perception of cluster\nseparation, which remains challenging. We present a multi-stage user study\nfocusing on four factors---distribution size of clusters, number of points,\nsize of points, and opacity of points---that influence cluster identification\nin scatterplots. From these parameters, we have constructed two models, a\ndistance-based model, and a density-based model, using the merge tree data\nstructure from Topological Data Analysis. Our analysis demonstrates that these\nfactors play an important role in the number of clusters perceived, and it\nverifies that the distance-based and density-based models can reasonably\nestimate the number of clusters a user observes. Finally, we demonstrate how\nthese models can be used to optimize visual encodings on real-world data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.02691,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Finally a Case for Collaborative VR?: The Need to Design for Remote\n  Multi-Party Conversations\n\n  Amid current social distancing measures requiring people to work from home,\nthere has been renewed interest on how to effectively converse and collaborate\nremotely utilizing currently available technologies. On the surface, VR\nprovides a perfect platform for effective remote communication. It can transfer\ncontextual and environmental cues and facilitate a shared perspective while\nalso allowing people to be virtually co-located. Yet we argue that currently VR\nis not adequately designed for such a communicative purpose. In this paper, we\noutline three key barriers to using VR for conversational activity : (1)\nvariability of social immersion, (2) unclear user roles, and (3) the need for\neffective shared visual reference. Based on this outline, key design topics are\ndiscussed through a user experience design perspective for considerations in a\nfuture collaborative design framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04495,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Hack.VR: A Programming Game in Virtual Reality\n\n  In this article we describe Hack.VR, an object-oriented programming game in\nvirtual reality. Hack.VR uses a VR programming language in which nodes\nrepresent functions and node connections represent data flow. Using this\nprogramming framework, players reprogram VR objects such as elevators, robots,\nand switches. Hack.VR has been designed to be highly interactable both\nphysically and semantically.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.05473,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Analysing gamification elements in educational environments using an\n  existing Gamification taxonomy\n\n  Gamification has been widely employed in the educational domain over the past\neight years when the term became a trend. However, the literature states that\ngamification still lacks formal definitions to support the design and analysis\nof gamified strategies. This paper analysed the game elements employed in\ngamified learning environments through a previously proposed and evaluated\ntaxonomy while detailing and expanding this taxonomy. In the current paper, we\ndescribe our taxonomy in-depth as well as expand it. Our new structured results\ndemonstrate an extension of the proposed taxonomy which results from this\nprocess, is divided into five dimensions, related to the learner and the\nlearning environment. Our main contribution is the detailed taxonomy that can\nbe used to design and evaluate gamification design in learning environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.1304,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"\"It took me almost 30 minutes to practice this\". Performance and\n  Production Practices in Dance Challenge Videos on TikTok\n\n  TikTok is a music-based video sharing social media app famous for users\ncreating short meme and dance videos. TikTok videos are largely based on\npopular song snippets, which is why lip syncing and dance moves evolve as\nsignificant user performance practices in videos. User prosumption has not yet\nbeen studied regarding the characteristics of TikTok. This paper is based on\nsocial practice and performance theory, social media studies, and participatory\nonline video culture. It uses the #distantdance challenge on TikTok to analyze\nproduction practices and strategies of users through qualitative video product\nanalysis. 92 videos were coded and categorized regarding their visual content\n(who participated in which way) and paratextual elements (used tags and\ncaptions). The visual and (para-)textual elements were then analyzed regarding\nindicators that allow to draw conclusions on users' video creation strategies\nand performance practices in participating in the #distantdance challenge. The\nresults show videos are mainly performed by single white female teenagers\nwearing casual outfits in their bedrooms. Users shared their experiences about\nlearning and performing the dance in video captions. While users prepared\nsettings and outfits for their performance, the majority of performances seems\nrather unplanned or spontaneous. This indicates most videos might be part of a\nseries of user attempts to master the dance challenge resulting in posting the\nfirst successful video performance to TikTok. In addition to the dance moves,\nparticipants also added gestures as closing elements to their performances.\nThis indicates their knowledge of using signals as part of an online community\nwhile at the same time manifesting their belongingness to the community. These\nfirst results of a qualitative product analysis illustrate some of users'\nmotivations and effort to participate in TikTok dance challenges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.08806,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Varying Annotations in the Steps of the Visual Analysis\n\n  Annotations in Visual Analytics (VA) have become a common means to support\nthe analysis by integrating additional information into the VA system. That\nadditional information often depends on the current process step in the visual\nanalysis. For example, the data preprocessing step has data structuring\noperations while the data exploration step focuses on user interaction and\ninput. Describing suitable annotations to meet the goals of the different steps\nis challenging. To tackle this issue, we identify individual annotations for\neach step and outline their gathering and design properties for the visual\nanalysis of heterogeneous clinical data. We integrate our annotation design\ninto a visual analysis tool to show its applicability to data from the\nophthalmic domain. In interviews and application sessions with experts we asses\nits usefulness for the analysis of patients with different medications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09233,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"An Examination of Grouping and Spatial Organization Tasks for\n  High-Dimensional Data Exploration\n\n  How do analysts think about grouping and spatial operations? This overarching\nquestion incorporates a number of points for investigation, including\nunderstanding how analysts begin to explore a dataset, the types of\ngrouping\/spatial structures created and the operations performed on them, the\nrelationship between grouping and spatial structures, the decisions analysts\nmake when exploring individual observations, and the role of external\ninformation. This work contributes the design and results of such a study, in\nwhich a group of participants are asked to organize the data contained within\nan unfamiliar quantitative dataset. We identify several overarching approaches\ntaken by participants to design their organizational space, discuss the\ninteractions performed by the participants, and propose design recommendations\nto improve the usability of future high-dimensional data exploration tools that\nmake use of grouping (clustering) and spatial (dimension reduction) operations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.01769,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"FaceOff: Detecting Face Touching with a Wrist-Worn Accelerometer\n\n  According to the CDC, one key step of preventing oneself from contracting\ncoronavirus (COVID-19) is to avoid touching eyes, nose, and mouth with unwashed\nhands. However, touching one's face is a frequent and spontaneous\nbehavior---one study observed subjects touching their faces on average 23 times\nper hour. Creative solutions have emerged amongst some recent commercial and\nhobbyists' projects, yet most either are closed-source or lack validation in\nperformance. We develop FaceOff---a sensing technique using a commodity\nwrist-worn accelerometer to detect face-touching behavior based on the specific\nmotion pattern of raising one's hand towards the face. We report a survey\n(N=20) that elicits different ways people touch their faces, an algorithm that\ntemporally ensembles data-driven models to recognize when a face touching\nbehavior occurs and results from a preliminary user testing (N=3 for a total of\nabout 90 minutes).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.04559,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000075168,
      "text":"Breaking the Screen: Interaction Across Touchscreen Boundaries in\n  Virtual Reality for Mobile Knowledge Workers\n\n  Virtual Reality (VR) has the potential to transform knowledge work. One\nadvantage of VR knowledge work is that it allows extending 2D displays into the\nthird dimension, enabling new operations, such as selecting overlapping objects\nor displaying additional layers of information. On the other hand, mobile\nknowledge workers often work on established mobile devices, such as tablets,\nlimiting interaction with those devices to a small input space. This challenge\nof a constrained input space is intensified in situations when VR knowledge\nwork is situated in cramped environments, such as airplanes and touchdown\nspaces.\n  In this paper, we investigate the feasibility of interacting jointly between\nan immersive VR head-mounted display and a tablet within the context of\nknowledge work. Specifically, we 1) design, implement and study how to interact\nwith information that reaches beyond a single physical touchscreen in VR; 2)\ndesign and evaluate a set of interaction concepts; and 3) build example\napplications and gather user feedback on those applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.08282,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000065234,
      "text":"Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic\n  Graphs\n\n  The overview-driven visual analysis of large-scale dynamic graphs poses a\nmajor challenge. We propose Multiscale Snapshots, a visual analytics approach\nto analyze temporal summaries of dynamic graphs at multiple temporal scales.\nFirst, we recursively generate temporal summaries to abstract overlapping\nsequences of graphs into compact snapshots. Second, we apply graph embeddings\nto the snapshots to learn low-dimensional representations of each sequence of\ngraphs to speed up specific analytical tasks (e.g., similarity search). Third,\nwe visualize the evolving data from a coarse to fine-granular snapshots to\nsemi-automatically analyze temporal states, trends, and outliers. The approach\nenables to discover similar temporal summaries (e.g., recurring states),\nreduces the temporal data to speed up automatic analysis, and to explore both\nstructural and temporal properties of a dynamic graph. We demonstrate the\nusefulness of our approach by a quantitative evaluation and the application to\na real-world dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.06099,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"Identifying Usability Issues of Software Analytics Applications in\n  Immersive Augmented Reality\n\n  Software analytics in augmented reality (AR) is said to have great potential.\nOne reason why this potential is not yet fully exploited may be usability\nproblems of the AR user interfaces. We present an iterative and qualitative\nusability evaluation with 15 subjects of a state-of-the-art application for\nsoftware analytics in AR. We could identify and resolve numerous usability\nissues. Most of them were caused by applying conventional user interface\nelements, such as dialog windows, buttons, and scrollbars. The used city\nvisualization, however, did not cause any usability issues. Therefore, we argue\nthat future work should focus on making conventional user interface elements in\nAR obsolete by integrating their functionality into the immersive\nvisualization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.1125,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"What Do We Actually Learn from Evaluations in the \"Heroic Era\" of\n  Visualization?\n\n  We often point to the relative increase in the amount and sophistication of\nevaluations of visualization systems versus the earliest days of the field as\nevidence that we are maturing as a field. I am not so convinced. In particular,\nI feel that evaluations of visualizations, as they are ordinarily performed in\nthe field or asked for by reviewers, fail to tell us very much that is useful\nor transferable about visualization systems, regardless of the statistical\nrigor or ecological validity of the evaluation. Through a series of thought\nexperiments, I show how our current conceptions of visualization evaluations\ncan be incomplete, capricious, or useless for the goal of furthering the field,\nmore in line with the \"heroic age\" of medical science than the rigorous\nevidence-based field we might aspire to be. I conclude by suggesting that our\nmodels for designing evaluations, and our priorities as a field, should be\nrevisited.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.13592,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000085102,
      "text":"Mapping the Global South: Equal-Area Projections for Choropleth Maps\n\n  Choropleth maps are among the most common visualization techniques used to\npresent geographical data. These maps require an equal-area projection but\nthere are no clear criteria for selecting one. We collaborated with 20 social\nscientists researching on the Global South, interested in using choropleth\nmaps, to investigate their design choices according to their research tasks. We\nasked them to design world choropleth maps through a survey, and analyzed their\nanswers both qualitatively and quantitatively. The results suggest that the\ndesign choices of map projection, center, scale, and color scheme, were\ninfluenced by their personal research goals and the tasks. The projection was\nconsidered the most important choice and the Equal Earth projection was the\nmost common projection used. Our study takes the first substantial step in\ninvestigating projection choices for world choropleth maps in applied\nvisualization research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11319,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"TradAO: A Visual Analytics System for Trading Algorithm Optimization\n\n  With the wide applications of algorithmic trading, it has become critical for\ntraders to build a winning trading algorithm to beat the market. However, due\nto the lack of efficient tools, traders mainly rely on their memory to manually\ncompare the algorithm instances of a trading algorithm and further select the\nbest trading algorithm instance for the real trading deployment. We work\nclosely with industry practitioners to discover and consolidate user\nrequirements and develop an interactive visual analytics system for trading\nalgorithm optimization. Structured expert interviews are conducted to\nevaluateTradAOand a representative case study is documented for illustrating\nthe system effectiveness. To the best of our knowledge, previous financial data\nvisual analyses have mainly aimed to assist investment managers in investment\nportfolio analysis but have neglected the need of traders in developing trading\nalgorithms for portfolio execution.TradAOis the first visual analytics system\nthat assists users in comprehensively exploring the performances of a trading\nalgorithm with different parameter settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02016,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000062585,
      "text":"Activity and mood-based routing for autonomous vehicles\n\n  A significant amount of our daily lives is dedicated to driving, leading to\nan unavoidable exposure to driving-related stress. The rise of autonomous\nvehicles will likely lessen the extent of this stress and enhance the routine\ntraveling experience. Yet, no matter how diverse they may be, current routing\ncriteria are limited to considering only the passive preferences of a vehicle's\nusers. Thus, to enhance the overall driving experience in autonomous vehicles,\nwe advocate here for the diversification of routing criteria, by additionally\nemphasizing activity- and mood-based requirements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.10723,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"NL4DV: A Toolkit for Generating Analytic Specifications for Data\n  Visualization from Natural Language Queries\n\n  Natural language interfaces (NLIs) have shown great promise for visual data\nanalysis, allowing people to flexibly specify and interact with visualizations.\nHowever, developing visualization NLIs remains a challenging task, requiring\nlow-level implementation of natural language processing (NLP) techniques as\nwell as knowledge of visual analytic tasks and visualization design. We present\nNL4DV, a toolkit for natural language-driven data visualization. NL4DV is a\nPython package that takes as input a tabular dataset and a natural language\nquery about that dataset. In response, the toolkit returns an analytic\nspecification modeled as a JSON object containing data attributes, analytic\ntasks, and a list of Vega-Lite specifications relevant to the input query. In\ndoing so, NL4DV aids visualization developers who may not have a background in\nNLP, enabling them to create new visualization NLIs or incorporate natural\nlanguage input within their existing systems. We demonstrate NL4DV's usage and\ncapabilities through four examples: 1) rendering visualizations using natural\nlanguage in a Jupyter notebook, 2) developing a NLI to specify and edit\nVega-Lite charts, 3) recreating data ambiguity widgets from the DataTone\nsystem, and 4) incorporating speech input to create a multimodal visualization\nsystem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11834,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"Conversations On Multimodal Input Design With Older Adults\n\n  Multimodal input systems can help bridge the wide range of physical abilities\nfound in older generations. After conducting a survey\/interview session with a\ngroup of older adults at an assisted living community we believe that gesture\nand speech should be the main inputs for that system. Additionally,\ncollaborative design of new systems was found to be useful for facilitating\nconversations around input design with this demographic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.1351,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Multi-Modal End-User Programming of Web-Based Virtual Assistant Skills\n\n  While Alexa can perform over 100,000 skills on paper, its capability covers\nonly a fraction of what is possible on the web. To reach the full potential of\nan assistant, it is desirable that individuals can create skills to automate\ntheir personal web browsing routines. Many seemingly simple routines, however,\nsuch as monitoring COVID-19 stats for their hometown, detecting changes in\ntheir child's grades online, or sending personally-addressed messages to a\ngroup, cannot be automated without conventional programming concepts such as\nconditional and iterative evaluation. This paper presents VASH (Voice Assistant\nScripting Helper), a new system that empowers users to create useful web-based\nvirtual assistant skills without learning a formal programming language. With\nVASH, the user demonstrates their task of interest in the browser and issues a\nfew voice commands, such as naming the skills and adding conditions on the\naction. VASH turns these multi-modal specifications into skills that can be\ninvoked invoice on a virtual assistant. These skills are represented in a\nformal programming language we designed called WebTalk, which supports\nparameterization, function invocation, conditionals, and iterative execution.\nVASH is a fully working prototype that works on the Chrome browser on\nreal-world websites. Our user study shows that users have many web routines\nthey wish to automate, 81% of which can be expressed using VASH. We found that\nVASH Is easy to learn, and that a majority of the users in our study want to\nuse our system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12147,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"Good for the Many or Best for the Few? A Dilemma in the Design of\n  Algorithmic Advice\n\n  Applications in a range of domains, including route planning and well-being,\noffer advice based on the social information available in prior users'\naggregated activity. When designing these applications, is it better to offer:\na) advice that if strictly adhered to is more likely to result in an individual\nsuccessfully achieving their goal, even if fewer users will choose to adopt it?\nor b) advice that is likely to be adopted by a larger number of users, but\nwhich is sub-optimal with regard to any particular individual achieving their\ngoal? We identify this dilemma, characterized as Goal-Directed vs.\nAdoption-Directed advice, and investigate the design questions it raises\nthrough an online experiment undertaken in four advice domains (financial\ninvestment, making healthier lifestyle choices, route planning, training for a\n5k run), with three user types, and across two levels of uncertainty. We report\nfindings that suggest a preference for advice favoring individual goal\nattainment over higher user adoption rates, albeit with significant variation\nacross advice domains; and discuss their design implications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.0983,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Brushing Feature Values in Immersive Graph Visualization Environment\n\n  There are a variety of graphs where multidimensional feature values are\nassigned to the nodes. Visualization of such datasets is not an easy task since\nthey are complex and often huge. Immersive Analytics is a powerful approach to\nsupport the interactive exploration of such large and complex data. Many recent\nstudies on graph visualization have applied immersive analytics frameworks.\nHowever, there have been few studies on immersive analytics for visualization\nof multidimensional attributes associated with the input graphs. This paper\npresents a new immersive analytics system that supports the interactive\nexploration of multidimensional feature values assigned to the nodes of input\ngraphs. The presented system displays label-axes corresponding to the\ndimensions of feature values, and label-edges that connect label-axes and\ncorresponding to the nodes. The system supports brushing operations which\ncontrols the display of edges that connect a label-axis and nodes of the graph.\nThis paper introduces visualization examples with a graph dataset of Twitter\nusers and reviews by experts on graph data analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.10915,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Towards Better Bus Networks: A Visual Analytics Approach\n\n  Bus routes are typically updated every 3-5 years to meet constantly changing\ntravel demands. However, identifying deficient bus routes and finding their\noptimal replacements remain challenging due to the difficulties in analyzing a\ncomplex bus network and the large solution space comprising alternative routes.\nMost of the automated approaches cannot produce satisfactory results in\nreal-world settings without laborious inspection and evaluation of the\ncandidates. The limitations observed in these approaches motivate us to\ncollaborate with domain experts and propose a visual analytics solution for the\nperformance analysis and incremental planning of bus routes based on an\nexisting bus network. Developing such a solution involves three major\nchallenges, namely, a) the in-depth analysis of complex bus route networks, b)\nthe interactive generation of improved route candidates, and c) the effective\nevaluation of alternative bus routes. For challenge a, we employ an\noverview-to-detail approach by dividing the analysis of a complex bus network\ninto three levels to facilitate the efficient identification of deficient\nroutes. For challenge b, we improve a route generation model and interpret the\nperformance of the generation with tailored visualizations. For challenge c, we\nincorporate a conflict resolution strategy in the progressive decision-making\nprocess to assist users in evaluating the alternative routes and finding the\nmost optimal one. The proposed system is evaluated with two usage scenarios\nbased on real-world data and received positive feedback from the experts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.04543,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Pen-based Interaction with Spreadsheets in Mobile Virtual Reality\n\n  Virtual Reality (VR) can enhance the display and interaction of mobile\nknowledge work and in particular, spreadsheet applications. While spreadsheets\nare widely used yet are challenging to interact with, especially on mobile\ndevices, using them in VR has not been explored in depth. A special uniqueness\nof the domain is the contrast between the immersive and large display space\nafforded by VR, contrasted by the very limited interaction space that may be\nafforded for the information worker on the go, such as an airplane seat or a\nsmall work-space. To close this gap, we present a tool-set for enhancing\nspreadsheet interaction on tablets using immersive VR headsets and pen-based\ninput. This combination opens up many possibilities for enhancing the\nproductivity for spreadsheet interaction. We propose to use the space around\nand in front of the tablet for enhanced visualization of spreadsheet data and\nmeta-data. For example, extending sheet display beyond the bounds of the\nphysical screen, or easier debugging by uncovering hidden dependencies between\nsheet's cells. Combining the precise on-screen input of a pen with spatial\nsensing around the tablet, we propose tools for the efficient creation and\nediting of spreadsheets functions such as off-the-screen layered menus,\nvisualization of sheets dependencies, and gaze-and-touch-based switching\nbetween spreadsheet tabs. We study the feasibility of the proposed tool-set\nusing a video-based online survey and an expert-based assessment of indicative\nhuman performance potential.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02582,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Silhouette Games: An Interactive One-Way Mirror Approach to Watching\n  Players in VR\n\n  Watching others play is a key ingredient of digital games and an important\naspect of games user research. However, spectatorship is not very popular in\nvirtual reality, as such games strongly rely on one's feelings of presence. In\nother words, the head-mounted display creates a barrier between the player and\nthe audience. We contribute an alternative watching approach consisting of two\nmajor components: a dynamic view frustum that renders the game scene from the\ncurrent spectator position and a one-way mirror in front of the screen. This\nmirror, together with our silhouetting algorithm, allows seeing the player's\nreflection at the correct position in the virtual world. An exploratory survey\nemphasizes the overall positive experience of the viewers in our setup. In\nparticular, the participants enjoyed their ability to explore the virtual\nsurrounding via physical repositioning and to observe the blended player during\nobject manipulations. Apart from requesting a larger screen, the participants\nexpressed a strong need to interact with the player. Consequently, we suggest\nutilizing our technology as a foundation for novel playful experiences with the\noverarching goal to transform the passive spectator into a collocated player.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00192,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Comparative Layouts Revisited: Design Space, Guidelines, and Future\n  Directions\n\n  We present a systematic review on three comparative layouts (i.e.,\njuxtaposition, superposition, and explicit-encoding) which are information\nvisualization (InfoVis) layouts designed to support comparison tasks. For the\nlast decade, these layouts have served as fundamental idioms in designing many\nvisualization systems. However, we found that the layouts have been used with\ninconsistent terms and confusion, and the lessons from previous studies are\nfragmented. The goal of our research is to distill the results from previous\nstudies into a consistent and reusable framework. We review 127 research\npapers, including 15 papers with quantitative user studies, which employed\ncomparative layouts. We first alleviate the ambiguous boundaries in the design\nspace of comparative layouts by suggesting lucid terminology (e.g., chart-wise\nand item-wise juxtaposition). We then identify the diverse aspects of\ncomparative layouts, such as the advantages and concerns of using each layout\nin the real-world scenarios and researchers' approaches to overcome the\nconcerns. Building our knowledge on top of the initial insights gained from the\nGleicher et al.'s survey, we elaborate on relevant empirical evidence that we\ndistilled from our survey (e.g., the actual effectiveness of the layouts in\ndifferent study settings) and identify novel facets that the original work did\nnot cover (e.g., the familiarity of the layouts to people). Finally, we show\nthe consistent and contradictory results on the performance of comparative\nlayouts and offer practical implications for using the layouts by suggesting\ntrade-offs and seven actionable guidelines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.03143,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Cyber-Human System for Remote Collaborators\n\n  With the increasing ubiquity of technology in our daily lives, the complexity\nof our environment and the mechanisms required to function have also increased\nexponentially. Failure of any of the mechanical and digital devices that we\nrely on can be extremely disruptive. At times, the presence of an expert is\nneeded to analyze, troubleshoot, and fix the problem. The increased demand and\nrapidly evolving mechanisms have led to an insufficient amount of skilled\nworkers, thus resulting in long waiting times for consumers, and\ncorrespondingly high prices for expert services. We assert that performing a\nrepair task with the guidance of experts from any geographical location\nprovides an appropriate solution to the growing demand for handyman skills.\nThis paper proposes an innovative mechanism for two geographically separated\npeople to collaborate on a physical task. It also offers novel methods to\nanalyze the efficiency of a collaboration system and a collaboration protocol\nthrough complexity indices. Using the innovative Collaborative Appliance for\nRemote-help (CARE) and with the support of a remote expert, fifty-nine subjects\nwith minimal or no prior mechanical knowledge were able to elevate a car for\nreplacing a tire; in a second experiment, thirty subjects with minimal or no\nprior plumbing knowledge were able to change the cartridge of a faucet. In both\ncases, average times were close to standard average repair times, and more\nimportantly, both tasks were completed with total accuracy. Our experiments and\nresults show that one can use the developed mechanism and methods for expanding\nthe protocols for a variety of home, vehicle, and appliance repairs and\ninstallations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.07322,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"dg2pix: Pixel-Based Visual Analysis of Dynamic Graphs\n\n  Presenting long sequences of dynamic graphs remains challenging due to the\nunderlying large-scale and high-dimensional data. We propose dg2pix, a novel\npixel-based visualization technique, to visually explore temporal and\nstructural properties in long sequences of large-scale graphs. The approach\nconsists of three main steps: (1) the multiscale modeling of the temporal\ndimension; (2) unsupervised graph embeddings to learn low-dimensional\nrepresentations of the dynamic graph data; and (3) an interactive pixel-based\nvisualization to simultaneously explore the evolving data at different temporal\naggregation scales. dg2pix provides a scalable overview of a dynamic graph,\nsupports the exploration of long sequences of high-dimensional graph data, and\nenables the identification and comparison of similar temporal states. We show\nthe applicability of the technique to synthetic and real-world datasets,\ndemonstrating that temporal patterns in dynamic graphs can be identified and\ninterpreted over time. dg2pix contributes a suitable intermediate\nrepresentation between node-link diagrams at the high detail end and matrix\nrepresentations on the low detail end.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000059605,
      "text":"CommunityClick: Capturing and Reporting Community Feedback from Town\n  Halls to Improve Inclusivity\n\n  Local governments still depend on traditional town halls for community\nconsultation, despite problems such as a lack of inclusive participation for\nattendees and difficulty for civic organizers to capture attendees' feedback in\nreports. Building on a formative study with 66 town hall attendees and 20\norganizers, we designed and developed CommunityClick, a communitysourcing\nsystem that captures attendees' feedback in an inclusive manner and enables\norganizers to author more comprehensive reports. During the meeting, in\naddition to recording meeting audio to capture vocal attendees' feedback, we\nmodify iClickers to give voice to reticent attendees by allowing them to\nprovide real-time feedback beyond a binary signal. This information then\nautomatically feeds into a meeting transcript augmented with attendees'\nfeedback and organizers' tags. The augmented transcript along with a\nfeedback-weighted summary of the transcript generated from text analysis\nmethods is incorporated into an interactive authoring tool for organizers to\nwrite reports. From a field experiment at a town hall meeting, we demonstrate\nhow CommunityClick can improve inclusivity by providing multiple avenues for\nattendees to share opinions. Additionally, interviews with eight expert\norganizers demonstrate CommunityClick's utility in creating more comprehensive\nand accurate reports to inform critical civic decision-making. We discuss the\npossibility of integrating CommunityClick with town hall meetings in the future\nas well as expanding to other domains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Understanding User Experience of COVID-19 Maps through Remote\n  Elicitation Interviews\n\n  During the coronavirus pandemic, visualizations gained a new level of\npopularity and meaning for a wider audience. People were bombarded with a wide\nset of public health visualizations ranging from simple graphs to complex\ninteractive dashboards. In a pandemic setting, where large amounts of the world\npopulation are socially distancing themselves, it becomes an urgent need to\nrefine existing user experience evaluation methods for remote settings to\nunderstand how people make sense out of COVID-19 related visualizations. When\nevaluating visualizations aimed towards the general public with vastly\ndifferent socio-demographic backgrounds and varying levels of technical\nsavviness and data literacy, it is important to understand user feedback beyond\naspects such as speed, task accuracy, or usability problems. As a part of this\nwider evaluation perspective, micro-phenomenology has been used to evaluate\nstatic and narrative visualizations to reveal the lived experience in a\ndetailed way. Building upon these studies, we conducted a user study to\nunderstand how to employ Elicitation (aka Micro-phenomenological) interviews in\nremote settings. In a case study, we investigated what experiences the\nparticipants had with map-based interactive visualizations. Our findings reveal\npositive and negative aspects of conducting Elicitation interviews remotely.\nOur results can inform the process of planning and executing remote Elicitation\ninterviews to evaluate interactive visualizations. In addition, we share\nrecommendations regarding visualization techniques and interaction design about\npublic health data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.08761,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Online Knowledge Base for Designing Shape-changing Interfaces using\n  Modular Workshop Elements\n\n  Building and maintaining knowledge about specific interface technologies is a\nchallenge. Current solutions include standard file-based document repositories,\nwikis, and other online tools. However, these solutions are often only\navailable in intranets, become outdated and do not support the acquisition of\nknowledge in an efficient manner. The effort to gain an overview and detailed\nknowledge about novel interface technologies can be overwhelming and requires\nto research and read many technical reports and scientific publications. We\npropose to implement open source online knowledge bases that include building\nblocks for creating custom workshops to understand and apply the contained\nknowledge. We demonstrate this concept with a knowledge base for shape-changing\ninterfaces (SCI-KB). The SCI-KB is hosted online at GitHub and fosters\ncollaborative creation of knowledge elements accompanied by practical exercises\nand workshop elements that can be combined and adapted by individuals or groups\nof people new to the topic of shape-changing interfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06155,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Chemicals in the Creek: designing a situated data physicalization of\n  open government data with the community\n\n  Over the last decade growing amounts of government data have been made\navailable in an attempt to increase transparency and civic participation, but\nit is unclear if this data serves non-expert communities due to gaps in access\nand the technical knowledge needed to interpret this \"open\" data. We conducted\na two-year design study focused on the creation of a community-based data\ndisplay using the United States Environmental Protection Agency data on water\npermit violations by oil storage facilities on the Chelsea Creek in\nMassachusetts to explore whether situated data physicalization and\nParticipatory Action Research could support meaningful engagement with open\ndata. We selected this data as it is of interest to local groups and available\nonline, yet remains largely invisible and inaccessible to the Chelsea\ncommunity. The resulting installation, Chemicals in the Creek, responds to the\ncall for community-engaged visualization processes and provides an application\nof situated methods of data representation. It proposes event-centered and\npower-aware modes of engagement using contextual and embodied data\nrepresentations. The design of Chemicals in the Creek is grounded in\ninteractive workshops and we analyze it through event observation, interviews,\nand community outcomes. We reflect on the role of community engaged research in\nthe Information Visualization community relative to recent conversations on new\napproaches to design studies and evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01785,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Data-First Visualization Design Studies\n\n  We introduce the notion of a data-first design study which is triggered by\nthe acquisition of real-world data instead of specific stakeholder analysis\nquestions. We propose an adaptation of the design study methodology framework\nto provide practical guidance and to aid transferability to other data-first\ndesign processes. We discuss opportunities and risks by reflecting on two of\nour own data-first design studies. We review 64 previous design studies and\nidentify 16 of them as edge cases with characteristics that may indicate a\ndata-first design process in action.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02288,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Externalizing Transformations of Historical Documents: Opportunities for\n  Provenance-Driven Visualization\n\n  Transcription, annotation, digitization and\/or visualization are common\ntransformations that historical documents such as national records, birth\/death\nregisters, university records, letters or books undergo. Reasons for those\ntransformations span from the (physical) protection of the original materials\nto disclosure of 'hidden' information or patterns within the documents. Even\nthough such transformations bring new insights and perspectives on the\ndocuments, they also modify the documents' content, structure, and\/or\nartifactual form and thus, occlude prior knowledge and interpretation. When it\ncomes to visualization as a means to transform historical documents from\nwritten to abstract visual form, there is typically little acknowledgment or\neven understanding of the previous transformation steps these documents have\ngone through. The 'tremendous rhetorical force' of visualization, we argue,\nshould not be at the expense of the multiple pasts, contexts, and curators that\nare inherent in historical record collections. Rather, the urgent question for\nthe fields of visualization and the (digital) humanities is how to better\nsupport awareness of these multiple layers of interpretation and the people\nbehind them when representing historical documents. We begin to address this\nquestion based on a collection of historical university records by (a)\ninvestigating common transformation processes of historical documents, and (b)\ndiscussing opportunities and challenges for making such transformations\ntransparent through what we call 'provenance-driven visualization'; the idea\nfor a visualization that makes visible the layers of transformation (including\ninterpretation, re-structuring, and curation) inherent in historical documents.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.11247,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Novel Computational Linguistic Measures, Dialogue System and the\n  Development of SOPHIE: Standardized Online Patient for Healthcare Interaction\n  Education\n\n  In this paper, we describe the iterative participatory design of SOPHIE, an\nonline virtual patient for feedback-based practice of sensitive\npatient-physician conversations, and discuss an initial qualitative evaluation\nof the system by professional end users. The design of SOPHIE was motivated\nfrom a computational linguistic analysis of the transcripts of 383\npatient-physician conversations from an essential office visit of late stage\ncancer patients with their oncologists. We developed methods for the automatic\ndetection of two behavioral paradigms, lecturing and positive language usage\npatterns (sentiment trajectory of conversation), that are shown to be\nsignificantly associated with patient prognosis understanding. These automated\nmetrics associated with effective communication were incorporated into SOPHIE,\nand a pilot user study identified that SOPHIE was favorably reviewed by a user\ngroup of practicing physicians.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02063,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000080466,
      "text":"ViS-\\'A-ViS : Detecting Similar Patterns in Annotated Literary Text\n\n  We present a web-based system called ViS-\\'A-ViS aiming to assist literary\nscholars in detecting repetitive patterns in an annotated textual corpus.\nPattern detection is made possible using distant reading visualizations that\nhighlight potentially interesting patterns. In addition, the system uses\ntime-series alignment algorithms, and in particular, dynamic time warping\n(DTW), to detect patterns automatically. We present a case-study where an\nancient Hebrew poetry corpus was manually annotated with figurative language\ndevices as metaphors and similes and then loaded into the system. Preliminary\nresults confirm the effectiveness of the system in analyzing the annotated data\nand in detecting literary patterns and similarities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"GazeBase: A Large-Scale, Multi-Stimulus, Longitudinal Eye Movement\n  Dataset\n\n  This manuscript presents GazeBase, a large-scale longitudinal dataset\ncontaining 12,334 monocular eye-movement recordings captured from 322\ncollege-aged subjects. Subjects completed a battery of seven tasks in two\ncontiguous sessions during each round of recording, including a - 1) fixation\ntask, 2) horizontal saccade task, 3) random oblique saccade task, 4) reading\ntask, 5\/6) free viewing of cinematic video task, and 7) gaze-driven gaming\ntask. A total of nine rounds of recording were conducted over a 37 month\nperiod, with subjects in each subsequent round recruited exclusively from the\nprior round. All data was collected using an EyeLink 1000 eye tracker at a\n1,000 Hz sampling rate, with a calibration and validation protocol performed\nbefore each task to ensure data quality. Due to its large number of subjects\nand longitudinal nature, GazeBase is well suited for exploring research\nhypotheses in eye movement biometrics, along with other emerging applications\napplying machine learning techniques to eye movement signal analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.12488,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Mental Health and Sensing\n\n  Mental health is a global epidemic, affecting close to half a billion people\nworldwide. Chronic shortage of resources hamper detection and recovery of\naffected people. Effective sensing technologies can help fight the epidemic\nthrough early detection, prediction, and resulting proper treatment. Existing\nand novel technologies for sensing mental health state could address the\naforementioned concerns by activating granular tracking of physiological,\nbehavioral, and social signals pertaining to problems in mental health. Our\npaper focuses on the available methods of sensing mental health problems\nthrough direct and indirect measures. We see how active and passive sensing by\ntechnologies as well as reporting from relevant sources can contribute toward\nthese detection methods. We also see available methods of therapeutic treatment\navailable through digital means. We highlight a few key intervention\ntechnologies that are being developed by researchers to fight against mental\nillness issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.07095,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Communicative Visualizations as a Learning Problem\n\n  Significant research has provided robust task and evaluation languages for\nthe analysis of exploratory visualizations. Unfortunately, these taxonomies\nfail when applied to communicative visualizations. Instead, designers often\nresort to evaluating communicative visualizations from the cognitive efficiency\nperspective: \"can the recipient accurately decode my message\/insight?\" However,\ndesigners are unlikely to be satisfied if the message went 'in one ear and out\nthe other.' The consequence of this inconsistency is that it is difficult to\ndesign or select between competing options in a principled way. The problem we\naddress is the fundamental mismatch between how designers want to describe\ntheir intent, and the language they have. We argue that visualization designers\ncan address this limitation through a learning lens: that the recipient is a\nstudent and the designer a teacher. By using learning objectives, designers can\nbetter define, assess, and compare communicative visualizations. We illustrate\nhow the learning-based approach provides a framework for understanding a wide\narray of communicative goals. To understand how the framework can be applied\n(and its limitations), we surveyed and interviewed members of the Data\nVisualization Society using their own visualizations as a probe. Through this\nstudy we identified the broad range of objectives in communicative\nvisualizations and the prevalence of certain objective types.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.05936,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Geo-Spatial Data Visualization and Critical Metrics Predictions for\n  Canadian Elections\n\n  Open data published by various organizations is intended to make the data\navailable to the public. All over the world, numerous organizations maintain a\nconsiderable number of open databases containing a lot of facts and numbers.\nHowever, most of them do not offer a concise and insightful data interpretation\nor visualization tool, which can help users to process all of the information\nin a consistently comparable way. Canadian Federal and Provincial Elections is\nan example of these databases. This information exists in numerous websites, as\nseparate tables so that the user needs to traverse through a tree structure of\nscattered information on the site, and the user is left with the comparison,\nwithout providing proper tools, data-interpretation or visualizations. In this\npaper, we provide technical details of addressing this problem, by using the\nCanadian Elections data (since 1867) as a specific case study as it has\nnumerous technical challenges. We hope that the methodology used here can help\nin developing similar tools to achieve some of the goals of publicly available\ndatasets. The developed tool contains data visualization, trend analysis, and\nprediction components. The visualization enables the users to interact with the\ndata through various techniques, including Geospatial visualization. To\nreproduce the results, we have open-sourced the tool.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00449,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Towards Evaluating Exploratory Model Building Process with AutoML\n  Systems\n\n  The use of Automated Machine Learning (AutoML) systems are highly open-ended\nand exploratory. While rigorously evaluating how end-users interact with AutoML\nis crucial, establishing a robust evaluation methodology for such exploratory\nsystems is challenging. First, AutoML is complex, including multiple\nsub-components that support a variety of sub-tasks for synthesizing ML\npipelines, such as data preparation, problem specification, and model\ngeneration, making it difficult to yield insights that tell us which components\nwere successful or not. Second, because the usage pattern of AutoML is highly\nexploratory, it is not possible to rely solely on widely used task efficiency\nand effectiveness metrics as success metrics. To tackle the challenges in\nevaluation, we propose an evaluation methodology that (1) guides AutoML\nbuilders to divide their AutoML system into multiple sub-system components, and\n(2) helps them reason about each component through visualization of end-users'\nbehavioral patterns and attitudinal data. We conducted a study to understand\nwhen, how, why, and applying our methodology can help builders to better\nunderstand their systems and end-users. We recruited 3 teams of professional\nAutoML builders. The teams prepared their own systems and let 41 end-users use\nthe systems. Using our methodology, we visualized end-users' behavioral and\nattitudinal data and distributed the results to the teams. We analyzed the\nresults in two directions: what types of novel insights the AutoML builders\nlearned from end-users, and (2) how the evaluation methodology helped the\nbuilders to understand workflows and the effectiveness of their systems. Our\nfindings suggest new insights explaining future design opportunities in the\nAutoML domain as well as how using our methodology helped the builders to\ndetermine insights and let them draw concrete directions for improving their\nsystems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.04192,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000306633,
      "text":"Brotate and Tribike: Designing Smartphone Control for Cycling\n\n  The more people commute by bicycle, the higher is the number of cyclists\nusing their smartphones while cycling and compromising traffic safety. We have\ndesigned, implemented and evaluated two prototypes for smartphone control\ndevices that do not require the cyclists to remove their hands from the\nhandlebars - the three-button device Tribike and the rotation-controlled\nBrotate. The devices were the result of a user-centred design process where we\nidentified the key features needed for a on-bike smartphone control device. We\nevaluated the devices in a biking exercise with 19 participants, where users\ncompleted a series of common smartphone tasks. The study showed that Brotate\nallowed for significantly more lateral control of the bicycle and both devices\nreduced the cognitive load required to use the smartphone. Our work contributes\ninsights into designing interfaces for cycling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01697,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"PRAGMA: Interactively Constructing Functional Brain Parcellations\n\n  A prominent goal of neuroimaging studies is mapping the human brain, in order\nto identify and delineate functionally-meaningful regions and elucidate their\nroles in cognitive behaviors. These brain regions are typically represented by\natlases that capture general trends over large populations. Despite being\nindispensable to neuroimaging experts, population-level atlases do not capture\nindividual differences in functional organization. In this work, we present an\ninteractive visualization method, PRAGMA, that allows domain experts to derive\nscan-specific parcellations from established atlases. PRAGMA features a\nuser-driven, hierarchical clustering scheme for defining temporally correlated\nparcels in varying granularity. The visualization design supports the user in\nmaking decisions on how to perform clustering, namely when to expand, collapse,\nor merge parcels. This is accomplished through a set of linked and coordinated\nviews for understanding the user's current hierarchy, assessing intra-cluster\nvariation, and relating parcellations to an established atlas. We assess the\neffectiveness of PRAGMA through a user study with four neuroimaging domain\nexperts, where our results show that PRAGMA shows the potential to enable\nexploration of individualized and state-specific brain parcellations and to\noffer interesting insights into functional brain networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02057,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Augmenting Sheet Music with Rhythmic Fingerprints\n\n  In this paper, we bridge the gap between visualization and musicology by\nfocusing on rhythm analysis tasks, which are tedious due to the complex visual\nencoding of the well-established Common Music Notation (CMN). Instead of\nreplacing the CMN, we augment sheet music with rhythmic fingerprints to\nmitigate the complexity originating from the simultaneous encoding of musical\nfeatures. The proposed visual design exploits music theory concepts such as the\nrhythm tree to facilitate the understanding of rhythmic information.\nJuxtaposing sheet music and the rhythmic fingerprints maintains the connection\nto the familiar representation. To investigate the usefulness of the rhythmic\nfingerprint design for identifying and comparing rhythmic patterns, we\nconducted a controlled user study with four experts and four novices. The\nresults show that the rhythmic fingerprints enable novice users to recognize\nrhythmic patterns that only experts can identify using non-augmented sheet\nmusic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.028,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Designing for Ambiguity: Visual Analytics in Avalanche Forecasting\n\n  Ambiguity, an information state where multiple interpretations are plausible,\nis a common challenge in visual analytics (VA) systems. We discuss lessons\nlearned from a case study designing VA tools for Canadian avalanche\nforecasters. Avalanche forecasting is a complex and collaborative risk-based\ndecision-making and analysis domain, demanding experience and knowledge-based\ninterpretation of human reported and uncertain data. Differences in reporting\npractices, organizational contexts, and the particularities of individual\nreports result in a variety of potential interpretations that have to be\nnegotiated as part of the forecaster's sensemaking processes. We describe our\npreliminary research using glyphs to support sensemaking under ambiguity.\nAmbiguity is not unique to public avalanche forecasting. There are many other\ndomains where the way data are measured and reported vary in ways not accounted\nexplicitly in the data and require analysts to negotiate multiple potential\nmeanings. We argue that ambiguity is under-served by visualization research and\nwould benefit from more explicit VA support.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13111,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Develop Health Monitoring and Management System to Track Health\n  Condition and Nutrient Balance for School Students\n\n  Health Monitoring and Management System (HMMS) is an emerging technology for\ndecades. Researchers are working on this field to track health conditions for\ndifferent users. Researchers emphasize tracking health conditions from an early\nstage to the human body. Therefore, different research works have been\nconducted to establish HMMS in schools. Researchers propose different\nframeworks and technologies for their HMMS to check student's health condition.\nIn this paper, we introduce a complete and scalable HMMS to track health\nconditions and nutrient balance for students from primary school. We define\nprocedures step by step to establish a robust HMMS where big data methodologies\ncan be used for further prediction for diseases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.16153,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Time-position characterization of conflicts: a case study of\n  collaborative editing\n\n  Collaborative editing (CE) became increasingly common, often compulsory in\nacademia and industry where people work in teams and are distributed across\nspace and time. We aim to study collabora-tive editing behavior in terms of\ncollaboration patterns users adopt and in terms of a characterisation of\nconflicts, i.e. edits from different users that occur close in time and\nposition in the document. The process of a CE can be split into several editing\n'sessions' which are performed by a single author ('single-authored session')\nor several authors ('co-authored session'). This fragmentation process requires\na pre-defined 'maximum time gap' between sessions which is not yet well defined\nin previous studies. In this study, we analysed CE logs of 108 collaboratively\nedited documents. We show how to establish a suitable 'maximum time gap' to\nsplit CE activities into sessions by evaluating the distribution of the time\ndistance between two adjacent sessions. We studied editing activities inside\neach 'co-author session' in order to define potential conflicts in terms of\ntime and position dimensions before they occur in the document. We also\nanalysed how many of these potential conflicts become real conflicts. Findings\nshow that potential conflicting cases are few. However, they are more likely to\nbecome real conflicts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.03676,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000300672,
      "text":"Evaluation of Motor Imagery-Based BCI methods in neurorehabilitation of\n  Parkinson's Disease patients\n\n  The study reports the performance of Parkinson's disease (PD) patients to\noperate Motor-Imagery based Brain-Computer Interface (MI-BCI) and compares\nthree selected pre-processing and classification approaches. The experiment was\nconducted on 7 PD patients who performed a total of 14 MI-BCI sessions\ntargeting lower extremities. EEG was recorded during the initial calibration\nphase of each session, and the specific BCI models were produced by using\nSpectrally weighted Common Spatial Patterns (SpecCSP), Source Power\nComodulation (SPoC) and Filter-Bank Common Spatial Patterns (FBCSP) methods.\nThe results showed that FBCSP outperformed SPoC in terms of accuracy, and both\nSPoC and SpecCSP in terms of the false-positive ratio. The study also\ndemonstrates that PD patients were capable of operating MI-BCI, although with\nlower accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.06694,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000124839,
      "text":"Easy, Reproducible and Quality-Controlled Data Collection with Crowdaq\n\n  High-quality and large-scale data are key to success for AI systems. However,\nlarge-scale data annotation efforts are often confronted with a set of common\nchallenges: (1) designing a user-friendly annotation interface; (2) training\nenough annotators efficiently; and (3) reproducibility. To address these\nproblems, we introduce Crowdaq, an open-source platform that standardizes the\ndata collection pipeline with customizable user-interface components, automated\nannotator qualification, and saved pipelines in a re-usable format. We show\nthat Crowdaq simplifies data annotation significantly on a diverse set of data\ncollection use cases and we hope it will be a convenient tool for the\ncommunity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.10967,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Safe Handover in Mixed-Initiative Control for Cyber-Physical Systems\n\n  For mixed-initiative control between cyber-physical systems (CPS) and its\nusers, it is still an open question how machines can safely hand over control\nto humans. In this work, we propose a concept to provide technological support\nthat uses formal methods from AI -- description logic (DL) and automated\nplanning -- to predict more reliably when a hand-over is necessary, and to\nincrease the advance notice for handovers by planning ahead of runtime. We\ncombine this with methods from human-computer interaction (HCI) and natural\nlanguage generation (NLG) to develop solutions for safe and smooth handovers\nand provide an example autonomous driving scenario. A study design is proposed\nwith the assessment of qualitative feedback, cognitive load and trust in\nautomation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07209,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000161264,
      "text":"HeartBees: Visualizing Crowd Affects\n\n  Affective sharing within groups strengthens coordination and empathy, leads\nto better health outcomes, and increases productivity and performance. Existing\ntools for affective sharing face one main challenge: creating a representation\nof collective emotional states that is relatable and universally accessible. To\novercome this challenge, we propose HeartBees, a bio-feedback system for\nvisualizing collective emotional states, which maps a multi-dimensional emotion\nmodel into a metaphorical visualization of flocks of birds. Grounded on\nAffective Computing literature and physiological sensing, we mapped\nphysiological indicators that could be obtained from wearable devices into a\nmulti-dimensional emotion model, which, in turn, our HeartBees can make use of.\nWe evaluated our nature-inspired interactive system with 353 online\nparticipants, whose responses showed good consensus in the way they\nsubjectively perceived the visualizations. Last, we discuss practical\napplications of HeartBees.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13901,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"An investigation of Modern Foreign Language (MFL) teachers and their\n  cognitions of Computer Assisted Language Learning (CALL) amid the COVID-19\n  health pandemic\n\n  A study was performed with 33 Modern Foreign Language (MFL) teachers to\nafford insight into how classroom practitioners interact with Computer Assisted\nLanguage Learning (CALL) in Second Language (L2) pedagogy. A questionnaire with\nCALL specific statements was completed by MFL teachers who were recruited via\nUK based Facebook groups. Significantly, participants acknowledged a gap in\npractice from the expectation of CALL in the MFL classroom. Overall,\nrespondents were shown to be interested and regular consumers of CALL who\nperceived its ease and importance in L2 teaching and learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.14069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"Let's Gamble: How a Poor Visualization Can Elicit Risky Behavior\n\n  Data visualizations are standard tools for assessing and communicating risks.\nHowever, it is not always clear which designs are optimal or how encoding\nchoices might influence risk perception and decision-making. In this paper, we\nreport the findings of a large-scale gambling game that immersed participants\nin an environment where their actions impacted their bonuses. Participants\nchose to either enter a lottery or receive guaranteed monetary gains based on\nfive common visualization designs. By measuring risk perception and observing\ndecision-making, we showed that icon arrays tended to elicit economically sound\nbehavior. We also found that people were more likely to gamble when presented\narea proportioned triangle and circle designs. Using our results, we model risk\nperception and discuss how our findings can improve visualization selection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.08457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Multi-Modal Data Collection for Measuring Health, Behavior, and Living\n  Environment of Large-Scale Participant Cohorts: Conceptual Framework and\n  Findings from Deployments\n\n  As mobile technologies become ever more sensor-rich, portable, and\nubiquitous, data captured by smart devices are lending rich insights into\nusers' daily lives with unprecedented comprehensiveness, unobtrusiveness, and\necological validity. A number of human-subject studies have been conducted in\nthe past decade to examine the use of mobile sensing to uncover individual\nbehavioral patterns and health outcomes. While understanding health and\nbehavior is the focus for most of these studies, we find that minimal attention\nhas been placed on measuring personal environments, especially together with\nother human-centric data modalities. Moreover, the participant cohort size in\nmost existing studies falls well below a few hundred, leaving questions open\nabout the reliability of findings on the relations between mobile sensing\nsignals and human outcomes. To address these limitations, we developed a home\nenvironment sensor kit for continuous indoor air quality tracking and deployed\nit in conjunction with established mobile sensing and experience sampling\ntechniques in a cohort study of up to 1584 student participants per data type\nfor 3 weeks at a major research university in the United States. In this paper,\nwe begin by proposing a conceptual framework that systematically organizes\nhuman-centric data modalities by their temporal coverage and spatial freedom.\nThen we report our study design and procedure, technologies and methods\ndeployed, descriptive statistics of the collected data, and results from our\nextensive exploratory analyses. Our novel data, conceptual development, and\nanalytical findings provide important guidance for data collection and\nhypothesis generation in future human-centric sensing studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.01316,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000109275,
      "text":"Accounts, Accountability and Agency for Safe and Ethical AI\n\n  We examine the problem of explainable AI (xAI) and explore what delivering\nxAI means in practice, particularly in contexts that involve formal or informal\nand ad-hoc collaboration where agency and accountability in decision-making are\nachieved and sustained interactionally. We use an example from an earlier study\nof collaborative decision-making in screening mammography and the difficulties\nusers faced when trying to interpret the behavior of an AI tool to illustrate\nthe challenges of delivering usable and effective xAI. We conclude by setting\nout a study programme for future research to help advance our understanding of\nxAI requirements for safe and ethical AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.03047,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000042717,
      "text":"Psychophysiological responses to takeover requests in conditionally\n  automated driving\n\n  In SAE Level 3 automated driving, taking over control from automation raises\nsignificant safety concerns because drivers out of the vehicle control loop\nhave difficulty negotiating takeover transitions. Existing studies on takeover\ntransitions have focused on drivers' behavioral responses to takeover requests\n(TORs). As a complement, this exploratory study aimed to examine drivers'\npsychophysiological responses to TORs as a result of varying\nnon-driving-related tasks (NDRTs), traffic density and TOR lead time. A total\nnumber of 102 drivers were recruited and each of them experienced 8 takeover\nevents in a high fidelity fixed-base driving simulator. Drivers' gaze\nbehaviors, heart rate (HR) activities, galvanic skin responses (GSRs), and\nfacial expressions were recorded and analyzed during two stages. First, during\nthe automated driving stage, we found that drivers had lower heart rate\nvariability, narrower horizontal gaze dispersion, and shorter eyes-on-road time\nwhen they had a high level of cognitive load relative to a low level of\ncognitive load. Second, during the takeover transition stage, 4s lead time led\nto inhibited blink numbers and larger maximum and mean GSR phasic activation\ncompared to 7s lead time, whilst heavy traffic density resulted in increased HR\nacceleration patterns than light traffic density. Our results showed that\npsychophysiological measures can indicate specific internal states of drivers,\nincluding their workload, emotions, attention, and situation awareness in a\ncontinuous, non-invasive and real-time manner. The findings provide additional\nsupport for the value of using psychophysiological measures in automated\ndriving and for future applications in driver monitoring systems and adaptive\nalert systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.09975,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Calliope: Automatic Visual Data Story Generation from a Spreadsheet\n\n  Visual data stories shown in the form of narrative visualizations such as a\nposter or a data video, are frequently used in data-oriented storytelling to\nfacilitate the understanding and memorization of the story content. Although\nuseful, technique barriers, such as data analysis, visualization, and\nscripting, make the generation of a visual data story difficult. Existing\nauthoring tools rely on users' skills and experiences, which are usually\ninefficient and still difficult. In this paper, we introduce a novel visual\ndata story generating system, Calliope, which creates visual data stories from\nan input spreadsheet through an automatic process and facilities the easy\nrevision of the generated story based on an online story editor. Particularly,\nCalliope incorporates a new logic-oriented Monte Carlo tree search algorithm\nthat explores the data space given by the input spreadsheet to progressively\ngenerate story pieces (i.e., data facts) and organize them in a logical order.\nThe importance of data facts is measured based on information theory, and each\ndata fact is visualized in a chart and captioned by an automatically generated\ndescription. We evaluate the proposed technique through three example stories,\ntwo controlled experiments, and a series of interviews with 10 domain experts.\nOur evaluation shows that Calliope is beneficial to efficient visual data story\ngeneration.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.01651,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Interface Design for HCI Classroom: From Learners' Perspective\n\n  Having a good Human-Computer Interaction (HCI) design is challenging.\nPrevious works have contributed significantly to fostering HCI, including\ndesign principle with report study from the instructor view. The questions of\nhow and to what extent students perceive the design principles are still left\nopen. To answer this question, this paper conducts a study of HCI adoption in\nthe classroom. The studio-based learning method was adapted to teach 83\ngraduate and undergraduate students in 16 weeks long with four activities. A\nstandalone presentation tool for instant online peer feedback during the\npresentation session was developed to help students justify and critique\nother's work. Our tool provides a sandbox, which supports multiple application\ntypes, including Web-applications, Object Detection, Web-based Virtual Reality\n(VR), and Augmented Reality (AR). After presenting one assignment and two\nprojects, our results showed that students acquired a better understanding of\nthe Golden Rules principle over time, which was demonstrated by the development\nof visual interface design. The Wordcloud reveals the primary focus was on the\nuser interface and shed some light on students' interest in user experience.\nThe inter-rater score indicates the agreement among students that they have the\nsame level of understanding of the principles. The results show a high level of\nguideline compliance with HCI principles, in which we witnessed variations in\nvisual cognitive styles. Regardless of diversity in visual preference, the\nstudents presented high consistency and a similar perspective on adopting HCI\ndesign principles. The results also elicited suggestions into the development\nof the HCI curriculum in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.081,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"When Virtual Therapy and Art Meet: A Case Study of Creative Drawing Game\n  in Virtual Environments\n\n  There have been a resurge lately on virtual therapy and other virtual- and\ntele-medicine services due to the new normal of practicing 'shelter at home'.\nIn this paper, we propose a creative drawing game for virtual therapy and\ninvestigate user's comfort and movement freedom in a pilot study. In a\nmixed-design study, healthy participants (N=16, 8 females) completed one of the\neasy or hard trajectories of the virtual therapy game in standing and seated\narrangements using a virtual-reality headset. The results from participants'\nmovement accuracy, task completion time, and usability questionnaires indicate\nthat participants had significant performance differences on two levels of the\ngame based on its difficulty (between-subjects factor), but no difference in\nseated and standing configurations (within-subjects factor). Also, the hard\nmode was more favorable among participants. This work offers implications on\nvirtual reality and 3D-interactive systems, with specific contributions to\nvirtual therapy, and serious games for healthcare applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.0316,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Vision Skills Needed to Answer Visual Questions\n\n  The task of answering questions about images has garnered attention as a\npractical service for assisting populations with visual impairments as well as\na visual Turing test for the artificial intelligence community. Our first aim\nis to identify the common vision skills needed for both scenarios. To do so, we\nanalyze the need for four vision skills---object recognition, text recognition,\ncolor recognition, and counting---on over 27,000 visual questions from two\ndatasets representing both scenarios. We next quantify the difficulty of these\nskills for both humans and computers on both datasets. Finally, we propose a\nnovel task of predicting what vision skills are needed to answer a question\nabout an image. Our results reveal (mis)matches between aims of real users of\nsuch services and the focus of the AI community. We conclude with a discussion\nabout future directions for addressing the visual question answering task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.05988,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Evaluating Mixed and Augmented Reality: A Systematic Literature Review\n  (2009-2019)\n\n  We present a systematic review of 458 papers that report on evaluations in\nmixed and augmented reality (MR\/AR) published in ISMAR, CHI, IEEE VR, and UIST\nover a span of 11 years (2009-2019). Our goal is to provide guidance for future\nevaluations of MR\/AR approaches. To this end, we characterize publications by\npaper type (e.g., technique, design study), research topic (e.g., tracking,\nrendering), evaluation scenario (e.g., algorithm performance, user\nperformance), cognitive aspects (e.g., perception, emotion), and the context in\nwhich evaluations were conducted (e.g., lab vs. in-the-wild). We found a strong\ncoupling of types, topics, and scenarios. We observe two groups: (a)\ntechnology-centric performance evaluations of algorithms that focus on\nimproving tracking, displays, reconstruction, rendering, and calibration, and\n(b) human-centric studies that analyze implications of applications and design,\nhuman factors on perception, usability, decision making, emotion, and\nattention. Amongst the 458 papers, we identified 248 user studies that involved\n5,761 participants in total, of whom only 1,619 were identified as female. We\nidentified 43 data collection methods used to analyze 10 cognitive aspects. We\nfound nine objective methods, and eight methods that support qualitative\nanalysis. A majority (216\/248) of user studies are conducted in a laboratory\nsetting. Often (138\/248), such studies involve participants in a static way.\nHowever, we also found a fair number (30\/248) of in-the-wild studies that\ninvolve participants in a mobile fashion. We consider this paper to be relevant\nto academia and industry alike in presenting the state-of-the-art and guiding\nthe steps to designing, conducting, and analyzing results of evaluations in\nMR\/AR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.03781,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000105633,
      "text":"VirusBoxing: A HIIT-based VR boxing game\n\n  Physical activity or exercise can improve people's health and reduce their\nrisk of developing several diseases; most importantly, regular activity can\nimprove the quality of life. However, lack of time is one of the major barriers\nfor people doing exercise. High-intensity interval training (HIIT) can reduce\nthe time required for a healthy exercise regime but also bring similar benefits\nof regular exercise. We present a boxing-based VR exergame called VirusBoxing\nto promote physical activity for players. VirusBoxing provides players with a\nplatform for HIIT and empowers them with additional abilities to jab a distant\nobject without the need to aim at it precisely. In this paper, we discuss how\nwe adapted the HIIT protocol and gameplay features to empower players in a VR\nexergame to give players an efficient, effective, and enjoyable exercise\nexperience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11761,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"A Comparative Analysis of Industry Human-AI Interaction Guidelines\n\n  With the recent release of AI interaction guidelines from Apple, Google, and\nMicrosoft, there is clearly interest in understanding the best practices in\nhuman-AI interaction. However, industry standards are not determined by a\nsingle company, but rather by the synthesis of knowledge from the whole\ncommunity. We have surveyed all of the design guidelines from each of these\nmajor companies and developed a single, unified structure of guidelines, giving\ndevelopers a centralized reference. We have then used this framework to compare\neach of the surveyed companies to find differences in areas of emphasis.\nFinally, we encourage people to contribute additional guidelines from other\ncompanies, academia, or individuals, to provide an open and extensible\nreference of AI design guidelines at\nhttps:\/\/ai-open-guidelines.readthedocs.io\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.02561,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Comparing Pedestrian Navigation Methods in Virtual Reality and Real Life\n\n  Mobile navigation apps are among the most used mobile applications and are\noften used as a baseline to evaluate new mobile navigation technologies in\nfield studies. As field studies often introduce external factors that are hard\nto control for, we investigate how pedestrian navigation methods can be\nevaluated in virtual reality (VR). We present a study comparing navigation\nmethods in real life (RL) and VR to evaluate if VR environments are a viable\nalternative to RL environments when it comes to testing these. In a series of\nstudies, participants navigated a real and a virtual environment using a paper\nmap and a navigation app on a smartphone. We measured the differences in\nnavigation performance, task load and spatial knowledge acquisition between RL\nand VR. From these we formulate guidelines for the improvement of pedestrian\nnavigation systems in VR like improved legibility for small screen devices. We\nfurthermore discuss appropriate low-cost and low-space VR-locomotion techniques\nand discuss more controllable locomotion techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.1257,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Eye Tracking Data Collection Protocol for VR for Remotely Located\n  Subjects using Blockchain and Smart Contracts\n\n  Eye tracking data collection in the virtual reality context is typically\ncarried out in laboratory settings, which usually limits the number of\nparticipants or consumes at least several months of research time. In addition,\nunder laboratory settings, subjects may not behave naturally due to being\nrecorded in an uncomfortable environment. In this work, we propose a\nproof-of-concept eye tracking data collection protocol and its implementation\nto collect eye tracking data from remotely located subjects, particularly for\nvirtual reality using Ethereum blockchain and smart contracts. With the\nproposed protocol, data collectors can collect high quality eye tracking data\nfrom a large number of human subjects with heterogeneous socio-demographic\ncharacteristics. The quality and the amount of data can be helpful for various\ntasks in data-driven human-computer interaction and artificial intelligence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11398,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Designing for Interpersonal Museum Experiences\n\n  What does the age of participation look like from the perspective of a museum\nvisitor? Arguably, the concept of participative experiences is already so\ndeeply ingrained in our culture that we may not even think about it as\nparticipation. Museum visitors engage in a number of activities, of which\nobserving the exhibits is only one part. Since most visitors come to the museum\ntogether with someone else, they spend time and attention on the people they\ncame with, and often the needs of the group are given priority over individual\npreferences. How can museums tap into these activities - and make themselves\nrelevant to visitors? In this chapter we will try to approach this\nconstructively, as a design opportunity. Could it be productive for the museum\nto consider itself not only as a disseminator of knowledge, but also as the\nfacilitator of participative activities between visitors?\n  In what follows, we will outline a range of practical design projects that\nserve as examples of this approach. These projects were part of the European\nUnion funded Horizon2020 project GIFT, a cross-disciplinary collaboration\nbetween researchers, artists, designers and many international museums and\nheritage organisations, exploring the concept of interpersonal museum\nexperiences (see https:\/\/gifting.digital\/). What the projects have in common is\nthat they build on visitors co-creating and sharing their own narratives in the\nmuseum context. We suggest that these projects demonstrate a spectrum of\npossibilities: From experiences that take place almost without any museum\ninvolvement, to those that give museums a role in curating these narratives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"App Limits Bar: A Progress of App Limits for Overcoming Smartphone\n  Overuse\n\n  Smartphone overuse has many negative effects on human beings. The function\nApp Limits in our phones, which belongs to behavior reinforcement strategies\nfor overcoming problematic smartphone overuse, constrains users by shutting\nthem completely out of an app after a certain period of time. While it has the\neffectiveness to some extent, lacking procedural detection of problematic\nbehavior always brings about anxiety and depression for users. We proposed App\nLimits Bar (ALB) to progress the traditional App Limits, aiming to mitigate the\nnegative feelings about using App Limits and enhance users' abilities to manage\ntheir time on addictive apps. Meanwhile, we took a three-phase user study to\nanswer whether ALB helps users be more content with using the app limits\nfunction and whether ALB performs better than traditional App Limits in terms\nof reducing usage time and open times on addictive apps. Future work will study\nthe best shape of the edge screen for visualization when the front screen faces\nus.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.05863,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Wearable Sensors for Individual Grip Force Profiling\n\n  Biosensors and wearable sensor systems with transmitting capabilities are\ncurrently developed and used for the monitoring of health data, exercise\nactivities, and other performance data. Unlike conventional approaches, these\ndevices enable convenient, continuous, and\/or unobtrusive monitoring of user\nbehavioral signals in real time. Examples include signals relative to body\nmotion, body temperature, blood flow parameters and a variety of biological or\nbiochemical markers and, as will be shown in this chapter here, individual grip\nforce data that directly translate into spatiotemporal grip force profiles for\ndifferent locations on the fingers and palm of the hand. Wearable sensor\nsystems combine innovation in sensor design, electronics, data transmission,\npower management, and signal processing for statistical analysis, as will be\nfurther shown herein. The first section of this chapter will provide an\noverview of the current state of the art in grip force profiling to highlight\nimportant functional aspects to be considered. In the next section, the\ncontribution of wearable sensor technology in the form of sensor glove systems\nfor the real-time monitoring of surgical task skill evolution in novices\ntraining in a simulator task will be described on the basis of recent examples.\nIn the discussion, advantages and limitations will be weighed against each\nother.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.0751,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000117222,
      "text":"Model-Driven Synthesis for Programming Tutors\n\n  When giving automated feedback to a student working on a beginner's exercise,\nmany programming tutors run into a completeness problem. On the one hand, we\nwant a student to experiment freely. On the other hand, we want a student to\nwrite her program in such a way that we can provide constructive feedback. We\npropose to investigate how we can overcome this problem by using program\nsynthesis, which we use to generate correct solutions that closely match a\nstudent program, and give feedback based on the results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13079,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"A Visual Analytics Approach for Hardware System Monitoring with\n  Streaming Functional Data Analysis\n\n  Many real-world applications involve analyzing time-dependent phenomena,\nwhich are intrinsically functional, consisting of curves varying over a\ncontinuum (e.g., time). When analyzing continuous data, functional data\nanalysis (FDA) provides substantial benefits, such as the ability to study the\nderivatives and to restrict the ordering of data. However, continuous data\ninherently has infinite dimensions, and for a long time series, FDA methods\noften suffer from high computational costs. The analysis problem becomes even\nmore challenging when updating the FDA results for continuously arriving data.\nIn this paper, we present a visual analytics approach for monitoring and\nreviewing time series data streamed from a hardware system with a focus on\nidentifying outliers by using FDA. To perform FDA while addressing the\ncomputational problem, we introduce new incremental and progressive algorithms\nthat promptly generate the magnitude-shape (MS) plot, which conveys both the\nfunctional magnitude and shape outlyingness of time series data. In addition,\nby using an MS plot in conjunction with an FDA version of principal component\nanalysis, we enhance the analyst's ability to investigate the\nvisually-identified outliers. We illustrate the effectiveness of our approach\nwith two use scenarios using real-world datasets. The resulting tool is\nevaluated by industry experts using real-world streaming datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13779,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000050664,
      "text":"An Integrated Approach Towards the Construction of an HCI Methodological\n  Framework\n\n  We present a methodological framework aiming at the support of HCI\npractitioners and researchers in selecting and applying the most appropriate\ncombination of HCI methods for particular problems. We highlight the need for a\nclear and effective overview of methods and provide further discussion on\npossible extensions that can support recent trends and needs, such as the focus\non specific application domains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09696,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"Towards Emotion-Aware User Simulator for Task-Oriented Dialogue\n\n  The performance of a task-completion dialogue agent usually affects the user\nexperience: when the conversation system yields an unreasonable response, users\nmay feel dissatisfied. Besides, early termination often occurs in disappointing\nconversations. However, existing off-the-shelf user simulators generally assume\nan ideal and cooperative user, which is somewhat different from a real user,\nand inevitably lead to a sub-optimal dialogue policy. In this paper, we propose\nan emotion-aware user simulation framework for task-oriented dialogue, which is\nbased on the OCC emotion model to update user emotions and drive user actions,\nto generate simulated behaviors that more similar to real users. We present a\nlinear implementation (The source code will be released soon.) that is easy to\nunderstand and extend, and evaluate it on two domain-specific datasets. The\nexperimental results show that the emotional simulation results of our proposed\nframework conform to common sense and have good versatility for different\ndomains. Meanwhile, our framework provides us with another perspective to\nunderstand the improvement process of the dialogue policy model based on\nreinforcement learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.0913,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000429153,
      "text":"Visual Drift Detection for Sequence Data Analysis of Business Processes\n\n  Event sequence data is increasingly available in various application domains,\nsuch as business process management, software engineering, or medical pathways.\nProcesses in these domains are typically represented as process diagrams or\nflow charts. So far, various techniques have been developed for automatically\ngenerating such diagrams from event sequence data. An open challenge is the\nvisual analysis of drift phenomena when processes change over time. In this\npaper, we address this research gap. Our contribution is a system for\nfine-granular process drift detection and corresponding visualizations for\nevent logs of executed business processes. We evaluated our system both on\nsynthetic and real-world data. On synthetic logs, we achieved an average\nF-score of 0.96 and outperformed all the state-of-the-art methods. On\nreal-world logs, we identified all types of process drifts in a comprehensive\nmanner. Finally, we conducted a user study highlighting that our visualizations\nare easy to use and useful as perceived by process mining experts. In this way,\nour work contributes to research on process mining, event sequence analysis,\nand visualization of temporal data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01644,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Understanding Usability and User Acceptance of Usage-Based Insurance\n  from Users' View\n\n  Intelligent Transportation Systems (ITS) cover a variety of services related\nto topics such as traffic control and safe driving, among others. In the\ncontext of car insurance, a recent application for ITS is known as Usage-Based\nInsurance (UBI). UBI refers to car insurance policies that enable insurance\ncompanies to collect individual driving data using a telematics device.\nCollected data is analysed and used to offer individual discounts based on\ndriving behaviour and to provide feedback on driving performance. Although\nthere are plenty of advertising materials about the benefits of UBI, the user\nacceptance and the usability of UBI systems have not received research\nattention so far. To this end, we conduct two user studies: semi-structured\ninterviews with UBI users and a qualitative analysis of 186 customer inquiries\nfrom a web forum of a German insurance company. We find that under certain\ncircumstances, UBI provokes dangerous driving behaviour. These situations could\nbe mitigated by making UBI transparent and the feedback customisable by\ndrivers. Moreover, the country driving conditions, the policy conditions, and\nthe perceived driving style influence UBI acceptance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13347,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000106295,
      "text":"Online asynchronous detection of error-related potentials in\n  participants with a spinal cord injury using a generic classifier\n\n  A BCI user awareness of an error is associated with a cortical signature\nnamed error-related potential (ErrP). The incorporation of ErrPs' detection in\nBCIs can improve BCIs' performance. This work is three-folded. First, we\ninvestigate if an ErrP classifier is transferable from able-bodied participants\nto participants with spinal cord injury (SCI). Second, we test this generic\nErrP classifier with SCI and control participants, in an online experiment\nwithout offline calibration. Third, we investigate the morphology of ErrPs in\nboth groups of participants. We used previously recorded\nelectroencephalographic (EEG) data from able-bodied participants to train an\nErrP classifier. We tested the classifier asynchronously, in an online\nexperiment with 16 new participants: 8 participants with SCI and 8 able-bodied\ncontrol participants. The experiment had no offline calibration and\nparticipants received feedback regarding the ErrPs' detection from its start.\nThe generic classifier was not trained with the user's brain signals. Still,\nits performance was optimized during the online experiment with the use of\npersonalized decision thresholds. Participants with SCI presented a\nnon-homogenous ErrP morphology, and four of them did not present clear ErrP\nsignals. The generic classifier performed above chance level in participants\nwith clear ErrP signals, independently of the SCI (11 out of 16 participants).\nThree out of the five participants that obtained chance level results with the\ngeneric classifier would have not benefited from the use of a personalized\nclassifier. This work shows the feasibility of transferring an ErrP classifier\nfrom able-bodied participants to participants with SCI, for asynchronous\ndetection of ErrPs in an online experiment without offline calibration, which\nprovided immediate feedback to the users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.0257,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Interactive Tools for Reproducible Science -- Understanding, Supporting,\n  and Motivating Reproducible Science Practices\n\n  Reproducibility should be a cornerstone of science as it enables validation\nand reuse. In recent years, the scientific community and the general public\nbecame increasingly aware of the reproducibility crisis, i.e. the wide-spread\ninability of researchers to reproduce published work, including their own.\nScientific research is increasingly focused on the creation, observation,\nprocessing, and analysis of large data volumes. On the one hand, this\ntransition towards computational and data-intensive science poses new\nchallenges for research reproducibility and reuse. On the other hand, increased\navailability and advances in computation and web technologies offer new\nopportunities to address the reproducibility crisis. This thesis reports on\nuser-centered design research conducted at CERN, a key laboratory in\ndata-intensive particle physics.\n  In this thesis, we build a wider understanding of researchers' interactions\nwith tools that support research documentation, preservation, and sharing. From\na Human-Computer Interaction (HCI) perspective the following aspects are\nfundamental: (1) Characterize and map requirements and practices around\nresearch preservation and reuse. (2) Understand the wider role and impact of\nresearch data management (RDM) tools in scientific workflows. (3) Design tools\nand interactions that promote, motivate, and acknowledge reproducible research\npractices. Research reported in this thesis represents the first systematic\napplication of HCI methods in the study and design of interactive tools for\nreproducible science. We advocate the unique role of HCI in supporting,\nmotivating, and transforming reproducible research practices through the design\nof tools that enable effective RDM. This thesis paves new ways for interaction\nwith RDM tools that support and motivate reproducible science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01931,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Sanguine: Visual Analysis for Patient Blood Management\n\n  Blood transfusion is a frequently performed medical procedure in surgical and\nnonsurgical contexts. Although it is frequently necessary or even life-saving,\nit has been identified as one of the most overused procedures in hospitals.\nUnnecessary transfusions not only waste resources but can also be detrimental\nto patient outcomes. Patient blood management (PBM) is the clinical practice of\noptimizing transfusions and associated outcomes. In this paper, we introduce\nSanguine, a visual analysis tool for transfusion data and related patient\nmedical records. Sanguine was designed with two user groups in mind: PBM\nexperts and clinicians who conduct transfusions. PBM experts use Sanguine to\nexplore and analyze transfusion practices and its associated medical outcomes.\nThey can compare individual surgeons, or compare outcomes or time periods, such\nas before and after an intervention regarding transfusion practices. PBM\nexperts then curate and annotate views for communication with clinicians, with\nthe goal of improving their transfusion practices. Such a review session could\nbe in person or through a shared link. We validate the utility and\neffectiveness of Sanguine through case studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02569,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Assessing the usability of the mobile application game Call of Duty\n\n  The technological advances in the smartphone application market has expanded\nat great lengths in recent years (Kaya, Ozturk & Gumussoy 2019). One of the\nmain smartphone applications that often sees record breaking downloads are\ngaming apps. The gaming application that recently received the most downloads\nof all time, 100 million in one week, was the mobile game Call of Duty (CoD).\nWith the game generating revenues of over $8 million across app stores, it is\nimportant that the users are satisfied with the game and its experience\n(Cuthbertson, 2019). Unlike other applications, gamers are drawn to gaming\napplications for their experience more than their functionality. Ensuring that\nthe game is performing with efficiency and of a high level of satisfaction, is\nkey in enhancing the users experience and remaining popular in such a\ncompetitive market (Barnett, Harvey and Gatzidis 2018). There are significant\nadvances of mobile applications such as portability and accessibility but the\nchange in architecture has meant that some aspects of design and usability have\nhad to be constrained. Therefore, mobile games need to be more user focused\n(Nayebi, Desharnais and Abran 2012).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09896,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"TBSSvis: Visual Analytics for Temporal Blind Source Separation\n\n  Temporal Blind Source Separation (TBSS) is used to obtain the true underlying\nprocesses from noisy temporal multivariate data, such as electrocardiograms.\nTBSS has similarities to Principal Component Analysis (PCA) as it separates the\ninput data into univariate components and is applicable to suitable datasets\nfrom various domains, such as medicine, finance, or civil engineering. Despite\nTBSS's broad applicability, the involved tasks are not well supported in\ncurrent tools, which offer only text-based interactions and single static\nimages. Analysts are limited in analyzing and comparing obtained results, which\nconsist of diverse data such as matrices and sets of time series. Additionally,\nparameter settings have a big impact on separation performance, but as a\nconsequence of improper tooling, analysts currently do not consider the whole\nparameter space. We propose to solve these problems by applying visual\nanalytics (VA) principles. Our primary contribution is a design study for TBSS,\nwhich so far has not been explored by the visualization community. We developed\na task abstraction and visualization design in a user-centered design process.\nTask-specific assembling of well-established visualization techniques and\nalgorithms to gain insights in the TBSS processes is our secondary\ncontribution. We present TBSSvis, an interactive web-based VA prototype, which\nwe evaluated extensively in two interviews with five TBSS experts. Feedback and\nobservations from these interviews show that TBSSvis supports the actual\nworkflow and combination of interactive visualizations that facilitate the\ntasks involved in analyzing TBSS results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09988,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000172191,
      "text":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics\n\n  Mixed-initiative visual analytics systems incorporate well-established design\nprinciples that improve users' abilities to solve problems. As these systems\nconsider whether to take initiative towards achieving user goals, many current\nsystems address the potential for cognitive bias in human initiatives\nstatically, relying on fixed initiatives they can take instead of identifying,\ncommunicating and addressing the bias as it occurs. We argue that\nmixed-initiative design principles can and should incorporate cognitive bias\nmitigation strategies directly through development of mitigation techniques\nembedded in the system to address cognitive biases in situ. We identify domain\nexperts in machine learning adopting visual analytics techniques and systems\nthat incorporate existing mixed-initiative principles and examine their\npotential to support bias mitigation strategies. This examination considers the\nunique perspective these experts bring to visual analytics and is situated in\nexisting user-centered systems that make exemplary use of design principles\ninformed by cognitive theory. We then suggest informed opportunities for domain\nexperts to take initiative toward addressing cognitive biases in light of their\nexisting contributions to the field. Finally, we contribute open questions and\nresearch directions for designers seeking to adopt visual analytics techniques\nthat incorporate bias-aware initiatives in future systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11317,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000822809,
      "text":"Corona-Warn-App: Erste Ergebnisse einer Onlineumfrage zur\n  (Nicht-)Nutzung und Gebrauch\n\n  In this study, the German \"Corona-Warn-App\" of the German Federal Government\nand the Robert-Koch-Institute is examined by means of a non-representative\nonline survey with 1482 participants for reasons of use and non-use. The study\nprovides insights into user behavior with the app during the Corona pandemic,\nhighlights the topic of data protection and how the app is used in general. Our\nresults show that the app is often not used due to privacy concerns, but that\nthere are also technical problems and doubts about its usefulness. In addition,\nthe app is mainly used due to altruistic reasons and is often opened to view\nthe own risk assessment and to ensure its functionality. To better understand\nthe results, we compare our results with a sample of infas 360 with 10553\nparticipants. It is shown that the results of this study can be compared to a\nlarger population. Finally, the results are discussed and recommendations for\naction are derived.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.06456,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Combining visual contrast information with sound can produce faster\n  decisions\n\n  Pierons and Chocholles seminal psychophysical work predicts that human\nresponse time to information relative to visual contrast and sound frequency\ndecreases when contrast intensity or sound frequency increases. The goal of\nthis study is to bring to the fore the ability of individuals to use visual\ncontrast intensity and sound frequency in combination for faster perceptual\ndecisions of relative depth in planar object configurations on the basis of\nphysical variations in luminance contrast. Computer controlled images with two\nabstract patterns of varying contrast intensity, one on the left and one on the\nright, preceded or not by a pure tone of varying frequency, were shown to\nhealthy young humans in controlled experimental sequences. Their task was to\ndecide as quickly as possible which of two patterns, the left or the right one,\nin a given image appeared to stand out as if it were nearer in terms of\napparent or subjective visual depth. The results show that the combinations of\nvarying relative visual contrast with sounds of varying frequency exploited\nhere produced an additive effect on choice response times in terms of\nfacilitation, where a stronger visual contrast combined with a higher sound\nfrequency produced shorter forced choice response times. This new effect is\npredicted by crossmodal audiovisual probability summation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.07926,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Student and Teacher Meet in a Shared Virtual Reality: A one-on-one\n  Tutoring System for Anatomy Education\n\n  We introduce a Virtual Reality (VR) one-on-one tutoring system to support\nanatomy education. A student uses a fully immersive VR headset to explore the\nanatomy of the base of the human skull. A teacher guides the student by using\nthe semi-immersive zSpace. Both systems are connected via network and each\naction is synchronized between both systems.\n  The teacher is provided with various features to direct the student through\nthe immersive learning experience. She can influence the student's navigation\nor provide annotations on the fly and, hereby, improve the students learning\nexperience. The system is implemented using the \\textit{Unity} game engine. A\nqualitative user study demonstrates that the one-on-one tutoring approach is\nfeasible and sets a solid base for future research in the area of shared\nvirtual environments for anatomy education.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11375,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000054638,
      "text":"The Diary of Niels: Affective engagement through tangible interaction\n  with museum artifacts\n\n  This paper presents a research through design exploration using tangible\ninteractions in order to seamlessly integrate technology in a historical house\nmuseum. The study addresses a longstanding concern in museum exhibition design\nthat interactive technologies may distract from the artifacts on display.\nThrough an iterative design process including user studies, a co-creation\nworkshop with museum staff and several prototypes, we developed an interactive\ninstallation called The Diary of Niels that combines physical objects, RFID\nsensors and an elaborate fiction in order to facilitate increased visitor\nengagement. Insights from the research process and user tests indicate that the\nintegration of technology and artifacts is meaningful and engaging for users,\nand helps introduce museum visitors to the historic theme of the exhibition and\nthe meaning of the artifacts. The study also points to continued challenges in\nintegrating such hybrid experiences fully with the rest of the exhibition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.0357,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Look Before You Leap: Trusted User Interfaces for the Immersive Web\n\n  Part of what makes the web successful is that anyone can publish content and\nbrowsers maintain certain safety guarantees. For example, it's safe to travel\nbetween links and make other trust decisions on the web because users can\nalways identify the location they are at. If we want virtual and augmented\nreality to be successful, we need that same safety. On the traditional,\ntwo-dimensional (2D) web, this user interface (UI) is provided by the browser\nbars and borders (also known as the chrome). However, the immersive,\nthree-dimensional (3D) web has no concept of a browser chrome, preventing\nroutine user inspection of URLs. In this paper, we discuss the unique\nchallenges that fully immersive head-worn computing devices provide to this\nmodel, evaluate three different strategies for trusted immersive UI, and make\nspecific recommendations to increase user safety and reduce the risks of\nspoofing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03598,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000098017,
      "text":"Self-supervised Deep Learning for Reading Activity Classification\n\n  Reading analysis can give important information about a user's confidence and\nhabits and can be used to construct feedback to improve a user's reading\nbehavior. A lack of labeled data inhibits the effective application of\nfully-supervised Deep Learning (DL) for automatic reading analysis. In this\npaper, we propose a self-supervised DL method for reading analysis and evaluate\nit on two classification tasks. We first evaluate the proposed self-supervised\nDL method on a four-class classification task on reading detection using\nelectrooculography (EOG) glasses datasets, followed by an evaluation of a\ntwo-class classification task of confidence estimation on answers of\nmultiple-choice questions (MCQs) using eye-tracking datasets. Fully-supervised\nDL and support vector machines (SVMs) are used to compare the performance of\nthe proposed self-supervised DL method. The results show that the proposed\nself-supervised DL method is superior to the fully-supervised DL and SVM for\nboth tasks, especially when training data is scarce. This result indicates that\nthe proposed self-supervised DL method is the superior choice for reading\nanalysis tasks. The results of this study are important for informing the\ndesign and implementation of automatic reading analysis platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000079142,
      "text":"We Dare You: A Lifecycle Study of a Substitutional Reality Installation\n  in a Museum Space\n\n  In this article, we present a lifecycle study of We Dare You, a\nSubstitutional Reality (SR) installation that combines visual and tactile\nstimuli. The installation is set up in a center for architecture, and invites\nvisitors to explore its facade while playing with vertigo, in a visual Virtual\nReality (VR) environment that replicates the surrounding physical space of the\ninstallation. Drawing on an ethnographic approach, including observations and\ninterviews, we researched the exhibit from its opening, through the initial\nmonths plagued by technical problems, its subsequent success as a social and\nplayful installation, on to its closure, due to COVID-19, and its subsequent\nreopening. Our findings explore the challenges caused by both the hybrid nature\nof the installation, as well as the visitor' playful use of the installation\nwhich made the experience social and performative - but also caused some\nproblems. We also discuss the problems We Dare You faced in light of hygiene\ndemands due to COVID-19. The analysis contrasts the design processes and\nexpectations of stakeholders with the audience's playful appropriation, which\nled the stakeholders to see the installation as both a success and a failure.\nEvaluating the design and redesign through use on behalf of visitors, we argue\nthat an approach that further opens up the post-production experience to a\nprocess of continuous redesign based on the user input - what has been termed\n\"design-after-design\" - could facilitate the design of similar experiences in\nthe museum and heritage sector, supporting a participatory agenda in the design\nprocess, and helping to resolve the tension between stakeholders' expectations\nand visitors' playful appropriations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.04142,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Technology-driven Alteration of Nonverbal Cues and its Effects on\n  Negotiation\n\n  A person's appearance, identity, and other nonverbal cues can substantially\ninfluence how one is perceived by a negotiation counterpart, potentially\nimpacting the outcome of the negotiation. With recent advances in technology,\nit is now possible to alter such cues through real-time video communication. In\nmany cases, a person's physical presence can explicitly be replaced by 2D\/3D\nrepresentations in live interactive media. In other cases, technologies such as\ndeepfake can subtly and implicitly alter many nonverbal cues -- including a\nperson's appearance and identity -- in real-time. In this article, we look at\nsome state-of-the-art technological advances that can enable such explicit and\nimplicit alteration of nonverbal cues. We also discuss the implications of such\ntechnology for the negotiation landscape and highlight ethical considerations\nthat warrant deep, ongoing attention from stakeholders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.00922,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Cross-Modal Terrains: Navigating Sonic Space through Haptic Feedback\n\n  This paper explores the idea of using virtual textural terrains as a means of\ngenerating haptic profiles for force-feedback controllers. This approach breaks\nfrom the para-digm established within audio-haptic research over the last few\ndecades where physical models within virtual environments are designed to\ntransduce gesture into sonic output. We outline a method for generating\nmultimodal terrains using basis functions, which are rendered into\nmonochromatic visual representations for inspection. This visual terrain is\ntraversed using a haptic controller, the NovInt Falcon, which in turn receives\nforce information based on the grayscale value of its location in this virtual\nspace. As the image is traversed by a performer the levels of resistance vary,\nand the image is realized as a physical terrain. We discuss the potential of\nthis approach to afford engaging musical experiences for both the performer and\nthe audience as iterated through numerous performances.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02927,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Using voice note-taking to promote learners' conceptual understanding\n\n  Though recent technological advances have enabled note-taking through\ndifferent modalities (e.g., keyboard, digital ink, voice), there is still a\nlack of understanding of the effect of the modality choice on learning. In this\npaper, we compared two note-taking input modalities -- keyboard and voice -- to\nstudy their effects on participants' learning. We conducted a study with 60\nparticipants in which they were asked to take notes using voice or keyboard on\ntwo independent digital text passages while also making a judgment about their\nperformance on an upcoming test. We built mixed-effects models to examine the\neffect of the note-taking modality on learners' text comprehension, the content\nof notes and their meta-comprehension judgement. Our findings suggest that\ntaking notes using voice leads to a higher conceptual understanding of the text\nwhen compared to typing the notes. We also found that using voice also triggers\ngenerative processes that result in learners taking more elaborate and\ncomprehensive notes. The findings of the study imply that note-taking tools\ndesigned for digital learning environments could incorporate voice as an input\nmodality to promote effective note-taking and conceptual understanding of the\ntext.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.05637,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000084109,
      "text":"Simplify Node-RED For End User Development in SeismoCloud\n\n  Networks of IoT devices often require configuration and definition of\nbehavior by the final user. Node-RED is a flow-based programming platform\ncommonly used for End User Development, but it requires networking and\nprotocols skills in order to be efficiently used. We add a level of abstraction\nto Node-RED nodes in order to allow non-skilled users to configure and control\nnetworks of IoT devices and online services. We applied such abstractions to\nthe SeismoCloud application for earthquake monitoring.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14201,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"StudyU: a platform for designing and conducting innovative digital\n  N-of-1 trials\n\n  N-of-1 trials are the gold standard study design to evaluate individual\ntreatment effects and derive personalized treatment strategies. Digital tools\nhave the potential to initiate a new era of N-of-1 trials in terms of scale and\nscope, but fully-functional platforms are not yet available. Here, we present\nthe open source StudyU platform which includes the StudyU designer and StudyU\napp. With the StudyU designer, scientists are given a collaborative web\napplication to digitally specify, publish, and conduct N-of-1 trials. The\nStudyU app is a smartphone application with innovative user-centric elements\nfor participants to partake in the published trials and assess the effects of\ndifferent interventions on their health. Thereby, the StudyU platform allows\nclinicians and researchers worldwide to easily design and conduct digital\nN-of-1 trials in a safe manner. We envision that StudyU can change the\nlandscape of personalized treatments both for patients and healthy individuals,\ndemocratize and personalize evidence generation for self-optimization and\nmedicine, and can be integrated in clinical practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.11976,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000056293,
      "text":"A Maturity Assessment Framework for Conversational AI Development\n  Platforms\n\n  Conversational Artificial Intelligence (AI) systems have recently\nsky-rocketed in popularity and are now used in many applications, from car\nassistants to customer support. The development of conversational AI systems is\nsupported by a large variety of software platforms, all with similar goals, but\ndifferent focus points and functionalities. A systematic foundation for\nclassifying conversational AI platforms is currently lacking. We propose a\nframework for assessing the maturity level of conversational AI development\nplatforms. Our framework is based on a systematic literature review, in which\nwe extracted common and distinguishing features of various open-source and\ncommercial (or in-house) platforms. Inspired by language reference frameworks,\nwe identify different maturity levels that a conversational AI development\nplatform may exhibit in understanding and responding to user inputs. Our\nframework can guide organizations in selecting a conversational AI development\nplatform according to their needs, as well as helping researchers and platform\ndevelopers improving the maturity of their platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.09603,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Towards Grad-CAM Based Explainability in a Legal Text Processing\n  Pipeline\n\n  Explainable AI(XAI)is a domain focused on providing interpretability and\nexplainability of a decision-making process. In the domain of law, in addition\nto system and data transparency, it also requires the (legal-) decision-model\ntransparency and the ability to understand the models inner working when\narriving at the decision. This paper provides the first approaches to using a\npopular image processing technique, Grad-CAM, to showcase the explainability\nconcept for legal texts. With the help of adapted Grad-CAM metrics, we show the\ninterplay between the choice of embeddings, its consideration of contextual\ninformation, and their effect on downstream processing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.00378,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Mobile Game User Research: The World as Your Lab?\n\n  With the advent of mobile games and the according growing and competitive\nmarket, game user research can provide valuable insights and a competitive edge\nif methods and procedures are employed that match the distinct challenges that\nmobile devices, games and usage scenarios induce. We present a summary of\nparameters that frame the research setup and procedure, focusing on the\ntrade-offs between lab and field studies and the related decision whether to\npursue large-scale and quantitative or small-scale focused research accompanied\nby qualitative methods. We then illustrate the implications of these\nconsiderations on real world projects along the lines of two evaluations of\ndifferent input methods for the action-puzzle mobile game Somyeol: a local\nstudy with 37 participants and a mixed design of qualitative and quantitative\nmethods, and the strictly quantitative analysis of game-play data from 117,118\nusers. The findings underline the importance of small-scale evaluations prior\nto release.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.13265,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000008146,
      "text":"Hearing through Vibrations: Perception of Musical Emotions by Profoundly\n  Deaf People\n\n  Advances in tactile-audio feedback technology have created new possibilities\nfor deaf people to feel music. However, little is known about deaf individuals'\nperception of musical emotions through vibrotactile feedback. In this paper, we\npresent the findings from a mixed-methods study with 16 profoundly deaf\nparticipants. The study protocol was designed to explore how users of a\nbackpack-style vibrotactile display perceive intended emotions in twenty music\nexcerpts. Quantitative analysis demonstrated that participants correctly\nidentified happy and angry excerpts and rated them as more arousing than sad\nand peaceful excerpts. More positive emotions were experienced during happy\ncompared to angry excerpts while peaceful and sad excerpts were hard to be\ndifferentiated. Based on qualitative data, we highlight the benefits and\nlimitations of using vibrations to convey musical emotions to profoundly deaf\nusers. Finally, we provide guidelines for designing accessible music\nexperiences for the deaf community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.0352,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000055631,
      "text":"Functional Connectivity of Imagined Speech and Visual Imagery based on\n  Spectral Dynamics\n\n  Recent advances in brain-computer interface technology have shown the\npotential of imagined speech and visual imagery as a robust paradigm for\nintuitive brain-computer interface communication. However, the internal\ndynamics of the two paradigms along with their intrinsic features haven't been\nrevealed. In this paper, we investigated the functional connectivity of the two\nparadigms, considering various frequency ranges. The dataset of sixteen\nsubjects performing thirteen-class imagined speech and visual imagery were used\nfor the analysis. The phase-locking value of imagined speech and visual imagery\nwas analyzed in seven cortical regions with four frequency ranges. We compared\nthe functional connectivity of imagined speech and visual imagery with the\nresting state to investigate the brain alterations during the imagery. The\nphase-locking value in the whole brain region exhibited a significant decrease\nduring both imagined speech and visual imagery. Broca and Wernicke's area along\nwith the auditory cortex mainly exhibited a significant decrease in the\nimagined speech, and the prefrontal cortex and the auditory cortex have shown a\nsignificant decrease in the visual imagery paradigm. Further investigation on\nthe brain connectivity along with the decoding performance of the two paradigms\nmay play a crucial role as a performance predictor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.0913,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Empathic Chatbot: Emotional Intelligence for Empathic Chatbot: Emotional\n  Intelligence for Mental Health Well-being\n\n  Conversational chatbots are Artificial Intelligence (AI)-powered applications\nthat assist users with various tasks by responding in natural language and are\nprevalent across different industries. Most of the chatbots that we encounter\non websites and digital assistants such as Alexa, Siri does not express empathy\ntowards the user, and their ability to empathise remains immature. Lack of\nempathy towards the user is not critical for a transactional or interactive\nchatbot, but the bots designed to support mental healthcare patients need to\nunderstand the emotional state of the user and tailor the conversations. This\nresearch explains the different types of emotional intelligence methodologies\nadopted in the development of an empathic chatbot and how far they have been\nadopted and succeeded.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.00923,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000166562,
      "text":"Nuanced and Interrelated Mediations and Exigencies (NIME): Addressing\n  the Prevailing Political and Epistemological Crises\n\n  Nearly two decades after its inception as a workshop at the ACM Conference on\nHuman Factors in Computing Systems, NIME exists as an established international\nconference significantly distinct from its precursor. While this origin story\nis often noted, the implications of NIME's history as emerging from a field\npredominantly dealing with human-computer interaction have rarely been\ndiscussed. In this paper we highlight many of the recent -- and some not so\nrecent -- challenges that have been brought upon the NIME community as it\nattempts to maintain and expand its identity as a platform for\nmultidisciplinary research into HCI, interface design, and electronic and\ncomputer music. We discuss the relationship between the market demands of the\nneoliberal university -- which have underpinned academia's drive for innovation\n-- and the quantification and economisation of research performance which have\nfacilitated certain disciplinary and social frictions to emerge within\nNIME-related research and practice. Drawing on work that engages with feminist\ntheory and cultural studies, we suggest that critical reflection and moreover\nmediation is necessary in order to address burgeoning concerns which have been\nraised within the NIME discourse in relation to methodological approaches,\n`diversity and inclusion', `accessibility', and the fostering of rigorous\ninterdisciplinary research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.13961,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Mastering Music Instruments through Technology in Solo Learning Sessions\n\n  Mastering a musical instrument requires time-consuming practice even if\nstudents are guided by an expert. In the overwhelming majority of the time, the\nstudents practice by themselves and traditional teaching materials, such as\nvideos or textbooks, lack interaction and guidance possibilities. Adequate\nfeedback, however, is highly important to prevent the acquirement of wrong\nmotions and to avoid potential health problems. In this paper, we envision\nmusical instruments as smart objects to enhance solo learning sessions. We give\nan overview of existing approaches and setups and discuss them. Finally, we\nconclude with recommendations for designing smart and augmented musical\ninstruments for learning purposes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.10999,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Exploring Effectiveness of Inter-Microtask Qualification Tests in\n  Crowdsourcing\n\n  Qualification tests in crowdsourcing are often used to pre-filter workers by\nmeasuring their ability in executing microtasks.While creating qualification\ntests for each task type is considered as a common and reasonable way, this\nstudy investigates into its worker-filtering performance when the same\nqualification test is used across multiple types of tasks.On Amazon Mechanical\nTurk, we tested the annotation accuracy in six different cases where tasks\nconsisted of two different difficulty levels, arising from the identical\nreal-world domain: four combinatory cases in which the qualification test and\nthe actual task were the same or different from each other, as well as two\nother cases where workers with Masters Qualification were asked to perform the\nactual task only.The experimental results demonstrated the two following\nfindings: i) Workers that were assigned to a difficult qualification test\nscored better annotation accuracy regardless of the difficulty of the actual\ntask; ii) Workers with Masters Qualification scored better annotation accuracy\non the low-difficulty task, but were not as accurate as those who passed a\nqualification test on the high-difficulty task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.00855,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000186099,
      "text":"A Review of Deep Learning Approaches to EEG-Based Classification of\n  Cybersickness in Virtual Reality\n\n  Cybersickness is an unpleasant side effect of exposure to a virtual reality\n(VR) experience and refers to such physiological repercussions as nausea and\ndizziness triggered in response to VR exposure. Given the debilitating effect\nof cybersickness on the user experience in VR, academic interest in the\nautomatic detection of cybersickness from physiological measurements has\ncrested in recent years. Electroencephalography (EEG) has been extensively used\nto capture changes in electrical activity in the brain and to automatically\nclassify cybersickness from brainwaves using a variety of machine learning\nalgorithms. Recent advances in deep learning (DL) algorithms and increasing\navailability of computational resources for DL have paved the way for a new\narea of research into the application of DL frameworks to EEG-based detection\nof cybersickness. Accordingly, this review involved a systematic review of the\npeer-reviewed papers concerned with the application of DL frameworks to the\nclassification of cybersickness from EEG signals. The relevant literature was\nidentified through exhaustive database searches, and the papers were\nscrutinized with respect to experimental protocols for data collection, data\npreprocessing, and DL architectures. The review revealed a limited number of\nstudies in this nascent area of research and showed that the DL frameworks\nreported in these studies (i.e., DNN, CNN, and RNN) could classify\ncybersickness with an average accuracy rate of 93%. This review provides a\nsummary of the trends and issues in the application of DL frameworks to the\nEEG-based detection of cybersickness, with some guidelines for future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.06753,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000191728,
      "text":"Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive\n  Sense of Touch\n\n  Noninvasive brain-computer interface (BCI) is widely used to recognize users'\nintentions. Especially, BCI related to tactile and sensation decoding could\nprovide various effects on many industrial fields such as manufacturing\nadvanced touch displays, controlling robotic devices, and more immersive\nvirtual reality or augmented reality. In this paper, we introduce haptic and\nsensory perception-based BCI systems called neurohaptics. It is a preliminary\nstudy for a variety of scenarios using actual touch and touch imagery\nparadigms. We designed a novel experimental environment and a device that could\nacquire brain signals under touching designated materials to generate natural\ntouch and texture sensations. Through the experiment, we collected the\nelectroencephalogram (EEG) signals with respect to four different texture\nobjects. Seven subjects were recruited for the experiment and evaluated\nclassification performances using machine learning and deep learning\napproaches. Hence, we could confirm the feasibility of decoding actual touch\nand touch imagery on EEG signals to develop practical neurohaptics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03617,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000255307,
      "text":"Motor Imagery Classification Emphasizing Corresponding Frequency Domain\n  Method based on Deep Learning Framework\n\n  The electroencephalogram, a type of non-invasive-based brain signal that has\na user intention-related feature provides an efficient bidirectional pathway\nbetween user and computer. In this work, we proposed a deep learning framework\nbased on corresponding frequency empahsize method to decode the motor imagery\n(MI) data from 2020 International BCI competition dataset. The MI dataset\nconsists of 3-class, namely 'Cylindrical', 'Spherical', and 'Lumbrical'. We\nutilized power spectral density as an emphasize method and a convolutional\nneural network to classify the modified MI data. The results showed that\nMI-related frequency range was activated during MI task, and provide\nneurophysiological evidence to design the proposed method. When using the\nproposed method, the average classification performance in intra-session\ncondition was 69.68% and the average classification performance in\ninter-session condition was 52.76%. Our results provided the possibility of\ndeveloping a BCI-based device control system for practical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03533,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"Domain Generalization for Session-Independent Brain-Computer Interface\n\n  The inter\/intra-subject variability of electroencephalography (EEG) makes the\npractical use of the brain-computer interface (BCI) difficult. In general, the\nBCI system requires a calibration procedure to acquire subject\/session-specific\ndata to tune the model every time the system is used. This problem is\nrecognized as a major obstacle to BCI, and to overcome it, an approach based on\ndomain generalization (DG) has recently emerged. The main purpose of this paper\nis to reconsider how the zero-calibration problem of BCI for a realistic\nsituation can be overcome from the perspective of DG tasks. In terms of the\nrealistic situation, we have focused on creating an EEG classification\nframework that can be applied directly in unseen sessions, using only\nmulti-subject\/-session data acquired previously. Therefore, in this paper, we\ntested four deep learning models and four DG algorithms through\nleave-one-session-out validation. Our experiment showed that deeper and larger\nmodels were effective in cross-session generalization performance. Furthermore,\nwe found that none of the explicit DG algorithms outperformed empirical risk\nminimization. Finally, by comparing the results of fine-tuning using\nsubject-specific data, we found that subject-specific data may deteriorate\nunseen session classification performance due to inter-session variability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.0763,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Promoting Self-Efficacy Through an Effective Human-Powered Nonvisual\n  Smartphone Task Assistant\n\n  Accessibility assessments typically focus on determining a binary measurement\nof task performance success\/failure; and often neglect to acknowledge the\nnuances of those interactions. Although a large population of blind people find\nsmartphone interactions possible, many experiences take a significant toll and\ncan have a lasting negative impact on the individual and their willingness to\nstep out of technological comfort zones. There is a need to assist and support\nindividuals with the adoption and learning process of new tasks to mitigate\nthese negative experiences. We contribute with a human-powered nonvisual task\nassistant for smartphones to provide pervasive assistance. We argue, in\naddition to success, one must carefully consider promoting and evaluating\nfactors such as self-efficacy and the belief in one's own abilities to control\nand learn to use technology. In this paper, we show effective assistant\npositively affects self-efficacy when performing new tasks with smartphones,\naffects perceptions of accessibility and enables systemic task-based learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09999,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000647704,
      "text":"Democratizing information visualization. A study to map the value of\n  graphic design to easier knowledge transfer of scientific research\n\n  Visual representations are becoming important in science communication and\neducation. This explorative study investigates the perception of STEM\nresearchers, without any specific visual design background, and the value of\nvisual representations as tools to support the communication of technical and\nscientific knowledge among academics and a wider non-technical community. Early\nfindings show that visual representations can positively support scientists to\nshare research outcomes in a more compelling, visually clear, and impactful\nmanner, reaching a wider audience across different disciplines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06315,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"A Multi-Platform Study of Crowd Signals Associated with Successful\n  Online Fundraising\n\n  The growing popularity of online fundraising (aka \"crowdfunding\") has\nattracted significant research on the subject. In contrast to previous studies\nthat attempt to predict the success of crowdfunded projects based on specific\ncharacteristics of the projects and their creators, we present a more general\napproach that focuses on crowd dynamics and is robust to the particularities of\ndifferent crowdfunding platforms. We rely on a multi-method analysis to\ninvestigate the correlates, predictive importance, and quasi-causal effects of\nfeatures that describe crowd dynamics in determining the success of crowdfunded\nprojects. By applying a multi-method analysis to a study of fundraising in\nthree different online markets, we uncover general crowd dynamics that\nultimately decide which projects will succeed. In all analyses and across the\nthree different platforms, we consistently find that funders' behavioural\nsignals (1) are significantly correlated with fundraising success; (2)\napproximate fundraising outcomes better than the characteristics of projects\nand their creators such as credit grade, company valuation, and subject domain;\nand (3) have significant quasi-causal effects on fundraising outcomes while\ncontrolling for potentially confounding project variables. By showing that\nuniversal features deduced from crowd behaviour are predictive of fundraising\nsuccess on different crowdfunding platforms, our work provides design-relevant\ninsights about novel types of collective decision-making online. This research\ninspires thus potential ways to leverage cues from the crowd and catalyses\nresearch into crowd-aware system design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.00792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000035763,
      "text":"Eye Tracking to Understand Impact of Aging on Mobile Phone Applications\n\n  Usage of smartphones and tablets have been increasing rapidly with\nmulti-touch interaction and powerful configurations. Performing tasks on mobile\nphones become more complex as people age, thereby increasing their cognitive\nworkload. In this context, we conducted an eye tracking study with 50\nparticipants between the age of 20 to 60 years and above, living in Bangalore,\nIndia. This paper focuses on visual nature of interaction with mobile user\ninterfaces. The study aims to investigate how aging affects user experience on\nmobile phones while performing complex tasks, and estimate cognitive workload\nusing eye tracking metrics. The study consisted of five tasks that were\nperformed on an android mobile phone under naturalistic scenarios using eye\ntracking glasses. We recorded ocular parameters like fixation rate, saccadic\nrate, average fixation duration, maximum fixation duration and standard\ndeviation of pupil dilation for left and right eyes respectively for each\nparticipant. Results from our study show that aging has a bigger effect on\nperformance of using mobile phones irrespective of any complex task given to\nthem. We noted that, participants aged between 50 to 60+ years had difficulties\nin completing tasks and showed increased cognitive workload. They took longer\nfixation duration to complete tasks which involved copy-paste operations.\nFurther, we identifed design implications and provided design recommendations\nfor designers and manufacturers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.04743,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Mapping the Landscape of COVID-19 Crisis Visualizations\n\n  In response to COVID-19, a vast number of visualizations have been created to\ncommunicate information to the public. Information exposure in a public health\ncrisis can impact people's attitudes towards and responses to the crisis and\nrisks, and ultimately the trajectory of a pandemic. As such, there is a need\nfor work that documents, organizes, and investigates what COVID-19\nvisualizations have been presented to the public. We address this gap through\nan analysis of 668 COVID-19 visualizations. We present our findings through a\nconceptual framework derived from our analysis, that examines who, (uses) what\ndata, (to communicate) what messages, in what form, under what circumstances in\nthe context of COVID-19 crisis visualizations. We provide a set of factors to\nbe considered within each component of the framework. We conclude with\ndirections for future crisis visualization research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.00812,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Smartphone Sensor-based Human Activity Recognition Robust to Different\n  Sampling Rates\n\n  There is a research field of human activity recognition that automatically\nrecognizes a user's physical activity through sensing technology incorporated\nin smartphones and other devices. When sensing daily activity, various\nmeasurement conditions, such as device type, possession method, wearing method,\nand measurement application, are often different depending on the user and the\ndate of the measurement. Models that predict activity from sensor values are\noften implemented by machine learning and are trained using a large amount of\nactivity-labeled sensor data measured from many users who provide labeled\nsensor data. However, collecting activity-labeled sensor data using each user's\nindividual smartphones causes data being measured in inconsistent environments\nthat may degrade the estimation accuracy of machine learning. In this study, I\npropose an activity recognition method that is robust to different sampling\nrates -- even in the measurement environment. The proposed method applies an\nadversarial network and data augmentation by downsampling to a common activity\nrecognition model to achieve the acquisition of feature representations that\nmake the sampling rate unspecifiable. Using the Human Activity Sensing\nConsortium (HASC), which is a dataset of basic activity recognition using\nsmartphone sensors, I conducted an evaluation experiment to simulate an\nenvironment in which various sampling rates were measured. As a result, I found\nthat estimation accuracy was reduced by the conventional method in the above\nenvironment and could be improved by my proposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.11278,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"\"Can I Touch This?\": Survey of Virtual Reality Interactions via Haptic\n  Solutions\n\n  Haptic feedback has become crucial to enhance the user experiences in Virtual\nReality (VR). This justifies the sudden burst of novel haptic solutions\nproposed these past years in the HCI community. This article is a survey of\nVirtual Reality interactions, relying on haptic devices. We propose two\ndimensions to describe and compare the current haptic solutions: their degree\nof physicality, as well as their degree of actuation. We depict a compromise\nbetween the user and the designer, highlighting how the range of required or\nproposed stimulation in VR is opposed to the haptic interfaces flexibility and\ntheir deployment in real-life use-cases. This paper (1) outlines the variety of\nhaptic solutions and provides a novel perspective for analysing their\nassociated interactions, (2) highlights the limits of the current evaluation\ncriteria regarding these interactions, and finally (3) reflects the\ninteraction, operation and conception potentials of \"encountered-type of haptic\ndevices\".\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10245,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"AirWare: Utilizing Embedded Audio and Infrared Signals for In-Air\n  Hand-Gesture Recognition\n\n  We introduce AirWare, an in-air hand-gesture recognition system that uses the\nalready embedded speaker and microphone in most electronic devices, together\nwith embedded infrared proximity sensors. Gestures identified by AirWare are\nperformed in the air above a touchscreen or a mobile phone. AirWare utilizes\nconvolutional neural networks to classify a large vocabulary of hand gestures\nusing multi-modal audio Doppler signatures and infrared (IR) sensor\ninformation. As opposed to other systems which use high frequency Doppler\nradars or depth cameras to uniquely identify in-air gestures, AirWare does not\nrequire any external sensors. In our analysis, we use openly available APIs to\ninterface with the Samsung Galaxy S5 audio and proximity sensors for data\ncollection. We find that AirWare is not reliable enough for a deployable\ninteraction system when trying to classify a gesture set of 21 gestures, with\nan average true positive rate of only 50.5% per gesture. To improve\nperformance, we train AirWare to identify subsets of the 21 gestures vocabulary\nbased on possible usage scenarios. We find that AirWare can identify three\ngesture sets with average true positive rate greater than 80% using 4--7\ngestures per set, which comprises a vocabulary of 16 unique in-air gestures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.11326,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000074506,
      "text":"See-Through Captions: Real-Time Captioning on Transparent Display for\n  Deaf and Hard-of-Hearing People\n\n  Real-time captioning is a useful technique for deaf and hard-of-hearing (DHH)\npeople to talk to hearing people. With the improvement in device performance\nand the accuracy of automatic speech recognition (ASR), real-time captioning is\nbecoming an important tool for helping DHH people in their daily lives. To\nrealize higher-quality communication and overcome the limitations of mobile and\naugmented-reality devices, real-time captioning that can be used comfortably\nwhile maintaining nonverbal communication and preventing incorrect recognition\nis required. Therefore, we propose a real-time captioning system that uses a\ntransparent display. In this system, the captions are presented on both sides\nof the display to address the problem of incorrect ASR, and the highly\ntransparent display makes it possible to see both the body language and the\ncaptions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09161,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer\n  Design Feedback in the Classroom\n\n  Feedback is an important aspect of design education, and crowdsourcing has\nemerged as a convenient way to obtain feedback at scale. In this paper, we\ninvestigate how crowdsourced design feedback compares to peer design feedback\nwithin a design-oriented HCI class and across two metrics: perceived quality\nand perceived fairness. We also examine the perceived monetary value of\ncrowdsourced feedback, which provides an interesting contrast to the typical\nrequester-centric view of the value of labor on crowdsourcing platforms. Our\nresults reveal that the students (N=106) perceived the crowdsourced design\nfeedback as inferior to peer design feedback in multiple ways. However, they\nalso identified various positive aspects of the online crowds that peers cannot\nprovide. We discuss the meaning of the findings and provide suggestions for\nteachers in HCI and other researchers interested in crowd feedback systems on\nusing crowds as a potential complement to peers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000063909,
      "text":"Proxemics and Social Interactions in an Instrumented Virtual Reality\n  Workshop\n\n  Virtual environments (VEs) can create collaborative and social spaces, which\nare increasingly important in the face of remote work and travel reduction.\nRecent advances, such as more open and widely available platforms, create new\npossibilities to observe and analyse interaction in VEs. Using a custom\ninstrumented build of Mozilla Hubs to measure position and orientation, we\nconducted an academic workshop to facilitate a range of typical workshop\nactivities. We analysed social interactions during a keynote, small group\nbreakouts, and informal networking\/hallway conversations. Our mixed-methods\napproach combined environment logging, observations, and semi-structured\ninterviews. The results demonstrate how small and large spaces influenced group\nformation, shared attention, and personal space, where smaller rooms\nfacilitated more cohesive groups while larger rooms made small group formation\nchallenging but personal space more flexible. Beyond our findings, we show how\nthe combination of data and insights can fuel collaborative spaces' design and\ndeliver more effective virtual workshops.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08046,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Extended Reality (XR) Remote Research: a Survey of Drawbacks and\n  Opportunities\n\n  Extended Reality (XR) technology - such as virtual and augmented reality - is\nnow widely used in Human Computer Interaction (HCI), social science and\npsychology experimentation. However, these experiments are predominantly\ndeployed in-lab with a co-present researcher. Remote experiments, without\nco-present researchers, have not flourished, despite the success of remote\napproaches for non-XR investigations. This paper summarises findings from a\n30-item survey of 46 XR researchers to understand perceived limitations and\nbenefits of remote XR experimentation. Our thematic analysis identifies\nconcerns common with non-XR remote research, such as participant recruitment,\nas well as XR-specific issues, including safety and hardware variability. We\nidentify potential positive affordances of XR technology, including leveraging\ndata collection functionalities builtin to HMDs (e.g. hand, gaze tracking) and\nthe portability and reproducibility of an experimental setting. We suggest that\nXR technology could be conceptualised as an interactive technology and a\ncapable data-collection device suited for remote experimentation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.07999,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"No More Handshaking: How have COVID-19 pushed the expansion of\n  computer-mediated communication in Japanese idol culture?\n\n  In Japanese idol culture, meet-and-greet events where fans were allowed to\nhandshake with an idol member for several seconds were regarded as its\nessential component until the spread of COVID-19. Now, idol groups are\nstruggling in the transition of such events to computer-mediated communication\nbecause these events had emphasized meeting face-to-face over communicating, as\nwe can infer from their length of time. I anticipated that investigating this\nemerging transition would provide implications because their communication has\na unique characteristic that is distinct from well-studied situations, such as\nworkplace communication and intimate relationships. Therefore, I first\nconducted a quantitative survey to develop a precise understanding of the\ntransition, and based on its results, had semi-structured interviews with idol\nfans about their perceptions of the transition. The survey revealed distinctive\napproaches, including one where fans gathered at a venue but were isolated from\nthe idol member by an acrylic plate and talked via a video call. Then the\ninterviews not only provided answers to why such an approach would be\nreasonable but also suggested the existence of a large gap between conventional\noffline events and emerging online events in their perceptions. Based on the\nresults, I discussed how we can develop interaction techniques to support this\ntransition and how we can apply it to other situations outside idol culture,\nsuch as computer-mediated performing arts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.11054,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"Remote Learners, Home Makers: How Digital Fabrication Was Taught Online\n  During a Pandemic\n\n  Digital fabrication courses that relied on physical makerspaces were severely\ndisrupted by COVID-19. As universities shut down in Spring 2020, instructors\ndeveloped new models for digital fabrication at a distance. Through interviews\nwith faculty and students and examination of course materials, we recount the\nexperiences of eight remote digital fabrication courses. We found that learning\nwith hobbyist equipment and online social networks could emulate using\nindustrial equipment in shared workshops. Furthermore, at-home digital\nfabrication offered unique learning opportunities including more iteration,\nmachine tuning, and maintenance. These opportunities depended on new forms of\nlabor and varied based on student living situations. Our findings have\nimplications for remote and in-person digital fabrication instruction. They\nindicate how access to tools was important, but not as critical as providing\nopportunities for iteration; they show how remote fabrication exacerbated\nstudent inequities; and they suggest strategies for evaluating trade-offs in\nremote fabrication models with respect to learning objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08048,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Exploring Design and Governance Challenges in the Development of\n  Privacy-Preserving Computation\n\n  Homomorphic encryption, secure multi-party computation, and differential\nprivacy are part of an emerging class of Privacy Enhancing Technologies which\nshare a common promise: to preserve privacy whilst also obtaining the benefits\nof computational analysis. Due to their relative novelty, complexity, and\nopacity, these technologies provoke a variety of novel questions for design and\ngovernance. We interviewed researchers, developers, industry leaders,\npolicymakers, and designers involved in their deployment to explore\nmotivations, expectations, perceived opportunities and barriers to adoption.\nThis provided insight into several pertinent challenges facing the adoption of\nthese technologies, including: how they might make a nebulous concept like\nprivacy computationally tractable; how to make them more usable by developers;\nand how they could be explained and made accountable to stakeholders and wider\nsociety. We conclude with implications for the development, deployment, and\nresponsible governance of these privacy-preserving computation techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02576,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Discussing the Risks of Adaptive Virtual Environments for User Autonomy\n\n  Adaptive virtual environments are an opportunity to support users and\nincrease their flow, presence, immersion, and overall experience. Possible\nfields of application are adaptive individual education, gameplay adjustment,\nprofessional work, and personalized content. But who benefits more from this\nadaptivity, the users who can enjoy a greater user experience or the companies\nor governments who are completely in control of the provided content. While the\nuser autonomy decreases for individuals, the power of institutions raises, and\nthe risk exists that personal opinions are precisely controlled. In this\nposition paper, we will argue that researchers should not only propose the\nbenefits of their work but also critically discuss what are possible abusive\nuse cases. Therefore, we will examine two use cases in the fields of\nprofessional work and personalized content and show possible abusive use.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.03648,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"User perspectives on critical factors for collaborative playlists\n\n  Collaborative playlists (CP) enable listeners to curate music together,\ntranslating long-standing social practices around music consumption into the\nage of streaming. Yet despite their role in connecting people through music, we\nlack an understanding of factors that are critical to CPs and their enjoyment.\nTo understand what users consider important to CPs and their usage, we\ninvestigated aspects that are perceived to be most useful and lacking in\ntoday's CP implementations. We conducted a survey to collect open-ended text\nresponses from real-world CP users. Using thematic analysis, we derived the\nCodebook of Critical CP Factors, which comprises eight aspects. We gained\ninsights into which aspects are particularly useful, and which are absent and\ndesired by current CP users. From these findings we propose design implications\nto inform further design of CP functionalities and platforms, and highlight\npotential benefits and challenges related to their adoption in current music\nservices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12715,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"Disparate Impact Diminishes Consumer Trust Even for Advantaged Users\n\n  Systems aiming to aid consumers in their decision-making (e.g., by\nimplementing persuasive techniques) are more likely to be effective when\nconsumers trust them. However, recent research has demonstrated that the\nmachine learning algorithms that often underlie such technology can act\nunfairly towards specific groups (e.g., by making more favorable predictions\nfor men than for women). An undesired disparate impact resulting from this kind\nof algorithmic unfairness could diminish consumer trust and thereby undermine\nthe purpose of the system. We studied this effect by conducting a\nbetween-subjects user study investigating how (gender-related) disparate impact\naffected consumer trust in an app designed to improve consumers' financial\ndecision-making. Our results show that disparate impact decreased consumers'\ntrust in the system and made them less likely to use it. Moreover, we find that\ntrust was affected to the same degree across consumer groups (i.e., advantaged\nand disadvantaged users) despite both of these consumer groups recognizing\ntheir respective levels of personal benefit. Our findings highlight the\nimportance of fairness in consumer-oriented artificial intelligence systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12284,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"AffectiveSpotlight: Facilitating the Communication of Affective\n  Responses from Audience Members during Online Presentations\n\n  The ability to monitor audience reactions is critical when delivering\npresentations. However, current videoconferencing platforms offer limited\nsolutions to support this. This work leverages recent advances in affect\nsensing to capture and facilitate communication of relevant audience signals.\nUsing an exploratory survey (N = 175), we assessed the most relevant audience\nresponses such as confusion, engagement, and head-nods. We then implemented\nAffectiveSpotlight, a Microsoft Teams bot that analyzes facial responses and\nhead gestures of audience members and dynamically spotlights the most\nexpressive ones. In a within-subjects study with 14 groups (N = 117), we\nobserved that the system made presenters significantly more aware of their\naudience, speak for a longer period of time, and self-assess the quality of\ntheir talk more similarly to the audience members, compared to two control\nconditions (randomly-selected spotlight and default platform UI). We provide\ndesign recommendations for future affective interfaces for online presentations\nbased on feedback from the study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08155,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000069539,
      "text":"On the Readability of Abstract Set Visualizations\n\n  Set systems are used to model data that naturally arises in many contexts:\nsocial networks have communities, musicians have genres, and patients have\nsymptoms. Visualizations that accurately reflect the information in the\nunderlying set system make it possible to identify the set elements, the sets\nthemselves, and the relationships between the sets. In static contexts, such as\nprint media or infographics, it is necessary to capture this information\nwithout the help of interactions. With this in mind, we consider three\ndifferent systems for medium-sized set data, LineSets, EulerView, and\nMetroSets, and report the results of a controlled human-subjects experiment\ncomparing their effectiveness. Specifically, we evaluate the performance, in\nterms of time and error, on tasks that cover the spectrum of static set-based\ntasks. We also collect and analyze qualitative data about the three different\nvisualization systems. Our results include statistically significant\ndifferences, suggesting that MetroSets performs and scales better.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11819,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000081791,
      "text":"I Want My App That Way: Reclaiming Sovereignty Over Personal Devices\n\n  Dark patterns in mobile apps take advantage of cognitive biases of end-users\nand can have detrimental effects on people's lives. Despite growing research in\nidentifying remedies for dark patterns and established solutions for desktop\nbrowsers, there exists no established methodology to reduce dark patterns in\nmobile apps. Our work introduces GreaseDroid, a community-driven app\nmodification framework enabling non-expert users to disable dark patterns in\napps selectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09803,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Meeting Effectiveness and Inclusiveness in Remote Collaboration\n\n  A primary goal of remote collaboration tools is to provide effective and\ninclusive meetings for all participants. To study meeting effectiveness and\nmeeting inclusiveness, we first conducted a large-scale email survey (N=4,425;\nafter filtering N=3,290) at a large technology company (pre-COVID-19); using\nthis data we derived a multivariate model of meeting effectiveness and show how\nit correlates with meeting inclusiveness, participation, and feeling\ncomfortable to contribute. We believe this is the first such model of meeting\neffectiveness and inclusiveness. The large size of the data provided the\nopportunity to analyze correlations that are specific to sub-populations such\nas the impact of video. The model shows the following factors are correlated\nwith inclusiveness, effectiveness, participation, and feeling comfortable to\ncontribute in meetings: sending a pre-meeting communication, sending a\npost-meeting summary, including a meeting agenda, attendee location,\nremote-only meeting, audio\/video quality and reliability, video usage, and\nmeeting size. The model and survey results give a quantitative understanding of\nhow and where to improve meeting effectiveness and inclusiveness and what the\npotential returns are.\n  Motivated by the email survey results, we implemented a post-meeting survey\ninto a leading computer-mediated communication (CMC) system to directly measure\nmeeting effectiveness and inclusiveness (during COVID-19). Using initial\nresults based on internal flighting we created a similar model of effectiveness\nand inclusiveness, with many of the same findings as the email survey. This\nshows a method of measuring and understanding these metrics which are both\npractical and useful in a commercial CMC system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12606,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"3D4ALL: Toward an Inclusive Pipeline to Classify 3D Contents\n\n  Algorithmic content moderation manages an explosive number of user-created\ncontent shared online everyday. Despite a massive number of 3D designs that are\nfree to be downloaded, shared, and 3D printed by the users, detecting\nsensitivity with transparency and fairness has been controversial. Although\nsensitive 3D content might have a greater impact than other media due to its\npossible reproducibility and replicability without restriction, prevailed\nunawareness resulted in proliferation of sensitive 3D models online and a lack\nof discussion on transparent and fair 3D content moderation. As the 3D content\nexists as a document on the web mainly consisting of text and images, we first\nstudy the existing algorithmic efforts based on text and images and the prior\nendeavors to encompass transparency and fairness in moderation, which can also\nbe useful in a 3D printing domain. At the same time, we identify 3D specific\nfeatures that should be addressed to advance a 3D specialized algorithmic\nmoderation. As a potential solution, we suggest a human-in-the-loop pipeline\nusing augmented learning, powered by various stakeholders with different\nbackgrounds and perspectives in understanding the content. Our pipeline aims to\nminimize personal biases by enabling diverse stakeholders to be vocal in\nreflecting various factors to interpret the content. We add our initial\nproposal for redesigning metadata of open 3D repositories, to invoke users'\nresponsible actions of being granted consent from the subject upon sharing\ncontents for free in the public spaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03742,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Reconstructing Detailed Browsing Activities from Browser History\n\n  Users' detailed browsing activity - such as what sites they are spending time\non and for how long, and what tabs they have open and which one is focused at\nany given time - is useful for a number of research and practical applications.\nGathering such data, however, requires that users install and use a monitoring\ntool over long periods of time. In contrast, browser extensions can gain\ninstantaneous access months of browser history data. However, the browser\nhistory is incomplete: it records only navigation events, missing important\ninformation such as time spent or tab focused. In this work, we aim to\nreconstruct time spent on sites with only users' browsing histories. We\ngathered three months of browsing history and two weeks of ground-truth\ndetailed browsing activity from 185 participants. We developed a machine\nlearning algorithm that predicts whether the browser window is focused and\nactive at one second-level granularity with an F1-score of 0.84. During periods\nwhen the browser is active, the algorithm can predict which the domain the user\nwas looking at with 76.2% accuracy. We can use these results to reconstruct the\ntotal time spent online for each user with an R^2 value of 0.96, and the total\ntime each user spent on each domain with an R^2 value of 0.92.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01196,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"Soliciting Stakeholders' Fairness Notions in Child Maltreatment\n  Predictive Systems\n\n  Recent work in fair machine learning has proposed dozens of technical\ndefinitions of algorithmic fairness and methods for enforcing these\ndefinitions. However, we still lack an understanding of how to develop machine\nlearning systems with fairness criteria that reflect relevant stakeholders'\nnuanced viewpoints in real-world contexts. To address this gap, we propose a\nframework for eliciting stakeholders' subjective fairness notions. Combining a\nuser interface that allows stakeholders to examine the data and the algorithm's\npredictions with an interview protocol to probe stakeholders' thoughts while\nthey are interacting with the interface, we can identify stakeholders' fairness\nbeliefs and principles. We conduct a user study to evaluate our framework in\nthe setting of a child maltreatment predictive system. Our evaluations show\nthat the framework allows stakeholders to comprehensively convey their fairness\nviewpoints. We also discuss how our results can inform the design of predictive\nsystems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09453,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Towards augmented reality for corporate training\n\n  Corporate training relates to employees acquiring essential skills to operate\nequipment or effectively performing required tasks both competently and safely.\nUnlike formal education, training can be incorporated into the task workflow\nand performed during working hours. Increasingly, organizations adopt different\ntechnologies to develop both individual skills and improve their organization.\nStudies indicate that Augmented Reality (AR) is quickly becoming an effective\ntechnology for training programs. This systematic literature review (SLR) aims\nto screen works published on AR for corporate training. We describe AR training\napplications, discuss current challenges, literature gaps, opportunities, and\ntendencies of corporate AR solutions. We structured a protocol to define\nkeywords, the semantics of research, and databases used as sources of this SLR.\nFrom a primary analysis, we considered 1952 articles in the review for\nqualitative synthesis. We selected 60 among the selected articles for this\nstudy. The survey shows a large number of 41.7% of applications focused on\nautomotive and medical training. Additionally, 20% of selected publications use\na camera-display with a tablet device, while 40% refer to\nhead-mounted-displays, and many surveyed approaches (45%) adopt marker-based\ntracking. Results indicate that publications on AR for corporate training\nincreased significantly in recent years. AR has been used in many areas,\nexhibiting high quality and provides viable approaches to On-The-Job training.\nFinally, we discuss future research issues related to increasing relevance\nregarding AR for corporate training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.06231,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"To Reuse or Not To Reuse? A Framework and System for Evaluating\n  Summarized Knowledge\n\n  As the amount of information online continues to grow, a correspondingly\nimportant opportunity is for individuals to reuse knowledge which has been\nsummarized by others rather than starting from scratch. However, appropriate\nreuse requires judging the relevance, trustworthiness, and thoroughness of\nothers' knowledge in relation to an individual's goals and context. In this\nwork, we explore augmenting judgements of the appropriateness of reusing\nknowledge in the domain of programming, specifically of reusing artifacts that\nresult from other developers' searching and decision making. Through an\nanalysis of prior research on sensemaking and trust, along with new interviews\nwith developers, we synthesized a framework for reuse judgements. The\ninterviews also validated that developers express a desire for help with\njudging whether to reuse an existing decision. From this framework, we\ndeveloped a set of techniques for capturing the initial decision maker's\nbehavior and visualizing signals calculated based on the behavior, to\nfacilitate subsequent consumers' reuse decisions, instantiated in a prototype\nsystem called Strata. Results of a user study suggest that the system\nsignificantly improves the accuracy, depth, and speed of reusing decisions.\nThese results have implications for systems involving user-generated content in\nwhich other users need to evaluate the relevance and trustworthiness of that\ncontent.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03955,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"A Probabilistic Interpretation of Motion Correlation Selection\n  Techniques\n\n  Motion correlation interfaces are those that present targets moving in\ndifferent patterns, which the user can select by matching their motion. In this\npaper, we re-formulate the task of target selection as a probabilistic\ninference problem. We demonstrate that previous interaction techniques can be\nmodelled using a Bayesian approach and that how modelling the selection task as\ntransmission of information can help us make explicit the assumptions behind\nsimilarity measures. We propose ways of incorporating uncertainty into the\ndecision-making process and demonstrate how the concept of entropy can\nilluminate the measurement of the quality of a design. We apply these\ntechniques in a case study and suggest guidelines for future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01273,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Technology Developments in Touch-Based Accessible Graphics: A Systematic\n  Review of Research 2010-2020\n\n  This paper presents a systematic literature review of 292 publications from\n97 unique venues on touch-based graphics for people who are blind or have low\nvision, from 2010 to mid-2020. It is the first review of its kind on\ntouch-based accessible graphics. It is timely because it allows us to assess\nthe impact of new technologies such as commodity 3D printing and low-cost\nelectronics on the production and presentation of accessible graphics. As\nexpected our review shows an increase in publications from 2014 that we can\nattribute to these developments. It also reveals the need to: broaden\napplication areas, especially to the workplace; broaden end-user participation\nthroughout the full design process; and conduct more in situ evaluation. This\nwork is linked to an online living resource to be shared with the wider\ncommunity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.08512,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Addressing the Need for Remote Patient Monitoring Applications in\n  Appalachian Areas\n\n  There is a need to address the urban-rural disparities in healthcare\nregarding equal access and quality of care. Due to higher rates of chronic\ndisease, reduced access to providers, and a continuous decline in rural\nhospitals, it is imperative that Appalachian cancer patients adopt the use of\nhealth information technology (HIT). The NCCN Distress Thermometer and Problem\nList (DT) is under-utilized, not patient-centered, does not consider provider\nneeds, and is outdated in the current digital landscape. Digitizing patient\ndistress screening poses advantages, such as allowing for more frequent\nscreenings, removing geographical barriers, and rural patient autonomy. In this\npaper, we discuss how knowledge gained from patient-centered design led to the\nunderpinnings of developing a rural remote patient monitoring app that provides\ndelightful and insightful experiences to users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12592,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Documentation Matters: Human-Centered AI System to Assist Data Science\n  Code Documentation in Computational Notebooks\n\n  Computational notebooks allow data scientists to express their ideas through\na combination of code and documentation. However, data scientists often pay\nattention only to the code, and neglect creating or updating their\ndocumentation during quick iterations. Inspired by human documentation\npractices learned from 80 highly-voted Kaggle notebooks, we design and\nimplement Themisto, an automated documentation generation system to explore how\nhuman-centered AI systems can support human data scientists in the machine\nlearning code documentation scenario. Themisto facilitates the creation of\ndocumentation via three approaches: a deep-learning-based approach to generate\ndocumentation for source code, a query-based approach to retrieve online API\ndocumentation for source code, and a user prompt approach to nudge users to\nwrite documentation. We evaluated Themisto in a within-subjects experiment with\n24 data science practitioners, and found that automated documentation\ngeneration techniques reduced the time for writing documentation, reminded\nparticipants to document code they would have ignored, and improved\nparticipants' satisfaction with their computational notebook.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.02437,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"EUCA: the End-User-Centered Explainable AI Framework\n\n  The ability to explain decisions to end-users is a necessity to deploy AI as\ncritical decision support. Yet making AI explainable to non-technical end-users\nis a relatively ignored and challenging problem. To bridge the gap, we first\nidentify twelve end-user-friendly explanatory forms that do not require\ntechnical knowledge to comprehend, including feature-, example-, and rule-based\nexplanations. We then instantiate the explanatory forms as prototyping cards in\nfour AI-assisted critical decision-making tasks, and conduct a user study to\nco-design low-fidelity prototypes with 32 layperson participants. The results\nconfirm the relevance of using explanatory forms as building blocks of\nexplanations, and identify their proprieties - pros, cons, applicable\nexplanation goals, and design implications. The explanatory forms, their\nproprieties, and prototyping supports (including a suggested prototyping\nprocess, design templates and exemplars, and associated algorithms to actualize\nexplanatory forms) constitute the End-User-Centered explainable AI framework\nEUCA, and is available at http:\/\/weinajin.github.io\/end-user-xai . It serves as\na practical prototyping toolkit for HCI\/AI practitioners and researchers to\nunderstand user requirements and build end-user-centered explainable AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.08909,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000188086,
      "text":"Surveying the Landscape of Ethics-Focused Design Methods\n\n  Over the past decade, HCI researchers, design researchers, and practitioners\nhave increasingly addressed ethics-focused issues through a range of\ntheoretical, methodological and pragmatic contributions to the field. While\nmany forms of design knowledge have been proposed and described, we focus\nexplicitly on knowledge that has been codified as \"methods,\" which we define as\nany supports for everyday work practices of designers. In this paper, we\nidentify, analyze, and map a collection of 63 existing ethics-focused methods\nintentionally designed for ethical impact. We present a content analysis,\nproviding a descriptive record of how they operationalize ethics, their\nintended audience or context of use, their \"core\" or \"script,\" and the means by\nwhich these methods are formulated, articulated, and languaged. Building on\nthese results, we provide an initial definition of ethics-focused methods,\nidentifying potential opportunities for the development of future methods to\nsupport design practice and research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11207,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Remote VR Studies -- A Framework for Running Virtual Reality Studies\n  Remotely Via Participant-Owned HMDs\n\n  We investigate the opportunities and challenges of running virtual reality\n(VR) studies remotely. Today, many consumers own head-mounted displays (HMDs),\nallowing them to participate in scientific studies from their homes using their\nown equipment. Researchers can benefit from this approach by being able to\nreach a more diverse study population and to conduct research at times when it\nis difficult to get people into the lab (cf. the COVID pandemic). We first\nconducted an online survey (N=227), assessing HMD owners' demographics, their\nVR setups, and their attitudes towards remote participation. We then identified\ndifferent approaches to running remote studies and conducted two case studies\nfor an in-depth understanding. We synthesize our findings into a framework for\nremote VR studies, discuss the strengths and weaknesses of the different\napproaches, and derive best practices. Our work is valuable for HCI researchers\nconducting VR studies outside labs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.0747,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000085764,
      "text":"Creepy Technology: What Is It and How Do You Measure It?\n\n  Interactive technologies are getting closer to our bodies and permeate the\ninfrastructure of our homes. While such technologies offer many benefits, they\ncan also cause an initial feeling of unease in users. It is important for\nHuman-Computer Interaction to manage first impressions and avoid designing\ntechnologies that appear creepy. To that end, we developed the Perceived\nCreepiness of Technology Scale (PCTS), which measures how creepy a technology\nappears to a user in an initial encounter with a new artefact. The scale was\ndeveloped based on past work on creepiness and a set of ten focus groups\nconducted with users from diverse backgrounds. We followed a structured process\nof analytically developing and validating the scale. The PCTS is designed to\nenable designers and researchers to quickly compare interactive technologies\nand ensure that they do not design technologies that produce initial feelings\nof creepiness in users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.00589,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000212921,
      "text":"Tale of Seven Alerts: Enhancing Wireless Emergency Alerts (WEAs) to\n  Reduce Cellular Network Usage During Disasters\n\n  In weather disasters, first responders access dedicated communication\nchannels different from civilian commercial channels to facilitate rescues.\nHowever, rescues in recent disasters have increasingly involved civilian and\nvolunteer forces, requiring civilian channels not to be overloaded with\ntraffic. We explore seven enhancements to the wording of Wireless Emergency\nAlerts (WEAs) and their effectiveness in getting smartphone users to comply,\nincluding reducing frivolous mobile data consumption during critical weather\ndisasters. We conducted a between-subjects survey (N=898), in which\nparticipants were either assigned no alert (control) or an alert framed as\nBasic Information, Altruism, Multimedia, Negative Feedback, Positive Feedback,\nReward, or Punishment. We find that Basic Information alerts resulted in the\nlargest reduction of multimedia and video services usage; we also find that\nPunishment alerts have the lowest absolute compliance. This work has\nimplications for creating more effective WEAs and providing a better\nunderstanding of how wording can affect emergency alert compliance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07127,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Affective State Recognition through EEG Signals Feature Level Fusion and\n  Ensemble Classifier\n\n  Human affects are complex paradox and an active research domain in affective\ncomputing. Affects are traditionally determined through a self-report based\npsychometric questionnaire or through facial expression recognition. However,\nfew state-of-the-arts pieces of research have shown the possibilities of\nrecognizing human affects from psychophysiological and neurological signals. In\nthis article, electroencephalogram (EEG) signals are used to recognize human\naffects. The electroencephalogram (EEG) of 100 participants are collected where\nthey are given to watch one-minute video stimuli to induce different affective\nstates. The videos with emotional tags have a variety range of affects\nincluding happy, sad, disgust, and peaceful. The experimental stimuli are\ncollected and analyzed intensively. The interrelationship between the EEG\nsignal frequencies and the ratings given by the participants are taken into\nconsideration for classifying affective states. Advanced feature extraction\ntechniques are applied along with the statistical features to prepare a fused\nfeature vector of affective state recognition. Factor analysis methods are also\napplied to select discriminative features. Finally, several popular supervised\nmachine learning classifier is applied to recognize different affective states\nfrom the discriminative feature vector. Based on the experiment, the designed\nrandom forest classifier produces 89.06% accuracy in classifying four basic\naffective states.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.13407,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"The Virtual Emotion Loop: Towards Emotion-Driven Services via Virtual\n  Reality\n\n  The importance of emotions in service and in product design is well known.\nHowever, it is still not very well understood how users' emotions can be\nincorporated in a product or service lifecycle. We argue that this gap is due\nto a lack of a methodological framework for an effective investigation of the\nemotional response of persons when using products and services. Indeed, the\nemotional response of users is generally investigated by means of methods\n(e.g., surveys) that are not effective for this purpose. In our view, Virtual\nReality (VR) technologies represent the perfect medium to evoke and recognize\nusers' emotional response, as well as to prototype products and services (and,\nfor the latter, even deliver them). In this paper, we first provide our\ndefinition of emotion-driven services, and then we propose a novel\nmethodological framework, referred to as the Virtual-Reality-Based\nEmotion-Elicitation-and-Recognition loop (VEE-loop), that can be exploited to\nrealize it. Specifically, the VEE-loop consists in a continuous monitoring of\nusers' emotions, which are then provided to service designers as an implicit\nusers' feedback. This information is used to dynamically change the content of\nthe VR environment, until the desired affective state is solicited. Finally, we\ndiscuss issues and opportunities of this VEE-loop, and we also present\npotential applications of the VEE-loop in research and in various application\nareas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10685,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"EvoK: Connecting loved ones through Heart Rate sharing\n\n  In this work, we present EvoK, a new way of sharing one's heart rate with\nfeedback from their close contacts to alleviate social isolation and\nloneliness. EvoK consists of a pair of wearable prototype devices (i.e., sender\nand receiver). The sender is designed as a headband enabling continuous sensing\nof heart rate with aesthetic designs to maximize social acceptance. The\nreceiver is designed as a wristwatch enabling unobtrusive receiving of the\nloved one's continuous heart rate with multi-modal notification systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03796,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000102652,
      "text":"Brain-computer interface with rapid serial multimodal presentation using\n  artificial facial images and voice\n\n  Electroencephalography (EEG) signals elicited by multimodal stimuli can drive\nbrain-computer interfaces (BCIs), and research has demonstrated that visual and\nauditory stimuli can be employed simultaneously to improve BCI performance.\nHowever, no studies have investigated the effect of multimodal stimuli in rapid\nserial visual presentation (RSVP) BCIs. In the present study, we propose a\nrapid serial multimodal presentation (RSMP) BCI that incorporates artificial\nfacial images and artificial voice stimuli. To clarify the effect of\naudiovisual stimuli on the RSMP BCI, scrambled images and masked sounds were\napplied instead of visual and auditory stimuli, respectively. Our findings\nindicated that the audiovisual stimuli improved the performance of the RSMP\nBCI, and that the P300 at Pz contributed to classification accuracy. Online\naccuracy of BCI reached 85.7+-11.5%. Taken together, these findings may aid in\nthe development of better gaze-independent BCI systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.00148,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000118547,
      "text":"Showing Academic Performance Predictions during Term Planning: Effects\n  on Students' Decisions, Behaviors, and Preferences\n\n  Course selection is a crucial activity for students as it directly impacts\ntheir workload and performance. It is also time-consuming, prone to\nsubjectivity, and often carried out based on incomplete information. This task\ncan, nevertheless, be assisted with computational tools, for instance, by\npredicting performance based on historical data. We investigate the effects of\nshowing grade predictions to students through an interactive visualization\ntool. A qualitative study suggests that in the presence of predictions,\nstudents may focus too much on maximizing their performance, to the detriment\nof other factors such as the workload. A follow-up quantitative study explored\nwhether these effects are mitigated by changing how predictions are conveyed.\nOur observations suggest the presence of a framing effect that induces students\nto put more effort into course selection when faced with more specific\npredictions. We discuss these and other findings and outline considerations for\ndesigning better data-driven course selection tools.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.12645,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"FoamFactor: Hydrogel-Foam Composite with Tunable Stiffness and\n  Compressibility\n\n  This paper presents FoamFactor, a novel material with tunable stiffness and\ncompressibility between hydration states, and a tailored pipeline to design and\nfabricate artifacts consisting of it. This technique compounds hydrogel with\nopen-cell foams via additive manufacturing to produce a water-responsive\ncomposite material. Enabled by the large volumetric changes of hydrogel\ndispersions, the material is soft and compressible when dehydrated and becomes\nstiffer and rather incompressible when hydrated. Leveraging this material\nproperty transition, we explore its design space in various aspects pertaining\nto the transition of hydration states, including multi-functional shoes,\namphibious cars, mechanical transmission systems, and self-deploying robotic\ngrippers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.052,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Virtual Reality Sickness Mitigation Methods: A Comparative Study in a\n  Racing Game\n\n  Using virtual reality (VR) head-mounted displays (HMDs) can induce VR\nsickness. VR sickness can cause strong discomfort, decrease users' presence and\nenjoyment, especially in games, shorten the duration of the VR experience, and\ncan even pose health risks. Previous research has explored different VR\nsickness mitigation methods by adding visual effects or elements. Field of View\n(FOV) reduction, Depth of Field (DOF) blurring, and adding a rest frame into\nthe virtual environment are examples of such methods. Although useful in some\ncases, they might result in information loss. This research is the first to\ncompare VR sickness, presence, workload to complete a search task, and\ninformation loss of these three VR sickness mitigation methods in a racing game\nwith two levels of control. To do this, we conducted a mixed factorial user\nstudy (N = 32) with degree of control as the between-subjects factor and the VR\nsickness mitigation techniques as the within-subjects factor. Participants were\nrequired to find targets with three difficulty levels while steering or not\nsteering a car in a virtual environment. Our results show that there are no\nsignificant differences in VR sickness, presence and workload among these\ntechniques under two levels of control in our VR racing game. We also found\nthat changing FOV dynamically or using DOF blur effects would result in\ninformation loss while adding a target reticule as a rest frame would not.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00741,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000121858,
      "text":"Deep Colormap Extraction from Visualizations\n\n  This work presents a new approach based on deep learning to automatically\nextract colormaps from visualizations. After summarizing colors in an input\nvisualization image as a Lab color histogram, we pass the histogram to a\npre-trained deep neural network, which learns to predict the colormap that\nproduces the visualization. To train the network, we create a new dataset of\n64K visualizations that cover a wide variety of data distributions, chart\ntypes, and colormaps. The network adopts an atrous spatial pyramid pooling\nmodule to capture color features at multiple scales in the input color\nhistograms. We then classify the predicted colormap as discrete or continuous\nand refine the predicted colormap based on its color histogram. Quantitative\ncomparisons to existing methods show the superior performance of our approach\non both synthetic and real-world visualizations. We further demonstrate the\nutility of our method with two use cases,i.e., color transfer and color\nremapping.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08324,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Classroom Technology Deployment Matrix: A Planning, Monitoring,\n  Evaluating and Reporting Tool\n\n  We present the Classroom Technology Deployment Matrix (CTDM), a tool for\nhigh-level Planning, Monitoring, Evaluating and Reporting of classroom\ndeployments of educational technologies, enabling researchers, teachers and\nschools to work together for successful deployments. The tool is de-rived from\na review of literature on technology adaptation (at the individual, process and\norganisation level), concluding that Normalization Process Theory, which seeks\nto explain the social processes that lead to the routine embedding of\ninnovative technology in an existing system, would a suitable foundation for\ndeveloping this matrix. This can be leveraged in the specific context of the\nclassroom, specifically including the Normal Desired State of teachers. We\nexplore this classroom context, and the developed CTDM, through look-ing at two\nseparate deployments (different schools and teachers) of the same technology\n(Collocated Collaborative Writing), observing how lessons learned from the\nfirst changed our approach to the second. The descriptive and an-alytical value\nof the tool is then demonstrated through map-ping these observation to the\nmatrix and can be applied to future deployments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.171,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"What Do We See: An Investigation Into the Representation of Disability\n  in Video Games\n\n  There has been a large body of research focused on the representation of\ngender in video games. Disproportionately, there has been very little research\nin respect to the representation of disability. This research was aimed at\nexamining the representation of disabled characters through a method of content\nanalysis of trailers combined with a survey of video gamers. The overall\nresults showed that disabled characters were under-represented in videogames\ntrailers, and respondents to the survey viewed disabled characters as the least\nrepresented group. Both methods of research concluded that the representation\nof disabled characters was low. Additionally, the characters represented were\npredominantly secondary, non-playable characters not primary. However, the\nresearch found that the defined character type was a mixture of protagonists\nand antagonists, bucking the standard view of disabled characters in video\ngames.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.14956,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Dark Patterns in the Interaction with Cookie Banners\n\n  Dark patterns are interface designs that nudge users towards behavior that is\nagainst their best interests. Since humans are often not even aware that they\nare influenced by these malicious patterns, research has to identify ways to\nprotect web users against them. One approach to this is the automatic detection\nof dark patterns which enables the development of tools that are able to\nprotect users by proactively warning them in cases where they face a dark\npattern. In this paper, we present ongoing work in the direction of automatic\ndetection of dark patterns, and outline an example to detect malicious patterns\nwithin the domain of cookie banners.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.14792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000146363,
      "text":"Using Eye-tracking Data to Predict Situation Awareness in Real Time\n  during Takeover Transitions in Conditionally Automated Driving\n\n  Situation awareness (SA) is critical to improving takeover performance during\nthe transition period from automated driving to manual driving. Although many\nstudies measured SA during or after the driving task, few studies have\nattempted to predict SA in real time in automated driving. In this work, we\npropose to predict SA during the takeover transition period in conditionally\nautomated driving using eye-tracking and self-reported data. First, a tree\nensemble machine learning model, named LightGBM (Light Gradient Boosting\nMachine), was used to predict SA. Second, in order to understand what factors\ninfluenced SA and how, SHAP (SHapley Additive exPlanations) values of\nindividual predictor variables in the LightGBM model were calculated. These\nSHAP values explained the prediction model by identifying the most important\nfactors and their effects on SA, which further improved the model performance\nof LightGBM through feature selection. We standardized SA between 0 and 1 by\naggregating three performance measures (i.e., placement, distance, and speed\nestimation of vehicles with regard to the ego-vehicle) of SA in recreating\nsimulated driving scenarios, after 33 participants viewed 32 videos with six\nlengths between 1 and 20 s. Using only eye-tracking data, our proposed model\noutperformed other selected machine learning models, having a root-mean-squared\nerror (RMSE) of 0.121, a mean absolute error (MAE) of 0.096, and a 0.719\ncorrelation coefficient between the predicted SA and the ground truth. The code\nis available at https:\/\/github.com\/refengchou\/Situation-awareness-prediction.\nOur proposed model provided important implications on how to monitor and\npredict SA in real time in automated driving using eye-tracking data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.04924,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Data Management for Building Information Modelling in a Real-Time\n  Adaptive City Platform\n\n  Legacy Building Information Modelling (BIM) systems are not designed to\nprocess the high-volume, high-velocity data emitted by in-building\nInternet-of-Things (IoT) sensors. Historical lack of consideration for the\nreal-time nature of such data means that outputs from such BIM systems\ntypically lack the timeliness necessary for enacting decisions as a result of\npatterns emerging in the sensor data. Similarly, as sensors are increasingly\ndeployed in buildings, antiquated Building Management Systems (BMSs) struggle\nto maintain functionality as interoperability challenges increase. In\ncombination these motivate the need to fill an important gap in smart buildings\nresearch, to enable faster adoption of these technologies, by combining BIM,\nBMS and sensor data. This paper describes the data architecture of the Adaptive\nCity Platform, designed to address these combined requirements by enabling\nintegrated BIM and real-time sensor data analysis across both time and space.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05984,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0001023213,
      "text":"Mixed Reality Interaction Techniques\n\n  This chapter gives an overview of interaction techniques for mixed reality\nincluding augmented and virtual reality (AR\/VR). Various modalities for input\nand output are discussed. Specifically, techniques for tangible and\nsurface-based interaction, gesture-based, pen-based, gaze-based, keyboard and\nmouse-based, as well as haptic interaction are discussed. Furthermore, the\ncombination of multiple modalities in multisensory and multimodal interaction,\nas well as interaction using multiple physical or virtual displays, are\npresented. Finally, interaction with intelligent virtual agents is considered.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.11104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"RLTIR: Activity-based Interactive Person Identification based on\n  Reinforcement Learning Tree\n\n  Identity recognition plays an important role in ensuring security in our\ndaily life. Biometric-based (especially activity-based) approaches are favored\ndue to their fidelity, universality, and resilience. However, most existing\nmachine learning-based approaches rely on a traditional workflow where models\nare usually trained once for all, with limited involvement from end-users in\nthe process and neglecting the dynamic nature of the learning process. This\nmakes the models static and can not be updated in time, which usually leads to\nhigh false positive or false negative. Thus, in practice, an expert is desired\nto assist with providing high-quality observations and interpretation of model\noutputs. It is expedient to combine both advantages of human experts and the\ncomputational capability of computers to create a tight-coupling incremental\nlearning process for better performance. In this study, we develop RLTIR, an\ninteractive identity recognition approach based on reinforcement learning, to\nadjust the identification model by human guidance. We first build a base\ntree-structured identity recognition model. And an expert is introduced in the\nmodel for giving feedback upon model outputs. Then, the model is updated\naccording to strategies that are automatically learned under a designated\nreinforcement learning framework. To the best of our knowledge, it is the very\nfirst attempt to combine human expert knowledge with model learning in the area\nof identity recognition. The experimental results show that the reinforced\ninteractive identity recognition framework outperforms baseline methods with\nregard to recognition accuracy and robustness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01078,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Um Estudo sobre Atividades Participativas para Solu\\c{c}\\~oes IoT para o\n  Home care de Pessoas Idosas\n\n  Population aging in Brazil and in the world occurs at the same time of\nadvances and evolutions in technology. Thus, opportunities for new solutions\narise for the elderly, such as innovations in Home Care. With the Internet of\nThings, it is possible to improve the elderly autonomy, safety and quality of\nlife. However, the design of IoT solutions for elderly Home Care poses new\nchallenges. In this context, this technical report aims to detail activities\ndeveloped as a case study to evaluate the IoT-PMHCS Method, which was developed\nin the context of the Master's program in Computer Science at UNIFACCAMP,\nBrazil. This report includes the planning and results of interviews,\nparticipatory workshops, validations, simulation of solutions, among other\nactivities. This document reports the practical experience of applying the\nIoT-PMHCS Method.\n  --\n  O envelhecimento populacional no Brasil e no mundo ocorre ao mesmo tempo que\nos avan\\c{c}os e evolu\\c{c}\\~oes na tecnologia. Desta forma, surgem\noportunidades de novas solu\\c{c}\\~oes para o p\\'ublico idoso, tais como\ninova\\c{c}\\~oes em Home Care. Com a Internet das Coisas \\'e poss\\'ivel promover\nmaior autonomia, seguran\\c{c}a e qualidade de vida aos idosos. Entretanto, o\ndesign de solu\\c{c}\\~oes de IoT para Home Care de pessoas idosas traz novos\ndesafios. Diante disto, este relat\\'orio t\\'ecnico tem o objetivo de detalhar\natividades desenvolvidas como estudo de caso para avalia\\c{c}\\~ao do M\\'etodo\nIoT-PMHCS, desenvolvido no contexto do programa de Mestrado em Ci\\^encia da\nComputa\\c{c}\\~ao da UNIFACCAMP, Brasil. O relat\\'orio inclui o planejamento e\nresultados de entrevistas, workshops participativos, pesquisas de\nvalida\\c{c}\\~ao, simula\\c{c}\\~ao de solu\\c{c}\\~oes, dentre outras atividades.\nEste documento relata a experi\\^encia pr\\'atica da aplica\\c{c}\\~ao do M\\'etodo\nIoT-PMHCS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.02851,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000206629,
      "text":"Visual Motion Imagery Classification with Deep Neural Network based on\n  Functional Connectivity\n\n  Brain-computer interfaces (BCIs) use brain signals such as\nelectroencephalography to reflect user intention and enable two-way\ncommunication between computers and users. BCI technology has recently received\nmuch attention in healthcare applications, such as neurorehabilitation and\ndiagnosis. BCI applications can also control external devices using only brain\nactivity, which can help people with physical or mental disabilities,\nespecially those suffering from neurological and neuromuscular diseases such as\nstroke and amyotrophic lateral sclerosis. Motor imagery (MI) has been widely\nused for BCI-based device control, but we adopted intuitive visual motion\nimagery to overcome the weakness of MI. In this study, we developed a\nthree-dimensional (3D) BCI training platform to induce users to imagine\nupper-limb movements used in real-life activities (picking up a cell phone,\npouring water, opening a door, and eating food). We collected intuitive visual\nmotion imagery data and proposed a deep learning network based on functional\nconnectivity as a mind-reading technique. As a result, the proposed network\nrecorded a high classification performance on average (71.05%). Furthermore, we\napplied the leave-one-subject-out approach to confirm the possibility of\nimprovements in subject-independent classification performance. This study will\ncontribute to the development of BCI-based healthcare applications for\nrehabilitation, such as robotic arms and wheelchairs, or assist daily life.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.03898,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Reducing cybersickness in 360-degree virtual reality\n\n  Despite the technological advancements in Virtual Reality (VR), users are\nconstantly combating feelings of nausea and disorientation, the so called\ncybersickness. Cybersickness symptoms cause severe discomfort and hinder the\nimmersive VR experience. Here we investigated cybersickness in 360-degree\nhead-mounted display VR. In traditional 360-degree VR experiences,\ntranslational movement in the real world is not reflected in the virtual world,\nand therefore self-motion information is not corroborated by matching visual\nand vestibular cues, which may trigger symptoms of cybersickness. We have\nevaluated whether a new Artificial Intelligence (AI) software designed to\nsupplement the 360-degree VR experience with artificial 6-degrees-of-freedom\nmotion may reduce cybersickness. Explicit (simulator sickness questionnaire and\nfast motion sickness rating) and implicit (heart rate) measurements were used\nto evaluate cybersickness symptoms during and after 360-degree VR exposure.\nSimulator sickness scores showed a significant reduction in feelings of nausea\nduring the AI supplemented 6-degrees-of-freedom motion VR compared to\ntraditional 360-degree VR. However, 6-degrees-of-freedom motion VR did not\nreduce oculomotor or disorientation measures of sickness. No changes have been\nobserved in fast motion sickness and heart rate measures. Improving the\ncongruency between visual and vestibular cues in 360-degree VR, as provided by\nthe AI supplemented 6-degrees-of-freedom motion system considered, is essential\nto provide a more engaging, immersive and safe VR, which is critical for\neducational, cultural and entertainment applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06379,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Learning to Trust: Understanding Editorial Authority and Trust in\n  Recommender Systems for Education\n\n  Trust in a recommendation system (RS) is often algorithmically incorporated\nusing implicit or explicit feedback of user-perceived trustworthy social\nneighbors, and evaluated using user-reported trustworthiness of recommended\nitems. However, real-life recommendation settings can feature group disparities\nin trust, power, and prerogatives. Our study examines a complementary view of\ntrust which relies on the editorial power relationships and attitudes of all\nstakeholders in the RS application domain. We devise a simple, first-principles\nmetric of editorial authority, i.e., user preferences for recommendation\nsourcing, veto power, and incorporating user feedback, such that one RS user\ngroup confers trust upon another by ceding or assigning editorial authority. In\na mixed-methods study at Virginia Tech, we surveyed faculty, teaching\nassistants, and students about their preferences of editorial authority, and\nhypothesis-tested its relationship with trust in algorithms for a hypothetical\n`Suggested Readings' RS. We discover that higher RS editorial authority\nassigned to students is linked to the relative trust the course staff allocates\nto RS algorithm and students. We also observe that course staff favors higher\ncontrol for the RS algorithm in sourcing and updating the recommendations\nlong-term. Using content analysis, we discuss frequent staff-recommended\nstudent editorial roles and highlight their frequent rationales, such as\nperceived expertise, scaling the learning environment, professional curriculum\nneeds, and learner disengagement. We argue that our analyses highlight critical\nuser preferences to help detect editorial power asymmetry and identify RS\nuse-cases for supporting teaching and research\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.16399,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"The Development and Validation of the Technology-Supported Reflection\n  Inventory\n\n  Reflection is an often addressed design goal in Human-Computer Interaction\n(HCI) research. An increasing number of artefacts for reflection have been\ndeveloped in recent years. However, evaluating if and how an interactive\ntechnology helps a user reflect is still complex. This makes it difficult to\ncompare artefacts (or prototypes) for reflection, impeding future design\nefforts. To address this issue, we developed the \\emph{Technology-Supported\nReflection Inventory} (TSRI), which is a scale that evaluates how effectively a\nsystem supports reflection. We first created a list of possible scale items\nbased on past work in defining reflection. The items were then reviewed by\nexperts. Next, we performed exploratory factor analysis to reduce the scale to\nits final length of nine items. Subsequently, we confirmed test-retest validity\nof our instrument, as well as its construct validity. The TSRI enables\nresearchers and practitioners to compare prototypes designed to support\nreflection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.07757,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Model-based Task Analysis and Large-scale Video-based Remote Evaluation\n  Methods for Extended Reality Research\n\n  In this paper, we introduce two remote extended reality (XR) research methods\nthat can overcome the limitations of lab-based controlled experiments,\nespecially during the COVID-19 pandemic: (1) a predictive model-based task\nanalysis and (2) a large-scale video-based remote evaluation. We used a box\nstacking task including three interaction modalities - two multimodal\ngaze-based interactions as well as a unimodal hand-based interaction which is\ndefined as our baseline. For the first evaluation, a GOMS-based task analysis\nwas performed by analyzing the tasks to understand human behaviors in XR and\npredict task execution times. For the second evaluation, an online survey was\nadministered using a series of the first-person point of view videos where a\nuser performs the corresponding task with three interaction modalities. A total\nof 118 participants were asked to compare the interaction modes based on their\njudgment. Two standard questionnaires were used to measure perceived workload\nand the usability of the modalities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06181,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"\"This Browser is Lightning Fast\": The Effects of Message Content on\n  Perceived Performance\n\n  With technical performance being similar for various web browsers, improving\nuser perceived performance is integral to optimizing browser quality. We\ninvestigated the importance of priming, which has a well-documented ability to\naffect people's beliefs, on users' perceptions of web browser performance. We\nstudied 1495 participants who read either an article about performance\nimprovements to Mozilla Firefox, an article about user interface updates to\nFirefox, or an article about self-driving cars, and then watched video clips of\nbrowser tasks. As the priming effect would suggest, we found that reading\narticles about Firefox increased participants' perceived performance of Firefox\nover the most widely used web browser, Google Chrome. In addition, we found\nthat article content mattered, as the article about performance improvements\nled to higher performance ratings than the article about UI updates. Our\nfindings demonstrate how perceived performance can be improved without making\ntechnical improvements and that designers and developers must consider a wider\npicture when trying to improve user attitudes about technology.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15462,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Situated Case Studies for a Human-Centered Design of Explanation User\n  Interfaces\n\n  Researchers and practitioners increasingly consider a human-centered\nperspective in the design of machine learning-based applications, especially in\nthe context of Explainable Artificial Intelligence (XAI). However, clear\nmethodological guidance in this context is still missing because each new\nsituation seems to require a new setup, which also creates different\nmethodological challenges. Existing case study collections in XAI inspired us;\ntherefore, we propose a similar collection of case studies for human-centered\nXAI that can provide methodological guidance or inspiration for others. We want\nto showcase our idea in this workshop by describing three case studies from our\nresearch. These case studies are selected to highlight how apparently small\ndifferences require a different set of methods and considerations. With this\nworkshop contribution, we would like to engage in a discussion on how such a\ncollection of case studies can provide a methodological guidance and critical\nreflection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06571,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Interface to Query and Visualise Definitions from a Knowledge Base\n\n  The semantic linked data model is at the core of the Web due to its ability\nto model real world entities, connect them via relationships and provide\ncontext, which could help to transform data into information and information\ninto knowledge. Linked Data, in the form of ontologies and knowledge graphs\ncould be stored locally or could be made available to everyone online. For\nexample, the DBpedia knowledge base, which provides global and unified access\nto knowledge graphs is open access. However, both access and usage of Linked\nData require individuals to have expert knowledge in the field of the Semantic\nWeb. Many of the existing solutions that are powered by Linked Data are\ndeveloped for specific use cases such as building and exploring ontologies\nvisually and are aimed at researchers with knowledge of semantic technology.\nThe solutions that are aimed at non-experts are generic and, in most cases,\ninformation visualisation is not available. Instead, information is presented\nin textual format, which does not ease cognitive processes such as\ncomprehension and could lead to problems such as information overload. In this\npaper, we present a web application with a user interface (UI), which combines\nfeatures from applications for both experts and non-experts. The UI allows\nindividuals with no previous knowledge of the Semantic Web to query the DBpedia\nknowledge base for definitions of a specific word and to view a graphical\nvisualisation of the query results (the search keyword itself and concepts\nrelated to it).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0534,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000041392,
      "text":"Speaking of Trust -- Speech as a Measure of Trust\n\n  Since trust measures in human-robot interaction are often subjective or not\npossible to implement real-time, we propose to use speech cues (on what, when\nand how the user talks) as an objective real-time measure of trust. This could\nbe implemented in the robot to calibrate towards appropriate trust. However, we\nwould like to open the discussion on how to deal with the ethical implications\nsurrounding this trust measure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.13889,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Driver State and Behavior Detection Through Smart Wearables\n\n  Integrating driver, in-cabin, and outside environment's contextual cues into\nthe vehicle's decision making is the centerpiece of semi-automated vehicle\nsafety. Multiple systems have been developed for providing context to the\nvehicle, which often rely on video streams capturing drivers' physical and\nenvironmental states. While video streams are a rich source of information,\ntheir ability in providing context can be challenging in certain situations,\nsuch as low illuminance environments (e.g., night driving), and they are highly\nprivacy-intrusive. In this study, we leverage passive sensing through\nsmartwatches for classifying elements of driving context. Specifically, through\nusing the data collected from 15 participants in a naturalistic driving study,\nand by using multiple machine learning algorithms such as random forest, we\nclassify driver's activities (e.g., using phone and eating), outside events\n(e.g., passing intersection and changing lane), and outside road attributes\n(e.g., driving in a city versus a highway) with an average F1 score of 94.55,\n98.27, and 97.86 % respectively, through 10-fold cross-validation. Our results\nshow the applicability of multimodal data retrieved through smart wearable\ndevices in providing context in real-world driving scenarios and pave the way\nfor a better shared autonomy and privacy-aware driving data-collection,\nanalysis, and feedback for future autonomous vehicles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06603,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Mitigating the Effects of Reading Interruptions by Providing Reviews and\n  Previews\n\n  As reading on mobile devices is becoming more ubiquitous, content is consumed\nin shorter intervals and is punctuated by frequent interruptions. In this work,\nwe explore the best way to mitigate the effects of reading interruptions on\nlonger text passages. Our hypothesis is that short summaries of either\npreviously read content (reviews) or upcoming content (previews) will help the\nreader re-engage with the reading task. Our target use case is for students who\nstudy using electronic textbooks and who are frequently mobile. We present a\nseries of pilot studies that examine the benefits of different types of\nsummaries and their locations, with respect to variations in text content and\nparticipant cohorts. We find that users prefer reviews after an interruption,\nbut that previews shown after interruptions have a larger positive influence on\ncomprehension. Our work is a first step towards smart reading applications that\nproactively provide text summaries to mitigate interruptions on the go.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.05979,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Investigating Opportunities to Support Kids' Agency and Well-being: A\n  Review of Kids' Wearables\n\n  Wearable devices hold great potential for promoting children's health and\nwell-being. However, research on kids' wearables is sparse and often focuses on\ntheir use in the context of parental surveillance. To gain insight into the\ncurrent landscape of kids' wearables, we surveyed 47 wearable devices marketed\nfor children. We collected rich data on the functionality of these devices and\nassessed how different features satisfy parents' information needs, and\nidentified opportunities for wearables to support children's needs and\ninterests. We found that many kids' wearables are technologically sophisticated\ndevices that focus on parents' ability to communicate with their children and\nkeep them safe, as well as encourage physical activity and nurture good habits.\nWe discuss how our findings could inform the design of wearables that serve as\nmore than monitoring devices, and instead support children and parents as equal\nstakeholders, providing implications for kids' agency, long-term development,\nand overall well-being. Finally, we identify future research efforts related to\ndesigning for kids' self-tracking and collaborative tracking with parents.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.13823,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Investigating Perceptions of Social Intelligence in Simulated\n  Human-Chatbot Interactions\n\n  With the ongoing penetration of conversational user interfaces, a better\nunderstanding of social and emotional characteristic inherent to dialogue is\nrequired. Chatbots in particular face the challenge of conveying human-like\nbehaviour while being restricted to one channel of interaction, i.e., text. The\ngoal of the presented work is thus to investigate whether characteristics of\nsocial intelligence embedded in human-chatbot interactions are perceivable by\nhuman interlocutors and if yes, whether such influences the experienced\ninteraction quality. Focusing on the social intelligence dimensions\nAuthenticity, Clarity and Empathy, we first used a questionnaire survey\nevaluating the level of perception in text utterances, and then conducted a\nWizard of Oz study to investigate the effects of these utterances in a more\ninteractive setting. Results show that people have great difficulties\nperceiving elements of social intelligence in text. While on the one hand they\nfind anthropomorphic behaviour pleasant and positive for the naturalness of a\ndialogue, they may also perceive it as frightening and unsuitable when\nexpressed by an artificial agent in the wrong way or at the wrong time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.14699,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Why should we care about register? Reflections on chatbot language\n  design\n\n  This position paper discusses the relevance of register as a theoretical\nframework for chatbot language design. We present the concept of register and\ndiscuss how using register-specific language influence the user's perceptions\nof the interaction with chatbots. Additionally, we point several research\nopportunities that are important to pursue to establish register as a\nfoundation for advancing chatbot's communication skills.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12182,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000232458,
      "text":"Comparing Hand Gestures and a Gamepad Interface for Locomotion in\n  Virtual Environments\n\n  Hand gesture is a new and promising interface for locomotion in virtual\nenvironments. While several previous studies have proposed different hand\ngestures for virtual locomotion, little is known about their differences in\nterms of performance and user preference in virtual locomotion tasks. In the\npresent paper, we presented three different hand gesture interfaces and their\nalgorithms for locomotion, which are called the Finger Distance gesture, the\nFinger Number gesture and the Finger Tapping gesture. These gestures were\ninspired by previous studies of gesture-based locomotion interfaces and are\ntypical gestures that people are familiar with in their daily lives.\nImplementing these hand gesture interfaces in the present study enabled us to\nsystematically compare the differences between these gestures. In addition, to\ncompare the usability of these gestures to locomotion interfaces using\ngamepads, we also designed and implemented a gamepad interface based on the\nXbox One controller. We conducted empirical studies to compare these four\ninterfaces through two virtual locomotion tasks. A desktop setup was used\ninstead of sharing a head-mounted display among participants due to the concern\nof the Covid-19 situation. Through these tasks, we assessed the performance and\nuser preference of these interfaces on speed control and waypoints navigation.\nResults showed that user preference and performance of the Finger Distance\ngesture were close to that of the gamepad interface. The Finger Number gesture\nalso had close performance and user preference to that of the Finger Distance\ngesture. Our study demonstrates that the Finger Distance gesture and the Finger\nNumber gesture are very promising interfaces for virtual locomotion. We also\ndiscuss that the Finger Tapping gesture needs further improvements before it\ncan be used for virtual walking.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11043,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"Agent vs. Avatar: Comparing Embodied Conversational Agents Concerning\n  Characteristics of the Uncanny Valley\n\n  Visual appearance is an important aspect influencing the perception and\nconsequent acceptance of Embodied Conversational Agents (ECA). To this end, the\nUncanny Valley theory contradicts the common assumption that increased\nhumanization of characters leads to better acceptance. Rather, it shows that\nanthropomorphic behavior may trigger feelings of eeriness and rejection in\npeople. The work presented in this paper explores whether four different\nautonomous ECAs, specifically build for a European research project, are\naffected by this effect, and how they compare to two slightly more\nrealistically looking human-controlled, i.e. face-tracked, ECAs with respect to\nperceived humanness, eeriness, and attractiveness. Short videos of the ECAs in\ncombination with a validated questionnaire were used to investigate potential\ndifferences. Results support existing theories highlighting that increased\nperceived humanness correlates with increased perceived eeriness. Furthermore,\nit was found, that neither the gender of survey participants, their age, nor\nthe sex of the ECA influences this effect, and that female ECAs are perceived\nto be significantly more attractive than their male counterparts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08123,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"A context-aware pedestrian trajectory prediction framework for automated\n  vehicles\n\n  With the unprecedented shift towards automated urban environments in recent\nyears, a new paradigm is required to study pedestrian behaviour. Studying\npedestrian behaviour in futuristic scenarios requires modern data sources that\nconsider both the Automated Vehicle (AV) and pedestrian perspectives. Current\nopen datasets on AVs predominantly fail to account for the latter, as they do\nnot include an adequate number of events and associated details that involve\npedestrian and vehicle interactions. To address this issue, we propose using\nVirtual Reality (VR) data as a complementary resource to current datasets,\nwhich can be designed to measure pedestrian behaviour under specific\nconditions. In this research, we focus on the context-aware pedestrian\ntrajectory prediction framework for automated vehicles at mid-block\nunsignalized crossings. For this purpose, we develop a novel multi-input\nnetwork of Long Short-Term Memory (LSTM) and fully connected dense layers. In\naddition to past trajectories, the proposed framework incorporates pedestrian\nhead orientations and distance to the upcoming vehicles as sequential input\ndata. By merging the sequential data with contextual information of the\nenvironment, we train a model to predict the future pedestrian trajectory. Our\nresults show that the prediction error and overfitting to the training data are\nreduced by considering contextual information in the model. To analyze the\napplication of the methods to real AV data, the proposed framework is trained\nand applied to pedestrian trajectory extracted from an open-access video\ndataset. Finally, by implementing a game theory-based model interpretability\nmethod, we provide detailed insights and propose recommendations to improve the\ncurrent automated vehicle sensing systems from a pedestrian-oriented point of\nview.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11806,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"Novices' Learning Barriers When Using Code Examples in Open-Ended\n  Programming\n\n  Open-ended programming increases students' motivation by allowing them to\nsolve authentic problems and connect programming to their own interests.\nHowever, such open-ended projects are also challenging, as they often encourage\nstudents to explore new programming features and attempt tasks that they have\nnot learned before. Code examples are effective learning materials for students\nand are well-suited to supporting open-ended programming. However, there is\nlittle work to understand how novices learn with examples during open-ended\nprogramming, and few real-world deployments of such tools. In this paper, we\nexplore novices' learning barriers when interacting with code examples during\nopen-ended programming. We deployed Example Helper, a tool that offers\ngalleries of code examples to search and use, with 44 novice students in an\nintroductory programming classroom, working on an open-ended project in Snap.\nWe found three high-level barriers that novices encountered when using\nexamples: decision, search and integration barriers. We discuss how these\nbarriers arise and design opportunities to address them.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.10438,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000120203,
      "text":"Unification of computer reality\n\n  The work attempts to unify the conceptual model of the user's virtual\ncomputer environment, with the aim of combining the local environments of\noperating systems and the global Internet environment into a single virtual\nenvironment built on general principles. To solve this problem, it is proposed\nto unify the conceptual basis of these environments. The existing conceptual\nbasis of operating systems, built on the \"desktop\" metaphor, contains redundant\nconcepts associated with computer architecture. The use of the spatial\nconceptual basis \"object - place\" with the concepts of \"domain\", \"site\", and\n\"data object\" allows to completely virtualize the user environment, separating\nit from the hardware concepts. The virtual concept \"domain\" is becoming a\nuniversal way of structuring the user's space. The introduction of this concept\nto describe the environments of operating systems provides at the mental level\nthe integration of the structures of the local and global space. The use of the\nconcept of \"personal domain\" will allow replacing the concept of \"personal\ncomputer\" in the mind of the user. The virtual concept of \"site\" as an\nenvironment for activities and data storage will allow abandoning such concepts\nas \"application\" (program), or \"memory device\". The site in the mind of the\nuser is a virtual environment that includes both places for storing data\nobjects and places for working with them. The introduction of the concept\n\"site\" into the structure of operating systems environments and the concept of\n\"data site\" into the structure of the global network integrates the structure\nof the global and local space in the user's mind. The introduction of the\nconcept of \"portal\" as a means of integrating information necessary for\ninteraction, allows ensuring the methodological homogeneity of the user's work\nin a single virtual environment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.086,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Remote smartphone-based speech collection: acceptance and barriers in\n  individuals with major depressive disorder\n\n  The ease of in-the-wild speech recording using smartphones has sparked\nconsiderable interest in the combined application of speech, remote measurement\ntechnology (RMT) and advanced analytics as a research and healthcare tool. For\nthis to be realised, the acceptability of remote speech collection to the user\nmust be established, in addition to feasibility from an analytical perspective.\nTo understand the acceptance, facilitators, and barriers of smartphone-based\nspeech recording, we invited 384 individuals with major depressive disorder\n(MDD) from the Remote Assessment of Disease and Relapse - Central Nervous\nSystem (RADAR-CNS) research programme in Spain and the UK to complete a survey\non their experiences recording their speech. In this analysis, we demonstrate\nthat study participants were more comfortable completing a scripted speech task\nthan a free speech task. For both speech tasks, we found depression severity\nand country to be significant predictors of comfort. Not seeing smartphone\nnotifications of the scheduled speech tasks, low mood and forgetfulness were\nthe most commonly reported obstacles to providing speech recordings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06221,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Questionnaires and Qualitative Feedback Methods to Measure User\n  Experience in Mixed Reality\n\n  Evaluating the user experience of a software system is an essential final\nstep of every research. Several concepts such as flow, affective state,\npresences, or immersion exist to measure user experience. Typical measurement\ntechniques analyze physiological data, gameplay data, and questionnaires.\nQualitative feedback methods are another approach to collect detailed user\ninsights. In this position paper, we will discuss how we used questionnaires\nand qualitative feedback methods in previous mixed reality work to measure user\nexperience. We will present several measurement examples, discuss their current\nlimitations, and provide guideline propositions to support comparable mixed\nreality user experience research in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.03795,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Experiences with User Studies in Augmented Reality\n\n  The research field of augmented reality (AR) is of increasing popularity, as\nseen, among others, in several recently published surveys. To produce further\nadvancements in AR, it is not only necessary to create new systems or\napplications, but also to evaluate them. One important aspect in regards to the\nevaluation is the general understanding of how users experience a given AR\napplication, which can also be seen by the increased number of papers focusing\non this topic that were published in the last years. With the steadily growing\nunderstanding and development of AR in general, it is only a matter of time\nuntil AR devices make the leap into the consumer market where such an in-depth\nuser understanding is even more essential. Thus, a better understanding of\nfactors that could influence the design and results of user experience studies\ncan help us to make them more robust and dependable in the future.\n  In this position paper, we describe three challenges which researchers face\nwhile designing and conducting AR users studies. We encountered these\nchallenges in our past and current research, including papers that focus on\nperceptual studies of visualizations, interaction studies, and studies\nexploring the use of AR applications and their design spaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07598,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Can Artificial Intelligence Make Art?\n\n  In two experiments (total N=693) we explored whether people are willing to\nconsider paintings made by AI-driven robots as art, and robots as artists.\nAcross the two experiments, we manipulated three factors: (i) agent type\n(AI-driven robot v. human agent), (ii) behavior type (intentional creation of a\npainting v. accidental creation), and (iii) object type (abstract v.\nrepresentational painting). We found that people judge robot paintings and\nhuman painting as art to roughly the same extent. However, people are much less\nwilling to consider robots as artists than humans, which is partially explained\nby the fact that they are less disposed to attribute artistic intentions to\nrobots.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06916,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Autonomous Vehicles Drive into Shared Spaces: eHMI Design Concept\n  Focusing on Vulnerable Road Users\n\n  In comparison to conventional traffic designs, shared spaces promote a more\npleasant urban environment with slower motorized movement, smoother traffic,\nand less congestion. In the foreseeable future, shared spaces will be populated\nwith a mixture of autonomous vehicles (AVs) and vulnerable road users (VRUs)\nlike pedestrians and cyclists. However, a driver-less AV lacks a way to\ncommunicate with the VRUs when they have to reach an agreement of a\nnegotiation, which brings new challenges to the safety and smoothness of the\ntraffic. To find a feasible solution to integrating AVs seamlessly into\nshared-space traffic, we first identified the possible issues that the\nshared-space designs have not considered for the role of AVs. Then an online\nquestionnaire was used to ask participants about how they would like a driver\nof the manually driving vehicle to communicate with VRUs in a shared space. We\nfound that when the driver wanted to give some suggestions to the VRUs in a\nnegotiation, participants thought that the communications via the driver's body\nbehaviors were necessary. Besides, when the driver conveyed information about\nher\/his intentions and cautions to the VRUs, participants selected different\ncommunication methods with respect to their transport modes (as a driver,\npedestrian, or cyclist). These results suggest that novel eHMIs might be useful\nfor AV-VRU communication when the original drivers are not present. Hence, a\npotential eHMI design concept was proposed for different VRUs to meet their\nvarious expectations. In the end, we further discussed the effects of the eHMIs\non improving the sociality in shared spaces and the autonomous driving systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07724,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000082784,
      "text":"Design Patterns and Trade-Offs in Responsive Visualization for\n  Communication\n\n  Increased access to mobile devices motivates the need to design communicative\nvisualizations that are responsive to varying screen sizes. However, relatively\nlittle design guidance or tooling is currently available to authors. We\ncontribute a detailed characterization of responsive visualization strategies\nin communication-oriented visualizations, identifying 76 total strategies by\nanalyzing 378 pairs of large screen (LS) and small screen (SS) visualizations\nfrom online articles and reports. Our analysis distinguishes between the\nTargets of responsive visualization, referring to what elements of a design are\nchanged and Actions representing how targets are changed. We identify key\ntrade-offs related to authors' need to maintain graphical density, referring to\nthe amount of information per pixel, while also maintaining the \"message\" or\nintended takeaways for users of a visualization. We discuss implications of our\nfindings for future visualization tool design to support responsive\ntransformation of visualization designs, including requirements for automated\nrecommenders for communication-oriented responsive visualizations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11386,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000364582,
      "text":"Recording Reusable and Guided Analytics From Interaction Histories\n\n  The use of visual analytics tools has gained popularity in various domains,\nhelping users discover meaningful information from complex and large data sets.\nUsers often face difficulty in disseminating the knowledge discovered without\nclear recall of their exploration paths and analysis processes. We introduce a\nvisual analysis tool that allows analysts to record reusable and guided\nanalytics from their interaction logs. To capture the analysis process, we use\na decision tree whose node embeds visualizations and guide to define a visual\nanalysis task. The tool enables analysts to formalize analysis strategies,\nbuild best practices, and guide novices through systematic workflows.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.01991,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000146694,
      "text":"MeetDurian: A Gameful Mobile App to Prevent COVID-19 Infection\n\n  The COVID-19 problem has not gone away with the passing of the seasons. Even\nthough most countries have achieved remarkable results in fighting against\nepidemic diseases and preventing and controlling viruses, the general public is\nstill far from understanding the new crown virus and lacks imagination on its\ntransmission law. In this paper, we propose MeetDurian: a cross-platform mobile\napplication that exploits a location-based game to improve users' hygiene\nhabits and reduce virus dispersal. We present its main features, its\narchitecture, and its core technologies. Finally, we report a set of\nexperiments that prove the acceptability and usability of MeetDurian. An\nillustrative demo of the mobile app features is shown in the following video:\nhttps:\/\/youtu.be\/Vqg7nFDQuOU.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06536,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Lets Make A Story Measuring MR Child Engagement\n\n  We present the result of a pilot study measuring child engagement with the\nLets Make A Story system, a novel mixed reality, MR, collaborative storytelling\nsystem designed for grandparents and grandchildren. We compare our MR\nexperience against an equivalent paper story experience. The goal of our pilot\nwas to test the system with actual child users and assess the goodness of using\nmetrics of time, user generated story content and facial expression analysis as\nmetrics of child engagement. We find that multiple confounding variables make\nthese metrics problematic including attribution of engagement time, spontaneous\nnon-story related conversation and having the childs full forward face\ncontinuously in view during the story. We present our platform and experiences\nand our finding that the strongest metric was user comments in the\npost-experiential interview.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05849,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000392066,
      "text":"Orienting, Framing, Bridging, Magic, and Counseling: How Data Scientists\n  Navigate the Outer Loop of Client Collaborations in Industry and Academia\n\n  Data scientists often collaborate with clients to analyze data to meet a\nclient's needs. What does the end-to-end workflow of a data scientist's\ncollaboration with clients look like throughout the lifetime of a project? To\ninvestigate this question, we interviewed ten data scientists (5 female, 4\nmale, 1 non-binary) in diverse roles across industry and academia. We\ndiscovered that they work with clients in a six-stage outer-loop workflow,\nwhich involves 1) laying groundwork by building trust before a project begins,\n2) orienting to the constraints of the client's environment, 3) collaboratively\nframing the problem, 4) bridging the gap between data science and domain\nexpertise, 5) the inner loop of technical data analysis work, 6) counseling to\nhelp clients emotionally cope with analysis results. This novel outer-loop\nworkflow contributes to CSCW by expanding the notion of what collaboration\nmeans in data science beyond the widely-known inner-loop technical workflow\nstages of acquiring, cleaning, analyzing, modeling, and visualizing data. We\nconclude by discussing the implications of our findings for data science\neducation, parallels to design work, and unmet needs for tool development.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00727,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Surgical navigation systems based on augmented reality technologies\n\n  This study considers modern surgical navigation systems based on augmented\nreality technologies. Augmented reality glasses are used to construct holograms\nof the patient's organs from MRI and CT data, subsequently transmitted to the\nglasses. This, in addition to seeing the actual patient, the surgeon gains\nvisualization inside the patient's body (bones, soft tissues, blood vessels,\netc.). The solutions developed at Peter the Great St. Petersburg Polytechnic\nUniversity allow reducing the invasiveness of the procedure and preserving\nhealthy tissues. This also improves the navigation process, making it easier to\nestimate the location and size of the tumor to be removed. We describe the\napplication of developed systems to different types of surgical operations\n(removal of a malignant brain tumor, removal of a cyst of the cervical spine).\nWe consider the specifics of novel navigation systems designed for anesthesia,\nfor endoscopic operations. Furthermore, we discuss the construction of novel\nvisualization systems for ultrasound machines. Our findings indicate that the\ntechnologies proposed show potential for telemedicine.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0839,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"3D Displays: Their Evolution, Inherent Challenges & Future Perspectives\n\n  The popularity of 3D displays has risen drastically over the past few decades\nbut these displays are still merely a novelty compared to their true potential.\nThe development has mostly focused on Head Mounted Displays (HMD) development\nfor Virtual Reality and in general ignored non-HMD 3D displays. This is due to\nthe inherent difficulty in the creation of these displays and their\nimpracticability in general use due to cost, performance, and lack of\nmeaningful use cases. In fairness to the hardware manufacturers who have made\nstriking innovations in this field, there has been a dereliction of duty of\nsoftware developers and researchers in terms of developing software to best\nutilize these displays.\n  This paper will seek to identify what areas of future software development\ncould mitigate this dereliction. To achieve this goal, the paper will first\nexamine the current state of the art and perform a comparative analysis on\ndifferent types of 3D displays, from this analysis a clear researcher gap\nexists in terms of software development for Light field displays which are the\ncurrent state of the art of non-HMD-based 3D displays.\n  The paper will then outline six distinct areas where the context-awareness\nconcept will allow for non-HMD-based 3D displays in particular light field\ndisplays that can not only compete but surpass their HMD-based brethren for\nmany specific use cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.10614,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Human-AI Collaboration with Bandit Feedback\n\n  Human-machine complementarity is important when neither the algorithm nor the\nhuman yield dominant performance across all instances in a given domain. Most\nresearch on algorithmic decision-making solely centers on the algorithm's\nperformance, while recent work that explores human-machine collaboration has\nframed the decision-making problems as classification tasks. In this paper, we\nfirst propose and then develop a solution for a novel human-machine\ncollaboration problem in a bandit feedback setting. Our solution aims to\nexploit the human-machine complementarity to maximize decision rewards. We then\nextend our approach to settings with multiple human decision makers. We\ndemonstrate the effectiveness of our proposed methods using both synthetic and\nreal human responses, and find that our methods outperform both the algorithm\nand the human when they each make decisions on their own. We also show how\npersonalized routing in the presence of multiple human decision-makers can\nfurther improve the human-machine team performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11216,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad\n  Virtual Y Conocimiento Experto para generaci\\'on de entornos de aprendizaje\n  Hombre-M\\'aquina\n\n  This work presents the results of project CONECT4, which addresses the\nresearch and development of new non-intrusive communication methods for the\ngeneration of a human-machine learning ecosystem oriented to predictive\nmaintenance in the automotive industry. Through the use of innovative\ntechnologies such as Augmented Reality, Virtual Reality, Digital Twin and\nexpert knowledge, CONECT4 implements methodologies that allow improving the\nefficiency of training techniques and knowledge management in industrial\ncompanies. The research has been supported by the development of content and\nsystems with a low level of technological maturity that address solutions for\nthe industrial sector applied in training and assistance to the operator. The\nresults have been analyzed in companies in the automotive sector, however, they\nare exportable to any other type of industrial sector. -- --\n  En esta publicaci\\'on se presentan los resultados del proyecto CONECT4, que\naborda la investigaci\\'on y desarrollo de nuevos m\\'etodos de comunicaci\\'on no\nintrusivos para la generaci\\'on de un ecosistema de aprendizaje\nhombre-m\\'aquina orientado al mantenimiento predictivo en la industria de\nautomoci\\'on. A trav\\'es del uso de tecnolog\\'ias innovadoras como la Realidad\nAumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,\nCONECT4 implementa metodolog\\'ias que permiten mejorar la eficiencia de las\nt\\'ecnicas de formaci\\'on y gesti\\'on de conocimiento en las empresas\nindustriales. La investigaci\\'on se ha apoyado en el desarrollo de contenidos y\nsistemas con un nivel de madurez tecnol\\'ogico bajo que abordan soluciones para\nel sector industrial aplicadas en la formaci\\'on y asistencia al operario. Los\nresultados han sido analizados en empresas del sector de automoci\\'on, no\nobstante, son exportables a cualquier otro tipo de sector industrial.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.13675,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Creating and Implementing a Smart Speaker\n\n  We have seen significant advancements in Artificial Intelligence and Machine\nLearning in the 21st century. It has enabled a new technology where we can have\na human-like conversation with the machines. The most significant use of this\nspeech recognition and contextual understanding technology exists in the form\nof a Smart Speaker. We have a wide variety of Smart Speaker products available\nto us. This paper aims to decode its creation and explain the technology that\nmakes these Speakers, \"Smart.\"\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.1237,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Design Your Life: User-Initiated Design of Technology to Support\n  Independent Living of Young Autistic Adults\n\n  This paper describes the development of and first experiences with 'Design\nYour Life': a novel method aimed at user-initiated design of technologies\nsupporting young autistic adults in independent living. A conceptual,\nphenomenological background resulting in four core principles is described.\nTaking a practice-oriented Research-through-Design approach, three co-design\ncase studies were conducted, in which promising methods from the co-design\nliterature with the lived experiences and practical contexts of autistic young\nadults and their caregivers is contrasted. This explorative inquiry provided\nsome first insights into several design directions of the Design Your\nLife-process. In a series of new case studies that shall follow, the Design\nYour Life-method will be iteratively developed, refined and ultimately\nvalidated in practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05182,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000088414,
      "text":"PTeacher: a Computer-Aided Personalized Pronunciation Training System\n  with Exaggerated Audio-Visual Corrective Feedback\n\n  Second language (L2) English learners often find it difficult to improve\ntheir pronunciations due to the lack of expressive and personalized corrective\nfeedback. In this paper, we present Pronunciation Teacher (PTeacher), a\nComputer-Aided Pronunciation Training (CAPT) system that provides personalized\nexaggerated audio-visual corrective feedback for mispronunciations. Though the\neffectiveness of exaggerated feedback has been demonstrated, it is still\nunclear how to define the appropriate degrees of exaggeration when interacting\nwith individual learners. To fill in this gap, we interview 100 L2 English\nlearners and 22 professional native teachers to understand their needs and\nexperiences. Three critical metrics are proposed for both learners and teachers\nto identify the best exaggeration levels in both audio and visual modalities.\nAdditionally, we incorporate the personalized dynamic feedback mechanism given\nthe English proficiency of learners. Based on the obtained insights, a\ncomprehensive interactive pronunciation training course is designed to help L2\nlearners rectify mispronunciations in a more perceptible, understandable, and\ndiscriminative manner. Extensive user studies demonstrate that our system\nsignificantly promotes the learners' learning efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.1072,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Designing Limitless Path in Virtual Reality Environment\n\n  Walking in a Virtual Environment is a bounded task. It is challenging for a\nsubject to navigate a large virtual environment designed in a limited physical\nspace. External hardware support may be required to achieve such an act in a\nconcise physical area without compromising navigation and virtual scene\nrendering quality. This paper proposes an algorithmic approach to let a subject\nnavigate a limitless virtual environment within a limited physical space with\nno additional external hardware support apart from the regular\nHead-Mounted-Device (HMD) itself. As part of our work, we developed a Virtual\nArt Gallery as a use-case to validate our algorithm. We conducted a simple\nuser-study to gather feedback from the participants to evaluate the ease of\nlocomotion of the application. The results showed that our algorithm could\ngenerate limitless paths of our use-case under predefined conditions and can be\nextended to other use-cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01878,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000138084,
      "text":"The EMPATHIC Project: Mid-term Achievements\n\n  The goal of active aging is to promote changes in the elderly community so as\nto maintain an active, independent and socially-engaged lifestyle.\nTechnological advancements currently provide the necessary tools to foster and\nmonitor such processes. This paper reports on mid-term achievements of the\nEuropean H2020 EMPATHIC project, which aims to research, innovate, explore and\nvalidate new interaction paradigms and platforms for future generations of\npersonalized virtual coaches to assist the elderly and their carers to reach\nthe active aging goal, in the vicinity of their home. The project focuses on\nevidence-based, user-validated research and integration of intelligent\ntechnology, and context sensing methods through automatic voice, eye and facial\nanalysis, integrated with visual and spoken dialogue system capabilities. In\nthis paper, we describe the current status of the system, with a special\nemphasis on its components and their integration, the creation of a Wizard of\nOz platform, and findings gained from user interaction studies conducted\nthroughout the first 18 months of the project.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.12453,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"How Do Users Interact with an Error-Prone In-Air Gesture Recognizer?\n\n  We present results of two pilot studies that investigated human error\nbehaviours with an error prone in-air gesture recognizer. During the studies,\nusers performed a small set of simple in-air gestures. In the first study,\nthese gestures were abstract. The second study associated concrete tasks with\neach gesture. Interestingly, the error patterns observed in the two studies\nwere substantially different.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.00502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"What Way Is It Meant To Be Played?\n\n  The most commonly used interface between a video game and the human user is a\nhandheld \"game controller\", \"game pad\", or in some occasions an \"arcade stick.\"\nDirectional pads, analog sticks and buttons - both digital and analog - are\nlinked to in-game actions. One or multiple simultaneous inputs may be necessary\nto communicate the intentions of the user. Activating controls may be more or\nless convenient depending on their position and size. In order to enable the\nuser to perform all inputs which are necessary during gameplay, it is thus\nimperative to find a mapping between in-game actions and buttons, analog\nsticks, and so on. We present simple formats for such mappings as well as for\nthe constraints on possible inputs which are either determined by a physical\ngame controller or required to be met for a game software, along with methods\nto transform said constraints via a button-action mapping and to check one\nconstraint set against another, i.e., to check whether a button-action mapping\nallows a controller to be used in conjunction with a game software, while\npreserving all desired properties.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.1468,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000320541,
      "text":"ThumbTrak: Recognizing Micro-finger Poses Using a Ring with Proximity\n  Sensing\n\n  ThumbTrak is a novel wearable input device that recognizes 12 micro-finger\nposes in real-time. Poses are characterized by the thumb touching each of the\n12 phalanges on the hand. It uses a thumb-ring, built with a flexible printed\ncircuit board, which hosts nine proximity sensors. Each sensor measures the\ndistance from the thumb to various parts of the palm or other fingers.\nThumbTrak uses a support-vector-machine (SVM) model to classify finger poses\nbased on distance measurements in real-time. A user study with ten participants\nshowed that ThumbTrak could recognize 12 micro finger poses with an average\naccuracy of 93.6%. We also discuss potential opportunities and challenges in\napplying ThumbTrak in real-world applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11037,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Visualization -- a vital decision driving tool for enterprises\n\n  This report documents the results found through surveys and interviews on how\nvisualizations help the employees in their workspace. The objectives of this\nstudy were to get in-depth knowledge on what prepares an employee to have the\nright skill set in constructing an informative visualization as well as the\ntools and techniques that they use on their daily basis for analysis and\nvisualization purposes. Using the results gathered, we sorted the information\nin many different ways for analysis and came to conclusions ranging from\ncorporation-based strategies to individualized employee and position\npreferences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.1447,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"Internet of Everything Driven Neuromarketing: Key Technologies and\n  Challenges\n\n  Preserving customers' expectations and understanding factors affecting their\npurchasing decisions would significantly affect designing effective marketing\nand advertising strategies. However, constantly and swiftly changing the\ncustomers' interests and consumption behaviors, make it inevitable to utilize\nthe sophisticated tools and approaches based on advanced technologies. Among\nthem, neuromarketing by measuring the customers' physiological and neural\nsignals, studying the consumers' cognitive, and affective responses to\nmarketing stimulus, provides deep insight into the customers' motivations,\npreferences, and decisions. Recently, the Internet of Everything (IoE) has\nbrought many new opportunities to the industry and has attracted the attention\nof many researchers in recent years. The main objective of this paper is to\naddress how the Internet of Everything (IoE) would empower neuromarketing\ntechniques. In particular, applications of IoE gadgets and devices in eleven\ngroups of neuromarketing techniques are discussed to present numerous solutions\nthat would help meet this goal. Moreover, we present an in-depth understanding\nof current research issues as well as emerging trends.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05492,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Producing Liveness: The Trials of Moving Folk Clubs Online During the\n  Global Pandemic\n\n  The global pandemic has driven musicians online. We report an ethnographic\naccount of how two traditional folk clubs with little previous interest in\ndigital platforms transitioned to online experiences. They followed very\ndifferent approaches: one adapted their existing singaround format to video\nconferencing while the other evolved a weekly community-produced, pre-recorded\nshow that could be watched together. However, despite their successes,\nparticipants ultimately remained unable to sing in chorus due to network\nconstraints. We draw on theories of liveness from performance studies to\nexplain our findings, arguing that HCI might orientate itself to online\nliveness as being co-produced through rich participatory structures that\ndissolve traditional distinctions between live and recorded and performer and\naudience. We discuss how participants appropriated existing platforms to\nachieve this, but these in turn shaped their practices in unforeseen ways. We\ndraw out implications for the design and deployment of future live performance\nplatforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05322,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000062916,
      "text":"Diplomat: A conversational agent framework for goal-oriented group\n  discussion\n\n  Recent work in human-computer interaction has explored the use of\nconversational agents as facilitators for group goal-oriented discussions.\nInspired by this work and by the apparent lack of tooling available to support\nit, we created Diplomat, a Python-based framework for building conversational\nagent facilitators. Diplomat is designed to support simple specification of\nagent functionality as well as customizable integration with online chat\nservices. We document a preliminary user study we conducted to help inform the\ndesign of Diplomat. We also describe the architecture, capabilities, and\nlimitations of our tool, which we have shared on GitHub.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.09457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"The Challenge of Variable Effort Crowdsourcing and How Visible Gold Can\n  Help\n\n  We consider a class of variable effort human annotation tasks in which the\nnumber of labels required per item can greatly vary (e.g., finding all faces in\nan image, named entities in a text, bird calls in an audio recording, etc.). In\nsuch tasks, some items require far more effort than others to annotate.\nFurthermore, the per-item annotation effort is not known until after each item\nis annotated since determining the number of labels required is an implicit\npart of the annotation task itself. On an image bounding-box task with\ncrowdsourced annotators, we show that annotator accuracy and recall\nconsistently drop as effort increases. We hypothesize reasons for this drop and\ninvestigate a set of approaches to counteract it. Firstly, we benchmark on this\ntask a set of general best-practice methods for quality crowdsourcing. Notably,\nonly one of these methods actually improves quality: the use of visible gold\nquestions that provide periodic feedback to workers on their accuracy as they\nwork. Given these promising results, we then investigate and evaluate variants\nof the visible gold approach, yielding further improvement. Final results show\na 7% improvement in bounding-box accuracy over the baseline. We discuss the\ngenerality of the visible gold approach and promising directions for future\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07445,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"Enhancing the Usability of Self-service Kiosks for Older Adults: Effects\n  of Using Privacy Partitions and Chairs\n\n  This study aimed to evaluate the effects of possible physical design features\nof self-service kiosks (SSK), side and back partitions and chairs, on workload\nand task performance of older users during a typical SSK task. The study\ncomparatively evaluated eight physical SSK design alternatives, and younger and\nolder participants performed a menu ordering task using each physical design\nalternative. Older participants showed a large variation in task performance\nacross the design alternatives indicating stronger impacts of the physical\ndesign features. In particular, sitting significantly reduced task completion\ntime and workload in multiple dimensions, including time pressure and\nfrustration. In addition, the use of either side or back partitions reduced\nmean ratings of mental demand and effort. The study suggests placing chairs and\neither side or back partitions to enhance older adults' user experience. The\nuse of the proposed physical design recommendations would greatly help them use\nSSK more effectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.04961,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"You Are How (and Where) You Search? Comparative Analysis of Web Search\n  Behaviour Using Web Tracking Data\n\n  We conduct a comparative analysis of desktop web search behaviour of users\nfrom Germany (n=558) and Switzerland (n=563) based on a combination of web\ntracking and survey data. We find that web search accounts for 13% of all\ndesktop browsing, with the share being higher in Switzerland than in Germany.\nWe find that in over 50% of cases users clicked on the first search result,\nwith over 97% of all clicks being made on the first page of search outputs.\nMost users rely on Google when conducting searches, and users preferences for\nother engines are related to their demographics. We also test relationships\nbetween user demographics and daily number of searches, average share of search\nactivities among tracked events by user as well as the tendency to click on\nhigher- or lower-ranked results. We find differences in such relationships\nbetween the two countries that highlights the importance of comparative\nresearch in this domain. Further, we observe differences in the temporal\npatterns of web search use between women and men, marking the necessity of\ndisaggregating data by gender in observational studies regarding online\ninformation behaviour.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.14802,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Communication Analysis through Visual Analytics: Current Practices,\n  Challenges, and New Frontiers\n\n  The automated analysis of digital human communication data often focuses on\nspecific aspects such as content or network structure in isolation. This can\nprovide limited perspectives while making cross-methodological analyses,\noccurring in domains like investigative journalism, difficult. Communication\nresearch in psychology and the digital humanities instead stresses the\nimportance of a holistic approach to overcome these limiting factors. In this\nwork, we conduct an extensive survey on the properties of over forty\nsemi-automated communication analysis systems and investigate how they cover\nconcepts described in theoretical communication research. From these\ninvestigations, we derive a design space and contribute a conceptual framework\nbased on communication research, technical considerations, and the surveyed\napproaches. The framework describes the systems' properties, capabilities, and\ncomposition through a wide range of criteria organized in the dimensions (1)\nData, (2) Processing and Models, (3) Visual Interface, and (4) Knowledge\nGeneration. These criteria enable a formalization of digital communication\nanalysis through visual analytics, which, we argue, is uniquely suited for this\ntask by tackling automation complexity while leveraging domain knowledge. With\nour framework, we identify shortcomings and research challenges, such as group\ncommunication dynamics, trust and privacy considerations, and holistic\napproaches. Simultaneously, our framework supports the evaluation of systems\nand promotes the mutual exchange between researchers through a structured\ncommon language, laying the foundations for future research on communication\nanalysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.01838,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000033776,
      "text":"Acoustic-based Object Detection for Pedestrian Using Smartphone\n\n  Walking while using a smartphone is becoming a major pedestrian safety\nconcern as people may unknowingly bump into various obstacles that could lead\nto severe injuries. In this paper, we propose ObstacleWatch, an acoustic-based\nobstacle collision detection system to improve the safety of pedestrians who\nare engaged in smartphone usage while walking. ObstacleWatch leverages the\nadvanced audio hardware of the smartphone to sense the surrounding obstacles\nand infers fine-grained information about the frontal obstacle for collision\ndetection. In particular, our system emits well-designed inaudible beep signals\nfrom the smartphone built-in speaker and listens to the reflections with the\nstereo recording of the smartphone. By analyzing the reflected signals received\nat two microphones, ObstacleWatch is able to extract fine-grained information\nof the frontal obstacle including the distance, angle, and size for detecting\nthe possible collisions and to alert users. Our experimental evaluation under\ntwo real-world environments with different types of phones and obstacles shows\nthat ObstacleWatch achieves over 92% accuracy in predicting obstacle collisions\nwith distance estimation errors at about 2 cm. Results also show that\nObstacleWatch is robust to different sizes of objects and is compatible with\ndifferent phone models with low energy consumption.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.15005,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Untidy Data: The Unreasonable Effectiveness of Tables\n\n  Working with data in table form is usually considered a preparatory and\ntedious step in the sensemaking pipeline; a way of getting the data ready for\nmore sophisticated visualization and analytical tools. But for many people,\nspreadsheets -- the quintessential table tool -- remain a critical part of\ntheir information ecosystem, allowing them to interact with their data in ways\nthat are hidden or abstracted in more complex tools. This is particularly true\nfor data workers: people who work with data as part of their job but do not\nidentify as professional analysts or data scientists. We report on a\nqualitative study of how these workers interact with and reason about their\ndata. Our findings show that data tables serve a broader purpose beyond data\ncleanup at the initial stage of a linear analytic flow: users want to see and\n\"get their hands on\" the underlying data throughout the analytics process,\nreshaping and augmenting it to support sensemaking. They reorganize, mark up,\nlayer on levels of detail, and spawn alternatives within the context of the\nbase data. These direct interactions and human-readable table representations\nform a rich and cognitively important part of building understanding of what\nthe data mean and what they can do with it. We argue that interactive tables\nare an important visualization idiom in their own right; that the direct data\ninteraction they afford offers a fertile design space for visual analytics; and\nthat sense making can be enriched by more flexible human-data interaction than\nis currently supported in visual analytics tools.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.0914,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000100666,
      "text":"Human-AI Interactions Through A Gricean Lens\n\n  Grice's Cooperative Principle (1975) describes the implicit maxims that guide\nconversation between humans. As humans begin to interact with non-human\ndialogue systems more frequently and in a broader scope, an important question\nemerges: what principles govern those interactions? The present study addresses\nthis question by evaluating human-AI interactions using Grice's four maxims; we\ndemonstrate that humans do, indeed, apply these maxims to interactions with AI,\neven making explicit references to the AI's performance through a Gricean lens.\nTwenty-three participants interacted with an American English-speaking Alexa\nand rated and discussed their experience with an in-lab researcher. Researchers\nthen reviewed each exchange, identifying those that might relate to Grice's\nmaxims: Quantity, Quality, Manner, and Relevance. Many instances of explicit\nuser frustration stemmed from violations of Grice's maxims. Quantity violations\nwere noted for too little but not too much information, while Quality\nviolations were rare, indicating trust in Alexa's responses. Manner violations\nfocused on speed and humanness. Relevance violations were the most frequent,\nand they appear to be the most frustrating. While the maxims help describe many\nof the issues participants encountered, other issues do not fit neatly into\nGrice's framework. Participants were particularly averse to Alexa initiating\nexchanges or making unsolicited suggestions. To address this gap, we propose\nthe addition of human Priority to describe human-AI interaction. Humans and AIs\nare not conversational equals, and human initiative takes priority. We suggest\nthat the application of Grice's Cooperative Principles to human-AI interactions\nis beneficial both from an AI development perspective and as a tool for\ndescribing an emerging form of interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.13747,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Unpacking Adherence and Engagement in Pervasive Health Games\n\n  Pervasive health games have a potential to impact health-related behaviors.\nAnd, similar to other types of interventions, engagement and adherence in\nhealth games is the keystone for examining their short- and long-term effects.\nMany health-based applications have turned to gamification principles\nspecifically to. enhance their engagement. However, according to many reports,\nonly 41% of participants are retained in single player games and 29% in social\ngames after 90 days. These statistics raise multiple questions about factors\ninfluencing adherence and engagement. This paper presents an in-depth\nmixed-methods investigation of game design factors affecting engagement with\nand adherence to a pervasive commercial health game, called SpaPlay. We\nanalyzed interview and game behavior log data using theoretical constructs of\nsustained engagement to identify design elements affecting engagement and\nadherence. Our findings indicate that design elements associated with autonomy.\nand relatedness from the Self-Determination Theory and integrability, a measure\nof how well activities align with a person's life style, are important factors\naffecting engagement and adherence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00865,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Multi-User Activity Recognition and Tracking Using Commodity WiFi\n\n  This paper presents MultiTrack, a commodity WiFi-based human sensing system\nthat can track multiple users and recognize the activities of multiple users\nperforming them simultaneously. Such a system can enable easy and large-scale\ndeployment for multi-user tracking and sensing without the need for additional\nsensors through the use of existing WiFi devices (e.g., desktops, laptops, and\nsmart appliances). The basic idea is to identify and extract the signal\nreflection corresponding to each individual user with the help of multiple WiFi\nlinks and all the available WiFi channels at 5GHz. Given the extracted signal\nreflection of each user, MultiTrack examines the path of the reflected signals\nat multiple links to simultaneously track multiple users. It further\nreconstructs the signal profile of each user as if only a single user has\nperformed activity in the environment to facilitate multi-user activity\nrecognition. We evaluate MultiTrack in different multipath environments with up\nto 4 users for multi-user tracking and up to 3 users for activity recognition.\nExperimental results show that our system can achieve decimeter localization\naccuracy and over 92% activity recognition accuracy under multi-user scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.057,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"A Wearable Virtual Touch System for Cars\n\n  In automotive domain, operation of secondary tasks like accessing\ninfotainment system, adjusting air conditioning vents, and side mirrors\ndistract drivers from driving. Though existing modalities like gesture and\nspeech recognition systems facilitate undertaking secondary tasks by reducing\nduration of eyes off the road, those often require remembering a set of\ngestures or screen sequences. In this paper, we have proposed two different\nmodalities for drivers to virtually touch the dashboard display using a laser\ntracker with a mechanical switch and an eye gaze switch. We compared\nperformances of our proposed modalities against conventional touch modality in\nautomotive environment by comparing pointing and selection times of\nrepresentative secondary task and also analysed effect on driving performance\nin terms of deviation from lane, average speed, variation in perceived workload\nand system usability. We did not find significant difference in driving and\npointing performance between laser tracking system and existing touchscreen\nsystem. Our result also showed that the driving and pointing performance of the\nvirtual touch system with eye gaze switch was significantly better than the\nsame with mechanical switch. We evaluated the efficacy of the proposed virtual\ntouch system with eye gaze switch inside a real car and investigated acceptance\nof the system by professional drivers using qualitative research. The\nquantitative and qualitative studies indicated importance of using multimodal\nsystem inside car and highlighted several criteria for acceptance of new\nautomotive user interface.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.02077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Eliciting Spoken Interruptions to Inform Proactive Speech Agent Design\n\n  Current speech agent interactions are typically user-initiated, limiting the\ninteractions they can deliver. Future functionality will require agents to be\nproactive, sometimes interrupting users. Little is known about how these spoken\ninterruptions should be designed, especially in urgent interruption contexts.\nWe look to inform design of proactive agent interruptions through investigating\nhow people interrupt others engaged in complex tasks. We therefore developed a\nnew technique to elicit human spoken interruptions of people engaged in other\ntasks. We found that people interrupted sooner when interruptions were urgent.\nSome participants used access rituals to forewarn interruptions, but most\nrarely used them. People balanced speed and accuracy in timing interruptions,\noften using cues from the task they interrupted. People also varied phrasing\nand delivery of interruptions to reflect urgency. We discuss how our findings\ncan inform speech agent design and how our paradigm can help gain insight into\nhuman interruptions in new contexts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.14505,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Interactions and Actions in One Touch Gesture Mobile Games\n\n  A player plays a game by sending messages into the game world using an\ninteraction technique. These messages are then translated into actions\nperformed on or by game objects towards achieving the game's objectives. A\ngame's interaction model is the bridge between the player's interaction and its\nin-game actions by defining what the player may and may not act upon at any\ngiven moment. This makes the choice of interaction technique, its associated\nactions, and interaction model critical for designing games that are engaging,\nimmersive, and intuitive to play. This paper presents a study focused on\nOne-Touch-Gesture mobile games, with the aim of identifying the touch gestures\nused in popular games of this type, the types of in-game actions associated\nwith these gestures, and the interaction models used by these games. The study\nwas conducted by reviewing 77 of the most popular games in the last two years\nthrough playtesting by two researchers. The results of the study contribute to\nexisting knowledge by providing an insight into the interactions and actions of\npopular 1TG games and providing a guide to aid in developing games of the type.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.02604,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Do Persuasive Designs Make Smartphones More Addictive? -- A\n  Mixed-Methods Study on Chinese University Students\n\n  Persuasive designs become prevalent on smartphones, and an increasing number\nof users report having problematic smartphone use behaviours. Persuasive\ndesigns in smartphones might be accountable for the development and\nreinforcement of such problematic use. This paper uses a mixed-methods approach\nto study the relationship between persuasive designs and problematic smartphone\nuse: (1) questionnaires (N=183) to investigate the proportion of participants\nhaving multiple problematic smartphone use behaviours and smartphone designs\nand applications (apps) that they perceived affecting their attitudes and\nbehaviours, and (2) interviews (N=10) to deepen our understanding of users'\nobservations and evaluations of persuasive designs. 25\\% of the participants\nself-reported having multiple problematic smartphone use behaviours, with short\nvideo, social networking, game and learning apps perceived as most attitude and\nbehaviour-affecting. Interviewees identified multiple persuasive designs in\nmost of these apps and stated that persuasive designs prolonged their screen\ntime, reinforced phone-checking habits, and caused distractions. Overall, this\nstudy provides evidence to argue that persuasive designs contribute to\nproblematic smartphone use, potentially making smartphones more addictive. We\nend our study by discussing the ethical implications of persuasive designs that\nbecame salient in our study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.05854,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Do you feel safe with your robot? Factors Influencing Perceived Safety\n  in Human-Robot Interaction based on Subjective and Objective Measures\n\n  Safety in human-robot interaction can be divided into physical safety and\nperceived safety, where the latter is still under-addressed in the literature.\nInvestigating perceived safety in human-robot interaction requires a\nmultidisciplinary perspective. Indeed, perceived safety is often considered as\nbeing associated with several common factors studied in other disciplines,\ni.e., comfort, predictability, sense of control, and trust. In this paper, we\ninvestigated the relationship between these factors and perceived safety in\nhuman-robot interaction using subjective and objective measures. We conducted a\ntwo-by-five mixed-subjects design experiment. The five within-subjects\nconditions correspond to (1) baseline, and the manipulations of robot behaviors\nto stimulate: (2) discomfort, (3) decreased perceived safety, (4) decreased\nsense of control and (5) distrust. Twenty-seven young adult participants took\npart in the experiments. Participants were asked to answer questionnaires that\nmeasure the manipulated factors after within-subjects conditions. Besides\nquestionnaire data, we collected objective measures such as videos and\nphysiological data. The questionnaire results show a correlation between\ncomfort, sense of control, trust, and perceived safety. We also discuss the\neffect of individual human characteristics (such as personality and gender)\nthat they could be predictors of perceived safety. We used the physiological\nsignal data and facial affect from videos for estimating perceived safety where\nparticipants' subjective ratings were utilized as labels. The data from\nobjective measures revealed that the prediction rate was higher from\nphysiological signal data. This paper can play an important role in the goal of\nbetter understanding perceived safety in human-robot interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.04257,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"Augmenting Teleportation in Virtual Reality With Discrete Rotation\n  Angles\n\n  Locomotion is one of the most essential interaction tasks in virtual reality\n(VR) with teleportation being widely accepted as the state-of-the-art\nlocomotion technique at the time of this writing. A major draw-back of\nteleportation is the accompanying physical rotation that is necessary to adjust\nthe users' orientation either before or after teleportation. This is a limiting\nfactor for tethered head-mounted displays (HMDs) and static body postures and\ncan induce additional simulator sickness for HMDs with three degrees-of-freedom\n(DOF) due to missing parallax cues. To avoid physical rotation, previous work\nproposed discrete rotation at fixed intervals (InPlace) as a controller-based\ntechnique with low simulator sickness, yet the impact of varying intervals on\nspatial disorientation, user presence and performance remains to be explored.\nAn unevaluated technique found in commercial VR games is reorientation during\nthe teleportation process (TeleTurn), which prevents physical rotation but\npotentially increases interaction time due to its continuous orientation\nselection. In an exploratory user study, where participants were free to apply\nboth techniques, we evaluated the impact of rotation parameters of either\ntechnique on user performance and preference. Our results indicate that\ndiscrete InPlace rotation introduced no significant spatial disorientation,\nwhile user presence scores were increased. Discrete TeleTurn and teleportation\nwithout rotation was ranked higher and achieved a higher presence score than\ncontinuous TeleTurn, which is the current state-of-the-art found in VR games.\nBased on observations, that participants avoided TeleTurn rotation when\ndiscrete InPlace rotation was available, we distilled guidelines for designing\nteleportation without physical rotation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.15297,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Socially Intelligent Interfaces for Increased Energy Awareness in the\n  Home\n\n  This paper describes how home appliances might be enhanced to improve user\nawareness of energy usage. Households wish to lead comfortable and manageable\nlives. Balancing this reasonable desire with the environmental and political\ngoal of reducing electricity usage is a challenge that we claim is best met\nthrough the design of interfaces that allows users better control of their\nusage and unobtrusively informs them of the actions of their peers. A set of\ndesign principles along these lines is formulated in this paper. We have built\na fully functional prototype home appliance with a socially aware interface to\nsignal the aggregate usage of the users peer group according to these\nprinciples, and present the prototype in the paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.14068,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000065565,
      "text":"Effects of Head-locked Augmented Reality on User's performance and\n  perceived workload\n\n  An augmented reality (AR) environment includes a set of digital elements with\nwhich the users interact while performing certain tasks. Recent AR head-mounted\ndisplays allow users to select how these elements are presented. However, few\nstudies have been conducted to examine the effect of the way of presenting\naugmented content on user performance and workload. This study aims to evaluate\ntwo methods of presenting augmented content - world-locked and head-locked\nmodes in a data entry task. A total of eighteen participants performed the data\nentry task in this study. The effectiveness of each mode is evaluated in terms\nof task performance, muscle activity, perceived workload, and usability. The\nresults show that the task completion time is shorter and the typing speed is\nsignificantly faster in the head-locked mode while the world-locked mode\nachieved higher scores in terms of preference. The findings of this study can\nbe applied to AR user interfaces to improve content presentation and enhance\nthe user experience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00931,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000200338,
      "text":"Understanding the Design Space of Mouth Microgestures\n\n  As wearable devices move toward the face (i.e. smart earbuds, glasses), there\nis an increasing need to facilitate intuitive interactions with these devices.\nCurrent sensing techniques can already detect many mouth-based gestures;\nhowever, users' preferences of these gestures are not fully understood. In this\npaper, we investigate the design space and usability of mouth-based\nmicrogestures. We first conducted brainstorming sessions (N=16) and compiled an\nextensive set of 86 user-defined gestures. Then, with an online survey (N=50),\nwe assessed the physical and mental demand of our gesture set and identified a\nsubset of 14 gestures that can be performed easily and naturally. Finally, we\nconducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to\nvarious daily smartphone operations under a sitting and walking context. From\nthese studies, we develop a taxonomy for mouth gestures, finalize a practical\ngesture set for common applications, and provide design guidelines for future\nmouth-based gesture interactions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.09906,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Influence of agent's self-disclosure on human empathy\n\n  As AI technologies progress, social acceptance of AI agents, including\nintelligent virtual agents and robots, is becoming even more important for more\napplications of AI in human society. One way to improve the relationship\nbetween humans and anthropomorphic agents is to have humans empathize with the\nagents. By empathizing, humans act positively and kindly toward agents, which\nmakes it easier for them to accept the agents. In this study, we focus on\nself-disclosure from agents to humans in order to increase empathy felt by\nhumans toward anthropomorphic agents. We experimentally investigate the\npossibility that self-disclosure from an agent facilitates human empathy. We\nformulate hypotheses and experimentally analyze and discuss the conditions in\nwhich humans have more empathy toward agents. Experiments were conducted with a\nthree-way mixed plan, and the factors were the agents' appearance (human,\nrobot), self-disclosure (high-relevance self-disclosure, low-relevance\nself-disclosure, no self-disclosure), and empathy before\/after a video\nstimulus. An analysis of variance (ANOVA) was performed using data from 918\nparticipants. We found that the appearance factor did not have a main effect,\nand self-disclosure that was highly relevant to the scenario used facilitated\nmore human empathy with a statistically significant difference. We also found\nthat no self-disclosure suppressed empathy.These results support our\nhypotheses. This study reveals that self-disclosure represents an important\ncharacteristic of anthropomorphic agents which helps humans to accept them.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00065,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Design and Evaluation of Scalable Representations of Communication in\n  Gantt Charts for Large-scale Execution Traces\n\n  Gantt charts are frequently used to explore execution traces of large-scale\nparallel programs found in high-performance computing (HPC). In these\nvisualizations, each parallel processor is assigned a row showing the\ncomputation state of a processor at a particular time. Lines are drawn between\nrows to show communication between these processors. When drawn to align\nequivalent calls across rows, structures can emerge reflecting communication\npatterns employed by the executing code. However, though these structures have\nthe same definition at any scale, they are obscured by the density of rendered\nlines when displaying more than a few hundred processors. A more scalable\nmetaphor is necessary to aid HPC experts in understanding communication in\nlarge-scale traces. To address this issue, we first conduct an exploratory\nstudy to identify what visual features are critical for determining similarity\nbetween structures shown at different scales. Based on these findings, we\ndesign a set of glyphs for displaying these structures in dense charts. We then\nconduct a pre-registered user study evaluating how well people interpret\ncommunication using our new representation versus their base depictions in\nlarge-scale Gantt charts. Through our evaluation, we find that our\nrepresentation enables users to more accurately identify communication patterns\ncompared to full renderings of dense charts. We discuss the results of our\nevaluation and findings regarding the design of metaphors for extensible\nstructures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.07408,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Using Eye Tracker To Evaluate Cockpit Design -- A Flight Simulation\n  Study\n\n  This paper investigates applications of eye tracking in transport aircraft\ndesign evaluations. Piloted simulations were conducted for a complete flight\nprofile including take off, cruise and landing flight scenario using the\ntransport aircraft flight simulator at CSIR National Aerospace Laboratories.\nThirty-one simulation experiments were carried out with three pilots and\nengineers while recording the ocular parameters and the flight data.\nSimulations were repeated for high workload conditions like flying with\ndegraded visibility and during stall. Pilots visual scan behaviour and workload\nlevels were analysed using ocular parameters; while comparing with the\nstatistical deviations from the desired flight path. Conditions for fatigue\nwere also recreated through long duration simulations and signatures for the\nsame from the ocular parameters were assessed. Results from the study found\ncorrelation between the statistical inferences obtained from the ocular\nparameters with those obtained from the flight path deviations. The paper also\ndemonstrates an evaluators console that assists the designers or evaluators for\nbetter understanding of pilots attentional resource allocation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00857,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000086096,
      "text":"Fine-grained Finger Gesture Recognition Using WiFi Signals\n\n  Gesture recognition has become increasingly important in human-computer\ninteraction and can support different applications such as smart home, VR, and\ngaming. Traditional approaches usually rely on dedicated sensors that are worn\nby the user or cameras that require line of sight. In this paper, we present\nfine-grained finger gesture recognition by using commodity WiFi without\nrequiring user to wear any sensors. Our system takes advantages of the\nfine-grained Channel State Information available from commodity WiFi devices\nand the prevalence of WiFi network infrastructures. It senses and identifies\nsubtle movements of finger gestures by examining the unique patterns exhibited\nin the detailed CSI. We devise environmental noise removal mechanism to\nmitigate the effect of signal dynamic due to the environment changes. Moreover,\nwe propose to capture the intrinsic gesture behavior to deal with individual\ndiversity and gesture inconsistency. Lastly, we utilize multiple WiFi links and\nlarger bandwidth at 5GHz to achieve finger gesture recognition under multi-user\nscenario. Our experimental evaluation in different environments demonstrates\nthat our system can achieve over 90% recognition accuracy and is robust to both\nenvironment changes and individual diversity. Results also show that our system\ncan provide accurate gesture recognition under different scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.10275,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"A Systematic Review of Computational Thinking in Early Ages\n\n  Nowadays, technology has become dominant in the daily lives of most people\naround the world. From children to older people, technology is present, helping\nin the most diverse daily tasks and allowing accessibility. However, many times\nthese people are just end-users, without any incentive to the development of\ncomputational thinking (CT). With advances in technologies, the abstraction of\ncoding, programming languages, and the hardware resources involved will become\na reality. However, while we have not progressed to this stage, it is necessary\nto encourage the development of CT teaching from an early age. This work will\npresent state of the art concerning teaching initiatives and tools on\nprogramming (e.g., ScratchJr), robotics (e.g., KIBO), and other playful tools\n(e.g., Happy Maps) for the development of CT in the early ages, specifically\nfilling the gap of CT at the kindergarten level. This survey presents a\nsystematic review of the literature, emphasizing computational and robotic\ntools used in preschool classes to develop the CT. The systematic review\nevaluated more than 60 papers from 2010 to December 2020, electing 31 papers\nand adding three papers from the qualitative stage. The paper's amount was\nclassified in taxonomy to show CT's principal tools and initiates applied to\nchildren early. To conclude this survey, an extensive discussion about the\nterms and authors related to this research area is present.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.11367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Rethinking the Ranks of Visual Channels\n\n  Data can be visually represented using visual channels like position, length\nor luminance. An existing ranking of these visual channels is based on how\naccurately participants could report the ratio between two depicted values.\nThere is an assumption that this ranking should hold for different tasks and\nfor different numbers of marks. However, there is little existing work testing\nassumption, especially given that visually computing ratios is relatively\nunimportant in real-world visualizations, compared to seeing, remembering, and\ncomparing trends and motifs, across displays that almost universally depict\nmore than two values.\n  We asked participants to immediately reproduce a set of values from memory.\nWith a Bayesian multilevel modeling approach, we observed how the relevant rank\npositions of visual channels shift across different numbers of marks (2, 4 or\n8) and for bias, precision, and error measures. The ranking did not hold, even\nfor reproductions of only 2 marks, and the new ranking was highly inconsistent\nfor reproductions of different numbers of marks. Other factors besides channel\nchoice far more influence on performance, such as the number of values in the\nseries (e.g. more marks led to larger errors), or the value of each mark (e.g.\nsmall values are systematically overestimated).\n  Recall was worse for displays with 8 marks than 4, consistent with\nestablished limits on visual memory. These results show that we must move\nbeyond two-value ratio judgments as a baseline for ranking the quality of a\nvisual channel, including testing new tasks (detection of trends or motifs),\ntimescales (immediate computation, or later comparison), and the number of\nvalues (from a handful, to thousands).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.05408,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Don't Touch Me! A Comparison of Usability on Touch and Non-Touch Inputs\n\n  Public touchscreens are filthy and, regardless of how often they are cleaned,\nthey pose a considerable risk in the transmission of bacteria and viruses.\nWhile we rely on their use, we should find a feasible alternative to touch\ndevices. Non-touch (touchless) interaction, via the use of mid-air gestures,\nhas been previously labelled as not user friendly and unsuitable. However,\nprevious works have extensively compared such interaction to precise mouse\nmovements. In this paper, we investigate and compare the usability of an\ninterface controlled via a touchscreen and a non-touch device. Participants\n(N=22) using a touchscreen and the Leap Motion Controller, performed tasks on a\nmock-up ticketing machine, later evaluating their experience using the System\nUsability and Gesture Usability scales. Results show that, in contrast to the\nprevious works, the non-touch method was usable and quickly learnable. We\nconclude with recommendations for future work on making a non-touch interface\nmore user-friendly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.08573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"AffectiveTDA: Using Topological Data Analysis to Improve Analysis and\n  Explainability in Affective Computing\n\n  We present an approach utilizing Topological Data Analysis to study the\nstructure of face poses used in affective computing, i.e., the process of\nrecognizing human emotion. The approach uses a conditional comparison of\ndifferent emotions, both respective and irrespective of time, with multiple\ntopological distance metrics, dimension reduction techniques, and face\nsubsections (e.g., eyes, nose, mouth, etc.). The results confirm that our\ntopology-based approach captures known patterns, distinctions between emotions,\nand distinctions between individuals, which is an important step towards more\nrobust and explainable emotion recognition by machines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12696,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"A tactile closed-loop device for musical interaction\n\n  This paper presents a device implementing a closed tactile loop for musical\ninteraction, based on a small freely held magnet which serves as the medium for\nboth input and output. The component parts as well as an example of its\nprogrammable behaviour are described.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.08268,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"From Flow to Fuse: A Cognitive Perspective\n\n  The concept of flow is used extensively in HCI, video games, and many other\nfields, but its prevalent definition is conceptually vague and alternative\ninterpretations have contributed to ambiguity in the literature. To address\nthis, we use cognitive science theory to expose inconsistencies in flow's\nprevalent definition, and introduce fuse, a concept related to flow but\nconsistent with cognitive science, and defined as the \"fusion of\nactivity-related sensory stimuli and awareness\". Based on this definition, we\ndevelop a preliminary model that hypothesizes fuse's underlying cognitive\nprocesses. To illustrate the model's practical value, we derive a set of design\nheuristics that we exemplify in the context of video games. Together, the fuse\ndefinition, model and design heuristics form our theoretical framework, and are\na product of rethinking flow from a cognitive perspective with the purpose of\nimproving conceptual clarity and theoretical robustness in the literature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.09038,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Code and Structure Editing for Teaching: A Case Study in using\n  Bibliometrics to Guide Computer Science Research\n\n  Structure or projectional editors are a well-studied concept among\nresearchers and some practitioners. They have the huge advantage of preventing\nsyntax and in some cases type errors, and aid the discovery of syntax by users\nunfamiliar with a language. This begs the question: why are they not widely\nused in education? To answer this question we performed a systematic review of\n57 papers and performed a bibliometric analysis which extended to 381 papers.\nFrom these we generated two hypotheses: (1) a lack of empirical evidence\nprevents educators from committing to this technology, and (2) existing tools\nhave not been designed based on actual user needs as they would be if\nhuman-centered design principles were used. Given problems we encountered with\nexisting resources to support a systematic review, and the role of bibliometric\ntools in overcoming those obstacles, we also detail our methods so that they\nmay be used as a guide for researchers or graduate students unfamiliar with\nbibliometrics. In particular, we report on which tools provide reliable and\nplentiful information in the field of computer science, and which have\ninsufficient coverage and interoperability issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.11633,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"CleanAirNowKC: Building Community Power by Improving Data Accessibility\n\n  As cities continue to grow globally, air pollution is increasing at an\nalarming rate, causing a significant negative impact on public health. One way\nto affect the negative impact is to regulate the producers of such pollution\nthrough policy implementation and enforcement. CleanAirNowKC (CAN-KC) is an\nenvironmental justice organization based in Kansas City (KC), Kansas. As part\nof their organizational objectives, they have to date deployed nine PurpleAir\nair quality sensors in different locations about which the community has\nexpressed concern. In this paper, we have implemented an interactive map that\ncan help the community members to monitor air quality efficiently. The system\nalso allows for reporting and tracking industrial emissions or toxic releases,\nwhich will further help identify major contributors to pollution. These\nresources can serve an important role as evidence that will assist in\nadvocating for community-driven just policies to improve the air quality\nregulation in Kansas City.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00389,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"Investigating the Reliability of Self-report Data in the Wild: The Quest\n  for Ground Truth\n\n  Inferring human mental state (e.g., emotion, depression, engagement) with\nsensing technology is one of the most valuable challenges in the affective\ncomputing area, which has a profound impact in all industries interacting with\nhumans. The self-report survey is the most common way to quantify how people\nthink, but prone to subjectivity and various responses bias. It is usually used\nas the ground truth for human mental state prediction. In recent years, many\ndata-driven machine learning models are built based on self-report annotations\nas the target value. In this research, we investigate the reliability of\nself-report surveys in the wild by studying the confidence level of responses\nand survey completion time. We conduct a case study (i.e., student engagement\ninference) by recruiting 23 students in a high school setting over a period of\n4 weeks. Our participants volunteered 488 self-reported responses and data from\ntheir wearable sensors. We also find the physiologically measured student\nengagement and perceived student engagement are not always consistent. The\nfindings from this research have great potential to benefit future studies in\npredicting engagement, depression, stress, and other emotion-related states in\nthe field of affective computing and sensing technologies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.07893,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Dark Patterns in Online Shopping: Of Sneaky Tricks, Perceived Annoyance\n  and Respective Brand Trust\n\n  Dark patterns utilize interface elements to trick users into performing\nunwanted actions. Online shopping websites often employ these manipulative\nmechanisms so as to increase their potential customer base, to boost their\nsales, or to optimize their advertising efforts. Although dark patterns are\noften successful, they clearly inhibit positive user experiences. Particularly,\nwith respect to customers' perceived annoyance and trust put into a given\nbrand, they may have negative effects. To investigate respective connections\nbetween the use of dark patterns, users' perceived level of annoyance and their\nexpressed brand trust, we conducted an experiment-based survey. We implemented\ntwo versions of a fictitious online shop; i.e. one which used five different\ntypes of dark patterns and a similar one without such manipulative user\ninterface elements. A total of $n=204$ participants were then forwarded to one\nof the two shops (approx. $2\/3$ to the shop which used the dark patterns) and\nasked to buy a specific product. Subsequently, we measured participants'\nperceived annoyance level, their expressed brand trust and their affinity for\ntechnology. Results show a higher level of perceived annoyance with those who\nused the dark pattern version of the online shop. Also, we found a significant\nconnection between perceived annoyance and participants' expressed brand trust.\nA connection between participants' affinity for technology and their ability to\nrecognize and consequently counter dark patterns, however, is not supported by\nour data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.10249,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Investigating External Interaction Modality and Design Between Automated\n  Vehicles and Pedestrians at Crossings\n\n  In this study, we investigated the effectiveness and user acceptance of three\nexternal interaction modalities (i.e., visual, auditory, and visual+auditory)\nin promoting communications between automated vehicle systems (AVS) and\npedestrians at a crosswalk through a large number of combined designs. For this\npurpose, an online survey was designed and distributed to 68 participants. All\nparticipants reported their overall preferences for safety, comfort, trust,\nease of understanding, usability, and acceptance towards the systems. Results\nshowed that the visual+auditory interaction modality was the mostly preferred,\nfollowed by the visual interaction modality and then the auditory one. We also\ntested different visual and auditory interaction methods, and found that\n\"Pedestrian silhouette on the front of the vehicle\" was the best preferred\noption while middle-aged participants liked \"Chime\" much better than young\nparticipants though it was overall better preferred than others. Finally,\ncommunication between the AVS and pedestrians' phones was not well received due\nto privacy concerns. These results provided important interface design\nrecommendations in identifying better combination of visual and auditory\ndesigns and therefore improving AVS communicating their intention with\npedestrians.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12599,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Design Guidelines to Increase the Persuasiveness of Achievement Goals\n  for Physical Activity\n\n  Achievement goals are frequently used to support behavior change. However,\nthey are often not specifically designed for this purpose nor account for the\ndegree to which a user is already intending to perform the target behavior. In\nthis paper, we investigate the perceived persuasiveness of different goal types\nas defined by the 3x2 Achievement Goal Model, what people like and dislike\nabout them and the role that behavior change intentions play when aiming at\nincreasing step counts. We created visualizations for each goal type based on a\nqualitative pre-study (N=18) and ensured their comprehensibility (N=18). In an\nonline experiment (N=118), we show that there are differences in the perception\nof these goal types and that behavior change intentions should be considered to\nmaximize their persuasiveness as goals evolve. Next, we derive design\nguidelines on when to use which type of achievement goal and what to consider\nwhen using them\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.0157,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Using Computer Simulations to Investigate the Potential Performance of\n  'A to B' Routing Systems for People with Mobility Impairments\n\n  Navigating from 'A to B' remains a serious problem for many people with\nmobility impairments, due to the need to avoid accessibility barriers. Yet\nthere is currently no effective routing tool that is regularly used by people\nwith disabilities in order to effectively avoid accessibility barriers in the\nbuilt environment. To explore what is required to produce an effective routing\ntool, we have conducted Monte-Carlo simulations, simulating over 460 million\njourneys. This work illustrates the need to focus on barrier minimization,\ninstead of barrier avoidance, due to the limitations of what can be achieved by\nany accessibility documentation tool. We also make a substantial contribution\nto the concern of meaningful performance metrics for activity recognition,\nillustrating how simulations can operate as useful real-world performance\nmetrics for information sources utilized by navigation systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12709,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"Developing the cyclotactor\n\n  This paper presents developments in the technology underlying the\ncyclotactor, a finger-based tactile I\/O device for musical interaction. These\ninclude significant improvements both in the basic characteristics of tactile\ninteraction and in the related (vibro)tactile sample rates, latencies, and\ntiming precision. After presenting the new prototype's tactile output force\nlandscape, some of the new possibilities for interaction are discussed,\nespecially those for musical interaction with zero audio\/tactile latency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03823,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Investigating the Sense of Presence Between Handcrafted and Panorama\n  Based Virtual Environments\n\n  Virtual Reality applications are becoming increasingly mature. The\nrequirements and complexity of such systems is steadily increasing. Realistic\nand detailed environments are often omitted in order to concentrate on the\ninteraction possibilities within the application. Creating an accurate and\nrealistic virtual environment is not a task for laypeople, but for experts in\n3D design and modeling. To save costs and avoid hiring experts, panorama images\nare often used to create realistic looking virtual environments. These images\ncan be captured and provided by non-experts. Panorama images are an alternative\nto handcrafted 3D models in many cases because they offer immersion and a scene\ncan be captured in great detail with the touch of a button. This work\ninvestigates whether it is advisable to recreate an environment in detail by\nhand or whether it is recommended to use panorama images for virtual\nenvironments in certain scenarios. For this purpose, an interactive virtual\nenvironment was created in which a handmade 3D environment is almost\nindistinguishable from an environment created with panorama images. Interactive\nelements were added and a user study was conducted to investigate the effect of\nboth environments to the user. The study conducted indicates that panorama\nimages can be a useful substitute for 3D modeled environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.09015,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Generative Design Inspiration for Glyphs with Diatoms\n\n  We introduce Diatoms, a technique that generates design inspiration for\nglyphs by sampling from palettes of mark shapes, encoding channels, and glyph\nscaffold shapes. Diatoms allows for a degree of randomness while respecting\nconstraints imposed by columns in a data table: their data types and domains as\nwell as semantic associations between columns as specified by the designer. We\npair this generative design process with two forms of interactive design\nexternalization that enable comparison and critique of the design alternatives.\nFirst, we incorporate a familiar small multiples configuration in which every\ndata point is drawn according to a single glyph design, coupled with the\nability to page between alternative glyph designs. Second, we propose a small\npermutables design gallery, in which a single data point is drawn according to\neach alternative glyph design, coupled with the ability to page between data\npoints. We demonstrate an implementation of our technique as an extension to\nTableau featuring three example palettes, and to better understand how Diatoms\ncould fit into existing design workflows, we conducted interviews and\nchauffeured demos with 12 designers. Finally, we reflect on our process and the\ndesigners' reactions, discussing the potential of our technique in the context\nof visualization authoring systems. Ultimately, our approach to glyph design\nand comparison can kickstart and inspire visualization design, allowing for the\nserendipitous discovery of shape and channel combinations that would have\notherwise been overlooked.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.10309,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Improving Visualization Interpretation Using Counterfactuals\n\n  Complex, high-dimensional data is used in a wide range of domains to explore\nproblems and make decisions. Analysis of high-dimensional data, however, is\nvulnerable to the hidden influence of confounding variables, especially as\nusers apply ad hoc filtering operations to visualize only specific subsets of\nan entire dataset. Thus, visual data-driven analysis can mislead users and\nencourage mistaken assumptions about causality or the strength of relationships\nbetween features. This work introduces a novel visual approach designed to\nreveal the presence of confounding variables via counterfactual possibilities\nduring visual data analysis. It is implemented in CoFact, an interactive\nvisualization prototype that determines and visualizes \\textit{counterfactual\nsubsets} to better support user exploration of feature relationships. Using\npublicly available datasets, we conducted a controlled user study to\ndemonstrate the effectiveness of our approach; the results indicate that users\nexposed to counterfactual visualizations formed more careful judgments about\nfeature-to-outcome relationships.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12548,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"KG4Vis: A Knowledge Graph-Based Approach for Visualization\n  Recommendation\n\n  Visualization recommendation or automatic visualization generation can\nsignificantly lower the barriers for general users to rapidly create effective\ndata visualizations, especially for those users without a background in data\nvisualizations. However, existing rule-based approaches require tedious manual\nspecifications of visualization rules by visualization experts. Other machine\nlearning-based approaches often work like black-box and are difficult to\nunderstand why a specific visualization is recommended, limiting the wider\nadoption of these approaches. This paper fills the gap by presenting KG4Vis, a\nknowledge graph (KG)-based approach for visualization recommendation. It does\nnot require manual specifications of visualization rules and can also guarantee\ngood explainability. Specifically, we propose a framework for building\nknowledge graphs, consisting of three types of entities (i.e., data features,\ndata columns and visualization design choices) and the relations between them,\nto model the mapping rules between data and effective visualizations. A\nTransE-based embedding technique is employed to learn the embeddings of both\nentities and relations of the knowledge graph from existing\ndataset-visualization pairs. Such embeddings intrinsically model the desirable\nvisualization rules. Then, given a new dataset, effective visualizations can be\ninferred from the knowledge graph with semantically meaningful rules. We\nconducted extensive evaluations to assess the proposed approach, including\nquantitative comparisons, case studies and expert interviews. The results\ndemonstrate the effectiveness of our approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04681,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000098017,
      "text":"A Survey on Personal Image Retrieval Systems\n\n  The number of photographs taken worldwide is growing rapidly and steadily.\nWhile a small subset of these images is annotated and shared by users through\nsocial media platforms, due to the sheer number of images in personal photo\nrepositories (shared or not shared), finding specific images remains\nchallenging. This survey explores existing image retrieval techniques as well\nas photo-organizer applications to highlight their relative strengths in\naddressing this challenge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04799,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"TEVISE: An Interactive Visual Analytics Tool to Explore Evolution of\n  Keywords' Relations in Tweet Data\n\n  Recently, a new window to explore tweet data has been opened in TExVis tool\nthrough visualizing the relations between the frequent keywords. However,\ntimeline exploration of tweet data, not present in TExVis, could play a\ncritical factor in understanding the changes in people's feedback and reaction\nover time. Targeting this, we present our visual analytics tool, called TEVisE.\nIt uses an enhanced adjacency matrix diagram to overcome the cluttering problem\nin TExVis and visualizes the evolution of frequent keywords and the relations\nbetween these keywords over time. We conducted two user studies to find answers\nof our two formulated research questions. In the first user study, we focused\non evaluating the used visualization layouts in both tools from the\nperspectives of common usability metrics and cognitive load theory. We found\nbetter accuracy in our TEVisE tool for tasks related to reading exploring\nrelations between frequent keywords. In the second study, we collected users'\nfeedback towards exploring the summary view and the new timeline evolution view\ninside TEVisE. In the second study, we collected users' feedback towards\nexploring the summary view and the new timeline evolution view inside TEVisE.\nWe found that participants preferred both view, one to get overall glance while\nthe other to get the trends changes over time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.12244,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"AutoGrad: Automated Grading Software for Mobile Game Assignments in\n  SuaCode Courses\n\n  Automatic grading systems have been in existence since the turn of the\nhalf-century. Several systems have been developed in the literature with either\nstatic analysis and dynamic analysis or a hybrid of both methodologies for\ncomputer science courses. This paper presents AutoGrad, a novel portable\ncross-platform automatic grading system for graphical Processing programs\ndeveloped on Android smartphones during an online course. AutoGrad uses\nProcessing, which is used in the emerging Interactive Media Arts, and pioneers\ngrading systems utilized outside the sciences to assist tuition in the Arts. It\nalso represents the first system built and tested in an African context across\nover thirty-five countries across the continent. This paper first explores the\ndesign and implementation of AutoGrad. AutoGrad employs APIs to download the\nassignments from the course platform, performs static and dynamic analysis on\nthe assignment to evaluate the graphical output of the program, and returns the\ngrade and feedback to the student. It then evaluates AutoGrad by analyzing data\ncollected from the two online cohorts of 1000+ students of our SuaCode\nsmartphone-based course. From the analysis and students' feedback, AutoGrad is\nshown to be adequate for automatic assessment, feedback provision to students,\nand easy integration for both cloud and standalone usage by reducing the time\nand effort required in grading the 4 assignments required to complete the\ncourse.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.03043,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Sequen-C: A Multilevel Overview of Temporal Event Sequences\n\n  Building a visual overview of temporal event sequences with an optimal\nlevel-of-detail (i.e. simplified but informative) is an ongoing challenge -\nexpecting the user to zoom into every important aspect of the overview can lead\nto missing insights. We propose a technique to build a multilevel overview of\nevent sequences, whose granularity can be transformed across sequence clusters\n(vertical level-of-detail) or longitudinally (horizontal level-of-detail),\nusing hierarchical aggregation and a novel cluster data representation\nAlign-Score-Simplify. By default, the overview shows an optimal number of\nsequence clusters obtained through the average silhouette width metric - then\nusers are able to explore alternative optimal sequence clusterings. The\nvertical level-of-detail of the overview changes along with the number of\nclusters, whilst the horizontal level-of-detail refers to the level of\nsummarization applied to each cluster representation. The proposed technique\nhas been implemented into a visualization system called Sequence Cluster\nExplorer (Sequen-C) that allows multilevel and detail-on-demand exploration\nthrough three coordinated views, and the inspection of data attributes at\ncluster, unique sequence, and individual sequence level. We present two case\nstudies using real-world datasets in the healthcare domain: CUREd and\nMIMIC-III; which demonstrate how the technique can aid users to obtain a\nsummary of common and deviating pathways, and explore data attributes for\nselected patterns.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09393,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"hEARt: Motion-resilient Heart Rate Monitoring with In-ear Microphones\n\n  With the soaring adoption of in-ear wearables, the research community has\nstarted investigating suitable in-ear heart rate (HR) detection systems. HR is\na key physiological marker of cardiovascular health and physical fitness.\nContinuous and reliable HR monitoring with wearable devices has therefore\ngained increasing attention in recent years. Existing HR detection systems in\nwearables mainly rely on photoplethysmography (PPG) sensors, however, these are\nnotorious for poor performance in the presence of human motion. In this work,\nleveraging the occlusion effect that enhances low-frequency bone-conducted\nsounds in the ear canal, we investigate for the first time \\textit{in-ear\naudio-based motion-resilient} HR monitoring. We first collected HR-induced\nsounds in the ear canal leveraging an in-ear microphone under stationary and\nthree different activities (i.e., walking, running, and speaking). Then, we\ndevised a novel deep learning based motion artefact (MA) mitigation framework\nto denoise the in-ear audio signals, followed by an HR estimation algorithm to\nextract HR. With data collected from 20 subjects over four activities, we\ndemonstrate that hEARt, our end-to-end approach, achieves a mean absolute error\n(MAE) of 3.02 $\\pm$ 2.97~BPM, 8.12 $\\pm$ 6.74~BPM, 11.23 $\\pm$ 9.20~BPM and\n9.39 $\\pm$ 6.97~BPM for stationary, walking, running and speaking,\nrespectively, opening the door to a new non-invasive and affordable HR\nmonitoring with usable performance for daily activities. Not only does hEARt\noutperform previous in-ear HR monitoring work, but it outperforms reported\nin-ear PPG performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.04367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"An Autonomous Driving System - Dedicated Vehicle for People with ASD and\n  their Caregivers\n\n  Automated driving system - dedicated vehicles (ADS-DVs), specially designed\nfor people with various disabilities, can be beneficial to improve their\nmobility. However, research related to autonomous vehicles (AVs) for people\nwith cognitive disabilities, especially Autism Spectrum Disorder (ASD) is\nlimited. Thus, in this study, we focused on the challenge that we framed: \"How\nmight we design an ADS-DV that benefits people with ASD and their caregivers?\".\nIn order to address the design challenge, we followed the human-centered design\nprocess. First, we conducted user research with caregivers of people with ASD.\nSecond, we identified their user needs, including safety, monitoring and\nupdates, individual preferences, comfort, trust, and reliability. Third, we\ngenerated a large number of ideas with brainstorming and affinity diagrams,\nbased on which we proposed an ADS-DV prototype with a mobile application and an\ninterior design. Fourth, we tested both the low-fidelity and high-fidelity\nprototypes to fix the possible issues. Our preliminary results showed that such\nan ASD-DV would potentially improve the mobility of those with ASD without\nworries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.01536,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"NudgeCred: Supporting News Credibility Assessment on Social Media\n  Through Nudges\n\n  Struggling to curb misinformation, social media platforms are experimenting\nwith design interventions to enhance consumption of credible news on their\nplatforms. Some of these interventions, such as the use of warning messages,\nare examples of nudges -- a choice-preserving technique to steer behavior.\nDespite their application, we do not know whether nudges could steer people\ninto making conscious news credibility judgments online and if they do, under\nwhat constraints. To answer, we combine nudge techniques with heuristic based\ninformation processing to design NudgeCred -- a browser extension for Twitter.\nNudgeCred directs users' attention to two design cues: authority of a source\nand other users' collective opinion on a report by activating three design\nnudges -- Reliable, Questionable, and Unreliable, each denoting particular\nlevels of credibility for news tweets. In a controlled experiment, we found\nthat NudgeCred significantly helped users (n=430) distinguish news tweets'\ncredibility, unrestricted by three behavioral confounds -- political ideology,\npolitical cynicism, and media skepticism. A five-day field deployment with\ntwelve participants revealed that NudgeCred improved their recognition of news\nitems and attention towards all of our nudges, particularly towards\nQuestionable. Among other considerations, participants proposed that designers\nshould incorporate heuristics that users' would trust. Our work informs\nnudge-based system design approaches for online media.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.01923,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Exploring Interactions Between Trust, Anthropomorphism, and Relationship\n  Development in Voice Assistants\n\n  Modern conversational agents such as Alexa and Google Assistant represent\nsignificant progress in speech recognition, natural language processing, and\nspeech synthesis. But as these agents have grown more realistic, concerns have\nbeen raised over how their social nature might unconsciously shape our\ninteractions with them. Through a survey of 500 voice assistant users, we\nexplore whether users' relationships with their voice assistants can be\nquantified using the same metrics as social, interpersonal relationships; as\nwell as if this correlates with how much they trust their devices and the\nextent to which they anthropomorphise them. Using Knapp's staircase model of\nhuman relationships, we find that not only can human-device interactions be\nmodelled in this way, but also that relationship development with voice\nassistants correlates with increased trust and anthropomorphism.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.03738,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning\n  Models\n\n  The interpretation of deep neural networks (DNNs) has become a key topic as\nmore and more people apply them to solve various problems and making critical\ndecisions. Concept-based explanations have recently become a popular approach\nfor post-hoc interpretation of DNNs. However, identifying human-understandable\nvisual concepts that affect model decisions is a challenging task that is not\neasily addressed with automatic approaches. We present a novel\nhuman-in-the-loop approach to generate user-defined concepts for model\ninterpretation and diagnostics. Central to our proposal is the use of active\nlearning, where human knowledge and feedback are combined to train a concept\nextractor with very little human labeling effort. We integrate this process\ninto an interactive system, ConceptExtract. Through two case studies, we show\nhow our approach helps analyze model behavior and extract human-friendly\nconcepts for different machine learning tasks and datasets and how to use these\nconcepts to understand the predictions, compare model performance and make\nsuggestions for model refinement. Quantitative experiments show that our active\nlearning approach can accurately extract meaningful visual concepts. More\nimportantly, by identifying visual concepts that negatively affect model\nperformance, we develop the corresponding data augmentation strategy that\nconsistently improves model performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10894,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Evaluating Effects of Background Stories on Graph Perception\n\n  A graph is an abstract model that represents relations among entities, for\nexample, the interactions between characters in a novel. A background story\nendows entities and relations with real-world meanings and describes the\nsemantics and context of the abstract model, for example, the actual story that\nthe novel presents. Considering practical experience and prior research, human\nviewers who are familiar with the background story of a graph and those who do\nnot know the background story may perceive the same graph differently. However,\nno previous research has adequately addressed this problem. This research paper\nthus presents an evaluation that investigated the effects of background stories\non graph perception. Three hypotheses that focused on the role of visual focus\nareas, graph structure identification, and mental model formation on graph\nperception were formulated and guided three controlled experiments that\nevaluated the hypotheses using real-world graphs with background stories. An\nanalysis of the resulting experimental data, which compared the performance of\nparticipants who read and did not read the background stories, obtained a set\nof instructive findings. First, having knowledge about a graph's background\nstory influences participants' focus areas during interactive graph\nexplorations. Second, such knowledge significantly affects one's ability to\nidentify community structures but not high degree and bridge structures. Third,\nthis knowledge influences graph recognition under blurred visual conditions.\nThese findings can bring new considerations to the design of storytelling\nvisualizations and interactive graph explorations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0515,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Exploring the Links between Personality Traits and Suscep;bility to\n  Disinformation\n\n  The growth of online Digital\/social media has allowed a variety of ideas and\nopinions to coexist. Social Media has appealed users due to the ease of fast\ndissemination of information at low cost and easy access. However, due to the\ngrowth in affordance of Digital platforms, users have become prone to consume\ndisinformation, misinformation, propaganda, and conspiracy theories. In this\npaper, we wish to explore the links between the personality traits given by the\nBig Five Inventory and their susceptibility to disinformation. More\nspeciDically, this study is attributed to capture the short- term as well as\nthe long-term effects of disinformation and its effects on the Dive personality\ntraits. Further, we expect to observe that different personalities traits have\ndifferent shifts in opinion and different increase or decrease of uncertainty\non an issue after consuming the disinformation. Based on the Dindings of this\nstudy, we would like to propose a personalized narrative-based change in\nbehavior for different personality traits.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10665,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"Sharing Practices for Datasets Related to Accessibility and Aging\n\n  Datasets sourced from people with disabilities and older adults play an\nimportant role in innovation, benchmarking, and mitigating bias for both\nassistive and inclusive AI-infused applications. However, they are scarce. We\nconduct a systematic review of 137 accessibility datasets manually located\nacross different disciplines over the last 35 years. Our analysis highlights\nhow researchers navigate tensions between benefits and risks in data collection\nand sharing. We uncover patterns in data collection purpose, terminology,\nsample size, data types, and data sharing practices across communities of\nfocus. We conclude by critically reflecting on challenges and opportunities\nrelated to locating and sharing accessibility datasets calling for technical,\nlegal, and institutional privacy frameworks that are more attuned to concerns\nfrom these communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.04401,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"A Framework of Severity for Harmful Content Online\n\n  The proliferation of harmful content on online social media platforms has\nnecessitated empirical understandings of experiences of harm online and the\ndevelopment of practices for harm mitigation. Both understandings of harm and\napproaches to mitigating that harm, often through content moderation, have\nimplicitly embedded frameworks of prioritization - what forms of harm should be\nresearched, how policy on harmful content should be implemented, and how\nharmful content should be moderated. To aid efforts of better understanding the\nvariety of online harms, how they relate to one another, and how to prioritize\nharms relevant to research, policy, and practice, we present a theoretical\nframework of severity for harmful online content. By employing a grounded\ntheory approach, we developed a framework of severity based on interviews and\ncard-sorting activities conducted with 52 participants over the course of ten\nmonths. Through our analysis, we identified four Types of Harm (physical,\nemotional, relational, and financial) and eight Dimensions along which the\nseverity of harm can be understood (perspectives, intent, agency, experience,\nscale, urgency, vulnerability, sphere). We describe how our framework can be\napplied to both research and policy settings towards deeper understandings of\nspecific forms of harm (e.g., harassment) and prioritization frameworks when\nimplementing policies encompassing many forms of harm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.11455,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Evaluating Students Perspectives on ICT Readiness in Somali Higher\n  Education towards Teaching -- Learning Acceptance\n\n  Along the rapid development of Information and communication technology (ICT)\ntools and growth of Internet access offer opportunities that facilitate\nteaching and learning activities in the context of higher education. However,\nthe study of ICTs readiness and acceptance in Somalia higher education is\nmeagre. This research aims to examine the current state of ICT readiness among\nuniversity students and explores the factors that affect their readiness\nacceptance. It proposes an extended model, based on the Technology Acceptance\nModel (TAM), which explains how University students beliefs influence their\nreadiness to accept ICT applications in their learning. Survey responses of 304\nstudents from undergraduate and Graduate in Somalia higher education were\ncollected and analyzed using structural equation modelling. The results of the\ndata analysis demonstrated that the TAM explained university students readiness\nacceptance of ICT applications reasonably well. More specifically, perceived\nusefulness, Ease of Use, ICT Selfefficacy, Teaching-Learning autonomy, Students\nOptimism and Availability of ICT infrastructure are robust predictors of\nStudents ICT readiness acceptance. Results also showed that internet\naffordability, network speed and quality, innovativeness, discomfort and\ninsecurity do not have a meaningful effect on perceived usefulness and Ease of\nUse towards ICT readiness acceptance. Through the empirical results, this study\nhelped us understand why students choose to engage in ICT applications for\ntheir learning context. Keywords: ICT readiness acceptance, Higher\neducation,Teaching- Learning, Technology Acceptance Model\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.00588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Measuring User Experience Inclusivity in Human-AI Interaction via Five\n  User Problem-Solving Styles\n\n  Motivations: Recent research has emerged on generally how to improve AI\nproduct user experiences, but relatively little is known about an AI product's\ninclusivity. For example, what kinds of users does it support well, and who\ndoes it leave out? And what changes in the product would make it more\ninclusive?\n  Objectives: Our overall objective is to help fill this gap, investigating\nwhat kinds of diverse users an AI product leaves out, and how to act upon that\nknowledge. To bring actionability to our findings, we focus on users' diversity\nof problem-solving attributes. Thus, our specific objectives were: (1) to\nreveal whether participants with diverse problem-solving styles were left\nbehind in a set of AI products; and (2) to relate participants' problem-solving\ndiversity to their demographic diversity, specifically, gender and age.\n  Methods: We performed 18 experiments, discarding two that failed manipulation\nchecks. Each experiment was a 2x2 factorial experiment with online\nparticipants. Each experiment compared two AI products: one deliberately\nviolating an HAI guideline and the other applying the guideline. For our first\nobjective, we analyzed how much each AI product gained\/lost inclusivity\ncompared to its counterpart, where inclusivity was supportiveness to\nparticipants with particular problem-solving styles. For our second objective,\nwe analyzed how participants' problem-solving styles aligned with their\ndemographics, namely their genders and ages.\n  Results & Implications: Participants' diverse problem-solving styles revealed\nsix types of inclusivity results: (1) the AI products that followed an HAI\nguideline were almost always more inclusive across diversity of problem-solving\nstyles than the products that did not follow that guideline-but the \"who\" that\ngot most of the inclusivity varied widely by guideline and by problem-solving\nstyle...\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02839,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Through the Looking Glass: Insights into Visualization Pedagogy through\n  Sentiment Analysis of Peer Review Text\n\n  Peer review is a widely utilized feedback mechanism for engaging students. As\na pedagogical method, it has been shown to improve educational outcomes, but we\nhave found limited empirical measurement of peer review in visualization\ncourses. In addition to increasing engagement, peer review provides diverse\nfeedback and reinforces recently-learned course concepts through critical\nevaluation of others' work. We discuss the construction and application of peer\nreview in two visualization courses from different colleges at the University\nof South Florida. We then analyze student projects and peer review text via\nsentiment analysis to infer insights for visualization educators, including the\nfocus of course content, engagement across student groups, student mastery of\nconcepts, course trends over time, and expert intervention effectiveness.\nFinally, we provide suggestions for adapting peer review to other visualization\ncourses to engage students and increase instructor understanding of the peer\nreview process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000098348,
      "text":"Exploring Head-based Mode-Switching in Virtual Reality\n\n  Mode-switching supports multilevel operations using a limited number of input\nmethods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches\nfor mode-switching use buttons, controllers, and users' hands. However, they\nare inefficient and challenging to do with tasks that require both hands (e.g.,\nwhen users need to use two hands during drawing operations). Using head\ngestures for mode-switching can be an efficient and cost-effective way,\nallowing for a more continuous and smooth transition between modes. In this\npaper, we explore the use of head gestures for mode-switching especially in\nscenarios when both users' hands are performing tasks. We present a first user\nstudy that evaluated eight head gestures that could be suitable for VR HMD with\na dual-hand line-drawing task. Results show that move forward, move backward,\nroll left, and roll right led to better performance and are preferred by\nparticipants. A second study integrating these four gestures in Tilt Brush, an\nopen-source painting VR application, is conducted to further explore the\napplicability of these gestures and derive insights. Results show that Tilt\nBrush with head gestures allowed users to change modes with ease and led to\nimproved interaction and user experience. The paper ends with a discussion on\nsome design recommendations for using head-based mode-switching in VR HMD.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09639,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Classifying In-Place Gestures with End-to-End Point Cloud Learning\n\n  Walking in place for moving through virtual environments has attracted\nnoticeable attention recently. Recent attempts focused on training a classifier\nto recognize certain patterns of gestures (e.g., standing, walking, etc) with\nthe use of neural networks like CNN or LSTM. Nevertheless, they often consider\nvery few types of gestures and\/or induce less desired latency in virtual\nenvironments. In this paper, we propose a novel framework for accurate and\nefficient classification of in-place gestures. Our key idea is to treat several\nconsecutive frames as a \"point cloud\". The HMD and two VIVE trackers provide\nthree points in each frame, with each point consisting of 12-dimensional\nfeatures (i.e., three-dimensional position coordinates, velocity, rotation,\nangular velocity). We create a dataset consisting of 9 gesture classes for\nvirtual in-place locomotion. In addition to the supervised point-based network,\nwe also take unsupervised domain adaptation into account due to inter-person\nvariations. To this end, we develop an end-to-end joint framework involving\nboth a supervised loss for supervised point learning and an unsupervised loss\nfor unsupervised domain adaptation. Experiments demonstrate that our approach\ngenerates very promising outcomes, in terms of high overall classification\naccuracy (95.0%) and real-time performance (192ms latency). Our code will be\npublicly available at: https:\/\/github.com\/ZhaoLizz\/PCT-MCD.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.06035,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Narrative Sensemaking: Strategies for Narrative Maps Construction\n\n  Narrative sensemaking is a fundamental process to understand sequential\ninformation. Narrative maps are a visual representation framework that can aid\nanalysts in this process. They allow analysts to understand the big picture of\na narrative, uncover new relationships between events, and model connections\nbetween storylines. As a sensemaking tool, narrative maps have applications in\nintelligence analysis, misinformation modeling, and computational journalism.\nIn this work, we seek to understand how analysts construct narrative maps in\norder to improve narrative map representation and extraction methods. We\nperform an experiment with a data set of news articles. Our main contribution\nis an analysis of how analysts construct narrative maps. The insights extracted\nfrom our study can be used to design narrative map visualizations, extraction\nalgorithms, and visual analytics tools to support the sensemaking process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0092,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Interactive Visual Facets to Support Fluid Exploratory Search\n\n  Exploratory search starts with ill-defined goals and involves browsing,\nlearning, and formulating new targets for search. To fluidly support such\ndynamic search behaviours, we focus on devising interactive visual facets\n(IVF), visualising information facets to support user comprehension and control\nof the information space. To do this, we reviewed existing faceted search\ninterfaces and derived two design requirements (DR) that have not been fully\naddressed to support fluid interactions in exploratory search. We then\nexemplified the requirements through devising an IVF tool, which coordinates a\nlinear and a categorical facet representing the distribution and summarisation\nof items, respectively, and providing context for faceted exploration (DR1). To\nsupport rapid transitions between search criteria (DR2), the tool introduces a\nnovel design concept of using facets to select items without filtering the item\nspace. Particularly, we propose a filter-swipe technique that enables users to\ndrag a categorical facet value sequentially over linear facet bars to view the\nitems in the intersection of the two facets along with the categorical facet\ndynamically summarizing the items in the interaction. Three applications\ndemonstrate how the features support information discovery with ease. A user\nstudy of 11 participants with realistic email search tasks shows that dynamic\nsuggestions through the timeline navigation can help discover useful\nsuggestions for search; the novel design concept was favoured over using facet\nvalues as filters. Based on these practices, we derive IVF design implications\nfor fluid, exploratory searches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.04931,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"Toward Systematic Considerations of Missingness in Visual Analytics\n\n  Data-driven decision making has been a common task in today's big data era,\nfrom simple choices such as finding a fast way to drive home, to complex\ndecisions on medical treatment. It is often supported by visual analytics. For\nvarious reasons (e.g., system failure, interrupted network, intentional\ninformation hiding, or bias), visual analytics for sensemaking of data involves\nmissingness (e.g., data loss and incomplete analysis), which impacts human\ndecisions. For example, missing data can cost a business millions of dollars,\nand failing to recognize key evidence can put an innocent person in jail. Being\naware of missingness is critical to avoid such catastrophes. To fulfill this,\nas an initial step, we consider missingness in visual analytics from two\naspects: data-centric and human-centric. The former emphasizes missingness in\nthree data-related categories: data composition, data relationship, and data\nusage. The latter focuses on the human-perceived missingness at three levels:\nobserved-level, inferred-level, and ignored-level. Based on them, we discuss\npossible roles of visualizations for handling missingness, and conclude our\ndiscussion with future research opportunities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.1239,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Two-In-One: A Design Space for Mapping Unimanual Input into Bimanual\n  Interactions in VR for Users with Limited Movement\n\n  Virtual Reality (VR) applications often require users to perform actions with\ntwo hands when performing tasks and interacting with objects in virtual\nenvironments. Although bimanual interactions in VR can resemble real-world\ninteractions -- thus increasing realism and improving immersion -- they can\nalso pose significant accessibility challenges to people with limited mobility,\nsuch as for people who have full use of only one hand. An opportunity exists to\ncreate accessible techniques that take advantage of users' abilities, but\ndesigners currently lack structured tools to consider alternative approaches.\nTo begin filling this gap, we propose Two-in-One, a design space that\nfacilitates the creation of accessible methods for bimanual interactions in VR\nfrom unimanual input. Our design space comprises two dimensions, bimanual\ninteractions and computer assistance, and we provide a detailed examination of\nissues to consider when creating new unimanual input techniques that map to\nbimanual interactions in VR. We used our design space to create three\ninteraction techniques that we subsequently implemented for a subset of\nbimanual interactions and received user feedback through a video elicitation\nstudy with 17 people with limited mobility. Our findings explore complex\ntradeoffs associated with autonomy and agency and highlight the need for\nadditional settings and methods to make VR accessible to people with limited\nmobility.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09448,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000098348,
      "text":"Thing Constellation Visualizer: Exploring Emergent Relationships of\n  Everyday Objects\n\n  Designing future IoT ecosystems requires new approaches and perspectives to\nunderstand everyday practices. While researchers recognize the importance of\nunderstanding social aspects of everyday objects, limited studies have explored\nthe possibilities of combining data-driven patterns with human interpretations\nto investigate emergent relationships among objects. This work presents Thing\nConstellation Visualizer (thingCV), a novel interactive tool for visualizing\nthe social network of objects based on their co-occurrence as computed from a\nlarge collection of photos. ThingCV enables perspective-changing design\nexplorations over the network of objects with scalable links. Two exploratory\nworkshops were conducted to investigate how designers navigate and make sense\nof a network of objects through thingCV. The results of eight participants\nshowed that designers were actively engaged in identifying interesting objects\nand their associated clusters of related objects. The designers projected\nsocial qualities onto the identified objects and their communities.\nFurthermore, the designers changed their perspectives to revisit familiar\ncontexts and to generate new insights through the exploration process. This\nwork contributes a novel approach to combining data-driven models with\ndesignerly interpretations of thing constellation towards More-Than\nHuman-Centred Design of IoT ecosystems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.03631,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000143713,
      "text":"Renovo: Sensor-Based Visual Assistive Technology for Physiotherapists in\n  the Rehabilitation of Stroke Patients with Upper Limb Motor Impairments\n\n  Stroke patients with upper limb motor impairments are re-acclimated to their\ncorresponding motor functionalities through therapeutic interventions.\nPhysiotherapists typically assess these functionalities using various\nqualitative protocols. However, such assessments are often biased and prone to\nerrors, reducing rehabilitation efficacy. Therefore, real-time visualization\nand quantitative analysis of performance metrics, such as range of motion,\nrepetition rate, velocity, etc., are crucial for accurate progress assessment.\nThis study introduces Renovo, a working prototype of a wearable motion\nsensor-based assistive technology that assists physiotherapists with real-time\nvisualization of these metrics. We also propose a novel mathematical framework\nfor generating quantitative performance scores without relying on any machine\nlearning model. We present the results of a three-week pilot study involving 16\nstroke patients with upper limb disabilities, evaluated across three successive\nsessions at one-week intervals by both Renovo and physiotherapists (N=5).\nResults suggest that while the expertise of a physiotherapist is irreplaceable,\nRenovo can assist in the decision-making process by providing valuable\nquantitative information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.12055,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000043048,
      "text":"Using Physiological Information to Classify Task Difficulty in\n  Human-Swarm Interaction\n\n  Human-swarm interaction has recently gained attention due to its plethora of\nnew applications in disaster relief, surveillance, rescue, and exploration.\nHowever, if the task difficulty increases, the performance of the human\noperator decreases, thereby decreasing the overall efficacy of the human-swarm\nteam. Thus, it is critical to identify the task difficulty and adaptively\nallocate the task to the human operator to maintain optimal performance. In\nthis direction, we study the classification of task difficulty in a human-swarm\ninteraction experiment performing a target search mission. The human may\ncontrol platoons of unmanned aerial vehicles (UAVs) and unmanned ground\nvehicles (UGVs) to search a partially observable environment during the target\nsearch mission. The mission complexity is increased by introducing adversarial\nteams that humans may only see when the environment is explored. While the\nhuman is completing the mission, their brain activity is recorded using an\nelectroencephalogram (EEG), which is used to classify the task difficulty. We\nhave used two different approaches for classification: A feature-based approach\nusing coherence values as input and a deep learning-based approach using raw\nEEG as input. Both approaches can classify the task difficulty well above the\nchance. The results showed the importance of the occipital lobe (O1 and O2)\ncoherence feature with the other brain regions. Moreover, we also study\nindividual differences (expert vs. novice) in the classification results. The\nanalysis revealed that the temporal lobe in experts (T4 and T3) is predominant\nfor task difficulty classification compared with novices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10412,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000288751,
      "text":"Social, Environmental, and Technical: Factors at Play in the Current Use\n  and Future Design of Small-Group Captioning\n\n  Real-time captioning is a critical accessibility tool for many d\/Deaf and\nhard of hearing (DHH) people. While the vast majority of captioning work has\nfocused on formal settings and technical innovations, in contrast, we\ninvestigate captioning for informal, interactive small-group conversations,\nwhich have a high degree of spontaneity and foster dynamic social interactions.\nThis paper reports on semi-structured interviews and design probe activities we\nconducted with 15 DHH participants to understand their use of existing\nreal-time captioning services and future design preferences for both in-person\nand remote small-group communication. We found that our participants'\nexperiences of captioned small-group conversations are shaped by social,\nenvironmental, and technical considerations (e.g., interlocutors'\npre-established relationships, the type of captioning displays available, and\nhow far captions lag behind speech). When considering future captioning tools,\nparticipants were interested in greater feedback on non-speech elements of\nconversation (e.g., speaker identity, speech rate, volume) both for their\npersonal use and to guide hearing interlocutors toward more accessible\ncommunication. We contribute a qualitative account of DHH people's real-time\ncaptioning experiences during small-group conversation and future design\nconsiderations to better support the groups being captioned, both in person and\nonline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.01186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Exploratory Design of a Hands-free Video Game Controller for a\n  Quadriplegic Individual\n\n  From colored pixels to hyper-realistic 3D landscapes of virtual reality,\nvideo games have evolved immensely over the last few decades. However, video\ngame input still requires two-handed dexterous finger manipulations for\nsimultaneous joystick and trigger or mouse and keyboard presses. In this work,\nwe explore the design of a hands-free game control method using realtime facial\nexpression recognition for individuals with neurological and neuromuscular\ndiseases who are unable to use traditional game controllers. Similar to other\nAssistive Technologies (AT), our facial input technique is also designed and\ntested in collaboration with a graduate student who has Spinal Muscular\nAtrophy. Our preliminary evaluation shows the potential of facial expression\nrecognition for augmenting the lives of quadriplegic individuals by enabling\nthem to accomplish things like walking, running, flying or other adventures\nthat may not be so attainable otherwise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.13778,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000044372,
      "text":"Data-driven insight into the puzzle-based cybersecurity training\n\n  Puzzle-based training is a common type of hands-on activity accompanying\nformal and informal cybersecurity education, much like programming or other IT\nskills. However, there is a lack of tools to help the educators with the\npost-training data analysis.\n  Through a visualization design study, we designed the Training Analysis Tool\nthat supports learning analysis of a single hands-on session. It allows an\nin-depth trainee comparison and enables the identification of flaws in puzzle\nassignments. We also performed a qualitative evaluation with cybersecurity\nexperts and students. The participants apprised the positive influence of the\ntool on their workflows. Our insights and recommendations could aid the design\nof future tools supporting educators, even beyond cyber security.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.03016,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000068214,
      "text":"SpatialViewer: A Remote Work Sharing Tool that Considers Intimacy Among\n  Workers\n\n  Due to the influence of the new coronavirus disease (COVID-19), teleworking\nhas been expanding rapidly. Although existing interactive remote working\nsystems are convenient, they do not allow users to adjust their spatial\ndistance to team members at will, %\"Arbitrarily\" is probably not the best word\nhere. It means without apparent reason. A better expression might be \"at will.\"\nand they ignore the discomfort caused by different levels of intimacy. To solve\nthis issue, we propose a telework support system using spatial augmented\nreality technology. This system calibrates the space in which videos are\nprojected with real space and adjusts the spatial distance between users by\nchanging the position of projections. Users can switch the projection position\nof the video using hand-wave gestures. We also synchronize audio according to\ndistance to further emphasize the sense of space within the remote interaction:\nthe distance between projection position and user is inversely proportional to\nthe audio volume. We conducted a telework experiment and a questionnaire survey\nto evaluate our system. The results show that the system enables users to\nadjust distance according to intimacy and thus improve the users' comfort.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08723,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Understanding the Effects of Visualizing Missing Values on Visual Data\n  Exploration\n\n  When performing data analysis, people often confront data sets containing\nmissing values. We conducted an empirical study to understand the effects of\nvisualizing those missing values on participants' decision-making processes\nwhile performing a visual data exploration task. More specifically, our study\nparticipants purchased a hypothetical portfolio of stocks based on a dataset\nwhere some stocks had missing values for attributes such as PE ratio, beta, and\nEPS. The experiment used scatterplots to communicate the stock data. For one\ngroup of participants, stocks with missing values simply were not shown, while\nthe second group saw such stocks depicted with estimated values as points with\nerror bars. We measured participants' cognitive load involved in\ndecision-making with data with missing values. Our results indicate that their\ndecision-making workflow was different across two conditions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.03013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"SketchMeHow: Interactive Projection Guided Task Instruction with User\n  Sketches\n\n  In this work, we propose an interactive general instruction framework\nSketchMeHow to guidance the common users to complete the daily tasks in\nreal-time. In contrast to the conventional augmented reality-based instruction\nsystems, the proposed framework utilizes the user sketches as system inputs to\nacquire the users' production intentions from the drawing interfaces. Given the\nuser sketches, the designated task instruction can be analyzed based on the\nsub-task division and spatial localization for each task. The projector-camera\nsystem is adopted in the projection guidance to the end-users with the spatial\naugmented reality technology. To verify the proposed framework, we conducted\ntwo case studies of domino arrangement and bento production. From our user\nstudies, the proposed systems can help novice users complete the tasks\nefficiently with user satisfaction. We believe the proposed SketchMeHow can\nbroaden the research topics in sketch-based real-world applications in\nhuman-computer interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14965,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Bridging Social Distance During Social Distancing: Exploring Social Talk\n  and Remote Collegiality in Video Conferencing\n\n  Video conferencing systems have long facilitated work-related conversations\namong remote teams. However, social distancing due to the COVID-19 pandemic has\nforced colleagues to use video conferencing platforms to additionally fulfil\nsocial needs. Social talk, or informal talk, is an important workplace practice\nthat is used to build and maintain bonds in everyday interactions among\ncolleagues. Currently, there is a limited understanding of how video\nconferencing facilitates multiparty social interactions among colleagues. In\nour paper, we examine social talk practices during the COVID-19 pandemic among\nremote colleagues through semi-structured interviews. We uncovered three key\nthemes in our interviews, discussing 1) the changing purposes and opportunities\nafforded by using video conferencing for social talk with colleagues, 2) how\nthe nature of existing relationships and status of colleagues influences social\nconversations and 3) the challenges and changing conversational norms around\npoliteness and etiquette when using video conferencing to hold social\nconversations. We discuss these results in relation to the impact that video\nconferencing tools have on remote social talk between colleagues and outline\ndesign and best practice considerations for multiparty videoconferencing social\ntalk in the workplace.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14185,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"RelicVR: A Virtual Reality Game for Active Exploration of Archaeological\n  Relics\n\n  Digitalization is changing how people visit museums and explore the artifacts\nthey house. Museums, as important educational venues outside classrooms, need\nto actively explore the application of digital interactive media, including\ngames that can balance entertainment and knowledge acquisition. In this paper,\nwe introduce RelicVR, a virtual reality (VR) game that encourages players to\ndiscover artifacts through physical interaction in a game-based approach.\nPlayers need to unearth artifacts hidden in a clod enclosure by using available\ntools and physical movements. The game relies on the dynamic voxel deformation\ntechnique to allow players to chip away earth covering the artifacts. We added\nuncertainty in the exploration process to bring it closer to how archaeological\ndiscovery happens in real life. Players do not know the shape or features of\nthe hidden artifact and have to take away the earth gradually but strategically\nwithout hitting the artifact itself. From playtesting sessions with eight\nparticipants, we found that the uncertainty elements are conducive to their\nengagement and exploration experience. Overall, RelicVR is an innovative game\nthat can improve players' learning motivation and outcomes of ancient\nartifacts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.04791,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"ANTASID: A Novel Temporal Adjustment to Shannon's Index of Difficulty\n  for Quantifying the Perceived Difficulty of Uncontrolled Pointing Tasks\n\n  Shannon's Index of Difficulty ($ID$), reputable for quantifying the perceived\ndifficulty of pointing tasks as a logarithmic relationship between\nmovement-amplitude ($A$) and target-width ($W$), is used for modelling the\ncorresponding observed movement-times ($MT_O$) in such tasks in controlled\nexperimental setup. However, real-life pointing tasks are both spatially and\ntemporally uncontrolled, being influenced by factors such as - human aspects,\nsubjective behavior, the context of interaction, the inherent speed-accuracy\ntrade-off where, emphasizing accuracy compromises speed of interaction and vice\nversa, and so on. Effective target-width ($W_e$) is considered as spatial\nadjustment for compensating accuracy. However, no significant adjustment exists\nin the literature for compensating speed in different contexts of interaction\nin these tasks. As a result, without any temporal adjustment, the true\ndifficulty of an uncontrolled pointing task may be inaccurately quantified\nusing Shannon's ID. To verify this, we propose the ANTASID (A Novel Temporal\nAdjustment to Shannon's ID) formulation with detailed performance analysis. We\nhypothesized a temporal adjustment factor ($t$) as a binary logarithm of\n$MT_O$, compensating for speed due to contextual differences and minimizing the\nnon-linearity between movement-amplitude and target-width. Considering spatial\nand\/or temporal adjustments to ID, we conducted regression analysis using our\nown and Benchmark datasets in both controlled and uncontrolled scenarios of\npointing tasks with a generic mouse.ANTASID formulation showed significantly\nsuperior fitness values and throughput in all the scenarios while reducing the\nstandard error. Furthermore, the quantification of ID with ANTASID varied\nsignificantly compared to the classical formulations of Shannon's ID,\nvalidating the purpose of this study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.13334,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"On deploying the Artificial Sport Trainer into practice\n\n  Computational Intelligence methods for automatic generation of sport training\nplans in individual sport disciplines have achieved a mature phase. In order to\nconfirm their added value, they have been deployed into practice. As a result,\nseveral methods have been developed for generating well formulated training\nplans on computers automatically that, typically, depend on the collection of\npast sport activities. However, monitoring the realization of the performed\ntraining sessions still represents a bottleneck in automating the process of\nsport training as a whole. The objective of this paper is to present a new\nlow-cost and efficient embedded device for monitoring the realization of sport\ntraining sessions that is dedicated to monitor cycling training sessions. We\ndesigned and developed a new bike computer, i.e. the AST-Monitor, that can be\nmounted easily on almost every bicycle. The aforementioned bike computer is\nbased on the Raspberry Pi device that supports different external sensors for\ncapturing the data during the realization of sport training sessions. An\nadjusted GUI tailored to the needs of athletes is developed, along with the\nhardware. The proof of concept study, using the AST-Monitor in practice,\nrevealed the potential of the proposed solution for monitoring of realized\nsport training sessions automatically. The new device also opens the door for\nthe future utilization of Artificial Intelligence in a wide variety of sports.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11365,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Tumera: Tutor of Photography Beginners\n\n  With the popularity of photographic equipment, more and more people are\nstarting to learn photography by themselves. Although they have easy access to\nphotographic materials, it is uneasy to obtain professional feedback or\nguidance that can help them improve their photography skills. Therefore, we\ndevelop an intelligently interactive system, Tumera, that provides aesthetics\nguidance for photography beginners. When shooting, Tumera gives timely feedback\non the pictures in the view port. After shooting, scores evaluating the\naesthetic quality of different aspects of the photos and corresponding\nimprovement suggestions are given. Tumera allows users to share, rank, discuss,\nand learn from their works and interaction with the system based on the scores\nand suggestions. In the experiment, Tumera showed good accuracy, real-time\ncomputing ability, and effective guiding performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08183,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000119209,
      "text":"Trust in Prediction Models: a Mixed-Methods Pilot Study on the Impact of\n  Domain Expertise\n\n  People's trust in prediction models can be affected by many factors,\nincluding domain expertise like knowledge about the application domain and\nexperience with predictive modelling. However, to what extent and why domain\nexpertise impacts people's trust is not entirely clear. In addition, accurately\nmeasuring people's trust remains challenging. We share our results and\nexperiences of an exploratory pilot study in which four people experienced with\npredictive modelling systematically explore a visual analytics system with an\nunknown prediction model. Through a mixed-methods approach involving\nLikert-type questions and a semi-structured interview, we investigate how\npeople's trust evolves during their exploration, and we distil six themes that\naffect their trust in the prediction model. Our results underline the\nmulti-faceted nature of trust, and suggest that domain expertise alone cannot\nfully predict people's trust perceptions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.12976,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Retrofitting Meetings for Psychological Safety\n\n  Meetings are the fuel of organizations' productivity. At times, however, they\nare perceived as wasteful vaccums that deplete employee morale and\nproductivity. Current meeting tools, to a great extent, have simplified and\naugmented the ways meetings are conducted by enabling participants to ``get\nthings done'' and experience a comfortable physical environment. However, an\nimportant yet less explored element of these tools' design space is that of\npsychological safety -- the extent to which participants feel listened to, or\nmotivated to be part of a meeting. We argue that an interdisciplinary approach\nwould benefit the creation of new tools designed for retrofitting meetings for\npsychological safety. This approach comes with not only research opportunities\n-- ranging from sensing to modeling to user interface design -- but also\nchallenges -- ranging from privacy to workplace surveillance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.04995,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"A Systematic Review of Extended Reality (XR) for Understanding and\n  Augmenting Vision Loss\n\n  Over the past decade, extended reality (XR) has emerged as an assistive\ntechnology not only to augment residual vision of people losing their sight but\nalso to study the rudimentary vision restored to blind people by a visual\nneuroprosthesis. To make the best use of these emerging technologies, it is\nvaluable and timely to understand the state of this research and identify any\nshortcomings that are present. Here we present a systematic literature review\nof 227 publications from 106 different venues assessing the potential of XR\ntechnology to further visual accessibility. In contrast to other reviews, we\nsample studies from multiple scientific disciplines, focus on augmentation of a\nperson's residual vision, and require studies to feature a quantitative\nevaluation with appropriate end users. We summarize prominent findings from\ndifferent XR research areas, show how the landscape has changed over the last\ndecade, and identify scientific gaps in the literature. Specifically, we\nhighlight the need for real-world validation, the broadening of end-user\nparticipation, and a more nuanced understanding of the suitability and\nusability of different XR-based accessibility aids. By broadening end-user\nparticipation to early stages of the design process and shifting the focus from\nbehavioral performance to qualitative assessments of usability, future research\nhas the potential to develop XR technologies that may not only allow for\nstudying vision loss, but also enable novel visual accessibility aids with the\npotential to impact the lives of millions of people living with vision loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11424,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Big Data driven Product Design: A Survey\n\n  With the improvement of living standards, user requirements of modern\nproducts are becoming increasingly more diversified and personalized.\nTraditional product design methods can no longer satisfy the market needs due\nto their strong subjectivity, small survey scope, poor real-time data, and lack\nof visual display, which calls for the development of big data driven product\ndesign methodology. Big data in the product lifecycle contains valuable\ninformation for guiding product design, such as customer preferences, market\ndemands, product evaluation, and visual display: online product reviews reflect\ncustomer evaluations and requirements; product images contain information of\nshape,color, and texture which can inspire designers to get initial design\nschemes more quickly or even directly generate new product images. How to\nefficiently collect product design related data and exploit them effectively\nduring the whole product design process is thus critical to modern product\ndesign. This paper aims to conduct a comprehensive survey on big data driven\nproduct design. It will help researchers and practitioners to comprehend the\nlatest development of relevant studies and applications centered on how big\ndata can be processed, analyzed, and exploited in aiding product design. We\nfirst introduce several representative traditional product design methods and\nhighlight their limitations. Then we discuss current and potential applications\nof textual data, image data, audio data, and video data in product design\ncycles. Finally, major deficiencies of existing data driven product design\nstudies and future research directions are summarized. We believe that this\nstudy can draw increasing attention to modern data driven product design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.09618,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Automatic Y-axis Rescaling in Dynamic Visualizations\n\n  Animated and interactive data visualizations dynamically change the data\nrendered in a visualization (e.g., bar chart). As the data changes, the y-axis\nmay need to be rescaled as the domain of the data changes. Each axis rescaling\npotentially improves the readability of the current chart, but may also\ndisorient the user. In contrast to static visualizations, where there is\nconsiderable literature to help choose the appropriate y-axis scale, there is a\nlack of guidance about how and when rescaling should be used in dynamic\nvisualizations. Existing visualization systems and libraries adapt a fixed\nglobal y-axis, or rescale every time the data changes. Yet, professional\nvisualizations, such as in data journalism, do not adopt either strategy. They\ninstead carefully and manually choose when to rescale based on the analysis\ntask and data. To this end, we conduct a series of Mechanical Turk experiments\nto study the potential of dynamic axis rescaling and the factors that affect\nits effectiveness. We find that the appropriate rescaling policy is both task-\nand data-dependent, and we do not find one clear policy choice for all\nsituations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.03479,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000438425,
      "text":"VideoModerator: A Risk-aware Framework for Multimodal Video Moderation\n  in E-Commerce\n\n  Video moderation, which refers to remove deviant or explicit content from\ne-commerce livestreams, has become prevalent owing to social and engaging\nfeatures. However, this task is tedious and time consuming due to the\ndifficulties associated with watching and reviewing multimodal video content,\nincluding video frames and audio clips. To ensure effective video moderation,\nwe propose VideoModerator, a risk-aware framework that seamlessly integrates\nhuman knowledge with machine insights. This framework incorporates a set of\nadvanced machine learning models to extract the risk-aware features from\nmultimodal video content and discover potentially deviant videos. Moreover,\nthis framework introduces an interactive visualization interface with three\nviews, namely, a video view, a frame view, and an audio view. In the video\nview, we adopt a segmented timeline and highlight high-risk periods that may\ncontain deviant information. In the frame view, we present a novel visual\nsummarization method that combines risk-aware features and video context to\nenable quick video navigation. In the audio view, we employ a storyline-based\ndesign to provide a multi-faceted overview which can be used to explore audio\ncontent. Furthermore, we report the usage of VideoModerator through a case\nscenario and conduct experiments and a controlled user study to validate its\neffectiveness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10899,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Learning Geometric Transformations for Parametric Design: An Augmented\n  Reality (AR)-Powered Approach\n\n  Despite the remarkable development of parametric modeling methods for\narchitectural design, a significant problem still exists, which is the lack of\nknowledge and skill regarding the professional implementation of parametric\ndesign in architectural modeling. Considering the numerous advantages of\ndigital\/parametric modeling in rapid prototyping and simulation most\ninstructors encourage students to use digital modeling even from the early\nstages of design; however, an appropriate context to learn the basics of\ndigital design thinking is rarely provided in architectural pedagogy. This\npaper presents an educational tool, specifically an Augmented Reality (AR)\nintervention, to help students understand the fundamental concepts of\npara-metric modeling before diving into complex parametric modeling platforms.\nThe goal of the AR intervention is to illustrate geometric transformation and\nthe associated math functions so that students learn the mathematical logic\nbehind the algorithmic thinking of parametric modeling. We have developed\nBRICKxAR_T, an educational AR prototype, that intends to help students learn\ngeometric transformations in an immersive spatial AR environment. A LEGO set is\nused within the AR intervention as a physical manipulative to support physical\ninteraction and im-prove spatial skill through body gesture.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.07806,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000419882,
      "text":"Communicating Patient Health Data: A Wicked Problem\n\n  Designing patient-collected health data visualizations to support discussing\npatient data during clinical visits is a challenging problem due to the\nheterogeneity of the parties involved: patients, healthcare providers, and\nhealthcare systems. Designers must ensure that all parties' needs are met. This\ncomplexity makes it challenging to find a definitive solution that can work for\nevery individual. We have approached this research problem -- communicating\npatient data during clinical visits -- as a wicked problem. In this article, we\noutline how wicked problem characteristics apply to our research problem. We\nthen describe the research methodologies we employed to explore the design\nspace of individualized patient data visualization solutions. Last, we reflect\non the insights and experiences we gained through this exploratory design\nprocess. We conclude with a call to action for researchers and visualization\ndesigners to consider patients' and healthcare providers' individualities when\ndesigning patient data visualizations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11986,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000107951,
      "text":"Local, Interactive, and Actionable: a Pandemic Behavioral Nudge\n\n  The informational environment surrounding the Covid-19 pandemic has been\nwidely recognized as fragmented, politicized, and complex [1]. This has\nresulted in polarized public views regarding the veracity of scientific\ncommunication, the severity of the threat posed by the virus, and the necessity\nof nonpharmaceutical interventions (NPIs) which can slow the spread of\ninfections [2]. This paper describes CovidCommitment.org, an effort toward\nenhancing NPI adoption through the combination of a social behavioral\ncommitment device and interactive map-based visualizations of localized\ninfection data as tabulated via a 1-hourdrive-time isochrone. This paper\ndescribes the system design and presents a preliminary analysis of user\nbehavior within the system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11744,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Critiquing-based Modeling of Subjective Preferences\n\n  Applications designed for entertainment and other non-instrumental purposes\nare challenging to optimize because the relationships between system parameters\nand user experience can be unclear. Ideally, we would crowdsource these design\nquestions, but existing approaches are geared towards evaluation or ranking\ndiscrete choices and not for optimizing over continuous parameter spaces. In\naddition, users are accustomed to informally expressing opinions about\nexperiences as critiques (e.g. it's too cold, too spicy, too big), rather than\ngiving precise feedback as an optimization algorithm would require.\nUnfortunately, it can be difficult to analyze qualitative feedback, especially\nin the context of quantitative modeling. In this article, we present collective\ncriticism, a critiquing-based approach for modeling relationships between\nsystem parameters and subjective preferences. We transform critiques, such as\n\"it was too easy\/too challenging\", into censored intervals and analyze them\nusing interval regression. Collective criticism has several advantages over\nother approaches: \"too much\/too little\"-style feedback is intuitive for users\nand allows us to build predictive models for the optimal parameterization of\nthe variables being critiqued. We present two studies where we model: (i)\naesthetic preferences for images generated with neural style transfer, and (ii)\nusers' experiences of challenge in the video game Tetris. These studies\ndemonstrate the flexibility of our approach, and show that it produces robust\nresults that are straightforward to interpret and inline with users' stated\npreferences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.02597,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Cookie Banners, What's the Purpose? Analyzing Cookie Banner Text Through\n  a Legal Lens\n\n  A cookie banner pops up when a user visits a website for the first time,\nrequesting consent to the use of cookies and other trackers for a variety of\npurposes. Unlike prior work that has focused on evaluating the user interface\n(UI) design of cookie banners, this paper presents an in-depth analysis of what\ncookie banners say to users to get their consent. We took an interdisciplinary\napproach to determining what cookie banners should say. Following the legal\nrequirements of the ePrivacy Directive (ePD) and the General Data Protection\nRegulation (GDPR), we manually annotated around 400 cookie banners presented on\nthe most popular English-speaking websites visited by users residing in the EU.\nWe focused on analyzing the purposes of cookie banners and how these purposes\nwere expressed (e.g., any misleading or vague language, any use of jargon). We\nfound that 89% of cookie banners violated applicable laws. In particular, 61%\nof banners violated the purpose specificity requirement by mentioning vague\npurposes, including \"user experience enhancement\". Further, 30% of banners used\npositive framing, breaching the freely given and informed consent requirements.\nBased on these findings, we provide recommendations that regulators can find\nuseful. We also describe future research directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08584,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000196364,
      "text":"Surveying Wonderland for many more literature visualization techniques\n\n  There are still many potential literature visualizations to be discovered. By\nfocusing on a single text, the author surveys many existing visualizations\nacross research domains, in the wild, and creates new visualizations. 58\ntechniques are indicated, suggesting a wider variety of visualizations beyond\nresearch disciplines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Towards Automatic Grading of D3.js Visualizations\n\n  Manually grading D3 data visualizations is a challenging endeavor, and is\nespecially difficult for large classes with hundreds of students. Grading an\ninteractive visualization requires a combination of interactive, quantitative,\nand qualitative evaluation that are conventionally done manually and are\ndifficult to scale up as the visualization complexity, data size, and number of\nstudents increase. We present a first-of-its kind automatic grading method for\nD3 visualizations that scalably and precisely evaluates the data bindings,\nvisual encodings, interactions, and design specifications used in a\nvisualization. Our method has shown potential to enhance students' learning\nexperience, enabling them to submit their code frequently and receive rapid\nfeedback to better inform iteration and improvement to their code and\nvisualization design. Our method promotes consistent grading and enables\ninstructors to dedicate more focus to assist students in gaining visualization\nknowledge and experience. We have successfully deployed our method and\nauto-graded D3 submissions from more than 1000 undergraduate and graduate\nstudents in Georgia Tech's CSE6242 Data and Visual Analytics course, and\nreceived positive feedback and encouragement for expanding its adoption.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.12596,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"GeoSneakPique: Visual Autocompletion for Geospatial Queries\n\n  How many crimes occurred in the city center? And exactly which part of town\nis the 'city center'? While location is at the heart of many data questions,\ngeographic location can be difficult to specify in natural language (NL)\nqueries. This is especially true when working with fuzzy cognitive regions or\nregions that may be defined based on data distributions instead of absolute\nadministrative location (e.g., state, country). GeoSneakPique presents a novel\nmethod for using a mapping widget to support the NL query process, allowing\nusers to specify location via direct manipulation with data-driven guidance on\nspatial distributions to help select the area of interest. Users receive\nfeedback to help them evaluate and refine their spatial selection interactively\nand can save spatial definitions for re-use in subsequent queries. We conduct a\nqualitative evaluation of the GeoSneakPique that indicates the usefulness of\nthe interface as well as opportunities for better supporting geospatial\nworkflows in visual analysis tasks employing cognitive regions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.1406,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Argo Scholar: Interactive Visual Exploration of Literature in Browsers\n\n  Discovering and making sense of relevant research literature is fundamental\nto becoming knowledgeable in any scientific discipline. Visualization can aid\nthis process; however, existing tools' adoption and impact have often been\nconstrained, such as by their reliance on small curated paper datasets that\nquickly become outdated or a lack of support for personalized exploration. We\nintroduce Argo Scholar, an open-source, web-based visualization tool for\ninteractive exploration of literature and easy sharing of exploration results.\nArgo Scholar queries and visualizes Semantic Scholar's live data of almost 200\nmillion papers, enabling users to generate personalized literature exploration\nresults in real-time through flexible, incremental exploration, a common and\neffective method for researchers to discover relevant work. Our tool allows\nusers to easily share their literature exploration results as a URL or\nweb-embedded IFrame application. Argo Scholar is open-sourced and available at\nhttps:\/\/poloclub.github.io\/argo-scholar\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10131,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000069208,
      "text":"Personal Health Knowledge Graph for Clinically Relevant Diet\n  Recommendations\n\n  We propose a knowledge model for capturing dietary preferences and personal\ncontext to provide personalized dietary recommendations. We develop a knowledge\nmodel called the Personal Health Ontology, which is grounded in semantic\ntechnologies, and represents a patient's combined medical information, social\ndeterminants of health, and observations of daily living elicited from\ninterviews with diabetic patients. We then generate a personal health knowledge\ngraph that captures temporal patterns from synthetic food logs, annotated with\nconcepts from the Personal Health Ontology. We further discuss how lifestyle\nguidelines grounded in semantic technologies can be reasoned with the generated\npersonal health knowledge graph to provide appropriate dietary recommendations\nthat satisfy the user's medical and other lifestyle needs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.0018,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"RescueAR: Augmented Reality Supported Collaboration for UAV Driven\n  Emergency Response Systems\n\n  Emergency response events are fast-paced, noisy, and they require teamwork to\naccomplish the mission. Furthermore, the increasing deployment of Unmanned\nAerial Vehicles (UAVs) alongside emergency responders, demands a new form of\npartnership between humans and UAVs. Traditional radio-based information\nexchange between humans during an emergency response suffers from a lack of\nvisualization and often results in miscommunication. This paper presents a\nnovel collaboration platform: RescueAR, which utilizes the paradigm of\nLocation-based Augmented Reality to geotag, share, and visualize information.\nRescueAR aims to support the two-way communication between humans and UAVs,\nfacilitate collaboration across diverse responders, and visualize scene\ninformation relevant to the rescue team's role. According to our feasibility\nstudy, a user study, followed by a focus group session with police officers,\nRescueAR can support rescue teams in developing the spatial cognition of the\nscene, facilitate the exchange of geolocation information, and complement\nexisting communication tools during the UAV-supported emergency response.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00087,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"Improving Driver Situation Awareness Prediction using Human Visual\n  Sensory and Memory Mechanism\n\n  Situation awareness (SA) is generally considered as the perception,\nunderstanding, and projection of objects' properties and positions. We believe\nif the system can sense drivers' SA, it can appropriately provide warnings for\nobjects that drivers are not aware of. To investigate drivers' awareness, in\nthis study, a human-subject experiment of driving simulation was conducted for\ndata collection. While a previous predictive model for drivers' situation\nawareness utilized drivers' gaze movement only, this work utilizes object\nproperties, characteristics of human visual sensory and memory mechanism. As a\nresult, the proposed driver SA prediction model achieves over 70% accuracy and\noutperforms the baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.13211,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Investigating the Perceived Precision and validity of a Field-Deployable\n  Machine Learning-based Tool to Detect Post-Traumatic Stress Disorder (PTSD)\n  Hyperarousal Events\n\n  Post Traumatic Stress Disorder is a psychiatric condition experienced by\nindividuals after exposure to a traumatic event. Prior work has shown promise\nin detecting PTSD using physiological data such as heart rate. Despite the\npromise shown by the machine learning based algorithms for PTSD, the validation\napproaches used in previous research largely rely on theoretical and\ncomputational validation methods rather than naturalistic evaluations that\naccount for users perceived precision and validity. Previous research has shown\nthat users perceptions of physiological changes may not always align well with\nautomated detection of such variables and such misalignment may lead to\ndistrust in automated detection which may affect adoption or sustainable usage\nof such technologies. Therefore, the goal of this article is to investigate the\nperceived precision of the PTSD hyperarousal detection tool (developed\npreviously) in a home study with a group of PTSD patients. Naturalistic\nevaluation of such data driven algorithms may provide foundational insight into\nthe efficacy of such tools for non intrusive and cost efficient remote\nmonitoring of PTSD symptoms and will pave the way for their future adoption and\nsustainable use. The results showed over sixty five percent of perceived\nprecision in naturalistic validation of the detection tool. Further, the\nresults indicated that longitudinal exposure to the detection tool might\ncalibrate users trust in automation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01727,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Multimodal Driver State Modeling through Unsupervised Learning\n\n  Naturalistic driving data (NDD) can help understand drivers' reactions to\neach driving scenario and provide personalized context to driving behavior.\nHowever, NDD requires a high amount of manual labor to label certain driver's\nstate and behavioral patterns. Unsupervised analysis of NDD can be used to\nautomatically detect different patterns from the driver and vehicle data. In\nthis paper, we propose a methodology to understand changes in driver's\nphysiological responses within different driving patterns. Our methodology\nfirst decomposes a driving scenario by using a Bayesian Change Point detection\nmodel. We then apply the Latent Dirichlet Allocation method on both driver\nstate and behavior data to detect patterns. We present two case studies in\nwhich vehicles were equipped to collect exterior, interior, and driver\nbehavioral data. Four patterns of driving behaviors (i.e., harsh brake, normal\nbrake, curved driving, and highway driving), as well as two patterns of\ndriver's heart rate (HR) (i.e., normal vs. abnormal high HR), and gaze entropy\n(i.e., low versus high), were detected in these two case studies. The findings\nof these case studies indicated that among our participants, the drivers' HR\nhad a higher fraction of abnormal patterns during harsh brakes, accelerating\nand curved driving. Additionally, free-flow driving with close to zero\naccelerations on the highway was accompanied by more fraction of normal HR as\nwell as a lower gaze entropy pattern. With the proposed methodology we can\nbetter understand variations in driver's psychophysiological states within\ndifferent driving scenarios. The findings of this work, has the potential to\nguide future autonomous vehicles to take actions that are fit to each specific\ndriver.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00686,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Touching Art -- A Method for Visualizing Tactile Experience\n\n  It is human to want to touch artworks, to feel their surface curvature and\ntexture, their shapes and structures, and to feel the hand of the artist.\nMuseum guards need to be constantly vigilant to protect art objects from\nadoring and exploring touches by visitors. This paper introduces a novel\ntechnique for capturing where and how art objects are touched. In this method,\nthe users' touch either adds, or subtracts, microscopic fluorescent particles\nfrom a three-dimensional art object. Viewing the object under ultraviolet light\nreveals their touch traces and gestures. We present human touch behavior for a\nthree-dimensional stylized landscape, and for two abstract and two\nrepresentational art objects. We also present the results of video recordings\nof real-time behavior and user interviews. The resulting data show the kinds of\ntouches, and where they are directed, and also reveal important individual\ndifferences. We feel this method opens the door to studying art perception\nthrough touch, and also enables new kinds of studies into touch behavior in\nother applications, including visualization, embodied cognition, and design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.13535,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"An in-depth Analysis of Occasional and Recurring Collaborations in\n  Online Music Co-creation\n\n  The success of online creative communities depends on the will of\nparticipants to create and derive content in a collaborative environment.\nDespite their growing popularity, the factors that lead to remixing existing\ncontent in online creative communities are not entirely understood. In this\npaper, we focus on overdubbing, that is, a dyadic collaboration where one\nauthor mixes one new track with an audio recording previously uploaded by\nanother. We study musicians who collaborate regularly, that is, frequently\noverdub each other's songs. Building on frequent pattern mining techniques, we\ndevelop an approach to seek instances of such recurring collaborations in the\nSongtree community. We identify 43 instances involving two or three members\nwith a similar reputation in the community. Our findings highlight common and\ndifferent remix factors in occasional and recurring collaborations.\nSpecifically, fresh and less mature songs are generally overdubbed more;\ninstead, exchanging messages and invitations to collaborate are significant\nfactors only for songs generated through recurring collaborations whereas\nauthor reputation (ranking) and applying metadata tags to songs have a positive\neffect only in occasional collaborations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.14312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"A Visualization Authoring Model for Post-WIMP Interfaces\n\n  Besides the ability to utilize visualizations, the process of creating and\nauthoring them is of equal importance. However, for visualization environments\nbeyond the desktop, like multi-display or immersive analytics environments,\nthis process is often decoupled from the place where the visualization is\nactually used. This separation makes it hard for authors, developers, or users\nof such systems to understand, what consequences different choices they made\nwill have for the created visualizations. We present an extended visualization\nauthoring model for Post-WIMP interfaces, which support designers by a more\nseamless approach of developing and utilizing visualizations. With it, our\nemphasis is on the iterative nature of creating and configuring visualizations,\nthe existence of multiple views in the same system, and requirements for the\ndata analysis process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01781,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Model-Adaptive Interface Generation for Data-Driven Discovery\n\n  Discovery of new knowledge is increasingly data-driven, predicated on a\nteam's ability to collaboratively create, find, analyze, retrieve, and share\npertinent datasets over the duration of an investigation. This is especially\ntrue in the domain of scientific discovery where generation, analysis, and\ninterpretation of data are the fundamental mechanisms by which research teams\ncollaborate to achieve their shared scientific goal. Data-driven discovery in\ngeneral, and scientific discovery in particular, is distinguished by complex\nand diverse data models and formats that evolve over the lifetime of an\ninvestigation. While databases and related information systems have the\npotential to be valuable tools in the discovery process, developing effective\ninterfaces for data-driven discovery remains a roadblock to the application of\ndatabase technology as an essential tool in scientific investigations. In this\npaper, we present a model-adaptive approach to creating interaction\nenvironments for data-driven discovery of scientific data that automatically\ngenerates interactive user interfaces for editing, searching, and viewing\nscientific data based entirely on introspection of an extended relational data\nmodel. We have applied model-adaptive interface generation to many active\nscientific investigations spanning domains of proteomics, bioinformatics,\nneuroscience, occupational therapy, stem cells, genitourinary, craniofacial\ndevelopment, and others. We present the approach, its implementation, and its\nevaluation through analysis of its usage in diverse scientific settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.14845,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"Clinical Brain-Computer Interface Challenge 2020 (CBCIC at WCCI2020):\n  Overview, methods and results\n\n  In the field of brain-computer interface (BCI) research, the availability of\nhigh-quality open-access datasets is essential to benchmark the performance of\nemerging algorithms. The existing open-access datasets from past competitions\nmostly deal with healthy individuals' data, while the major application area of\nBCI is in the clinical domain. Thus the newly proposed algorithms to enhance\nthe performance of BCI technology are very often tested against the healthy\nsubjects' datasets only, which doesn't guarantee their success on patients'\ndatasets which are more challenging due to the presence of more nonstationarity\nand altered neurodynamics. In order to partially mitigate this scarcity,\nClinical BCI Challenge aimed to provide an open-access rich dataset of stroke\npatients recorded similar to a neurorehabilitation paradigm. Another key\nfeature of this challenge is that unlike many competitions in the past, it was\ndesigned for algorithms in both with-in subject and cross-subject categories as\na major thrust area of current BCI technology is to realize calibration-free\nBCI designs. In this paper, we have discussed the winning algorithms and their\nperformances across both competition categories which may help develop advanced\nalgorithms for reliable BCIs for real-world practical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.04323,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Snowy: Recommending Utterances for Conversational Visual Analysis\n\n  Natural language interfaces (NLIs) have become a prevalent medium for\nconducting visual data analysis, enabling people with varying levels of\nanalytic experience to ask questions of and interact with their data. While\nthere have been notable improvements with respect to language understanding\ncapabilities in these systems, fundamental user experience and interaction\nchallenges including the lack of analytic guidance (i.e., knowing what aspects\nof the data to consider) and discoverability of natural language input (i.e.,\nknowing how to phrase input utterances) persist. To address these challenges,\nwe investigate utterance recommendations that contextually provide analytic\nguidance by suggesting data features (e.g., attributes, values, trends) while\nimplicitly making users aware of the types of phrasings that an NLI supports.\nWe present SNOWY, a prototype system that generates and recommends utterances\nfor visual analysis based on a combination of data interestingness metrics and\nlanguage pragmatics. Through a preliminary user study, we found that utterance\nrecommendations in SNOWY support conversational visual analysis by guiding the\nparticipants' analytic workflows and making them aware of the system's language\ninterpretation capabilities. Based on the feedback and observations from the\nstudy, we discuss potential implications and considerations for incorporating\nrecommendations in future NLIs for visual analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11475,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Future of Smart Classroom in the Era of Wearable Neurotechnology\n\n  Interdisciplinary research among engineering, computer science, and\nneuroscience to understand and utilize the human brain signals resulted in\nadvances and widespread applicability of wearable neurotechnology in adaptive\nhuman-in-the-loop smart systems. Considering these advances, we envision that\nfuture education will exploit the advances in wearable neurotechnology and move\ntoward more personalized smart classrooms where instructions and interactions\nare tailored towards. students' individual strengths and needs. In this paper,\nwe discuss the future of smart classrooms and how advances in neuroscience,\nmachine learning, and embedded systems as key enablers will provide the\ninfrastructure for envisioned smart classrooms and personalized education along\nwith open challenges that are required to be addressed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.14681,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Human-machine Symbiosis: A Multivariate Perspective for Physically\n  Coupled Human-machine Systems\n\n  The notion of symbiosis has been increasingly mentioned in research on\nphysically coupled human-machine systems. Yet, a uniform specification on which\naspects constitute human-machine symbiosis is missing. By combining the\nexpertise of different disciplines, we elaborate on a multivariate perspective\nof symbiosis as the highest form of physically coupled human-machine systems.\nFour dimensions are considered: Task, interaction, performance, and experience.\nFirst, human and machine work together to accomplish a common task\nconceptualized on both a decision and an action level (task dimension). Second,\neach partner possesses an internal representation of own as well as the other\npartner's intentions and influence on the environment. This alignment, which is\nthe core of the interaction, constitutes the symbiotic understanding between\nboth partners, being the basis of a joint, highly coordinated and effective\naction (interaction dimension). Third, the symbiotic interaction leads to\nsynergetic effects regarding the intention recognition and complementary\nstrengths of the partners, resulting in a higher overall performance\n(performance dimension). Fourth, symbiotic systems specifically change the\nuser's experiences, like flow, acceptance, sense of agency, and embodiment\n(experience dimension). This multivariate perspective is flexible and generic\nand is also applicable in diverse human-machine scenarios, helping to bridge\nbarriers between different disciplines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.1331,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Evaluating Trust in the Context of Conversational Information Systems\n  for new users of the Internet\n\n  Most online information sources are text-based and in Western Languages like\nEnglish. However, many new and first time users of the Internet are in contexts\nwith low English proficiency and are unable to access vital information online.\nSeveral researchers have focused on building conversational information systems\nover voice for this demographic, and also highlighted the importance of\nbuilding trust towards the information source. In this work we develop four\nversions of a voice based chat-bot on the Google Assistant platform in which we\nvary the gender, friendliness and personalisation of the bot. We find that the\nusers rank the female version of the bot with more personalisations over the\nothers; however when rating the bots individually, the ratings depend on the\nability of the bot to understand the users' spoken query and respond\naccurately.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12153,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Methodology and feasibility of neurofeedback to improve visual attention\n  to letters in mild Alzheimer's disease\n\n  Brain computer interfaces systems are controlled by users through\nneurophysiological input for a variety of applications including communication,\nenvironmental control, motor rehabilitation, and cognitive training. Although\nindividuals with severe speech and physical impairment are the primary users of\nthis technology, BCIs have emerged as a potential tool for broader populations,\nespecially with regards to delivering cognitive training or interventions with\nneurofeedback. The goal of this study was to investigate the feasibility of\nusing a BCI system with neurofeedback as an intervention for people with mild\nAlzheimer's disease. The study focused on visual attention and language since\nad is often associated with functional impairments in language and reading. The\nstudy enrolled five adults with mild ad in a nine to thirteen week BCI EEG\nbased neurofeedback intervention to improve attention and reading skills. Two\nparticipants completed intervention entirely. The remaining three participants\ncould not complete the intervention phase because of restrictions related to\ncovid. Pre and post assessment measures were used to assess reliability of\noutcome measures and generalization of treatment to functional reading,\nprocessing speed, attention, and working memory skills. Participants\ndemonstrated steady improvement in most cognitive measures across experimental\nphases, although there was not a significant effect of NFB on most measures of\nattention. One subject demonstrated significantly significant improvement in\nletter cancellation during NFB. All participants with mild AD learned to\noperate a BCI system with training. Results have broad implications for the\ndesign and use of bci systems for participants with cognitive impairment.\nPreliminary evidence justifies implementing NFB-based cognitive measures in AD.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09243,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"An Investigation into Keystroke Dynamics and Heart Rate Variability as\n  Indicators of Stress\n\n  Lifelogging has become a prominent research topic in recent years. Wearable\nsensors like Fitbits and smart watches are now increasingly popular for\nrecording ones activities. Some researchers are also exploring keystroke\ndynamics for lifelogging. Keystroke dynamics refers to the process of measuring\nand assessing a persons typing rhythm on digital devices. A digital footprint\nis created when a user interacts with devices like keyboards, mobile phones or\ntouch screen panels and the timing of the keystrokes is unique to each\nindividual though likely to be affected by factors such as fatigue, distraction\nor emotional stress. In this work we explore the relationship between keystroke\ndynamics as measured by the timing for the top-10 most frequently occurring\nbi-grams in English, and the emotional state and stress of an individual as\nmeasured by heart rate variability (HRV). We collected keystroke data using the\nLoggerman application while HRV was simultaneously gathered. With this data we\nperformed an analysis to determine the relationship between variations in\nkeystroke dynamics and variations in HRV. Our conclusion is that we need to use\na more detailed representation of keystroke timing than the top-10 bigrams,\nprobably personalised to each user.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.03942,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Extended Reality for Knowledge Work in Everyday Environments\n\n  Virtual and Augmented Reality have the potential to change information work.\nThe ability to modify the workers senses can transform everyday environments\ninto a productive office, using portable head-mounted displays combined with\nconventional interaction devices, such as keyboards and tablets. While a stream\nof better, cheaper and lighter HMDs have been introduced for consumers in\nrecent years, there are still many challenges to be addressed to allow this\nvision to become reality. This chapter summarizes the state of the art in the\nfield of extended reality for knowledge work in everyday environments and\nproposes steps to address the open challenges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00714,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000043048,
      "text":"Bridging Action Frames: Instagram Infographics in U.S.Ethnic Movements\n\n  Instagram infographics are a digital activism tool that have redefined action\nframes for technology-facilitated social movements. From the 1960s through the\n1980s, United States ethnic movements practiced collective action:\nideologically unified, resource-intensive traditional activism. Today,\ntechnologically enabled movements have been categorized as practicing\nconnective action: individualized, low-resource online activism. Yet, we argue\nthat Instagram infographics are both connective and collective. This paper\njuxtaposes the insights of past and present U.S. ethnic movement activists and\nanalyzes Black Lives Matter Instagram data over the course of 7 years\n(2014-2020). We find that Instagram infographic activism bridges connective and\ncollective action in three ways: (1) Scope for Education: Visually enticing and\ndigestible infographics reduce the friction of information dissemination,\nfacilitating collective movement education while preserving customizability.\n(2) Reconciliation for Credibility: Activists use connective features to combat\ninfographic misinformation and resolve internal differences, creating a trusted\ncollective movement front. (3) High-Resource Efforts for Transformative Change:\nInstagram infographic activism has been paired with boots on the ground and\naction-oriented content, curating a connective-to-collective pipeline that\nexpends movement resources. Our work unveils the vitality of evaluating digital\nactivism action frames at the movement integration level, exemplifies the\npowerful coexistence of connective and collective action, and offers meaningful\ndesign implications for activists seeking to leverage this novel tool.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.14366,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Exploring technologies to better link physical evidence and digital\n  information for disaster victim identification\n\n  Disaster victim identification (DVI) entails a protracted process of evidence\ncollection and data matching to reconcile physical remains with victim\nidentity. Technology is critical to DVI by enabling the linkage of physical\nevidence to information. However, labelling physical remains and collecting\ndata at the scene are dominated by low-technology paper-based practices. We\nask, how can technology help us tag and track the victims of disaster? Our\nresponse has two parts. First, we conducted a human-computer interaction led\ninvestigation into the systematic factors impacting DVI tagging and tracking\nprocesses. Through interviews with Australian DVI practitioners, we explored\nhow technologies to improve linkage might fit with prevailing work practices\nand preferences; practical and social considerations; and existing systems and\nprocesses. Using insights from these interviews and relevant literature, we\nidentified four critical themes: protocols and training; stress and stressors;\nthe plurality of information capture and management systems; and practicalities\nand constraints. Second, we applied the themes identified in the first part of\nthe investigation to critically review technologies that could support DVI\npractitioners by enhancing DVI processes that link physical evidence to\ninformation. This resulted in an overview of candidate technologies matched\nwith consideration of their key attributes. This study recognises the\nimportance of considering human factors that can affect technology adoption\ninto existing practices. We provide a searchable table (Supplementary\nInformation) that relates technologies to the key attributes relevant to DVI\npractice, for the reader to apply to their own context. While this research\ndirectly contributes to DVI, it also has applications to other domains in which\na physical\/digital linkage is required, particularly within high-stress\nenvironments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.02775,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000639757,
      "text":"A Tutorial of Cyber-Syndrome viewed from Cyber-Physical-Social-Thinking\n  Space and Maslow's Hierarchy of Needs\n\n  With the increase of active Internet users, various physical, social, and\nmental disorders have recently emerged because of the excessive use of\ntechnology. Cyber-Syndrome is known as the condition that appears due to the\nexcessive interaction with the cyberspace, and it affects the users' physical,\nsocial, and mental states. In this paper, we discuss the etiology and symptoms\nof Cyber-Syndrome according to theories of Cyber-Physical-Social-Thinking\n(CPST) space and Maslow's Hierarchy of Needs. In addition, we also propose an\nentropy-based mechanism for recovery of Cyber-Syndrome, to provide potential\nguidance for clinical detection and diagnosis. Cyber-Syndrome has attracted\nmuch attention these days, and more in-depth exploration is needed in the\nfuture.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01919,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000457962,
      "text":"On Female Audience Sending Virtual Gifts to Male Streamers on Douyin\n\n  Live streaming has become increasingly popular. Our study focuses on the\nemerging Chinese female audiences who send virtual gifts to young male\nstreamers. We observe a reversed entertainer-viewer gender relationship. We aim\nto study why they watch young male streamers, why they send gifts, and their\nrelationships with these streamers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00831,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000108944,
      "text":"User-friendly Composition of FAIR Workflows in a Notebook Environment\n\n  There has been a large focus in recent years on making assets in scientific\nresearch findable, accessible, interoperable and reusable, collectively known\nas the FAIR principles. A particular area of focus lies in applying these\nprinciples to scientific computational workflows. Jupyter notebooks are a very\npopular medium by which to program and communicate computational scientific\nanalyses. However, they present unique challenges when it comes to reuse of\nonly particular steps of an analysis without disrupting the usual flow and\nbenefits of the notebook approach, making it difficult to fully comply with the\nFAIR principles. Here we present an approach and toolset for adding the power\nof semantic technologies to Python-encoded scientific workflows in a simple,\nautomated and minimally intrusive manner. The semantic descriptions are\npublished as a series of nanopublications that can be searched and used in\nother notebooks by means of a Jupyter Lab plugin. We describe the\nimplementation of the proposed approach and toolset, and provide the results of\na user study with 15 participants, designed around image processing workflows,\nto evaluate the usability of the system and its perceived effect on FAIRness.\nOur results show that our approach is feasible and perceived as user-friendly.\nOur system received an overall score of 78.75 on the System Usability Scale,\nwhich is above the average score reported in the literature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08214,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"Exploring Augmented Reality Games in Accessible Learning: A Systematic\n  Review\n\n  Augmented Reality (AR) learning games, on average, have been shown to have a\npositive impact on student learning. However, the exploration of AR learning\ngames in special education settings, where accessibility is a concern, has not\nbeen well explored. Thus, the purpose of this study is to explore the use of AR\ngames in accessible learning applications and to provide a comprehensive\nunderstanding of its advantages over traditional learning approaches. In this\npaper, we present our systematic review of previous studies included in major\ndatabases in the past decade. We explored the characteristics of user\nevaluation, learning effects on students, and features of implemented systems\nmentioned in the literature. The results showed that AR game applications can\npromote students learning activities from three perspectives: cognitive,\naffective, and retention. We also found there were still several drawbacks to\ncurrent AR learning game designs for special needs despite the positive effects\nassociated with AR game use. Based on our findings, we propose potential design\nstrategies for future AR learning games for accessible education.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.01354,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"SmartKC: Smartphone-based Corneal Topographer for Keratoconus Detection\n\n  Keratoconus is a severe eye disease affecting the cornea (the clear,\ndome-shaped outer surface of the eye), causing it to become thin and develop a\nconical bulge. The diagnosis of keratoconus requires sophisticated ophthalmic\ndevices which are non-portable and very expensive. This makes early detection\nof keratoconus inaccessible to large populations in low- and middle-income\ncountries, making it a leading cause for partial\/complete blindness among such\npopulations. We propose SmartKC, a low-cost, smartphone-based keratoconus\ndiagnosis system comprising of a 3D-printed placido's disc attachment, an LED\nlight strip, and an intelligent smartphone app to capture the reflection of the\nplacido rings on the cornea. An image processing pipeline analyzes the corneal\nimage and uses the smartphone's camera parameters, the placido rings' 3D\nlocation, the pixel location of the reflected placido rings and the setup's\nworking distance to construct the corneal surface, via the Arc-Step method and\nZernike polynomials based surface fitting. In a clinical study with 101\ndistinct eyes, we found that SmartKC achieves a sensitivity of 94.1% and a\nspecificity of 100.0%. Moreover, the quantitative curvature estimates (sim-K)\nstrongly correlate with a gold-standard medical device (Pearson correlation\ncoefficient =0.78). Our results indicate that SmartKC has the potential to be\nused as a keratoconus screening tool under real-world medical settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05738,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"Preventing Handheld Phone Distraction for Drivers by Sensing the\n  Gripping Hand\n\n  Handheld phone distraction is the leading cause of traffic accidents.\nHowever, few efforts have been devoted to detecting when the phone distraction\nhappens, which is a critical input for taking immediate safety measures. This\nwork proposes a phone-use monitoring system, which detects the start of the\ndriver's handheld phone use and eliminates the distraction at once.\nSpecifically, the proposed system emits periodic ultrasonic pulses to sense if\nthe phone is being held in hand or placed on support surfaces (e.g., seat and\ncup holder) by capturing the unique signal interference resulted from the\ncontact object's damping, reflection and refraction. We derive the short-time\nFourier transform from the microphone data to describe such impacts and develop\na CNN-based binary classifier to discriminate the phone use between the\nhandheld and the handsfree status. Additionally, we design an adaptive\nwindow-based filter to correct the classification errors and identify each\nhandheld phone distraction instance, including its start, end, and duration.\nExtensive experiments with fourteen people, three phones and two car models\nshow that our system achieves 99% accuracy of recognizing handheld phone-use\ninstances and 0.76-second median error to estimate the distraction's start\ntime.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.14322,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000110931,
      "text":"Proceedings of the CSCW 2021 Workshop -- Investigating and Mitigating\n  Biases in Crowdsourced Data\n\n  This volume contains the position papers presented at CSCW 2021 Workshop -\nInvestigating and Mitigating Biases in Crowdsourced Data, held online on 23rd\nOctober 2021, at the 24th ACM Conference on Computer-Supported Cooperative Work\nand Social Computing (CSCW 2021). The workshop explored how specific\ncrowdsourcing workflows, worker attributes, and work practices contribute to\nbiases in data. The workshop also included discussions on research directions\nto mitigate labelling biases, particularly in a crowdsourced context, and the\nimplications of such methods for the workers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0492,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"PopBlends: Strategies for Conceptual Blending with Large Language Models\n\n  Pop culture is an important aspect of communication. On social media people\noften post pop culture reference images that connect an event, product or other\nentity to a pop culture domain. Creating these images is a creative challenge\nthat requires finding a conceptual connection between the users' topic and a\npop culture domain. In cognitive theory, this task is called conceptual\nblending. We present a system called PopBlends that automatically suggests\nconceptual blends. The system explores three approaches that involve both\ntraditional knowledge extraction methods and large language models. Our\nannotation study shows that all three methods provide connections with similar\naccuracy, but with very different characteristics. Our user study shows that\npeople found twice as many blend suggestions as they did without the system,\nand with half the mental demand. We discuss the advantages of combining large\nlanguage models with knowledge bases for supporting divergent and convergent\nthinking.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.15053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Deep Learning for Enhanced Scratch Input\n\n  The vibrations generated from scratching and tapping on surfaces can be\nhighly expressive and recognizable, and have therefore been proposed as a\nmethod of natural user interface (NUI). Previous systems require custom sensor\nhardware such as contact microphones and have struggled with gesture\nclassification accuracy.\n  We propose a deep learning approach to scratch input. Using smartphones and\ntablets laid on tabletops or other similar surfaces, our system achieved a\ngesture classification accuracy of 95.8\\%, substantially reducing gesture\nmisclassification from previous works. Further, our system achieved this\nperformance when tested on a wide variety of surfaces, mobile devices, and in\nhigh noise environments.\n  The results indicate high potential for the application of deep learning\ntechniques to natural user interface (NUI) systems that can readily convert\nlarge unpowered surfaces into a user interface using just a smartphone with no\nspecial-purpose sensors or hardware.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12938,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"SCLAiR : Supervised Contrastive Learning for User and Device Independent\n  Airwriting Recognition\n\n  Airwriting Recognition is the problem of identifying letters written in free\nspace with finger movement. It is essentially a specialized case of gesture\nrecognition, wherein the vocabulary of gestures corresponds to letters as in a\nparticular language. With the wide adoption of smart wearables in the general\npopulation, airwriting recognition using motion sensors from a smart-band can\nbe used as a medium of user input for applications in Human-Computer\nInteraction. There has been limited work in the recognition of in-air\ntrajectories using motion sensors, and the performance of the techniques in the\ncase when the device used to record signals is changed has not been explored\nhitherto. Motivated by these, a new paradigm for device and user-independent\nairwriting recognition based on supervised contrastive learning is proposed. A\ntwo stage classification strategy is employed, the first of which involves\ntraining an encoder network with supervised contrastive loss. In the subsequent\nstage, a classification head is trained with the encoder weights kept frozen.\nThe efficacy of the proposed method is demonstrated through experiments on a\npublicly available dataset and also with a dataset recorded in our lab using a\ndifferent device. Experiments have been performed in both supervised and\nunsupervised settings and compared against several state-of-the-art domain\nadaptation techniques. Data and the code for our implementation will be made\navailable at https:\/\/github.com\/ayushayt\/SCLAiR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0846,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Enabling human-centered AI: A new junction and shared journey between AI\n  and HCI communities\n\n  Artificial intelligence (AI) has brought benefits, but it may also cause harm\nif it is not appropriately developed. Current development is mainly driven by a\n\"technology-centered\" approach, causing many failures. For example, the AI\nIncident Database has documented over a thousand AI-related accidents. To\naddress these challenges, a human-centered AI (HCAI) approach has been promoted\nand has received a growing level of acceptance over the last few years. HCAI\ncalls for combining AI with user experience (UX) design will enable the\ndevelopment of AI systems (e.g., autonomous vehicles, intelligent user\ninterfaces, or intelligent decision-making systems) to achieve its design goals\nsuch as usable\/explainable AI, human-controlled AI, and ethical AI. While HCAI\npromotion continues, it has not specifically addressed the collaboration\nbetween AI and human-computer interaction (HCI) communities, resulting in\nuncertainty about what action should be taken by both sides to apply HCAI in\ndeveloping AI systems. This Viewpoint focuses on the collaboration between the\nAI and HCI communities, which leads to nine recommendations for effective\ncollaboration to enable HCAI in developing AI systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.07365,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Learning Enhancement in Higher Education with Wearable Technology\n\n  Wearable technologies have traditionally been used to measure and monitor\nvital human signs for well-being and healthcare applications. However, there is\na growing interest in using and deploying these technologies to facilitate\nteaching and learning, particularly in a higher education environment. The aim\nof this paper is therefore to systematically review the range of wearable\ndevices that have been used for enhancing the teaching and delivery of\nengineering curricula in higher education. Moreover, we compare the advantages\nand disadvantages of these devices according to the location in which they are\nworn on the human body. According to our survey, wearable devices for enhanced\nlearning have mainly been worn on the head (e.g. eyeglasses), wrist (e.g.\nwatches) and chest (e.g. electrocardiogram patch). In fact, among those\nlocations, head-worn devices enable better student engagement with the learning\nmaterials, improved student attention as well as higher spatial and visual\nawareness. We identify the research questions and discuss the research\ninclusion and exclusion criteria to present the challenges faced by researchers\nin implementing learning technologies for enhanced engineering education.\nFurthermore, we provide recommendations on using wearable devices to improve\nthe teaching and learning of engineering courses in higher education.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05974,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"User Centered Design (VII): From Automated Flight Deck to Intelligent\n  Flight Deck\n\n  Driven by the \"user-centered design\" philosophy, this paper first outlines\nthe human factors problems of the flight deck automation for large civil\naircraft and the human factors research carried out based on the\n\"human-centered automation\" approach. This paper then reviews the previous\ninitial human factors research on intelligent civil flight deck based on the\n\"human-centered AI\" approach and discusses the prospects for future human\nfactors research. Based on our proposed human factors engineering model for\nintelligent human-computer interaction and the framework of joint cognitive\neco-systems, this paper proposes an initial human factors solution for the\nsingle-pilot operations of large civil aircraft and presents preliminary\nsuggestions for future human factors research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.07148,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"Decoding 3D Representation of Visual Imagery EEG using Attention-based\n  Dual-Stream Convolutional Neural Network\n\n  A deep neural network has been successfully applied to an\nelectroencephalogram (EEG)-based brain-computer interface. However, in most\nstudies, the correlation between EEG channels and inter-region relationships\nare not well utilized, resulting in sub-optimized spatial feature extraction.\nIn this study, we propose an attention-based dual-stream 3D-convolutional\nneural network that can enhance spatial feature extraction by emphasizing the\nrelationship between channels with dot product-based channel attention and 3D\nconvolution. The proposed method showed superior performance than the\ncomparative models by achieving an accuracy of 0.58 for 4-class visual imagery\n(VI) EEG classification. Through statistical and neurophysiological analysis,\nvisual motion imagery showed higher alpha-power spectral density (PSD) over the\nvisual cortex than static VI. Also, the VI of swarm dispersion showed higher\nbeta-PSD over the pre-frontal cortex than the VI of swarm aggregation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.035,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Understanding Distributed Tutorship in Online Language Tutoring\n\n  With the rise of the gig economy, online language tutoring platforms are\nbecoming increasingly popular. They provide temporary and flexible jobs for\nnative speakers as tutors and allow language learners to have one-on-one\nspeaking practices on demand. However, the lack of stable relationships hinders\ntutors and learners from building long-term trust. \"Distributed tutorship\" --\ntemporally discontinuous learning experience with different tutors -- has been\nunderexplored yet has many implications for modern learning platforms. In this\npaper, we analyzed tutorship sequences of 15,959 learners and found that around\n40% of learners change to new tutors every session; 44% learners change to new\ntutors while reverting to previous tutors sometimes; only 16% learners change\nto new tutors and then fix on one tutor. We also found suggestive evidence that\nhigher distributedness -- higher diversity and lower continuity in tutorship --\nis correlated to slower improvements in speaking performance scores with a\nsimilar number of sessions. We further surveyed 519 and interviewed 40 learners\nand found that more learners preferred fixed tutorship while some do not have\nit due to various reasons. Finally, we conducted semi-structured interviews\nwith three tutors and one product manager to discuss the implications for\nimproving the continuity in learning under distributed tutorship.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06429,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000048677,
      "text":"Decoding Visual Imagery from EEG Signals using Visual Perception Guided\n  Network Training Method\n\n  An electroencephalogram is an effective approach that provides a\nbidirectional pathway between user and computer in a non-invasive way. In this\nstudy, we adopted the visual perception data for training the visual imagery\ndecoding network. We proposed a visual perception-guided network training\napproach for decoding visual imagery. Visual perception decreases the power of\nthe alpha frequency range of the visual cortex over time when the user\nperformed the task, and visual imagery increases the power of the alpha\nfrequency range of the visual cortex over time as the user performed with the\ntask. Generated brain signals when the user performing visual imagery and\nvisual perception have opposite brain activity tendencies, and we used these\ncharacteristics to design the proposed network. When using the proposed method,\nthe average classification performance of visual imagery with the visual\nperception data was 0.7008. Our results provide the possibility of using the\nvisual perception data as a guide of the visual imagery classification network\ntraining.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.03179,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"User-Driven Support for Visualization Prototyping in D3\n\n  Templates have emerged as an effective approach to simplifying the\nvisualization design and programming process. For example, they enable users to\nquickly generate multiple visualization designs even when using complex\ntoolkits like D3. However, these templates are often treated as rigid artifacts\nthat respond poorly to changes made outside of the template's established\nparameters, limiting user creativity. Preserving the user's creative flow\nrequires a more dynamic approach to template-based visualization design, where\ntools can respond gracefully to users' edits when they modify templates in\nunexpected ways. In this paper, we leverage the structural similarities\nrevealed by templates to design resilient support features for prototyping D3\nvisualizations: recommendations to suggest complementary interactions for a\nuser's D3 program; and code augmentation to implement recommended interactions\nwith a single click, even when users deviate from pre-defined templates. We\ndemonstrate the utility of these features in Mirny, a d design-focused\nprototyping environment for D3. In a user study with 20 D3 users, we find that\nthese automated features enable participants to prototype their design ideas\nwith significantly fewer programming iterations. We also characterize key\nmodification strategies used by participants to customize D3 templates.\nInformed by our findings and participants' feedback, we discuss the key\nimplications of the use of templates for interleaving visualization programming\nand design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.05118,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Web Platform for Visualisation of Kinematic Data captured from a Motor\n  Tele-rehabilitation System\n\n  Stroke can have a severe impact on an individual's quality of life, leading\nto consequences such as motor loss and communication problems, especially among\nthe elderly. Studies have shown that early and easy access to stroke\nrehabilitation can improve an elderly individual's quality of life, and that\ntelerehabilitation is a solution that facilitates this improvement. In this\nwork, we visualize movement to music during rehabilitation exercises captured\nby the Kinect motion sensor, using a dedicated Serious Game called `Move to the\nMusic'(MoMu). Our system provides a quantitative view of progress made by\npatients during a motor rehabilitation regime for healthcare professionals to\ntrack remotely (tele-rehab).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08842,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Ubiq: A System to Build Flexible Social Virtual Reality Experiences\n\n  While they have long been a subject of academic study, social virtual reality\n(SVR) systems are now attracting increasingly large audiences on current\nconsumer virtual reality systems. The design space of SVR systems is very\nlarge, and relatively little is known about how these systems should be\nconstructed in order to be usable and efficient. In this paper we present Ubiq,\na toolkit that focuses on facilitating the construction of SVR systems. We\nargue for the design strategy of Ubiq and its scope. Ubiq is built on the Unity\nplatform. It provides core functionality of many SVR systems such as connection\nmanagement, voice, avatars, etc. However, its design remains easy to extend. We\ndemonstrate examples built on Ubiq and how it has been successfully used in\nclassroom teaching. Ubiq is open source (Apache License) and thus enables\nseveral use cases that commercial systems cannot.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.12399,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Towards identifying optimal biased feedback for various user states and\n  traits in motor imagery BCI\n\n  Objective. Neural self-regulation is necessary for achieving control over\nbrain-computer interfaces (BCIs). This can be an arduous learning process\nespecially for motor imagery BCI. Various training methods were proposed to\nassist users in accomplishing BCI control and increase performance. Notably the\nuse of biased feedback, i.e. non-realistic representation of performance.\nBenefits of biased feedback on performance and learning vary between users\n(e.g. depending on their initial level of BCI control) and remain speculative.\nTo disentangle the speculations, we investigate what personality type, initial\nstate and calibration performance (CP) could benefit from a biased feedback.\nMethods. We conduct an experiment (n=30 for 2 sessions). The feedback provided\nto each group (n=10) is either positively, negatively or not biased. Results.\nStatistical analyses suggest that interactions between bias and: 1) workload,\n2) anxiety, and 3) self-control significantly affect online performance. For\ninstance, low initial workload paired with negative bias is associated to\nhigher peak performances (86%) than without any bias (69%). High anxiety\nrelates negatively to performance no matter the bias (60%), while low anxiety\nmatches best with negative bias (76%). For low CP, learning rate (LR) increases\nwith negative bias only short term (LR=2%) as during the second session it\nseverely drops (LR=-1%). Conclusion. We unveil many interactions between said\nhuman factors and bias. Additionally, we use prediction models to confirm and\nreveal even more interactions. Significance. This paper is a first step towards\nidentifying optimal biased feedback for a personality type, state, and CP in\norder to maximize BCI performance and learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06463,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000044372,
      "text":"Possibility of Sleep Induction using Auditory Stimulation based on\n  Mental States\n\n  Sleep has a significant role to maintain our health. However, people have\nstruggled with sleep induction because of noise, emotion, and complicated\nthoughts. We hypothesized that there was more effective auditory stimulation to\ninduce sleep based on their mental states. We investigated five auditory\nstimulation: sham, repetitive beep, binaural beat, white noise, and rainy\nsounds. The Pittsburgh sleep quality index was performed to divide subjects\ninto good and poor sleep groups. To verify the subject's mental states between\ninitiation of sessions, a psychomotor vigilance task and Stanford sleepiness\nscale (SSS) were performed before auditory stimulation. After auditory\nstimulation, we asked subjects to report their sleep experience during auditory\nstimulation. We also calculated alpha dominant duration that was the period\nthat represents the wake period during stimulation. We showed that there were\nno differences in reaction time and SSS between sessions. It indicated sleep\nexperience is not related to the timeline. The good sleep group fell asleep\nmore frequently than the poor sleep group when they hear white noise and rainy\nsounds. Moreover, when subjects failed to fall asleep during sham, most\nsubjects fell asleep during rainy sound (Cohen's kappa: -0.588). These results\nhelp people to select suitable auditory stimulation to induce sleep based on\ntheir mental states.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.10061,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Redycler: Daily Outfit Texture Fabrication Appliance Using\n  Re-Programmable Dyes\n\n  We present a speculative design for a novel appliance for future fabrication\nin the home to revitalize textiles using re-programmable multi-color textures.\nUtilizing colored photochromic dyes activated by ultraviolet (UV) light, we can\nselectively deactivate hues using complementary colors in visible light to\nresult in the final desired dye pattern. Our proposed appliance would automate\nthis process within a box placed in the bedroom. We envision a future where\npeople are able to transform old apparel into unique and fashionable pieces of\nclothing.\n  We discuss how the user would interact with the appliance and how this device\nelongates the life-cycle of clothing through modification. We also outline the\ncentral issues to integrate such a concept into the home. Finally, we analyze\nhow this device fits into personal modification trends in HCI to show how this\ndevice could change existing conceptions around sustainable fashion and\npersonal style.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.05905,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"UbiNIRS: A Software Framework for Miniaturized NIRS-based Applications\n\n  We present UbiNIRS, a software framework for rapid development and deployment\nof applications using miniaturized near-infrared spectroscopy (NIRS). NIRS is\nan emerging material sensing technology that has shown a great potential in\nrecent work from the HCI community such as in situ pill testing. However,\nexisting methods require significant programming efforts and professional\nknowledge of NIRS, and hence, challenge the creation of new NIRS based\napplications. Our system helps to resolve this issue by providing a generic\nserver and a mobile app, using the best practices for NIRS applications in\nliterature. The server creates and manages UbiNIRS instances without the need\nfor any coding or professional knowledge of NIRS. The mobile app can register\nmultiple UbiNIRS instances by communicating with the server for different NIRS\nbased applications. Furthermore, UbiNIRS enables NIRS spectrum crowdsourcing\nfor building a knowledge base.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.05734,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"MTV: Visual Analytics for Detecting, Investigating, and Annotating\n  Anomalies in Multivariate Time Series\n\n  Detecting anomalies in time-varying multivariate data is crucial in various\nindustries for the predictive maintenance of equipment. Numerous machine\nlearning (ML) algorithms have been proposed to support automated anomaly\nidentification. However, a significant amount of human knowledge is still\nrequired to interpret, analyze, and calibrate the results of automated\nanalysis. This paper investigates current practices used to detect and\ninvestigate anomalies in time series data in industrial contexts and identifies\ncorresponding needs. Through iterative design and working with nine experts\nfrom two industry domains (aerospace and energy), we characterize six design\nelements required for a successful visualization system that supports effective\ndetection, investigation, and annotation of time series anomalies. We summarize\nan ideal human-AI collaboration workflow that streamlines the process and\nsupports efficient and collaborative analysis. We introduce MTV (Multivariate\nTime Series Visualization), a visual analytics system to support such workflow.\nThe system incorporates a set of novel visualization and interaction designs to\nsupport multi-faceted time series exploration, efficient in-situ anomaly\nannotation, and insight communication. Two user studies, one with 6 spacecraft\nexperts (with routine anomaly analysis tasks) and one with 25 general end-users\n(without such tasks), are conducted to demonstrate the effectiveness and\nusefulness of MTV.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.12126,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000128481,
      "text":"Eliciting Gestures for Novel Note-taking Interactions\n\n  Handwriting recognition is improving in leaps and bounds, and this opens up\nnew opportunities for stylus-based interactions. In particular, note-taking\napplications can become a more intelligent user interface, incorporating new\nfeatures like autocomplete and integrated search. In this work we ran a gesture\nelicitation study, asking 21 participants to imagine how they would interact\nwith an imaginary, intelligent note-taking application. We report agreement on\nthe elicited gestures, finding that while existing common interactions are\nprevalent (like double taps and long presses) a number of more novel\ninteractions (like dragging selected items to hotspots or using annotations)\nwere also well-represented. We discuss the mental models participants drew on\nwhen explaining their gestures and what kind of feedback users might need to\nmove to more stylus-centric interactions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11007,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000212259,
      "text":"Pseudo-Haptic Button for Improving User Experience of Mid-Air\n  Interaction in VR\n\n  Mid-air interaction is one of the promising interaction modalities in virtual\nreality (VR) due to its merits in naturalness and intuitiveness, but the\ninteraction suffers from the lack of haptic feedback as no force or\nvibrotactile feedback can be provided in mid-air. As a breakthrough to\ncompensate for this insufficiency, the application of pseudo-haptic features\nwhich create the visuo-haptic illusion without actual physical haptic stimulus\ncan be explored. Therefore, this study aimed to investigate the effect of four\npseudo-haptic features: proximity feedback, protrusion, hit effect, and\npenetration blocking on user experience for free-hand mid-air button\ninteraction in VR. We conducted a user study on 21 young subjects to collect\nuser ratings on various aspects of user experience while users were freely\ninteracting with 16 buttons with different combinations of four features.\nResults indicated that all investigated features significantly improved user\nexperience in terms of haptic illusion, embodiment, sense of reality,\nspatiotemporal perception, satisfaction, and hedonic quality. In addition,\nprotrusion and hit effect were more beneficial in comparison with the other two\nfeatures. It is recommended to utilize the four proposed pseudo-haptic features\nin 3D user interfaces (UIs) to make users feel more pleased and amused, but\ncaution is needed when using proximity feedback together with other features.\nThe findings of this study could be helpful for VR developers and UI designers\nin providing better interactive buttons in the 3D interfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13555,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"VibEmoji: Exploring User-authoring Multi-modal Emoticons in Social\n  Communication\n\n  Emoticons are indispensable in online communications. With users' growing\nneeds for more customized and expressive emoticons, recent messaging\napplications begin to support (limited) multi-modal emoticons: e.g., enhancing\nemoticons with animations or vibrotactile feedback. However, little empirical\nknowledge has been accumulated concerning how people create, share and\nexperience multi-modal emoticons in everyday communication, and how to better\nsupport them through design. To tackle this, we developed VibEmoji, a\nuser-authoring multi-modal emoticon interface for mobile messaging. Extending\nexisting designs, VibEmoji grants users greater flexibility to combine various\nemoticons, vibrations, and animations on-the-fly, and offers non-aggressive\nrecommendations based on these components' emotional relevance. Using VibEmoji\nas a probe, we conducted a four-week field study with 20 participants, to gain\nnew understandings from in-the-wild usage and experience, and extract\nimplications for design. We thereby contribute both a novel system and various\ninsights for supporting users' creation and communication of multi-modal\nemoticons.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13459,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Exploration Strategies for Tactile Graphics Displayed by\n  Electrovibration on a Touchscreen\n\n  Advancements in surface haptics technology have given rise to the development\nof interactive applications displaying tactile content on touch surfaces such\nas images, signs, diagrams, plots, charts, graphs, maps, networks, and tables.\nIn those applications, users manually explore the touch surface to interact\nwith the tactile data using some intuitive strategies. The user's exploration\nstrategy, tactile data's complexity, and tactile rendering method all affect\nthe user's haptic perception, which plays a critical role in design and\nprototyping of those applications. In this study, we conducted experiments with\nhuman participants to investigate the recognition rate and time of five tactile\nshapes rendered by electrovibration on a touchscreen using three different\nmethods and displayed in prototypical orientation and non-prototypical\norientations. The results showed that the correct recognition rate of the\nshapes was higher when the haptically active area was larger. However, as the\nnumber of edges increased, the recognition time increased and the recognition\nrate dropped significantly, arriving to a value slightly higher than the chance\nrate of 20% for non-prototypical octagon. We also recorded the participants'\nfinger movements on the touchscreen to examine their haptic exploration\nstrategies. Our analyses revealed that the participants first used global\nscanning to extract the coarse features of the displayed shapes, and then they\napplied local scanning to identify finer details, but needed another global\nscan for final confirmation in the case of non-prototypical shapes, possibly\ndue to the current limitations of electrovibration technology in displaying\ntactile stimuli to a user. We observed that it was highly difficult to follow\nthe edges of shapes and recognize shapes with more than five edges under\nelectrovibration when a single finger was used for exploration.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0846,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000220868,
      "text":"Friendscope: Exploring In-the-Moment Experience Sharing on Camera\n  Glasses via a Shared Camera\n\n  We introduce Friendscope, an instant, in-the-moment experience sharing system\nfor lightweight commercial camera glasses. Friendscope explores a new concept\ncalled a shared camera. This concept allows a wearer to share control of their\ncamera with a remote friend, making it possible for both people to capture\nphotos\/videos from the camera in the moment. Through a user study with 48\nparticipants, we found that users felt connected to each other, describing the\nshared camera as a more intimate form of livestreaming. Moreover, even\nprivacy-sensitive users were able to retain their sense of privacy and control\nwith the shared camera. Friendscope's different shared camera configurations\ngive wearers ultimate control over who they share the camera with and what\nphotos\/videos they share. We conclude with design implications for future\nexperience sharing systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0605,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"Real-Time Detection of Crowded Buses via Mobile Phones\n\n  Automated passenger counting (APC) technology is central to many aspects of\nthe public transit experience. APC information informs public transit planners\nabout utilization in a public transit system and operations about dynamic\nfluctuations in demand. Perhaps most importantly, APC information provides one\nmetric to the rider experience - standing during a long ride because of a\ncrowded vehicle is an unpleasant experience. Several technologies have been\nsuccessfully used for APC including light beam sensing and video image\nanalysis. However, these technologies are expensive and must be installed in\nbuses. In this paper, we analyze a new source of data using statistical models:\nrider smartphone accelerometers. Smartphones are ubiquitous in society and\naccelerometers have been shown to accurately model user states such as walking\nand sitting. We extend these models to use accelerometers to detect if the\nrider is standing or sitting on a bus. Standing riders are a signal that the\nbus is crowded. This paper provides evidence that user smartphones are a valid\nsource of participatory sensing and thus a new source of automated passenger\ncounting data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.15169,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Investigations of Smart Health Reliability\n\n  A balanced investigation into the reliability of wireless smart health\ndevices when it comes to the collection of biometric data under varying\nnetwork\/environmental conditions. Followed by a program implementation to begin\nintroductory analysis on measurement accuracy and data collection to gauge the\nreliability of smart health devices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.14928,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"An empirical user-study of text-based nonverbal annotation systems for\n  human-human conversations\n\n  the substantial increase in the number of online human-human conversations\nand the usefulness of multimodal transcripts, there is a rising need for\nautomated multimodal transcription systems to help us better understand the\nconversations. In this paper, we evaluated three methods to perform multimodal\ntranscription. They were (1) Jefferson -- an existing manual system used widely\nby the linguistics community, (2) MONAH -- a system that aimed to make\nmultimodal transcripts accessible and automated, (3) MONAH+ -- a system that\nbuilds on MONAH that visualizes machine attention. Based on 104 participants\nresponses, we found that (1) all text-based methods significantly reduced the\namount of information for the human users, (2) MONAH was found to be more\nusable than Jefferson, (3) Jefferson's relative strength was in chronemics\n(pace \/ delay) and paralinguistics (pitch \/ volume) annotations, whilst MONAH's\nrelative strength was in kinesics (body language) annotations, (4) enlarging\nwords' font-size based on machine attention was confusing human users as\nloudness. These results pose considerations for researchers designing a\nmultimodal annotation system for the masses who would like a fully-automated or\nhuman-augmented conversational analysis system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.12009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Travel Guides for Creative Tourists, Powered by Geotagged Social Media\n\n  Many modern tourists want to know about everyday life and spend time like a\nlocal in a new city. Current tools and guides typically provide them with lists\nof sights to see, which do not meet their needs. Manually building new tools\nfor them would not scale. However, public geotagged social media data, like\ntweets and photos, have the potential to fill this gap, showing users an\ninteresting and unique side of a place. Through three studies surrounding the\ndesign and construction of a social-media-powered Neighborhood Guides website,\nwe show recommendations for building such a site. Our findings highlight an\nimportant aspect of social media: while it lacks the user base and consistency\nto directly reflect users' lives, it does reveal the idealized everyday life\nthat so many visitors want to know about.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.0374,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"A Grammar-Based Approach for Applying Visualization Taxonomies to\n  Interaction Logs\n\n  Researchers collect large amounts of user interaction data with the goal of\nmapping user's workflows and behaviors to their higher-level motivations,\nintuitions, and goals. Although the visual analytics community has proposed\nnumerous taxonomies to facilitate this mapping process, no formal methods exist\nfor systematically applying these existing theories to user interaction logs.\nThis paper seeks to bridge the gap between visualization task taxonomies and\ninteraction log data by making the taxonomies more actionable for interaction\nlog analysis. To achieve this, we leverage structural parallels between how\npeople express themselves through interactions and language by reformulating\nexisting theories as regular grammars. We represent interactions as terminals\nwithin a regular grammar, similar to the role of individual words in a\nlanguage, and patterns of interactions or non-terminals as regular expressions\nover these terminals to capture common language patterns. To demonstrate our\napproach, we generate regular grammars for seven visualization taxonomies and\ndevelop code to apply them to three interaction log datasets. In analyzing our\nresults, we find that existing taxonomies at the low-level (i.e., terminals)\nshow mixed results in expressing multiple interaction log datasets, and\ntaxonomies at the high-level (i.e., regular expressions) have limited\nexpressiveness, due to primarily two challenges: inconsistencies in interaction\nlog dataset granularity and structure, and under-expressiveness of certain\nterminals. Based on our findings, we suggest new research directions for the\nvisualization community for augmenting existing taxonomies, developing new\nones, and building better interaction log recording processes to facilitate the\ndata-driven development of user behavior taxonomies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02795,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000056293,
      "text":"Controlling camera movement in VR colonography\n\n  Immersive Colonography allows medical professionals to navigate inside the\nintricate tubular geometries of subject-specific 3D colon images using Virtual\nReality displays. Typically, camera travel is performed via Fly-Through or\nFly-Over techniques that enable semi-automatic traveling through a constrained,\nwell-defined path at user-controlled speeds. However, Fly-Through is known to\nlimit the visibility of lesions located behind or inside haustral folds. At the\nsame time, Fly-Over requires splitting the entire colon visualization into two\nspecific halves. In this paper, we study the effect of immersive Fly-Through\nand Fly-Over techniques on lesion detection and introduce a camera travel\ntechnique that maintains a fixed camera orientation throughout the entire\nmedial axis path. While these techniques have been studied in non-VR desktop\nenvironments, their performance is not well understood in VR setups. We\nperformed a comparative study to ascertain which camera travel technique is\nmore appropriate for constrained path navigation in Immersive Colonography and\nvalidated our conclusions with two radiologists. To this end, we asked 18\nparticipants to navigate inside a 3D colon to find specific marks. Our results\nsuggest that the Fly-Over technique may lead to enhanced lesion detection at\nthe cost of higher task completion times. Nevertheless, the Fly-Through method\nmay offer a more balanced trade-off between speed and effectiveness, whereas\nthe fixed camera orientation technique provided seemingly inferior performance\nresults. Our study further provides design guidelines and informs future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03147,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Effective Representation to Capture Collaboration Behaviors between\n  Explainer and User\n\n  An explainable AI (XAI) model aims to provide transparency (in the form of\njustification, explanation, etc) for its predictions or actions made by it.\nRecently, there has been a lot of focus on building XAI models, especially to\nprovide explanations for understanding and interpreting the predictions made by\ndeep learning models. At UCLA, we propose a generic framework to interact with\nan XAI model in natural language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04273,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000127157,
      "text":"Effects of Virtual Room Size and Objects on Relative Translation Gain\n  Thresholds in Redirected Walking\n\n  This paper investigates how the size of virtual space and objects within it\naffect the threshold range of relative translation gains, a Redirected Walking\n(RDW) technique that scales the user's movement in virtual space in different\nratios for the width and depth. While previous studies assert that a virtual\nroom's size affects relative translation gain thresholds on account of the\nvirtual horizon's location, additional research is needed to explore this\nassumption through a structured approach to visual perception in Virtual\nReality (VR). We estimate the relative translation gain thresholds in six\nspatial conditions configured by three room sizes and the presence of virtual\nobjects (3 X 2), which were set according to differing Angles of Declination\n(AoDs) between eye-gaze and the forward-gaze. Results show that both size and\nvirtual objects significantly affect the threshold range, it being greater in\nthe large-sized condition and furnished condition. This indicates that the\neffect of relative translation gains can be further increased by constructing a\nperceived virtual movable space that is even larger than the adjusted virtual\nmovable space and placing objects in it. Our study can be applied to adjust\nvirtual spaces in synchronizing heterogeneous spaces without coordinate\ndistortion where real and virtual objects can be leveraged to create realistic\nmutual spaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12153,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Improving Pre-movement Pattern Detection with Filter Bank Selection\n\n  Pre-movement decoding plays an important role in movement detection and is\nable to detect movement onset with low-frequency electroencephalogram (EEG)\nsignals before the limb moves. In related studies, pre-movement decoding with\nstandard task-related component analysis (STRCA) has been demonstrated to be\nefficient for classification between movement state and resting state. However,\nthe accuracies of STRCA differ among subbands in the frequency domain. Due to\nindividual differences, the best subband differs among subjects and is\ndifficult to be determined. This study aims to improve the performance of the\nSTRCA method by a feature selection on multiple subbands and avoid the\nselection of best subbands. This study first compares three frequency range\nsettings ($M_1$: subbands with equally spaced bandwidths; $M_2$: subbands whose\nhigh cut-off frequencies are twice the low cut-off frequencies; $M_3$: subbands\nthat start at some specific fixed frequencies and end at the frequencies in an\narithmetic sequence.). Then, we develop a mutual information based technique to\nselect the features in these subbands. A binary support vector machine\nclassifier is used to classify the selected essential features. The results\nshow that $M_3$ is a better setting than the other two settings. With the\nfilter banks in $M_3$, the classification accuracy of the proposed FBTRCA\nachieves 0.8700$\\pm$0.1022, which means a significantly improved performance\ncompared to STRCA (0.8287$\\pm$0.1101) as well as to the cross validation and\ntesting method (0.8431$\\pm$0.1078).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03817,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000008146,
      "text":"Tackling Multipath and Biased Training Data for IMU-Assisted BLE\n  Proximity Detection\n\n  Proximity detection is to determine whether an IoT receiver is within a\ncertain distance from a signal transmitter. Due to its low cost and high\npopularity, Bluetooth low energy (BLE) has been used to detect proximity based\non the received signal strength indicator (RSSI). To address the fact that RSSI\ncan be markedly influenced by device carriage states, previous works have\nincorporated RSSI with inertial measurement unit (IMU) using deep learning.\nHowever, they have not sufficiently accounted for the impact of multipath.\nFurthermore, due to the special setup, the IMU data collected in the training\nprocess may be biased, which hampers the system's robustness and\ngeneralizability. This issue has not been studied before. We propose PRID, an\nIMU-assisted BLE proximity detection approach robust against RSSI fluctuation\nand IMU data bias. PRID histogramizes RSSI to extract multipath features and\nuses carriage state regularization to mitigate overfitting due to IMU data\nbias. We further propose PRID-lite based on a binarized neural network to\nsubstantially cut memory requirements for resource-constrained devices. We have\nconducted extensive experiments under different multipath environments, data\nbias levels, and a crowdsourced dataset. Our results show that PRID\nsignificantly reduces false detection cases compared with the existing arts (by\nover 50%). PRID-lite further reduces over 90% PRID model size and extends 60%\nbattery life, with a minor compromise in accuracy (7%).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.00329,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000103977,
      "text":"A Systematic Literature Review on Persuasive Technology at the Workplace\n\n  Employees face decisions every day - in the absence of supervision. The\noutcome of these decisions can be influenced by digital workplace design\nthrough the power of persuasive technology. This paper provides a structured\nliterature review based on recent research on persuasive technology in the\nworkplace. It examines the design and use of persuasive systems from a variety\nof disciplinary perspectives and theories. The reviewed studies were\ncategorized into the research streams of technology design, user-centered\nresearch, and gamification. The purpose of the studies is categorized using a\nmodified definition of the persuasive systems design model. A number of\nexperimental studies show that alignment of the employee's behavior with the\nemployer's agenda can be achieved. A robust finding is the key role of\ninteractivity in granting employees a subjective experience of rapid and\nmeaningful feedback when using the interface.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.10186,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Gesture-based Human-Machine Interaction: Taxonomy, Problem Definition,\n  and Analysis\n\n  The possibility for humans to interact with physical or virtual systems using\ngestures has been vastly explored by researchers and designers in the last\ntwenty years to provide new and intuitive interaction modalities.\nUnfortunately, the literature about gestural interaction is not homogeneous,\nand it is characterised by a lack of shared terminology. This leads to\nfragmented results and makes it difficult for research activities to build on\ntop of state-of-the-art results and approaches. The analysis in this paper aims\nat creating a common conceptual design framework to enforce development efforts\nin gesture-based human-machine interaction. The main contributions of the paper\ncan be summarised as follows: (i) we provide a broad definition for the notion\nof functional gesture in human-machine interaction, (ii) we design a flexible\nand expandable gesture taxonomy, and (iii) we put forward a detailed problem\nstatement for gesture-based human-machine interaction. Finally, to support our\nmain contribution, the paper presents, and analyses 83 most pertinent articles\nclassified on the basis of our taxonomy and problem statement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.01688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Different Length, Different Needs: Qualitative Analysis of Threads in\n  Online Health Communities\n\n  Online health communities provide a knowledge exchange platform for a wide\nrange of diseases and health conditions. Informational and emotional support\nhelps forum participants orient around health issues beyond in-person doctor\nvisits. So far, little is known about the relation between the level of\nparticipation and participants' contributions in online health communities. To\ngain insights on the issue, we analyzed 456 posts in 56 threads from the\nDermatology sub-forum of an online health community. While low participation\nthreads (short threads) revolved around solving an individual's health issue\nthrough diagnosis suggestions and medical advice, participants in high\nparticipation threads (long threads) built collective knowledge and a sense of\ncommunity, typically discussing chronic and rare conditions that medical\nprofessionals were unfamiliar with or could not treat effectively. Our results\nsuggest that in short threads an individual's health issue is addressed, while\nin long threads, sub-communities about specific rare and chronic diseases\nemerge. This has implications for the user interface design of health forums,\nwhich could be developed to better support community building elements, even in\nshort threads.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05798,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"AI-Assisted Design Concept Exploration Through Character Space\n  Construction\n\n  We propose an AI-assisted design concept exploration tool, the \"Character\nSpace Construction\" (\"CSC\"). Concept designers explore and articulate the\ntarget product aesthetics and semantics in language, which is expressed using\n\"Design Concept Phrases\" (\"DCPs\"), that is, compound adjective phrases, and\ncontrasting terms that convey what are not their target design concepts.\nDesigners often utilize this dichotomy technique to communicate the nature of\ntheir aesthetic and semantic design concepts with stakeholders, especially in\nan early design development phase. The CSC assists this designers' cognitive\nactivity by constructing a \"Character Space\" (\"CS\"), which is a semantic\nquadrant system, in a structured manner. A CS created by designers with the\nassistance of the CSC enables them to discern and explain their design concepts\nin contrast with opposing terms. These terms in a CS are retrieved and combined\nin the CSC by using a knowledge graph. The CSC presents terms and phrases as\nlists of candidates to users from which users will choose in order to define\nthe target design concept, which is then visualized in a CS. The participants\nin our experiment, who were in the \"arts and design\" profession, were given two\nconditions under which to create DCPs and explain them. One group created and\nexplained the DCPs with the assistance of the proposed CSC, and the other did\nthe same task without this assistance, given the freedom to use any publicly\navailable web search tools instead. The result showed that the group assisted\nby the CSC indicated their tasks were supported significantly better,\nespecially in exploration, as measured by the Creativity Support Index (CSI).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12026,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Dynamic pricing and discounts by means of interactive presentation\n  systems in stationary point of sales\n\n  The main purpose of this article was to create a model and simulate the\nprofitability conditions of an interactive presentation system (IPS) with the\nrecommender system (RS) used in the kiosk. 90 million simulations have been run\nin Python with SymPy to address the problem of discount recommendation offered\nto the clients according to their usage of the IPS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04263,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"The Human Factor in AI Safety\n\n  AI-based systems have been used widely across various industries for\ndifferent decisions ranging from operational decisions to tactical and\nstrategic ones in low- and high-stakes contexts. Gradually the weaknesses and\nissues of these systems have been publicly reported including, ethical issues,\nbiased decisions, unsafe outcomes, and unfair decisions, to name a few.\nResearch has tended to optimize AI less has focused on its risk and unexpected\nnegative consequences. Acknowledging this serious potential risks and scarcity\nof re-search I focus on unsafe outcomes of AI. Specifically, I explore this\nissue from a Human-AI interaction lens during AI deployment. It will be\ndiscussed how the interaction of individuals and AI during its deployment\nbrings new concerns, which need a solid and holistic mitigation plan. It will\nbe dis-cussed that only AI algorithms' safety is not enough to make its\noperation safe. The AI-based systems' end-users and their decision-making\narchetypes during collaboration with these systems should be considered during\nthe AI risk management. Using some real-world scenarios, it will be highlighted\nthat decision-making archetypes of users should be considered a design\nprinciple in AI-based systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04593,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000126163,
      "text":"Ability-Based Methods for Personalized Keyboard Generation\n\n  This study introduces an ability-based method for personalized keyboard\ngeneration, wherein an individual's own movement and human-computer interaction\ndata are used to automatically compute a personalized virtual keyboard layout.\nOur approach integrates a multidirectional point-select task to characterize\ncursor control over time, distance, and direction. The characterization is\nautomatically employed to develop a computationally efficient keyboard layout\nthat prioritizes each user's movement abilities through capturing directional\nconstraints and preferences. We evaluated our approach in a study involving 16\nparticipants using inertial sensing and facial electromyography as an access\nmethod, resulting in significantly increased communication rates using the\npersonalized keyboard (52.0 bits\/min) when compared to a generically optimized\nkeyboard (47.9 bits\/min). Our results demonstrate the ability to effectively\ncharacterize an individual's movement abilities to design a personalized\nkeyboard for improved communication. This work underscores the importance of\nintegrating a user's motor abilities when designing virtual interfaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04453,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Obstacle avoidance for blind people using a 3D camera and a haptic\n  feedback sleeve\n\n  Navigation and obstacle avoidance are some of the hardest tasks for the\nvisually impaired. Recent research projects have proposed technological\nsolutions to tackle this problem. So far most systems fail to provide\nmultidimensional feedback while working under various lighting conditions. We\npresent a novel obstacle avoidance system by combining a 3D camera with a\nhaptic feedback sleeve. Our system uses the distance information of the camera\nand maps it onto a 2D vibration array on the forearm. In our functionality\ntests of the haptic feedback sleeve, users were able to correctly identify and\nlocalize 98,6% of single motor vibration patterns and 70% of multidirectional\nand multi-motor vibration patterns. The combined obstacle avoidance system was\nevaluated on a testing route in the dark, simulating a navigation task. All\nusers were able to complete the task and showed performance improvement over\nmultiple runs. The system is independent of lighting conditions and can be used\nindoors and outdoors. Therefore, the obstacle avoidance system demonstrates a\npromising approach towards using technology to enable more independence for the\nvisually impaired.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Real-Time Gaze Tracking with Event-Driven Eye Segmentation\n\n  Gaze tracking is increasingly becoming an essential component in Augmented\nand Virtual Reality. Modern gaze tracking al gorithms are heavyweight; they\noperate at most 5 Hz on mobile processors despite that near-eye cameras\ncomfortably operate at a r eal-time rate ($>$ 30 Hz). This paper presents a\nreal-time eye tracking algorithm that, on average, operates at 30 Hz on a\nmobile processor, achieves \\ang{0.1}--\\ang{0.5} gaze accuracies, all the while\nrequiring only 30K parameters, one to two orders of magn itude smaller than\nstate-of-the-art eye tracking algorithms. The crux of our algorithm is an\nAuto~ROI mode, which continuously pr edicts the Regions of Interest (ROIs) of\nnear-eye images and judiciously processes only the ROIs for gaze estimation. To\nthat end, we introduce a novel, lightweight ROI prediction algorithm by\nemulating an event camera. We discuss how a software emulation of events\nenables accurate ROI prediction without requiring special hardware. The code of\nour paper is available at https:\/\/github.com\/horizon-research\/edgaze.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02694,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"To Trust or to Stockpile: Modeling Human-Simulation Interaction in\n  Supply Chain Shortages\n\n  Understanding decision-making in dynamic and complex settings is a challenge\nyet essential for preventing, mitigating, and responding to adverse events\n(e.g., disasters, financial crises). Simulation games have shown promise to\nadvance our understanding of decision-making in such settings. However, an open\nquestion remains on how we extract useful information from these games. We\ncontribute an approach to model human-simulation interaction by leveraging\nexisting methods to characterize: (1) system states of dynamic simulation\nenvironments (with Principal Component Analysis), (2) behavioral responses from\nhuman interaction with simulation (with Hidden Markov Models), and (3)\nbehavioral responses across system states (with Sequence Analysis). We\ndemonstrate this approach with our game simulating drug shortages in a supply\nchain context. Results from our experimental study with 135 participants show\ndifferent player types (hoarders, reactors, followers), how behavior changes in\ndifferent system states, and how sharing information impacts behavior. We\ndiscuss how our findings challenge existing literature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.11028,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Exploring the Social Context of Collaborative Driving\n\n  The automation of the driving task affects both the primary driving task and\nthe automotive user interfaces. The liberation of user interface space and\ncognitive load on the driver allows for new ways to think about driving.\nRelated work showed that activities such as sleeping, watching TV, or working\nwill become more prevalent in the future. However, social aspects according to\nMaslow's hierarchy of needs have not yet been accounted for. We provide\ninsights of a focus group with N=5 experts in automotive user experience\nrevealing current practices such as social need fulfillment on journeys and\nsharing practices via messengers and a user study with N=12 participants of a\nfirst prototype supporting these needs in various automation levels showing\ngood usability and high potential to improve user experience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05533,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"GaVe: A Webcam-Based Gaze Vending Interface Using One-Point Calibration\n\n  Even before the Covid-19 pandemic, beneficial use cases for hygienic,\ntouchless human-machine interaction have been explored. Gaze input, i.e.,\ninformation input via eye-movements of users, represents a promising method for\ncontact-free interaction in human-machine systems. In this paper, we present\nthe GazeVending interface (GaVe), which lets users control actions on a display\nwith their eyes. The interface works on a regular webcam, available on most of\ntoday's laptops, and only requires a one-point calibration before use. GaVe is\ndesigned in a hierarchical structure, presenting broad item cluster to users\nfirst and subsequently guiding them through another selection round, which\nallows the presentation of a large number of items. Cluster\/item selection in\nGaVe is based on the dwell time of fixations, i.e., the time duration that\nusers look at a given Cluster\/item. A user study (N=22) was conducted to test\noptimal dwell time thresholds and comfortable human-to-display distances.\nUsers' perception of the system, as well as error rates and task completion\ntime were registered. We found that all participants were able to use the\nsystem with a short time training, and showed good performance during system\nusage, selecting a target item within a group of 12 items in 6.76 seconds on\naverage. Participants were able to quickly understand and know how to interact\nwith the interface. We provide design guidelines for GaVe and discuss the\npotentials of the system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07729,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Ergonomics Integrated Design Methodology using Parameter Optimization,\n  Computer-Aided Design, and Digital Human Modelling: A Case Study of a\n  Cleaning Equipment\n\n  Challenges of enhancing productivity by amplifying efficiency and man-machine\ncompatibility of equipment can be achieved by adopting advanced technologies.\nThis study aims to present and exemplify methodology for incorporating\nergonomics pro-actively into the design using computer-aided design and digital\nhuman modeling-based analysis. The cleaning equipment is parametrized to detect\nthe critical variables. The relations are then constrained through the 3DSSPP\nsoftware-based biomechanical and experimental analysis using a prototype.\nMATLAB and Minitab software is used for optimizing efficiency while satisfying\nthe established constraints. The experiment showed nearly 67%, 120%, and 241%\nsuccessive improvement in the mechanical advantage in comparison to their\nimmediate predecessors. A significant (6 point) reduction in rapid entire body\nassessment score has been observed in the final posture while working with the\nmanipulator. 3DSSPP suggested that the joint forces during the actuation of the\nmanipulator were acceptable to 99% of the working population. The study\ndemonstrated the potential of the methodology in revamping the equipment for\nimproved ergonomic design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.10148,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000067883,
      "text":"Toward a Minecraft Mod for Early Detection of Alzheimer's Disease in\n  Young Adults\n\n  This paper proposes a Minecraft-based system for early detection of\nAlzheimer's disease in young adults. Early detection, where spatial navigation\nis a crucial key, is regarded as an important way to prevent the disease. The\nproposed system is compared with a recent existing and thoroughly studied\nsystem using a game called Sea Hero Quest (SHQ), by analyzing spatial\nnavigational patterns of players. Our preliminary results show that spatial\nnavigational patterns in both systems are highly correlated, indicating that\nthe proposed system is likely as effective as the SHQ system for the detection\ntask.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13444,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"The Impact of Explanations on Layperson Trust in Artificial\n  Intelligence-Driven Symptom Checker Apps: Experimental Study\n\n  To achieve the promoted benefits of an AI symptom checker, laypeople must\ntrust and subsequently follow its instructions. In AI, explanations are seen as\na tool to communicate the rationale behind black-box decisions to encourage\ntrust and adoption. However, the effectiveness of the types of explanations\nused in AI-driven symptom checkers has not yet been studied. Social theories\nsuggest that why-explanations are better at communicating knowledge and\ncultivating trust among laypeople. This study ascertains whether explanations\nprovided by a symptom checker affect explanatory trust among laypeople (N=750)\nand whether this trust is impacted by their existing knowledge of disease.\n  Results suggest system builders developing explanations for symptom-checking\napps should consider the recipient's knowledge of a disease and tailor\nexplanations to each user's specific need. Effort should be placed on\ngenerating explanations that are personalized to each user of a symptom checker\nto fully discount the diseases that they may be aware of and to close their\ninformation gap.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.132,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"The Dark Side of Perceptual Manipulations in Virtual Reality\n\n  \"Virtual-Physical Perceptual Manipulations\" (VPPMs) such as redirected\nwalking and haptics expand the user's capacity to interact with Virtual Reality\n(VR) beyond what would ordinarily physically be possible. VPPMs leverage\nknowledge of the limits of human perception to effect changes in the user's\nphysical movements, becoming able to (perceptibly and imperceptibly) nudge\ntheir physical actions to enhance interactivity in VR. We explore the risks\nposed by the malicious use of VPPMs. First, we define, conceptualize and\ndemonstrate the existence of VPPMs. Next, using speculative design workshops,\nwe explore and characterize the threats\/risks posed, proposing mitigations and\npreventative recommendations against the malicious use of VPPMs. Finally, we\nimplement two sample applications to demonstrate how existing VPPMs could be\ntrivially subverted to create the potential for physical harm. This paper aims\nto raise awareness that the current way we apply and publish VPPMs can lead to\nmalicious exploits of our perceptual vulnerabilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000060267,
      "text":"SummaryLens -- A Smartphone App for Exploring Interactive Use of\n  Automated Text Summarization in Everyday Life\n\n  We present SummaryLens, a concept and prototype for a mobile tool that\nleverages automated text summarization to enable users to quickly scan and\nsummarize physical text documents. We further combine this with a\ntext-to-speech system to read out the summary on demand. With this concept, we\npropose and explore a concrete application case of bringing ongoing progress in\nAI and Natural Language Processing to a broad audience with interactive use\ncases in everyday life. Based on our implemented features, we describe a set of\npotential usage scenarios and benefits, including support for low-vision,\nlow-literate and dyslexic users. A first usability study shows that the\ninteractive use of automated text summarization in everyday life has noteworthy\npotential. We make the prototype available as an open-source project to\nfacilitate further research on such tools.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.11168,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"\"You have to prove the threat is real\": Understanding the needs of\n  Female Journalists and Activists to Document and Report Online Harassment\n\n  Online harassment is a major societal challenge that impacts multiple\ncommunities. Some members of community, like female journalists and activists,\nbear significantly higher impacts since their profession requires easy\naccessibility, transparency about their identity, and involves highlighting\nstories of injustice. Through a multi-phased qualitative research study\ninvolving a focus group and interviews with 27 female journalists and\nactivists, we mapped the journey of a target who goes through harassment. We\nintroduce PMCR framework, as a way to focus on needs for Prevention,\nMonitoring, Crisis and Recovery. We focused on Crisis and Recovery, and\ndesigned a tool to satisfy a target's needs related to documenting evidence of\nharassment during the crisis and creating reports that could be shared with\nsupport networks for recovery. Finally, we discuss users' feedback to this\ntool, highlighting needs for targets as they face the burden and offer\nrecommendations to future designers and scholars on how to develop tools that\ncan help targets manage their harassment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06494,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"BROOK Dataset: A Playground for Exploiting Data-Driven Techniques in\n  Human-Vehicle Interactive Designs\n\n  Emerging Autonomous Vehicles (AV) breed great potentials to exploit\ndata-driven techniques for adaptive and personalized Human-Vehicle\nInteractions. However, the lack of high-quality and rich data supports limits\nthe opportunities to explore the design space of data-driven techniques, and\nvalidate the effectiveness of concrete mechanisms. Our goal is to initialize\nthe efforts to deliver the building block for exploring data-driven\nHuman-Vehicle Interaction designs. To this end, we present BROOK dataset, a\nmulti-modal dataset with facial video records. We first brief our rationales to\nbuild BROOK dataset. Then, we elaborate how to build the current version of\nBROOK dataset via a year-long study, and give an overview of the dataset. Next,\nwe present three example studies using BROOK to justify the applicability of\nBROOK dataset. We also identify key learning lessons from building BROOK\ndataset, and discuss about how BROOK dataset can foster an extensive amount of\nfollow-up studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.11195,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Recommendations to Develop, Distribute and Market Sonification Apps\n\n  After decades of research, sonification is still rarely adopted in consumer\nelectronics, software and user interfaces. Outside the science and arts scenes\nthe term sonification seems not well known to the public. As a means of science\ncommunication, and in order to make software developers, producers of consumer\nelectronics and end users aware of sonification, we developed, distributed, and\npromoted Tiltification. This smartphone app utilizes sonification to inform\nusers about the tilt angle of their phone, so that they can use it as a torpedo\nlevel. In this paper we report on our app development, distribution and\npromotion strategies and reflect on their success in making the app in\nparticular, and sonification in general, better known to the public. Finally,\nwe give recommendations on how to develop, distribute and market sonification\napps. This article is dedicated to research institutions without commercial\ninterests.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06509,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000080797,
      "text":"PR-PL: A Novel Transfer Learning Framework with Prototypical\n  Representation based Pairwise Learning for EEG-Based Emotion Recognition\n\n  Affective brain-computer interfaces based on electroencephalography (EEG) is\nan important branch in the field of affective computing. However, individual\ndifferences and noisy labels seriously limit the effectiveness and\ngeneralizability of EEG-based emotion recognition models. In this paper, we\npropose a novel transfer learning framework with Prototypical Representation\nbased Pairwise Learning (PR-PL) to learn discriminative and generalized\nprototypical representations for emotion revealing across individuals and\nformulate emotion recognition as pairwise learning for alleviating the reliance\non precise label information. Extensive experiments are conducted on two\nbenchmark databases under four cross-validation evaluation protocols\n(cross-subject cross-session, cross-subject within-session, within-subject\ncross-session, and within-subject within-session). The experimental results\ndemonstrate the superiority of the proposed PR-PL against the state-of-the-arts\nunder all four evaluation protocols, which shows the effectiveness and\ngeneralizability of PR-PL in dealing with the ambiguity of EEG responses in\naffective studies. The source code is available at\nhttps:\/\/github.com\/KAZABANA\/PR-PL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.01335,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Recommendations for Visualization Recommendations: Exploring Preferences\n  and Priorities in Public Health\n\n  The promise of visualization recommendation systems is that analysts will be\nautomatically provided with relevant and high-quality visualizations that will\nreduce the work of manual exploration or chart creation. However, little\nresearch to date has focused on what analysts value in the design of\nvisualization recommendations. We interviewed 18 analysts in the public health\nsector and explored how they made sense of a popular in-domain dataset. in\nservice of generating visualizations to recommend to others. We also explored\nhow they interacted with a corpus of both automatically- and manually-generated\nvisualization recommendations, with the goal of uncovering how the design\nvalues of these analysts are reflected in current visualization recommendation\nsystems. We find that analysts champion simple charts with clear takeaways that\nare nonetheless connected with existing semantic information or domain\nhypotheses. We conclude by recommending that visualization recommendation\ndesigners explore ways of integrating context and expectation into their\nsystems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06839,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Close-up and Whispering: An Understanding of Multimodal and Parasocial\n  Interactions in YouTube ASMR videos\n\n  ASMR (Autonomous Sensory Meridian Response) has grown to immense popularity\non YouTube and drawn HCI designers' attention to its effects and applications\nin design. YouTube ASMR creators incorporate visual elements, sounds, motifs of\ntouching and tasting, and other scenarios in multisensory video interactions to\ndeliver enjoyable and relaxing experiences to their viewers. ASMRtists engage\nviewers by social, physical, and task attractions. Research has identified the\nbenefits of ASMR in mental wellbeing. However, ASMR remains an understudied\nphenomenon in the HCI community, constraining designers' ability to incorporate\nASMR in video-based designs. This work annotates and analyzes the interaction\nmodalities and parasocial attractions of 2663 videos to identify unique\nexperiences. YouTube comment sections are also analyzed to compare viewers'\nresponses to different ASMR interactions. We find that ASMR videos are\nexperiences of multimodal social connection, relaxing physical intimacy, and\nsensory-rich activity observation. Design implications are discussed to foster\nfuture ASMR-augmented video interactions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.04743,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000054638,
      "text":"Auditory Feedback for Standing Balance Improvement in Virtual Reality\n\n  Virtual Reality (VR) users often experience postural instability, i.e.,\nbalance problems, which could be a major barrier to universal usability and\naccessibility for all, especially for persons with balance impairments. Prior\nresearch has confirmed the imbalance effect, but minimal research has been\nconducted to reduce this effect. We recruited 42 participants (with balance\nimpairments: 21, without balance impairments: 21) to investigate the impact of\nseveral auditory techniques on balance in VR, specifically spatial audio,\nstatic rest frame audio, rhythmic audio, and audio mapped to the center of\npressure (CoP). Participants performed two types of tasks - standing visual\nexploration and standing reach and grasp. Within-subject results showed that\neach auditory technique improved balance in VR for both persons with and\nwithout balance impairments. Spatial and CoP audio improved balance\nsignificantly more than other auditory conditions. The techniques presented in\nthis research could be used in future virtual environments to improve standing\nbalance and help push VR closer to universal usability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.01374,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Personal Data Visualisation on Mobile Devices: A Systematic Literature\n  Review\n\n  Personal data cover multiple aspects of our daily life and activities,\nincluding health, finance, social, Internet, Etc. Personal data visualisations\naim to improve the user experience when exploring these large amounts of\npersonal data and potentially provide insights to assist individuals in their\ndecision making and achieving goals. People with different backgrounds, gender\nand ages usually need to access their data on their mobile devices. Although\nthere are many personal tracking apps, the user experience when using these\napps and visualisations is not evaluated yet. There are publications on\npersonal data visualisation in the literature. Still, no systematic literature\nreview investigated the gaps in this area to assist in developing new personal\ndata visualisation techniques focusing on user experience. In this systematic\nliterature review, we considered studies published between 2010 and 2020 in\nthree online databases. We screened 195 studies and identified 29 papers that\nmet our inclusion criteria. Our key findings are various types of personal\ndata, and users have been addressed well in the found papers, including health,\nsport, diet, Driving habits, lifelogging, productivity, Etc. The user types\nrange from naive users to expert and developers users based on the experiment's\ntarget. However, mobile device capabilities and limitations regarding data\nvisualisation tasks have not been well addressed. There are no studies on the\nbest practices of personal data visualisation on mobile devices, assessment\nframeworks for data visualisation, or design frameworks for personal data\nvisualisations\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13344,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Drowsiness detection using combined neuroimaging: Overview and\n  Challenges\n\n  Brain-computer interfaces (BCIs) collect, analyze, and convert brain activity\ninto instructions and send it to the detection system. BCI is becoming popular\nin under-brain activities in certain conditions such as attention-based tasks.\nResearchers have recently used combined neuroimaging techniques such as\nEEG+fNIRS and EEG+fMRI to solve many real-world problems. Drowsiness detection\nor sleep inertia is one of the central research areas for the combined\nneuroimaging techniques. This paper aims to investigate the recent application\nof combined neuroimaging-based BCI on drowsiness detection or sleep inertia. To\nthis end, this is the only overview paper of the combined neuroimaging-based\ndrowsiness detection system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02484,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"A \"Distance Matters\" Paradox: Facilitating Intra-Team Collaboration Can\n  Harm Inter-Team Collaboration\n\n  By identifying the socio-technical conditions required for teams to work\neffectively remotely, the Distance Matters framework has been influential in\nCSCW since its introduction in 2000. Advances in collaboration technology and\npractices have since brought teams increasingly closer to achieving these\nconditions. This paper presents a ten-month ethnography in a remote\norganization, where we observed that despite exhibiting excellent remote\ncollaboration, teams paradoxically struggled to collaborate across team\nboundaries. We extend the Distance Matters framework to account for inter-team\ncollaboration, arguing that challenges analogous to those in the original\nintra-team framework -- common ground, collaboration readiness, collaboration\ntechnology readiness, and coupling of work -- persist but are actualized\ndifferently at the inter-team scale. Finally, we identify a fundamental tension\nbetween the intra- and inter-team layers: the collaboration technology and\npractices that help individual teams thrive (e.g., adopting customized\ncollaboration software) can also prompt collaboration challenges in the\ninter-team layer, and conversely the technology and practices that facilitate\ninter-team collaboration (e.g., strong centralized IT organizations) can harm\npractices at the intra-team layer. The addition of the inter-team layer to the\nDistance Matters framework opens new opportunities for CSCW, where balancing\nthe tension between team and organizational collaboration needs will be a\ncritical technological, operational, and organizational challenge for remote\nwork in the coming decades.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09634,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Teaching Drones on the Fly: Can Emotional Feedback Serve as Learning\n  Signal for Training Artificial Agents?\n\n  We investigate whether naturalistic emotional human feedback can be directly\nexploited as a reward signal for training artificial agents via interactive\nhuman-in-the-loop reinforcement learning. To answer this question, we devise an\nexperimental setting inspired by animal training, in which human test subjects\ninteractively teach an emulated drone agent their desired\ncommand-action-mapping by providing emotional feedback on the drone's action\nselections. We present a first empirical proof-of-concept study and analysis\nconfirming that human facial emotion expression can be directly exploited as\nreward signal in such interactive learning settings. Thereby, we contribute\nempirical findings towards more naturalistic and intuitive forms of\nreinforcement learning especially designed for non-expert users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07614,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Judging a socially assistive robot (SAR) by its cover; The effect of\n  body structure, outline, and color on users' perception\n\n  Socially assistive robots (SARs) aim to provide assistance through social\ninteraction. Previous studies contributed to understanding users` perceptions\nand preferences regarding existing commercially available SARs. Yet, very few\nstudies regarding SARs' appearance used designated SAR designs, and even fewer\nevaluated isolated visual qualities (VQ). In this work, we aim to assess the\neffect of isolated VQs systematically. To achieve this, we first conducted\nmarket research and deconstructed the VQs attributed to SARs. Then, a\nreconstruction of body structure, outline, and color scheme was done, resulting\nin the creation of 30 new SAR models that differ in their VQs, allowing us to\nisolate one character at a time. We used these new designs to evaluate users'\npreferences and perceptions in two empirical studies. Our empirical findings\nlink VQs with perceptions of SAR characteristics. These can lead to forming\nguidelines for the industrial design processes of new SARs to match user\nexpectations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.1228,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Tactile Materials in Practice: Understanding the Experiences of Teachers\n  of the Visually Impaired\n\n  Teachers of the visually impaired (TVIs) regularly present tactile materials\n(tactile graphics, 3D models, and real objects) to students with vision\nimpairments. Researchers have been increasingly interested in designing tools\nto support the use of tactile materials, but we still lack an in-depth\nunderstanding of how tactile materials are created and used in practice today.\nTo address this gap, we conducted interviews with 21 TVIs and a 3-week diary\nstudy with eight of them. We found that tactile materials were regularly used\nfor academic as well as non-academic concepts like tactile literacy, motor\nability, and spatial awareness. Real objects and 3D models served as \"stepping\nstones\" to tactile graphics and our participants preferred to teach with 3D\nmodels, despite finding them difficult to create, obtain, and modify. Use of\ncertain materials also carried social implications; participants selected\nmaterials that fostered student independence and allow classroom inclusion. We\ncontribute design considerations, encouraging future work on tactile materials\nto enable student and TVI co-creation, facilitate rapid prototyping, and\npromote movement and spatial awareness. To support future research in this\narea, our paper provides a fundamental understanding of current practices. We\nbridge these practices to established pedagogical approaches and highlight\nopportunities for growth regarding this important genre of educational\nmaterials.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.10564,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000069208,
      "text":"Human-in-the-loop Machine Learning: A Macro-Micro Perspective\n\n  Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.04259,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Colouring the sculpture through corresponding area from 2D to 3D with\n  augmented reality\n\n  With the development of 3D modelling techniques and AR techniques, the\ntraditional methods of establishing 2D to 3D relation is no longer sufficient\nto meet the demand for complex models and rapid relation building. This\ndissertation presents a prototype development implemented that creating many-\nto-many correspondences by marking image and 3D model regions that can be used\nto colouring 3D model by colouring image for the end user. After comparing the\nthree methods in the conceptual design, I chose the creating render textures\nrelation to further development by changing to Zeus bust model and connecting\nthe AR environment. The results of testing each part of the prototype shows the\nviability of the creating render textures relation method. The advantages of it\nare easy to build many-to-many relations and adaptable for any model with\nproperly UV mapping. But there are still three main limitations and future work\nwill focus on solving them, building a database to store relation information\ndata and printing 3D coloured models in real world.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02281,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000068545,
      "text":"\"I'm Just Overwhelmed\": Investigating Physical Therapy Accessibility and\n  Technology Interventions for People with Disabilities and\/or Chronic\n  Conditions\n\n  Many individuals with disabilities and\/or chronic conditions (da\/cc)\nexperience symptoms that may require intermittent or on-going medical care.\nHowever, healthcare is an often-overlooked domain for accessibility work, where\naccess needs associated with temporary and long-term disability must be\naddressed to increase the utility of physical and digital interactions with\nhealthcare workers and spaces. Our work focuses on a specific domain of\nhealthcare often used by individuals with da\/cc: physical therapy (PT). Through\na twelve-person interview study, we examined how people's access to PT for\ntheir da\/cc is hampered by social (e.g., physically visiting a PT clinic) and\nphysiological (e.g., chronic pain) barriers, and how technology could improve\nPT access. In-person PT is often inaccessible to our participants due to lack\nof transportation and insufficient insurance coverage. As such, many of our\nparticipants relied on at-home PT to manage their da\/cc symptoms and work\ntowards PT goals. Participants felt that PT barriers, such as having\nparticularly bad symptoms or feeling short on time, could be addressed with\nwell-designed technology that flexibly adapts to the person's dynamically\nchanging needs while supporting their PT goals. We introduce core design\nprinciples (adaptability, movement tracking, community building) and tensions\n(insurance) to consider when developing technology to support PT access.\nRethinking da\/cc access to PT from a lens that includes social and\nphysiological barriers presents opportunities to integrate accessibility and\nadaptability into PT technology.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13468,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Roadway Design Matters: Variation in Bicyclists' Psycho-Physiological\n  Responses in Different Urban Roadway Designs\n\n  As a healthier and more sustainable way of mobility, cycling has been\nadvocated by literature and policy. However, current trends in bicyclist crash\nfatalities suggest deficiencies in current roadway design in protecting these\nvulnerable road users. The lack of cycling data is a common challenge for\nstudying bicyclists' safety, behavior, and comfort levels under different\ndesign contexts. To understand bicyclists' behavioral and physiological\nresponses in an efficient and safe way, this study uses a bicycle simulator\nwithin an immersive virtual environment (IVE). Off-the-shelf sensors are\nutilized to evaluate bicyclists' cycling performance (speed and lane position)\nand physiological responses (eye tracking and heart rate (HR)). Participants\nbike in a simulated virtual environment modeled to scale from a real-world\nstreet with a shared bike lane (sharrow) to evaluate how introduction of a bike\nlane and a protected bike lane with pylons may impact perceptions of safety, as\nwell as behavioral and psycho-physiological responses. Results from 50\nparticipants show that the protected bike lane design received the highest\nperceived safety rating and exhibited the lowest average cycling speed.\nFurthermore, both the bike lane and the protected bike lane scenarios show a\nless dispersed gaze distribution than the as-built sharrow scenario, reflecting\na higher gaze focus among bicyclists on the biking task in the bike lane and\nprotected bike lane scenarios, compared to when bicyclists share right of way\nwith vehicles. Additionally, heart rate change point results from the study\nsuggest that creating dedicated zones for bicyclists (bike lanes or protected\nbike lanes) has the potential to reduce bicyclists' stress levels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09577,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"MolecuSense: Using Force-Feedback Gloves for Creating and Interacting\n  with Ball-and-Stick Molecules in VR\n\n  We contribute MolecuSense, a virtual version of a physical molecule\nconstruction kit, based on visualization in Virtual Reality (VR) and\ninteraction with force-feedback gloves. Targeting at chemistry education, our\ngoal is to make virtual molecule structures more tangible. Results of an\ninitial user study indicate that the VR molecular construction kit was\npositively received. Compared to a physical construction kit, the VR molecular\nconstruction kit is on the same level in terms of natural interaction. Besides,\nit fosters the typical digital advantages though, such as saving, exporting,\nand sharing of molecules. Feedback from the study participants has also\nrevealed potential future avenues for tangible molecule visualizations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.02186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000168549,
      "text":"Anatomy Studio II: A Cross-Reality Application for Teaching Anatomy\n\n  Virtual Reality has become an important educational tool, due to the pandemic\nand increasing globalization of education. This paper presents a framework for\nteaching Virtual Anatomy at the university level. Virtual classes have become a\nstaple of today's curricula because of the isolation and quarantine\nrequirements and the increased international collaboration. Our work builds on\nthe Visible Human Projects for Virtual Dissection material and provides a\nmedium for groups of students to do collaborative anatomical dissections in\nreal-time using sketching and 3D visualizations and audio coupled with\ninteractive 2D tablets for precise drawing. We describe the system\narchitecture, compare requirements with those of previous development, and\ndiscuss the preliminary results. Discussions with Anatomists show that this is\nan effective tool. We introduce avenues for further research and discuss\ncollaboration challenges posed by this context.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.15073,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Understanding Questions that Arise When Working with Business Documents\n\n  While digital assistants are increasingly used to help with various\nproductivity tasks, less attention has been paid to employing them in the\ndomain of business documents. To build an agent that can handle users'\ninformation needs in this domain, we must first understand the types of\nassistance that users desire when working on their documents. In this work, we\npresent results from two user studies that characterize the information needs\nand queries of authors, reviewers, and readers of business documents. In the\nfirst study, we used experience sampling to collect users' questions in-situ as\nthey were working with their documents, and in the second, we built a\nhuman-in-the-loop document Q&A system which rendered assistance with a variety\nof users' questions. Our results have implications for the design of document\nassistants that complement AI with human intelligence including whether\nparticular skillsets or roles within the document are needed from human\nrespondents, as well as the challenges around such systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.02001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"LegalVis: Exploring and Inferring Precedent Citations in Legal Documents\n\n  To reduce the number of pending cases and conflicting rulings in the\nBrazilian Judiciary, the National Congress amended the Constitution, allowing\nthe Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a\nset of understandings that both Executive and lower Judiciary branches must\nfollow. The STF's justices frequently cite the 58 existing BPs in their\ndecisions, and it is of primary relevance that judicial experts could identify\nand analyze such citations. To assist in this problem, we propose LegalVis, a\nweb-based visual analytics system designed to support the analysis of legal\ndocuments that cite or could potentially cite a BP. We model the problem of\nidentifying potential citations (i.e., non-explicit) as a classification\nproblem. However, a simple score is not enough to explain the results; that is\nwhy we use an interpretability machine learning method to explain the reason\nbehind each identified citation. For a compelling visual exploration of\ndocuments and BPs, LegalVis comprises three interactive visual components: the\nfirst presents an overview of the data showing temporal patterns, the second\nallows filtering and grouping relevant documents by topic, and the last one\nshows a document's text aiming to interpret the model's output by pointing out\nwhich paragraphs are likely to mention the BP, even if not explicitly\nspecified. We evaluated our identification model and obtained an accuracy of\n96%; we also made a quantitative and qualitative analysis of the results. The\nusefulness and effectiveness of LegalVis were evaluated through two usage\nscenarios and feedback from six domain experts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.14277,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"UAST: Unicode Aware Sanskrit Transliteration\n\nDevan\\=agar\\=i is the writing system that is adapted by various languages like Sanskrit. International Alphabet of Sanskrit Transliteration (IAST) is a transliteration scheme for romanisation of Sanskrit language. IAST makes use of diacritics to represent various characters. On a computer, these are represented using Unicode standard which differs from how the Sanskrit language behaves at a very fundamental level. This results in an issue that is encountered while designing typesetting software for devan\\=agar\\=i and IAST. We hereby discuss the problems and provide a solution that solves the issue of incompatibilities between various transliteration and encoding schemes. The base implementation that should be used is available at https:\/\/github.com\/dhruvildave\/uast.rs. Another implementation that extends UAST to around $10$ scripts is available at https:\/\/github.com\/aneri0x4f\/uast-cli and https:\/\/github.com\/dhruvildave\/uast .",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.15418,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"Comparative Evaluations of Visualization Onboarding Methods\n\n  Comprehending and exploring large and complex data is becoming increasingly\nimportant for users in a wide range of application domains. Still, non-experts\nin visual data analysis often have problems with correctly reading and\ninterpreting information from visualizations that are new to them. To support\nnovices in learning how to use new digital technologies, the concept of\nonboarding has been successfully applied in other fields and first approaches\nalso exist in the visualization domain. However, empirical evidence on the\neffectiveness of such approaches is scarce. Therefore, we conducted 3 studies:\n1) Firstly, we explored the effect of vis onboarding, using an interactive\nstep-by-step guide, on user performance for four increasingly complex\nvisualization techniques. We performed a between-subject experiment with 596\nparticipants in total. The results showed that there are no significant\ndifferences between the answer correctness of the questions with and without\nonboarding. Furthermore, participants commented that for highly familiar\nvisualization types no onboarding is needed. 2) Second, we performed another\nstudy with MTurk workers to assess if there is a difference in user\nperformances on different onboarding types: step-by-step, scrollytelling\ntutorial, and video tutorial. The study revealed that the video tutorial was\nranked as the most positive on average, based on sentiment analysis, followed\nby the scrollytelling tutorial and the interactive step-by-step guide. 3) For\nour third study with students, we gathered data on users' experience in using\nan in-situ scrollytelling for the VA tool. The results showed that they\npreferred scrollytelling over the tutorial integrated into the landing page. In\nsummary, the in-situ scrollytelling approach works well for visualization\nonboarding and a video tutorial can help to introduce interaction techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.00834,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Driver State Modeling through Latent Variable State Space Framework in\n  the Wild\n\n  Analyzing the impact of the environment on drivers' stress level and workload\nis of high importance for designing human-centered driver-vehicle interaction\nsystems and to ultimately help build a safer driving experience. However,\ndriver's state, including stress level and workload, are psychological\nconstructs that cannot be measured on their own and should be estimated through\nsensor measurements such as psychophysiological measures. We propose using a\nlatent-variable state-space modeling framework for driver state analysis. By\nusing latent-variable state-space models, we model drivers' workload and stress\nlevels as latent variables estimated through multimodal human sensing data,\nunder the perturbations of the environment in a state-space format and in a\nholistic manner. Through using a case study of multimodal driving data\ncollected from 11 participants, we first estimate the latent stress level and\nworkload of drivers from their heart rate, gaze measures, and intensity of\nfacial action units. We then show that external contextual elements such as the\nnumber of vehicles as a proxy for traffic density and secondary task demands\nmay be associated with changes in driver's stress levels and workload. We also\nshow that different drivers may be impacted differently by the aforementioned\nperturbations. We found out that drivers' latent states at previous timesteps\nare highly associated with their current states. Additionally, we discuss the\nutility of state-space models in analyzing the possible lag between the two\nconstructs of stress level and workload, which might be indicative of\ninformation transmission between the different parts of the driver's\npsychophysiology in the wild.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.1512,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"The...Tinderverse?: Opportunities and Challenges for User Safety in\n  Extended Reality (XR) Dating Apps\n\n  Dating apps such as Tinder have announced plans for a dating metaverse: the\nincorporation of XR technologies into the online dating process to augment\ninteractions between potential sexual partners across virtual and physical\nworlds. While the dating metaverse is still in conceptual stages we can\nforecast significant harms that it may expose daters to given prior research\ninto the frequency and severity of sexual harms facilitated by dating apps as\nwell as harms within social VR environments. In this workshop paper we envision\nhow XR could enrich virtual-to-physical interaction between potential sexual\npartners and outline harms that it will likely perpetuate as well. We then\nintroduce our ongoing research to preempt such harms: a participatory design\nstudy with sexual violence experts and demographics at disproportionate risk of\nsexual violence to produce mitigative solutions to sexual violence perpetuated\nby XR-enabled dating apps.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11479,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000152323,
      "text":"How Interest-Driven Content Creation Shapes Opportunities for Informal\n  Learning in Scratch: A Case Study on Novices' Use of Data Structures\n\n  Through a mixed-method analysis of data from Scratch, we examine how novices\nlearn to program with simple data structures by using community-produced\nlearning resources. First, we present a qualitative study that describes how\ncommunity-produced learning resources create archetypes that shape exploration\nand may disadvantage some with less common interests. In a second quantitative\nstudy, we find broad support for this dynamic in several hypothesis tests. Our\nfindings identify a social feedback loop that we argue could limit sources of\ninspiration, pose barriers to broadening participation, and confine learners'\nunderstanding of general concepts. We conclude by suggesting several approaches\nthat may mitigate these dynamics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00118,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Designing for emotion regulation interventions: an agenda for HCI theory\n  and research\n\n  There is a growing interest in HCI to envision, design, and evaluate\ntechnology-enabled interventions that support users' emotion regulation. This\ninterest stems in part from increased recognition that the ability to regulate\nemotions is critical to mental health, and that a lack of effective emotion\nregulation is a transdiagnostic factor for mental illness. However, the\npotential to combine innovative HCI designs with the theoretical grounding and\nstate-of-art interventions from psychology has yet to be fully realised. In\nthis paper, we synthesise HCI work on emotion regulation interventions and\npropose a three-part framework to guide technology designers in making: (i)\ntheory-informed decisions about intervention targets; (ii) strategic decisions\nregarding the technology-enabled intervention mechanisms to be included in the\nsystem; and (iii) practical decisions around previous implementations of the\nselected intervention components. We show how this framework can both\nsystematise HCI work to date and suggest a research agenda for future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.15834,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000062916,
      "text":"Collaborative Learning and Patterns of Practice\n\n  In this article, an overview of the background, the research approaches and\nthe patterns of practice in the field of collaborative learning are provided. A\ndefinition of collaborative learning and an overview of fundamental aspects\nthat shape research and practice in this field are included. Pedagogies and\nlearning theories that are used as foundations of the field alongside goals and\nobjectives of collaborative learning approaches are discussed. Popular patterns\nof practice, exploring their application in classrooms and elaborating on the\nstate of the art around those practices in research are outlined. A discussion\nabout important topics, open questions and future directions are provided in\nconclusion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05037,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"Human-GDPR Interaction: Practical Experiences of Accessing Personal Data\n\n  In our data-centric world, most services rely on collecting and using\npersonal data. The EU's General Data Protection Regulation (GDPR) aims to\nenhance individuals' control over their data, but its practical impact is not\nwell understood. We present a 10-participant study, where each participant\nfiled 4-5 data access requests. Through interviews accompanying these requests\nand discussions scrutinising returned data, it appears that GDPR falls short of\nits goals due to non-compliance and low-quality responses. Participants found\ntheir hopes to understand providers' data practices or harness their own data\nunmet. This causes increased distrust without any subjective improvement in\npower, although more transparent providers do earn greater trust. We propose\ndesigning more effective, data-inclusive and open policies and data access\nsystems to improve both customer relations and individual agency, and also that\nwider public use of GDPR rights could help with delivering accountability and\nmotivating providers to improve data practices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.12749,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Passive Haptic Rehearsal for Accelerated Piano Skill Acquisition\n\n  Passive haptic learning (PHL) uses vibrotactile stimulation to train piano\nsongs using repetition, even when the recipient of stimulation is focused on\nother tasks. However, many of the benefits of playing piano cannot be acquired\nwithout actively playing the instrument. In this position paper, we posit that\npassive haptic rehearsal, where active piano practice is assisted by separate\nsessions of passive stimulation, is of greater everyday use than solely PHL. We\npropose a study to examine the effects of passive haptic rehearsal for\nself-paced piano learners and consider how to incorporate passive rehearsal\ninto everyday practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11134,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Visualization Onboarding Grounded in Educational Theories\n\n  The aim of visualization is to support people in dealing with large and\ncomplex information structures, to make these structures more comprehensible,\nfacilitate exploration, and enable knowledge discovery. However, users often\nhave problems reading and interpreting data from visualizations, in particular\nwhen they experience them for the first time. A lack of visualization literacy,\ni.e., knowledge in terms of domain, data, visual encoding, interaction, and\nalso analytical methods can be observed. To support users in learning how to\nuse new digital technologies, the concept of onboarding has been successfully\napplied in other domains. However, it has not received much attention from the\nvisualization community so far. This chapter aims to fill this gap by defining\nthe concept and systematically laying out the design space of onboarding in the\ncontext of visualization as a descriptive design space. On this basis, we\npresent a survey of approaches from the academic community as well as from\ncommercial products, especially surveying educational theories that inform the\nonboarding strategies. Additionally, we derived design considerations based on\nprevious publications and present some guidelines for the design of\nvisualization onboarding concepts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.01157,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Artificial Concepts of Artificial Intelligence: Institutional Compliance\n  and Resistance in AI Startups\n\n  Scholars and industry practitioners have debated how to best develop\ninterventions for ethical artificial intelligence (AI). Such interventions\nrecommend that companies building and using AI tools change their technical\npractices, but fail to wrangle with critical questions about the organizational\nand institutional context in which AI is developed. In this paper, we\ncontribute descriptive research around the life of \"AI\" as a discursive concept\nand organizational practice in an understudied sphere--emerging AI\nstartups--and with a focus on extra-organizational pressures faced by\nentrepreneurs. Leveraging a theoretical lens for how organizations change, we\nconducted semi-structured interviews with 23 entrepreneurs working at\nearly-stage AI startups. We find that actors within startups both conform to\nand resist institutional pressures. Our analysis identifies a central tension\nfor AI entrepreneurs: they often valued scientific integrity and methodological\nrigor; however, influential external stakeholders either lacked the technical\nknowledge to appreciate entrepreneurs' emphasis on rigor or were more focused\non business priorities. As a result, entrepreneurs adopted hyped marketing\nmessages about AI that diverged from their scientific values, but attempted to\npreserve their legitimacy internally. Institutional pressures and\norganizational constraints also influenced entrepreneurs' modeling practices\nand their response to actual or impending regulation. We conclude with a\ndiscussion for how such pressures could be used as leverage for effective\ninterventions towards building ethical AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.01041,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Sensitive Pictures: Emotional Interpretation in the Museum\n\n  Museums are interested in designing emotional visitor experiences to\ncomplement traditional interpretations. HCI is interested in the relationship\nbetween Affective Computing and Affective Interaction. We describe Sensitive\nPictures, an emotional visitor experience co-created with the Munch art museum.\nVisitors choose emotions, locate associated paintings in the museum, experience\nan emotional story while viewing them, and self-report their response. A\nsubsequent interview with a portrayal of the artist employs computer vision to\nestimate emotional responses from facial expressions. Visitors are given a\nsouvenir postcard visualizing their emotional data. A study of 132 members of\nthe public (39 interviewed) illuminates key themes: designing emotional\nprovocations; capturing emotional responses; engaging visitors with their data;\na tendency for them to align their views with the system's interpretation; and\nintegrating these elements into emotional trajectories. We consider how\nAffective Computing can hold up a mirror to our emotions during Affective\nInteraction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.13592,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000098348,
      "text":"ILoveEye: Eyeliner Makeup Guidance System with Eye Shape Features\n\n  Drawing eyeliner is not an easy task for whom lacks experience in eye makeup.\nEveryone has a unique pair of eyes, so they need to draw eyeliner in a style\nthat suits their eyes. We proposed ILoveEye, an interactive system that\nsupports eye-makeup novices to draw natural and suitable eyeliner. The proposed\nsystem analyzes the shape of the user's eyes and classifies the eye types from\ncamera frame. The system can recommend the eyeliner style to the user based on\nthe designed recommendation rules. Then, the system can generate the original\npatterns corresponding to the eyeliner style, and the user can draw the\neyeliner while observing the real-time makeup guidance. The user evaluation\nexperiments are conducted to verify that the proposed ILoveEye system can help\nsome users to draw reasonable eyeliner based on the specific eye shapes and\nimprove their eye makeup skills.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.08314,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Cicero: A Declarative Grammar for Responsive Visualization\n\n  Designing responsive visualizations can be cast as applying transformations\nto a source view to render it suitable for a different screen size. However,\ndesigning responsive visualizations is often tedious as authors must manually\napply and reason about candidate transformations. We present Cicero, a\ndeclarative grammar for concisely specifying responsive visualization\ntransformations which paves the way for more intelligent responsive\nvisualization authoring tools. Cicero's flexible specifier syntax allows\nauthors to select visualization elements to transform, independent of the\nsource view's structure. Cicero encodes a concise set of actions to encode a\ndiverse set of transformations in both desktop-first and mobile-first design\nprocesses. Authors can ultimately reuse design-agnostic transformations across\ndifferent visualizations. To demonstrate the utility of Cicero, we develop a\ncompiler to an extended version of Vega-Lite, and provide principles for our\ncompiler. We further discuss the incorporation of Cicero into responsive\nvisualization authoring tools, such as a design recommender.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.00998,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000391404,
      "text":"Adorned in Memes: Exploring the Adoption of Social Wearables in Nordic\n  Student Culture\n\n  Social wearables promise to augment and enhance social interactions. However,\ndespite two decades of HCI research on wearables, we are yet to see widespread\nadoption of social wearables into everyday life. More in-situ investigations\ninto the social dynamics and cultural practices afforded by wearing interactive\ntechnology are needed to understand the drivers and barriers to adoption. To\nthis end, we study social wearables in the context of Nordic student culture\nand the students' practice of adorning boiler suits. Through a co-creation\nprocess, we designed Digi Merkki, a personalised interactive clothing patch. In\na two-week elicitation diary study, we captured how 16 students adopted Digi\nMerkki into their social practices. We found that Digi Merkki afforded a\nvariety of social interaction strategies, including sharing, spamming, and\nstealing pictures, which supported meaning-making and community-building. Based\non our findings, we articulate Memetic Expression as a strong concept for\ndesigning social wearables.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16495,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Crowdsourcing Creative Work\n\n  This article-based doctoral thesis explores the stakeholder perspectives and\nexperiences of crowdsourced creative work on two of the leading crowdsourcing\nplatforms. The thesis has two parts. In the first part, we explore creative\nwork from the perspective of the crowd worker. In the second part, we explore\nand study the requester's perspective in different contexts and several case\nstudies. The research is exploratory and we contribute empirical insights using\nsurvey-based and artefact-based approaches common in the field of\nHuman-Computer Interaction (HCI). In the former approach, we explore the key\nissues that may limit creative work on paid crowdsourcing platforms. In the\nlatter approach, we create computational artefacts to elicit authentic\nexperiences from both crowd workers and requesters of crowdsourced creative\nwork. The thesis contributes a classification of crowd workers into five\narchetypal profiles, based on the crowd workers' demographics, disposition, and\npreferences for creative work. We propose a three-part classification of\ncreative work on crowdsourcing platforms: creative tasks, creativity tests, and\ncreativity judgements (also referred to as creative feedback). The thesis\nfurther investigates the emerging research topic of how requesters can be\nsupported in interpreting and evaluating complex creative work. Last, we\ndiscuss the design implications for research and practice and contribute a\nvision of creative work on future crowdsourcing platforms with the aim of\nempowering crowd workers and fostering an ecosystem around tailored platforms\nfor creative microwork.\n  Keywords: creative work, creativity, creativity support tools, crowdsourcing\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07897,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000096361,
      "text":"Making Hidden Bias Visible: Designing a Feedback Ecosystem for Primary\n  Care Providers\n\n  Implicit bias may perpetuate healthcare disparities for marginalized patient\npopulations. Such bias is expressed in communication between patients and their\nproviders. We design an ecosystem with guidance from providers to make this\nbias explicit in patient-provider communication. Our end users are providers\nseeking to improve their quality of care for patients who are Black,\nIndigenous, People of Color (BIPOC) and\/or Lesbian, Gay, Bisexual, Transgender,\nand Queer (LGBTQ). We present wireframes displaying communication metrics that\nnegatively impact patient-centered care divided into the following categories:\ndigital nudge, dashboard, and guided reflection. Our wireframes provide\nquantitative, real-time, and conversational feedback promoting provider\nreflection on their interactions with patients. This is the first design\niteration toward the development of a tool to raise providers' awareness of\ntheir own implicit biases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.08156,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Interaction Design of Dwell Selection Toward Gaze-based AR\/VR\n  Interaction\n\n  In this paper, we first position the current dwell selection among gaze-based\ninteractions and its advantages against head-gaze selection, which is the\nmainstream interface for HMDs. Next, we show how dwell selection and head-gaze\nselection are used in an actual interaction situation. By comparing these two\nselection methods, we describe the potential of dwell selection as an essential\nAR\/VR interaction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.08805,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"PoseCoach: A Customizable Analysis and Visualization System for\n  Video-based Running Coaching\n\n  Videos are an accessible form of media for analyzing sports postures and\nproviding feedback to athletes. Existing sport-specific systems embed bespoke\nhuman pose attributes and thus can be hard to scale for new attributes,\nespecially for users without programming experiences. Some systems retain\nscalability by directly showing the differences between two poses, but they\nmight not clearly visualize the key differences that viewers would like to\npursue. Besides, video-based coaching systems often present feedback on the\ncorrectness of poses by augmenting videos with visual markers or reference\nposes. However, previewing and augmenting videos limit the analysis and\nvisualization of human poses due to the fixed viewpoints in videos, which\nconfine the observation of captured human movements and cause ambiguity in the\naugmented feedback. To address these issues, we study customizable human pose\ndata analysis and visualization in the context of running pose attributes, such\nas joint angles and step distances. Based on existing literature and a\nformative study, we have designed and implemented a system, PoseCoach, to\nprovide feedback on running poses for amateurs by comparing the running poses\nbetween a novice and an expert. PoseCoach adopts a customizable data analysis\nmodel to allow users' controllability in defining pose attributes of their\ninterests through our interface. To avoid the influence of viewpoint\ndifferences and provide intuitive feedback, PoseCoach visualizes the pose\ndifferences as part-based 3D animations on a human model to imitate the\ndemonstration of a human coach. We conduct a user study to verify our design\ncomponents and conduct expert interviews to evaluate the usefulness of the\nsystem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.09745,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000077817,
      "text":"ColorCode: A Bayesian Approach to Augmentative and Alternative\n  Communication with Two Buttons\n\n  Many people with severely limited muscle control can only communicate through\naugmentative and alternative communication (AAC) systems with a small number of\nbuttons. In this paper, we present the design for ColorCode, which is an AAC\nsystem with two buttons that uses Bayesian inference to determine what the user\nwishes to communicate. Our information-theoretic analysis of ColorCode\nsimulations shows that it is efficient in extracting information from the user,\neven in the presence of errors, achieving nearly optimal error correction.\nColorCode is provided as open source software\n(https:\/\/github.com\/mrdaly\/ColorCode).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.00188,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000173847,
      "text":"Finding Strategies Against Misinformation in Social Media: A Qualitative\n  Study\n\n  Misinformation spread through social media has become a fundamental challenge\nin modern society. Recent studies have evaluated various strategies for\naddressing this problem, such as by modifying social media platforms or\neducating people about misinformation, to varying degrees of success. Our goal\nis to develop a new strategy for countering misinformation: intelligent tools\nthat encourage social media users to foster metacognitive skills \"in the wild.\"\nAs a first step, we conducted focus groups with social media users to discover\nhow they can be best supported in combating misinformation. Qualitative\nanalyses of the discussions revealed that people find it difficult to detect\nmisinformation. Findings also indicated a need for but lack of resources to\nsupport cross-validation of information. Moreover, misinformation had a nuanced\nemotional impact on people. Suggestions for the design of intelligent tools\nthat support social media users in information selection, information\nengagement, and emotional response management are presented.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.02308,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"CalmResponses: Displaying Collective Audience Reactions in Remote\n  Communication\n\n  We propose a system displaying audience eye gaze and nod reactions for\nenhancing synchronous remote communication. Recently, we have had increasing\nopportunities to speak to others remotely. In contrast to offline situations,\nhowever, speakers often have difficulty observing audience reactions at once in\nremote communication, which makes them feel more anxious and less confident in\ntheir speeches. Recent studies have proposed methods of presenting various\naudience reactions to speakers. Since these methods require additional devices\nto measure audience reactions, they are not appropriate for practical\nsituations. Moreover, these methods do not present overall audience reactions.\nIn contrast, we design and develop CalmResponses, a browser-based system which\nmeasures audience eye gaze and nod reactions only with a built-in webcam and\ncollectively presents them to speakers. The results of our two user studies\nindicated that the number of fillers in speaker's speech decreases when\naudiences' eye gaze is presented, and their self-rating score increases when\naudiences' nodding is presented. Moreover, comments from audiences suggested\nbenefits of CalmResponses for them in terms of co-presence and privacy\nconcerns.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.02823,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000163582,
      "text":"\"Merging Results Is No Easy Task\": An International Survey Study of\n  Collaborative Data Analysis Practices Among UX Practitioners\n\n  Analysis is a key part of usability testing where UX practitioners seek to\nidentify usability problems and generate redesign suggestions. Although\nprevious research reported how analysis was conducted, the findings were\ntypically focused on individual analysis or based on a small number of\nprofessionals in specific geographic regions. We conducted an online\ninternational survey of 279 UX practitioners on their practices and challenges\nwhile collaborating during data analysis. We found that UX practitioners were\noften under time pressure to conduct analysis and adopted three modes of\ncollaboration: independently analyze different portions of the data and then\ncollaborate, collaboratively analyze the session with little or no independent\nanalysis, and independently analyze the same set of data and then collaborate.\nMoreover, most encountered challenges related to lack of resources,\ndisagreements with colleagues regarding usability problems, and difficulty\nmerging analysis from multiple practitioners. We discuss design implications to\nbetter support collaborative data analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.13494,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"A Spiral into the Mind: Gaze Spiral Visualization for Mobile Eye\n  Tracking\n\n  Comparing mobile eye tracking data from multiple participants without\ninformation about areas of interest (AOIs) is challenging because of individual\ntiming and coordinate systems. We present a technique, the gaze spiral, that\nvisualizes individual recordings based on image content of the stimulus. The\nspiral layout of the slitscan visualization is used to create a compact\nrepresentation of scanpaths. The visualization provides an overview of multiple\nrecordings even for long time spans and helps identify and annotate recurring\npatterns within recordings. The gaze spirals can also serve as glyphs that can\nbe projected to 2D space based on established scanpath metrics in order to\ninterpret the metrics and identify groups of similar viewing behavior. We\npresent examples based on two egocentric datasets to demonstrate the\neffectiveness of our approach for annotation and comparison tasks. Our examples\nshow that the technique has the potential to let users compare even long-term\nrecordings of pervasive scenarios without manual annotation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.05421,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000014868,
      "text":"A Survey on Crowdsourcing Applications in Smart Cities\n\n  With the emergence of the Internet of things (IoT), human life is now\nprogressing towards smartification faster than ever before. Thus, smart cities\nbecome automated in different aspects such as business, education, economy,\nmedicine, and urban areas. Since smartification requires a variety of dynamic\ninformation in different urban dimensions, mobile crowdsourcing has gained\nimportance in smart cities. This chapter systematically reviews the related\napplications of smart cities that use mobile crowdsourcing for data\nacquisition. For this purpose, the applications are classified as\nenvironmental, urban life, and transportation categories and then investigated\nin detail. This survey helps in understanding the current situation of smart\ncities from the viewpoint of crowdsourcing and discusses the future research\ndirections in this field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06377,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"EMMI: Empathic Human-Machine Interaction for Establishing Trust in\n  Automated Driving\n\n  Highly automated vehicles represent one of the most crucial development\nefforts in the automotive industry. In addition to the use of research\nvehicles, production vehicles for the general public are realistic in the near\nfuture. However, to fully exploit the benefits of these systems, it is\nfundamental that users have an appropriate level of trust in automation. Recent\nstudies indicate that more research is needed in this area. Furthermore, beyond\nthe management of user trust, the system should also convey a perceptible added\nvalue to realize not only trust, but also acceptance and thus use. The EMMI\nproject pushes both, the management of user trust while conveying added value\nto the user. Therefore, an advanced socio-emotional user model for estimating\nuser trust and various user-centered HMI systems with unique UX are being\ndeveloped. Together, the systems are employed to induce changes in the users'\ntrust in automated vehicles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.00079,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"\"Short on time and big on ideas\": Perspectives from Lab Members on\n  DIYBio Work in Community Biolabs\n\n  DIYbio challenges the status quo by positioning laboratory biology work\noutside of traditional institutions. HCI has increasingly explored the DIYbio\nmovement, but we lack insight into sites of practice such as community biolabs.\nTherefore, we gathered data on eleven community biolabs by interviewing sixteen\nlab managers and members. These labs represent half of identified organizations\nin scope worldwide. Participants detailed their practices and motivations,\noutlining the constraints and opportunities of their community biolabs. We\nfound that lab members conducted technically challenging project work with\naccess to high-end equipment and professional expertise. We found that the\nunique nature of biowork exacerbated challenges for cooperative work, partially\ndue to the particular time sensitivities of work with living organisms.\nBuilding on our findings, we discuss how community biolab members are creating\nnew approaches to laboratory biology and how this has design implications for\nsystems that support non-traditional settings for scientific practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.10191,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Alexa as an Active Listener: How Backchanneling Can Elicit\n  Self-Disclosure and Promote User Experience\n\n  Active listening is a well-known skill applied in human communication to\nbuild intimacy and elicit self-disclosure to support a wide variety of\ncooperative tasks. When applied to conversational UIs, active listening from\nmachines can also elicit greater self-disclosure by signaling to the users that\nthey are being heard, which can have positive outcomes. However, it takes\nconsiderable engineering effort and training to embed active listening skills\nin machines at scale, given the need to personalize active-listening cues to\nindividual users and their specific utterances. A more generic solution is\nneeded given the increasing use of conversational agents, especially by the\ngrowing number of socially isolated individuals. With this in mind, we\ndeveloped an Amazon Alexa skill that provides privacy-preserving and\npseudo-random backchanneling to indicate active listening. User study (N = 40)\ndata show that backchanneling improves perceived degree of active listening by\nsmart speakers. It also results in more emotional disclosure, with participants\nusing more positive words. Perception of smart speakers as active listeners is\npositively associated with perceived emotional support. Interview data\ncorroborate the feasibility of using smart speakers to provide emotional\nsupport. These findings have important implications for smart speaker\ninteraction design in several domains of cooperative work and social computing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Designing AI for Online-to-Offline Safety Risks with Young Women: The\n  Context of Social Matching\n\n  In this position paper we draw attention to safety risks against youth and\nyoung adults that originate through the combination of online and in-person\ninteraction, and opportunities for AI to address these risks. Our context of\nstudy is social matching systems (e.g., Tinder, Bumble), which are used by\nyoung adults for online-to-offline interaction with strangers, and which are\ncorrelated with sexual violence both online and in-person. The paper presents\nearly insights from an ongoing participatory AI design study in which young\nwomen build directly explainable models for detecting risk associated with\ndiscovered social opportunities, and articulate what AI should do once risk has\nbeen detected. We seek to advocate for participatory AI design as a way to\ndirectly incorporate youth and young adults into the design of a safer\nInternet. We also draw attention to challenges with the method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"Empathy-Centric Design At Scale\n\n  EmpathiCH aims at bringing together and blend different expertise to develop\nnew research agenda in the context of \"Empathy-Centric Design at Scale\". The\nmain research question is to investigate how new technologies can contribute to\nthe elicitation of empathy across and within multiple stakeholders at scale;\nand how empathy can be used to design solutions to societal problems that are\nnot only effective but also balanced, inclusive, and aware of their effect on\nsociety. Through presentations, participatory sessions, and a living experiment\n-- where data about the peoples' interactions is collected throughout the event\n-- we aim to make this workshop the ideal venue to foster collaboration, build\nnetworks, and shape the future direction of \"Empathy-Centric Design at Scale\".\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03546,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Conversational agents for fostering curiosity-driven learning in\n  children\n\n  Curiosity is an important factor that favors independent and individualized\nlearning in children. Research suggests that it is also a competence that can\nbe fostered by training specific metacognitive skills and information-searching\nbehaviors. In this light, we develop a conversational agent that helps children\ngenerate curiosity-driven questions, and encourages their use to lead\nautonomous explorations and gain new knowledge. The study was conducted with 51\nprimary school students who interacted with either a neutral agent or an\nincentive agent that helped curiosity-driven questioning by offering specific\nsemantic cues. Results showed a significant increase in the number and the\nquality of the questions generated with the incentive agent. This interaction\nalso resulted in longer explorations and stronger learning progress. Together,\nour results suggest that the more our agent is able to train children's\ncuriosity-related metacognitive skills, the better they can maintain their\ninformation-searching behaviors and the more new knowledge they are likely to\nacquire.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.01805,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Using Elo Rating as a Metric for Comparative Judgement in Educational\n  Assessment\n\n  Marking and feedback are essential features of teaching and learning, across\nthe overwhelming majority of educational settings and contexts. However, it can\ntake a great deal of time and effort for teachers to mark assessments, and to\nprovide useful feedback to the students. Furthermore, it also creates a\nsignificant cognitive load on the assessors, especially in ensuring fairness\nand equity. Therefore, an alternative approach to marking called comparative\njudgement (CJ) has been proposed in the educational space. Inspired by the law\nof comparative judgment (LCJ). This pairwise comparison for as many pairs as\npossible can then be used to rank all submissions. Studies suggest that CJ is\nhighly reliable and accurate while making it quick for the teachers.\nAlternative studies have questioned this claim suggesting that the process can\nincrease bias in the results as the same submission is shown many times to an\nassessor for increasing reliability. Additionally, studies have also found that\nCJ can result in the overall marking process taking longer than a more\ntraditional method of marking as information about many pairs must be\ncollected.\n  In this paper, we investigate Elo, which has been extensively used in rating\nplayers in zero-sum games such as chess. We experimented on a large-scale\nTwitter dataset on the topic of a recent major UK political event (\"Brexit\",\nthe UK's political exit from the European Union) to ask users which tweet they\nfound funnier between a pair selected from ten tweets. Our analysis of the data\nreveals that the Elo rating is statistically significantly similar to the CJ\nranking with a Kendall's tau score of 0.96 and a p-value of 1.5x10^(-5). We\nfinish with an informed discussion regarding the potential wider application of\nthis approach to a range of educational contexts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07899,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000222855,
      "text":"QTBIPOC PD: Exploring the Intersections of Race, Gender, and Sexual\n  Orientation in Participatory Design\n\n  As Human-Computer Interaction (HCI) research aims to be inclusive and\nrepresentative of many marginalized identities, there is still a lack of\navailable literature and research on intersectional considerations of race,\ngender, and sexual orientation, especially when it comes to participatory\ndesign. We aim to create a space to generate community recommendations for\neffectively and appropriately engaging Queer, Transgender, Black, Indigenous,\nPeople of Color (QTBIPOC) populations in participatory design, and discuss\nmethods of dissemination for recommendations. Workshop participants will engage\nwith critical race theory, queer theory, and feminist theory to reflect on\ncurrent exclusionary HCI and participatory design methods and practices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12071,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Modeling the Noticeability of User-Avatar Movement Inconsistency for\n  Sense of Body Ownership Intervention\n\n  An avatar mirroring the user's movement is commonly adopted in Virtual\nReality(VR). Maintaining the user-avatar movement consistency provides the user\na sense of body ownership and thus an immersive experience. However, breaking\nthis consistency can enable new interaction functionalities, such as pseudo\nhaptic feedback or input augmentation, at the expense of immersion. We propose\nto quantify the probability of users noticing the movement inconsistency while\nthe inconsistency amplitude is being enlarged, which aims to guide the\nintervention of the users' sense of body ownership in VR. We applied angular\noffsets to the avatar's shoulder and elbow joints and recorded whether the user\nidentified the inconsistency through a series of three user studies and built a\nstatistical model based on the results. Results show that the noticeability of\nmovement inconsistency increases roughly quadratically with the enlargement of\noffsets and the offsets at two joints negatively affect the probability\ndistributions of each other. Leveraging the model, we implemented a technique\nthat amplifies the user's arm movements with unnoticeable offsets and then\nevaluated implementations with different parameters(offset strength, offset\ndistribution). Results show that the technique with medium-level and\nbalanced-distributed offsets achieves the best overall performance. Finally, we\ndemonstrated our model's extendability in interventions in the sense of body\nownership with three VR applications including stroke rehabilitation, action\ngame and widget arrangement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06809,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Uplifting Interviews in Social Science with Individual Data\n  Visualization: the case of Music Listening\n\n  Collecting accurate and fine-grain information about the music people like,\ndislike and actually listen to has long been a challenge for sociologists. As\nmillions of people now use online music streaming services, research can build\nupon the individual listening history data that are collected by these\nplatforms. Individual interviews in particular can benefit from such data, by\nallowing the interviewers to immerse themselves in the musical universe of\nconsenting respondents, and thus ask them contextualized questions and get more\nprecise answers. Designing a visual exploration tool allowing such an immersion\nis however difficult, because of the volume and heterogeneity of the listening\ndata, the unequal \"visual literacy\" of the prospective users, or the\ninterviewers' potential lack of knowledge of the music listened to by the\nrespondents. In this case study we discuss the design and evaluation of such a\ntool. Designed with social scientists, its purpose is to help them in preparing\nand conducting semi-structured interviews that address various aspects of the\nlistening experience. It was evaluated during thirty interviews with consenting\nusers of a streaming platform in France.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.08437,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Strolling in Room-Scale VR: Hex-Core-MK1 Omnidirectional Treadmill\n\n  The natural locomotion interface is critical to the development of many VR\napplications. For household VR applications, there are two basic requirements:\nnatural immersive experience and minimized space occupation. The existing\nlocomotion strategies generally do not simultaneously satisfy these two\nrequirements well. This paper presents a novel omnidirectional treadmill (ODT)\nsystem, named Hex-Core-MK1 (HCMK1). By implementing two kinds of mirror\nsymmetrical spiral rollers to generate the omnidirectional velocity field, this\nproposed system is capable of providing real walking experiences with a\nfull-degree of freedom in an area as small as 1.76 m^2, while delivering great\nadvantages over several existing ODT systems in terms of weight, volume,\nlatency and dynamic performance. Compared with the sizes of Infinadeck and HCP,\nthe two best motor-driven ODTs so far, the 8 cm height of HCMK1 is only 20% of\nInfinadeck and 50% of HCP. In addition, HCMK1 is a lightweight device weighing\nonly 110 kg, which provides possibilities of further expanding VR scenarios,\nsuch as terrain simulation. The latency of HCMK1 is only 23ms. The experiments\nshow that HCMK1 can deliver on a starting acceleration of 16.00 m\/s^2 and a\nbraking acceleration of 30.00 m\/s^2.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.07041,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"VRCockpit: Mitigating Simulator Sickness in VR Games Using Multiple\n  Egocentric 2D View Frames\n\n  Virtual reality head-mounted displays (VR HMDs) have become a popular\nplatform for gaming. However, simulator sickness (SS) is still an impediment to\nVR's wider adoption, particularly in gaming. It can induce strong discomfort\nand impair players' immersion, performance, and enjoyment. Researchers have\nexplored techniques to mitigate SS. While these techniques have been shown to\nhelp lessen SS, they may not be applicable to games because they cannot be\neasily integrated into various types of games without impacting gameplay,\nimmersion, and performance. In this research, we introduce a new SS mitigation\ntechnique, VRCockpit. VRCockpit is a visual technique that surrounds the player\nwith four 2D views, one for each cardinal direction, that show 2D copies of the\nareas of the 3D environment around the player. To study its effectiveness, we\nconducted two different experiments, one with a car racing game, followed by a\nfirst-person shooter game. Our results show that VRCockpit has the potential to\nmitigate SS and still allows players to have the same level of immersion and\ngameplay performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.04845,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000066227,
      "text":"Revisiting Walking-in-Place by Introducing Step-Height Control, Elastic\n  Input, and Pseudo-Haptic Feedback\n\n  Walking-in-place (WIP) is a locomotion technique that enables users to \"walk\ninfinitely\" through vast virtual environments using walking-like gestures\nwithin a limited physical space. This paper investigates alternative\ninteraction schemes for WIP, addressing successively the control, input, and\noutput of WIP. First, we introduce a novel height-based control to increase\nadvanced speed. Second, we introduce a novel input system for WIP based on\nelastic and passive strips. Third, we introduce the use of pseudo-haptic\nfeedback as a novel output for WIP meant to alter walking sensations. The\nresults of a series of user studies show that height and frequency based\ncontrol of WIP can facilitate higher virtual speed with greater efficacy and\nease than in frequency-based WIP. Second, using an upward elastic input system\ncan result in a stable virtual speed control, although excessively strong\nelastic forces may impact the usability and user experience. Finally, using a\npseudo-haptic approach can improve the perceived realism of virtual slopes.\nTaken together, our results suggest that, for future VR applications, there is\nvalue in further research into the use of alternative interaction schemes for\nwalking-in-place.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.06116,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"How are Drivers' Stress Levels and Emotions Associated with the Driving\n  Context? A Naturalistic Study\n\n  Understanding and mitigating drivers' negative emotions, stress levels, and\nanxiety is of high importance for decreasing accident rates, and enhancing road\nsafety. While detecting drivers' stress and negative emotions can significantly\nhelp with this goal, understanding what might be associated with increases in\ndrivers' negative emotions and high stress level, might better help with\nplanning interventions. While studies have provided significant insight into\ndetecting drivers' emotions and stress levels, not many studies focused on the\nreasons behind changes in stress levels and negative emotions. In this study,\nby using a naturalistic driving study database, we analyze the changes in the\ndriving scene, including road objects and the dynamical relationship between\nthe ego vehicle and the lead vehicle with respect to changes in drivers'\npsychophysiological metrics (i.e., heart rate (HR) and facial expressions). Our\nresults indicate that different road objects might be associated with varying\nlevels of increase in drivers' HR as well as different proportions of negative\nfacial emotions detected through computer vision. Larger vehicles on the road,\nsuch as trucks and buses, are associated with the highest amount of increase in\ndrivers' HR as well as negative emotions. Additionally, shorter distances and\nhigher standard deviation in the distance to the lead vehicle are associated\nwith a higher number of abrupt increases in drivers' HR, depicting a possible\nincrease in stress level. Our finding indicates more positive emotions, lower\nfacial engagement, and a lower abrupt increase in HR at a higher speed of\ndriving, which often happens in highway environments. This research\ncollectively shows that driving at higher speeds happening in highways by\navoiding certain road objects might be a better fit for keeping drivers in a\ncalmer, more positive state.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15476,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"Augmenting Scientific Creativity with an Analogical Search Engine\n\n  Analogies have been central to creative problem-solving throughout the\nhistory of science and technology. As the number of scientific papers continues\nto increase exponentially, there is a growing opportunity for finding diverse\nsolutions to existing problems. However, realizing this potential requires the\ndevelopment of a means for searching through a large corpus that goes beyond\nsurface matches and simple keywords. Here we contribute the first end-to-end\nsystem for analogical search on scientific papers and evaluate its\neffectiveness with scientists' own problems. Using a human-in-the-loop AI\nsystem as a probe we find that our system facilitates creative ideation, and\nthat ideation success is mediated by an intermediate level of matching on the\nproblem abstraction (i.e., high versus low). We also demonstrate a fully\nautomated AI search engine that achieves a similar accuracy with the\nhuman-in-the-loop system. We conclude with design implications for enabling\nautomated analogical inspiration engines to accelerate scientific innovation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0689,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"AI and Citizen Science for Serendipity\n\n  It has been argued that introducing AI to creative practices destroys\nspontaneity, intuition and serendipity. However, the design of systems that\nleverage complex interactions between citizen scientists (members of the public\nengaged in research tasks) and computational AI methods have the potential to\nfacilitate creative exploration and chance encounters. Drawing from theories\nand literature about serendipity and computation, this article points to three\ninterrelated aspects that support the emergence of serendipity in hybrid\ncitizen science systems: the task environment; the characteristics of citizen\nscientists; and anomalies and errors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.04917,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Rich Screen Reader Experiences for Accessible Data Visualization\n\n  Current web accessibility guidelines ask visualization designers to support\nscreen readers via basic non-visual alternatives like textual descriptions and\naccess to raw data tables. But charts do more than summarize data or reproduce\ntables; they afford interactive data exploration at varying levels of\ngranularity -- from fine-grained datum-by-datum reading to skimming and\nsurfacing high-level trends. In response to the lack of comparable non-visual\naffordances, we present a set of rich screen reader experiences for accessible\ndata visualization and exploration. Through an iterative co-design process, we\nidentify three key design dimensions for expressive screen reader\naccessibility: structure, or how chart entities should be organized for a\nscreen reader to traverse; navigation, or the structural, spatial, and targeted\noperations a user might perform to step through the structure; and,\ndescription, or the semantic content, composition, and verbosity of the screen\nreader's narration. We operationalize these dimensions to prototype\nscreen-reader-accessible visualizations that cover a diverse range of chart\ntypes and combinations of our design dimensions. We evaluate a subset of these\nprototypes in a mixed-methods study with 13 blind and low vision readers. Our\nfindings demonstrate that these designs help users conceptualize data\nspatially, selectively attend to data of interest at different levels of\ngranularity, and experience control and agency over their data analysis\nprocess. An accessible HTML version of this paper is available at:\nhttp:\/\/vis.csail.mit.edu\/pubs\/rich-screen-reader-vis-experiences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.04149,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Identifying synthetic voices qualities for conversational agents\n\n  The present study aims to explore user acceptance and perceptions toward\ndifferent quality levels of synthetical voices. To achieve this, four voices\nhave been exploited considering two main factors: the quality of the voices\n(low vs high) and their gender (male and female). 186 volunteers were recruited\nand subsequently allocated into four groups of different ages respec-tively,\nadolescents, young adults, middle-aged and seniors. After having randomly\nlistened to each voice, participants were asked to fill the Virtual Agent Voice\nAcceptance Questionnaire (VAVAQ). Outcomes show that the two higher quality\nvoices of Antonio and Giulia were more appreciated than the low-quality voices\nof Edoardo and Clara by the whole sample in terms of pragmatic, hedonic and\nattractiveness qualities attributed to the voices. Concerning preferences\ntowards differently aged voices, it clearly appeared that they varied according\nto participants age' ranges examined. Furthermore, in terms of suitability to\nperform different tasks, participants considered Antonio and Giulia equally\nadapt for healthcare and front office jobs. Antonio was also judged to be\nsignificantly more qualified to accomplish protection and security tasks, while\nEdoardo was classified as the absolute least skilled in conducting household\nchores.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.02713,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000082122,
      "text":"Coding IxD: Enabling Interdisciplinary Education by Sparking Reflection\n\n  Educating students from diverse disciplinary backgrounds is challenging. In\nthis article, we report on our interdisciplinary course coding interaction and\ndesign (Coding IxD), which is designed for computer science and design students\nalike. This course has been developed over several years by consciously\ndeliberating on existing hurdles within the educational concept. First, we\nmotivate the need for Coding IxD and introduce the teaching principles that\nhelped shape the course's general structure. Our teaching principles\nmaterialize in four method-based phases derived from research through design.\nEach phase consists of several methods that emerged to be suitable in an\ninterdisciplinary context. Then, based on two selected student projects, we\nexemplify how interdisciplinary teams can arrive at novel interactive\nprototypes. We conclude by reflecting on our teaching practice as essential for\na meaningful learning experience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0751,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Crowdsourced Hypothesis Generation and their Verification: A Case Study\n  on Sleep Quality Improvement\n\n  A clinical study is often necessary for exploring important research\nquestions; however, this approach is sometimes time and money consuming.\nAnother extreme approach, which is to collect and aggregate opinions from\ncrowds, provides a result drawn from the crowds' past experiences and\nknowledge. To explore a solution that takes advantage of both the rigid\nclinical approach and the crowds' opinion-based approach, we design a framework\nthat exploits crowdsourcing as a part of the research process, whereby crowd\nworkers serve as if they were a scientist conducting a \"pseudo\" prospective\nstudy. This study evaluates the feasibility of the proposed framework to\ngenerate hypotheses on a specified topic and verify them in the real world by\nemploying many crowd workers. The framework comprises two phases of crowd-based\nworkflow. In Phase 1 - the hypothesis generation and ranking phase - our system\nasks workers two types of questions to collect a number of hypotheses and rank\nthem. In Phase 2 - the hypothesis verification phase - the system asks workers\nto verify the top-ranked hypotheses from Phase 1 by implementing one of them in\nreal life. Through experiments, we explore the potential and limitations of the\nframework to generate and evaluate hypotheses about the factors that result in\na good night's sleep. Our results on significant sleep quality improvement show\nthe basic feasibility of our framework, suggesting that crowd-based research is\ncompatible with experts' knowledge in a certain domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.06113,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000161595,
      "text":"Mask Wearing Status Estimation with Smartwatches\n\n  We present MaskReminder, an automatic mask-wearing status estimation system\nbased on smartwatches, to remind users who may be exposed to the COVID-19 virus\ntransmission scenarios, to wear a mask. MaskReminder with the powerful\nMLP-Mixer deep learning model can effectively learn long-short range\ninformation from the inertial measurement unit readings, and can recognize the\nmask-related hand movements such as wearing a mask, lowering the metal strap of\nthe mask, removing the strap from behind one side of the ears, etc. Extensive\nexperiments on 20 volunteers and 8000+ data samples show that the average\nrecognition accuracy is 89%. Moreover, MaskReminder is capable to remind a user\nto wear with a success rate of 90% even in the user-independent setting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0611,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Social Distancing Alert with Smartwatches\n\n  Social distancing is an efficient public health practice during the COVID-19\npandemic. However, people would violate the social distancing practice\nunconsciously when they conduct some social activities such as handshaking,\nhugging, kissing on the face or forehead, etc. In this paper, we present SoDA,\na social distancing practice violation alert system based on smartwatches, for\npreventing COVID-19 virus transmission. SoDA utilizes recordings of\naccelerometers and gyroscopes to recognize activities that may violate social\ndistancing practice with simple yet effective Vision Transformer models.\nExtensive experiments over 10 volunteers and 1800+ samples demonstrate that\nSoDA achieves social activity recognition with the accuracy of 94.7%, 1.8%\nnegative alert, and 2.2% missing alert.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.11014,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"The Impact of Surrounding Road Objects and Conditions on Drivers Abrupt\n  Heart Rate Changes\n\n  Recent studies have pointed out the importance of mitigating drivers stress\nand negative emotions. These studies show that certain road objects such as big\nvehicles might be associated with higher stress levels based on drivers\nsubjective stress measures. Additionally, research shows strong correlations\nbetween drivers stress levels and increased heart rate (HR). In this paper,\nbased on a naturalistic multimodal driving dataset, we analyze the visual\nscenes of driving in the vicinity of abrupt increases in drivers HR for the\npresence of certain stress-inducing road objects. We show that the probability\nof the presence of such objects increases when becoming closer to the abrupt\nincrease in drivers HR. Additionally, we show that drivers facial engagement\nchanges significantly in the vicinity of abrupt increases in HR. Our results\nlay the ground for a human-centered driving experience by detecting and\nmitigating drivers stress levels in the wild.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03183,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000136097,
      "text":"Visual Data Analysis with Task-based Recommendations\n\n  General visualization recommendation systems typically make design decisions\nfor the dataset automatically. However, most of them can only prune meaningless\nvisualizations but fail to recommend targeted results. This paper contributes\nTaskVis, a task-oriented visualization recommendation system that allows users\nto select their tasks precisely on the interface. We first summarize a task\nbase with 18 classical analytic tasks by a survey both in academia and\nindustry. On this basis, we maintain a rule base, which extends empirical\nwisdom with our targeted modeling of the analytic tasks. Then, our rule-based\napproach enumerates all the candidate visualizations through answer set\nprogramming. After that, the generated charts can be ranked by four ranking\nschemes. Furthermore, we introduce a task-based combination recommendation\nstrategy, leveraging a set of visualizations to give a brief view of the\ndataset collaboratively. Finally, we evaluate TaskVis through a series of use\ncases and a user study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03324,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000082453,
      "text":"Living Innovation Lab: A Human Centric Computing toward Healthy Living\n\n  Living Lab is an umbrella term used for referring to a methodology of\nuser-centric innovation in real-life environments within a wider network of\nrelevant stake holders. Real-life environment refers to living houses and\nhospitals inter wined and connected together in a way which promotes direct\nusability of research by the end users. It primarily consists of three stages,\nDesign thinking to actual Conceptualisation, Evaluation and Prototyping and\nFinal product prototyping to commercialisation. The increasing demand of\ncutting age healthcare system is in itself a challenge and requires user\ninvolvement to mobilise knowledge to build a patient centered and\nknowledge-based economy. Innovations are constantly needed to reduce the\nproblematic barriers to efficient knowledge exchange and improve collaborative\nproblem solving. Living Innovation Lab, as open knowledge system, have immense\npotential to address these gaps that are underexplored in the healthcare\nsystem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.1357,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"ClinicalPath: a Visualization tool to Improve the Evaluation of\n  Electronic Health Records in Clinical Decision-Making\n\n  Physicians work at a very tight schedule and need decision-making support\ntools to help on improving and doing their work in a timely and dependable\nmanner. Examining piles of sheets with test results and using systems with\nlittle visualization support to provide diagnostics is daunting, but that is\nstill the usual way for the physicians' daily procedure, especially in\ndeveloping countries. Electronic Health Records systems have been designed to\nkeep the patients' history and reduce the time spent analyzing the patient's\ndata. However, better tools to support decision-making are still needed. In\nthis paper, we propose ClinicalPath, a visualization tool for users to track a\npatient's clinical path through a series of tests and data, which can aid in\ntreatments and diagnoses. Our proposal is focused on patient's data analysis,\npresenting the test results and clinical history longitudinally. Both the\nvisualization design and the system functionality were developed in close\ncollaboration with experts in the medical domain to ensure a right fit of the\ntechnical solutions and the real needs of the professionals. We validated the\nproposed visualization based on case studies and user assessments through tasks\nbased on the physician's daily activities. Our results show that our proposed\nsystem improves the physicians' experience in decision-making tasks, made with\nmore confidence and better usage of the physicians' time, allowing them to take\nother needed care for the patients.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15133,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Compressing and Comparing the Generative Spaces of Procedural Content\n  Generators\n\n  The past decade has seen a rapid increase in the level of research interest\nin procedural content generation (PCG) for digital games, and there are now\nnumerous research avenues focused on new approaches for driving and applying\nPCG systems. An area in which progress has been comparatively slow is the\ndevelopment of generalisable approaches for comparing alternative PCG systems,\nespecially in terms of their generative spaces. It is to this area that this\npaper aims to make a contribution, by exploring the utility of data compression\nalgorithms in compressing the generative spaces of PCG systems. We hope that\nthis approach could be the basis for developing useful qualitative tools for\ncomparing PCG systems to help designers better understand and optimize their\ngenerators. In this work we assess the efficacy of a selection of algorithms\nacross sets of levels for 2D tile-based games by investigating how much their\nrespective generative space compressions correlate with level behavioral\ncharacteristics. We conclude that the approach looks to be a promising one\ndespite some inconsistency in efficacy in alternative domains, and that of the\nalgorithms tested Multiple Correspondence Analysis appears to perform the most\neffectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0098,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Multi-dimensional parameter-space partitioning of spatio-temporal\n  simulation ensembles\n\n  Numerical simulations are commonly used to understand the parameter\ndependence of given spatio-temporal phenomena. Sampling a multi-dimensional\nparameter space and running the respective simulations leads to an ensemble of\na large number of spatio-temporal simulation runs. A main objective for\nanalyzing the ensemble is to partition (or segment) the multi-dimensional\nparameter space into connected regions of simulation runs with similar\nbehavior. To facilitate such an analysis, we propose a novel visualization\nmethod for multi-dimensional parameter-space partitions. Our visualization is\nbased on the concept of a hyper-slicer, which allows for undistorted views of\nthe parameter-space segments' extent and transitions. For navigation within the\nparameter space, interactions with a 2D embedding of the parameter-space\nsamples, including their segment memberships, are supported. Parameter-space\npartitions are generated in a semi-automatic fashion by analyzing the\nsimilarity space of the ensemble's simulation runs. Clusters of similar\nsimulation runs induce the segments of the parameter-space partition. We link\nthe parameter-space partitioning visualizations to similarity-space\nvisualizations of the ensemble's simulation runs and embed them into an\ninteractive visual analysis tool that supports the analysis of all facets of\nthe spatio-temporal simulation ensemble targeted at the overarching goal of\nanalyzing the parameter-space partitioning. The partitioning can then be\nvisually analyzed and interactively refined. We evaluated our approach with\nexperts within case studies from three different domains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000118547,
      "text":"LV-Linker: Supporting Linked Exploration of Phone Usage Log Data and\n  Screen Video Data\n\n  Prior HCI studies often analyzed smartphone app usage data for usability and\nuser experience research purposes. App usage videos are often collected by a\nscreen recording app in order to better analyze the app usage behaviors (e.g.,\napp usage time, screen transition, and notification handling). However, it is\ndifficult to analyze app usage videos along with multiple user interaction\nstream data. When the length of a video is long, data analysis tends to take a\nlong time due to the volume of user interaction data. This is even more\ndifficult for novice researchers due to a lack of data analysis experience. In\nthis paper, we propose LV-Linker (Log and Video Linker), a visualization tool\nthat helps researchers quickly explore the app usage log and video data by\nlinking multiple time series log data with the video data. We conducted a\npreliminary user study with eight participants to evaluate the benefits of\nlinking, by measuring task completion time, helpfulness, and subjective task\nworkload. Our results showed that offering a linking feature significantly\nlowers the task completion time and task workload.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.00104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Smart operators in industry 4.0: A human-centered approach to enhance\n  operators' capabilities and competencies within the new smart factory context\n\n  As the Industry 4.0 takes shape, human operators experience an increased\ncomplexity of their daily tasks: they are required to be highly flexible and to\ndemonstrate adaptive capabilities in a very dynamic working environment. It\ncalls for tools and approaches that could be easily embedded into everyday\npractices and able to combine complex methodologies with high usability\nrequirements. In this perspective, the proposed research work is focused on the\ndesign and development of a practical solution, called Sophos-MS, able to\nintegrate augmented reality contents and intelligent tutoring systems with\ncutting-edge fruition technologies for operators' support in complex\nman-machine interactions. After establishing a reference methodological\nframework for the smart operator concept within the Industry 4.0 paradigm, the\nproposed solution is presented, along with its functional and non-function\nrequirements. Such requirements are fulfilled through a structured design\nstrategy whose main outcomes include a multi-layered modular solution,\nSophos-MS, that relies on Augmented Reality contents and on an intelligent\npersonal digital assistant with vocal interaction capabilities. The proposed\napproach has been deployed and its training potentials have been investigated\nwith field experiments. The experimental campaign results have been firstly\nchecked to ensure their statistical relevance and then analytically assessed in\norder to show that the proposed solution has a real impact on operators'\nlearning curves and can make the difference between who uses it and who does\nnot.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.05424,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"\"If it didn't happen, why would I change my decision?\": How Judges\n  Respond to Counterfactual Explanations for the Public Safety Assessment\n\n  Many researchers and policymakers have expressed excitement about algorithmic\nexplanations enabling more fair and responsible decision-making. However,\nrecent experimental studies have found that explanations do not always improve\nhuman use of algorithmic advice. In this study, we shed light on how people\ninterpret and respond to counterfactual explanations (CFEs) -- explanations\nthat show how a model's output would change with marginal changes to its\ninput(s) -- in the context of pretrial risk assessment instruments (PRAIs). We\nran think-aloud trials with eight sitting U.S. state court judges, providing\nthem with recommendations from a PRAI that includes CFEs. We found that the\nCFEs did not alter the judges' decisions. At first, judges misinterpreted the\ncounterfactuals as real -- rather than hypothetical -- changes to defendants.\nOnce judges understood what the counterfactuals meant, they ignored them,\nstating their role is only to make decisions regarding the actual defendant in\nquestion. The judges also expressed a mix of reasons for ignoring or following\nthe advice of the PRAI without CFEs. These results add to the literature\ndetailing the unexpected ways in which people respond to algorithms and\nexplanations. They also highlight new challenges associated with improving\nhuman-algorithm collaborations through explanations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03189,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Quantifying the Effects of Working in VR for One Week\n\n  Virtual Reality (VR) provides new possibilities for modern knowledge work.\nHowever, the potential advantages of virtual work environments can only be used\nif it is feasible to work in them for an extended period of time. Until now,\nthere are limited studies of long-term effects when working in VR. This paper\naddresses the need for understanding such long-term effects. Specifically, we\nreport on a comparative study (n=16), in which participants were working in VR\nfor an entire week -- for five days, eight hours each day -- as well as in a\nbaseline physical desktop environment. This study aims to quantify the effects\nof exchanging a desktop-based work environment with a VR-based environment.\nHence, during this study, we do not present the participants with the best\npossible VR system but rather a setup delivering a comparable experience to\nworking in the physical desktop environment. The study reveals that, as\nexpected, VR results in significantly worse ratings across most measures. Among\nother results, we found concerning levels of simulator sickness, below average\nusability ratings and two participants dropped out on the first day using VR,\ndue to migraine, nausea and anxiety. Nevertheless, there is some indication\nthat participants gradually overcame negative first impressions and initial\ndiscomfort. Overall, this study helps lay the groundwork for subsequent\nresearch, by clearly highlighting current shortcomings and identifying\nopportunities for improving the experience of working in VR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.1099,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Bridging the gap: Generating a design space model of Socially Assistive\n  Robots (SARs) for Older Adults using Participatory Design (PD)\n\n  Participatory Design (PD) methods are effective in understanding older\nadults' perspectives, concerns, and wishes and generating ideas for new\nintelligent aids. The aim of our study was first to map perceptions and explore\nthe needs of older users from socially assistive robots (SARs) and then, to\nintegrate new tools and experiences for end users to express their needs as\npart of a PD process. The outcome of the process is a design space model of\nfunctional, behavioral, and visual relationships and elicited emotions. This\nprocess enables to explore, map, and understand the needs of older users before\nand after experiencing with SARs, and to learn how they impact robotic designs,\nbehavior, and functionality. First, by interviewing older adults, caregivers,\nand relatives we learned older adults' daily routines, habits, and wishes.\nThen, we reconstructed those needs into design requirements and further\ndetailed them with older adults using focus groups. Based on the functional,\nbehavioral, and visual design factors that emerged from this phase, we built\nexperimental human-robot tasks, on a commercially available robot, to examine\nfeasibility and acceptance of the technology in one-on-one interactions.\nParticipants' responses throughout the study led to the creation of the design\nspace model mapping relationships and elicited emotions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.15021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000064572,
      "text":"A Novel Position-based VR Online Shopping Recommendation System based on\n  Optimized Collaborative Filtering Algorithm\n\n  This paper proposes a VR supermarket with an intelligent recommendation,\nwhich consists of three parts. The VR supermarket, the recommendation system,\nand the database. The VR supermarket provides a 360-degree virtual environment\nfor users to move and interact in the virtual environment through VR devices.\nThe recommendation system will make intelligent recommendations to the target\nusers based on the data in the database. The intelligent recommendation system\nis developed based on item similarity (ICF), which solves the cold start\nproblem of ICF. This allows VR supermarkets to present real-time\nrecommendations in any situation. It not only makes up for the lack of user\nperception of item attributes in traditional online shopping systems but also\nVR Supermarket improves the shopping efficiency of users through the\nintelligent recommendation system. The application can be extended to\nenterprise-level systems, which adds new possibilities for users to do VR\nshopping at home.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0356,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Mobile phone enabled Supply chain management in the RMG sector: A\n  conceptual framework\n\n  Relatively little is known about mobile phone use in a Supply Chain\nManagement (SCM) context, especially in the Bangladeshi Ready-Made Garment\n(RMG) industry. RMG is a very important industry for the Bangladeshi economy\nbut is criticized for long product supply times due to poor SCM. RMG requires\nobtaining real-time information and enhanced dynamic control, through utilizing\ninformation sharing and connecting stakeholders in garment manufacturing.\nHowever, a lack of IT support in the Bangladeshi RMG sector, the high price of\ncomputers and the low level of adoption of the computer-based internet are\nobstacles to providing sophisticated computer-aided SCM. Alternatively, the\nexplosive adoption of mobile phones and continuous improvement of this\ntechnology is an opportunity to provide mobile-based SCM for the RMG sector.\nThis research presents a mobile phone-based SCM framework for the Bangladeshi\nRMG sector. The proposed framework shows that mobile phone-based SCM can\npositively impact communication, information exchange, information retrieval\nand flow, coordination and management, which represent the main processes of\neffective SCM. However, to capitalize on these benefits, it is also important\nto discover the critical success factors and barriers to mobile SCM systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0893,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Wearable Haptic Device for Individuals with Congenital Absence of\n  Proprioception\n\n  A rare genetic condition, PIEZO2 loss of function (LOF) is characterized by\nabsence of proprioception and light touch, which makes functional tasks (e.g.,\nwalking, manipulation) difficult. There are no pharmacological treatments or\nassistive technologies available for individuals with PIEZO2-LOF. We propose a\nsensory substitution device that communicates proprioceptive feedback via\ndetectable haptic stimuli. We created a wearable prototype that maps\nmeasurements of elbow movement to deep pressure applied to the forearm. The\nprototype applies up to 18 N, includes an embedded force sensor, and is\nprogrammable to allow for various angle-to-pressure mappings. Future work\nincludes comparing proprioceptive acuity and movement ability with and without\nthe device in healthy and PIEZO2-LOF individuals, developing low-profile\ndevices using soft robotics, providing sensory substitution for multiple joints\nsimultaneously, and encoding additional aspects of joint dynamics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01678,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Toward a Sensitivity-based Implicit Measure of Patients' Important Goal\n  Pursuits\n\n  When individuals arrive to receive help from mental health providers, they do\nnot always have well specified and well established goals. It is the mental\nhealth providers responsibility to work collaboratively with patients to\nclarify their goals in the therapy sessions as well as life in general through\nclinical interviews, diagnostic assessments, and thorough observations.\nHowever, recognizing individuals important life goals is not always\nstraightforward. Here we introduce a novel method that gauges a patient\nimportant goal pursuits from their relative sensitivity to goal related words.\nPast research has shown that a person active goal pursuits cause them to be\nmore sensitive to the presence of goal related stimuli in the environment being\nable to consciously report those stimuli when others cannot see them. By\npresenting words related to a variety of different life goal pursuits very\nquickly for 50 msec or less, the patient would be expected to notice and be\naware of words related to their strongest motivations but not the other goal\nrelated words. These may or may not be among the goals they have identified in\ntherapy sessions, and the ones not previously identified can be fertile grounds\nfor further discussion and exploration in subsequent therapy sessions. Results\nfrom eight patient volunteers are described and discussed in terms of the\npotential utility of this supplemental personal therapy aid.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01421,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000104639,
      "text":"12 Years of Self-tracking for Promoting Physical Activity from a User\n  Diversity Perspective: Taking Stock and Thinking Ahead\n\n  Despite the indisputable personal and societal benefits of regular physical\nactivity, a large portion of the population does not follow the recommended\nguidelines, harming their health and wellness. The World Health Organization\nhas called upon governments, practitioners, and researchers to accelerate\naction to address the global prevalence of physical inactivity. To this end, an\nemerging wave of research in ubiquitous computing has been exploring the\npotential of interactive self-tracking technology in encouraging positive\nhealth behavior change. Numerous findings indicate the benefits of\npersonalization and inclusive design regarding increasing the motivational\nappeal and overall effectiveness of behavior change systems, with the ultimate\ngoal of empowering and facilitating people to achieve their goals. However,\nmost interventions still adopt a \"one-size-fits-all\" approach to their design,\nassuming equal effectiveness for all system features in spite of individual and\ncollective user differences. To this end, we analyze a corpus of 12 years of\nresearch in self-tracking technology for health behavior change, focusing on\nphysical activity, to identify those design elements that have proven most\neffective in inciting desirable behavior across diverse population segments. We\nthen provide actionable recommendations for designing and evaluating behavior\nchange self-tracking technology based on age, gender, occupation, fitness, and\nhealth condition. Finally, we engage in a critical commentary on the diversity\nof the domain and discuss ethical concerns surrounding tailored interventions\nand directions for moving forward.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.10967,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Audience Response Prediction from Textual Context\n\n  Humans' perception system closely monitors audio-visual cues during\nmultiparty interactions to react timely and naturally. Learning to predict\ntiming and type of reaction responses during human-human interactions may help\nus to enrich human-computer interaction applications. In this paper we consider\na presenter-audience setting and define an audience response prediction task\nfrom the presenter's textual speech. The task is formulated as a binary\nclassification problem as occurrence and absence of response after the\npresenter's textual speech. We use the BERT model as our classifier and\ninvestigate models with different textual contexts under causal and non-causal\nprediction settings. While the non-causal textual context, one sentence\npreceding and one sentence following the response event, can hugely improve the\naccuracy of predictions, we showed that longer textual contexts with causal\nsettings attain UAR and $F1$-Score improvements matching and exceeding the\nnon-causal textual context performance within the experimental evaluations on\nthe OPUS and TED datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.05522,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000086096,
      "text":"The Effects of Spatial Configuration on Relative Translation Gain\n  Thresholds in Redirected Walking\n\n  In this study, we explore how spatial configurations can be reflected in\ndetermining the threshold range of Relative Translation Gains (RTGs), a\ntranslation gain-based Redirected Walking (RDW) technique that scales the\nuser's movement in Virtual Reality (VR) in different ratios for width and\ndepth. While previous works have shown that various cognitive factors or\nindividual differences influence the RDW threshold, constructive studies\ninvestigating the impact of the environmental composition on the RDW threshold\nwith regard to the user's visual perception were lacking. Therefore, we\nexamined the effect of spatial configurations on the RTG threshold by analyzing\nthe participant's responses and gaze distribution data in two user studies. The\nfirst study concerned the size of the virtual room and the existence of objects\nwithin it, and the second study focused on the combined impact of room size and\nthe spatial layout. Our results show that three compositions of spatial\nconfiguration (size, object existence, spatial layout) significantly affect the\nRTG threshold range. Based on our findings, we proposed virtual space rescaling\nguidelines to increase the range of adjustable movable space with RTGs for\ndevelopers: placing distractors in the room, setting the perceived movable\nspace to be larger than the adjusted movable space if it's an empty room, and\navoid placing objects together as centered layout. Our findings can be used to\nadaptively rescale VR users' space according to the target virtual space's\nconfiguration with a unified coordinate system that enables the utilization of\nphysical objects in a virtual scene.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0448,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Comparison Study of Inertial Sensor Signal Combination for Human\n  Activity Recognition based on Convolutional Neural Networks\n\n  Human Activity Recognition (HAR) is one of the essential building blocks of\nso many applications like security, monitoring, the internet of things and\nhuman-robot interaction. The research community has developed various\nmethodologies to detect human activity based on various input types. However,\nmost of the research in the field has been focused on applications other than\nhuman-in-the-centre applications. This paper focused on optimising the input\nsignals to maximise the HAR performance from wearable sensors. A model based on\nConvolutional Neural Networks (CNN) has been proposed and trained on different\nsignal combinations of three Inertial Measurement Units (IMU) that exhibit the\nmovements of the dominant hand, leg and chest of the subject. The results\ndemonstrate k-fold cross-validation accuracy between 99.77 and 99.98% for\nsignals with the modality of 12 or higher. The performance of lower dimension\nsignals, except signals containing information from both chest and ankle, was\nfar inferior, showing between 73 and 85% accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.05312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000058942,
      "text":"Vehicle-To-Pedestrian Communication Feedback Module: A Study on\n  Increasing Legibility, Public Acceptance and Trust\n\n  Vehicle pedestrian communication is extremely important when developing\nautonomy for an autonomous vehicle. Enabling bidirectional nonverbal\ncommunication between pedestrians and autonomous vehicles will lead to an\nimprovement of pedestrians' safety in autonomous driving. If a pedestrian wants\nto communicate, the autonomous vehicle should provide feedback to the human\nabout what it is about to do. The user study presented in this paper\ninvestigated several possible options for an external vehicle display for\neffective nonverbal communication between an autonomous vehicle and a human.\nThe result of this study will guide the development of the feedback module in\nfuture studies, optimizing for public acceptance and trust in the autonomous\nvehicle's decision while being legible to the widest range of potential users.\nThe results of this study show that participants prefer symbols over text,\nlights and road projection. Additionally, participants prefer the combination\nof symbols and text as interaction modes to be displayed if the autonomous\nvehicle is not driving. Further, the results show that the text interaction\nmode option \"Safe to cross\" should be used combined with the symbol interaction\nmode option that displays a symbol of a walking person. We plan to elaborate\nand focus on the selected interaction modes via Virtual Reality and in the real\nworld in ongoing and future studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.15037,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000067552,
      "text":"Privacy Research with Marginalized Groups: What We Know, What's Needed,\n  and What's Next\n\n  People who are marginalized experience disproportionate harms when their\nprivacy is violated. Meeting their needs is vital for developing equitable and\nprivacy-protective technologies. In response, research at the intersection of\nprivacy and marginalization has acquired newfound urgency in the HCI and social\ncomputing community. In this literature review, we set out to understand how\nresearchers have investigated this area of study. What topics have been\nexamined, and how? What are the key findings and recommendations? And,\ncrucially, where do we go from here? Based on a review of papers on privacy and\nmarginalization published between 2010-2020 across HCI, Communication, and\nPrivacy-focused venues, we make three main contributions: (1) we identify key\nthemes in existing work and introduce the Privacy Responses and Costs framework\nto describe the tensions around protecting privacy in marginalized contexts,\n(2) we identify understudied research topics (e.g., race) and other avenues for\nfuture work, and (3) we characterize trends in research practices, including\nthe under-reporting of important methodological choices, and provide\nsuggestions for establishing shared best practices for this growing research\narea.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.06128,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000076493,
      "text":"Agents facilitate one category of human empathy through task difficulty\n\n  One way to improve the relationship between humans and anthropomorphic agents\nis to have humans empathize with the agents. In this study, we focused on a\ntask between agents and humans. We experimentally investigated hypotheses\nstating that task difficulty and task content facilitate human empathy. The\nexperiment was a two-way analysis of variance (ANOVA) with four conditions:\ntask difficulty (high, low) and task content (competitive, cooperative). The\nresults showed no main effect for the task content factor and a significant\nmain effect for the task difficulty factor. In addition, pre-task empathy\ntoward the agent decreased after the task. The ANOVA showed that one category\nof empathy toward the agent increased when the task difficulty was higher than\nwhen it was lower.This indicated that this category of empathy was more likely\nto be affected by the task. The task itself used can be an important factor\nwhen manipulating each category of empathy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02562,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"floodlight -- A high-level, data-driven sports analytics framework\n\n  The present work introduces floodlight, an open source Python package built\nto support and automate team sport data analysis. It is specifically designed\nfor the scientific analysis of spatiotemporal tracking data, event data, and\ngame codes in disciplines such as match and performance analysis, exercise\nphysiology, training science, and collective movement behavior analysis. It is\ncompletely provider- and sports-independent and includes a high-level interface\nsuitable for programming beginners. The package includes routines for most\naspects of the data analysis process, including dedicated data classes, file\nparsing functionality, public dataset APIs, pre-processing routines, common\ndata models and several standard analysis algorithms previously used in the\nliterature, as well as basic visualization functionality. The package is\nintended to make team sport data analysis more accessible to sport scientists,\nfoster collaborations between sport and computer scientists, and strengthen the\ncommunity's culture of open science and inclusion of previous works in future\nworks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.14863,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Mapping the Design Space of Human-AI Interaction in Text Summarization\n\n  Automatic text summarization systems commonly involve humans for preparing\ndata or evaluating model performance, yet, there lacks a systematic\nunderstanding of humans' roles, experience, and needs when interacting with or\nbeing assisted by AI. From a human-centered perspective, we map the design\nopportunities and considerations for human-AI interaction in text summarization\nand broader text generation tasks. We first conducted a systematic literature\nreview of 70 papers, developing a taxonomy of five interactions in AI-assisted\ntext generation and relevant design dimensions. We designed text summarization\nprototypes for each interaction. We then interviewed 16 users, aided by the\nprototypes, to understand their expectations, experience, and needs regarding\nefficiency, control, and trust with AI in text summarization and propose design\nconsiderations accordingly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.10273,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Embrace your incompetence! Designing appropriate CUI communication\n  through an ecological approach\n\n  People form impressions of their dialogue partners, be they other people or\nmachines, based on cues drawn from their communicative style. Recent work has\nsuggested that the gulf between people's expectations and the reality of CUI\ninteraction widens when these impressions are misaligned with the actual\ncapabilities of conversational user interfaces (CUIs). This has led some to\nrally against a perceived overriding concern for naturalness, calling instead\nfor more representative, or appropriate communicative cues. Indeed, some have\nargued for a move away from naturalness as a goal for CUI design and\ncommunication. We contend that naturalness need not be abandoned, if we instead\naim for ecologically grounded design. We also suggest a way this might be\nachieved and call on CUI designers to embrace incompetence! By letting CUIs\nexpress uncertainty and embarrassment through ecologically valid and\nappropriate cues that are ubiquitous in human communication - CUI designers can\nachieve more appropriate communication without turning away from naturalness\nentirely.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.14401,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Spectral-Loc: Indoor Localization using Light Spectral Information\n\n  For indoor settings, we investigate the impact of location on the spectral\ndistribution of the received light, i.e., the intensity of light for different\nwavelengths. Our investigations confirm that even under the same light source,\ndifferent locations exhibit slightly different spectral distribution due to\nreflections from their localised environment containing different materials or\ncolours. By exploiting this observation, we propose Spectral-Loc, a novel\nindoor localization system that uses light spectral information to identify the\nlocation of the device. With spectral sensors finding their way in latest\nproducts and applications, such as white balancing in smartphone photography,\nSpectral-Loc can be readily deployed without requiring any additional hardware\nor infrastructure. We prototype Spectral-Loc using a commercial-off-the-shelf\nlight spectral sensor, AS7265x, which can measure light intensity over 18\ndifferent wavelength sub-bands. We benchmark the localisation accuracy of\nSpectral-Loc against the conventional light intensity sensors that provide only\na single intensity value. Our evaluations over two different indoor spaces, a\nmeeting room and a large office space, demonstrate that use of light spectral\ninformation significantly reduces the localization error for the different\npercentiles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0296,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Mobile Health Solution for College Student Mental Health: Interview\n  Study and Design Requirement Analysis\n\n  Background: Mental health problems are prevalent in college students. The\nCOVID-19 pandemic exacerbated the problems, and created a surge in the\npopularity of telehealth and mobile health solutions. Despite that mobile\nhealth is a promising approach to help students with mental health needs, few\nstudies exist in investigating key features students need in a mental health\nself-management tool. Objective: The objective of our study was to identified\nkey requirements and features for the design of a student-centered mental\nhealth self-management tool. Methods: An interview study was first conducted to\nunderstand college students' needs and preferences on a mental health\nself-management tool. Functional information requirement analysis was then\nconducted to translate the needs into design implications. Results: A total of\n153 university students were recruited for the semi-structured interview. The\nparticipants mentioned several features including coping techniques, artificial\nintelligence, time management, tracking, and communication with others.\nParticipant's preferences on usability and privacy settings were also\ncollected. The desired functions were analyzed and turned into design-agnostic\ninformation requirements. Conclusions: This study documents findings from\ninterviews with university students to understand their needs and preferences\nfor a tool to help with self-management of mental health.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.07555,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Respect as a Lens for the Design of AI Systems\n\n  Critical examinations of AI systems often apply principles such as fairness,\njustice, accountability, and safety, which is reflected in AI regulations such\nas the EU AI Act. Are such principles sufficient to promote the design of\nsystems that support human flourishing? Even if a system is in some sense fair,\njust, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or\notherwise conflict with cultural, individual, or social values. This paper\nproposes a dimension of interactional ethics thus far overlooked: the ways AI\nsystems should treat human beings. For this purpose, we explore the\nphilosophical concept of respect: if respect is something everyone needs and\ndeserves, shouldn't technology aim to be respectful? Despite its intuitive\nsimplicity, respect in philosophy is a complex concept with many disparate\nsenses. Like fairness or justice, respect can characterise how people deserve\nto be treated; but rather than relating primarily to the distribution of\nbenefits or punishments, respect relates to how people regard one another, and\nhow this translates to perception, treatment, and behaviour. We explore respect\nbroadly across several literatures, synthesising perspectives on respect from\nKantian, post-Kantian, dramaturgical, and agential realist design perspectives\nwith a goal of drawing together a view of what respect could mean for AI. In so\ndoing, we identify ways that respect may guide us towards more sociable\nartefacts that ethically and inclusively honour and recognise humans using the\nrich social language that we have evolved to interact with one another every\nday.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.04767,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"What Do We Mean When We Say \"Insight\"? A Formal Synthesis of Existing\n  Theory\n\n  Researchers have derived many theoretical models for specifying users'\ninsights as they interact with a visualization system. These representations\nare essential for understanding the insight discovery process, such as when\ninferring user interaction patterns that lead to insight or assessing the rigor\nof reported insights. However, theoretical models can be difficult to apply to\nexisting tools and user studies, often due to discrepancies in how insight and\nits constituent parts are defined. This paper calls attention to the consistent\nstructures that recur across the visualization literature and describes how\nthey connect multiple theoretical representations of insight. We synthesize a\nunified formalism for insights using these structures, enabling a wider\naudience of researchers and developers to adopt the corresponding models.\nThrough a series of theoretical case studies, we use our formalism to compare\nand contrast existing theories, revealing interesting research challenges in\nreasoning about a user's domain knowledge and leveraging synergistic approaches\nin data mining and data management research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.05401,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"Who benefits from Visualization Adaptations? Towards a better\n  Understanding of the Influence of Visualization Literacy\n\n  The ability to read, understand, and comprehend visual information\nrepresentations is subsumed under the term visualization literacy (VL). One\npossibility to improve the use of information visualizations is to introduce\nadaptations. However, it is yet unclear whether people with different VL\nbenefit from adaptations to the same degree. We conducted an online experiment\n(n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis)\nof visualizations (bar charts, scatter plots) on performance (accuracy, time)\nand user experiences depends on users' VL level. Using linear mixed models for\nthe analyses, we found a positive impact of the De-Emphasis adaptation across\nall conditions, as well as an interaction effect of adaptation and VL on the\ntask completion time for bar charts. This work contributes to a better\nunderstanding of the intertwined relationship of VL and visual adaptations and\nmotivates future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.13749,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Nutzungsverhalten und Funktionsanforderungen digitaler\n  Trainingsanwendungen w\\\"ahrend der Pandemie\n\n  Due to contact restrictions, closure of fitness centers and quarantine\nmeasures, the SARS-CoV-2 pandemic led to a considerable decline of sporting\nactivities. The first relaxation of these restrictions allowed German citizens\nto mostly return to their normal training and exercise behavior, yet the\nlong-term impact of the recurring measures (i.e. the \"Lockdown\", \"Lockdown\nlight\" as well as the \"Corona Emergency Break\" in the case of Germany) remain\nrather under-investigated. Using a survey of (n=108) German sportspersons, we\nmeasured a significant decline of sporting activities even within the\nintermediary phases without major pandemic constraints. To evaluate the\ncapabilities of digital training applications in countering these effects, we\nadditionally recorded the usage of, among others, apps, trackers, videos and\nconferencing systems and identified the most important as well as missing\nand\/or essential features with regards to their capabilities of facilitating\nindividual sport and training in times without access to facilities or social\ncontacts. Effectively, the usage of smart watches, online videos and\nconferences increased significantly when compared to before the pandemic; and\nespecially online videos and conferences contributed to higher training\nfrequencies. Data-driven or individual feedback, motivation and collaboration\nrevealed to be the most important or even necessary functions for users of\ndigital training applications to counter the decline of social components of\ntraining.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07558,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Toward Systematic Design Considerations of Organizing Multiple Views\n\n  Multiple-view visualization (MV) has been used for visual analytics in\nvarious fields (e.g., bioinformatics, cybersecurity, and intelligence\nanalysis). Because each view encodes data from a particular perspective,\nanalysts often use a set of views laid out in 2D space to link and synthesize\ninformation. The difficulty of this process is impacted by the spatial\norganization of these views. For instance, connecting information from views\nfar from each other can be more challenging than neighboring ones. However,\nmost visual analysis tools currently either fix the positions of the views or\ncompletely delegate this organization of views to users (who must manually drag\nand move views). This either limits user involvement in managing the layout of\nMV or is overly flexible without much guidance. Then, a key design challenge in\nMV layout is determining the factors in a spatial organization that impact\nunderstanding. To address this, we review a set of MV-based systems and\nidentify considerations for MV layout rooted in two key concerns: perception,\nwhich considers how users perceive view relationships, and content, which\nconsiders the relationships in the data. We show how these allow us to study\nand analyze the design of MV layout systematically.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08401,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Marvista: Exploring the Design of a Human-AI Collaborative News Reading\n  Tool\n\n  We explore the design of Marvista -- a human-AI collaborative tool that\nemploys a suite of natural language processing models to provide end-to-end\nsupport for reading online news articles. Before reading an article, Marvista\nhelps a user plan what to read by filtering text based on how much time one can\nspend and what questions one is interested to find out from the article. During\nreading, Marvista helps the user reflect on their understanding of each\nparagraph with AI-generated questions. After reading, Marvista generates an\nexplainable human-AI summary that combines both AI's processing of the text,\nthe user's reading behavior, and user-generated data in the reading process. In\ncontrast to prior work that offered (content-independent) interaction\ntechniques or devices for reading, Marvista takes a human-AI collaborative\napproach that contributes text-specific guidance (content-aware) to support the\nentire reading process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07765,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"FairFuse: Interactive Visual Support for Fair Consensus Ranking\n\n  Fair consensus building combines the preferences of multiple rankers into a\nsingle consensus ranking, while ensuring any group defined by a protected\nattribute (such as race or gender) is not disadvantaged compared to other\ngroups. Manually generating a fair consensus ranking is time-consuming and\nimpractical -- even for a fairly small number of candidates. While algorithmic\napproaches for auditing and generating fair consensus rankings have been\ndeveloped, these have not been operationalized in interactive systems. To\nbridge this gap, we introduce FairFuse, a visualization system for generating,\nanalyzing, and auditing fair consensus rankings. We construct a data model\nwhich includes base rankings entered by rankers, augmented with measures of\ngroup fairness, and algorithms for generating consensus rankings with varying\ndegrees of fairness. We design novel visualizations that encode these measures\nin a parallel-coordinates style rank visualization, with interactions for\ngenerating and exploring fair consensus rankings. We describe use cases in\nwhich FairFuse supports a decision-maker in ranking scenarios in which fairness\nis important, and discuss emerging challenges for future efforts supporting\nfairness-oriented rank analysis. Code and demo videos available at\nhttps:\/\/osf.io\/hd639\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.04095,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"An Evaluation Study of 2D and 3D Teleconferencing for Remote Physical\n  Therapy\n\n  The present research investigates the effectiveness of using a telepresence\nsystem compared to a video conferencing system and the effectiveness of using\ntwo cameras compared to one camera for remote physical therapy. We used Telegie\nas our telepresence system, which allowed users to see an environment captured\nwith RGBD cameras in 3D through a VR headset. Since both telepresence and the\ninclusion of a second camera provide users with additional spatial information,\nwe examined this affordance within the relevant context of remote physical\ntherapy. Our dyadic study across different time zones paired 11 physical\ntherapists with 76 participants who took on the role of patients for a remote\nsession. Our quantitative questionnaire data and qualitative interviews with\ntherapists revealed several important findings. First, after controlling for\nindividual differences between participants, using two cameras had a marginally\nsignificant positive effect on physical therapy assessment scores from\ntherapists. Second, the spatial ability of patients was a strong predictor of\ntherapist assessment. And third, the video clarity of remote communication\nsystems mattered. Based on our findings, we offer several suggestions and\ninsights towards the future use of telepresence systems for remote\ncommunication.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.0231,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Personal Investigator: a Therapeutic 3D Game for Teenagers\n\n  This position paper describes the implementation and initial findings of a\ngame called Personal Investigator (PI). PI is an online 3D detective game that\nimplements a model of Brief Solution Focused Therapy (BSFT). It aims to help\nteenagers overcome mental health problems and engage with traditional mental\nhealth care services. It is predicted that the combination of goal-oriented\ngaming with a model of goal-oriented therapy will help to attract and sustain\nthe interest of teenagers, a group that therapists often have difficulty\nengaging with. PI is the first game to integrate this established psychotherapy\napproach into an engaging online 3D game.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.0335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000258287,
      "text":"A Case-Study on Variations Observed in Accelerometers Across Devices\n\n  Every year we grow more dependent on wearable devices to gather personalized\ndata, such as our movements, heart rate, respiration, etc. To capture this\ndata, devices contain sensors, such as accelerometers and gyroscopes, that are\nable to measure changes in their surroundings and pass along the information\nfor better informed decisions. Although these sensors should behave similarly\nin different devices, that is not always the case. In this case study, we\nanalyze accelerometers from three different devices recording the same actions\nwith an aim to determine whether the discrepancies are due to variability\nwithin or between devices. We found the most significant variation between\ndevices with different specifications, such as sensitivity and sampling\nfrequency. Nevertheless, variance in the data should be assumed, even if data\nis gathered from the same person, activity, and type of sensor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06837,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Revisiting Interest Indicators Derived from Web Reading Behavior for\n  Implicit User Modeling\n\n  Today, intelligent user interfaces on the web often come in form of\nrecommendation services tailoring content to individual users. Recommendation\nof web content such as news articles often requires a certain amount of\nexplicit ratings to allow for satisfactory results, i.e., the selection of\ncontent actually relevant for the user. Yet, the collection of such explicit\nratings is time-consuming and dependent on users' willingness to provide the\nrequired information on a regular basis. Thus, using implicit interest\nindicators can be a helpful complementation to relying on explicitly entered\ninformation only. Analysis of reading behavior on the web can be the basis for\nthe derivation of such implicit indicators. Previous work has already\nidentified several indicators and discussed how they can be used as a basis for\nuser models. However, most earlier work is either of conceptual nature and does\nnot involve studies to prove the suggested concepts or relies on meanwhile\npotentially outdated technology. All earlier discussions of the topic further\nhave in common that they do not yet consider mobile contexts. This paper builds\nupon earlier work, however providing a major update regarding technology and\nweb reading context, distinguishing between desktop and mobile settings. This\nupdate also allowed us to identify a set of new indicators that so far have not\nyet been discussed. This paper describes (i) our technical work, a framework\nfor analyzing user interactions with the browser relying on latest web\ntechnologies, (ii) the implicit interest indicators we either revisited or\nnewly identified, and (iii) the results of an online study on web reading\nbehavior as a basis for derivation of interest we conducted with 96\nparticipants.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.04284,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"How Do Drivers Self-Regulate their Secondary Task Engagements? The\n  Effect of Driving Automation on Touchscreen Interactions and Glance Behavior\n\n  With ever-improving driver assistance systems and large touchscreens becoming\nthe main in-vehicle interface, drivers are more tempted than ever to engage in\ndistracting non-driving-related tasks. However, little research exists on how\ndriving automation affects drivers' self-regulation when interacting with\ncenter stack touchscreens. To investigate this, we employ multilevel models on\na real-world driving dataset consisting of 10,139 sequences. Our results show\nsignificant differences in drivers' interaction and glance behavior in response\nto varying levels of driving automation, vehicle speed, and road curvature.\nDuring partially automated driving, drivers are not only more likely to engage\nin secondary touchscreen tasks, but their mean glance duration toward the\ntouchscreen also increases by 12% (Level 1) and 20% (Level 2) compared to\nmanual driving. We further show that the effect of driving automation on\ndrivers' self-regulation is larger than that of vehicle speed and road\ncurvature. The derived knowledge can facilitate the safety evaluation of\ninfotainment systems and the development of context-aware driver monitoring\nsystems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.00484,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0001129839,
      "text":"Multi-task Driver Steering Behaviour Modeling Using Time-Series\n  Transformer\n\n  Human intention prediction provides an augmented solution for the design of\nassistants and collaboration between the human driver and intelligent vehicles.\nIn this study, a multi-task sequential learning framework is developed to\npredict future steering torques and steering postures based on the upper limb\nneuromuscular Electromyography (EMG) signals. A single-right-hand driving mode\nis particularly studied. For this driving mode, three different driving\npostures are also evaluated. Then, a multi-task time-series transformer network\n(MTS-Trans) is developed to predict the steering torques and driving postures.\nTo evaluate the multi-task learning performance, four different frameworks are\nassessed. Twenty-one participants are involved in the driving simulator-based\nexperiment. The proposed model achieved accurate prediction results on the\nfuture steering torque prediction and driving postures recognition for\nsingle-hand driving modes. The proposed system can contribute to the\ndevelopment of advanced driver steering assistant systems and ensure mutual\nunderstanding between human drivers and intelligent vehicles.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.00496,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"Wigglite: Low-cost Information Collection and Triage\n\n  Consumers conducting comparison shopping, researchers making sense of\ncompetitive space, and developers looking for code snippets online all face the\nchallenge of capturing the information they find for later use without\ninterrupting their current flow. In addition, during many learning and\nexploration tasks, people need to externalize their mental context, such as\nestimating how urgent a topic is to follow up on, or rating a piece of evidence\nas a \"pro\" or \"con,\" which helps scaffold subsequent deeper exploration.\nHowever, current approaches incur a high cost, often requiring users to select,\ncopy, context switch, paste, and annotate information in a separate document\nwithout offering specific affordances that capture their mental context. In\nthis work, we explore a new interaction technique called \"wiggling,\" which can\nbe used to fluidly collect, organize, and rate information during early\nsensemaking stages with a single gesture. Wiggling involves rapid\nback-and-forth movements of a pointer or up-and-down scrolling on a smartphone,\nwhich can indicate the information to be collected and its valence, using a\nsingle, light-weight gesture that does not interfere with other interactions\nthat are already available. Through implementation and user evaluation, we\nfound that wiggling helped participants accurately collect information and\nencode their mental context with a 58% reduction in operational cost while\nbeing 24% faster compared to a common baseline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06769,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Virtual reality (VR) as a testing bench for consumer optical solutions:\n  A machine learning approach (GBR) to visual comfort under simulated\n  progressive addition lenses (PALS) distortions\n\n  For decades, manufacturers have attempted to reduce or eliminate the optical\naberrations that appear on the progressive addition lens' surfaces during\nmanufacturing. Besides every effort made, some of these distortions are\ninevitable given how lenses are fabricated, where in fact, astigmatism appears\non the surface and cannot be entirely removed or where non-uniform\nmagnification becomes inherent to the power change across the lens. Some\npresbyopes may refer to certain discomfort when wearing these lenses for the\nfirst time, and a subset of them might never adapt. Developing, prototyping,\ntesting and purveying those lenses into the market come at a cost, which is\nusually reflected in the retail price. This study aims to test the feasibility\nof virtual reality for testing customers' satisfaction with these lenses, even\nbefore getting them onto production. VR offers a controlled environment where\ndifferent parameters affecting progressive lens comforts, such as distortions,\nimage displacement or optical blurring, can be analysed separately. In this\nstudy, the focus was set on the distortions and image displacement, not taking\nblur into account. Behavioural changes (head and eye movements) were recorded\nusing the built-in eye tracker. Participants were significantly more displeased\nin the presence of highly distorted lens simulations. In addition, a gradient\nboosting regressor was fitted to the data, so predictors of discomfort could be\nunveiled, and ratings could be predicted without performing additional\nmeasurements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01995,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Data-driven Steering Torque Behaviour Modelling with Hidden Markov\n  Models\n\n  Modern Advanced Driver Assistance Systems (ADAS) are limited in their ability\nto consider the drivers intention, resulting in unnatural guidance and low\ncustomer acceptance. In this research, we focus on a novel data-driven approach\nto predict driver steering torque. In particular, driver behavior is modeled by\nlearning the parameters of a Hidden Markov Model (HMM) and estimation is\nperformed with Gaussian Mixture Regression (GMR). An extensive parameter\nselection framework enables us to objectively select the model hyper-parameters\nand prevents overfitting. The final model behavior is optimized with a cost\nfunction balancing between accuracy and smoothness. Naturalistic driving data\ncovering seven participants is obtained using a static driving simulator at\nToyota Motor Europe for the training, evaluation, and testing of the proposed\nmodel. The results demonstrate that our approach achieved a 92% steering torque\naccuracy with a 37% increase in signal smoothness and 90% fewer data compared\nto a baseline. In addition, our model captures the complex and nonlinear human\nbehavior and inter-driver variability from novice to expert drivers, showing an\ninteresting potential to become a steering performance predictor in future\nuser-oriented ADAS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06692,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000005232,
      "text":"Visualizing Gender Gap in Film Industry over the Past 100 Years\n\n  Visualizing big data can provide valuable insights into social science\nresearch. In this project, we focused on visualizing the potential gender gap\nin the global film industry over the past 100 years. We profiled the\ndifferences both for the actors\/actresses and male\/female movie audiences and\nanalyzed the IMDb data of the most popular 10,000 movies (the composition and\nimportance of casts of different genders, the cooperation network of the\nactors\/actresses, the movie genres, the movie descriptions, etc.) and audience\nratings (the differences between male's and female's ratings). Findings suggest\nthat the gender gap has been distinct in many aspects, but a recent trend is\nthat this gap narrows down and women are gaining discursive power in the film\nindustry. Our study presented rich data, vivid illustrations, and novel\nperspectives that can serve as the foundation for further studies on related\ntopics and their social implications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07727,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"OSCAR: A Semantic-based Data Binning Approach\n\n  Binning is applied to categorize data values or to see distributions of data.\nExisting binning algorithms often rely on statistical properties of data.\nHowever, there are semantic considerations for selecting appropriate binning\nschemes. Surveys, for instance, gather respondent data for demographic-related\nquestions such as age, salary, number of employees, etc., that are bucketed\ninto defined semantic categories. In this paper, we leverage common semantic\ncategories from survey data and Tableau Public visualizations to identify a set\nof semantic binning categories. We employ these semantic binning categories in\nOSCAR: a method for automatically selecting bins based on the inferred semantic\ntype of the field. We conducted a crowdsourced study with 120 participants to\nbetter understand user preferences for bins generated by OSCAR vs. binning\nprovided in Tableau. We find that maps and histograms using binned values\ngenerated by OSCAR are preferred by users as compared to binning schemes based\npurely on the statistical properties of the data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07998,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"No Grammar to Rule Them All: A Survey of JSON-style DSLs for\n  Visualization\n\n  There has been substantial growth in the use of JSON-based grammars, as well\nas other standard data serialization languages, to create visualizations. Each\nof these grammars serves a purpose: some focus on particular computational\ntasks (such as animation), some are concerned with certain chart types (such as\nmaps), and some target specific data domains (such as ML). Despite the\nprominence of this interface form, there has been little detailed analysis of\nthe characteristics of these languages. In this study, we survey and analyze\nthe design and implementation of 57 JSON-style DSLs for visualization. We\nanalyze these languages supported by a collected corpus of examples for each\nDSL (consisting of 4395 instances) across a variety of axes organized into\nconcerns related to domain, conceptual model, language relationships,\naffordances, and general practicalities. We identify tensions throughout these\nareas, such as between formal and colloquial specifications, among types of\nusers, and within the composition of languages. Through this work, we seek to\nsupport language implementers by elucidating the choices, opportunities, and\ntradeoffs in visualization DSL design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.13664,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Generic Approach to Visualization of Time Series Data\n\n  Time series is a collection of data instances that are ordered according to a\ntime stamp. Stock prices, temperature, etc are examples of time series data in\nreal life. Time series data are used for forecasting sales, predicting trends.\nVisualization is the process of visually representing data or the relationship\nbetween features of a data either in a two-dimensional plot or a\nthree-dimensional plot. Visualizing the time series data constitutes an\nimportant part of the process for working with a time series dataset.\nVisualizing the data not only helps in the modelling process but it can also be\nused to identify trends and features that cause those trends. In this work, we\ntake a real-life time series dataset and analyse how the target feature relates\nto other features of the dataset through visualization. From the work that has\nbeen carried out, we present an effective method of visualization for time\nseries data which will be much useful for machine learning modelling with such\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07643,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"ARShopping: In-Store Shopping Decision Support Through Augmented Reality\n  and Immersive Visualization\n\n  Online shopping gives customers boundless options to choose from, backed by\nextensive product details and customer reviews, all from the comfort of home;\nyet, no amount of detailed, online information can outweigh the instant\ngratification and hands-on understanding of a product that is provided by\nphysical stores. However, making purchasing decisions in physical stores can be\nchallenging due to a large number of similar alternatives and limited\naccessibility of the relevant product information (e.g., features, ratings, and\nreviews). In this work, we present ARShopping: a web-based prototype to\nvisually communicate detailed product information from an online setting on\nportable smart devices (e.g., phones, tablets, glasses), within the physical\nspace at the point of purchase. This prototype uses augmented reality (AR) to\nidentify products and display detailed information to help consumers make\npurchasing decisions that fulfill their needs while decreasing the\ndecision-making time. In particular, we use a data fusion algorithm to improve\nthe precision of the product detection; we then integrate AR visualizations\ninto the scene to facilitate comparisons across multiple products and features.\nWe designed our prototype based on interviews with 14 participants to better\nunderstand the utility and ease of use of the prototype.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08967,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000042717,
      "text":"Low Cost Portable Touch Screen Technology Applied to University Teaching\n\n  This article describes an implementation of low-cost portable touch screen\ntechnology, applied to university teaching, using as a base the remote control\nof the Nintendo Wii console (known as Wiimote), a normal projector, a computer\nand free software. The purpose is to show the feasibility of such\nimplementation to improve teaching\/learning processes, without incurring high\ncosts associated with unaffordable technological equipment, special\ninfrastructure in classrooms, or expensive computer programs. Also included is\na summary of a test of the system in two college courses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06679,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000008742,
      "text":"Neural Encoding of Songs is Modulated by Their Enjoyment\n\n  We examine user and song identification from neural (EEG) signals. Owing to\nperceptual subjectivity in human-media interaction, music identification from\nbrain signals is a challenging task. We demonstrate that subjective differences\nin music perception aid user identification, but hinder song identification. In\nan attempt to address intrinsic complexities in music identification, we\nprovide empirical evidence on the role of enjoyment in song recognition. Our\nfindings reveal that considering song enjoyment as an additional factor can\nimprove EEG-based song recognition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.06155,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"What Features Influence Impact Feel? A Study of Impact Feedback in\n  Action Games\n\n  Making the hit effect satisfy players is a long-standing problem faced by\naction game designers. However, no research systematically analyzed which game\ndesign elements affect such game feel. There is not even a term to describe it.\nSo, we propose to use impact feel to describe the player's feeling when\nreceiving juicy impact feedback. After collecting player's comments on action\ngames from Steam's top seller list, we trained a natural language processing\n(NLP) model to rank action games with their performance on impact feel. We\npresented a 19-feature framework of impact feedback design and examined it in\nthe top eight and last eight games. We listed an inventory of the usage of\nfeatures and found that hit stop, sound coherence, and camera control may\nstrongly influence players' impact feel. A lack of dedicated design on one of\nthese three features may ruin players' impact feel. Our findings may become an\nevaluation metric for future studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.05347,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Listen to Users, but Only 85% of the Time: How Black Swans Can Save\n  Innovation in a Data-Driven World\n\n  Data-driven design is a proven success factor that more and more digital\nbusinesses embrace. At the same time, academics and practitioners alike warn\nthat when virtually everything must be tested and proven with numbers, that can\nstifle creativity and innovation. This article argues that Taleb's Black Swan\ntheory can solve this dilemma. It shows that online experimentation, and\ntherefore digital design, are fat-tailed phenomena and, hence, prone to Black\nSwans. It introduces the notion of Black Swan designs -- \"crazy\" designs that\nmake sense only in hindsight -- along with four specific criteria. To ensure\nincremental improvements and their potential for innovation, businesses should\napply Taleb's barbell strategy: Invest 85-90% of resources into data-driven\napproaches and 10-15% into potential Black Swans.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.0839,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000004371,
      "text":"Auditory Feedback to Make Walking in Virtual Reality More Accessible\n\n  The objective of this study is to investigate the impact of several auditory\nfeedback modalities on gait (i.e., walking patterns) in virtual reality (VR).\nPrior research has substantiated gait disturbances in VR users as one of the\nprimary obstacles to VR usability. However, minimal research has been done to\nmitigate this issue. We recruited 39 participants (with mobility impairments:\n18, without mobility impairments: 21) who completed timed walking tasks in a\nreal-world environment and the same tasks in a VR environment with various\ntypes of auditory feedback. Within-subject results showed that each auditory\ncondition significantly improved gait performance while in VR (p < .001)\ncompared to the no auditory condition in VR for both groups of participants\nwith and without mobility impairments. Moreover, spatial audio improved gait\nperformance significantly (p < .001) compared to other auditory conditions for\nboth groups of participants. This research could help to make walking in VR\nmore accessible for people with and without mobility impairments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.0541,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"TagTeam: Towards Wearable-Assisted, Implicit Guidance for Human--Drone\n  Teams\n\n  The availability of sensor-rich smart wearables and tiny, yet capable,\nunmanned vehicles such as nano quadcopters, opens up opportunities for a novel\nclass of highly interactive, attention-shared human--machine teams. Reliable,\nlightweight, yet passive exchange of intent, data and inferences within such\nhuman--machine teams make them suitable for scenarios such as search-and-rescue\nwith significantly improved performance in terms of speed, accuracy and\nsemantic awareness. In this paper, we articulate a vision for such human--drone\nteams and key technical capabilities such teams must encompass. We present\nTagTeam, an early prototype of such a team and share promising demonstration of\na key capability (i.e., motion awareness).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04024,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Social Simulacra: Creating Populated Prototypes for Social Computing\n  Systems\n\n  Social computing prototypes probe the social behaviors that may arise in an\nenvisioned system design. This prototyping practice is currently limited to\nrecruiting small groups of people. Unfortunately, many challenges do not arise\nuntil a system is populated at a larger scale. Can a designer understand how a\nsocial system might behave when populated, and make adjustments to the design\nbefore the system falls prey to such challenges? We introduce social simulacra,\na prototyping technique that generates a breadth of realistic social\ninteractions that may emerge when a social computing system is populated.\nSocial simulacra take as input the designer's description of a community's\ndesign -- goal, rules, and member personas -- and produce as output an instance\nof that design with simulated behavior, including posts, replies, and\nanti-social behaviors. We demonstrate that social simulacra shift the behaviors\nthat they generate appropriately in response to design changes, and that they\nenable exploration of \"what if?\" scenarios where community members or\nmoderators intervene. To power social simulacra, we contribute techniques for\nprompting a large language model to generate thousands of distinct community\nmembers and their social interactions with each other; these techniques are\nenabled by the observation that large language models' training data already\nincludes a wide variety of positive and negative behavior on social media\nplatforms. In evaluations, we show that participants are often unable to\ndistinguish social simulacra from actual community behavior and that social\ncomputing designers successfully refine their social computing designs when\nusing social simulacra.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04458,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network\n  Representations\n\n  This work investigates and compares the performance of node-link diagrams,\nadjacency matrices, and bipartite layouts for visualizing networks. In a\ncrowd-sourced user study (n = 150), we measure the task accuracy and completion\ntime of the three representations for different network classes and properties.\nIn contrast to the literature, which covers mostly topology-based tasks (e.g.,\npath finding) in small datasets, we mainly focus on overview tasks for large\nand directed networks. We consider three overview tasks on networks with 500\nnodes: (T1) network class identification, (T2) cluster detection, and (T3)\nnetwork density estimation, and two detailed tasks: (T4) node in-degree vs.\nout-degree and (T5) representation mapping, on networks with 50 and 20 nodes,\nrespectively. Our results show that bipartite layouts are beneficial for\nrevealing the overall network structure, while adjacency matrices are most\nreliable across the different tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02043,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"SmartControllerJS: A JavaScript library to turn smartphones into\n  controllers for web-based interactive experiments\n\n  We introduce SmartControllerJS, a new JavaScript library for fast,\ncost-effective designing of web applications controlled via everyday\nsmartphones. At its core, SmartControllerJS establishes a connection between\ntwo webpages, one page running on a desktop browser and the other on the user's\nsmartphone. The smartphone webpage loads a controller interface allowing users\nto control a web application running on their computer's browser. The\nSmartControllerJS framework enables fast iteration loops when designing\ninteractive user experiments because it has minimal friction and allows for\nscaling, while having no running costs. We first describe how this library is\nbuilt, how it can be used, and provide interactive examples. We then present\ntwo games designed for public screens along with results from user studies\nevaluating acceptability and ease of use. Finally, we implement a custom\ncontroller based on user feedback and introduce connection monitoring tools. We\nbelieve SmartControllerJS can accelerate the design of interactive experiments\nfor researchers in Human-Computer Interaction, and be a useful tool for\neducational projects. All our code is available at\nhttps:\/\/github.com\/SmartControllerJS and links to all demos can be found in\nTable I. To explore our demos, we recommend reading this work on a desktop\ncomputer with your smartphone in hand.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03455,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Threddy: An Interactive System for Personalized Thread-based Exploration\n  and Organization of Scientific Literature\n\n  Reviewing the literature to understand relevant threads of past work is a\ncritical part of research and vehicle for learning. However, as the scientific\nliterature grows the challenges for users to find and make sense of the many\ndifferent threads of research grow as well. Previous work has helped scholars\nto find and group papers with citation information or textual similarity using\nstandalone tools or overview visualizations. Instead, in this work we explore a\ntool integrated into users' reading process that helps them with leveraging\nauthors' existing summarization of threads, typically in introduction or\nrelated work sections, in order to situate their own work's contributions. To\nexplore this we developed a prototype that supports efficient extraction and\norganization of threads along with supporting evidence as scientists read\nresearch articles. The system then recommends further relevant articles based\non user-created threads. We evaluate the system in a lab study and find that it\nhelps scientists to follow and curate research threads without breaking out of\ntheir flow of reading, collect relevant papers and clips, and discover\ninteresting new articles to further grow threads.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.11144,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000069208,
      "text":"CrossA11y: Identifying Video Accessibility Issues via Cross-modal\n  Grounding\n\n  Authors make their videos visually accessible by adding audio descriptions\n(AD), and auditorily accessible by adding closed captions (CC). However,\ncreating AD and CC is challenging and tedious, especially for non-professional\ndescribers and captioners, due to the difficulty of identifying accessibility\nproblems in videos. A video author will have to watch the video through and\nmanually check for inaccessible information frame-by-frame, for both visual and\nauditory modalities. In this paper, we present CrossA11y, a system that helps\nauthors efficiently detect and address visual and auditory accessibility issues\nin videos. Using cross-modal grounding analysis, CrossA11y automatically\nmeasures accessibility of visual and audio segments in a video by checking for\nmodality asymmetries. CrossA11y then displays these segments and surfaces\nvisual and audio accessibility issues in a unified interface, making it\nintuitive to locate, review, script AD\/CC in-place, and preview the described\nand captioned video immediately. We demonstrate the effectiveness of CrossA11y\nthrough a lab study with 11 participants, comparing to existing baseline.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement\n  Learning\n\n  Analytical dashboards are popular in business intelligence to facilitate\ninsight discovery with multiple charts. However, creating an effective\ndashboard is highly demanding, which requires users to have adequate data\nanalysis background and be familiar with professional tools, such as Power BI.\nTo create a dashboard, users have to configure charts by selecting data columns\nand exploring different chart combinations to optimize the communication of\ninsights, which is trial-and-error. Recent research has started to use deep\nlearning methods for dashboard generation to lower the burden of visualization\ncreation. However, such efforts are greatly hindered by the lack of large-scale\nand high-quality datasets of dashboards. In this work, we propose using deep\nreinforcement learning to generate analytical dashboards that can use\nwell-established visualization knowledge and the estimation capacity of\nreinforcement learning. Specifically, we use visualization knowledge to\nconstruct a training environment and rewards for agents to explore and imitate\nhuman exploration behavior with a well-designed agent network. The usefulness\nof the deep reinforcement learning model is demonstrated through ablation\nstudies and user studies. In conclusion, our work opens up new opportunities to\ndevelop effective ML-based visualization recommenders without beforehand\ntraining datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.14316,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Compensating for the Absence of a Required Accompanying Person: A Draft\n  of a Functional System Architecture for an Automated Vehicle\n\n  A major challenge in the development of a fully automated vehicle is to\nenable a large variety of users to use the vehicle independently and safely.\nParticular demands arise from user groups who rely on human assistance when\nusing conventional cars. For the independent use of a vehicle by such groups,\nthe vehicle must compensate for the absence of an accompanying person, whose\nactions and decisions ensure the accompanied person's safety even in unknown\nsituations. The resulting requirements cannot be fulfilled only by the\ngeometric design of the vehicle and the nature of its control elements. Special\nuser needs must be taken into account in the entire automation of the vehicle.\nIn this paper, we describe requirements for compensating for the absence of an\naccompanying person and show how required functions can be located in a\nhierarchical functional system architecture of an automated vehicle. In\naddition, we outline the relevance of the vehicle's operational design domain\nin this context and present a use case for the described functionalities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03261,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Remote Assistance with Mixed Reality for Procedural Tasks\n\n  We present a volumetric communication system that is designed for remote\nassistance of procedural tasks. The system allows a remote expert to visually\nguide a local operator. The two parties share a view that is spatially\nidentical, but for the local operator it is of the object on which they\noperate, while for the remote expert, the object is presented as a mixed\nreality \"hologram\". Guidance is provided by voice, gestures, and annotations\nperformed directly on the object of interest or its hologram. At each end of\nthe communication, spatial is visualized using mixed-reality glasses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.08651,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Modeling road user response timing in naturalistic settings: a\n  surprise-based framework\n\n  There is currently no established method for evaluating human response timing\nacross a range of naturalistic traffic conflict types. Traditional notions\nderived from controlled experiments, such as perception-response time, fail to\naccount for the situation-dependency of human responses and offer no clear way\nto define the stimulus in many common traffic conflict scenarios. As a result,\nthey are not well suited for application in naturalistic settings. Our main\ncontribution is the development of a novel framework for measuring and modeling\nresponse times in naturalistic traffic conflicts applicable to automated\ndriving systems as well as other traffic safety domains. The framework suggests\nthat response timing must be understood relative to the subject's current\n(prior) belief and is always embedded in, and dependent on, the dynamically\nevolving situation. The response process is modeled as a belief update process\ndriven by perceived violations to this prior belief, that is, by surprising\nstimuli. The framework resolves two key limitations with traditional notions of\nresponse time when applied in naturalistic scenarios: (1) The strong\nsituation-dependence of response timing and (2) how to unambiguously define the\nstimulus. Resolving these issues is a challenge that must be addressed by any\nresponse timing model intended to be applied in naturalistic traffic conflicts.\nWe show how the framework can be implemented by means of a relatively simple\nheuristic model fit to naturalistic human response data from real crashes and\nnear crashes from the SHRP2 dataset and discuss how it is, in principle,\ngeneralizable to any traffic conflict scenario. We also discuss how the\nresponse timing framework can be implemented computationally based on evidence\naccumulation enhanced by machine learning-based generative models and the\ninformation-theoretic concept of surprise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.14842,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000120203,
      "text":"Design of an XR Collab. Arch. for Mixed Immersive and MS Interaction\n\n  EXtended Reality (XR) is a rapidly developing paradigm for computer\nentertainment, and is also increasingly used for simulation, training, data\nanalysis, and other non-entertainment purposes, often employing head-worn XR\ndevices like the Microsoft HoloLens. In XR, integration with the physical world\nshould also include integration with commonly used digital devices. This paper\nproposes an architecture to integrate head-worn displays with touchscreen\ndevices, such as phones, tablets, or large tabletop or wall displays. The\narchitecture emerged through the iterative development of a prototype for\ncollaborative analysis and decision-making for the maritime domain. However,\nour architecture can flexibly support a range of domains and purposes. XR\ndesigners, XR entertainment researchers, and game developers can benefit from\nour architecture to propose new ways of gaming, considering multiple devices as\nuser interfaces in an immersive collaborative environment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03804,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Mixels: Fabricating Interfaces using Programmable Magnetic Pixels\n\n  In this paper, we present Mixels, programmable magnetic pixels that can be\nrapidly fabricated using an electromagnetic printhead mounted on an\noff-the-shelve 3-axis CNC machine. The ability to program magnetic material\npixel-wise with varying magnetic force enables Mixels to create new tangible,\ntactile, and haptic interfaces. To facilitate the creation of interactive\nobjects with Mixels, we provide a user interface that lets users specify the\nhigh-level magnetic behavior and that then computes the underlying magnetic\npixel assignments and fabrication instructions to program the magnetic surface.\nOur custom hardware add-on based on an electromagnetic printhead and hall\neffect sensor clips onto a standard 3-axis CNC machine and can both write and\nread magnetic pixel values from magnetic material. Our evaluation shows that\nour system can reliably program and read magnetic pixels of various strengths,\nthat we can predict the behavior of two interacting magnetic surfaces before\nprogramming them, that our electromagnet is strong enough to create pixels that\nutilize the maximum magnetic strength of the material being programmed, and\nthat this material remains magnetized when removed from the magnetic plotter.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.10926,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Voice Chatbot for Hospitality\n\n  Chatbot is a machine with the ability to answer automatically through a\nconversational interface. A chatbot is considered as one of the most\nexceptional and promising expressions of human computer interaction.\nVoice-based chatbots or artificial intelligence devices transform\nhuman-computer bidirectional interactions that allow users to navigate an\ninteractive voice response system with their voice generally using natural\nlanguage. In this paper, we focus on voice based chatbots for mediating\ninteractions between hotels and guests from both the hospitality technology\nproviders' and guests' perspectives. We developed a hotel web application with\nthe capability to receive a voice input. The application was developed with\nSpeech recognition and deep synthesis API for voice to text and text to voice\nconversion, a closed domain question answering NLP solution was used for query\nthe answer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.05456,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Revisiting Piggyback Prototyping: Examining Benefits and Tradeoffs in\n  Extending Existing Social Computing Systems\n\n  The CSCW community has a history of designing, implementing, and evaluating\nnovel social interactions in technology, but the process requires significant\ntechnical effort for uncertain value. We discuss the opportunities and\napplications of \"piggyback prototyping\", building and evaluating new ideas for\nsocial computing on top of existing ones, expanding on its potential to\ncontribute design recommendations. Drawing on about 50 papers which use the\nmethod, we critically examine the intellectual and technical benefits it\nprovides, such as ecological validity and leveraging well-tested features, as\nwell as research-product and ethical tensions it imposes, such as limits to\ncustomization and violation of participant privacy. We discuss considerations\nfor future researchers deciding whether to use piggyback prototyping and point\nto new research agendas which can reduce the burden of implementing the method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.05069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Experimenting with Experimentation: Rethinking The Role of\n  Experimentation in Educational Design\n\n  What if we take a broader view of what it means to run an education\nexperiment? In this paper, we explore opportunities that arise when we think\nbeyond the commonly-held notion that the purpose of an experiment is to either\naccept or reject a pre-defined hypothesis and instead, reconsider\nexperimentation as a means to explore the complex design space of creating and\nimproving instructional content. This is an approach we call\nexperiment-inspired design. Then, to operationalize these ideas in a real-world\nexperimentation venue, we investigate the implications of running a sequence of\ninterventions teaching first-year students \"meta-skills\": transferable skills\napplicable to multiple areas of their lives, such as planning, and managing\nstress. Finally, using two examples as case studies for meta-skills\ninterventions (stress-reappraisal and mental contrasting with implementation\nintentions), we reflect on our experiences with experiment-inspired design and\nshare six preliminary lessons on how to use experimentation for design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04181,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"An Empirical Evaluation of Predicted Outcomes as Explanations in\n  Human-AI Decision-Making\n\n  In this work, we empirically examine human-AI decision-making in the presence\nof explanations based on predicted outcomes. This type of explanation provides\na human decision-maker with expected consequences for each decision alternative\nat inference time - where the predicted outcomes are typically measured in a\nproblem-specific unit (e.g., profit in U.S. dollars). We conducted a pilot\nstudy in the context of peer-to-peer lending to assess the effects of providing\npredicted outcomes as explanations to lay study participants. Our preliminary\nfindings suggest that people's reliance on AI recommendations increases\ncompared to cases where no explanation or feature-based explanations are\nprovided, especially when the AI recommendations are incorrect. This results in\na hampered ability to distinguish correct from incorrect AI recommendations,\nwhich can ultimately affect decision quality in a negative way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05734,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Evaluating User Experience in Literary and Film Geography-based Apps\n  with a Cartographical User-Centered Design Lens\n\n  Geography scholarship currently includes interdisciplinary approaches and\ntheories and reflects shifts in research methodologies. Since the spatial turn\nin geographical thought and the emergence of geo-web technologies, geography\nscholarship has leaned more toward interdisciplinarity. In recent years\ngeographical research methods have relied on various disciplines ranging from\ndata science to arts and design. Literary geography and film geography are two\nsubfields of geography that employ novels and films in exploring spatiality,\nrespectively. In addition to geographical concepts, these courses include many\naspects of relations in space, including human-human relations,\nhuman-environment relations, et cetera, which were barely addressed in\ntraditional geography courses. However, a review of the employment of geoweb\ntechnologies in literary and film geography practices reveals that these\npractices have mostly remained limited to isolating geographical passages from\nnovels or movies. This paper explores new opportunities for designing film and\nliterary geography-based apps using a cartographical user-centered design\nframework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.06825,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000079804,
      "text":"An Exploration of Hands-free Text Selection for Virtual Reality\n  Head-Mounted Displays\n\n  Hand-based interaction, such as using a handheld controller or making hand\ngestures, has been widely adopted as the primary method for interacting with\nboth virtual reality (VR) and augmented reality (AR) head-mounted displays\n(HMDs). In contrast, hands-free interaction avoids the need for users' hands\nand although it can afford additional benefits, there has been limited research\nin exploring and evaluating hands-free techniques for these HMDs. As VR HMDs\nbecome ubiquitous, people will need to do text editing, which requires\nselecting text segments. Similar to hands-free interaction, text selection is\nunderexplored. This research focuses on both, text selection via hands-free\ninteraction. Our exploration involves a user study with 24 participants to\ninvestigate the performance, user experience, and workload of three hands-free\nselection mechanisms (Dwell, Blink, Voice) to complement head-based pointing.\nResults indicate that Blink outperforms Dwell and Voice in completion time.\nUsers' subjective feedback also shows that Blink is the preferred technique for\ntext selection. This work is the first to explore hands-free interaction for\ntext selection in VR HMDs. Our results provide a solid platform for further\nresearch in this important area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.07025,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000158946,
      "text":"Dynamic X-Ray Vision in Mixed Reality\n\n  X-ray vision, a technique that allows users to see through walls and other\nobstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality\n(MR). In this paper, we demonstrate a dynamic X-ray vision window that is\nrendered in real-time based on the user's current position and changes with\nmovement in the physical environment. Moreover, the location and transparency\nof the window are also dynamically rendered based on the user's eye gaze. We\nbuild this X-ray vision window for a current state-of-the-art MR Head-Mounted\nDevice (HMD) -- HoloLens 2 by integrating several different features: scene\nunderstanding, eye tracking, and clipping primitive.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02308,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Is it Fun?: Understanding Enjoyment in Non-Game HCI Research\n\n  An experience of fun can be an important factor for validating the value of\ngames. Research on non-game HCI has been attempted to measure the enjoyment of\nwork. However, a majority of the studies do not discuss the importance and\nvalue of the result. It is not clear as to how the term fun is understood in a\nnon-game context. To analyze this shortcoming, we reviewed extant studies, and\nexplored as to how researchers determine if the value of an activity is fun.\nConsequently, we discussed and categorized the usage of the terms and analyzed\nthe methodologies that are used in extant studies that evaluate the effects of\nfun and related terms. To gain a better understanding of fun in HCI, we\nprovided several directions that can be discussed for strengthening enjoyable\nHCI research beyond applications involving games.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.00394,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000111262,
      "text":"The Nudging Effect on Tracking Activity\n\n  Wearables activity trackers are becoming widely adopted to understand\nindividual behavior. Understanding behavior may help in self-regulation such as\nself-monitoring, goal-setting, self-corrective, etc.; Nevertheless, challenges\nexist in attaining consistent use and adoption of wearables, which hinders\nbehavior understanding. Research has suggested that nudging strategies may\nchange and sustain human engagement. However, it is still unknown how nudging\nmay affect human wearing behavior on an individual level. We conducted a\nsix-month study in which we tested several nudging techniques on the same\nparticipants. The preliminary results of our research show that participants\nperform better when a nudging strategy is applied. In addition, participants\nresponded differently to different nudging techniques. Future research can\nfocus on developing an individual-based nudging mechanism to encourage users to\nwear their devices consistently.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11311,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"Spatial model personalization in Gboard\n\n  We introduce a framework for adapting a virtual keyboard to individual user\nbehavior by modifying a Gaussian spatial model to use personalized key center\noffset means and, optionally, learned covariances. Through numerous real-world\nstudies, we determine the importance of training data quantity and weights, as\nwell as the number of clusters into which to group keys to avoid overfitting.\nWhile past research has shown potential of this technique using\nartificially-simple virtual keyboards and games or fixed typing prompts, we\ndemonstrate effectiveness using the highly-tuned Gboard app with a\nrepresentative set of users and their real typing behaviors. Across a variety\nof top languages, we achieve small-but-significant improvements in both typing\nspeed and decoder accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.15622,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"A Functional Model For Information Exploration Systems\n\n  Information exploration tasks are inherently complex, ill-structured, and\ninvolve sequences of actions usually spread over many sessions. When exploring\na dataset, users tend to experiment higher degrees of uncertainty, mostly\nraised by knowledge gaps concerning the information sources, the task, and the\nefficiency of the chosen exploration actions, strategies, and tools in\nsupporting the task solution process. Provided these concerns, exploration\ntools should be designed with the goal of leveraging the mapping between user's\ncognitive actions and solution strategies onto the current systems' operations.\nHowever, state-of-the-art systems fail in providing an expressive set of\noperations that covers a wide range of exploration problems. There is not a\ncommon understanding of neither which operators are required nor in which ways\nthey can be used by explorers. In order to mitigate these shortcomings, this\nwork presents a formal framework of exploration operations expressive enough to\ndescribe at least the majority of state-of-the-art exploration interfaces and\ntasks. We also show how the framework leveraged a new evaluation approach,\nwhere we draw precise comparisons between tools concerning the range of\nexploration tasks they support.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05484,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"On the application of topological data analysis: a Z24 Bridge case study\n\n  Topological methods are very rarely used in structural health monitoring\n(SHM), or indeed in structural dynamics generally, especially when considering\nthe structure and topology of observed data. Topological methods can provide a\nway of proposing new metrics and methods of scrutinising data, that otherwise\nmay be overlooked. In this work, a method of quantifying the shape of data, via\na topic called topological data analysis will be introduced. The main tool\nwithin topological data analysis is persistent homology. Persistent homology is\na method of quantifying the shape of data over a range of length scales. The\nrequired background and a method of computing persistent homology is briefly\nintroduced here. Ideas from topological data analysis are applied to a Z24\nBridge case study, to scrutinise different data partitions, classified by the\nconditions at which the data were collected. A metric, from topological data\nanalysis, is used to compare between the partitions. The results presented\ndemonstrate that the presence of damage alters the manifold shape more\nsignificantly than the effects present from temperature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11765,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Process Diagrams\n\n  This paper is simply a collection of process diagrams for further use and\nreference. These are diagrams about different approaches to research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02576,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000246366,
      "text":"Cognitive Assistance for Inquiry-Based Modeling\n\n  Inquiry-based modeling is essential to scientific practice. However, modeling\nis difficult for novice scientists in part due to limited domain-specific\nknowledge and quantitative skills. VERA is an interactive tool that helps users\nconstruct conceptual models of ecological phenomena, run them as simulations,\nand examine their predictions. VERA provides cognitive scaffolding for modeling\nby supplying access to large-scale domain knowledge. The VERA system was tested\nby college-level students in two different settings: a general ecology lecture\ncourse (N=91) at a large southeastern R1 university and a controlled experiment\nin a research laboratory (N=15). Both studies indicated that engaging students\nin ecological modeling through VERA helped them better understand basic\nbiological concepts. The latter study additionally revealed that providing\naccess to domain knowledge helped students build more complex models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.14645,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Reducing Stress and Anxiety in the Metaverse: A Systematic Review of\n  Meditation, Mindfulness and Virtual Reality\n\n  Meditation, or mindfulness, is widely used to improve mental health. With the\nemergence of Virtual Reality technology, many studies have provided evidence\nthat meditation with VR can bring health benefits. However, to our knowledge,\nthere are no guidelines and comprehensive reviews in the literature on how to\nconduct such research in virtual reality. In order to understand the role of VR\ntechnology in meditation and future research opportunities, we conducted a\nsystematic literature review in the IEEE and ACM databases. Our process yielded\n19 eligible papers and we conducted a structured analysis. We understand the\nstate-of-art of meditation type, design consideration and VR and technology\nthrough these papers and conclude research opportunities and challenges for the\nfuture.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.10874,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Angular-based Edge Bundled Parallel Coordinates Plot for the Visual\n  Analysis of Large Ensemble Simulation Data\n\n  With the continuous increase in the computational power and resources of\nmodern high-performance computing (HPC) systems, large-scale ensemble\nsimulations have become widely used in various fields of science and\nengineering, and especially in meteorological and climate science. It is widely\nknown that the simulation outputs are large time-varying, multivariate, and\nmultivalued datasets which pose a particular challenge to the visualization and\nanalysis tasks. In this work, we focused on the widely used Parallel\nCoordinates Plot (PCP) to analyze the interrelations between different\nparameters, such as variables, among the members. However, PCP may suffer from\nvisual cluttering and drawing performance with the increase on the data size to\nbe analyzed, that is, the number of polylines. To overcome this problem, we\npresent an extension to the PCP by adding B\\'{e}zier curves connecting the\nangular distribution plots representing the mean and variance of the\ninclination of the line segments between parallel axes. The proposed\nAngular-based Parallel Coordinates Plot (APCP) is capable of presenting a\nsimplified overview of the entire ensemble data set while maintaining the\ncorrelation information between the adjacent variables. To verify its\neffectiveness, we developed a visual analytics prototype system and evaluated\nby using a meteorological ensemble simulation output from the supercomputer\nFugaku.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.15041,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000079473,
      "text":"Summarizing text to embed qualitative data into visualizations\n\n  Qualitative data can be conveyed with strings of text. Fitting longer text\ninto visualizations requires a) space to place the text inside the\nvisualization; and b) appropriate text to fit the space available. For\nquantitative visualizations, space is available in area marks; or within\nvisualization layouts where the marks have an implied space (e.g. bar charts).\nFor qualitative visualizations, space is defined in common text layouts such as\nprose paragraphs. To fit text within these layouts is a function for emerging\nNLP capabilities such as summarization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.04844,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Measuring Effects of Spatial Visualization and Domain on Visualization\n  Task Performance: A Comparative Study\n\n  Understanding your audience is foundational to creating high impact\nvisualization designs. However, individual differences and cognitive abilities\nalso influence interactions with information visualization. Differing user\nneeds and abilities suggest that an individual's background could influence\ncognitive performance and interactions with visuals in a systematic way. This\nstudy builds on current research in domain-specific visualization and cognition\nto address if domain and spatial visualization ability combine to affect\nperformance on information visualization tasks. We measure spatial\nvisualization and visual task performance between those with tertiary education\nand professional profile in business, law & political science, and math &\ncomputer science. We conducted an online study with 90 participants using an\nestablished psychometric test to assess spatial visualization ability, and bar\nchart layouts rotated along Cartesian and polar coordinates to assess\nperformance on spatially rotated data. Accuracy and response times varied with\ndomain across chart types and task difficulty. We found that accuracy and time\ncorrelate with spatial visualization level, and education in math & computer\nscience can indicate higher spatial visualization. Additionally, we found\ndistinct motivations can affect performance in that higher motivation could\ncontribute to increased levels of accuracy. Our findings indicate discipline\nnot only affects user needs and interactions with data visualization, but also\ncognitive traits. Our results can advance inclusive practices in visualization\ndesign and add to knowledge in domain-specific visual research that can empower\ndesigners across disciplines to create effective visualizations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.1434,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000090732,
      "text":"Using Processing Fluency as a Metric of Trust in Scatterplot\n  Visualizations\n\n  Establishing trust with readers is an important first step in visual data\ncommunication. But what makes a visualization trustworthy? Psychology and\nbehavioral economics research has found processing fluency (i.e., speed and\naccuracy of perceiving and processing a stimulus) is central to perceived\ntrust. We examine the association between processing fluency and trust in\nvisualizations through two empirical studies. In Experiment 1, we tested the\neffect of camouflaging a visualization on processing fluency. Participants\nestimated the proportion of data values within a specified range for six\ncamouflaged visualizations and one non-camouflaged control; they also reported\ntheir perceived difficulty for each of the visualizations. Camouflaged\nvisualizations produced less accurate estimations compared to the control. In\nExperiment 2, we created a decision task based on trust games adapted from\nbehavioral economics. We asked participants to invest money in two hypothetical\ncompanies and report how much they trust each company. One company communicates\nits strategy with a camouflaged visualization, the other with a controlled\nvisualization. Participants tended to invest less money in the company\npresenting a camouflaged visualization. Hence, we found support for the\nhypothesis that processing fluency is key to the perception of trust in visual\ndata communication.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.06139,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Electroencephalography (EEG), electromyography (EMG) and eye-tracking\n  for astronaut training and space exploration\n\n  The ongoing push to send humans back to the Moon and to Mars is giving rise\nto a wide range of novel technical solutions in support of prospective\nastronaut expeditions. Against this backdrop, the European Space Agency (ESA)\nhas recently launched an investigation into unobtrusive interface technologies\nas a potential answer to such challenges. Three particular technologies have\nshown promise in this regard: EEG-based brain-computer interfaces (BCI) provide\na non-invasive method of utilizing recorded electrical activity of a user's\nbrain, electromyography (EMG) enables monitoring of electrical signals\ngenerated by the user's muscle contractions, and finally, eye tracking enables,\nfor instance, the tracking of user's gaze direction via camera recordings to\nconvey commands. Beyond simply improving the usability of prospective technical\nsolutions, our findings indicate that EMG, EEG, and eye-tracking could also\nserve to monitor and assess a variety of cognitive states, including attention,\ncognitive load, and mental fatigue of the user, while EMG could furthermore\nalso be utilized to monitor the physical state of the astronaut. In this paper,\nwe elaborate on the key strengths and challenges of these three enabling\ntechnologies, and in light of ESA's latest findings, we reflect on their\napplicability in the context of human space flight. Furthermore, a timeline of\ntechnological readiness is provided. In so doing, this paper feeds into the\ngrowing discourse on emerging technology and its role in paving the way for a\nhuman return to the Moon and expeditions beyond the Earth's orbit.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0032,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"DramatVis Personae: Visual Text Analytics for Identifying Social Biases\n  in Creative Writing\n\n  Implicit biases and stereotypes are often pervasive in different forms of\ncreative writing such as novels, screenplays, and children's books. To\nunderstand the kind of biases writers are concerned about and how they mitigate\nthose in their writing, we conducted formative interviews with nine writers.\nThe interviews suggested that despite a writer's best interest, tracking and\nmanaging implicit biases such as a lack of agency, supporting or submissive\nroles, or harmful language for characters representing marginalized groups is\nchallenging as the story becomes longer and complicated. Based on the\ninterviews, we developed DramatVis Personae (DVP), a visual analytics tool that\nallows writers to assign social identities to characters, and evaluate how\ncharacters and different intersectional social identities are represented in\nthe story. To evaluate DVP, we first conducted think-aloud sessions with three\nwriters and found that DVP is easy-to-use, naturally integrates into the\nwriting process, and could potentially help writers in several critical bias\nidentification tasks. We then conducted a follow-up user study with 11 writers\nand found that participants could answer questions related to bias detection\nmore efficiently using DVP in comparison to a simple text editor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.04011,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"RecSys Fairness Metrics: Many to Use But Which One To Choose?\n\n  In recent years, recommendation and ranking systems have become increasingly\npopular on digital platforms. However, previous work has highlighted how\npersonalized systems might lead to unintentional harms for users. Practitioners\nrequire metrics to measure and mitigate these types of harms in production\nsystems. To meet this need, many fairness definitions have been introduced and\nexplored by the RecSys community. Unfortunately, this has led to a\nproliferation of possible fairness metrics from which practitioners can choose.\nThe increase in volume and complexity of metrics creates a need for\npractitioners to deeply understand the nuances of fairness definitions and\nimplementations. Additionally, practitioners need to understand the ethical\nguidelines that accompany these metrics for responsible implementation. Recent\nwork has shown that there is a proliferation of ethics guidelines and has\npointed to the need for more implementation guidance rather than principles\nalone. The wide variety of available metrics, coupled with the lack of accepted\nstandards or shared knowledge in practice leads to a challenging environment\nfor practitioners to navigate. In this position paper, we focus on this\nwidening gap between the research community and practitioners concerning the\navailability of metrics versus the ability to put them into practice. We\naddress this gap with our current work, which focuses on developing methods to\nhelp ML practitioners in their decision-making processes when picking fairness\nmetrics for recommendation and ranking systems. In our iterative design\ninterviews, we have already found that practitioners need both practical and\nreflective guidance when refining fairness constraints. This is especially\nsalient given the growing challenge for practitioners to leverage the correct\nmetrics while balancing complex fairness contexts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.00202,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000257293,
      "text":"The Quest for Omnioculars: Embedded Visualization for Augmenting\n  Basketball Game Viewing Experiences\n\n  Sports game data is becoming increasingly complex, often consisting of\nmultivariate data such as player performance stats, historical team records,\nand athletes' positional tracking information. While numerous visual analytics\nsystems have been developed for sports analysts to derive insights, few tools\ntarget fans to improve their understanding and engagement of sports data during\nlive games. By presenting extra data in the actual game views, embedded\nvisualization has the potential to enhance fans' game-viewing experience.\nHowever, little is known about how to design such kinds of visualizations\nembedded into live games. In this work, we present a user-centered design study\nof developing interactive embedded visualizations for basketball fans to\nimprove their live game-watching experiences. We first conducted a formative\nstudy to characterize basketball fans' in-game analysis behaviors and tasks.\nBased on our findings, we propose a design framework to inform the design of\nembedded visualizations based on specific data-seeking contexts. Following the\ndesign framework, we present five novel embedded visualization designs\ntargeting five representative contexts identified by the fans, including\nshooting, offense, defense, player evaluation, and team comparison. We then\ndeveloped Omnioculars, an interactive basketball game-viewing prototype that\nfeatures the proposed embedded visualizations for fans' in-game data analysis.\nWe evaluated Omnioculars in a simulated basketball game with basketball fans.\nThe study results suggest that our design supports personalized in-game data\nanalysis and enhances game understanding and engagement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.06156,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"SEER: Sustainable E-commerce with Environmental-impact Rating\n\n  With online shopping gaining massive popularity over the past few years,\ne-commerce platforms can play a significant role in tackling climate change and\nother environmental problems. In this study, we report that the\n\"attitude-behavior\" gap identified by prior sustainable consumption literature\nalso exists in an online setting. We propose SEER, a concept design for online\nshopping websites to help consumers make more sustainable choices. We introduce\nexplainable environmental impact ratings to increase knowledge, trust, and\nconvenience for consumers willing to purchase eco-friendly products. In our\nquasi-randomized case-control experiment with 98 subjects across the United\nStates, we found that the case group using SEER demonstrates significantly more\neco-friendly consumption behavior than the control group using a traditional\ne-commerce setting. While there are challenges in generating reliable\nexplanations and environmental ratings for products, if implemented, in the\nUnited States alone, SEER has the potential to reduce approximately 2.88\nmillion tonnes of carbon emission every year.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08703,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000285771,
      "text":"Spoken Dialogue System Based on Attribute Vector for Travel Agent Robot\n\n  In this study, we develop a dialogue system for a dialogue robot competition.\nIn the system, the characteristics of sightseeing spots are expressed as\n\"attribute vectors\" in advance, and the user is questioned on the different\nattributes of the two candidate spots. Consequently, the system can make\nrecommendations based on user intentions. A dialogue experiment is conducted\nduring a preliminary round of competition. The overall satisfaction score\nobtained is 40.1 out of 63 points, which is a reasonable result. Analysis of\nthe relationship between the system behavior and satisfaction scores reveals\nthat satisfaction increases when the system correctly understands the user\nintention and responds appropriately. However, a negative correlation is\nobserved between the number of user utterances and the satisfaction score. This\nimplies that inappropriate responses reduce the usefulness of the system as a\nconsultation partner.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000051326,
      "text":"Holistic Outpost Design for Lunar Lava Tubes\n\n  As the space industry continues its rapid development, humanity is poised to\nexpand beyond Low Earth Orbit (LEO), seeking to establish permanent presence on\nthe Moon and beyond. While space travel has traditionally been the domain of a\nsmall number of highly specialized professionals, a new era of human\nexploration, involving non-space actors and stakeholders, is now becoming a\nreality. In spite of this development, most space habitats are still designed\nfor a narrow target group. This paper seeks to address this deficit by\nrethinking the established design approaches, typically limited to tackling\nengineering and challenges of human space exploration (such as radiation or\nhypogravity), by instead adopting an interdisciplinary \"big picture\"\nperspective encompassing social, psychological and cultural aspects of future\nspace habitats. By elaborating and reflecting on our concept, this paper seeks\nto demonstrate the importance of a trans-disciplinary approach to designing\nthriving sustainable colonies beyond LEO. We demonstrate the potentially key\nrole of design as mediator in advancing macro-strategies promoting thriving\nexistence and sustainable growth. With this approach we tackle big-picture\nquestions about humanity's future and prospects amongst the stars.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03305,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000077155,
      "text":"How Do Data Science Workers Communicate Intermediate Results?\n\n  Data science workers increasingly collaborate on large-scale projects before\ncommunicating insights to a broader audience in the form of visualization.\nWhile prior work has modeled how data science teams, oftentimes with distinct\nroles and work processes, communicate knowledge to outside stakeholders, we\nhave little knowledge of how data science workers communicate intermediately\nbefore delivering the final products. In this work, we contribute a nuanced\ndescription of the intermediate communication process within data science\nteams. By analyzing interview data with 8 self-identified data science workers,\nwe characterized the data science intermediate communication process with four\nfactors, including the types of audience, communication goals, shared\nartifacts, and mode of communication. We also identified overarching challenges\nin the current communication process. We also discussed design implications\nthat might inform better tools that facilitate intermediate communication\nwithin data science teams.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.13283,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Content Transfer Across Multiple Screens with Combined Eye-Gaze and\n  Touch Interaction -- A Replication Study\n\n  In this paper, we describe the results of replicating one of our studies from\ntwo years ago which compares two techniques for transferring content across\nmultiple screens in VR. Results from the previous study have shown that a\ncombined gaze and touch input can outperform a bimanual touch-only input in\nterms of task completion time, simulator sickness, task load and usability.\nExcept for the simulator sickness, these findings could be validated by the\nreplication. The difference with regards to simulator sickness and variations\nin absolute scores of the other measures could be explained by a different set\nof user with less VR experience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08477,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Large-scale Text-to-Image Generation Models for Visual Artists' Creative\n  Works\n\n  Large-scale Text-to-image Generation Models (LTGMs) (e.g., DALL-E),\nself-supervised deep learning models trained on a huge dataset, have\ndemonstrated the capacity for generating high-quality open-domain images from\nmulti-modal input. Although they can even produce anthropomorphized versions of\nobjects and animals, combine irrelevant concepts in reasonable ways, and give\nvariation to any user-provided images, we witnessed such rapid technological\nadvancement left many visual artists disoriented in leveraging LTGMs more\nactively in their creative works. Our goal in this work is to understand how\nvisual artists would adopt LTGMs to support their creative works. To this end,\nwe conducted an interview study as well as a systematic literature review of 72\nsystem\/application papers for a thorough examination. A total of 28 visual\nartists covering 35 distinct visual art domains acknowledged LTGMs' versatile\nroles with high usability to support creative works in automating the creation\nprocess (i.e., automation), expanding their ideas (i.e., exploration), and\nfacilitating or arbitrating in communication (i.e., mediation). We conclude by\nproviding four design guidelines that future researchers can refer to in making\nintelligent user interfaces using LTGMs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000245041,
      "text":"\"If sighted people know, I should be able to know:\" Privacy Perceptions\n  of Bystanders with Visual Impairments around Camera-based Technology\n\n  Camera-based technology can be privacy-invasive, especially for bystanders\nwho can be captured by the cameras but do not have direct control or access to\nthe devices. The privacy threats become even more significant to bystanders\nwith visual impairments (BVI) since they cannot visually discover the use of\ncameras nearby and effectively avoid being captured. While some prior research\nhas studied visually impaired people's privacy concerns as direct users of\ncamera-based assistive technologies, no research has explored their unique\nprivacy perceptions and needs as bystanders. We conducted an in-depth interview\nstudy with 16 visually impaired participants to understand BVI's privacy\nconcerns, expectations, and needs in different camera usage scenarios. A\npreliminary survey with 90 visually impaired respondents and 96 sighted\ncontrols was conducted to compare BVI and sighted bystanders' general attitudes\ntowards cameras and elicit camera usage scenarios for the interview study. Our\nresearch revealed BVI's unique privacy challenges and perceptions around\ncameras, highlighting their needs for privacy awareness and protection. We\nsummarized design considerations for future privacy-enhancing technologies to\nfulfill BVI's privacy needs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03202,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000354316,
      "text":"Designing Virtual Environments for Social Engagement in Older Adults\n\n  Virtual reality (VR) is increasingly used as a platform for social\ninteraction, including as a means for older adults to maintain engagement.\nHowever, there has been limited research to examine the features of social VR\nthat are most relevant to older adults experiences. The current study was\nconducted to qualitatively analyze the behavior of older adults in a\ncollaborative VR environment and evaluate aspects of design that affected their\nengagement outcomes. We paired 36 participants over the age of 60, from three\ndiverse geographic locations, and asked them to interact in collaborative VR\nmodules. Video-based observation methods and thematic analyses were used to\nstudy the resulting interactions. The results indicated a strong link between\nperceived spatial presence in the VR and social engagement, while also\nhighlighting the importance of individual personality and compatibility. The\nstudy provides new insights into design guidelines that could improve social VR\nprograms for older adults.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.11674,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"WristSketcher: Creating Dynamic Sketches in AR with a Sensing Wristband\n\n  Restricted by the limited interaction area of native AR glasses (e.g., touch\nbars), it is challenging to create sketches in AR glasses. Recent works have\nattempted to use mobile devices (e.g., tablets) or mid-air bare-hand gestures\nto expand the interactive spaces and can work as the 2D\/3D sketching input\ninterfaces for AR glasses. Between them, mobile devices allow for accurate\nsketching but are often heavy to carry, while sketching with bare hands is\nzero-burden but can be inaccurate due to arm instability. In addition, mid-air\nbare-hand sketching can easily lead to social misunderstandings and its\nprolonged use can cause arm fatigue. As a new attempt, in this work, we present\nWristSketcher, a new AR system based on a flexible sensing wristband for\ncreating 2D dynamic sketches, featuring an almost zero-burden authoring model\nfor accurate and comfortable sketch creation in real-world scenarios.\nSpecifically, we have streamlined the interaction space from the mid-air to the\nsurface of a lightweight sensing wristband, and implemented AR sketching and\nassociated interaction commands by developing a gesture recognition method\nbased on the sensing pressure points on the wristband. The set of interactive\ngestures used by our WristSketcher is determined by a heuristic study on user\npreferences. Moreover, we endow our WristSketcher with the ability of animation\ncreation, allowing it to create dynamic and expressive sketches. Experimental\nresults demonstrate that our WristSketcher i) faithfully recognizes users'\ngesture interactions with a high accuracy of 96.0%; ii) achieves higher\nsketching accuracy than Freehand sketching; iii) achieves high user\nsatisfaction in ease of use, usability and functionality; and iv) shows\ninnovation potentials in art creation, memory aids, and entertainment\napplications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.01369,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"Understanding Older Adults' Perceptions and Challenges in Using\n  AI-enabled Everyday Technologies\n\n  Artificial intelligence (AI)-enabled everyday technologies could help address\nage-related challenges like physical impairments and cognitive decline. While\nrecent research studied older adults' experiences with specific AI-enabled\nproducts (e.g., conversational agents and assistive robots), it remains unknown\nhow older adults perceive and experience current AI-enabled everyday\ntechnologies in general, which could impact their adoption of future AI-enabled\nproducts. We conducted a survey study (N=41) and semi-structured interviews\n(N=15) with older adults to understand their experiences and perceptions of AI.\nWe found that older adults were enthusiastic about learning and using\nAI-enabled products, but they lacked learning avenues. Additionally, they\nworried when AI-enabled products outwitted their expectations, intruded on\ntheir privacy, or impacted their decision-making skills. Therefore, they held\nmixed views towards AI-enabled products such as AI, an aid, or an adversary. We\nconclude with design recommendations that make older adults feel inclusive,\nsecure, and in control of their interactions with AI-enabled products.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00472,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"VFLens: Co-design the Modeling Process for Efficient Vertical Federated\n  Learning via Visualization\n\n  As a decentralized training approach, federated learning enables multiple\norganizations to jointly train a model without exposing their private data.\nThis work investigates vertical federated learning (VFL) to address scenarios\nwhere collaborating organizations have the same set of users but with different\nfeatures, and only one party holds the labels. While VFL shows good\nperformance, practitioners often face uncertainty when preparing\nnon-transparent, internal\/external features and samples for the VFL training\nphase. Moreover, to balance the prediction accuracy and the resource\nconsumption of model inference, practitioners require to know which subset of\nprediction instances is genuinely needed to invoke the VFL model for inference.\nTo this end, we co-design the VFL modeling process by proposing an interactive\nreal-time visualization system, VFLens, to help practitioners with feature\nengineering, sample selection, and inference. A usage scenario, a quantitative\nexperiment, and expert feedback suggest that VFLens helps practitioners boost\nVFL efficiency at a lower cost with sufficient confidence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.07533,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking dataset\n  collected in virtual reality\n\n  We present GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking\n(ET) dataset collected at 250 Hz with an ET-enabled virtual-reality (VR)\nheadset. GazeBaseVR comprises 5,020 binocular recordings from a diverse\npopulation of 407 college-aged participants. Participants were recorded up to\nsix times each over a 26-month period, each time performing a series of five\ndifferent ET tasks: (1) a vergence task, (2) a horizontal smooth pursuit task,\n(3) a video-viewing task, (4) a self-paced reading task, and (5) a random\noblique saccade task. Many of these participants have also been recorded for\ntwo previously published datasets with different ET devices, and some\nparticipants were recorded before and after COVID-19 infection and recovery.\nGazeBaseVR is suitable for a wide range of research on ET data in VR devices,\nespecially eye movement biometrics due to its large population and longitudinal\nnature. In addition to ET data, additional participant details are provided to\nenable further research on topics such as fairness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.05078,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"Joint Human Orientation-Activity Recognition Using WiFi Signals for\n  Human-Machine Interaction\n\n  WiFi sensing is an important part of the new WiFi 802.11bf standard, which\ncan detect motion and measure distances. In recent years, some machine learning\nmethods have been proposed for human activity recognition from WiFi signals.\nHowever, to the best of our knowledge, none of these methods have explored\norientation prediction of the user using WiFi signals. Orientation prediction\nis particularly critical for human-machine interaction in an environment with\nmultiple smart devices. In this paper, we propose a data collection setup and\nmachine learning models for joint human orientation and activity recognition\nusing WiFi signals from a single access point (AP) or multiple APs. The results\nshow feasibility of joint orientation-activity recognition in an indoor\nenvironment with a high accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.09761,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000101659,
      "text":"Personality-adapted multimodal dialogue system\n\n  This paper describes a personality-adaptive multimodal dialogue system\ndeveloped for the Dialogue Robot Competition 2022. To realize a dialogue system\nthat adapts the dialogue strategy to individual users, it is necessary to\nconsider the user's nonverbal information and personality. In this competition,\nwe built a prototype of a user-adaptive dialogue system that estimates user\npersonality during dialogue. Pretrained DNN models are used to estimate user\npersonalities annotated as Big Five scores. This model is embedded in a\ndialogue system to estimate user personality from face images during the\ndialogue. We proposed a method for dialogue management that changed the\ndialogue flow based on the estimated personality characteristics and confirmed\nthat the system works in a real environment in the preliminary round of this\ncompetition. Furthermore, we implemented specific modules to enhance the\nmultimodal dialogue experience of the user, including personality assessment,\ncontrolling facial expressions and movements of the android, and dialogue\nmanagement to explain the attractiveness of sightseeing spots. The aim of\ndialogue based on personality assessment is to reduce the nervousness of users,\nand it acts as an ice breaker. The android's facial expressions and movements\nare necessary for a more natural android conversation. Since the task of this\ncompetition was to promote the appeal of sightseeing spots and to recommend an\nappropriate sightseeing spot, the dialogue process for how to explain the\nattractiveness of the spot is important. All results of the subjective\nevaluation by users were better than those of the baseline and other systems\ndeveloped for this competition. The proposed dialogue system ranked first in\nboth \"Impression Rating\" and \"Effectiveness of Android Recommendations\".\nAccording to the total evaluation in the competition, the proposed system was\nranked first overall.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.16463,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Augmented Reality and Mixed Reality Measurement Under Different\n  Environments: A Survey on Head-Mounted Devices\n\n  Augmented Reality (AR) and Mixed Reality (MR) have been two of the most\nexplosive research topics in the last few years. Head-Mounted Devices (HMDs)\nare essential intermediums for using AR and MR technology, playing an important\nrole in the research progress in these two areas. Behavioral research with\nusers is one way of evaluating the technical progress and effectiveness of\nHMDs. In addition, AR and MR technology is dependent upon virtual interactions\nwith the real environment. Thus, conditions in real environments can be a\nsignificant factor for AR and MR measurements with users. In this paper, we\nsurvey 87 environmental-related HMD papers with measurements from users,\nspanning over 32 years. We provide a thorough review of AR- and MR-related user\nexperiments with HMDs under different environmental factors. Then, we summarize\ntrends in this literature over time using a new classification method with four\nenvironmental factors, the presence or absence of user feedback in behavioral\nexperiments, and ten main categories to subdivide these papers (e.g., domain\nand method of user assessment). We also categorize characteristics of the\nbehavioral experiments, showing similarities and differences among papers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.0797,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000102984,
      "text":"Market Interventions in a Large-Scale Virtual Economy\n\n  Massively multiplayer online role-playing games often contain sophisticated\nin-game economies. Many important real-world economic phenomena, such as\ninflation, economic growth, and business cycles, are also present in these\nvirtual economies. One major difference between real-world and virtual\neconomies is the ease and frequency by which a policymaker, in this case, a\ngame developer, can introduce economic shocks. These economic shocks, typically\nimplemented with game updates or signaled through community channels, provide\nfertile ground to study the effects of economic interventions on markets. In\nthis work, we study the effect of in-game economic market interventions,\nnamely, a transaction tax and an item sink, in Old School RuneScape. Using\ncausal inference methods, we find that the tax did not meaningfully affect the\ntrading volume of items at the tax boundaries and that the item sink\ncontributed to the inflation of luxury good prices, without reducing trade\nvolume. Furthermore, we find evidence that the illicit gold trading market was\nrelatively unaffected by the implemented market interventions. Our findings\nyield useful insights not only into the effect of market interventions in\nvirtual economies but also for real-world markets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.07286,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Augmenting Online Classes with an Attention Tracking Tool May Improve\n  Student Engagement\n\n  Online remote learning has certain advantages, such as higher flexibility and\ngreater inclusiveness. However, a caveat is the teachers' limited ability to\nmonitor student interaction during an online class, especially while teachers\nare sharing their screens. We have taken feedback from 12 teachers experienced\nin teaching undergraduate-level online classes on the necessity of an attention\ntracking tool to understand student engagement during an online class. This\npaper outlines the design of such a monitoring tool that automatically tracks\nthe attentiveness of the whole class by tracking students' gazes on the screen\nand alerts the teacher when the attention score goes below a certain threshold.\nWe assume the benefits are twofold; 1) teachers will be able to ascertain if\nthe students are attentive or being engaged with the lecture contents and 2)\nthe students will become more attentive in online classes because of this\npassive monitoring system. In this paper, we present the preliminary design and\nfeasibility of using the proposed tool and discuss its applicability in\naugmenting online classes. Finally, we surveyed 31 students asking their\nopinion on the usability as well as the ethical and privacy concerns of using\nsuch a monitoring tool.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06974,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Evaluating Data-Driven Co-Speech Gestures of Embodied Conversational\n  Agents through Real-Time Interaction\n\n  Embodied Conversational Agents that make use of co-speech gestures can\nenhance human-machine interactions in many ways. In recent years, data-driven\ngesture generation approaches for ECAs have attracted considerable research\nattention, and related methods have continuously improved. Real-time\ninteraction is typically used when researchers evaluate ECA systems that\ngenerate rule-based gestures. However, when evaluating the performance of ECAs\nbased on data-driven methods, participants are often required only to watch\npre-recorded videos, which cannot provide adequate information about what a\nperson perceives during the interaction. To address this limitation, we\nexplored use of real-time interaction to assess data-driven gesturing ECAs. We\nprovided a testbed framework, and investigated whether gestures could affect\nhuman perception of ECAs in the dimensions of human-likeness, animacy,\nperceived intelligence, and focused attention. Our user study required\nparticipants to interact with two ECAs - one with and one without hand\ngestures. We collected subjective data from the participants' self-report\nquestionnaires and objective data from a gaze tracker. To our knowledge, the\ncurrent study represents the first attempt to evaluate data-driven gesturing\nECAs through real-time interaction and the first experiment using gaze-tracking\nto examine the effect of ECAs' gestures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.09291,
    "paper_type":"other",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Embodying the Glitch: Perspectives on Generative AI in Dance Practice\n\n  What role does the break from realism play in the potential for generative\nartificial intelligence as a creative tool? Through exploration of glitch, we\nexamine the prospective value of these artefacts in creative practice. This\npaper describes findings from an exploration of AI-generated \"mistakes\" when\nusing movement produced by a generative deep learning model as an inspiration\nsource in dance composition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000103315,
      "text":"PORTAL: Portal Widget for Remote Target Acquisition and Control in\n  Immersive Virtual Environments\n\n  This paper introduces PORTAL (POrtal widget for Remote Target Acquisition and\ncontroL) that allows the user to interact with out-of-reach objects in a\nvirtual environment. We describe the PORTAL interaction technique for placing a\nportal widget and interacting with target objects through the portal. We\nconduct two formal user studies to evaluate PORTAL for selection and\nmanipulation functionalities. The results show PORTAL supports participants to\ninteract with remote objects successfully and precisely. Following that, we\ndiscuss its potential and limitations, and future works.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.0266,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Representing Marginalized Populations: Challenges in Anthropographics\n\n  Anthropographics are human-shaped visualizations that have primarily been\nused within visualization research and data journalism to show humanitarian and\ndemographic data. However, anthropographics have typically been produced by a\nsmall group of designers, researchers, and journalists, and most use\nhomogeneous representations of marginalized populations-representations that\nmight have problematic implications for how viewers perceive the people they\nrepresent. In this paper, we use a critical lens to examine anthropographic\nvisualization practices in projects about marginalized populations. We present\ncritiques that identify three potential challenges related to the use of\nanthropographics and highlight possible unintended consequences-namely (1)\ncreating homogeneous depictions of marginalized populations, (2) treating\nmarginalization as an inclusion criteria, and (3) insufficiently\ncontextualizing datasets about marginalization. Finally, we highlight\nopportunities for anthropographics research, including the need to develop\ntechniques for representing demographic differences between marginalized\npopulations and for studies exploring other potential effects of\nanthropographics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13366,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000235107,
      "text":"Channel Optimized Visual Imagery based Robotic Arm Control under the\n  Online Environment\n\n  An electroencephalogram is an effective approach that provides a\nbidirectional pathway between the user and computer in a non-invasive way. In\nthis study, we adopted the visual imagery data for controlling the BCI-based\nrobotic arm. Visual imagery increases the power of the alpha frequency range of\nthe visual cortex over time as the user performs the task. We proposed a deep\nlearning architecture to decode the visual imagery data using only two channels\nand also we investigated the combination of two EEG channels that has\nsignificant classification performance. When using the proposed method, the\nhighest classification performance using two channels in the offline experiment\nwas 0.661. Also, the highest success rate in the online experiment using two\nchannels (AF3-Oz) was 0.78. Our results provide the possibility of controlling\nthe BCI-based robotic arm using visual imagery data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04618,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Discovering the Hidden Facts of User-Dispatcher Interactions via\n  Text-based Reporting Systems for Community Safety\n\n  Recently, an increasing number of safety organizations in the U.S. have\nincorporated text-based risk reporting systems to respond to safety incident\nreports from their community members. To gain a better understanding of the\ninteraction between community members and dispatchers using text-based risk\nreporting systems, this study conducts a system log analysis of LiveSafe, a\ncommunity safety reporting system, to provide empirical evidence of the\nconversational patterns between users and dispatchers using both quantitative\nand qualitative methods. We created an ontology to capture information (e.g.,\nlocation, attacker, target, weapon, start-time, and end-time, etc.) that\ndispatchers often collected from users regarding their incident tips. Applying\nthe proposed ontology, we found that dispatchers often asked users for\ndifferent information across varied event types (e.g., Attacker for Abuse and\nAttack events, Target for Harassment events). Additionally, using emotion\ndetection and regression analysis, we found an inconsistency in dispatchers'\nemotional support and responsiveness to users' messages between different\norganizations and between incident categories. The results also showed that\nusers had a higher response rate and responded quicker when dispatchers\nprovided emotional support. These novel findings brought significant insights\nto both practitioners and system designers, e.g., AI-based solutions to augment\nhuman agents' skills for improved service quality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2301.00838,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Categorisation of future applications for Augmented Reality in human\n  lunar exploration\n\n  The European Space Agency (ESA) has a clear mission to go forward to the Moon\nin preparation of human presence on Mars. One of the technologies looked at to\nincrease safety and efficiency of astronauts in this context is Augmented\nReality (AR). This technology allows digital visual information to be overlaid\nonto the user's environment through some type of display or projector. In\nrecent years separate studies have been conducted to test the potential value\nof AR for astronauts by implementing a few functionalities on an AR display\nfollowed by testing in terrestrial analogue environments. One of the groups\ncontributing to these investigations is Spaceship EAC (SSEAC). SSEAC is a group\nof interns and trainees at the European Astronaut Centre (EAC) focusing on\nemerging technologies for human space exploration. This paper presents an\noutcome of SSEAC's activities related to AR for lunar extravehicular activities\n(EVAs), in which an approach similar to design thinking was used to explore,\nidentify, and structure the opportunities offered by this technology. The\nresulting categorization of AR use cases can be used to identify new\nfunctionalities to test through prototyping and usability tests and can also be\nused to relate individual studies to each other to gain insight into the\noverall potential value AR has to offer to human lunar exploration. The\napproach adopted in this paper is based on the Fuzzy Front End (FFE) model from\nthe innovation management domain. Utilising a user-driven instead of\ntechnology-driven method resulted in findings that are relevant irrespective of\nthe hardware and software implementation. Instead, the outcome is an overview\nof use cases in which some type of AR system could provide value by\ncontributing to increased astronaut safety, efficiency and\/or efficacy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.09977,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000100997,
      "text":"DCPViz: A Visual Analytics Approach for Downscaled Climate Projections\n\n  This paper introduces a novel visual analytics approach, DCPViz, to enable\nclimate scientists to explore massive climate data interactively without\nrequiring the upfront movement of massive data. Thus, climate scientists are\nafforded more effective approaches to support the identification of potential\ntrends and patterns in climate projections and their subsequent impacts. We\ndesigned the DCPViz pipeline to fetch and extract NEX-DCP30 data with minimal\ndata transfer from their public sources. We implemented DCPViz to demonstrate\nits scalability and scientific value and to evaluate its utility under three\nuse cases based on different models and through domain expert feedback.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13909,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"The Magic of Slow-to-Fast and Constant: Evaluating Time Perception of\n  Progress Bars by Bayesian Model\n\n  Objective: We aimed to use adaptive psychophysics methods, which is a\nBayesian Model, to measure users' time perception of various progress bar\nquantitatively. Background: Progress bar informs users about the status of\nongoing processes. Progress bars frequently display nonuniform speed patterns,\nsuch as acceleration and deceleration. However, which progress bar is perceived\nfaster remain unclear. Methods: We measured the point of subject equality (PSE)\nof the constant progress bar toward four different 5-second progress bars with\na non-constant speed. To measure PSE, in each trial, a constant progress bar\nand a non-constant progress bar were presented to participants. Participants\nneeded to judge which one is shorter. Based on their choice, the model\ngenerated the time duration of constant progress bar in next trial. After 40\ntrials for each non-constant progress bar, the PSE was calculated by the model.\nEye tracking was recorded during the experiment.Results: Our results show that\nthe constant progress bar and speed-up progress bar are perceived to be faster.\nThe anchoring effect fits the results of our study, indicating that the final\npart of the progress bar is more important for time perception. Moreover, the\neye-tracking results indicate that the progress bar is perceived to be slower\nis related to the overload of cognitive resources.Conclusion: The constant\nprogress bar and speed-up progress bar are perceived as the quickest.\nApplication: The results suggest that UX design can use constant or speed-up\nprogress bar, in order to improve user experience in waiting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.0733,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000037087,
      "text":"Federated Learning for Appearance-based Gaze Estimation in the Wild\n\n  Gaze estimation methods have significantly matured in recent years, but the\nlarge number of eye images required to train deep learning models poses\nsignificant privacy risks. In addition, the heterogeneous data distribution\nacross different users can significantly hinder the training process. In this\nwork, we propose the first federated learning approach for gaze estimation to\npreserve the privacy of gaze data. We further employ pseudo-gradient\noptimisation to adapt our federated learning approach to the divergent model\nupdates to address the heterogeneous nature of in-the-wild gaze data in\ncollaborative setups. We evaluate our approach on a real-world dataset\n(MPIIGaze) and show that our work enhances the privacy guarantees of\nconventional appearance-based gaze estimation methods, handles the convergence\nissues of gaze estimators, and significantly outperforms vanilla federated\nlearning by 15.8% (from a mean error of 10.63 degrees to 8.95 degrees). As\nsuch, our work paves the way to develop privacy-aware collaborative learning\nsetups for gaze estimation while maintaining the model's performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.01015,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Self-assess Momentary Mood in Mobile Devices: a Case Study with Mature\n  Female Participants\n\n  Starting from the assumption that mood has a central role in domain-specific\npersuasion systems for well-being, the main goal of this study was to\ninvestigate the feasibility and acceptability of single-input methods to assess\nmomentary mood as a medium for further interventions in health-related mobile\napps destined for mature women. To this aim, we designed a very simple android\nApp providing four user interfaces, each one showing one interactive widget to\nself-assess mood. Two widgets report a hint about the momentary mood they\nrepresent; the last two do not have the hints but were previously refined\nthrough questionnaires administered to 63 women (age 45-65) in order to reduce\ntheir expressive ambiguity. Next, fifteen women (age 45-65 years) were\nrecruited to use the app for 15 days. Participants were polled about their mood\nfour times a day and data were saved in a remote database. Moreover, users were\nasked to fill out a preliminary questionnaire, at the first access to the app,\nand a feedback questionnaire at the end of the testing period. Results appear\nto prove the feasibility and acceptability of this approach to self-assess\nmomentary mood in the target population and provides some potential input\nmethods to be used in this context.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.10254,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"\"Being Simple on Complex Issues\" -- Accounts on Visual Data\n  Communication about Climate Change\n\n  Data visualizations play a critical role in both communicating scientific\nevidence about climate change and in stimulating engagement and action. To\ninvestigate how visualizations can be better utilized to communicate the\ncomplexities of climate change to different audiences, we conducted interviews\nwith 17 experts in the fields of climate change, data visualization, and\nscience communication, as well as with 12 laypersons. Besides questions about\nclimate change communication and various aspects of data visualizations, we\nalso asked participants to share what they think is the main takeaway message\nfor two exemplary climate change data visualizations. Through a thematic\nanalysis, we observe differences regarding the included contents, the length\nand abstraction of messages, and the sensemaking process between and among the\nparticipant groups. On average, experts formulated shorter and more abstract\nmessages, often referring to higher-level conclusions rather than specific\ndetails. We use our findings to reflect on design decisions for creating more\neffective visualizations, particularly in news media sources geared toward lay\naudiences. We hereby discuss the adaption of contents according to the needs of\nthe audience, the trade-off between simplification and accuracy, as well as\ntechniques to make a visualization attractive.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.16465,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"\"I Want to Figure Things Out\": Supporting Exploration in Navigation for\n  People with Visual Impairments\n\n  Navigation assistance systems (NASs) aim to help visually impaired people\n(VIPs) navigate unfamiliar environments. Most of today's NASs support VIPs via\nturn-by-turn navigation, but a growing body of work highlights the importance\nof exploration as well. It is unclear, however, how NASs should be designed to\nhelp VIPs explore unfamiliar environments. In this paper, we perform a\nqualitative study to understand VIPs' information needs and challenges with\nrespect to exploring unfamiliar environments, with the aim of informing the\ndesign of NASs that support exploration. Our findings reveal the types of\nspatial information that VIPs need as well as factors that affect VIPs'\ninformation preferences. We also discover specific challenges that VIPs face\nthat future NASs can address such as orientation and mobility education and\ncollaborating effectively with others. We present design implications for NASs\nthat support exploration, and we identify specific research opportunities and\ndiscuss open socio-technical challenges for making such NASs possible. We\nconclude by reflecting on our study procedure to inform future approaches in\nresearch on ethical considerations that may be adopted while interacting with\nthe broader VIP community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.03296,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"The Chart Excites Me! Exploring How Data Visualization Design Influences\n  Affective Arousal\n\n  As data visualizations have been increasingly applied in mass communication,\ndesigners often seek to grasp viewers immediately and motivate them to read\nmore. Such goals, as suggested by previous research, are closely associated\nwith the activation of emotion, namely affective arousal. Given this\nmotivation, this work takes initial steps toward understanding the\narousal-related factors in data visualization design. We collected a corpus of\n265 data visualizations and conducted a crowdsourcing study with 184\nparticipants during which the participants were asked to rate the affective\narousal elicited by data visualization design (all texts were blurred to\nexclude the influence of semantics) and provide their reasons. Based on the\ncollected data, first, we identified a set of arousal-related design features\nby analyzing user comments qualitatively. Then, we mapped these features to\ncomputable variables and constructed regression models to infer which features\nare significant contributors to affective arousal quantitatively. Through this\nexploratory study, we finally identified four design features (e.g.,\ncolorfulness, the number of different visual channels) cross-validated as\nimportant features correlated with affective arousal.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.04046,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0002214644,
      "text":"Many Destinations, Many Pathways: A Quantitative Analysis of Legitimate\n  Peripheral Participation in Scratch\n\n  Although informal online learning communities have proliferated over the last\ntwo decades, a fundamental question remains: What are the users of these\ncommunities expected to learn? Guided by the work of Etienne Wenger on\ncommunities of practice, we identify three distinct types of learning goals\ncommon to online informal learning communities: the development of domain\nskills, the development of identity as a community member, and the development\nof community-specific values and practices. Given these goals, what is the best\nway to support learning? Drawing from previous research in social computing, we\nask how different types of legitimate peripheral participation by\nnewcomers-contribution to core tasks, engagement with practice proxies, social\nbonding, and feedback exchange-may be associated with these three learning\ngoals. Using data from the Scratch online community, we conduct a quantitative\nanalysis to explore these questions. Our study contributes both theoretical\ninsights and empirical evidence on how different types of learning occur in\ninformal online environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.1581,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Practical Challenges in Indoor Mobile Recommendation\n\n  Recommendation systems are present in multiple contexts as e-commerce,\nwebsites, and media streaming services. As scenarios get more complex,\ntechniques and tools have to consider a number of variables. When recommending\nservices\/products to mobile users while they are in indoor environments next to\nthe object of the recommendation, variables as location, interests, route, and\ninteraction logs also need to be taken into account. In this context, this work\ndiscusses the practical challenges inherent to the context of indoor mobile\nrecommendation (e.g., mall, parking lot, museum, among others) grounded on a\ncase and a systematic review. With the presented results, one expects to\nsupport practitioners in the task of defining the proper approach, technology,\nand notification method when recommending services\/products to mobile users in\nindoor environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.0729,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000159277,
      "text":"AI-Based Emotion Recognition: Promise, Peril, and Prescriptions for\n  Prosocial Path\n\n  Automated emotion recognition (AER) technology can detect humans' emotional\nstates in real-time using facial expressions, voice attributes, text, body\nmovements, and neurological signals and has a broad range of applications\nacross many sectors. It helps businesses get a much deeper understanding of\ntheir customers, enables monitoring of individuals' moods in healthcare,\neducation, or the automotive industry, and enables identification of violence\nand threat in forensics, to name a few. However, AER technology also risks\nusing artificial intelligence (AI) to interpret sensitive human emotions. It\ncan be used for economic and political power and against individual rights.\nHuman emotions are highly personal, and users have justifiable concerns about\nprivacy invasion, emotional manipulation, and bias. In this paper, we present\nthe promises and perils of AER applications. We discuss the ethical challenges\nrelated to the data and AER systems and highlight the prescriptions for\nprosocial perspectives for future AER applications. We hope this work will help\nAI researchers and developers design prosocial AER applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.03767,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Novel Muscle Monitoring by Radiomyography(RMG) and Application to Hand\n  Gesture Recognition\n\n  Conventional electromyography (EMG) measures the continuous neural activity\nduring muscle contraction, but lacks explicit quantification of the actual\ncontraction. Mechanomyography (MMG) and accelerometers only measure body\nsurface motion, while ultrasound, CT-scan and MRI are restricted to in-clinic\nsnapshots. Here we propose a novel radiomyography (RMG) for continuous muscle\nactuation sensing that can be wearable and touchless, capturing both\nsuperficial and deep muscle groups. We verified RMG experimentally by a forearm\nwearable sensor for detailed hand gesture recognition. We first converted the\nradio sensing outputs to the time-frequency spectrogram, and then employed the\nvision transformer (ViT) deep learning network as the classification model,\nwhich can recognize 23 gestures with an average accuracy up to 99% on 8\nsubjects. By transfer learning, high adaptivity to user difference and sensor\nvariation were achieved at an average accuracy up to 97%. We further\ndemonstrated RMG to monitor eye and leg muscles and achieved high accuracy for\neye movement and body postures tracking. RMG can be used with synchronous EMG\nto derive stimulation-actuation waveforms for many future applications in\nkinesiology, physiotherapy, rehabilitation, and human-machine interface.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07832,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000480149,
      "text":"The Ball is in Our Court: Conducting Visualization Research with Sports\n  Experts\n\n  Most sports visualizations rely on a combination of spatial, highly temporal,\nand user-centric data, making sports a challenging target for visualization.\nEmerging technologies, such as augmented and mixed reality (AR\/XR), have\nbrought exciting opportunities along with new challenges for sports\nvisualization. We share our experience working with sports domain experts and\npresent lessons learned from conducting visualization research in SportsXR. In\nour previous work, we have targeted different types of users in sports,\nincluding athletes, game analysts, and fans. Each user group has unique design\nconstraints and requirements, such as obtaining real-time visual feedback in\ntraining, automating the low-level video analysis workflow, or personalizing\nembedded visualizations for live game data analysis. In this paper, we\nsynthesize our best practices and pitfalls we identified while working on\nSportsXR. We highlight lessons learned in working with sports domain experts in\ndesigning and evaluating sports visualizations and in working with emerging\nAR\/XR technologies. We envision that sports visualization research will benefit\nthe larger visualization community through its unique challenges and\nopportunities for immersive and situated analytics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.14609,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"BEAMERS: Brain-Engaged, Active Music-based Emotion Regulation System\n\n  With the increasing demands of emotion comprehension and regulation in our\ndaily life, a customized music-based emotion regulation system is introduced by\nemploying current EEG information and song features, which predicts users'\nemotion variation in the valence-arousal model before recommending music. The\nwork shows that: (1) a novel music-based emotion regulation system with a\ncommercial EEG device is designed without employing deterministic emotion\nrecognition models for daily usage; (2) the system considers users' variant\nemotions towards the same song, and by which calculate user's emotion\ninstability and it is in accordance with Big Five Personality Test; (3) the\nsystem supports different emotion regulation styles with users' designation of\ndesired emotion variation, and achieves an accuracy of over $0.85$ with\n2-seconds EEG data; (4) people feel easier to report their emotion variation\ncomparing with absolute emotional states, and would accept a more delicate\nmusic recommendation system for emotion regulation according to the\nquestionnaire.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.02208,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"Automated Logging Drone: A Computer Vision Drone Implementation\n\n  In recent years, Artificial Intelligence (AI) and Computer Vision (CV) have\nbecome the pinnacle of technology with new developments seemingly every day.\nThis technology along with more powerful drone technology have made autonomous\nsurveillance more sought after. Here an overview of the Automated Logging Drone\n(ALD) project is presented along with examples of how this project can be used\nwith more refining and added features.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.09933,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000079142,
      "text":"Fields: Towards Socially Intelligent Spatial Computing\n\n  In our everyday life, we intuitively use space to regulate our social\ninteractions. When we want to talk to someone, we approach them; if someone\njoins the conversation, we adjust our bodies to make space for them. In\ncontrast, devices are not as considerate: they interrupt us, require us to\ninput commands, and compete for our attention. In this paper, we introduce\nFields, a design framework for ubiquitous computing that informs the design of\nconnected products with social grace. Inspired by interactionist theories on\nsocial interaction, Fields builds on the idea that the physical space we share\nwith computers can be an interface to mediate interactions. It defines a\ngeneralized approach to spatial interactions, and a set of interaction patterns\nthat can be adapted to different ubiquitous computing systems. We investigated\nits value by implementing it in a set of prototypes and evaluating it in a lab\nsetting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.12846,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000131792,
      "text":"Assessment of Human Behavior in Virtual Reality by Eye Tracking\n\n  Virtual reality (VR) is not a new technology but has been in development for\ndecades, driven by advances in computer technology. Currently, VR technology is\nincreasingly being used in applications to enable immersive, yet controlled\nresearch settings. Education and entertainment are two important application\nareas, where VR has been considered a key enabler of immersive experiences and\ntheir further advancement. At the same time, the study of human behavior in\nsuch innovative environments is expected to contribute to a better design of VR\napplications. Therefore, modern VR devices are consistently equipped with\neye-tracking technology, enabling thus further studies of human behavior\nthrough the collection of process data. In particular, eye-tracking technology\nin combination with machine learning techniques and explainable models can\nprovide new insights for a deeper understanding of human behavior during\nimmersion in virtual environments.\n  In this work, a systematic computational framework based on eye-tracking and\nbehavioral user data and state-of-the-art machine learning approaches is\nproposed to understand human behavior and individual differences in VR\ncontexts. This computational framework is then employed in three user studies\nacross two different domains. In the educational domain, two different\nimmersive VR classrooms were created where students can learn and teachers can\ntrain. In terms of VR entertainment, eye movements open a new avenue to\nevaluate VR locomotion techniques from the perspective of user cognitive load\nand user experience. This work paves the way for assessing human behavior in VR\nscenarios and provides profound insights into the way of designing, evaluating,\nand improving interactive VR systems. In particular, more effective and\ncustomizable virtual environments can be created to provide users with tailored\nexperiences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13078,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Participation of Stakeholder in the Design of a Conception Application\n  of Augmentative and Alternative Communication\n\n  The objective of this paper is to describe the implication of an\ninterdisciplinary team involved during a user-centered design methodology to\ndesign the platform (WebSoKeyTo) that meets the needs of therapists to design\naugmentative and alternative communication (AAC) aids for disabled users. We\ndescribe the processes of the design process and the role of the various actors\n(therapists and human computer researchers) in the various phases of the\nprocess. Finally, we analyze a satisfaction scale of the therapists on their\nparticipation in the codesign process. This study demonstrates the interest in\nextending the design actors to other therapists and caregivers (professional\nand family) in the daily life of people with disabilities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03485,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000083778,
      "text":"Patterns of Sociotechnical Design Preferences of Chatbots for\n  Intergenerational Collaborative Innovation : A Q Methodology Study\n\n  Chatbot technology is increasingly emerging as a virtual assistant. Chatbots\ncould allow individuals and organizations to accomplish objectives that are\ncurrently not fully optimized for collaboration across an intergenerational\ncontext. This paper explores the preferences of chatbots as a companion in\nintergenerational innovation. The Q methodology was used to investigate\ndifferent types of collaborators and determine how different choices occur\nbetween collaborators that merge the problem and solution domains of chatbots'\ndesign within intergenerational settings. The study's findings reveal that\nvarious chatbot design priorities are more diverse among younger adults than\nsenior adults. Additionally, our research further outlines the principles of\nchatbot design and how chatbots will support both generations. This research is\nthe first step towards cultivating a deeper understanding of different age\ngroups' subjective design preferences for chatbots functioning as a companion\nin the workplace. Moreover, this study demonstrates how the Q methodology can\nguide technological development by shifting the approach from an age-focused\ndesign to a common goal-oriented design within a multigenerational context.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.08124,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Elasticity Solver in Minecraft for Learning Mechanics of Materials by\n  Gaming\n\n  Video games have emerged as a medium for learning by creating engaging\nenvironments, encouraging creative and deep thinking, and exposing learners to\ncomplex problems. Unfortunately, even though there are increasing examples of\nvideo games for many basic science and engineering concepts, similar efforts\nfor higher level engineering concepts such as mechanics of materials are still\nlacking. Here we present a mesh-free elasticity solver implementation in the\npopular video game Minecraft, a sandbox game where players can build any\nstructure they can imagine. Modifications to the game, called mods in the\nMinecraft community, are a common feature of this platform. Our elasticity mod\ncomputes the stress and deformation of arbitrary structures and colors the\nblocks with a heat-map to visualize the result of the analysis. We used this\nmod in the Honors section of two courses taught at Purdue University: Basic\nMechanics I Statics, Mechanics of Materials. This articles describes our\nexperience developing and deploying this tool to encourage its use in\nbiomedical engineering classrooms. A future goal is to engage the broader\naudience Minecraft players that already interact regularly with Minecraft mods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2301.07687,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000267559,
      "text":"Maybe, Maybe Not: A Survey on Uncertainty in Visualization\n\n  Understanding and evaluating uncertainty play a key role in decision-making.\nWhen a viewer studies a visualization that demands inference, it is necessary\nthat uncertainty is portrayed in it. This paper showcases the importance of\nrepresenting uncertainty in visualizations. It provides an overview of\nuncertainty visualization and the challenges authors and viewers face when\nworking with such charts. I divide the visualization pipeline into four parts,\nnamely data collection, preprocessing, visualization, and inference, to\nevaluate how uncertainty impacts them. Next, I investigate the authors'\nmethodologies to process and design uncertainty. Finally, I contribute by\nexploring future paths for uncertainty visualization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03186,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000056293,
      "text":"Towards Better User Requirements: How to Involve Human Participants in\n  XAI Research\n\n  Human-Center eXplainable AI (HCXAI) literature identifies the need to address\nuser needs. This paper examines how existing XAI research involves human users\nin designing and developing XAI systems and identifies limitations in current\npractices, especially regarding how researchers identify user requirements.\nFinally, we propose several suggestions on how to derive better user\nrequirements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07316,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000158615,
      "text":"Understanding Users' Interaction with Login Notifications\n\n  Login notifications intend to inform users about sign-ins and help them\nprotect their accounts from unauthorized access. Notifications are usually sent\nif a login deviates from previous ones, potentially indicating malicious\nactivity. They contain information like the location, date, time, and device\nused to sign in. Users are challenged to verify whether they recognize the\nlogin (because it was them or someone they know) or to protect their account\nfrom unwanted access. In a user study, we explore users' comprehension,\nreactions, and expectations of login notifications. We utilize two treatments\nto measure users' behavior in response to notifications sent for a login they\ninitiated or based on a malicious actor relying on statistical sign-in\ninformation. We find that users identify legitimate logins but need more\nsupport to halt malicious sign-ins. We discuss the identified problems and give\nrecommendations for service providers to ensure usable and secure logins for\neveryone.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07088,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"Unsupervised Time-Aware Sampling Network with Deep Reinforcement\n  Learning for EEG-Based Emotion Recognition\n\n  Recognizing human emotions from complex, multivariate, and non-stationary\nelectroencephalography (EEG) time series is essential in affective\nbrain-computer interface. However, because continuous labeling of ever-changing\nemotional states is not feasible in practice, existing methods can only assign\na fixed label to all EEG timepoints in a continuous emotion-evoking trial,\nwhich overlooks the highly dynamic emotional states and highly non-stationary\nEEG signals. To solve the problems of high reliance on fixed labels and\nignorance of time-changing information, in this paper we propose a time-aware\nsampling network (TAS-Net) using deep reinforcement learning (DRL) for\nunsupervised emotion recognition, which is able to detect key emotion fragments\nand disregard irrelevant and misleading parts. Extensive experiments are\nconducted on three public datasets (SEED, DEAP, and MAHNOB-HCI) for emotion\nrecognition using leave-one-subject-out cross-validation, and the results\ndemonstrate the superiority of the proposed method against previous\nunsupervised emotion recognition methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03452,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"\"just like therapy!\": Investigating the Potential of Storytelling in\n  Online Postpartum Depression Communities\n\n  One in seven women encounter postpartum depression upon transitioning to\nmotherhood. Many of them frequently seek social support in online support\ngroups. We conducted a mixed-methods formative research to assess the potential\nof digital storytelling on those online platforms. We observed that mothers\nacquire social support from online groups through storytelling. We present\ndesign recommendations for online postpartum depression (PPD) communities to\nutilize storytelling in fostering emotional support and providing relevant\ninformation and education through storytelling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04609,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"CBE Clima Tool: a free and open-source web application for climate\n  analysis tailored to sustainable building design\n\n  Buildings that are designed specifically to respond to the local climate can\nbe more comfortable, energy-efficient, and with a lower environmental impact.\nHowever, there are many social, cultural, and economic obstacles that might\nprevent the wide adoption of designing climate-adapted buildings. One of the\nsaid obstacles can be removed by enabling practitioners to easily access and\nanalyse local climate data. The CBE Clima Tool (Clima) is a free and\nopen-source web application that offers easy access to publicly available\nweather files (in EPW format) specifically created for building energy\nsimulation and design. It provides a series of interactive visualization of the\nvariables therein contained and several derived ones. It is aimed at students,\neducators, and practitioners in the architecture and engineering fields. Since\nits launch has been consistently recording over 3000 monthly unique users from\nover 70 countries worldwide, both in professional and educational settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.13733,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Redirected Walking in Infinite Virtual Indoor Environment Using\n  Change-blindness\n\n  We present a change-blindness based redirected walking algorithm that allows\na user to explore on foot a virtual indoor environment consisting of an\ninfinite number of rooms while at the same time ensuring collision-free walking\nfor the user in real space. This method uses change blindness to scale and\ntranslate the room without the user's awareness by moving the wall while the\nuser is not looking. Consequently, the virtual room containing the current user\nalways exists in the valid real space. We measured the detection threshold for\nwhether the user recognizes the movement of the wall outside the field of view.\nThen, we used the measured detection threshold to determine the amount of\nchanging the dimension of the room by moving that wall. We conducted a\nlive-user experiment to navigate the same virtual environment using the\nproposed method and other existing methods. As a result, users reported higher\nusability, presence, and immersion when using the proposed method while showing\nreduced motion sickness compared to other methods. Hence, our approach can be\nused to implement applications to allow users to explore an infinitely large\nvirtual indoor environment such as virtual museum and virtual model house while\nsimultaneously walking in a small real space, giving users a more realistic\nexperience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07083,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000077155,
      "text":"Decoding Multi-class Motor-related Intentions with User-optimized and\n  Robust BCI System Based on Multimodal Dataset\n\n  A brain-computer interface (BCI) based on electroencephalography (EEG) can be\nuseful for rehabilitation and the control of external devices. Five grasping\ntasks were decoded for motor execution (ME) and motor imagery (MI). During this\nexperiment, eight healthy subjects were asked to imagine and grasp five\nobjects. Analysis of EEG signals was performed after detecting muscle signals\non electromyograms (EMG) with a time interval selection technique on data taken\nfrom these ME and MI experiments. By refining only data corresponding to the\nexact time when the users performed the motor intention, the proposed method\ncan train the decoding model using only the EEG data generated by various motor\nintentions with strong correlation with a specific class. There was an accuracy\nof 70.73% for ME and 47.95% for MI for the five offline tasks. This method may\nbe applied to future applications, such as controlling robot hands with BCIs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03546,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Label Guidance based Object Locating in Virtual Reality\n\n  Object locating in virtual reality (VR) has been widely used in many VR\napplications, such as virtual assembly, virtual repair, virtual remote\ncoaching. However, when there are a large number of objects in the virtual\nenvironment(VE), the user cannot locate the target object efficiently and\ncomfortably. In this paper, we propose a label guidance based object locating\nmethod for locating the target object efficiently in VR. Firstly, we introduce\nthe label guidance based object locating pipeline to improve the efficiency of\nthe object locating. It arranges the labels of all objects on the same screen,\nlets the user select the target labels first, and then uses the flying labels\nto guide the user to the target object. Then we summarize five principles for\nconstructing the label layout for object locating and propose a two-level\nhierarchical sorted and orientated label layout based on the five principles\nfor the user to select the candidate labels efficiently and comfortably. After\nthat, we propose the view and gaze based label guidance method for guiding the\nuser to locate the target object based on the selected candidate labels.It\ngenerates specific flying trajectories for candidate labels, updates the flying\nspeed of candidate labels, keeps valid candidate labels , and removes the\ninvalid candidate labels in real time during object locating with the guidance\nof the candidate labels. Compared with the traditional method, the user study\nresults show that our method significantly improves efficiency and reduces task\nload for object locating.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2301.00001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"NFTrig\n\n  NFTrig is a web-based application created for use as an educational tool to\nteach trigonometry and block chain technology. Creation of the application\nincludes front and back end development as well as integration with other\noutside sources including MetaMask and OpenSea. The primary development\nlanguages include HTML, CSS (Bootstrap 5), and JavaScript as well as Solidity\nfor smart contract creation. The application itself is hosted on Moralis\nutilizing their Web3 API. This technical report describes how the application\nwas created, what the application requires, and smart contract design with\nsecurity considerations in mind. The NFTrig application has underwent\nsignificant testing and validation prior to and after deployment. Future\nsuggestions and recommendations for further development, maintenance, and use\nin other fields for education are also described.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.11768,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Occupant Privacy Perception, Awareness, and Preferences in Smart Office\n  Environments\n\n  Building management systems tout numerous benefits, such as energy efficiency\nand occupant comfort but rely on vast amounts of data from various sensors.\nAdvancements in machine learning algorithms make it possible to extract\npersonal information about occupants and their activities beyond the intended\ndesign of a non-intrusive sensor. However, occupants are not informed of data\ncollection and possess different privacy preferences and thresholds for privacy\nloss. While privacy perceptions and preferences are most understood in smart\nhomes, limited studies have evaluated these factors in smart office buildings,\nwhere there are more users and different privacy risks. To better understand\noccupants' perceptions and privacy preferences, we conducted twenty-four\nsemi-structured interviews between April 2022 and May 2022 on occupants of a\nsmart office building. We found that data modality features and personal\nfeatures contribute to people's privacy preferences. The features of the\ncollected modality define data modality features -- spatial, security, and\ntemporal context. In contrast, personal features consist of one's awareness of\ndata modality features and data inferences, definitions of privacy and\nsecurity, and the available rewards and utility. Our proposed model of people's\nprivacy preferences in smart office buildings helps design more effective\nmeasures to improve people's privacy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07764,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"DOPAMINE: Doppler frequency and Angle of arrival MINimization of\n  tracking Error for extended reality\n\n  In this paper, we investigate how Joint Communication And Sensing (JCAS) can\nbe used to improve the Inertial Measurement Unit (IMU)- based tracking accuracy\nof eXtended Reality (XR) Head-Mounted Displays (HMDs). Such tracking is used\nwhen optical and InfraRed (IR) tracking is lost, and its lack of accuracy can\nlead to disruption of the user experience. In particular, we analyze the impact\nof using doppler-based speed estimation to aid the accelerometer-based position\nestimation, and Angle of Arrival (AoA) estimation to aid the gyroscope-based\norientation estimation. Although less accurate than IMUs for short times in\nfact, the JCAS based methods require one fewer integration step, making the\ntracking more sustainable over time. Based on the proposed model, we conclude\nthat at least in the case of the position estimate, introducing JCAS can make\nlong lasting optical\/IR tracking losses more sustainable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.14336,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Exploring Text Selection in Augmented Reality Systems\n\n  Text selection is a common and essential activity during text interaction in\nall interactive systems. As Augmented Reality (AR) head-mounted displays (HMDs)\nbecome more widespread, they will need to provide effective interaction\ntechniques for text selection that ensure users can complete a range of text\nmanipulation tasks (e.g., to highlight, copy, and paste text, send instant\nmessages, and browse the web). As a relatively new platform, text selection in\nAR is largely unexplored and the suitability of interaction techniques\nsupported by current AR HMDs for text selection tasks is unclear. This research\naims to fill this gap and reports on an experiment with 12 participants, which\ncompares the performance and usability (user experience and workload) of four\npossible techniques (Hand+Pinch, Hand+Dwell, Head+Pinch, and Head+Dwell). Our\nresults suggest that Head+Dwell should be the default selection technique, as\nit is relatively fast, has the lowest error rate and workload, and has the\nhighest-rated user experience and social acceptance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.08688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000624193,
      "text":"Assessment of user-interaction strategies for neurosurgical data\n  navigation and annotation in virtual reality\n\n  While virtual-reality (VR) has shown great promise in radiological tasks,\neffective user-interaction strategies that can improve efficiency and\nergonomics are still under-explored and systematic evaluations of VR\ninteraction techniques in the context of complex anatomical models are rare.\nTherefore, our study aims to identify the most effective interaction techniques\nfor two common neurosurgical planning tasks in VR (point annotation and\nnote-taking) from the state-of-the-arts, and propose a novel technique for\nefficient sub-volume selection necessary in neuroanatomical navigation. We\nassessed seven user-interaction methods with multiple input modalities (gaze,\nhead motion, controller, and voice) for point placement and note-taking in the\ncontext of annotating brain aneurysms for cerebrovascular surgery. Furthermore,\nwe proposed and evaluated a novel technique, called magnified selection diorama\n(Maserama) for easy navigation and selection of complex 3D anatomies in VR.\nBoth quantitative and semi-quantitative (i.e., NASA Task Load Index) metrics\nwere employed through user studies to reveal the performance of each\ninteraction scheme in terms of accuracy, efficiency, and usability. Our\nevaluations demonstrated that controller-based interaction is preferred over\neye-tracking-based methods for point placement while voice recording and\nvirtual keyboard typing are better than freehand writing for note-taking.\nFurthermore, our new Maserama sub-volume selection technique was proven to be\nhighly efficient and easy-to-use. Our study is the first to provide a\nsystematic assessment of existing and new VR interaction schemes for\nneurosurgical data navigation and annotation. It offers valuable insights and\ntools to guide the design of future VR systems for radiological and surgical\napplications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09846,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"PullupStructs: Digital Fabrication for Folding Structures via Pull-up\n  Nets\n\n  In this paper, we introduce a method to rapidly create 3D geometries by\nfolding 2D sheets via pull-up nets. Given a 3D structure, we unfold its mesh\ninto a planar 2D sheet using heuristic algorithms and populate these with\ncutlines and throughholes. We develop a web-based simulation tool that\ntranslates users' 3D meshes into manufacturable 2D sheets. After laser-cutting\nthe sheet and feeding thread through these throughholes to form a pull-up net,\npulling the thread will fold the sheet into the 3D structure using a single\ndegree of freedom. We introduce the fabrication process and build a variety of\nprototypes demonstrating the method's ability to rapidly create a breadth of\ngeometries suitable for low-fidelity prototyping that are both load-bearing and\naesthetic across a range of scales. Future work will expand the breadth of\ngeometries available and evaluate the ability of our prototypes to sustain\nstructural loads.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09859,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"CompuMat: A Computational Composite Material for Tangible Interaction\n\n  This paper introduces a computational composite material comprising layers\nfor actuation, computation and energy storage. Key to its design is inexpensive\nmaterials assembled from traditionally available fabrication machines to\nsupport the rapid exploration of applications from computational composites.\nThe actuation layer is a soft magnetic sheet that is programmed to either bond,\nrepel, or remain agnostic to other areas of the sheet. The computation layer is\na flexible PCB made from copper-clad kapton engraved by a fiber laser, powered\nby a third energy-storage layer comprised of 0.4mm-thin lithium polymer\nbatteries. We present the material layup and an accompanying digital\nfabrication process enabling users to rapidly prototype their own untethered,\ninteractive and tangible prototypes. The material is low-profile, inexpensive,\nand fully untethered, capable of being used for a variety of applications in\nHCI and robotics including structural origami and proprioception.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.00519,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Corvo: Visualizing CellxGene Single-Cell Datasets in Virtual Reality\n\n  The CellxGene project has enabled access to single-cell data in the\nscientific community, providing tools for browsed-based no-code analysis of\nmore than 500 annotated datasets. However, single-cell data requires\ndimensional reduction to visualize, and 2D embedding does not take full\nadvantage of three-dimensional human spatial understanding and cognition.\nCompared to a 2D visualization that could potentially hide gene expression\npatterns, 3D Virtual Reality may enable researchers to make better use of the\ninformation contained within the datasets. For this purpose, we present\n\\emph{Corvo}, a fully free and open-source software tool that takes the\nvisualization and analysis of CellxGene single-cell datasets to 3D Virtual\nReality. Similar to CellxGene, Corvo takes a no-code approach for the end user,\nbut also offers multimodal user input to facilitate fast navigation and\nanalysis, and is interoperable with the existing Python data science ecosystem.\nIn this paper, we explain the design goals of Corvo, detail its approach to the\nVirtual Reality visualization and analysis of single-cell data, and briefly\ndiscuss limitations and future extensions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.00607,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000138415,
      "text":"Real-time Trust Prediction in Conditionally Automated Driving Using\n  Physiological Measures\n\n  Trust calibration presents a main challenge during the interaction between\ndrivers and automated vehicles (AVs). In order to calibrate trust, it is\nimportant to measure drivers' trust in real time. One possible method is\nthrough modeling its dynamic changes using machine learning models and\nphysiological measures. In this paper, we proposed a technique based on machine\nlearning models to predict drivers' dynamic trust in conditional AVs using\nphysiological measurements in real time. We conducted the study in a driving\nsimulator where participants were requested to take over control from automated\ndriving in three conditions that included a control condition, a false alarm\ncondition, a miss condition with eight takeover requests (TORs) in different\nscenarios. Drivers' physiological measures were recorded during the experiment,\nincluding galvanic skin response (GSR), heart rate (HR) indices, and\neye-tracking metrics. Using five machine learning models, we found that eXtreme\nGradient Boosting (XGBoost) performed the best and was able to predict drivers'\ntrust in real time with an f1-score of 89.1%. Our findings provide good\nimplications on how to design an in-vehicle trust monitoring system to\ncalibrate drivers' trust to facilitate interaction between the driver and the\nAV in real time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  }
]