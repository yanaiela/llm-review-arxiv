arxiv_id,paper_type,period,year,month,pangram_prediction
2301.10007,review,post_llm,2023,1,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Information Governance as a Socio-Technical Process in the Development\n  of Trustworthy Healthcare AI\n\n  In order to develop trustworthy healthcare artificial intelligence (AI)\nprospective and ergonomics studies that consider the complexity and reality of\nreal-world applications of AI systems are needed. To achieve this, technology\ndevelopers and deploying organisations need to form collaborative partnerships.\nThis entails access to healthcare data, which frequently might also include\npotentially identifiable data such as audio recordings of calls made to an\nambulance service call centre. Information Governance (IG) processes have been\nput in place to govern the use of personal confidential data. However,\nnavigating IG processes in the formative stages of AI development and\npre-deployment can be challenging, because the legal basis for data sharing is\nexplicit only for the purpose of delivering patient care, i.e., once a system\nis put into service. In this paper we describe our experiences of managing IG\nfor the assurance of healthcare AI, using the example of an\nout-of-hospital-cardiac-arrest recognition software within the context of the\nWelsh Ambulance Service. We frame IG as a socio-technical process. IG processes\nfor the development of trustworthy healthcare AI rely on information governance\nwork, which entails dialogue, negotiation, and trade-offs around the legal\nbasis for data sharing, data requirements and data control. Information\ngovernance work should start early in the design life cycle and will likely\ncontinue throughout. This includes a focus on establishing and building\nrelationships, as well as a focus on organisational readiness deeper\nunderstanding of both AI technologies as well as their safety assurance\nrequirements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.04715,regular,post_llm,2023,1,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'How do ""technical"" design-choices made when building algorithmic\n  decision-making tools for criminal justice authorities create constitutional\n  dangers? Part II\n\n  This two-part paper argues that seemingly ""technical"" choices made by\ndevelopers of machine-learning based algorithmic tools used to inform decisions\nby criminal justice authorities can create serious constitutional dangers,\nenhancing the likelihood of abuse of decision-making power and the scope and\nmagnitude of injustice. Drawing on three algorithmic tools in use, or recently\nused, to assess the ""risk"" posed by individuals to inform how they should be\ntreated by criminal justice authorities, we integrate insights from data\nscience and public law scholarship to show how public law principles and more\nspecific legal duties that are rooted in these principles, are routinely\noverlooked in algorithmic tool-building and implementation. We argue that\ntechnical developers must collaborate closely with public law experts to ensure\nthat if algorithmic decision-support tools are to inform criminal justice\ndecisions, those tools are configured and implemented in a manner that is\ndemonstrably compliant with public law principles and doctrine, including\nrespect for human rights, throughout the tool-building process.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.08627,regular,post_llm,2023,1,"{'ai_likelihood': 4.371007283528646e-06, 'text': '""This Applies to the RealWorld"": Student Perspectives on Integrating\n  Ethics into a Computer Science Assignment\n\n  There is a growing movement in undergraduate computer science (CS) programs\nto embed ethics across CS classes rather than relying solely on standalone\nethics courses. One strategy is creating assignments that encourage students to\nreflect on ethical issues inherent to the code they write. Building off prior\nwork that has surveyed students after doing such assignments in class, we\nconducted focus groups with students who reviewed a new introductory\nethics-based CS assignment. In this experience report, we present a case study\ndescribing our process of designing an ethics-based assignment and proposing\nthe assignment to students for feedback. Participants in our focus groups not\nonly shared feedback on the assignment, but also on the integration of ethics\ninto coding assignments in general, revealing the benefits and challenges of\nthis work from a student perspective. We also generated novel ethics-oriented\nassignment concepts alongside students. Deriving from tech controversies that\nparticipants felt most affected by, we created a bank of ideas as a starting\npoint for further curriculum development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.03544,review,post_llm,2023,1,"{'ai_likelihood': 2.4769041273328996e-05, 'text': 'Report on the Future of Conferences\n\n  In 2020, virtual conferences became almost the only alternative to\ncancellation. Now that the pandemic is subsiding, the pros and cons of virtual\nconferences need to be reevaluated. In this report, we scrutinize the dynamics\nand economics of conferences and highlight the history of successful virtual\nmeetings in industry. We also report on the attitudes of conference attendees\nfrom an informal survey we ran in spring 2022.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.02734,review,post_llm,2023,1,"{'ai_likelihood': 2.814663781060113e-06, 'text': ""Political, economic, and governance attitudes of blockchain users\n\n  We present a survey to evaluate crypto-political, crypto-economic, and\ncrypto-governance sentiment in people who are part of a blockchain ecosystem.\nBased on 3710 survey responses, we describe their beliefs, attitudes, and modes\nof participation in crypto and investigate how self-reported political\naffiliation and blockchain ecosystem affiliation are associated with these. We\nobserved polarization in questions on perceptions of the distribution of\neconomic power, personal attitudes towards crypto, normative beliefs about the\ndistribution of power in governance, and external regulation of blockchain\ntechnologies. Differences in political self-identification correlated with\nopinions on economic fairness, gender equity, decision-making power and how to\nobtain favorable regulation, while blockchain affiliation correlated with\nopinions on governance and regulation of crypto and respondents' semantic\nconception of crypto and personal goals for their involvement. We also find\nthat a theory-driven constructed political axis is supported by the data and\ninvestigate the possibility of other groupings of respondents or beliefs\narising from the data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10006,review,post_llm,2023,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'The Competitive Leverage Paradox Effect on Information Systems Life\n  Cycle\n\n  The fierce market competition has put pressure on organizations leveraging\ntheir value chains. The continuous development in strategic technologies such\nas Artificial Intelligence (AI) has pushed organizations to continuously\nacquire new Intelligent Information Systems (IIS) while underutilizing existing\nones leading to the competitive leverage paradox. However, research on\nunderutilizing IIS has focused on the social and organizational aspects of the\nproblem, ignoring the flaws in designing and evaluating IIS. One of the\noverlooked factors is the effective life span of an IIS. This research\nconducted a systematic literature review to profoundly investigate the\ndeterminants of the competitive leverage paradox and its effect on the IIS life\ncycle. The research studies the IISs from economic and design perspectives. We\nalso explore the design and strategic factors that led to defects in the\neffective life cycle of IIS. This research calls to consider the economic, and\ndesign factors in addressing the underutilization of IIS. The study also\npresents future research propositions to enhance IIS life cycle and return on\ninvestment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.01991,regular,post_llm,2023,1,"{'ai_likelihood': 2.1292103661431207e-05, 'text': 'Bubble or Not: Measurements, Analyses, and Findings on the Ethereum\n  ERC721 and ERC1155 Non-fungible Token Ecosystem\n\n  The non-fungible token (NFT) is an emergent type of cryptocurrency that has\ngarnered extensive attention since its inception. The uniqueness,\nindivisibility and humanistic value of NFTs are the key characteristics that\ndistinguish them from traditional tokens. The market capitalization of NFT\nreached 21.5 billion USD in 2021, almost 200 times of all previous\ntransactions. However, the subsequent rapid decline in NFT market fever in the\nsecond quarter of 2022 casts doubts on the ostensible boom in the NFT market.\nTo date, there has been no comprehensive and systematic study of the NFT trade\nmarket or of the NFT bubble and hype phenomenon. To fill this gap, we conduct\nan in-depth investigation of the whole Ethereum ERC721 and ERC1155 NFT\necosystem via graph analysis and apply several metrics to measure the\ncharacteristics of NFTs. By collecting data from the whole blockchain, we\nconstruct three graphs, namely NFT create graph, NFT transfer graph, and NFT\nhold graph, to characterize the NFT traders, analyze the characteristics of\nNFTs, and discover many observations and insights. Moreover, we propose new\nindicators to quantify the activeness and value of NFT and propose an algorithm\nthat combines indicators and graph analyses to find bubble NFTs. Real-world\ncases demonstrate that our indicators and approach can be used to discern\nbubble NFTs effectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.03784,regular,post_llm,2023,1,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Inside the Black Box: Detecting and Mitigating Algorithmic Bias across\n  Racialized Groups in College Student-Success Prediction\n\n  Colleges and universities are increasingly turning to algorithms that predict\ncollege-student success to inform various decisions, including those related to\nadmissions, budgeting, and student-success interventions. Because predictive\nalgorithms rely on historical data, they capture societal injustices, including\nracism. In this study, we examine how the accuracy of college student success\npredictions differs between racialized groups, signaling algorithmic bias. We\nalso evaluate the utility of leading bias-mitigating techniques in addressing\nthis bias. Using nationally representative data from the Education Longitudinal\nStudy of 2002 and various machine learning modeling approaches, we demonstrate\nhow models incorporating commonly used features to predict college-student\nsuccess are less accurate when predicting success for racially minoritized\nstudents. Common approaches to mitigating algorithmic bias are generally\nineffective at eliminating disparities in prediction outcomes and accuracy\nbetween racialized groups.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.05619,review,post_llm,2023,1,"{'ai_likelihood': 2.6490953233506946e-07, 'text': ""Understanding and improving social factors in education: a computational\n  social science approach\n\n  Over the past decade, an explosion in the availability of education-related\ndatasets has enabled new computational research in education. Much of this work\nhas investigated digital traces of online learners in order to better\nunderstand and optimize their cognitive learning processes. Yet cognitive\nlearning on digital platforms does not equal education. Instead, education is\nan inherently social, cultural, economic, and political process manifesting in\nphysical spaces, and educational outcomes are influenced by many factors that\nprecede and shape the cognitive learning process. Many of these are social\nfactors like children's connections to schools (including teachers, counselors,\nand role models), parents and families, and the broader neighborhoods in which\nthey live. In this article, we briefly discuss recent studies of learning\nthrough large-scale digital platforms, but largely focus on those exploring\nsociological aspects of education. We believe computational social scientists\ncan creatively advance this emerging research frontier-and in doing so, help\nfacilitate more equitable educational and life outcomes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.08405,regular,post_llm,2023,1,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'SugarChain: Blockchain technology meets Agriculture -- The case study\n  and analysis of the Indian sugarcane farming\n\n  Not only in our country and Asia, but the agriculture sector is also lagging\nall over the world while using new technologies and innovations. Farmers are\nnot getting the accurate price and compensation of their products because of\nseveral reasons. The intermediate persons or say middlemen are controlling the\nprices and product delivery on their own. Due to lack of education,\ntechnological advancement, market knowledge, post-harvesting processes, and\nmiddleman involvement, farmers are always deprived of their actual pay and\nefforts. The use of blockchain technology can help such farmers to automate the\nprocess with high trust. We have presented our case study and analysis for the\nIndian sugarcane farming with data collected from farmers. The system\nimplementation, testing, and result analysis has been shown based on the case\nstudy. The overall purpose of our research is to emphasize and motivate the\nagricultural products and benefit the farmers with the use of blockchain\ntechnology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10223,regular,post_llm,2023,1,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Case study Vanderbilt University Medical Center Data Chaos\n\n  The healthcare industry is growing rapidly in the United States because of\nthe increased number of the aging population, shared consciousness of personal\nhealth problems, and medical technology improvements. As a result of the\ngrowing industry of healthcare, new emerging issues occur in the collection and\nstorage of patient data, and new ways to process, analyze and distribute these\ndata. This has exposed various security threats to personal health data (Lee,\n2022). In this case study, we will discuss the issues Vanderbilt University\nMedical Center (VUMC) challenges while implementing EHR systems which are used\nto analyze and monitor health records by the users such as doctors,\norganizations staff, and pharmaceutical agencies(Kaul et al., 2020), and we\nwill analyze these issues and provide solutions and recommendations to solve\nthem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.0557,regular,post_llm,2023,1,"{'ai_likelihood': 4.470348358154297e-06, 'text': 'John Clark\'s Latin Verse Machine: 19th Century Computational Creativity\n\n  John Clark was inventor of the Eureka machine to generate hexameter Latin\nverse. He labored for 13 years from 1832 to implement the device that could\ncompose at random over 26 million different lines of well-formed verse. This\npaper proposes that Clark should be regarded as an early cognitive scientist.\nClark described his machine as an illustration of a theory of ""kaleidoscopic\nevolution"" whereby the Latin verse is ""conceived in the mind of the machine""\nthen mechanically produced and displayed. We describe the background to\nautomated generation of verse, the design and mechanics of Eureka, its\nreception in London in 1845 and its place in the history of language generation\nby machine. The article interprets Clark\'s theory of kaleidoscopic evolution in\nterms of modern cognitive science. It suggests that Clark has not been given\nthe recognition he deserves as a pioneer of computational creativity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.09874,regular,post_llm,2023,1,"{'ai_likelihood': 2.1954377492268882e-05, 'text': ""What Drives Virtual Influencer's Impact?\n\n  In the midst of the influencer marketing boom, more and more companies are\nshifting resources from real to virtual (or computer-generated) influencers.\nBut while virtual influencers have the potential to engage consumers and drive\naction, some posts resonate and boost sales, while others do not. What makes\nsome virtual influencer posts more impactful? This work examines how including\nsomeone else in photos shapes consumer responses to virtual influencers' posts.\nA multimethod investigation, combining automated image and text analysis of\nthousands of social media posts with controlled experiments, demonstrates that\ncompanion presence boosts impact. These effects are driven by trust. Companion\npresence makes virtual influencers seem more human, which makes them seem more\ntrustworthy, and thus increases the impact of their posts. Taken together, the\nfindings shed light on how others' presence shapes responses to virtual\ninfluencer content, reveal a psychological mechanism through which companions\naffect consumer perceptions, and provide actionable insights for designing more\nimpactful social media content.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.03954,regular,post_llm,2023,1,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'The Democratic Illusion through the Technological Illusion: a Case Study\n  of the Implementation of a Blockchain to Support an E-voting Platform in\n  Moscow (Active Citizen)\n\n  This paper presents an ongoing analyze of the Active Citizen e-voting system\nproposed by the Moscow city hall. This research points out that the main\nobjective of the platform is not to enhance the democratic power of the\nMuscovites, but to strengthen the position of Moscow as a modern city at a\nworld scale and the position of the city hall in the Russian political system.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.05996,review,post_llm,2023,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'Understanding Online Behaviors through a Temporal Lens\n\n  Timestamps in digital traces include significant detailed information on when\nhuman behaviors occur, which is universally available and standardized in all\ntypes of digital traces. Nevertheless, the concept of time is under-explicated\nin empirical studies of online behaviors. This paper discusses the\n(un)desirable properties of timestamps in digital traces and summarizes how\ntimestamps in digital traces have been utilized in existing studies of online\nbehaviors. The paper argues that time-in-behaviors perspective can provide a\nmicroscope with a renovated temporal lens to observe and understand online\nbehaviors. Going beyond the traditional behaviors-in-time perspective,\ntime-in-behaviors perspective enables empirical examination of online behaviors\nfrom multiple units of analysis (e.g., discrete behaviors, behavioral sessions,\nand behavioral trajectories) and from multiple dimensions (e.g., duration,\norder, transition, rhythm). The paper shows the potentials of the\ntime-in-behaviors perspective with several empirical cases and proposes future\ndirections in explicating the concept of time in computational social science.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10221,regular,post_llm,2023,1,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Social Metaverse: Challenges and Solutions\n\n  Social metaverse is a shared digital space combining a series of\ninterconnected virtual worlds for users to play, shop, work, and socialize. In\nparallel with the advances of artificial intelligence (AI) and growing\nawareness of data privacy concerns, federated learning (FL) is promoted as a\nparadigm shift towards privacy-preserving AI-empowered social metaverse.\nHowever, challenges including privacy-utility tradeoff, learning reliability,\nand AI model thefts hinder the deployment of FL in real metaverse applications.\nIn this paper, we exploit the pervasive social ties among users/avatars to\nadvance a social-aware hierarchical FL framework, i.e., SocialFL for a better\nprivacy-utility tradeoff in the social metaverse. Then, an aggregator-free\nrobust FL mechanism based on blockchain is devised with a new block structure\nand an improved consensus protocol featured with on/off-chain collaboration.\nFurthermore, based on smart contracts and digital watermarks, an automatic\nfederated AI (FedAI) model ownership provenance mechanism is designed to\nprevent AI model thefts and collusive avatars in social metaverse. Experimental\nfindings validate the feasibility and effectiveness of proposed framework.\nFinally, we envision promising future research directions in this emerging\narea.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.00435,regular,post_llm,2023,1,"{'ai_likelihood': 4.0398703681098095e-06, 'text': 'Trojaning semi-supervised learning model via poisoning wild images on\n  the web\n\n  Wild images on the web are vulnerable to backdoor (also called trojan)\npoisoning, causing machine learning models learned on these images to be\ninjected with backdoors. Most previous attacks assumed that the wild images are\nlabeled. In reality, however, most images on the web are unlabeled.\nSpecifically, we study the effects of unlabeled backdoor images under\nsemi-supervised learning (SSL) on widely studied deep neural networks. To be\nrealistic, we assume that the adversary is zero-knowledge and that the\nsemi-supervised learning model is trained from scratch. Firstly, we find the\nfact that backdoor poisoning always fails when poisoned unlabeled images come\nfrom different classes, which is different from poisoning the labeled images.\nThe reason is that the SSL algorithms always strive to correct them during\ntraining. Therefore, for unlabeled images, we implement backdoor poisoning on\nimages from the target class. Then, we propose a gradient matching strategy to\ncraft poisoned images such that their gradients match the gradients of target\nimages on the SSL model, which can fit poisoned images to the target class and\nrealize backdoor injection. To the best of our knowledge, this may be the first\napproach to backdoor poisoning on unlabeled images of trained-from-scratch SSL\nmodels. Experiments show that our poisoning achieves state-of-the-art attack\nsuccess rates on most SSL algorithms while bypassing modern backdoor defenses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10076,review,post_llm,2023,1,"{'ai_likelihood': 2.2517310248480903e-06, 'text': ""Influential Factors of Users' Trust in the Range Estimation Systems of\n  Battery Electric Vehicles -- A Survey Study in China\n\n  Although the rapid development of battery technology has greatly increased\nthe range of battery electric vehicle (BEV), the range anxiety is still a major\nconcern of BEV users or potential users. Previous work has proposed a framework\nexplaining the influential factors of range anxiety and users' trust toward the\nrange estimation system (RES) of BEV has been identified as a leading factor of\nrange anxiety. The trust in RES may further influence BEV users' charging\ndecisions. However, the formation of trust in RES of BEVs has not yet explored.\nIn this work, a questionnaire has been designed to investigate BEV users' trust\nin RES and further explore the influential factors of BEV users' charging\ndecision. In total, 152 samples collected from the BEV users in mainland China\nhave been analyzed. The BEV users' gender, driving area, knowledge of BEV or\nRES, system usability and trust in battery system of smartphones have been\nidentified as influential factors of RES in BEVs, supporting the three-layer\nframework in automation-related trust (i.e., dispositional trust, situational\ntrust and learned trust). A connection between smartphone charging behaviors\nand BEV charging behaviors has also been observed. The results from this study\ncan provide insights on the design of RES in BEVs in order to alleviate range\nanxiety among users. The results can also inform the design of strategies\n(e.g., advertising, training and in-vehicle HMI design) that can facilitate\nmore rational charging decisions among BEV users.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.06885,regular,post_llm,2023,1,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Computer Science for Future -- Sustainability and Climate Protection in\n  the Computer Science Courses of the HAW Hamburg\n\n  Computer Science for Future (CS4F) is an initiative in the Department of\nComputer Science at HAW Hamburg. The aim of the initiative is a paradigm shift\nin the discipline of computer science, thus establishing sustainability goals\nas a primary leitmotif for teaching and research. The focus is on teaching\nsince the most promising multipliers are the students of a university. The\nchange in teaching influences our research, the transfer to business and civil\nsociety as well as the change in our own institution. In this article, we\npresent the initiative CS4F and reflect primarily on the role of students as\namplifiers in the transformation process of computer science.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10229,regular,post_llm,2023,1,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'ETHNO-DAANN: Ethnographic Engagement Classification by Deep Adversarial\n  Transfer Learning\n\n  Student motivation is a key research agenda due to the necessity of both\npostcolonial education reform and youth job-market adaptation in ongoing fourth\nindustrial revolution. Post-communism era teachers are prompted to analyze\nstudent ethnicity information such as background, origin with the aim of\nproviding better education. With the proliferation of smart-device data,\never-increasing demand for distance learning platforms and various survey\nresults of virtual learning, we are fortunate to have some access to student\nengagement data. In this research, we are motivated to address the following\nquestions: can we predict student engagement from ethnographic information when\nwe have limited labeled knowledge? If the answer is yes, can we tell which\nfeatures are most influential in ethnographic engagement learning? In this\ncontext, we have proposed a deep neural network based transfer learning\nalgorithm ETHNO-DAANN with adversarial adaptation for ethnographic engagement\nprediction. We conduct a survey among participants about ethnicity-based\nstudent motivation to figure out the most influential feature helpful in final\nprediction. Thus, our research stands as a general solution for ethnographic\nmotivation parameter estimation in case of limited labeled data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.13537,regular,post_llm,2023,1,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Where You Are Is What You Do: On Inferring Offline Activities From\n  Location Data\n\n  In this paper we investigate the ability of modern machine learning\nalgorithms in inferring basic offline activities,~e.g., shopping and dining,\nfrom location data. Using anonymized data of thousands of users of a prominent\nlocation-based social network, we empirically demonstrate that not only\nstate-of-the-art machine learning excels at the task at hand~(F1 score>0.9) but\nalso tabular models are among the best performers. The findings we report here\nnot only fill an existing gap in the literature, but also highlight the\npotential risks of such capabilities given the ubiquity of location data and\nthe high accessibility of tabular machine learning models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.07471,regular,post_llm,2023,1,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Significance of anonymity and privacy in improving inclusivity and\n  diversity in Higher Education Learning Environments\n\n  Interactions between lecturers and students are the key to learning in the\nhigher education environment. In this paper, the investigation pursues two\ndifferent contexts to understand these interactions and the impact of anonymity\nand privacy in different interactions in the Computer Science (CS) department.\nThe first context ""different interaction between a lecturer and students"" is\ninvestigated using phenomenological research approach by interviewing lecturer\nin CS ($N_a = 5$). The second context ""the significance of anonymity and\nprivacy in interactions"" is investigated using a quantitative and qualitative\nquestionnaire-based research method using an online student questionnaire ($N_b\n= 53$). The study finds a large gap between students\' perception of preferred\ncommunication methods and the use of the same communication method. From the\nsecond context study, it is evident that ""anonymity and privacy"" in online\nsurveys and module evaluations are preferred by all student participants, thus\nsupporting diversity and inclusivity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10235,review,post_llm,2023,1,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'When the Metaverse Meets Carbon Neutrality: Ongoing Efforts and\n  Directions\n\n  The metaverse has recently gained increasing attention from the public. It\nbuilds up a virtual world where we can live as a new role regardless of the\nrole we play in the physical world. However, building and operating this\nvirtual world will generate an extraordinary amount of carbon emissions for\ncomputing, communicating, displaying, and so on. This inevitably hinders the\nrealization of carbon neutrality as a priority of our society, adding heavy\nburden to our earth. In this survey, we first present a green viewpoint of the\nmetaverse by investigating the carbon issues in its three core layers, namely\nthe infrastructure layer, the interaction layer, and the economy layer, and\nestimate their carbon footprints in the near future. Next, we analyze a range\nof current and emerging applicable green techniques for the purpose of reducing\nenergy usage and carbon emissions of the metaverse, and discuss their\nlimitations in supporting metaverse workloads. Then, in view of these\nlimitations, we discuss important implications and bring forth several insights\nand future directions to make each metaverse layer greener. After that, we\ninvestigate green solutions from the governance perspective, including both\npublic policies in the physical world and regulation of users in the virtual\nworld, and propose an indicator Carbon Utility (CU) to quantify the service\nquality brought by an user activity per unit of carbon emissions. Finally, we\nidentify an issue for the metaverse as a whole and summarize three directions:\n(1) a comprehensive consideration of necessary performance metrics, (2) a\ncomprehensive consideration of involved layers and multiple internal\ncomponents, and (3) a new assessing, recording, and regulating mechanism on\ncarbon footprints of user activities. Our proposed quantitative indicator CU\nwould be helpful in regulating user activities in the metaverse world.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10087,review,post_llm,2023,1,"{'ai_likelihood': 1.2483861711290148e-05, 'text': 'Building Resilience to Climate Driven Extreme Events with Computing\n  Innovations: A Convergence Accelerator Report\n\n  In 2022, the National Science Foundation (NSF) funded the Computing Research\nAssociation (CRA) to conduct a workshop to frame and scope a potential\nConvergence Accelerator research track on the topic of ""Building Resilience to\nClimate-Driven Extreme Events with Computing Innovations"". The CRA\'s research\nvisioning committee, the Computing Community Consortium (CCC), took on this\ntask, organizing a two-part community workshop series, beginning with a small,\nin-person brainstorming meeting in Denver, CO on 27-28 October 2022, followed\nby a virtual event on 10 November 2022. The overall objective was to develop\nideas to facilitate convergence research on this critical topic and encourage\ncollaboration among researchers across disciplines. Based on the CCC community\nwhite paper entitled Computing Research for the Climate Crisis, we initially\nfocused on five impact areas (i.e. application domains that are both important\nto society and critically affected by climate change): Energy, Agriculture,\nEnvironmental Justice, Transportation, and Physical Infrastructure.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.06176,regular,post_llm,2023,1,"{'ai_likelihood': 6.241930855645073e-05, 'text': '#EndSARS Protest: Discourse and Mobilisation on Twitter\n\n  Using the @NGRPresident Twitter handle, the Government of Nigeria issued a\nspecial directive banning Special Anti-Robbery Squad (SARS) with immediate\neffect. The SARS is a special police unit under the Nigeria Police Force tasked\nwith the responsibility of fighting violent crimes. However, the unit has been\naccused of waves of human rights abuse across the nation. According to a report\nby Amnesty International, between January 2017 and May 2020, 82 cases of police\nbrutality have been committed. This has led to one of the major protests\ndemanding more measures to be taken. The #EndSARS hashtag was widely used by\nthe protesters to amplify their messages and reach out to wider communities on\nTwitter. In this study, we present a critical analysis of how the online\nprotest unfolded. Essentially, we examine how the protest evolves on Twitter,\nthe nature of engagement with the protest themes, the factors influencing the\nprotest and public perceptions about the online movement. We found that the\nmobilisation strategies include direct and indirect engagements with\ninfluential users, sharing direct stories and vicarious experiences. Also,\nthere is evidence that suggests the deployment of automated accounts to promote\nthe course of the protest. In terms of participation, over 70% of the protest\nis confined within a few states in Nigeria, and the diaspora communities also\nlent their voices to the movement. The most active users are not those with\nhigh followership, and the majority of the protesters utilised mobile devices,\naccounting for 88% to mobilise and report on the protest. We also examined how\nsocial media users interact with the movement and the response from the wider\nonline communities. Needless to say, the themes in the online discourse are\nmostly about #EndSARS and vicarious experiences with the police, however, there\nare topics around police reform and demand for regime change.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.02532,regular,post_llm,2023,1,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Better Balance in Informatics: An Honest Discussion with Students\n\n  In recent years, there has been considerable effort to promote gender balance\nin the academic environment of Computer Science (CS). However, there is still a\ngender gap at all CS academic levels: from students, to PhD candidates, to\nfaculty members. This general trend is followed by the Department of Computer\nScience at UiT The Arctic University of Norway. To combat this trend within the\nCS environment at UiT, we embarked on structured discussions with students of\nour department. After analyzing the data collected from these discussions, we\nwere able to identify action items that could mitigate the existing gender gap\nat our department. In particular, these discussions elucidated ways to achieve\n(i) a balanced flow of students into CS undergraduate program, (ii) a balanced\nCS study environment, and (iii) a balanced flow of graduates into higher levels\nof the CS academia (e.g., PhD program). This paper presents the results of the\ndiscussions and the subsequent recommendations that we made to the\nadministration of the department. We also provide a road-map that other\ninstitutions could follow to organize similar events as part of their\ngender-balance action plan.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.03489,regular,post_llm,2023,1,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Unveiling and Mitigating Bias in Ride-Hailing Pricing for Equitable\n  Policy Making\n\n  Ride-hailing services have skyrocketed in popularity due to the convenience\nthey offer, but recent research has shown that their pricing strategies can\nhave a disparate impact on some riders, such as those living in disadvantaged\nneighborhoods with a greater share of residents of color or residents below the\npoverty line. Since these communities tend to be more dependent on ride-hailing\nservices due to lack of adequate public transportation, it is imperative to\naddress this inequity. To this end, this paper presents the first thorough\nstudy on fair pricing for ride-hailing services by devising applicable fairness\nmeasures and corresponding fair pricing mechanisms. By providing discounts that\nmay be subsidized by the government, our approach results in an increased\nnumber and more affordable rides for the disadvantaged community. Experiments\non real-world Chicago taxi data confirm our theoretical findings which provide\na basis for the government to establish fair ride-hailing policies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.01908,regular,post_llm,2023,1,"{'ai_likelihood': 6.788306766086155e-06, 'text': 'Virtual Reality Photo-based Tours for Teaching Filipino Vocabulary in an\n  Online Class in Japan: Transitioning into the New Normal\n\n  When educational institutions worldwide scrambled for ways to continue their\nclasses during lockdowns caused by the COVID-19 pandemic, the use of\ninformation and communication technology (ICT) for remote teaching has become\nwidely considered to be a potential solution. As universities raced to\nimplement emergency remote teaching (ERT) strategies in Japan, some have\nexplored innovative interventions other than webinar platforms and learning\nmanagement systems to bridge the gap caused by restricted mobility among\nteachers and learners. One such innovation is virtual reality (VR). VR has been\nchanging the landscape of higher education because of its ability to ""teleport""\nlearners to various places by simulating real-world environments in the virtual\nworld. Some teachers, including the authors of this paper, explored integrating\nVR into their activities to address issues caused by geographical limitations\nbrought about by the heightened restrictions in 2020. Results were largely\nencouraging. However, rules started relaxing in the succeeding years as more\npeople got vaccinated. Thus, some fully online classes in Japan shifted to\nblended learning as they moved toward fully returning to in-person classes\nprompting educators to modify how they implemented their VR-based\ninterventions. This paper describes how a class of university students in Japan\nwho were taking a Filipino language course experienced a VR-based intervention\nin blended mode, which was originally prototyped during the peak of the ERT\nera. Moreover, adjustments and comparisons regarding methodological\nidiosyncrasies and findings between the fully online iteration and the recently\nimplemented blended one are reported in detail.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.02611,regular,post_llm,2023,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'Identifying Different Student Clusters in Functional Programming\n  Assignments: From Quick Learners to Struggling Students\n\n  Instructors and students alike are often focused on the grade in programming\nassignments as a key measure of how well a student is mastering the material\nand whether a student is struggling. This can be, however, misleading.\nEspecially when students have access to auto-graders, their grades may be\nheavily skewed. In this paper, we analyze student assignment submission data\ncollected from a functional programming course taught at McGill university\nincorporating a wide range of features. In addition to the grade, we consider\nactivity time data, time spent, and the number of static errors. This allows us\nto identify four clusters of students: ""Quick-learning"", ""Hardworking"",\n""Satisficing"", and ""Struggling"" through cluster algorithms. We then analyze how\nwork habits, working duration, the range of errors, and the ability to fix\nerrors impact different clusters of students. This structured analysis provides\nvaluable insights for instructors to actively help different types of students\nand emphasize different aspects of their overall course design. It also\nprovides insights for students themselves to understand which aspects they\nstill struggle with and allows them to seek clarification and adjust their work\nhabits.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.13043,review,post_llm,2023,1,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Introducing Practicable Learning Analytics\n\n  Learning analytics have been argued as a key enabler to improving student\nlearning at scale. Yet, despite considerable efforts by the learning analytics\ncommunity across the world over the past decade, the evidence to support that\nclaim is hitherto scarce, as is the demand from educators to adopt it into\ntheir practice. We introduce the concept of practicable learning analytics to\nilluminate what learning analytics may look like from the perspective of\npractice, and how this practice can be incorporated in learning analytics\ndesigns so as to make them more attractive for practitioners. As a framework\nfor systematic analysis of the practice in which learning analytics tools and\nmethods are to be employed, we use the concept of Information Systems Artifact\n(ISA) which comprises three interrelated subsystems: the informational, the\nsocial and the technological artefacts. The ISA approach entails systemic\nthinking which is necessary for discussing data-driven decision making in the\ncontext of educational systems, practices, and situations. The ten chapters in\nthis book are presented and reflected upon from the ISA perspective, clarifying\nthat detailed attention to the social artefact is critical to the design of\npracticable learning analytics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10984,review,post_llm,2023,1,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Design aesthetics recommender system based on customer profile and\n  wanted affect\n\n  Product recommendation systems have been instrumental in online commerce\nsince the early days. Their development is expanded further with the help of\nbig data and advanced deep learning methods, where consumer profiling is\ncentral. The interest of the consumer can now be predicted based on the\npersonal past choices and the choices of similar consumers. However, what is\ncurrently defined as a choice is based on quantifiable data, like product\nfeatures, cost, and type. This paper investigates the possibility of profiling\ncustomers based on the preferred product design and wanted affects. We\nconsidered the case of vase design, where we study individual Kansei of each\ndesign. The personal aspects of the consumer considered in this study were\ndecided based on our literature review conclusions on the consumer response to\nproduct design. We build a representative consumer model that constitutes the\nrecommendation system's core using deep learning. It asks the new consumers to\nprovide what affect they are looking for, through Kansei adjectives, and\nrecommend; as a result, the aesthetic design that will most likely cause that\naffect.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.0176,review,post_llm,2023,1,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'Using Science Education Gateways to improve undergraduate STEM\n  education: The QUBES Platform as a case study\n\n  The QUBES platform was conceived as a ""science education gateway"" and\ndesigned to accelerate innovation in undergraduate STEM education. The\ntechnical infrastructure was purpose built to provide more equitable access to\nprofessional resources, support learning that reflects authentic science, and\npromote open education practices. Four platform services (OER Library Access;\nProfessional Learning; Partner Support; and Customizable Workspaces) support\noverlapping faculty user communities, provide multiple points of entry, and\nenable manifold use case scenarios. The integrated nature of the platform makes\nit possible to collect, curate, and disseminate a diverse array of reform\nresources in a scalable and sustainable manner. We believe that the QUBES\nplatform has the capacity to broaden participation in scholarship around\nteaching and learning and, furthermore, that it can help to lower faculty\nbarriers to the adoption of reform practices. The role of cyberinfrastructure\nin undergraduate STEM education is generally underappreciated and warrants\nfurther exploration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.10025,review,post_llm,2023,1,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Teaching Computer Science Students to Communicate Scientific Findings\n  More Effectively\n\n  Science communication forms the bridge between computer science researchers\nand their target audience. Researchers who can effectively draw attention to\ntheir research findings and communicate them comprehensibly not only help their\ntarget audience to actually learn something, but also benefit themselves from\nthe increased visibility of their work and person. However, the necessary\nskills for good science communication must also be taught, and this has so far\nbeen neglected in the field of software engineering education.\n  We therefore designed and implemented a science communication seminar for\nbachelor students of computer science curricula. Students take the position of\na researcher who, shortly after publication, is faced with having to draw\nattention to the paper and effectively communicate the contents of the paper to\none or more target audiences. Based on this scenario, each student develops a\ncommunication strategy for an already published software engineering research\npaper and tests the resulting ideas with the other seminar participants.\n  We explain our design decisions for the seminar, and combine our experiences\nwith responses to a participant survey into lessons learned. With this\nexperience report, we intend to motivate and enable other lecturers to offer a\nsimilar seminar at their university. Collectively, university lecturers can\nprepare the next generation of computer science researchers to not only be\nexperts in their field, but also to communicate research findings more\neffectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.08488,review,post_llm,2023,1,"{'ai_likelihood': 3.837876849704319e-05, 'text': ""Towards Openness Beyond Open Access: User Journeys through 3 Open AI\n  Collaboratives\n\n  Open Artificial Intelligence (Open source AI) collaboratives offer\nalternative pathways for how AI can be developed beyond well-resourced\ntechnology companies and who can be a part of the process. To understand how\nand why they work and what additionality they bring to the landscape, we focus\non three such communities, each focused on a different kind of activity around\nAI: building models (BigScience workshop), tools and ways of working (The\nTuring Way), and ecosystems (Mozilla Festival's Building Trustworthy AI Working\nGroup). First, we document the community structures that facilitate these\ndistributed, volunteer-led teams, comparing the collaboration styles that drive\neach group towards their specific goals. Through interviews with community\nleaders, we map user journeys for how members discover, join, contribute, and\nparticipate. Ultimately, this paper aims to highlight the diversity of AI work\nand workers that have come forth through these collaborations and how they\noffer a broader practice of openness to the AI space.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.04246,review,post_llm,2023,1,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Generative Language Models and Automated Influence Operations: Emerging\n  Threats and Potential Mitigations\n\n  Generative language models have improved drastically, and can now produce\nrealistic text outputs that are difficult to distinguish from human-written\ncontent. For malicious actors, these language models bring the promise of\nautomating the creation of convincing and misleading text for use in influence\noperations. This report assesses how language models might change influence\noperations in the future, and what steps can be taken to mitigate this threat.\nWe lay out possible changes to the actors, behaviors, and content of online\ninfluence operations, and provide a framework for stages of the language\nmodel-to-influence operations pipeline that mitigations could target (model\nconstruction, model access, content dissemination, and belief formation). While\nno reasonable mitigation can be expected to fully prevent the threat of\nAI-enabled influence operations, a combination of multiple mitigations may make\nan important difference.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.13045,regular,post_llm,2023,1,"{'ai_likelihood': 5.198849572075738e-06, 'text': ""CS-JEDI: Required DEI Education, by CS PhD Students, for CS PhD Students\n\n  Computer science (CS) has historically struggled with issues related to\ndiversity, equity, and inclusion (DEI). Based on how these issues were\naffecting PhD students in our department, we identified required DEI education\nfor PhD students as a potentially high-impact approach to improving the PhD\nstudent experience in our program. Given that no existing curriculum met the\ndesired criteria, we (PhD students) - along with many others at our school -\ndeveloped and implemented CS-JEDI: Justice, Equity, Diversity, and Inclusion in\nComputer Science. CS-JEDI is a 6-week DEI curriculum that is now taken by all\nfirst-year PhD students in our department. This paper covers CS-JEDI's\nmotivation and goals; describes how its evidence-based curriculum is tailored\nto these goals and to the CS PhD context; and gives a data-driven evaluation of\nthe extent to which CS-JEDI's first offering, in Spring 2022, achieved these\ngoals.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.01894,review,post_llm,2023,1,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Urban-Semantic Computer Vision: A Framework for Contextual Understanding\n  of People in Urban Spaces\n\n  Increasing computational power and improving deep learning methods have made\ncomputer vision technologies pervasively common in urban environments. Their\napplications in policing, traffic management, and documenting public spaces are\nincreasingly common. Despite the often-discussed biases in the algorithms'\ntraining and unequally borne benefits, almost all applications similarly reduce\nurban experiences to simplistic, reductive, and mechanistic measures. There is\na lack of context, depth, and specificity in these practices that enables\nsemantic knowledge or analysis within urban contexts, especially within the\ncontext of using and occupying urban space. This paper will critique existing\nuses of artificial intelligence and computer vision in urban practices to\npropose a new framework for understanding people, action, and public space.\n  This paper revisits Geertz's use of thick descriptions in generating\ninterpretive theories of culture and activity and uses this lens to establish a\nframework to evaluate the varied uses of computer vision technologies that\nweigh meaning. We discuss how the framework's positioning may differ (and\nconflict) between different users of the technology. This paper also discusses\nthe current use and training of deep learning algorithms and how this process\nlimits semantic learning and proposes three potential methodologies for gaining\na more contextually specific, urban-semantic, description of urban space\nrelevant to urbanists.\n  This paper contributes to the critical conversations regarding the\nproliferation of artificial intelligence by challenging the current\napplications of these technologies in the urban environment by highlighting\ntheir failures within this context while also proposing an evolution of these\nalgorithms that may ultimately make them sensitive and useful within this\nspatial and cultural milieu.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.12347,review,post_llm,2023,1,"{'ai_likelihood': 2.5828679402669272e-06, 'text': ""Academic Institutions in Multilateral Data Governance: Emerging\n  Arrangements for Negotiating Risk, Value and Ethics in the Big Data Economy\n\n  Data sharing partnerships are increasingly an imperative for research\ninstitutions and, at the same time, a challenge for established models of data\ngovernance and ethical research oversight. We analyse four cases of data\npartnership involving academic institutions and examine the role afforded to\nthe research partner in negotiating the relationship between risk, value, trust\nand ethics. Within this terrain, far from being a restraint on\nfinancialisation, the instrumentation of ethics forms part of the wider\nmobilisation of infrastructure for the realisation of profit in the big data\neconomy. Under what we term `combinatorial data governance' academic structures\nfor the management of research ethics are instrumentalised as organisational\nfunctions that serve to mitigate reputational damage and societal distrust. In\nthe alternative model of `experimental data governance' researchers propose\nframeworks and instruments for the rethinking of data ethics and the risks\nassociated with it - a model that is promising but limited in its practical\napplication.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2301.09606,regular,post_llm,2023,1,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Concept of Delivery System in the Smart City Environment\n\n  Regarding to the smart city infrastructures, there is a demand for big data\nprocessing and its further usage. This data can be gained by various means.\nThere are many IoT devices in the city, which can communicate and share the\ninformation about the environment which they are situated in. Moreover every\npersonal mobile device can also participate in this process and help to gain\ndata via various applications. Every app provides the useful data, enabling the\nlocation and data sharing. This data can be further processed and used for\nimproving the city infrastructure, transport or other services. We designed the\nsystem for shared delivery process, which can help to achieve the described\nsituation. It consists of frontend and backend part. The frontend part,\nmultiplatform mobile app, represents the graphical interface and the backend\npart represents the database for the gained data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.06917,regular,post_llm,2023,2,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'On Detecting Policy-Related Political Ads: An Exploratory Analysis of\n  Meta Ads in 2022 French Election\n\n  Online political advertising has become the cornerstone of political\ncampaigns. The budget spent solely on political advertising in the U.S. has\nincreased by more than 100% from \\$700 million during the 2017-2018 U.S.\nelection cycle to \\$1.6 billion during the 2020 U.S. presidential elections.\nNaturally, the capacity offered by online platforms to micro-target ads with\npolitical content has been worrying lawmakers, journalists, and online\nplatforms, especially after the 2016 U.S. presidential election, where\nCambridge Analytica has targeted voters with political ads congruent with their\npersonality\n  To curb such risks, both online platforms and regulators (through the DSA act\nproposed by the European Commission) have agreed that researchers, journalists,\nand civil society need to be able to scrutinize the political ads running on\nlarge online platforms. Consequently, online platforms such as Meta and Google\nhave implemented Ad Libraries that contain information about all political ads\nrunning on their platforms. This is the first step on a long path. Due to the\nvolume of available data, it is impossible to go through these ads manually,\nand we now need automated methods and tools to assist in the scrutiny of\npolitical ads.\n  In this paper, we focus on political ads that are related to policy.\nUnderstanding which policies politicians or organizations promote and to whom\nis essential in determining dishonest representations. This paper proposes\nautomated methods based on pre-trained models to classify ads in 14 main policy\ngroups identified by the Comparative Agenda Project (CAP). We discuss several\ninherent challenges that arise. Finally, we analyze policy-related ads featured\non Meta platforms during the 2022 French presidential elections period.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.07836,regular,post_llm,2023,2,"{'ai_likelihood': 8.808241950141059e-06, 'text': ""Assessing enactment of content regulation policies: A post hoc\n  crowd-sourced audit of election misinformation on YouTube\n\n  With the 2022 US midterm elections approaching, conspiratorial claims about\nthe 2020 presidential elections continue to threaten users' trust in the\nelectoral process. To regulate election misinformation, YouTube introduced\npolicies to remove such content from its searches and recommendations. In this\npaper, we conduct a 9-day crowd-sourced audit on YouTube to assess the extent\nof enactment of such policies. We recruited 99 users who installed a browser\nextension that enabled us to collect up-next recommendation trails and search\nresults for 45 videos and 88 search queries about the 2020 elections. We find\nthat YouTube's search results, irrespective of search query bias, contain more\nvideos that oppose rather than support election misinformation. However,\nwatching misinformative election videos still lead users to a small number of\nmisinformative videos in the up-next trails. Our results imply that while\nYouTube largely seems successful in regulating election misinformation, there\nis still room for improvement.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.08301,review,post_llm,2023,2,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'The right to audit and power asymmetries in algorithm auditing\n\n  In this paper, we engage with and expand on the keynote talk about the Right\nto Audit given by Prof. Christian Sandvig at the IC2S2 2021 through a critical\nreflection on power asymmetries in the algorithm auditing field. We elaborate\non the challenges and asymmetries mentioned by Sandvig - such as those related\nto legal issues and the disparity between early-career and senior researchers.\nWe also contribute a discussion of the asymmetries that were not covered by\nSandvig but that we find critically important: those related to other\ndisparities between researchers, incentive structures related to the access to\ndata from companies, targets of auditing and users and their rights. We also\ndiscuss the implications these asymmetries have for algorithm auditing research\nsuch as the Western-centrism and the lack of the diversity of perspectives.\nWhile we focus on the field of algorithm auditing specifically, we suggest some\nof the discussed asymmetries affect Computational Social Science more generally\nand need to be reflected on and addressed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.14404,review,post_llm,2023,2,"{'ai_likelihood': 1.2848112318250869e-05, 'text': 'The Ethics of Social Media Analytics in Migration Studies\n\n  The prevalence of social media platforms and their use across the globe makes\nthem attractive options for studying large groups of people, particularly when\nsome of these platforms provide access to large amounts of structured data.\nHowever, with the collection, storage, and use of this data comes ethical and\nlegal responsibilities, which are particularly important when looking at social\ngroups such as migrants, who are often stigmatised and criminalised. Various\nguidelines, frameworks and laws have been developed to ensure social media data\nis used in the most ethical way. However, they have quickly evolved within the\npast few years and are scattered across various fields and domains. To help\nresearchers navigate these issues, this chapter provides an overview of the\nethical considerations of studying migration via social media platforms.\nBuilding on relevant academic literature, as well as national and supranational\nframeworks and legislations, we review how the main ethical issues related to\nsocial media research have been discussed in the past twenty years and outline\ngood practice examples to mitigate them. This overview is designed to provide\nresearchers with theoretical and practical tools to consider and mitigate the\nethical challenges related to social media research in migration-related\ncontexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.06862,regular,post_llm,2023,2,"{'ai_likelihood': 1.6159481472439238e-05, 'text': 'Graph-based Village Level Poverty Identification\n\n  Poverty status identification is the first obstacle to eradicating poverty.\nVillage-level poverty identification is very challenging due to the arduous\nfield investigation and insufficient information. The development of the Web\ninfrastructure and its modeling tools provides fresh approaches to identifying\npoor villages. Upon those techniques, we build a village graph for village\npoverty status identification. By modeling the village connections as a graph\nthrough the geographic distance, we show the correlation between village\npoverty status and its graph topological position and identify two key factors\n(Centrality, Homophily Decaying effect) for identifying villages. We further\npropose the first graph-based method to identify poor villages. It includes a\nglobal Centrality2Vec module to embed village centrality into the dense vector\nand a local graph distance convolution module that captures the decaying\neffect. In this paper, we make the first attempt to interpret and identify\nvillage-level poverty from a graph perspective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.07873,review,post_llm,2023,2,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""Separating Technological and Clinical Safety Assurance for Medical\n  Devices\n\n  The safety and clinical effectiveness of medical devices are closely\nassociated with their specific use in clinical treatments. Assuring safety and\nthe desired clinical effectiveness is challenging. Different people may react\ndifferently to the same treatment due to variability in their physiology and\ngenetics. Thus, we need to consider the outputs and behaviour of the device\nitself as well as the effect of using the device to treat a wide variety of\npatients. High-intensity focused ultrasound systems and radiation therapy\nmachines are examples of systems in which this is a primary concern.\nConventional monolithic assurance cases are complex, and this complexity\naffects our ability to address these concerns adequately. Based on the\nprinciple of separation of concerns, we propose separating the assurance of the\nuse of these types of systems in clinical treatments into two linked assurance\ncases. The first assurance case demonstrates the safety of the manufacturer's\ndevice independent of the clinical treatment. The second demonstrates the\nsafety and clinical effectiveness of the device when it is used in a specific\nclinical treatment. We introduce the idea of these separate assurance cases,\nand describe briefly how they are separated and linked.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.09944,review,post_llm,2023,2,"{'ai_likelihood': 8.410877651638455e-06, 'text': 'Reflections on the Data Governance Act\n\n  The European Union (EU) has been pursuing a new strategy under the umbrella\nlabel of digital sovereignty. Data is an important element in this strategy. To\nthis end, a specific Data Governance Act was enacted in 2022. This new\nregulation builds upon two ideas: reuse of data held by public sector bodies\nand voluntary sharing of data under the label of data altruism. This short\ncommentary reviews the main content of the new regulation. Based on the review,\na few points are also raised about potential challenges.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.02274,review,post_llm,2023,2,"{'ai_likelihood': 4.337893591986763e-06, 'text': 'Towards a Contemporary Definition of Cybersecurity\n\n  The report provides an intricate analysis of cyber security defined in\ncontemporary operational digital environments. An extensive literature review\nis formed to determine how the construct is reviewed in modern scholarly\ncontexts. The article seeks to offer a comprehensive definition of the term\n""cybersecurity"" to accentuate its multidisciplinary perspectives. A meaningful\nconcise, and inclusive dimension will be provided to assist in designing\nscholarly discourse on the subject. The report will offer a unified framework\nfor examining activities that constitute the concept resulting in a new\ndefinition; ""Cybersecurity is the collection and concerting of resources\nincluding personnel and infrastructure, structures, and processes to protect\nnetworks and cyber-enabled computer systems from events that compromise the\nintegrity and interfere with property rights, resulting in some extent of the\nloss."" The encapsulation of the interdisciplinary domains will be critical in\nimproving understanding and response to emerging challenges in cyberspace.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.07081,review,post_llm,2023,2,"{'ai_likelihood': 1.0, 'text': 'Integrating Artificial Intelligence and Humanities in Healthcare\n\n  Artificial Intelligence (AI) and Medical Humanities have become two of the\nmost crucial and rapidly growing fields in the current world. AI has made\nsubstantial advancements in recent years, enabling the development of\nalgorithms and systems that can perform tasks traditionally done by humans.\nMedical Humanities, on the other hand, is the intersection of medical sciences,\nhumanities, and the social sciences, and deals with the cultural, historical,\nphilosophical, ethical, and social aspects of health, illness, and medicine.\nThe integration of AI and Medical Humanities can offer innovative solutions to\nsome of the pressing issues in the medical field.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.876953125, 'GPT4': 0.046600341796875, 'CLAUDE': 0.0008697509765625, 'GOOGLE': 0.0704345703125, 'OPENAI_O_SERIES': 0.0004546642303466797, 'DEEPSEEK': 4.0531158447265625e-06, 'GROK': 1.4185905456542969e-05, 'NOVA': 2.0563602447509766e-05, 'OTHER': 0.0044708251953125, 'HUMAN': 0.00017535686492919922}}"
2302.08946,review,post_llm,2023,2,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Personal autonomy and surveillance capitalism: possible future\n  developments\n\n  The rise of social media and the increase in the computational capabilities\nof computers have allowed tech companies such as Facebook and Google to gather\nincredibly large amounts of data and to be able to extract meaningful\ninformation to use for commercial purposes. Moreover, the algorithms behind\nthese platforms have shown the ability to influence feelings, behaviors, and\nopinions, representing a serious threat to the independence of their users. All\nof these practices have been referred to as ""surveillance capitalism"", a term\ncreated by Shoshana Zuboff. In this paper I focus on the threat imposed on the\nautonomy of human beings in the context of surveillance capitalism, providing\nboth an analysis of the reasons why this threat exists and what consequences we\ncould face if we take no action against such practices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.11225,regular,post_llm,2023,2,"{'ai_likelihood': 2.1523899502224394e-06, 'text': ""The Amplification Paradox in Recommender Systems\n\n  Automated audits of recommender systems found that blindly following\nrecommendations leads users to increasingly partisan, conspiratorial, or false\ncontent. At the same time, studies using real user traces suggest that\nrecommender systems are not the primary driver of attention toward extreme\ncontent; on the contrary, such content is mostly reached through other means,\ne.g., other websites. In this paper, we explain the following apparent paradox:\nif the recommendation algorithm favors extreme content, why is it not driving\nits consumption? With a simple agent-based model where users attribute\ndifferent utilities to items in the recommender system, we show through\nsimulations that the collaborative-filtering nature of recommender systems and\nthe nicheness of extreme content can resolve the apparent paradox: although\nblindly following recommendations would indeed lead users to niche content,\nusers rarely consume niche content when given the option because it is of low\nutility to them, which can lead the recommender system to deamplify such\ncontent. Our results call for a nuanced interpretation of ``algorithmic\namplification'' and highlight the importance of modeling the utility of content\nto users when auditing recommender systems. Code available:\nhttps://github.com/epfl-dlab/amplification_paradox.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.07787,review,post_llm,2023,2,"{'ai_likelihood': 2.4172994825575088e-06, 'text': 'Fairness in Socio-technical Systems: a Case Study of Wikipedia\n\nProblems broadly known as algorithmic bias frequently occur in the context of complex socio-technical systems (STS), where observed biases may not be directly attributable to a single automated decision algorithm. As a first investigation of fairness in STS, we focus on the case of Wikipedia. We systematically review 75 papers describing different types of bias in Wikipedia, which we classify and relate to established notions of harm from algorithmic fairness research. By analysing causal relationships between the observed phenomena, we demonstrate the complexity of the socio-technical processes causing harm. Finally, we identify the normative expectations of fairness associated with the different problems and discuss the applicability of existing criteria proposed for machine learning-driven decision systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.09939,review,post_llm,2023,2,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""A Text Mining Analysis of Data Protection Politics: The Case of Plenary\n  Sessions of the European Parliament\n\n  Data protection laws and policies have been studied extensively in recent\nyears, but little is known about the parliamentary politics of data protection.\nThis imitation applies even to the European Union (EU) that has taken the\nglobal lead in data protection and privacy regulation. For patching this\nnotable gap in existing research, this paper explores the data protection\nquestions raised by the Members of the European Parliament (MEPs) in the\nParliament's plenary sessions and the answers given to these by the European\nCommission. Over a thousand of such questions and answers are covered in a\nperiod from 1995 to early 2023. Given computational analysis based on text\nmining, the results indicate that (a) data protection has been actively debated\nin the Parliament during the past twenty years. No noticeable longitudinal\ntrends are present; the debates have been relatively constant. As could be\nexpected, (b) the specific data protection laws in the EU have frequently been\nreferenced in these debates, which (c) do not seem to align along conventional\npolitical dimensions such as the left-right axis. Furthermore, (d) numerous\ndistinct data protection topics have been debated by the parliamentarians,\nindicating that data protection politics in the EU go well-beyond the recently\nenacted regulations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.07074,review,post_llm,2023,2,"{'ai_likelihood': 2.3047129313151043e-05, 'text': 'Good practices for clinical data warehouse implementation: a case study\n  in France\n\n  Real World Data (RWD) bears great promises to improve the quality of care.\nHowever, specific infrastructures and methodologies are required to derive\nrobust knowledge and brings innovations to the patient. Drawing upon the\nnational case study of the 32 French regional and university hospitals\ngovernance, we highlight key aspects of modern Clinical Data Warehouses (CDWs):\ngovernance, transparency, types of data, data reuse, technical tools,\ndocumentation and data quality control processes. Semi-structured interviews as\nwell as a review of reported studies on French CDWs were conducted in a\nsemi-structured manner from March to November 2022. Out of 32 regional and\nuniversity hospitals in France, 14 have a CDW in production, 5 are\nexperimenting, 5 have a prospective CDW project, 8 did not have any CDW project\nat the time of writing. The implementation of CDW in France dates from 2011 and\naccelerated in the late 2020. From this case study, we draw some general\nguidelines for CDWs. The actual orientation of CDWs towards research requires\nefforts in governance stabilization, standardization of data schema and\ndevelopment in data quality and data documentation. Particular attention must\nbe paid to the sustainability of the warehouse teams and to the multi-level\ngovernance. The transparency of the studies and the tools of transformation of\nthe data must improve to allow successful multi-centric data reuses as well as\ninnovations in routine care.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.14661,review,post_llm,2023,2,"{'ai_likelihood': 5.198849572075738e-06, 'text': ""Explainability as a Requirement for Hardware: Introducing Explainable\n  Hardware (XHW)\n\n  In today's age of digital technology, ethical concerns regarding computing\nsystems are increasing. While the focus of such concerns currently is on\nrequirements for software, this article spotlights the hardware domain,\nspecifically microchips. For example, the opaqueness of modern microchips\nraises security issues, as malicious actors can manipulate them, jeopardizing\nsystem integrity. As a consequence, governments invest substantially to\nfacilitate a secure microchip supply chain. To combat the opaqueness of\nhardware, this article introduces the concept of Explainable Hardware (XHW).\nInspired by and building on previous work on Explainable AI (XAI) and\nexplainable software systems, we develop a framework for achieving XHW\ncomprising relevant stakeholders, requirements they might have concerning\nhardware, and possible explainability approaches to meet these requirements.\nThrough an exploratory survey among 18 hardware experts, we showcase\napplications of the framework and discover potential research gaps. Our work\nlays the foundation for future work and structured debates on XHW.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.09877,regular,post_llm,2023,2,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Economic Dynamics of Agents\n\n  Post-pandemic world has thrown up several challenges, such as, high\ninflation, low growth, high debt, collapse of economies, political instability,\njob losses, lowering of income in addition to damages caused natural disasters,\nmore convincing attributed to climate change, apart from existing inequalities.\nEfforts are being made to mitigate these challenges at various levels. To the\nbest of the knowledge of the author, most of the prior researches have focussed\non specific scenarios, use cases, inter-relationships between couple of sectors\nand more so on optimal policies, such as, impact of carbon tax on individuals,\ninteraction between taxes and welfare, etc. However, not much effort have been\nmade to understand the actual impact on individual agents due to diverse policy\nchanges and how agents cope with changing economic dynamics. This paper\nconsiders progressive deteriorating conditions of increase in expense,\ndegrading environmental utility, increase in taxation, decrease in welfare and\nlowering of income with recourse to inherited properties, credits and return on\ninvestments, and tries to understand how the agents cope with the changing\nsituations using an agent based model with matrices related to savings,\ncredits, assets. Results indicate that collapse of agents' economic conditions\ncan be quite fast, sudden and drastic for all income groups in most cases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.11894,review,post_llm,2023,2,"{'ai_likelihood': 1.7881393432617188e-06, 'text': ""Towards a conceptual model for the FAIR Digital Object Framework\n\n  The FAIR principles define a number of expected behaviours for the data and\nservices ecosystem with the goal of improving the findability, accessibility,\ninteroperability, and reusability of digital objects. A key aspiration of the\nprinciples is that they would lead to a scenario where autonomous computational\nagents are capable of performing a ``self-guided exploration of the global data\necosystem,'' and act properly with the encountered variety of types, formats,\naccess mechanisms and protocols. The lack of support for some of these expected\nbehaviours by current information infrastructures such as the internet and the\nWorld Wide Web motivated the emergence, in the last years, of initiatives such\nas the FAIR Digital Objects (FDOs) movement. This movement aims at an\ninfrastructure where digital objects can be exposed and explored according to\nthe FAIR principles. In this paper, we report the current status of the work\ntowards an ontology-driven conceptual model for FAIR Digital Objects. The\nconceptual model covers aspects of digital objects that are relevant to the\nFAIR principles such as the distinction between metadata and the digital object\nit describes, the classification of digital objects in terms of both their\ninformational value and their computational representation format, and the\nrelation between different types of FAIR Digital Objects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.06304,regular,post_llm,2023,2,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Programming Skills are Not Enough: a Greedy Strategy to Attract More\n  Girls to Study Computer Science\n\n  It has been observed in many studies that female students in general are\nunwilling to undertake a course of study in ICT. Recent literature has also\npointed out that undermining the prejudices of girls with respect to these\ndisciplines is very difficult in adolescence, suggesting that, to be effective,\nawareness programs on computer disciplines should be offered in pre-school or\nlower school age. On the other hand, even assuming that large-scale computer\nliteracy programs can be immediately activated in lower schools and\nkindergartens, we can't wait for >15-20 years before we can appreciate the\neffectiveness of these programs. The scarcity of women in ICT has a tangible\nnegative impact on countries' technological innovation, which requires\nimmediate action.\n  In this paper, we describe a strategy, and the details of a number of\nprograms coordinated by the Engineering and Computer Science Departments at\nSapienza University, to make high school girl students aware of the importance\nof new technologies and ICT. In addition to describing the theoretical\napproach, the paper offers some project examples.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.09072,regular,post_llm,2023,2,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'An Open Dataset of Sensor Data from Soil Sensors and Weather Stations at\n  Production Farms\n\n  Weather and soil conditions are particularly important when it comes to\nfarming activities. Study of these factors and their role in nutrient and\nnitrate absorption rates can lead to useful insights with benefits for both the\ncrop yield and the protection of the environment through the more controlled\nuse of fertilizers and chemicals. There is a paucity of public data from rural,\nagricultural sensor networks. This is partly due to the unique challenges faced\nduring the deployment and maintenance of IoT networks in rural agricultural\nareas. As part of a 5-year project called WHIN we have been deploying and\ncollecting sensor data from production and experimental agricultural farms in\nand around Purdue University in Indiana. Here we release a dataset comprising\nsoil sensor data from a representative sample of 3 nodes across 3 production\nfarms, each for 5 months. We correlate this data with the weather data and draw\nsome insights about the absorption of rain in the soil. We provide the dataset\nat: https://purduewhin.ecn.purdue.edu/dataset2021.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.06655,review,post_llm,2023,2,"{'ai_likelihood': 4.172325134277344e-06, 'text': 'On the importance of AI research beyond disciplines\n\n  As the impact of AI on various scientific fields is increasing, it is crucial\nto embrace interdisciplinary knowledge to understand the impact of technology\non society. The goal is to foster a research environment beyond disciplines\nthat values diversity and creates, critiques and develops new conceptual and\ntheoretical frameworks. Even though research beyond disciplines is essential\nfor understanding complex societal issues and creating positive impact it is\nnotoriously difficult to evaluate and is often not recognized by current\nacademic career progression. The motivation for this paper is to engage in\nbroad discussion across disciplines and identify guiding principles fir AI\nresearch beyond disciplines in a structured and inclusive way, revealing new\nperspectives and contributing to societal and human wellbeing and\nsustainability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.0361,regular,post_llm,2023,2,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'Evaluating a Learned Admission-Prediction Model as a Replacement for\n  Standardized Tests in College Admissions\n\n  A growing number of college applications has presented an annual challenge\nfor college admissions in the United States. Admission offices have\nhistorically relied on standardized test scores to organize large applicant\npools into viable subsets for review. However, this approach may be subject to\nbias in test scores and selection bias in test-taking with recent trends toward\ntest-optional admission. We explore a machine learning-based approach to\nreplace the role of standardized tests in subset generation while taking into\naccount a wide range of factors extracted from student applications to support\na more holistic review. We evaluate the approach on data from an undergraduate\nadmission office at a selective US institution (13,248 applications). We find\nthat a prediction model trained on past admission data outperforms an SAT-based\nheuristic and matches the demographic composition of the last admitted class.\nWe discuss the risks and opportunities for how such a learned model could be\nleveraged to support human decision-making in college admissions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.016,review,post_llm,2023,2,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'MetaOpera: A Cross-Metaverse Interoperability Protocol\n\n  With the rapid evolution of metaverse technologies, numerous metaverse\napplications have arisen for various purposes and scenarios. This makes\ninteroperability across metaverses becomes one of the fundamental technology\nenablers in the metaverse space. The aim of interoperability is to provide a\nseamless experience for users to interact with metaverses. However, the\ndevelopment of cross-metaverse interoperability is still in its initial stage\nin both industry and academia. In this paper, we review the state-of-the-art\ncross-metaverse interoperability schemes. These schemes are designed for\nspecific interoperating scenarios and do not generalize for all types of\nmetaverses. To this end, we propose MetaOpera, a generalized cross-metaverse\ninteroperability protocol. By connecting to the MetaOpera, users, and objects\nin metaverses that rely on centralized servers or decentralized blockchains are\nable to interoperate with each other. We also develop a proof-of-concept\nimplementation for MetaOpera, evaluate its performance, and compare it with a\nstate-of-the-art cross-metaverse scheme based on Sidechains. Simulation results\ndemonstrate that the size of cross-metaverse proof and the average time of\ncross-metaverse transactions using the proposed solution are respectively about\neight times and three times smaller than the Sidechains scheme. This paper also\nsuggests a number of open issues and challenges faced by cross-metaverse\ninteroperability that may inspire future research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.00108,regular,post_llm,2023,2,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Ranked Choice Voting And Condorcet Failure in the Alaska 2022 Special\n  Election: How Might Other Voting Systems Compare?\n\n  The August 2022 special election for the U.S. House of Representatives in\nAlaska featured three main candidates and was conducted by the single-winner\nranked choice voting system known as ""Instant Runoff Voting."" The results of\nthis election displayed a well-known but relatively rare phenomenon known as\n""Condorcet failure:"" Nick Begich was eliminated in the first round despite\nbeing more broadly acceptable to the electorate than either of the other two\ncandidates. More specifically, Begich was the ""Condorcet winner"" of this\nelection: Based on the Cast Vote Record, he would have defeated each of the\nother two candidates in head-to-head contests, but he was eliminated in the\nfirst round of ballot counting due to receiving the fewest first-place votes.\n  The purpose of this paper is to use the data in the Cast Vote Record to\nexplore the range of likely outcomes if this election had been conducted under\ntwo alternative voting systems: Approval Voting and STAR (""Score Then Automatic\nRunoff"") Voting. We find that under the best assumptions available about voter\nbehavior, it is likely -- but not at all certain -- that Peltola would still\nhave won the election under Approval Voting, while Begich would almost\ncertainly have won under STAR Voting.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.03886,regular,post_llm,2023,2,"{'ai_likelihood': 9.238719940185547e-06, 'text': ""AI Usage Cards: Responsibly Reporting AI-generated Content\n\n  Given AI systems like ChatGPT can generate content that is indistinguishable\nfrom human-made work, the responsible use of this technology is a growing\nconcern. Although understanding the benefits and harms of using AI systems\nrequires more time, their rapid and indiscriminate adoption in practice is a\nreality. Currently, we lack a common framework and language to define and\nreport the responsible use of AI for content generation. Prior work proposed\nguidelines for using AI in specific scenarios (e.g., robotics or medicine)\nwhich are not transferable to conducting and reporting scientific research. Our\nwork makes two contributions: First, we propose a three-dimensional model\nconsisting of transparency, integrity, and accountability to define the\nresponsible use of AI. Second, we introduce ``AI Usage Cards'', a standardized\nway to report the use of AI in scientific research. Our model and cards allow\nusers to reflect on key principles of responsible AI usage. They also help the\nresearch community trace, compare, and question various forms of AI usage and\nsupport the development of accepted community norms. The proposed framework and\nreporting system aims to promote the ethical and responsible use of AI in\nscientific research and provide a standardized approach for reporting AI usage\nacross different research fields. We also provide a free service to easily\ngenerate AI Usage Cards for scientific work via a questionnaire and export them\nin various machine-readable formats for inclusion in different work products at\nhttps://ai-cards.org.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.10329,review,post_llm,2023,2,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'Harms from Increasingly Agentic Algorithmic Systems\n\n  Research in Fairness, Accountability, Transparency, and Ethics (FATE) has\nestablished many sources and forms of algorithmic harm, in domains as diverse\nas health care, finance, policing, and recommendations. Much work remains to be\ndone to mitigate the serious harms of these systems, particularly those\ndisproportionately affecting marginalized communities. Despite these ongoing\nharms, new systems are being developed and deployed which threaten the\nperpetuation of the same harms and the creation of novel ones. In response, the\nFATE community has emphasized the importance of anticipating harms. Our work\nfocuses on the anticipation of harms from increasingly agentic systems. Rather\nthan providing a definition of agency as a binary property, we identify 4 key\ncharacteristics which, particularly in combination, tend to increase the agency\nof a given algorithmic system: underspecification, directness of impact,\ngoal-directedness, and long-term planning. We also discuss important harms\nwhich arise from increasing agency -- notably, these include systemic and/or\nlong-range impacts, often on marginalized stakeholders. We emphasize that\nrecognizing agency of algorithmic systems does not absolve or shift the human\nresponsibility for algorithmic harms. Rather, we use the term agency to\nhighlight the increasingly evident fact that ML systems are not fully under\nhuman control. Our work explores increasingly agentic algorithmic systems in\nthree parts. First, we explain the notion of an increase in agency for\nalgorithmic systems in the context of diverse perspectives on agency across\ndisciplines. Second, we argue for the need to anticipate harms from\nincreasingly agentic systems. Third, we discuss important harms from\nincreasingly agentic systems and ways forward for addressing them. We conclude\nby reflecting on implications of our work for anticipating algorithmic harms\nfrom emerging systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.11565,review,post_llm,2023,2,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Responsible and Inclusive Technology Framework: A Formative Framework to\n  Promote Societal Considerations in Information Technology Contexts\n\n  Technology development practices in industry are often primarily focused on\nbusiness results, which risks creating unbalanced power relations between\ncorporate interests and the needs or concerns of people who are affected by\ntechnology implementation and use. These practices, and their associated\ncultural norms, may result in uses of technology that have direct, indirect,\nshort-term, and even long-term negative effects on groups of people and/or the\nenvironment. This paper contributes a formative framework -- the Responsible\nand Inclusive Technology Framework -- that orients critical reflection around\nthe social contexts of technology creation and use; the power dynamics between\nself, business, and societal stakeholders; the impacts of technology on various\ncommunities across past, present, and future dimensions; and the practical\ndecisions that imbue technological artifacts with cultural values. We expect\nthat the implementation of the Responsible and Inclusive Technology framework,\nespecially in business-to-business industry settings, will serve as a catalyst\nfor more intentional and socially-grounded practices, thus bridging the\nresponsibility and principles-to-practice gap.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.14742,regular,post_llm,2023,2,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'A Big Data Driven Framework for Duplicate Device Detection from\n  Multi-sourced Mobile Device Location Data\n\n  Mobile Device Location Data (MDLD) has been popularly utilized in various\nfields. Yet its large-scale applications are limited because of either biased\nor insufficient spatial coverage of the data from individual data vendors. One\napproach to improve the data coverage is to leverage the data from multiple\ndata vendors and integrate them to build a more representative dataset. For\ndata integration, further treatments on the multi-sourced dataset are required\ndue to several reasons. First, the possibility of carrying more than one device\ncould result in duplicated observations from the same data subject.\nAdditionally, when utilizing multiple data sources, the same device might be\ncaptured by more than one data provider. Our paper proposes a data integration\nmethodology for multi-sourced data to investigate the feasibility of\nintegrating data from several sources without introducing additional biases to\nthe data. By leveraging the uniqueness of travel pattern of each device,\nduplicate devices are identified. The proposed methodology is shown to be\ncost-effective while it achieves the desired accuracy level. Our findings\nsuggest that devices sharing the same imputed home location and the top five\nmost-visited locations during a month can represent the same user in the MDLD.\nIt is shown that more than 99.6% of the sample devices having the\naforementioned attribute in common are observed at the same location\nsimultaneously. Finally, the proposed algorithm has been successfully applied\nto the national-level MDLD of 2020 to produce the national passenger\norigin-destination data for the NextGeneration National Household Travel Survey\n(NextGen NHTS) program.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.09628,review,post_llm,2023,2,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Energy Efficient Homes: The Social and Spatial Patterns of Residential\n  Energy Efficiency in England\n\n  Poor energy efficiency of homes is a major problem with urgent environmental\nand social implications. Housing in the UK relies heavily on fossil fuels for\nenergy supply and has some of the lowest energy efficiency in Europe. We\nexplore spatial variations in energy efficiency across England using data from\nEnergy Performance Certificates (EPCs), which cover approximately half of the\nresidential stock (14M homes between 2008-22). We examine variations between\nauthorities after accounting for the composition of the housing stock in terms\nof its fixed characteristics of property type, building age and size. We\nexplore variations in terms of geographical and social context (region,\nurban-rural and deprivation), which gives a picture of the scale of the\nchallenge each faces. We also examine variations in relation to the more\nreadily upgraded factors, such as glazing types, and in relation to local\nparticipation in improvement programmes which gives some insight into local\nactions or progress achieved.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2302.13731,review,post_llm,2023,2,"{'ai_likelihood': 3.6921766069200305e-05, 'text': 'Mapping and Comparing Data Governance Frameworks: A benchmarking\n  exercise to inform global data governance deliberations\n\n  Data has become a critical resource for organizations and society. Yet, it is\nnot always as valuable as it could be since there is no well-defined approach\nto managing and using it. This article explores the increasing importance of\nglobal data governance due to the rapid growth of data and the need for\nresponsible data use and protection. While historically associated with private\norganizational governance, data governance has evolved to include governmental\nand institutional bodies. However, the lack of a global consensus and\nfragmentation in policies and practices pose challenges to the development of a\ncommon framework. The purpose of this report is to compare approaches and\nidentify patterns in the emergent and fragmented data governance ecosystem\nwithin sectors close to the international development field, ultimately\npresenting key takeaways and reflections on when and why a global data\ngovernance framework may be needed. Overall, the report highlights the need for\na more holistic, coordinated transnational approach to data governance to\nmanage the global flow of data responsibly and for the public interest. The\narticle begins by giving an overview of the current fragmented data governance\necology, to then proceed to illustrate the methodology used. Subsequently, the\npaper illustrates the most relevant findings stemming from the research. These\nare organized according to six key elements: (a) purpose, (b) principles, (c)\nanchoring documents, (d) data description and lifecycle, (e) processes, and (f)\npractices. Finally, the article closes with a series of key takeaways and final\nreflections.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.15841,regular,post_llm,2023,3,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Does Money Laundering on Ethereum Have Traditional Traits?\n\n  As the largest blockchain platform that supports smart contracts, Ethereum\nhas developed with an incredible speed. Yet due to the anonymity of blockchain,\nthe popularity of Ethereum has fostered the emergence of various illegal\nactivities and money laundering by converting ill-gotten funds to cash. In the\ntraditional money laundering scenario, researchers have uncovered the prevalent\ntraits of money laundering. However, since money laundering on Ethereum is an\nemerging means, little is known about money laundering on Ethereum. To fill the\ngap, in this paper, we conduct an in-depth study on Ethereum money laundering\nnetworks through the lens of a representative security event on \\textit{Upbit\nExchange} to explore whether money laundering on Ethereum has traditional\ntraits. Specifically, we construct a money laundering network on Ethereum by\ncrawling the transaction records of \\textit{Upbit Hack}. Then, we present five\nquestions based on the traditional traits of money laundering networks. By\nleveraging network analysis, we characterize the money laundering network on\nEthereum and answer these questions. In the end, we summarize the findings of\nmoney laundering networks on Ethereum, which lay the groundwork for money\nlaundering detection on Ethereum.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.06795,regular,post_llm,2023,3,"{'ai_likelihood': 1.5066729651557076e-05, 'text': 'Roadmap towards Meta-being\n\n  Metaverse is a perpetual and persistent multi-user environment that merges\nphysical reality with digital virtuality. It is widely considered to be the\nnext revolution of the Internet. Digital humans are a critical part of\nMetaverse. They are driven by artificial intelligence (AI) and deployed in many\napplications. However, it is a complex process to construct digital humans\nwhich can be well combined with the Metaverse elements, such as immersion\ncreation, connection construction, and economic operation. In this paper, we\npresent the roadmap of Meta-being to construct the digital human in Metaverse.\nIn this roadmap, we first need to model and render a digital human model for\nimmersive display in Metaverse. Then we add a dialogue system with audio,\nfacial expressions, and movements for this digital human. Finally, we can apply\nour digital human in the fields of the economy in Metaverse with the\nconsideration of security. We also construct a digital human in Metaverse to\nimplement our roadmap. Numerous advanced technologies and devices, such as AI,\nNatural Language Processing (NLP), and motion capture, are used in our\nimplementation. This digital human can be applied to many applications, such as\neducation and exhibition.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.00651,review,post_llm,2023,3,"{'ai_likelihood': 4.513396157158746e-05, 'text': 'Algorithmic Governance for Explainability: A Comparative Overview of\n  Progress and Trends\n\n  The explainability of AI has transformed from a purely technical issue to a\ncomplex issue closely related to algorithmic governance and algorithmic\nsecurity. The lack of explainable AI (XAI) brings adverse effects that can\ncross all economic classes and national borders. Despite efforts in governance,\ntechnical, and policy exchange have been made in XAI by multiple stakeholders,\nincluding the public sector, enterprises, and international organizations,\nrespectively. XAI is still in its infancy. Future applications and\ncorresponding regulatory instruments are still dependent on the collaborative\nengagement of all parties.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06511,regular,post_llm,2023,3,"{'ai_likelihood': 2.347760730319553e-05, 'text': ""IoT-Based Remote Health Monitoring System Employing Smart Sensors for\n  Asthma Patients during COVID-19 Pandemic\n\n  COVID19 and asthma are respiratory diseases that can be life threatening in\nuncontrolled circumstances and require continuous monitoring. A poverty\nstricken South Asian country like Bangladesh has been bearing the brunt of the\nCOVID19 pandemic since its beginning. The majority of the country's population\nresides in rural areas, where proper healthcare is difficult to access. This\nemphasizes the necessity of telemedicine, implementing the concept of the\nInternet of Things (IoT), which is still under development in Bangladesh. This\npaper demonstrates how the current challenges in the healthcare system are\nresolvable through the design of a remote health and environment monitoring\nsystem, specifically for asthma patients who are at an increased risk of\nCOVID19. Since on-time treatment is essential, this system will allow doctors\nand medical staff to receive patient information in real time and deliver their\nservices immediately to the patient regardless of their location. The proposed\nsystem consists of various sensors collecting heart rate, body temperature,\nambient temperature, humidity, and air quality data and processing them through\nthe Arduino Microcontroller. It is integrated with a mobile application. All\nthis data is sent to the mobile application via a Bluetooth module and updated\nevery few seconds so that the medical staff can instantly track patients'\nconditions and emergencies. The developed prototype is portable and easily\nusable by anyone. The system has been applied to five people of different ages\nand medical histories over a particular period. Upon analyzing all their data,\nit became clear which participants were particularly vulnerable to health\ndeterioration and needed constant observation. Through this research, awareness\nabout asthmatic symptoms will improve and help prevent their severity through\neffective treatment anytime, anywhere.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06032,review,post_llm,2023,3,"{'ai_likelihood': 1.2252065870496963e-05, 'text': 'Web 3.0: The Future of Internet\n\n  With the rapid growth of the Internet, human daily life has become deeply\nbound to the Internet. To take advantage of massive amounts of data and\ninformation on the internet, the Web architecture is continuously being\nreinvented and upgraded. From the static informative characteristics of Web 1.0\nto the dynamic interactive features of Web 2.0, scholars and engineers have\nworked hard to make the internet world more open, inclusive, and equal. Indeed,\nthe next generation of Web evolution (i.e., Web 3.0) is already coming and\nshaping our lives. Web 3.0 is a decentralized Web architecture that is more\nintelligent and safer than before. The risks and ruin posed by monopolists or\ncriminals will be greatly reduced by a complete reconstruction of the Internet\nand IT infrastructure. In a word, Web 3.0 is capable of addressing web data\nownership according to distributed technology. It will optimize the internet\nworld from the perspectives of economy, culture, and technology. Then it\npromotes novel content production methods, organizational structures, and\neconomic forms. However, Web 3.0 is not mature and is now being disputed.\nHerein, this paper presents a comprehensive survey of Web 3.0, with a focus on\ncurrent technologies, challenges, opportunities, and outlook. This article\nfirst introduces a brief overview of the history of World Wide Web as well as\nseveral differences among Web 1.0, Web 2.0, Web 3.0, and Web3. Then, some\ntechnical implementations of Web 3.0 are illustrated in detail. We discuss the\nrevolution and benefits that Web 3.0 brings. Finally, we explore several\nchallenges and issues in this promising area.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.08939,regular,post_llm,2023,3,"{'ai_likelihood': 4.735257890489366e-06, 'text': ""Using 3D printed badges to improve student performance and reduce\n  dropout rates in STEM higher education\n\n  Students' perception of excessive difficulty in STEM degrees lowers their\nmotivation and therefore affects their performance. According to prior\nresearch, the use of gamification techniques promote engagement, motivation and\nfun when learning. Badges, which are a distinction that is given as a reward to\nstudents, are a well-known gamification tool. This contribution hypothesizes\nthat the use of badges, both physical and virtual, improves student performance\nand reduces dropout rates. To verify that hypothesis, a case study involving 99\nstudents enrolled in a Databases course of computer engineering degrees was\nconducted. The results show that the usage of badges improves student\nperformance and reduces dropout rates. However, negligible differences were\nfound between the use of different kind of badges.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06124,review,post_llm,2023,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'AI-assisted Protective Action: Study of ChatGPT as an Information Source\n  for a Population Facing Climate Hazards\n\n  ChatGPT has been emerging as a novel information source, and it is likely\nthat the public might seek information from ChatGPT while taking protective\nactions when facing climate hazards such as floods and hurricanes. The\nobjective of this study is to evaluate the accuracy and completeness of\nresponses generated by ChatGPT when individuals seek information about aspects\nof taking protective actions. The survey analysis results indicated that: (1)\nthe emergency managers considered the responses provided by ChatGPT as accurate\nand complete to a great extent; (2) it was statistically verified in\nevaluations that the generated information was accurate, but lacked\ncompleteness, implying that the extent of information provided is accurate; and\n(3) information generated for prompts related to hazard insurance received the\nhighest evaluation, whereas the information generated related to evacuation\nreceived the lowest. This last result implies that, for complex,\ncontext-specific protective actions (such as evacuation), the information was\nrated as less complete compared with other protective actions. Also, the\nresults showed that the perception of respondents regarding the utility of AI-\nassistive technologies (such as ChatGPT) for emergency preparedness and\nresponse improved after taking the survey and evaluating the information\ngenerated by ChatGPT. The findings from this study provide empirical evaluation\nregarding the utility of AI-assistive technologies for improving public\ndecision-making and protective actions in disasters.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.09001,review,post_llm,2023,3,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Reclaiming the Digital Commons: A Public Data Trust for Training Data\n\n  Democratization of AI means not only that people can freely use AI, but also\nthat people can collectively decide how AI is to be used. In particular,\ncollective decision-making power is required to redress the negative\nexternalities from the development of increasingly advanced AI systems,\nincluding degradation of the digital commons and unemployment from automation.\nThe rapid pace of AI development and deployment currently leaves little room\nfor this power. Monopolized in the hands of private corporations, the\ndevelopment of the most capable foundation models has proceeded largely without\npublic input. There is currently no implemented mechanism for ensuring that the\neconomic value generated by such models is redistributed to account for their\nnegative externalities. The citizens that have generated the data necessary to\ntrain models do not have input on how their data are to be used. In this work,\nwe propose that a public data trust assert control over training data for\nfoundation models. In particular, this trust should scrape the internet as a\ndigital commons, to license to commercial model developers for a percentage cut\nof revenues from deployment. First, we argue in detail for the existence of\nsuch a trust. We also discuss feasibility and potential risks. Second, we\ndetail a number of ways for a data trust to incentivize model developers to use\ntraining data only from the trust. We propose a mix of verification mechanisms,\npotential regulatory action, and positive incentives. We conclude by\nhighlighting other potential benefits of our proposed data trust and connecting\nour work to ongoing efforts in data and compute governance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.05862,review,post_llm,2023,3,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Monitoring Gender Gaps via LinkedIn Advertising Estimates: the case\n  study of Italy\n\n  Women remain underrepresented in the labour market. Although significant\nadvancements are being made to increase female participation in the workforce,\nthe gender gap is still far from being bridged. We contribute to the growing\nliterature on gender inequalities in the labour market, evaluating the\npotential of the LinkedIn estimates to monitor the evolution of the gender gaps\nsustainably, complementing the official data sources. In particular, assessing\nthe labour market patterns at a subnational level in Italy. Our findings show\nthat the LinkedIn estimates accurately capture the gender disparities in Italy\nregarding sociodemographic attributes such as gender, age, geographic location,\nseniority, and industry category. At the same time, we assess data biases such\nas the digitalisation gap, which impacts the representativity of the workforce\nin an imbalanced manner, confirming that women are under-represented in\nSouthern Italy. Additionally to confirming the gender disparities to the\nofficial census, LinkedIn estimates are a valuable tool to provide dynamic\ninsights; we showed an immigration flow of highly skilled women, predominantly\nfrom the South. Digital surveillance of gender inequalities with detailed and\ntimely data is particularly significant to enable policymakers to tailor\nimpactful campaigns.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.02863,review,post_llm,2023,3,"{'ai_likelihood': 6.788306766086155e-06, 'text': 'A Note on the Proposed Law for Improving the Transparency of Political\n  Advertising in the European Union\n\n  There is an increasing supply and demand for political advertising throughout\nthe world. At the same time, societal threats, such as election interference by\nforeign governments and other bad actors, continues to be a pressing concern in\nmany democracies. Furthermore, manipulation of electoral outcomes, whether by\nforeign or domestic forces, continues to be a concern of many citizens who are\nalso worried about their fundamental rights. To these ends, the European Union\n(EU) has launched several initiatives for tackling the issues. A new regulation\nwas proposed in 2020 also for improving the transparency of political\nadvertising in the union. This short commentary reviews the regulation proposed\nand raises a few points about its limitations and potential impacts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06143,regular,post_llm,2023,3,"{'ai_likelihood': 4.407432344224718e-05, 'text': 'Utilizing the International Classification of Functioning, Disability\n  and Health (ICF) in forming a personal health index\n\n  We propose a new model for comprehensively monitoring the health status of\nindividuals by calculating a personal health index. The central framework of\nthe model is the International Classification of Functioning, Disability and\nHealth (ICF) developed by the World Health Organization. The model is capable\nof handling incomplete and heterogeneous data sets collected using different\ntechniques. The health index was validated by comparing it to two self-assessed\nhealth measures provided by individuals undergoing rehabilitation. Results\nindicate that the model yields valid health index outcomes, suggesting that the\nproposed model is applicable in practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06504,regular,post_llm,2023,3,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Developing a Robust Computable Phenotype Definition Workflow to Describe\n  Health and Disease in Observational Health Research\n\n  Health informatics can inform decisions that practitioners, patients,\npolicymakers, and researchers need to make about health and disease. Health\ninformatics is built upon patient health data leading to the need to codify\npatient health information. Such standardization is required to compute\npopulation statistics (such as prevalence, incidence, etc.) that are common\nmetrics used in fields such as epidemiology. Reliable decision-making about\nhealth and disease rests on our ability to organize, analyze, and assess data\nrepositories that contain patient health data.\n  While standards exist to structure and analyze patient data across patient\ndata sources such as health information exchanges, clinical data repositories,\nand health data marketplaces, analogous best practices for rigorously defining\npatient populations in health informatics contexts do not exist. Codifying best\npractices for developing disease definitions could support the effective\ndevelopment of clinical guidelines, inform algorithms used in clinical decision\nsupport systems, and additional patient guidelines.\n  In this paper, we present a workflow for the development of phenotype\ndefinitions. This workflow presents a series of recommendations for defining\nhealth and disease. Various examples within this paper are presented to\ndemonstrate this workflow in health informatics contexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.13138,regular,post_llm,2023,3,"{'ai_likelihood': 5.529986487494575e-06, 'text': 'Online search is more likely to lead students to validate true news than\n  to refute false ones\n\n  With the spread of high-speed Internet and portable smart devices, the way\npeople access and consume information has drastically changed. However, this\npresents many challenges, including information overload, personal data\nleakage, and misinformation diffusion. Across the spectrum of risks that\nInternet users face nowadays, this work focuses on understanding how young\npeople perceive and deal with false information. Within an experimental\ncampaign involving 183 students, we presented six different news items to the\nparticipants and invited them to browse the Internet to assess the veracity of\nthe presented information. Our results suggest that online search is more\nlikely to lead students to validate true news than to refute false ones. We\nfound that students change their opinion about a specific piece of information\nmore often than their global idea about a broader topic. Also, our experiment\nreflected that most participants rely on online sources to obtain information\nand access the news, and those getting information from books and Internet\nbrowsing are the most accurate in assessing the veracity of a news item. This\nwork provides a principled understanding of how young people perceive and\ndistinguish true and false pieces of information, identifying strengths and\nweaknesses amidst young subjects and contributing to building tailored digital\ninformation literacy strategies for youth.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.17201,regular,post_llm,2023,3,"{'ai_likelihood': 2.814663781060113e-06, 'text': ""Quantifying the Academic Quality of Children's Videos using Machine\n  Comprehension\n\n  YouTube Kids (YTK) is one of the most popular kids' applications used by\nmillions of kids daily. However, various studies have highlighted concerns\nabout the videos on the platform, like the over-presence of entertaining and\ncommercial content. YouTube recently proposed high-quality guidelines that\ninclude `promoting learning' and proposed to use it in ranking channels.\nHowever, the concept of learning is multi-faceted, and it can be difficult to\ndefine and measure in the context of online videos. This research focuses on\nlearning in terms of what's taught in schools and proposes a way to measure the\nacademic quality of children's videos. Using a new dataset of questions and\nanswers from children's videos, we first show that a Reading Comprehension (RC)\nmodel can estimate academic learning. Then, using a large dataset of middle\nschool textbook questions on diverse topics, we quantify the academic quality\nof top channels as the number of children's textbook questions that an RC model\ncan correctly answer. By analyzing over 80,000 videos posted on the top 100\nchannels, we present the first thorough analysis of the academic quality of\nchannels on YTK.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.00877,regular,post_llm,2023,3,"{'ai_likelihood': 3.973642985026042e-06, 'text': 'Building Dynamic Ontological Models for Place using Social Media Data\n  from Twitter and Sina Weibo\n\n  Place holds human thoughts and experiences. Space is defined with geometric\nmeasurement and coordinate systems. Social media served as the connection\nbetween place and space. In this study, we use social media data (Twitter,\nWeibo) to build a dynamic ontological model in two separate areas: Beijing,\nChina and San Diego, the U.S.A. Three spatial analytics methods are utilized to\ngenerate the place name ontology: 1) Kernel Density Estimation (KDE); 2)\nDynamic Method Density-based spatial clustering of applications with noise\n(DBSCAN); 3) hierarchal clustering. We identified feature types of place name\nontologies from geotagged social media data and classified them by comparing\ntheir default search radius of KDE of geo-tagged points. By tracing the\nseasonal changes of highly dynamic non-administrative places, seasonal\nvariation patterns were observed, which illustrates the dynamic changes in\nplace ontology caused by the change in human activities and conversation over\ntime and space. We also investigate the semantic meaning of each place name by\nexamining Pointwise Mutual Information (PMI) scores and word clouds. The major\ncontribution of this research is to link and analyze the associations between\nplace, space, and their attributes in the field of geography. Researchers can\nuse crowd-sourced data to study the ontology of places rather than relying on\ntraditional gazetteers. The dynamic ontology in this research can provide\nbright insight into urban planning and re-zoning and other related industries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.09387,review,post_llm,2023,3,"{'ai_likelihood': 9.735425313313803e-06, 'text': 'Characterizing Manipulation from AI Systems\n\n  Manipulation is a common concern in many domains, such as social media,\nadvertising, and chatbots. As AI systems mediate more of our interactions with\nthe world, it is important to understand the degree to which AI systems might\nmanipulate humans without the intent of the system designers. Our work\nclarifies challenges in defining and measuring manipulation in the context of\nAI systems. Firstly, we build upon prior literature on manipulation from other\nfields and characterize the space of possible notions of manipulation, which we\nfind to depend upon the concepts of incentives, intent, harm, and covertness.\nWe review proposals on how to operationalize each factor. Second, we propose a\ndefinition of manipulation based on our characterization: a system is\nmanipulative if it acts as if it were pursuing an incentive to change a human\n(or another agent) intentionally and covertly. Third, we discuss the\nconnections between manipulation and related concepts, such as deception and\ncoercion. Finally, we contextualize our operationalization of manipulation in\nsome applications. Our overall assessment is that while some progress has been\nmade in defining and measuring manipulation from AI systems, many gaps remain.\nIn the absence of a consensus definition and reliable tools for measurement, we\ncannot rule out the possibility that AI systems learn to manipulate humans\nwithout the intent of the system designers. We argue that such manipulation\nposes a significant threat to human autonomy, suggesting that precautionary\nactions to mitigate it are warranted.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.13938,review,post_llm,2023,3,"{'ai_likelihood': 1.2483861711290148e-05, 'text': 'A Comparative Study of National Cyber Security Strategies of ten nations\n\n  This study compares the National Cybersecurity Strategies (NCSSs) of publicly\navailable documents of ten nations across Europe (United Kingdom, France,\nLithuania, Estonia, Spain, and Norway), Asia-Pacific (Singapore and Australia),\nand the American region (the United States of America and Canada). The study\nobserved that there is not a unified understanding of the term ""Cybersecurity"";\nhowever, a common trajectory of the NCSSs shows that the fight against\ncybercrime is a joint effort among various stakeholders, hence the need for\nstrong international cooperation. Using a comparative structure and an NCSS\nframework, the research finds similarities in protecting critical assets,\ncommitment to research and development, and improved national and international\ncollaboration. The study finds that the lack of a unified underlying\ncybersecurity framework leads to a disparity in the structure and contents of\nthe strategies. The strengths and weaknesses of the NCSSs from the research can\nbenefit countries planning to develop or update their cybersecurity strategies.\nThe study gives recommendations that strategy developers can consider when\ndeveloping an NCSS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.11408,regular,post_llm,2023,3,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Stable Bias: Analyzing Societal Representations in Diffusion Models\n\n  As machine learning-enabled Text-to-Image (TTI) systems are becoming\nincreasingly prevalent and seeing growing adoption as commercial services,\ncharacterizing the social biases they exhibit is a necessary first step to\nlowering their risk of discriminatory outcomes. This evaluation, however, is\nmade more difficult by the synthetic nature of these systems' outputs: common\ndefinitions of diversity are grounded in social categories of people living in\nthe world, whereas the artificial depictions of fictive humans created by these\nsystems have no inherent gender or ethnicity. To address this need, we propose\na new method for exploring the social biases in TTI systems. Our approach\nrelies on characterizing the variation in generated images triggered by\nenumerating gender and ethnicity markers in the prompts, and comparing it to\nthe variation engendered by spanning different professions. This allows us to\n(1) identify specific bias trends, (2) provide targeted scores to directly\ncompare models in terms of diversity and representation, and (3) jointly model\ninterdependent social variables to support a multidimensional analysis. We\nleverage this method to analyze images generated by 3 popular TTI systems\n(Dall-E 2, Stable Diffusion v 1.4 and 2) and find that while all of their\noutputs show correlations with US labor demographics, they also consistently\nunder-represent marginalized identities to different extents. We also release\nthe datasets and low-code interactive bias exploration platforms developed for\nthis work, as well as the necessary tools to similarly evaluate additional TTI\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.08266,regular,post_llm,2023,3,"{'ai_likelihood': 1.1920928955078125e-06, 'text': ""Rules of Engagement: Why and How Companies Participate in OSS\n\n  Company engagement in open source (OSS) is now the new norm. From large\ntechnology companies to startups, companies are participating in the OSS\necosystem by open-sourcing their technology, sponsoring projects through\nfunding or paid developer time. However, our understanding of the OSS ecosystem\nis rooted in the 'old world' model where individual contributors sustain OSS\nprojects. In this work, we create a more comprehensive understanding of the\nhybrid OSS landscape by investigating what motivates companies to contribute\nand how they contribute to OSS. We conducted interviews with 20 participants\nwho have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17\ndifferent companies of different sizes from large companies (e.g. Microsoft,\nRedHat, Google, Spotify) to startups. Data from semi-structured interviews\nreveal that company motivations can be categorized into four levels (Founders'\nVision, Reputation, Business Advantage, and Reciprocity) and companies\nparticipate through different mechanisms (e.g., Developers' Time, Mentoring\nTime, Advocacy & Promotion Time), each of which tie to the different types of\nmotivations. We hope our findings nudge more companies to participate in the\nOSS ecosystem, helping make it robust, diverse, and sustainable.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.16828,regular,post_llm,2023,3,"{'ai_likelihood': 1.688798268636068e-05, 'text': 'Tackling Hate Speech in Low-resource Languages with Context Experts\n\n  Given Myanmars historical and socio-political context, hate speech spread on\nsocial media has escalated into offline unrest and violence. This paper\npresents findings from our remote study on the automatic detection of hate\nspeech online in Myanmar. We argue that effectively addressing this problem\nwill require community-based approaches that combine the knowledge of context\nexperts with machine learning tools that can analyze the vast amount of data\nproduced. To this end, we develop a systematic process to facilitate this\ncollaboration covering key aspects of data collection, annotation, and model\nvalidation strategies. We highlight challenges in this area stemming from small\nand imbalanced datasets, the need to balance non-glamorous data work and\nstakeholder priorities, and closed data-sharing practices. Stemming from these\nfindings, we discuss avenues for further work in developing and deploying hate\nspeech detection systems for low-resource languages.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.12392,review,post_llm,2023,3,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'The LAVA Model: Learning Analytics Meets Visual Analytics\n\n  Human-Centered learning analytics (HCLA) is an approach that emphasizes the\nhuman factors in learning analytics and truly meets user needs. User\ninvolvement in all stages of the design, analysis, and evaluation of learning\nanalytics is the key to increase value and drive forward the acceptance and\nadoption of learning analytics. Visual analytics is a multidisciplinary data\nscience research field that follows a human-centered approach and thus has the\npotential to foster the acceptance of learning analytics. Although various\ndomains have already made use of visual analytics, it has not been considered\nmuch with respect to learning analytics. This paper explores the benefits of\nincorporating visual analytics concepts into the learning analytics process by\n(a) proposing the Learning Analytics and Visual Analytics (LAVA) model as\nenhancement of the learning analytics process with human in the loop, (b)\napplying the LAVA model in the Open Learning Analytics Platform (OpenLAP) to\nsupport humancentered indicator design, and (c) evaluating how blending\nlearning analytics and visual analytics can enhance the acceptance and adoption\nof learning analytics, based on the technology acceptance model (TAM).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.10476,regular,post_llm,2023,3,"{'ai_likelihood': 2.771615982055664e-05, 'text': ""Assessing Scientific Contributions in Data Sharing Spaces\n\n  In the present academic landscape, the process of collecting data is slow,\nand the lax infrastructures for data collaborations lead to significant delays\nin coming up with and disseminating conclusive findings. Therefore, there is an\nincreasing need for a secure, scalable, and trustworthy data-sharing ecosystem\nthat promotes and rewards collaborative data-sharing efforts among researchers,\nand a robust incentive mechanism is required to achieve this objective.\nReputation-based incentives, such as the h-index, have historically played a\npivotal role in the academic community. However, the h-index suffers from\nseveral limitations. This paper introduces the SCIENCE-index, a\nblockchain-based metric measuring a researcher's scientific contributions.\nUtilizing the Microsoft Academic Graph and machine learning techniques, the\nSCIENCE-index predicts the progress made by a researcher over their career and\nprovides a soft incentive for sharing their datasets with peer researchers. To\nincentivize researchers to share their data, the SCIENCE-index is augmented to\ninclude a data-sharing parameter. DataCite, a database of openly available\ndatasets, proxies this parameter, which is further enhanced by including a\nresearcher's data-sharing activity. Our model is evaluated by comparing the\ndistribution of its output for geographically diverse researchers to that of\nthe h-index. We observe that it results in a much more even spread of\nevaluations. The SCIENCE-index is a crucial component in constructing a\ndecentralized protocol that promotes trust-based data sharing, addressing the\ncurrent inequity in dataset sharing. The work outlined in this paper provides\nthe foundation for assessing scientific contributions in future data-sharing\nspaces powered by decentralized applications.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.10502,review,post_llm,2023,3,"{'ai_likelihood': 2.1192762586805556e-06, 'text': ""Cyberbullying in Text Content Detection: An Analytical Review\n\n  Technological advancements have resulted in an exponential increase in the\nuse of online social networks (OSNs) worldwide. While online social networks\nprovide a great communication medium, they also increase the user's exposure to\nlife-threatening situations such as suicide, eating disorder, cybercrime,\ncompulsive behavior, anxiety, and depression. To tackle the issue of\ncyberbullying, most existing literature focuses on developing approaches to\nidentifying factors and understanding the textual factors associated with\ncyberbullying. While most of these approaches have brought great success in\ncyberbullying research, data availability needed to develop model detection\nremains a challenge in the research space. This paper conducts a comprehensive\nliterature review to provide an understanding of cyberbullying detection.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.0907,regular,post_llm,2023,3,"{'ai_likelihood': 6.986988915337457e-06, 'text': ""LCS-TF: Multi-Agent Deep Reinforcement Learning-Based Intelligent\n  Lane-Change System for Improving Traffic Flow\n\n  Discretionary lane-change is one of the critical challenges for autonomous\nvehicle (AV) design due to its significant impact on traffic efficiency.\nExisting intelligent lane-change solutions have primarily focused on optimizing\nthe performance of the ego-vehicle, thereby suffering from limited\ngeneralization performance. Recent research has seen an increased interest in\nmulti-agent reinforcement learning (MARL)-based approaches to address the\nlimitation of the ego vehicle-based solutions through close coordination of\nmultiple agents. Although MARL-based approaches have shown promising results,\nthe potential impact of lane-change decisions on the overall traffic flow of a\nroad segment has not been fully considered. In this paper, we present a novel\nhybrid MARL-based intelligent lane-change system for AVs designed to jointly\noptimize the local performance for the ego vehicle, along with the global\nperformance focused on the overall traffic flow of a given road segment. With a\ncareful review of the relevant transportation literature, a novel state space\nis designed to integrate both the critical local traffic information pertaining\nto the surrounding vehicles of the ego vehicle, as well as the global traffic\ninformation obtained from a road-side unit (RSU) responsible for managing a\nroad segment. We create a reward function to ensure that the agents make\neffective lane-change decisions by considering the performance of the ego\nvehicle and the overall improvement of traffic flow. A multi-agent deep\nQ-network (DQN) algorithm is designed to determine the optimal policy for each\nagent to effectively cooperate in performing lane-change maneuvers. LCS-TF's\nperformance was evaluated through extensive simulations in comparison with\nstate-of-the-art MARL models. In all aspects of traffic efficiency, driving\nsafety, and driver comfort, the results indicate that LCS-TF exhibits superior\nperformance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.14597,review,post_llm,2023,3,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Smart Cities: Striking a Balance Between Urban Resilience and Civil\n  Liberties\n\n  Cities are becoming smarter and more resilient by integrating urban\ninfrastructure with information technology. However, concerns grow that smart\ncities might reverse progress on civil liberties when sensing, profiling, and\npredicting citizen activities; undermining citizen autonomy in connectivity,\nmobility, and energy consumption; and deprivatizing digital infrastructure. In\nresponse, cities need to deploy technical breakthroughs, such as\nprivacy-enhancing technologies, cohort modelling, and fair and explainable\nmachine learning. However, as throwing technologies at cities cannot always\naddress civil liberty concerns, cities must ensure transparency and foster\ncitizen participation to win public trust about the way resilience and\nliberties are balanced.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.02041,regular,post_llm,2023,3,"{'ai_likelihood': 9.90099377102322e-06, 'text': ""Likes and Fragments: Examining Perceptions of Time Spent on TikTok\n\n  Researchers use information about the amount of time people spend on digital\nmedia for numerous purposes. While social media platforms commonly do not allow\nexternal access to measure the use time directly, a usual alternative method is\nto use participants' self-estimation. However, doubts were raised about the\nself-estimation's accuracy, posing questions regarding the cognitive factors\nthat underline people's perceptions of the time they spend on social media. In\nthis work, we build on prior studies and explore a novel social media platform\nin the context of use time: TikTok. We conduct platform-independent\nmeasurements of people's self-reported and server-logged TikTok usage (n=255)\nto understand how users' demographics and platform engagement influence their\nperceptions of the time they spend on the platform and their estimation\naccuracy. Our work adds to the body of work seeking to understand time\nestimations in different digital contexts and identifies new influential\nengagement factors.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06491,regular,post_llm,2023,3,"{'ai_likelihood': 0.00010503662957085505, 'text': 'IoT-Based Water Quality Assessment System for Industrial Waste\n  WaterHealthcare Perspective\n\n  The environment, especially water, gets polluted due to industrialization and\nurbanization. Pollution due to industrialization and urbanization has harmful\neffects on both the environment and the lives on Earth. This polluted water can\ncause food poisoning, diarrhea, short-term gastrointestinal problems,\nrespiratory diseases, skin problems, and other serious health complications. In\na developing country like Bangladesh, where ready-made garments sector is one\nof the major sources of the total Gross Domestic Product (GDP), most of the\nwastes released from the garment factories are dumped into the nearest rivers\nor canals. Hence, the quality of the water of these bodies become very\nincompatible for the living beings, and so, it has become one of the major\nthreats to the environment and human health. In addition, the amount of fish in\nthe rivers and canals in Bangladesh is decreasing day by day as a result of\nwater pollution. Therefore, to save fish and other water animals and the\nenvironment, we need to monitor the quality of the water and find out the\nreasons for the pollution. Real-time monitoring of the quality of water is\nvital for controlling water pollution. Most of the approaches for controlling\nwater pollution are mainly biological and lab-based, which takes a lot of time\nand resources. To address this issue, we developed an Internet of Things\n(IoT)-based real-time water quality monitoring system, integrated with a mobile\napplication. The proposed system in this research measures some of the most\nimportant indexes of water, including the potential of hydrogen (pH), total\ndissolved solids (TDS), and turbidity, and temperature of water. The proposed\nsystem results will be very helpful in saving the environment, and thus,\nimproving the health of living creatures on Earth.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.15614,regular,post_llm,2023,3,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Modeling Population Movements under Uncertainty at the Border in\n  Humanitarian Crises: A Situational Analysis Tool\n\n  Humanitarian agencies must be prepared to mobilize quickly in response to\ncomplex emergencies, and their effectiveness depends on their ability to\nidentify, anticipate, and prepare for future needs. These are typically highly\nuncertain situations in which predictive modeling tools can be useful but\nchallenging to build. To better understand the need for humanitarian support --\nincluding shelter and assistance -- and strengthen contingency planning and\nprotection efforts for displaced populations, we present a situational analysis\ntool to help anticipate the number of migrants and forcibly displaced persons\nthat will cross a border in a humanitarian crisis. The tool consists of: (i)\nindicators of potential intent to move drawn from traditional and big data\nsources; (ii) predictive models for forecasting possible future movements; and\n(iii) a simulation of border crossings and shelter capacity requirements under\ndifferent conditions. This tool has been specifically adapted to contingency\nplanning in settings of high uncertainty, with an application to the\nBrazil-Venezuela border during the COVID-19 pandemic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.03956,regular,post_llm,2023,3,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Teaching AI and Robotics to Children in a Mexican town\n\n  In this paper, we present a pilot study aiming to investigate the challenges\nof teaching AI and Robotics to children in low- and middle-income countries.\nChallenges such as the little to none experts and the limited resources in a\nMexican town to teach AI and Robotics were addressed with the creation of\ninclusive learning activities with Montessori method and open-source\neducational robots. For the pilot study, we invited 14 participants of which 10\nwere able to attend, 6 male and 4 female of (age in years: mean=8 and\nstd=$\\pm$1.61) and four instructors of different teaching experience levels to\nyoung audiences. We reported results of a four-lesson curriculum that is both\ninclusive and engaging. We showed the impact on the increase of general\nagreement of participants on the understanding of what engineers and scientists\ndo in their jobs, with engineering attitudes surveys and Likert scale charts\nfrom the first and the last lesson. We concluded that this pilot study helped\nchildren coming from low- to mid-income families to learn fundamental concepts\nof AI and Robotics and aware them of the potential of AI and Robotics\napplications which might rule their adult lives. Future work might lead (a) to\nhave better understanding on the financial and logistical challenges to\norganise a workshop with a major number of participants for reliable and\nrepresentative data and (b) to improve pretest-posttest survey design and its\nstatistical analysis. The resources to reproduce this work are available at\n\\url{https://github.com/air4children/dei-hri2023}.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06483,regular,post_llm,2023,3,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Monetizing Explainable AI: A Double-edged Sword\n\n  Algorithms used by organizations increasingly wield power in society as they\ndecide the allocation of key resources and basic goods. In order to promote\nfairer, juster, and more transparent uses of such decision-making power,\nexplainable artificial intelligence (XAI) aims to provide insights into the\nlogic of algorithmic decision-making. Despite much research on the topic,\nconsumer-facing applications of XAI remain rare. A central reason may be that a\nviable platform-based monetization strategy for this new technology has yet to\nbe found. We introduce and describe a novel monetization strategy for fusing\nalgorithmic explanations with programmatic advertising via an explanation\nplatform. We claim the explanation platform represents a new,\nsocially-impactful, and profitable form of human-algorithm interaction and\nestimate its potential for revenue generation in the high-risk domains of\nfinance, hiring, and education. We then consider possible undesirable and\nunintended effects of monetizing XAI and simulate these scenarios using\nreal-world credit lending data. Ultimately, we argue that monetizing XAI may be\na double-edged sword: while monetization may incentivize industry adoption of\nXAI in a variety of consumer applications, it may also conflict with the\noriginal legal and ethical justifications for developing XAI. We conclude by\ndiscussing whether there may be ways to responsibly and democratically harness\nthe potential of monetized XAI to provide greater consumer access to\nalgorithmic explanations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.04019,regular,post_llm,2023,3,"{'ai_likelihood': 4.900826348198785e-06, 'text': 'Teaching Digital Manufacturing Experimenting Blended-Learning Models By\n  Combining MOOC And On-site Workshops In FabLabs\n\n  Teaching digital manufacturing at scale using MOOCs has opened opportunities\nfor IMT, a network of French graduate engineering schools, to work closely with\na community of learners and educators in physical spaces called Fab Labs. By\nsetting up a cohort of lifelong learning trainees taking the MOOC online and\nattending hands-on in-person workshops, IMT to experiment blended learning\nmodels and hybrid skills certification for project-based STEM courses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.04841,regular,post_llm,2023,3,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'The dynamic nature of trust: Trust in Human-Robot Interaction revisited\n\n  The role of robots is expanding from tool to collaborator. Socially assistive\nrobots (SARs) are an example of collaborative robots that assist humans in the\nreal world. As robots enter our social sphere, unforeseen risks occur during\nhuman-robot interaction (HRI), as everyday human space is full of\nuncertainties. Risk introduces an element of trust, so understanding human\ntrust in the robot is imperative to initiate and maintain interactions with\nrobots over time. While many scholars have investigated the issue of\nhuman-robot trust, a significant portion of that discussion is rooted in the\nhuman-automation interaction literature. As robots are no longer mere\ninstruments, but social agents that co-exist with humans, we need a new lens to\ninvestigate the longitudinal dynamic nature of trust in HRI. In this position\npaper, we contend that focusing on the dynamic nature of trust as a new inquiry\nwill help us better design trustworthy robots.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.07511,regular,post_llm,2023,3,"{'ai_likelihood': 1.3907750447591147e-06, 'text': ""The Evaluation of a New Daylighting System's Energy Performance:\n  Reversible Daylighting System (RDS)\n\n  This paper evaluates the energy performance of a new daylighting system,\npatented by the author, in a regular closed office space. The advantage of this\nnew system as opposed to conventional venetian blinds is its rotating\ncapability, which improves the energy efficiency of the space. Computer\nsimulation method has been conducted to examine the performance of this new\nsystem on the south aperture of a closed-office space with 30% Window to Wall\nratio (WWR) in three cities in Iran with different climate zones based on\nASHRAE: Tehran (3B), Tabriz (4B), and Yazd (2B). The simulation has been\nimplemented in Honeybee platform with EnergyPlus engine to simulate the\ncombined total load consisting of heating, cooling, and lighting loads. To\ncontrol lighting, a dimming control is applied to the space. The results of the\nstudy represent the benefits of the reversible daylighting system (RDS) over\nthe state of the art venetian blinds to improve the energy efficiency of the\nspace through just changing the location of the blind during heating/cooling\ndemand time of the year.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.07529,review,post_llm,2023,3,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Thinking Upstream: Ethics and Policy Opportunities in AI Supply Chains\n\n  After children were pictured sewing its running shoes in the early 1990s,\nNike at first disavowed the ""working conditions in its suppliers\' factories"",\nbefore public pressure led them to take responsibility for ethics in their\nupstream supply chain. In 2023, OpenAI responded to criticism that Kenyan\nworkers were paid less than $2 per hour to filter traumatic content from its\nChatGPT model by stating in part that it had outsourced the work to a\nsubcontractor, who managed workers\' payment and mental health concerns. In this\nposition paper, we argue that policy interventions for AI Ethics must consider\nAI as a supply chain problem, given how the political economy and intra-firm\nrelations structure AI production, in particular examining opportunities\nupstream.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.08731,review,post_llm,2023,3,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Bridging adaptive management and reinforcement learning for more robust\n  decisions\n\n  From out-competing grandmasters in chess to informing high-stakes healthcare\ndecisions, emerging methods from artificial intelligence are increasingly\ncapable of making complex and strategic decisions in diverse, high-dimensional,\nand uncertain situations. But can these methods help us devise robust\nstrategies for managing environmental systems under great uncertainty? Here we\nexplore how reinforcement learning, a subfield of artificial intelligence,\napproaches decision problems through a lens similar to adaptive environmental\nmanagement: learning through experience to gradually improve decisions with\nupdated knowledge. We review where reinforcement learning (RL) holds promise\nfor improving evidence-informed adaptive management decisions even when\nclassical optimization methods are intractable. For example, model-free deep RL\nmight help identify quantitative decision strategies even when models are\nnonidentifiable. Finally, we discuss technical and social issues that arise\nwhen applying reinforcement learning to adaptive management problems in the\nenvironmental domain. Our synthesis suggests that environmental management and\ncomputer science can learn from one another about the practices, promises, and\nperils of experience-based decision-making.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.14621,review,post_llm,2023,3,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'A Set of Essentials for Online Learning : CSE-SET\n\n  Distance learning is not a novel concept. Education or learning conducted\nonline is a form of distance education. Online learning presents a convenient\nalternative to traditional learning. Numerous researchers have investigated the\nusage of online education in educational institutions and across nations. A set\nof essentials for effective online learning are elaborated in this study to\nensure stakeholders would not get demotivated in the online learning process.\nAlso, the study lists a set of factors that motivate students and other\nstakeholders to engage in online learning with enthusiasm and work towards\nonline learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.08213,review,post_llm,2023,3,"{'ai_likelihood': 1.1854701571994357e-05, 'text': ""The Overview of Privacy Labels and their Compatibility with Privacy\n  Policies\n\n  Privacy nutrition labels provide a way to understand an app's key data\npractices without reading the long and hard-to-read privacy policies. Recently,\nthe app distribution platforms for iOS(Apple) and Android(Google) have\nimplemented mandates requiring app developers to fill privacy nutrition labels\nhighlighting their privacy practices such as data collection, data sharing, and\nsecurity practices. These privacy labels contain very fine-grained information\nabout the apps' data practices such as the data types and purposes associated\nwith each data type. This provides us with a unique vantage point from which we\ncan understand apps' data practices at scale.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.0603,review,post_llm,2023,3,"{'ai_likelihood': 0.0005335277981228299, 'text': ""The Role of Large Language Models in the Recognition of Territorial\n  Sovereignty: An Analysis of the Construction of Legitimacy\n\n  We examine the potential impact of Large Language Models (LLM) on the\nrecognition of territorial sovereignty and its legitimization. We argue that\nwhile technology tools, such as Google Maps and Large Language Models (LLM)\nlike OpenAI's ChatGPT, are often perceived as impartial and objective, this\nperception is flawed, as AI algorithms reflect the biases of their designers or\nthe data they are built on. We also stress the importance of evaluating the\nactions and decisions of AI and multinational companies that offer them, which\nplay a crucial role in aspects such as legitimizing and establishing ideas in\nthe collective imagination. Our paper highlights the case of three\ncontroversial territories: Crimea, West Bank and Transnitria, by comparing the\nresponses of ChatGPT against Wikipedia information and United Nations\nresolutions. We contend that the emergence of AI-based tools like LLMs is\nleading to a new scenario in which emerging technology consolidates power and\ninfluences our understanding of reality. Therefore, it is crucial to monitor\nand analyze the role of AI in the construction of legitimacy and the\nrecognition of territorial sovereignty.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.12909,review,post_llm,2023,3,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Ethics in Computing Education: Challenges and Experience with Embedded\n  Ethics\n\n  The next generation of computer engineers and scientists must be proficient\nin not just the technical knowledge required to analyze, optimize, and create\nemerging microelectronics systems, but also with the skills required to make\nethical decisions during design. Teaching computer ethics in computing\ncurricula is therefore becoming an important requirement with significant\nramifications for our increasingly connected and computing-reliant society. In\nthis paper, we reflect on the many challenges and questions with effectively\nintegrating ethics into modern computing curricula. We describe a case study of\nintegrating ethics modules into the computer engineering curricula at Colorado\nState University.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.11074,review,post_llm,2023,3,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Generative AI and the Digital Commons\n\n  Many generative foundation models (or GFMs) are trained on publicly available\ndata and use public infrastructure, but 1) may degrade the ""digital commons""\nthat they depend on, and 2) do not have processes in place to return value\ncaptured to data producers and stakeholders. Existing conceptions of data\nrights and protection (focusing largely on individually-owned data and\nassociated privacy concerns) and copyright or licensing-based models offer some\ninstructive priors, but are ill-suited for the issues that may arise from\nmodels trained on commons-based data. We outline the risks posed by GFMs and\nwhy they are relevant to the digital commons, and propose numerous\ngovernance-based solutions that include investments in standardized\ndataset/model disclosure and other kinds of transparency when it comes to\ngenerative models\' training and capabilities, consortia-based funding for\nmonitoring/standards/auditing organizations, requirements or norms for GFM\ncompanies to contribute high quality data to the commons, and structures for\nshared ownership based on individual or community provision of fine-tuning\ndata.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.00357,review,post_llm,2023,3,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Collective moderation of hate, toxicity, and extremity in online discussions\n\nIn the digital age, hate speech poses a threat to the functioning of social media platforms as spaces for public discourse. Top-down approaches to moderate hate speech encounter difficulties due to conflicts with freedom of expression and issues of scalability. Counter speech, a form of collective moderation by citizens, has emerged as a potential remedy. Here, we aim to investigate which counter speech strategies are most effective in reducing the prevalence of hate, toxicity, and extremity on online platforms. We analyze more than 130,000 discussions on German Twitter starting at the peak of the migrant crisis in 2015 and extending over four years. We use human annotation and machine learning classifiers to identify argumentation strategies, ingroup and outgroup references, emotional tone, and different measures of discourse quality. Using matching and time-series analyses we discern the effectiveness of naturally observed counter speech strategies on the micro-level (individual tweet pairs), meso-level (entire discussions) and macro-level (over days). We find that expressing straightforward opinions, even if not factual but devoid of insults, results in the least subsequent hate, toxicity, and extremity over all levels of analyses. This strategy complements currently recommended counter speech strategies and is easy for citizens to engage in. Sarcasm can also be effective in improving discourse quality, especially in the presence of organized extreme groups. Going beyond one-shot analyses on smaller samples prevalent in most prior studies, our findings have implications for the successful management of public online spaces through collective civic moderation.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06509,review,post_llm,2023,3,"{'ai_likelihood': 3.940529293484158e-06, 'text': 'Practices and challenges in clinical data sharing\n\n  The debate on data access and privacy is an ongoing one. It is kept alive by\nthe never-ending changes/upgrades in (i) the shape of the data collected (in\nterms of size, diversity, sensitivity and quality), (ii) the laws governing\ndata sharing, (iii) the amount of free public data available on individuals\n(social media, blogs, population-based databases, etc.), as well as (iv) the\navailable privacy enhancing technologies. This paper identifies current\ndirections, challenges and best practices in constructing a clinical\ndata-sharing framework for research purposes. Specifically, we create a\ntaxonomy for the framework, identify the design choices available within each\ntaxon, and demonstrate thew choices using current legal frameworks. The purpose\nis to devise best practices for the implementation of an effective, safe and\ntransparent research access framework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.05345,regular,post_llm,2023,3,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""TGDataset: Collecting and Exploring the Largest Telegram Channels\n  Dataset\n\n  Telegram is one of the most popular instant messaging apps in today's digital\nage. In addition to providing a private messaging service, Telegram, with its\nchannels, represents a valid medium for rapidly broadcasting content to a large\naudience (COVID-19 announcements), but, unfortunately, also for disseminating\nradical ideologies and coordinating attacks (Capitol Hill riot). This paper\npresents the TGDataset, a new dataset that includes 120,979 Telegram channels\nand over 400 million messages, making it the largest collection of Telegram\nchannels to the best of our knowledge. After a brief introduction to the data\ncollection process, we analyze the languages spoken within our dataset and the\ntopic covered by English channels. Finally, we discuss some use cases in which\nour dataset can be extremely useful to understand better the Telegram\necosystem, as well as to study the diffusion of questionable news. In addition\nto the raw dataset, we released the scripts we used to analyze the dataset and\nthe list of channels belonging to the network of a new conspiracy theory called\nSabmyk.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.06292,regular,post_llm,2023,3,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Multi-view shaker detection: Insights from a noise-immune influence\n  analysis Perspective\n\n  Entities whose changes will significantly affect others in a networked system\nare called shakers. In recent years, some models have been proposed to detect\nsuch shaker from evolving entities. However, limited work has focused on shaker\ndetection in very short term, which has many real-world applications. For\nexample, in financial market, it can enable both investors and governors to\nquickly respond to rapid changes. Under the short-term setting, conventional\nmethods may suffer from limited data sample problems and are sensitive to\ncynical manipulations, leading to unreliable results. Fortunately, there are\nmulti-attribute evolution records available, which can provide compatible and\ncomplementary information. In this paper, we investigate how to learn reliable\ninfluence results from the short-term multi-attribute evolution records. We\ncall entities with consistent influence among different views in short term as\nmulti-view shakers and study the new problem of multi-view shaker detection. We\nidentify the challenges as follows: (1) how to jointly detect short-term\nshakers and model conflicting influence results among different views? (2) how\nto filter spurious influence relation in each individual view for robust\ninfluence inference? In response, a novel solution, called Robust Influence\nNetwork from a noise-immune influence analysis perspective is proposed, where\nthe possible outliers are well modelled jointly with multi-view shaker\ndetection task. More specifically, we learn the influence relation from each\nview and transform influence relation from different views into an intermediate\nrepresentation. In the meantime, we uncover both the inconsistent and spurious\noutliers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.13684,regular,post_llm,2023,3,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Contextual Integrity of A Virtual (Reality) Classroom\n\n  The multicontextual nature of immersive VR makes it difficult to ensure\ncontextual integrity of VR-generated information flows using existing privacy\ndesign and policy mechanisms. In this position paper, we call on the HCI\ncommunity to do away with lengthy disclosures and permissions models and move\ntowards embracing privacy mechanisms rooted in Contextual Integrity theory.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.09602,regular,post_llm,2023,3,"{'ai_likelihood': 1.8477439880371094e-05, 'text': 'Um banco de dados de empregos formais georreferenciados em cidades\n  brasileiras\n\n  Currently, transport planning has changed its paradigm from projects oriented\nto guarantee service levels to projects oriented to guarantee accessibility to\nopportunities. In this context, a number of studies and tools aimed at\ncalculating accessibility are being made available, however these tools depend\non job location data that are not always easily accessible. Thus, this work\nproposes the creation of a database with the locations of formal jobs in\nBrazilian cities. The method uses the RAIS jobs database and the CNEFE street\nfaces database to infer the location of jobs in urban regions from the zip code\nand the number of non-residential addresses on street faces. As a result, jobs\ncan be located more accurately in large and medium-sized cities and\napproximately in single zip code cities. Finally, the databases are made\navailable openly so that researchers and planning professionals can easily\napply accessibility analyzes throughout the national territory.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.11146,review,post_llm,2023,3,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready\n  to Obtain a University Degree?\n\n  In late 2022, OpenAI released a new version of ChatGPT, a sophisticated\nnatural language processing system capable of holding natural conversations\nwhile preserving and responding to the context of the discussion. ChatGPT has\nexceeded expectations in its abilities, leading to extensive considerations of\nits potential applications and misuse. In this work, we evaluate the influence\nof ChatGPT on university education, with a primary focus on computer\nsecurity-oriented specialization. We gather data regarding the effectiveness\nand usability of this tool for completing exams, programming assignments, and\nterm papers. We evaluate multiple levels of tool misuse, ranging from utilizing\nit as a consultant to simply copying its outputs. While we demonstrate how\neasily ChatGPT can be used to cheat, we also discuss the potentially\nsignificant benefits to the educational system. For instance, it might be used\nas an aid (assistant) to discuss problems encountered while solving an\nassignment or to speed up the learning process. Ultimately, we discuss how\ncomputer science higher education should adapt to tools like ChatGPT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06123,regular,post_llm,2023,3,"{'ai_likelihood': 0.99755859375, 'text': 'The Impact of Large Language Multi-Modal Models on the Future of Job\n  Market\n\n  The rapid advancements in artificial intelligence, particularly in large\nlanguage multi-modal models like GPT-4, have raised concerns about the\npotential displacement of human workers in various industries. This position\npaper aims to analyze the current state of job replacement by AI models and\nexplores potential implications and strategies for a balanced coexistence\nbetween AI and human workers.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.028839111328125, 'GPT4': 0.0015087127685546875, 'CLAUDE': 0.0238037109375, 'GOOGLE': 0.9169921875, 'OPENAI_O_SERIES': 0.00026988983154296875, 'DEEPSEEK': 3.522634506225586e-05, 'GROK': 1.9729137420654297e-05, 'NOVA': 0.0004067420959472656, 'OTHER': 0.0277252197265625, 'HUMAN': 0.0003161430358886719}}"
2303.11221,regular,post_llm,2023,3,"{'ai_likelihood': 2.8477774726019966e-06, 'text': ""Towards Goldilocks Zone in Child-centered AI\n\n  Using YouTube Kids as an example, in this work, we argue the need to\nunderstand a child's interaction process with AI and its broader implication on\na child's emotional, social, and creative development. We present several\ndesign recommendations to create value-driven interaction in child-centric AI\nthat can guide designing compelling, age-appropriate, beneficial AI experiences\nfor children.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.13943,review,post_llm,2023,3,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Evaluating the impact of government Cyber Security initiatives in the UK\n\n  Cyber security initiatives provide immense opportunities for governments to\neducate, train, create awareness, and promote cyber hygiene among businesses\nand the general public. Creating and promoting these initiatives are necessary\nsteps governments take to ensure the cyber health of a nation. To ensure users\nare safe and confident, especially online, the UK government has created\ninitiatives designed to meet the needs of various users such as small charity\nguide for charity organisations, small business guide for small businesses, get\nsafe online for the general public, and cyber essentials for organisations,\namong many others. However, ensuring that these initiatives deliver on their\nobjectives can be daunting, especially when reaching out to the whole\npopulation. It is, therefore, vital for the government to intensify practical\nways of reaching out to users to make sure that they are aware of their\nobligation to cyber security. This study evaluates sixteen of the UK\ngovernment's cyber security initiatives and discovers four notable reasons why\nthese initiatives are failing. These reasons are insufficient awareness and\ntraining, non-evaluation of initiatives to measure impact, insufficient\nbehavioural change, and limited coverage to reach intended targets. The\nrecommendation based on these findings is to promote these initiatives both\nnationally and at community levels.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.06007,review,post_llm,2023,3,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Sustainability Analysis Framework for On-Demand Public Transit Systems\n\n  There is an increased interest from transit agencies to replace fixed-route\ntransit services with on-demand public transits (ODT). However, it is still\nunclear when and where such a service is efficient and sustainable. To this\nend, we provide a comprehensive framework for assessing the sustainability of\nODT systems from the perspective of overall efficiency, environmental\nfootprint, and social equity and inclusion. The proposed framework is\nillustrated by applying it to the Town of Innisfil, Ontario, where an ODT\nsystem has been implemented since 2017. It can be concluded that when there is\nadequate supply and no surge pricing, crowdsourced ODTs are the most\ncost-effective transit system when the demand is below 3.37 riders/km2/day.\nWith surge pricing applied to crowdsourced ODTs, hybrid systems become the most\ncost-effective transit solution when demand ranges between 1.18 and 3.37\nriders/km2/day. The use of private vehicles is more environmentally sustainable\nthan providing public transit service at all demand levels below 3.37\nriders/km2/day. However, the electrification of the public transit fleet along\nwith optimized charging strategies can reduce total yearly GHG emissions by\nmore than 98%. Furthermore, transit systems have similar equity distributions\nfor waiting and in-vehicle travel times.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2303.06642,review,post_llm,2023,3,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Frugal Computing -- On the need for low-carbon and sustainable computing\n  and the path towards zero-carbon computing\n\n  The current emissions from computing are almost 4% of the world total. This\nis already more than emissions from the airline industry and are projected to\nrise steeply over the next two decades. By 2040 emissions from computing alone\nwill account for more than half of the emissions budget to keep global warming\nbelow 1.5$^\\circ$C. Consequently, this growth in computing emissions is\nunsustainable. The emissions from production of computing devices exceed the\nemissions from operating them, so even if devices are more energy efficient\nproducing more of them will make the emissions problem worse. Therefore we must\nextend the useful life of our computing devices. As a society we need to start\ntreating computational resources as finite and precious, to be utilised only\nwhen necessary, and as effectively as possible. We need frugal computing:\nachieving our aims with less energy and material.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.127,review,post_llm,2023,4,"{'ai_likelihood': 4.106097751193576e-06, 'text': 'The Participation Game\n\n  Inspired by Turing\'s famous ""imitation game"" and recent advances in\ngenerative pre-trained transformers, we pose the participation game to point to\na new frontier in AI evolution where machines will join with humans as\nparticipants in social construction processes. The participation game is a\ncreative, playful competition that calls for applying, bending, and stretching\nthe categories humans use to make sense of and order their worlds. After\ndefining the game and giving reasons for moving beyond imitation as a test of\nAI, we highlight parallels between the participation game and processes of\nsocial construction, a hallmark of human intelligence. We then discuss\nimplications for fundamental constructs of societies and options for\ngovernance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.04101,review,post_llm,2023,4,"{'ai_likelihood': 1.0, 'text': ""Challenges of Blockchain Applications in Digital Health: A Systematic\n  Review\n\n  Digital health, an emerging field integrating digital technologies into\nhealthcare, is rapidly evolving and holds the potential to transform medical\npractices. Blockchain technology has garnered significant attention as a\npotential solution to various issues within digital health, including data\nsecurity, automation, interoperability, and patient data ownership. However,\ndespite the numerous advantages, blockchain faces several challenges and\nunknowns that must be addressed. This systematic literature review aims to\nexplore the challenges of blockchain applications in digital health and provide\nbest practices to overcome current and future roadblocks. Key issues identified\ninclude regulatory compliance, energy consumption, network effects, data\nstandards, and the accessibility of the technology to stakeholders. To ensure\nthe successful integration of blockchain within digital health, it is crucial\nto collaborate with healthcare stakeholders, pursue continued research and\ninnovation, and engage in open discussions about the technology's limitations\nand potential.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1455078125, 'GPT4': 0.2802734375, 'CLAUDE': 0.0031375885009765625, 'GOOGLE': 0.52978515625, 'OPENAI_O_SERIES': 0.0005240440368652344, 'DEEPSEEK': 1.1920928955078125e-05, 'GROK': 3.361701965332031e-05, 'NOVA': 0.00015437602996826172, 'OTHER': 0.0406494140625, 'HUMAN': 0.0002987384796142578}}"
2305.00199,regular,post_llm,2023,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Large-Scale Assessment of Labour Market Dynamics in China during the\n  COVID-19 Pandemic\n\n  The outbreak of the COVID-19 pandemic has had an unprecedented impact on\nChina's labour market, and has largely changed the structure of labour supply\nand demand in different regions. It becomes critical for policy makers to\nunderstand the emerging dynamics of the post-pandemic labour market and provide\nthe right policies for supporting the sustainable development of regional\neconomies. To this end, in this paper, we provide a data-driven approach to\nassess and understand the evolving dynamics in regions' labour markets with\nlarge-scale online job search queries and job postings. In particular, we model\nthe spatial-temporal patterns of labour flow and labour demand which reflect\nthe attractiveness of regional labour markets. Our analysis shows that regional\nlabour markets suffered from dramatic changes and demonstrated unusual signs of\nrecovery during the pandemic. Specifically, the intention of labour flow\nquickly recovered with a trend of migrating from large to small cities and from\nnorthern to southern regions, respectively. Meanwhile, due to the pandemic, the\ndemand of blue-collar workers has been substantially reduced compared to that\nof white-collar workers. In addition, the demand structure of blue-collar jobs\nalso changed from manufacturing to service industries. Our findings reveal that\nthe pandemic can cause varied impacts on regions with different structures of\nlabour demand and control policies. This analysis provides timely information\nfor both individuals and organizations in confronting the dynamic change in job\nmarkets during the extreme events, such as pandemics. Also, the governments can\nbe better assisted for providing the right policies on job markets in\nfacilitating the sustainable development of regions' economies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.02885,review,post_llm,2023,4,"{'ai_likelihood': 2.9802322387695312e-06, 'text': ""Connected and Automated Vehicles Investment and Smart Infrastructure in\n  Tennessee Part 3: Infrastructure and Vehicular communications: From Dedicated\n  Short-Range Communications to Cellular Vehicle-to-Everything\n\n  This report aims to support the Tennessee Department of Transportation's\ndecisions about vehicle and infrastructure communication technologies. The\ntransition from Dedicated Short-Range communication (DSRC) V2X to Cellular\nVehicle to Everything (C-V2X) is explored using USDOT guidance on relevant\nissues and presenting the results of experimentation in Tennessee and the\npotential pros and cons. DSRC V2X technology has been planned at traffic signal\nin Tennessee, e.g., 152 Roadside Units (RSUs) were planned by TDOT using DSRC\nV2X and Bluetooth combination units in the I-24 smart corridor. Similarly, many\npilot programs and testbeds around the nation have deployed DSRC V2X technology\nand are now impacted by the Federal Communication Commission's (FCC) ruling on\nopening safety band. The implication is that DSRC V2X deployments (and future\ndeployments) should migrate to C-V2X. Notably, dual-mode RSUs are available\nalong with LTE C-V2X. The transition can be done by working with vendors, but\nsurely this involves more than swapping DSRC V2X devices with LTE C-V2X\ndevices. Complicating the migration to C-V2X is TDOT's role in traffic signal\noperations and maintenance, which is limited to funding and\ndesigning/construction of traffic signals, but local agencies operate and\nmaintain signals. Hence, local agencies will work with TDOT to operate and\nmaintain C-V2X technology. Moreover, C-V2X technologies are not widely\ntested-interference by unlicensed devices and channel congestion can adversely\naffect safety-critical applications. Given the substantial uncertainties in\ntransitioning to these technologies, TDOT's discussion with IOOs about the\noperation and maintenance of C-V2X may have to wait for the resolution issues,\nwhile TDOT can invest in experimentation with dual-mode devices.\nRecommendations are provided about dual-mode devices, CAV data, and needed\nresearch and testing.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.11852,review,post_llm,2023,4,"{'ai_likelihood': 3.907415601942274e-06, 'text': ""Can we Trust Chatbots for now? Accuracy, reproducibility, traceability;\n  a Case Study on Leonardo da Vinci's Contribution to Astronomy\n\n  Large Language Models (LLM) are studied. Applications to chatbots and\neducation are considered. A case study on Leonardo's contribution to astronomy\nis presented. Major problems with accuracy, reproducibility and traceability of\nanswers are reported for ChatGPT, GPT-4, BLOOM and Google Bard. Possible\nreasons for problems are discussed and some solutions are proposed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.12851,review,post_llm,2023,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""Towards Smart Education through the Internet of Things: A Review\n\n  IoT is a fundamental enabling technology for creating smart spaces, which can\nassist the effective face-to-face and online education systems. The transition\nto smart education (integrating IoT and AI into the education system) is\nappealing, which has a concrete impact on learners' engagement, motivation,\nattendance, and deep learning. Traditional education faces many challenges,\nincluding administration, pedagogy, assessment, and classroom supervision.\nRecent developments in ICT (e.g., IoT, AI and 5G, etc.) have yielded lots of\nsmart solutions for various aspects of life; however, smart solutions are not\nwell integrated into the education system. In particular, the COVID-19 pandemic\nsituation had further emphasized the adoption of new smart solutions in\neducation. This study reviews the related studies and addresses the (i)\nproblems in the traditional education system with possible solutions, (ii) the\ntransition towards smart education, and (iii) research challenges in the\ntransition to smart education (i.e, computational and social resistance).\nConsidering these studies, smart solutions (e.g., smart pedagogy, smart\nassessment, smart classroom, smart administration, etc.) are introduced to the\nproblems of the traditional system. This exploratory study opens new trends for\nscholars and the market to integrate ICT, IoT, and AI into smart education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.05312,regular,post_llm,2023,4,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Fingerprint Liveness Detection using Minutiae-Independent Dense Sampling\n  of Local Patches\n\n  Fingerprint recognition and matching is a common form of user authentication.\nWhile a fingerprint is unique to each individual, authentication is vulnerable\nwhen an attacker can forge a copy of the fingerprint (spoof). To combat these\nspoofed fingerprints, spoof detection and liveness detection algorithms are\ncurrently being researched as countermeasures to this security vulnerability.\nThis paper introduces a fingerprint anti-spoofing mechanism using machine\nlearning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.00903,regular,post_llm,2023,4,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Adoption of Artificial Intelligence in Schools: Unveiling Factors\n  Influencing Teachers Engagement\n\n  Albeit existing evidence about the impact of AI-based adaptive learning\nplatforms, their scaled adoption in schools is slow at best. In addition, AI\ntools adopted in schools may not always be the considered and studied products\nof the research community. Therefore, there have been increasing concerns about\nidentifying factors influencing adoption, and studying the extent to which\nthese factors can be used to predict teachers engagement with adaptive learning\nplatforms. To address this, we developed a reliable instrument to measure more\nholistic factors influencing teachers adoption of adaptive learning platforms\nin schools. In addition, we present the results of its implementation with\nschool teachers (n=792) sampled from a large country-level population and use\nthis data to predict teachers real-world engagement with the adaptive learning\nplatform in schools. Our results show that although teachers knowledge,\nconfidence and product quality are all important factors, they are not\nnecessarily the only, may not even be the most important factors influencing\nthe teachers engagement with AI platforms in schools. Not generating any\nadditional workload, in-creasing teacher ownership and trust, generating\nsupport mechanisms for help, and assuring that ethical issues are minimised,\nare also essential for the adoption of AI in schools and may predict teachers\nengagement with the platform better. We conclude the paper with a discussion on\nthe value of factors identified to increase the real-world adoption and\neffectiveness of adaptive learning platforms by increasing the dimensions of\nvariability in prediction models and decreasing the implementation variability\nin practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06039,review,post_llm,2023,4,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""A new perspective on the prediction of the innovation performance: A\n  data driven methodology to identify innovation indicators through a\n  comparative study of Boston's neighborhoods\n\n  In an era of knowledge-based economy, commercialized research and globalized\ncompetition for talent, the creation of innovation ecosystems and innovation\nnetworks is at the forefront of efforts of cities. In this context, public\nauthorities, private organizations, and academics respond to the question of\nthe most promising indicators that can predict innovation with various\ninnovation scoreboards. The current paper aims at increasing the understanding\nof the existing indicators and complementing the various innovation assessment\ntoolkits, using large datasets from non-traditional sources. The success of\nboth top down implemented innovation districts and community-level innovation\necosystems is complex and has not been well examined. Yet, limited data shed\nlight on the association between indicators and innovation performance at the\nneighborhood level. For this purpose, the city of Boston has been selected as a\ncase study to reveal the importance of its neighborhood's different\ncharacteristics in achieving high innovation performance. The study uses a\nlarge geographically distributed dataset across Boston's 35 zip code areas,\nwhich contains various business, entrepreneurial-specific, socio-economic data\nand other types of data that can reveal contextual urban dimensions.\nFurthermore, in order to express the innovation performance of the zip code\nareas, new metrics are proposed connected to innovation locations. The outcomes\nof this analysis aim to introduce a 'Neighborhood Innovation Index' that will\ngenerate new planning models for higher innovation performance, which can be\neasily applied in other cases. By publishing this large-scale dataset of urban\ninformatics, the goal is to contribute to the innovation discourse and enable a\nnew theoretical framework that identifies the linkages among cities'\nsocio-economic characteristics and innovation performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.14921,regular,post_llm,2023,4,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'A technological framework for scalable ground-up formation of Circular\n  Societies\n\n  The Circular Economy (CE) is regarded as a solution to the environmental\ncrisis. However, mainstream CE measures skirt around challenging the ethos of\never-increasing economic growth, overlooking social impacts and\nunder-representing solutions such as reducing overall consumption. Circular\nSocieties (CS) address these concerns by challenging this ethos. They emphasize\nground-up social reorganization,address over-consumption through sufficiency\nstrategies, and highlight the need for considering the complex\ninter-dependencies between nature, society, and technology on local, regional\nand global levels. However, no blueprint exists for forming CSs. An initial\nobjective of my thesis is exploring existing social-network ontologies and\ndeveloping a broadly applicable model for CSs. Since ground-up social\nreorganization on local, regional, and global levels has compounding effects on\nnetwork complexities,a technological framework digitizing these\ninter-dependencies is necessary. Finally, adhering to CS principles of\ntransparency and democratization, a system of trust is necessary to achieve\ncollaborative consensus of the network state.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.11175,review,post_llm,2023,4,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Utilizing Online and Open-Source Machine Learning Toolkits to Leverage\n  the Future of Sustainable Engineering\n\n  Recently, there has been a national push to use machine learning (ML) and\nartificial intelligence (AI) to advance engineering techniques in all\ndisciplines ranging from advanced fracture mechanics in materials science to\nsoil and water quality testing in the civil and environmental engineering\nfields. Using AI, specifically machine learning, engineers can automate and\ndecrease the processing or human labeling time while maintaining statistical\nrepeatability via trained models and sensors. Edge Impulse has designed an\nopen-source TinyML-enabled Arduino education tool kit for engineering\ndisciplines. This paper discusses the various applications and approaches\nengineering educators have taken to utilize ML toolkits in the classroom. We\nprovide in-depth implementation guides and associated learning outcomes focused\non the Environmental Engineering Classroom. We discuss five specific examples\nof four standard Environmental Engineering courses for freshman and\njunior-level engineering. There are currently few programs in the nation that\nutilize machine learning toolkits to prepare the next generation of ML and\nAI-educated engineers for industry and academic careers. This paper will guide\neducators to design and implement ML/AI into engineering curricula (without a\nspecific AI or ML focus within the course) using simple, cheap, and open-source\ntools and technological aid from an online platform in collaboration with Edge\nImpulse.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.02201,review,post_llm,2023,4,"{'ai_likelihood': 0.0001457002427842882, 'text': 'ChatGPT in education: A discourse analysis of worries and concerns on\n  social media\n\n  The rapid advancements in generative AI models present new opportunities in\nthe education sector. However, it is imperative to acknowledge and address the\npotential risks and concerns that may arise with their use. We analyzed Twitter\ndata to identify key concerns related to the use of ChatGPT in education. We\nemployed BERT-based topic modeling to conduct a discourse analysis and social\nnetwork analysis to identify influential users in the conversation. While\nTwitter users generally ex-pressed a positive attitude towards the use of\nChatGPT, their concerns converged to five specific categories: academic\nintegrity, impact on learning outcomes and skill development, limitation of\ncapabilities, policy and social concerns, and workforce challenges. We also\nfound that users from the tech, education, and media fields were often\nimplicated in the conversation, while education and tech individual users led\nthe discussion of concerns. Based on these findings, the study provides several\nimplications for policymakers, tech companies and individuals, educators, and\nmedia agencies. In summary, our study underscores the importance of responsible\nand ethical use of AI in education and highlights the need for collaboration\namong stakeholders to regulate AI policy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.14342,regular,post_llm,2023,4,"{'ai_likelihood': 2.2517310248480903e-06, 'text': ""Thinking beyond chatbots' threat to education: Visualizations to\n  elucidate the writing and coding process\n\n  The landscape of educational practices for teaching and learning languages\nhas been predominantly centered around outcome-driven approaches. The recent\naccessibility of large language models has thoroughly disrupted these\napproaches. As we transform our language teaching and learning practices to\naccount for this disruption, it is important to note that language learning\nplays a pivotal role in developing human intelligence. Writing and computer\nprogramming are two essential skills integral to our education systems. What\nand how we write shapes our thinking and sets us on the path of self-directed\nlearning. While most educators understand that `process' and `product' are both\nimportant and inseparable, in most educational settings, providing constructive\nfeedback on a learner's formative process is challenging. For instance, it is\nstraightforward in computer programming to assess whether a learner-submitted\ncode runs. However, evaluating the learner's creative process and providing\nmeaningful feedback on the process can be challenging. To address this\nlong-standing issue in education (and learning), this work presents a new set\nof visualization tools to summarize the inherent and taught capabilities of a\nlearner's writing or programming process. These interactive Process\nVisualizations (PVs) provide insightful, empowering, and personalized\nprocess-oriented feedback to the learners. The toolbox is ready to be tested by\neducators and learners and is publicly available at www.processfeedback.org.\nFocusing on providing feedback on a learner's process--from self, peers, and\neducators--will facilitate learners' ability to acquire higher-order skills\nsuch as self-directed learning and metacognition.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.1019,regular,post_llm,2023,4,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'A Large-scale Examination of ""Socioeconomic"" Fairness in Mobile Networks\n\n  Internet access is a special resource of which needs has become universal\nacross the public whereas the service is operated in the private sector. Mobile\nNetwork Operators (MNOs) put efforts for management, planning, and\noptimization; however, they do not link such activities to socioeconomic\nfairness. In this paper, we make a first step towards understanding the\nrelation between socioeconomic status of customers and network performance, and\ninvestigate potential discrimination in network deployment and management. The\nscope of our study spans various aspects, including urban geography, network\nresource deployment, data consumption, and device distribution. A novel\nmethodology that enables a geo-socioeconomic perspective to mobile network is\ndeveloped for the study. The results are based on an actual infrastructure in\nmultiple cities, covering millions of users densely covering the socioeconomic\nscale. We report a thorough examination of the fairness status, its\nrelationship with various structural factors, and potential class specific\nsolutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.10777,regular,post_llm,2023,4,"{'ai_likelihood': 9.867880079481338e-06, 'text': ""Reddit in the Time of COVID\n\n  When the COVID-19 pandemic hit, much of life moved online. Platforms of all\ntypes reported surges of activity, and people remarked on the various important\nfunctions that online platforms suddenly fulfilled. However, researchers lack a\nrigorous understanding of the pandemic's impacts on social platforms, and\nwhether they were temporary or long-lasting. We present a conceptual framework\nfor studying the large-scale evolution of social platforms and apply it to the\nstudy of Reddit's history, with a particular focus on the COVID-19 pandemic. We\nstudy platform evolution through two key dimensions: structure vs. content and\nmacro- vs. micro-level analysis. Structural signals help us quantify how much\nbehavior changed, while content analysis clarifies exactly how it changed.\nApplying these at the macro-level illuminates platform-wide changes, while at\nthe micro-level we study impacts on individual users. We illustrate the value\nof this approach by showing the extraordinary and ordinary changes Reddit went\nthrough during the pandemic. First, we show that typically when rapid growth\noccurs, it is driven by a few concentrated communities and within a narrow\nslice of language use. However, Reddit's growth throughout COVID-19 was spread\nacross disparate communities and languages. Second, all groups were equally\naffected in their change of interest, but veteran users tended to invoke\nCOVID-related language more than newer users. Third, the new wave of users that\narrived following COVID-19 was fundamentally different from previous cohorts of\nnew users in terms of interests, activity, and likelihood of staying active on\nthe platform. These findings provide a more rigorous understanding of how an\nonline platform changed during the global pandemic.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.05274,regular,post_llm,2023,4,"{'ai_likelihood': 2.0331806606716582e-05, 'text': ""YouNICon: YouTube's CommuNIty of Conspiracy Videos\n\n  Conspiracy theories are widely propagated on social media. Among various\nsocial media services, YouTube is one of the most influential sources of news\nand entertainment. This paper seeks to develop a dataset, YOUNICON, to enable\nresearchers to perform conspiracy theory detection as well as classification of\nvideos with conspiracy theories into different topics. YOUNICON is a dataset\nwith a large collection of videos from suspicious channels that were identified\nto contain conspiracy theories in a previous study (Ledwich and Zaitsev 2020).\nOverall, YOUNICON will enable researchers to study trends in conspiracy\ntheories and understand how individuals can interact with the conspiracy theory\nproducing community or channel. Our data is available at:\nhttps://doi.org/10.5281/zenodo.7466262.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.14791,review,post_llm,2023,4,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Caught in the Game: On the History and Evolution of Web Browser Gaming\n\n  Web browsers have come a long way since their inception, evolving from a\nsimple means of displaying text documents over the network to complex software\nstacks with advanced graphics and network capabilities. As personal computers\ngrew in popularity, developers jumped at the opportunity to deploy\ncross-platform games with centralized management and a low barrier to entry.\nSimply going to the right address is now enough to start a game. From\ntext-based to GPU-powered 3D games, browser gaming has evolved to become a\nstrong alternative to traditional console and mobile-based gaming, targeting\nboth casual and advanced gamers. Browser technology has also evolved to\naccommodate more demanding applications, sometimes even supplanting functions\ntypically left to the operating system. Today, websites display rich,\ncomputationally intensive, hardware-accelerated graphics, allowing developers\nto build ever-more impressive applications and games.In this paper, we present\nthe evolution of browser gaming and the technologies that enabled it, from the\nrelease of the first text-based games in the early 1990s to current open-world\nand game-engine-powered browser games. We discuss the societal impact of\nbrowser gaming and how it has allowed a new target audience to accessdigital\ngaming. Finally, we review the potential future evolution ofthe browser gaming\nindustry.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.00456,regular,post_llm,2023,4,"{'ai_likelihood': 0.005577935112847223, 'text': ""Graph Global Attention Network with Memory for Fake News Detection\n\n  With the proliferation of social media, the detection of fake news has become\na critical issue that poses a significant threat to society. The dissemination\nof fake information can lead to social harm and damage the credibility of\ninformation. To address this issue, deep learning has emerged as a promising\napproach, especially with the development of natural language processing (NLP).\nThis study addresses the problem of detecting fake news on social media, which\nposes a significant challenge to society. This study proposes a new approach\nnamed GANM for fake news detection that employs NLP techniques to encode nodes\nfor news context and user content and uses three graph convolutional networks\nto extract features and aggregate users' endogenous and exogenous information.\nThe GANM employs a unique global attention mechanism with memory to learn the\nstructural homogeneity of news dissemination networks. The approach achieves\ngood results on a real dataset.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.1084,regular,post_llm,2023,4,"{'ai_likelihood': 4.0928522745768234e-05, 'text': ""IoT-Based Solution for Paraplegic Sufferer to Send Signals to Physician\n  via Internet\n\n  We come across hospitals and non-profit organizations that care for people\nwith paralysis who have experienced all or portion of their physique being\nincapacitated by the paralyzing attack. Due to a lack of motor coordination by\ntheir mind, these persons are typically unable to communicate their\nrequirements because they can speak clearly or use sign language. In such a\ncase, we suggest a system that enables a disabled person to move any area of\nhis body capable of moving to broadcast a text on the LCD. This method also\naddresses the circumstance in which the patient cannot be attended to in person\nand instead sends an SMS message using GSM. By detecting the user part's tilt\ndirection, our suggested system operates. As a result, patients can communicate\nwith physicians, therapists, or their loved ones at home or work over the web.\nCase-specific data, such as heart rate, must be continuously reported in health\ncenters. The suggested method tracks the body of the case's pulse rate and\nother comparable data. For instance, photoplethysmography is used to assess\nheart rate. The decoded periodic data is transmitted continually via a\nMicrocontroller coupled to a transmitting module. The croaker's cabin contains\na receiver device that obtains and deciphers data as well as constantly\nexhibits it on Graphical interfaces viewable on the laptop. As a result, the\ncroaker can monitor and handle multiple situations at once.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.01596,review,post_llm,2023,4,"{'ai_likelihood': 2.4504131740993925e-06, 'text': ""Mentions of Prejudice in News Media -- An International Comparison\n\n  Previous research has identified a post-2010 sharp increase of terms used to\ndenounce prejudice (i.e. racism, sexism, homophobia, Islamophobia,\nanti-Semitism, etc.) in U.S. and U.K. news media content. Here, we extend\nprevious analysis to an international sample of news media organizations. Thus,\nwe quantify the prevalence of prejudice-denouncing terms and social justice\nassociated terminology (diversity, inclusion, equality, etc.) in over 98\nmillion news and opinion articles across 124 popular news media outlets from 36\ncountries representing 6 different world regions: English-speaking West,\ncontinental Europe, Latin America, sub-Saharan Africa, Persian Gulf region and\nAsia. We find that the post-2010 increasing prominence in news media of the\nstudied terminology is not circumscribed to the U.S. and the U.K. but rather\nappears to be a mostly global phenomenon starting in the first half of the\n2010s decade in pioneering countries yet largely prevalent around the globe\npost-2015. However, different world regions' news media emphasize distinct\ntypes of prejudice with varying degrees of intensity. We find no evidence of\nU.S. news media having been first in the world in increasing the frequency of\nprejudice coverage in their content. The large degree of temporal synchronicity\nwith which the studied set of terms increased in news media across a vast\nmajority of countries raises important questions about the root causes driving\nthis phenomenon.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.03151,review,post_llm,2023,4,"{'ai_likelihood': 6.4240561591254345e-06, 'text': 'Assessing VoD pressure on network power consumption\n\n  Assessing the energy consumption or carbon footprint of data distribution of\nvideo streaming services is usually carried out through energy or carbon\nintensity figures (in Wh or gCO2e per GB). In this paper, we first review the\nreasons why such approaches are likely to lead to misunderstandings and\npotentially to erroneous conclusions. To overcome those shortcomings, we\npropose a new methodology whose key idea is to consider a video streaming usage\nat the whole scale of a territory, and evaluate the impact of this usage on the\nnetwork infrastructure. At the core of our methodology is a parametric model of\na simplified network and Content Delivery Network (CDN) infrastructure, which\nis automatically scaled according to peak usage needs. This allows us to\ncompare the power consumption of this infrastructure under different scenarios,\nranging from a sober baseline to a generalized use of high bitrate videos. Our\nresults show that classical efficiency indicators do not reflect the power\nconsumption increase of more intensive Internet usage, and might even lead to\nmisleading conclusions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.08923,review,post_llm,2023,4,"{'ai_likelihood': 7.64926274617513e-06, 'text': 'Questioning the impact of AI and interdisciplinarity in science: Lessons\n  from COVID-19\n\n  Artificial intelligence (AI) has emerged as one of the most promising\ntechnologies to support COVID-19 research, with interdisciplinary\ncollaborations between medical professionals and AI specialists being actively\nencouraged since the early stages of the pandemic. Yet, our analysis of more\nthan 10,000 papers at the intersection of COVID-19 and AI suggest that these\ncollaborations have largely resulted in science of low visibility and impact.\nWe show that scientific impact was not determined by the overall\ninterdisciplinarity of author teams, but rather by the diversity of knowledge\nthey actually harnessed in their research. Our results provide insights into\nthe ways in which team and knowledge structure may influence the successful\nintegration of new computational technologies in the sciences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.14577,review,post_llm,2023,4,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Toward an Ethics of AI Belief\n\n  In this paper we, an epistemologist and a machine learning scientist, argue\nthat we need to pursue a novel area of philosophical research in AI - the\nethics of belief for AI. Here we take the ethics of belief to refer to a field\nat the intersection of epistemology and ethics concerned with possible moral,\npractical, and other non-truth-related dimensions of belief. In this paper we\nwill primarily be concerned with the normative question within the ethics of\nbelief regarding what agents - both human and artificial - ought to believe,\nrather than with questions concerning whether beliefs meet certain evaluative\nstandards such as being true, being justified, constituting knowledge, etc. We\nsuggest four topics in extant work in the ethics of (human) belief that can be\napplied to an ethics of AI belief: doxastic wronging by AI (morally wronging\nsomeone in virtue of beliefs held about them); morally owed beliefs (beliefs\nthat agents are morally obligated to hold); pragmatic and moral encroachment\n(cases where the practical or moral features of a belief is relevant to its\nepistemic status, and in our case specifically to whether an agent ought to\nhold the belief); and moral responsibility for AI beliefs. We also indicate two\nrelatively nascent areas of philosophical research that haven't yet been\ngenerally recognized as ethics of AI belief research, but that do fall within\nthis field of research in virtue of investigating various moral and practical\ndimensions of belief: the epistemic and ethical decolonization of AI; and\nepistemic injustice in AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.02108,review,post_llm,2023,4,"{'ai_likelihood': 1.9172827402750652e-05, 'text': 'Explanatory Publics: Explainability and Democratic Thought\n\n  In order to legitimate and defend democratic politics under conditions of\ncomputational capital, my aim is to contribute a notion of what I am calling\nexplanatory publics. I will explore what is at stake when we question the\nsocial and political effects of the disruptive technologies, networks and\nvalues that are hidden within the ""black boxes"" of computational systems. By\n""explanatory publics"", I am gesturing to the need for frameworks of knowledge -\nwhether social, political, technical, economic, or cultural - to be justified\nthrough a social right to explanation. That is, for a polity to be considered\ndemocratic, it must ensure that its citizens are able to develop a capacity for\nexplanatory thought (in addition to other capacities), and, thereby, able to\nquestion ideas, practices, and institutions in society. This is to extend the\nnotion of a public sphere where citizens are able to question ideas, practices,\nand institutions in society more generally. But it also adds the corollary that\ncitizens can demand explanatory accounts from institutions and, crucially, the\ndigital technologies that they use.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.11271,review,post_llm,2023,4,"{'ai_likelihood': 3.1259324815538195e-05, 'text': ""Scientists' Warning on Technology\n\n  In the past several years, scientists have issued a series of warnings about\nthe threats of climate change and other forms of environmental disruption.\nHere, we provide a scientists' warning on how technology affects these issues.\nTechnology simultaneously provides substantial benefits for humanity, and also\nprofound costs. Current technological systems are exacerbating climate change\nand the wholesale conversion of the Earth's ecosystems. Adopting new\ntechnologies, such as clean energy technologies and artificial intelligence,\nmay be necessary for addressing these crises. Such transformation is not\nwithout risks, but it may help set human civilizations on a path to a\nsustainable future.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.05143,regular,post_llm,2023,4,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Measuring Teachers' Visual Expertise Using the Gaze Relational Index\n  Based on Real-world Eye-tracking Data and Varying Velocity Thresholds\n\n  This article adds to the understanding of teachers' visual expertise by\nmeasuring visual information processing in real-world classrooms (mobile\neye-tracking) with the newly introduced Gaze Relational Index (GRI) metric,\nwhich is defined as the ratio of mean fixation duration to mean fixation\nnumber. In addition, the aim was to provide a methodological contribution to\nfuture research by showing to what extent the selected configurations (i.e.\nvarying velocity thresholds and fixation merging) of the eye movement event\ndetection algorithm for detecting fixations and saccades influence the results\nof eye-tracking studies. Our study leads to two important take-home messages:\nFirst, by following a novice-expert paradigm (2 novice teachers & 2 experienced\nteachers), we found that the GRI can serve as a sensitive measure of visual\nexpertise. As hypothesized, experienced teachers' GRI was lower, suggesting\nthat their more fine-graded organization of domain-specific knowledge allows\nthem to fixate more rapidly and frequently in the classroom. Second, we found\nthat the selected velocity threshold parameter alter and, in the worst case,\nbias the results of an eye-tracking study. Therefore, in the interest of\nfurther generalizability of the results within visual expertise research, we\nemphasize that it is highly important to report configurations that are\nrelevant for the identification of eye movements.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.07737,review,post_llm,2023,4,"{'ai_likelihood': 3.265009986029731e-05, 'text': ""Pitfalls in Effective Knowledge Management: Insights from an International Information Technology Organization\n\nKnowledge is considered an essential resource for organizations. For organizations to benefit from their possessed knowledge, knowledge needs to be managed effectively. Despite knowledge sharing and management being viewed as important by practitioners, organizations fail to benefit from their knowledge, leading to issues in cooperation and the loss of valuable knowledge with departing employees. This study aims to identify hindering factors that prevent individuals from effectively sharing and managing knowledge and understand how to eliminate these factors. Empirical data were collected through semi-structured group interviews from 50 individuals working in an international large IT organization. This study confirms the existence of a gap between the perceived importance of knowledge management and how little this importance is reflected in practice. Several hindering factors were identified, grouped into personal social topics, organizational social topics, technical topics, environmental topics, and interrelated social and technical topics. The presented recommendations for mitigating these hindering factors are focused on improving employees' actions, such as offering training and guidelines to follow. The findings of this study have implications for organizations in knowledge-intensive fields, as they can use this knowledge to create knowledge sharing and management strategies to improve their overall performance."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.10911,regular,post_llm,2023,4,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Policy design in data economy: In need for a public online news\n  (eco)system?\n\n  Socio-technical design embeds social investigations and inquiries into\n(Information) Technology Design processes. In this position paper, we propose,\nby using the aforementioned approach the design of technology and policies can\nsimultaneously inform each other. Additionally we present data economy and\nparticularly anchored online journalism platforms as use cases of policy need\nand design potentials.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.0853,review,post_llm,2023,4,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""Popular Support for Balancing Equity and Efficiency in Resource\n  Allocation: A Case Study in Online Advertising to Increase Welfare Program\n  Awareness\n\n  Algorithmically optimizing the provision of limited resources is commonplace\nacross domains from healthcare to lending. Optimization can lead to efficient\nresource allocation, but, if deployed without additional scrutiny, can also\nexacerbate inequality. Little is known about popular preferences regarding\nacceptable efficiency-equity trade-offs, making it difficult to design\nalgorithms that are responsive to community needs and desires. Here we examine\nthis trade-off and concomitant preferences in the context of GetCalFresh, an\nonline service that streamlines the application process for California's\nSupplementary Nutrition Assistance Program (SNAP, formerly known as food\nstamps). GetCalFresh runs online advertisements to raise awareness of their\nmultilingual SNAP application service. We first demonstrate that when ads are\noptimized to garner the most enrollments per dollar, a disproportionately small\nnumber of Spanish speakers enroll due to relatively higher costs of non-English\nlanguage advertising. Embedding these results in a survey (N = 1,532) of a\ndiverse set of Americans, we find broad popular support for valuing equity in\naddition to efficiency: respondents generally preferred reducing total\nenrollments to facilitate increased enrollment of Spanish speakers. These\nresults buttress recent calls to reevaluate the efficiency-centric paradigm\npopular in algorithmic resource allocation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.11324,regular,post_llm,2023,4,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'An Investigation of Face Mask Use with Busking Videos on YouTube during\n  COVID-19: a Case Study in South Korea\n\n  Wearing face mask is an effective measure to reduce the risk of COVID-19\ninfections and control its transmission, thus its usage survey is important for\nbetter policy decision to mitigate the epidemic spread. Current existing\nworldwide surveys are mostly self-reported, whose accuracies are hard to\nguaranteed, and may exaggerate the percentage of face mask wearing. Therefore,\nwe collected busking videos with a large amount on YouTube from December 2019\nto December 2020, mainly from South Korea, and reported an objective\ninvestigation of face mask use in the crowds outdoor. It is found that the face\nmask wearing rate has an obvious positive correlation with effective\nreproductive number (Rt) in the South Korea, which indicates that the people in\nSouth Korea kept sensitive to the COVID-19 epidemic. The face mask wearing rate\nin South Korea is higher than some other countries, and two rate droppings in\nJune and September also corresponds to the temporary remission in 2020. This\nstudy shows significant potentials to utilize public big video data to make an\naccurate worldwide survey of face mask use with the support of deep learning\ntechnology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.07683,review,post_llm,2023,4,"{'ai_likelihood': 1.0, 'text': 'Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources,\n  Impacts, And Mitigation Strategies\n\n  The significant advancements in applying Artificial Intelligence (AI) to\nhealthcare decision-making, medical diagnosis, and other domains have\nsimultaneously raised concerns about the fairness and bias of AI systems. This\nis particularly critical in areas like healthcare, employment, criminal\njustice, credit scoring, and increasingly, in generative AI models (GenAI) that\nproduce synthetic media. Such systems can lead to unfair outcomes and\nperpetuate existing inequalities, including generative biases that affect the\nrepresentation of individuals in synthetic data. This survey paper offers a\nsuccinct, comprehensive overview of fairness and bias in AI, addressing their\nsources, impacts, and mitigation strategies. We review sources of bias, such as\ndata, algorithm, and human decision biases - highlighting the emergent issue of\ngenerative AI bias where models may reproduce and amplify societal stereotypes.\nWe assess the societal impact of biased AI systems, focusing on the\nperpetuation of inequalities and the reinforcement of harmful stereotypes,\nespecially as generative AI becomes more prevalent in creating content that\ninfluences public perception. We explore various proposed mitigation\nstrategies, discussing the ethical considerations of their implementation and\nemphasizing the need for interdisciplinary collaboration to ensure\neffectiveness. Through a systematic literature review spanning multiple\nacademic disciplines, we present definitions of AI bias and its different\ntypes, including a detailed look at generative AI bias. We discuss the negative\nimpacts of AI bias on individuals and society and provide an overview of\ncurrent approaches to mitigate AI bias, including data pre-processing, model\nselection, and post-processing. We emphasize the unique challenges presented by\ngenerative AI models and the importance of strategies specifically tailored to\naddress these.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 2.384185791015625e-07, 'CLAUDE': 1.7881393432617188e-07, 'GOOGLE': 1.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 5.960464477539063e-08, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 0.0}}"
2304.11893,review,post_llm,2023,4,"{'ai_likelihood': 1.1854701571994357e-05, 'text': ""The Design and Implementation of a National AI Platform for Public\n  Healthcare in Italy: Implications for Semantics and Interoperability\n\n  The Italian National Health Service is adopting Artificial Intelligence\nthrough its technical agencies, with the twofold objective of supporting and\nfacilitating the diagnosis and treatment. Such a vast programme requires\nspecial care in formalising the knowledge domain, leveraging domain-specific\ndata spaces and addressing data governance issues from an interoperability\nperspective. The healthcare data governance and interoperability legal\nframework is characterised by the interplay of different pieces of legislation.\nData law is the first to be taken into proper account. It primarily includes\nthe GDPR, the Data Governance Act, and the Open Data Directive. Also, the Data\nAct and the European Health Data Space proposals will have an impact on health\ndata sharing and therefore must be considered as well. The platform developed\nby the Italian NHL will have to be integrated in a harmonised manner with the\nsystems already used in the healthcare system and with the digital assets (data\nand software) used by healthcare professionals. Questions have been raised\nabout the impact that AI could have on patients, practitioners, and health\nsystems, as well as about its potential risks; therefore, all the parties\ninvolved are called to agree upon to express a common view based on the dual\npurpose of improving people's quality of life and keeping the whole healthcare\nsystem sustainable for society as a whole.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06924,review,post_llm,2023,4,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'A Design Guideline to Overcome Web Accessibility Issues Challenged by\n  Visually Impaired Community in Sri Lanka\n\n  Visual-impaired communities are one of the hindrances groups to accessing web\ncontent access in the world. The obstacles encountered by this community in\ntheir current practices and to develop best practice guidelines to overcome the\ndigital divide in Sri Lanka become gap filling of this domain. A preliminary\nsurvey indicated five main problems including access limited by the impairment,\nusability issues due to lack of design, unavailability of visually\nimpaired-friendly applications, lack of communication, and web navigation\nissues are the most dominant pertaining issues. To overcome those issues,\nsolutions are tested and validated using the Design Science approach. Result\nIndicate that significant factors need to be incorporated, ensuring\nkeyboard-friendly websites, easy accessibility and support with semantic\nannotation by adding alternative text for images. Furthermore, use headers to\nstructure the content correctly, design all forms to support accessibility in\nmind including Content developing and designing, navigation, the best colour\ncombination, Pre-recorded video with the audio facilities, braille support on\nthe web, the designing option has no significant impact on visually impaired\nweb users. Introducing a rating widget option to a website identifies the level\nof accessibility features availability facilitates, thereby overcoming the\ndisability digital divide. The results further conclude that a significant\ndifference exists in websites, with and without the involvement of the visually\nimpaired community. Semantic web and semantic annotations of the context of\npage elements, content serialization, and navigation by special keyboard\ncommands are also highly influencing the effective use of the web and\nincreasing the satisfaction level in the website accessing process.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.00927,regular,post_llm,2023,4,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Quantifying Carbon Emissions due to Online Third-Party Tracking\n\n  In the past decade, global warming made several headlines and turned the\nattention of the whole world to it. Carbon footprint is the main factor that\ndrives greenhouse emissions up and results in the temperature increase of the\nplanet with dire consequences. While the attention of the public is turned to\nreducing carbon emissions by transportation, food consumption and household\nactivities, we ignore the contribution of CO2eq emissions produced by online\nactivities. In the current information era, we spend a big amount of our days\nbrowsing online. This activity consumes electricity which in turn produces\nCO2eq. While website browsing contributes to the production of greenhouse gas\nemissions, the impact of the Internet on the environment is further exacerbated\nby the web-tracking practice. Indeed, most webpages are heavily loaded by\ntracking content used mostly for advertising, data analytics and usability\nimprovements. This extra content implies big data transmissions which results\nin higher electricity consumption and thus higher greenhouse gas emissions. In\nthis work, we focus on the overhead caused by web tracking and analyse both its\nnetwork and carbon footprint. By leveraging the browsing telemetry of 100k\nusers and the results of a crawling experiment of 2.7M websites, we find that\nweb tracking increases data transmissions upwards of 21%, which in turn implies\nthe additional emission of around 11 Mt of greenhouse gases in the atmosphere\nevery year. We find such contribution to be far from negligible, and comparable\nto many activities of modern life, such as meat production, transportation, and\neven cryptocurrency mining. Our study also highlights that there exist\nsignificant inequalities when considering the footprint of different countries,\nwebsite categories, and tracking organizations, with a few actors contributing\nto a much greater extent than the remaining ones.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.1254,regular,post_llm,2023,4,"{'ai_likelihood': 7.64926274617513e-06, 'text': ""Opinion Control under Adversarial Network Perturbation: A Stackelberg\n  Game Approach\n\n  The emerging social network platforms enable users to share their own\nopinions, as well as to exchange opinions with others. However, adversarial\nnetwork perturbation, where malicious users intentionally spread their extreme\nopinions, rumors, and misinformation to others, is ubiquitous in social\nnetworks. Such adversarial network perturbation greatly influences the opinion\nformation of the public and threatens our societies. Thus, it is critical to\nstudy and control the influence of adversarial network perturbation. Although\ntremendous efforts have been made in both academia and industry to guide and\ncontrol the public opinion dynamics, most of these works assume that the\nnetwork is static, and ignore such adversarial network perturbation. In this\nwork, based on the well-accepted Friedkin-Johnsen opinion dynamics model, we\nmodel the adversarial network perturbation and analyze its impact on the\nnetworks' opinion. Then, from the adversary's perspective, we analyze its\noptimal network perturbation, which maximally changes the network's opinion.\nNext, from the network defender's perspective, we formulate a Stackelberg game\nand aim to control the network's opinion even under such adversarial network\nperturbation. We devise a projected subgradient algorithm to solve the\nformulated Stackelberg game. Extensive simulations on real social networks\nvalidate our analysis of the adversarial network perturbation's influence and\nthe effectiveness of the proposed opinion control algorithm.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.10585,regular,post_llm,2023,4,"{'ai_likelihood': 1.0629494984944662e-05, 'text': 'VREd: A Virtual Reality-Based Classroom for Online Education Using\n  Unity3D WebGL\n\n  Virtual reality is the way of the future. The use of virtual reality is\nexpanding over time across all sectors, from the entertainment industry to the\nmilitary and space. VREd is a similar concept where a virtual reality-based\nclassroom is used for online education where the user will have better\ninteraction and more control. Unity3D and WebGL software have been used for\nimplementation. Students or learners accustomed to contemporary technologies\nmay find the traditional educational system unappealing because of its flaws.\nIncorporating the latest technologies can increase the curiosity and learning\nabilities of students. The system architecture of VREd is similar to that of an\nactual classroom, allowing both students and teachers to access all of the\ncourse materials and interact with one another using only an internet\nconnection. The environment and the background are also customizable.\nTherefore, all the users can comfortably use the system and feel at home. We\ncan create an effective educational system that raises educational quality by\nutilizing virtual reality.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.11069,review,post_llm,2023,4,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'Conceptualizing Approaches to Critical Computing Education: Inquiry,\n  Design and Reimagination\n\n  As several critical issues in computing such as algorithmic bias,\ndiscriminatory practices, and techno-solutionism have become more visible,\nnumerous efforts are being proposed to integrate criticality in K-16 computing\neducation. Yet, how exactly these efforts address criticality and translate it\ninto classroom practice is not clear. In this conceptual paper, we first\nhistoricize how current efforts in critical computing education draw on\nprevious work which has promoted learner empowerment through critical analysis\nand production. We then identify three emergent approaches: (1) inquiry, (2)\ndesign and (3) reimagination that build on and expand these critical traditions\nin computing education. Finally, we discuss how these approaches highlight\nissues to be addressed and provide directions for further computing education\nresearch.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06467,regular,post_llm,2023,4,"{'ai_likelihood': 1.0, 'text': 'Towards Understanding the Benefits and Challenges of Demand Responsive\n  Public Transit- A Case Study in the City of Charlotte, NC\n\n  Access to reliable public transportation is essential for addressing\nsocio-economic disparities, particularly in low-income communities that rely\nheavily on transit for accessing jobs, healthcare, and essential services. This\nstudy investigates the challenges faced by transit-dependent populations in\nCharlotte, NC, focusing on the spatial and service-related inequities within\nthe current public bus system. Our research initially evaluates critical issues\nsuch as extended wait times, unreliable schedules, and limited accessibility,\nwhich significantly impact the daily lives of low-income residents. In response\nto these challenges, we gathered data to assess the potential for a connected,\ndemand-responsive bus system designed to minimize transit gaps and enhance\nservice efficiency in the future. This evaluation included an analysis of the\nexisting Charlotte Area Transit System (CATS) mobile applications and the\nexploration of user acceptance for a proposed smart, on-demand transit\ntechnology. Through surveys conducted across key bus lines-including the\nSprinter line and Bus Lines 7, 9, and 97-99-we identified significant\nshortcomings in the current system. However, our findings also indicate a\nstrong willingness among participants to adopt new transit solutions, provided\nthat they effectively address current issues and alleviate concerns related to\nsmartphone accessibility, privacy, and trust. This research contributes\nvaluable insights into the modernization of public transit systems in\nCharlotte, highlighting the importance of user-centric approaches in developing\ninnovative, equitable, and efficient transportation solutions.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00011926889419555664, 'GPT4': 0.9853515625, 'CLAUDE': 0.0002073049545288086, 'GOOGLE': 0.004703521728515625, 'OPENAI_O_SERIES': 0.0094146728515625, 'DEEPSEEK': 3.314018249511719e-05, 'GROK': 2.5033950805664062e-06, 'NOVA': 5.9604644775390625e-06, 'OTHER': 0.0001614093780517578, 'HUMAN': 7.808208465576172e-06}}"
2304.07249,review,post_llm,2023,4,"{'ai_likelihood': 6.5896246168348525e-06, 'text': ""How to design an AI ethics board\n\n  Organizations that develop and deploy artificial intelligence (AI) systems\nneed to take measures to reduce the associated risks. In this paper, we examine\nhow AI companies could design an AI ethics board in a way that reduces risks\nfrom AI. We identify five high-level design choices: (1) What responsibilities\nshould the board have? (2) What should its legal structure be? (3) Who should\nsit on the board? (4) How should it make decisions and should its decisions be\nbinding? (5) What resources does it need? We break down each of these questions\ninto more specific sub-questions, list options, and discuss how different\ndesign choices affect the board's ability to reduce risks from AI. Several\nfailures have shown that designing an AI ethics board can be challenging. This\npaper provides a toolbox that can help AI companies to overcome these\nchallenges.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2304.06102,review,post_llm,2023,4,"{'ai_likelihood': 1.0, 'text': 'An Analysis of How COVID-19 Shaped the Realm of Online Gaming and Lesson\n  Delivery\n\n  The COVID-19 pandemic has forced schools and universities to adapt to remote\nlearning, and online gaming has emerged as a tool for education. Educational\ngames can make learning fun and engaging, help students develop important\nskills like problem-solving and collaboration, and reach students who are\nstruggling with traditional learning methods. While there are concerns about\nthe potential drawbacks of online gaming in education, its benefits are clear.\nAs the pandemic continues to disrupt education, online gaming is likely to\nbecome an increasingly important tool for teachers and students alike.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0038623809814453125, 'GPT4': 9.894371032714844e-06, 'CLAUDE': 3.2007694244384766e-05, 'GOOGLE': 0.99365234375, 'OPENAI_O_SERIES': 2.4437904357910156e-06, 'DEEPSEEK': 1.3113021850585938e-06, 'GROK': 6.556510925292969e-07, 'NOVA': 3.314018249511719e-05, 'OTHER': 0.00250244140625, 'HUMAN': 3.5762786865234375e-07}}"
2304.1393,regular,post_llm,2023,4,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Education in the Digital World: From the Lens of Millennial Learners\n\n  The objective of this study is to determine Education in the Digital World\nfrom the lens of millennial learners. This also identifies the cybergogical\nimplications of the issue with digital education as seen through the lens of\nthe outlier.\n  This study uses a mixed methods sequential explanatory design. A quantitative\nmethod was employed during the first phase and the instruments of the study\nwere distributed using google forms. The survey received a total of 85\nresponses and the results were analyzed using descriptive methods. Following up\nwith a qualitative method, during the second phase the outliers were\ninterviewed,and the results were analyzed using thematic analysis. The results\nof the mixed methods were interpreted in the form of cybergogical implications.\n  The digital education from the lens of millennial learners in terms of the\nBenefits of E-Learning and Students Perceptions of E Learning received an\noverall mean of 3.68 which was verbally interpreted as Highly acceptable. The\nresults reveal that millennial learners perceptions of digital education are\ninfluenced by the convenience in time and location, the fruit of collaboration\nusing online interaction, the skills and knowledge they will acquire using\ndigital resources, and the capability of improving themselves for the future.\n  Millennial learners were able to adopt and learned how to use e learning.\nAlso, since it is self paced learning,it allows them to study on their own time\nand schedule since e learning can be accessed anytime and anywhere. However,\nthe technological resources of the learners should be considered in the\nimplementation of e learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.02204,regular,post_llm,2023,4,"{'ai_likelihood': 2.1192762586805556e-06, 'text': 'PopSim: An Individual-level Population Simulator for Equitable\n  Allocation of City Resources\n\n  Historical systematic exclusionary tactics based on race have forced people\nof certain demographic groups to congregate in specific urban areas. Aside from\nthe ethical aspects of such segregation, these policies have implications for\nthe allocation of urban resources including public transportation, healthcare,\nand education within the cities. The initial step towards addressing these\nissues involves conducting an audit to assess the status of equitable resource\nallocation. However, due to privacy and confidentiality concerns,\nindividual-level data containing demographic information cannot be made\npublicly available. By leveraging publicly available aggregated demographic\nstatistics data, we introduce PopSim, a system for generating semi-synthetic\nindividual-level population data with demographic information. We use PopSim to\ngenerate multiple benchmark datasets for the city of Chicago and conduct\nextensive statistical evaluations to validate those. We further use our\ndatasets for several case studies that showcase the application of our system\nfor auditing equitable allocation of city resources.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.09675,review,post_llm,2023,5,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Spatial Computing Opportunities in Biomedical Decision Support: The\n  Atlas-EHR Vision\n\n  We consider the problem of reducing the time needed by healthcare\nprofessionals to understand patient medical history via the next generation of\nbiomedical decision support. This problem is societally important because it\nhas the potential to improve healthcare quality and patient outcomes. However,\nnavigating electronic health records is challenging due to the high\npatient-doctor ratios, potentially long medical histories, the urgency of\ntreatment for some medical conditions, and patient variability. The current\nelectronic health record systems provides only a longitudinal view of patient\nmedical history, which is time-consuming to browse, and doctors often need to\nengage nurses, residents, and others for initial analysis. To overcome this\nlimitation, we envision an alternative spatial representation of patients'\nhistories (e.g., electronic health records (EHRs)) and other biomedical data in\nthe form of Atlas-EHR. Just like Google Maps allows a global, national,\nregional, and local view, the Atlas-EHR may start with an overview of the\npatient's anatomy and history before drilling down to spatially anatomical\nsub-systems, their individual components, or sub-components. Atlas-EHR presents\na compelling opportunity for spatial computing since healthcare is almost a\nfifth of the US economy. However, the traditional spatial computing designed\nfor geographic use cases (e.g., navigation, land-surveys, mapping) faces many\nhurdles in the biomedical domain. This paper presents a number of open research\nquestions under this theme in five broad areas of spatial computing.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.09429,regular,post_llm,2023,5,"{'ai_likelihood': 0.0005711449517144097, 'text': ""Efficacy of Educational Misinformation Games\n\n  Misinformation has become a significant issue in today's society, with the\nproliferation of false information through various mediums such as social media\nand traditional news sources. The rapid spread of misinformation has made it\nincreasingly difficult for people to separate truth from fiction, and this has\nthe potential to cause significant harm to individuals and society as a whole.\nIn addition, there currently exists an information gap with regard to internet\neducation, with many schools across America not having the teaching personnel\nnor resources to adequately educate their students about the dangers of the\ninternet, specifically with regard to misinformation in the political sphere.\nTo address the dangers of misinformation, some game developers have created\nvideo games that aim to educate players on the issue and help them develop\ncritical thinking skills. These games can be used to raise awareness about the\nimportance of verifying information before sharing it. By doing so, they can\nhelp reduce the spread of misinformation and promote a more informed and\ndiscerning public. They can also provide players with a safe and controlled\nenvironment to practice these skills and build confidence in their ability to\nevaluate information. However, these existing games often suffer from various\nshortcomings such as failing to adequately address how misinformation\nspecifically exploits the biases within people to be effective and rarely\ncovering how evolving modern technologies like sophisticated chatbots and deep\nfakes have made individuals even more vulnerable to misinformation. The purpose\nof this study is to create an educational misinformation game to address this\ninformation gap and investigate its efficacy as an educational tool while also\niterating on the designs for previous games in the space.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.04672,review,post_llm,2023,5,"{'ai_likelihood': 4.967053731282553e-06, 'text': 'Augmented Datasheets for Speech Datasets and Ethical Decision-Making\n\n  Speech datasets are crucial for training Speech Language Technologies (SLT);\nhowever, the lack of diversity of the underlying training data can lead to\nserious limitations in building equitable and robust SLT products, especially\nalong dimensions of language, accent, dialect, variety, and speech impairment -\nand the intersectionality of speech features with socioeconomic and demographic\nfeatures. Furthermore, there is often a lack of oversight on the underlying\ntraining data - commonly built on massive web-crawling and/or publicly\navailable speech - with regard to the ethics of such data collection. To\nencourage standardized documentation of such speech data components, we\nintroduce an augmented datasheet for speech datasets, which can be used in\naddition to ""Datasheets for Datasets"". We then exemplify the importance of each\nquestion in our augmented datasheet based on in-depth literature reviews of\nspeech data used in domains such as machine learning, linguistics, and health.\nFinally, we encourage practitioners - ranging from dataset creators to\nresearchers - to use our augmented datasheet to better define the scope,\nproperties, and limits of speech datasets, while also encouraging consideration\nof data-subject protection and user community empowerment. Ethical dataset\ncreation is not a one-size-fits-all process, but dataset creators can use our\naugmented datasheet to reflexively consider the social context of related SLT\napplications and data sources in order to foster more inclusive SLT products\ndownstream.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.18317,regular,post_llm,2023,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""FOPPA: An Open Database of French Public Procurement Award Notices From\n  2010--2020\n\n  Public Procurement refers to governments' purchasing activities of goods,\nservices, and construction of public works. In the European Union (EU), it is\nan essential sector, corresponding to 15% of the GDP. EU public procurement\ngenerates large amounts of data, because award notices related to contracts\nexceeding a predefined threshold must be published on the TED (EU's official\njournal). Under the framework of the DeCoMaP project, which aims at leveraging\nsuch data in order to predict fraud in public procurement, we constitute the\nFOPPA (French Open Public Procurement Award notices) database. It contains the\ndescription of 1,380,965 lots obtained from the TED, covering the 2010--2020\nperiod for France. We detect a number of substantial issues in these data, and\npropose a set of automated and semi-automated methods to solve them and produce\na usable database. It can be leveraged to study public procurement in an\nacademic setting, but also to facilitate the monitoring of public policies, and\nto improve the quality of the data offered to buyers and suppliers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.01405,regular,post_llm,2023,5,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Adopting AI: How Familiarity Breeds Both Trust and Contempt\n\n  Despite pronouncements about the inevitable diffusion of artificial\nintelligence and autonomous technologies, in practice it is human behavior, not\ntechnology in a vacuum, that dictates how technology seeps into -- and changes\n-- societies. In order to better understand how human preferences shape\ntechnological adoption and the spread of AI-enabled autonomous technologies, we\nlook at representative adult samples of US public opinion in 2018 and 2020 on\nthe use of four types of autonomous technologies: vehicles, surgery, weapons,\nand cyber defense. By focusing on these four diverse uses of AI-enabled\nautonomy that span transportation, medicine, and national security, we exploit\nthe inherent variation between these AI-enabled autonomous use cases. We find\nthat those with familiarity and expertise with AI and similar technologies were\nmore likely to support all of the autonomous applications we tested (except\nweapons) than those with a limited understanding of the technology. Individuals\nthat had already delegated the act of driving by using ride-share apps were\nalso more positive about autonomous vehicles. However, familiarity cut both\nways; individuals are also less likely to support AI-enabled technologies when\napplied directly to their life, especially if technology automates tasks they\nare already familiar with operating. Finally, opposition to AI-enabled military\napplications has slightly increased over time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.13542,regular,post_llm,2023,5,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Bidding Strategies for Proportional Representation in Advertisement\n  Campaigns\n\n  Many companies rely on advertising platforms such as Google, Facebook, or\nInstagram to recruit a large and diverse applicant pool for job openings. Prior\nworks have shown that equitable bidding may not result in equitable outcomes\ndue to heterogeneous levels of competition for different types of individuals.\nSuggestions have been made to address this problem via revisions to the\nadvertising platform. However, it may be challenging to convince platforms to\nundergo a costly re-vamp of their system, and in addition it might not offer\nthe flexibility necessary to capture the many types of fairness notions and\nother constraints that advertisers would like to ensure. Instead, we consider\nalterations that make no change to the platform mechanism and instead change\nthe bidding strategies used by advertisers. We compare two natural fairness\nobjectives: one in which the advertisers must treat groups equally when bidding\nin order to achieve a yield with group-parity guarantees, and another in which\nthe bids are not constrained and only the yield must satisfy parity\nconstraints. We show that requiring parity with respect to both bids and yield\ncan result in an arbitrarily large decrease in efficiency compared to requiring\nequal yield proportions alone. We find that autobidding is a natural way to\nrealize this latter objective and show how existing work in this area can be\nextended to provide efficient bidding strategies that provide high utility\nwhile satisfying group parity constraints as well as deterministic and\nrandomized rounding techniques to uphold these guarantees. Finally, we\ndemonstrate the effectiveness of our proposed solutions on data adapted from a\nreal-world employment dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.11057,review,post_llm,2023,5,"{'ai_likelihood': 5.6624412536621094e-05, 'text': 'From Assistive Technologies to Metaverse: Technologies in Inclusive\n  Higher Education for Students with Specific Learning Difficulties\n\n  The development of new technologies and their expanding use in a wide range\nof educational environments are driving the transformation of higher education.\nAssistive technologies are a subset of cutting-edge technology that can help\nstudents learn more effectively and make education accessible to everyone.\nAssistive technology can enhance, maintain, or improve the capacities of\nstudents with learning difficulties. Students with learning difficulties will\nbe greatly benefited from the use of assistive technologies. If these\ntechnologies are used effectively, students with learning difficulties can\ncompete with their peers and complete their academic tasks. We aim to conduct\nthis review to better understand the role of assistive technologies in\nproviding inclusive higher education for students with learning difficulties.\nThe review begins with the introduction of learning difficulties and their\ncauses; inclusive education and the need for assistive technologies; the\nreasoning for conducting this review; and a summary of related reviews on\nassistive technologies for students with learning difficulties in inclusive\nhigher education. Then, we discuss the preliminaries for the learning\ndifficulties type and assistive technology. Later, we discuss the effects of\nassistive technology on inclusive higher education for students with learning\ndifficulties. Additionally, we discuss related projects and support tools\navailable in inclusive higher education for students with learning\ndifficulties. We also explore the challenges and possible solutions related to\nusing assistive technology in higher education to provide inclusive education\nfor students with learning difficulties. We conclude the review with a\ndiscussion of potential promising future directions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.03471,regular,post_llm,2023,5,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Streamlining personal data access requests: From obstructive procedures\n  to automated web workflows\n\n  Transparency and data portability are two core principles of modern privacy\nlegislations such as the GDPR. From the regulatory perspective, providing\nindividuals (data subjects) with access to their data is a main building block\nfor implementing these. Different from other privacy principles and respective\nregulatory provisions, however, this right to data access has so far only seen\nmarginal technical reflection. Processes related to performing data subject\naccess requests (DSARs) are thus still to be executed manually, hindering the\nconcept of data access from unfolding its full potential.\n  To tackle this problem, we present an automated approach to the execution of\nDSARs, employing modern techniques of web automation. In particular, we propose\na generic DSAR workflow model, a corresponding formal language for representing\nthe particular workflows of different service providers (controllers), a\npublicly accessible and extendable workflow repository, and a browser-based\nexecution engine, altogether providing ``one-click'' DSARs. To validate our\napproach and technical concepts, we examine, formalize and make publicly\navailable the DSAR workflows of 15 widely used service providers and implement\nthe execution engine in a publicly available browser extension. Altogether, we\nthereby pave the way for automated data subject access requests and lay the\ngroundwork for a broad variety of subsequent technical means helping web users\nto better understand their privacy-related exposure to different service\nproviders.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.0925,regular,post_llm,2023,5,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Towards a methodology to consider the environmental impacts of digital\n  agriculture\n\n  Agriculture affects global warming, while its yields are threatened by it.\nInformation and communication technology (ICT) is often considered as a\npotential lever to mitigate this tension, through monitoring and process\noptimization. However, while agricultural ICT is actively promoted, its\nenvironmental impact appears to be overlooked. Possible rebound effects could\nput at stake its net expected benefits and hamper agriculture sustainability.\nBy adapting environmental footprint assessment methods to digital agriculture\ncontext, this research aims at defining a methodology taking into account the\nenvironmental footprint of agricultural ICT systems and their required\ninfrastructures. The expected contribution is to propose present and\nprospective models based on possible digitalization scenarios, in order to\nassess effects and consequences of different technological paths on agriculture\nsustainability, sufficiency and resilience. The final results could be useful\nto enlighten societal debates and political decisions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.07478,review,post_llm,2023,5,"{'ai_likelihood': 2.5497542487250434e-06, 'text': ""Professional Ethics by Design: Co-creating Codes of Conduct for\n  Computational Practice\n\n  This paper deals with the importance of developing codes of conduct for\npractitioners--be it journalists, doctors, attorneys, or other\nprofessions--that are encountering ethical issues when using computation, but\ndo not have access to any framework of reference as to how to address those. At\nthe same time, legal and technological developments are calling for\nestablishing such guidelines, as shown in the European Union's and the United\nStates' efforts in regulating a wide array of artificial intelligence systems,\nand in the resurgence of rule-based models through 'neurosymbolic' AI, a hybrid\nformat that combines them with neural methods. Against this backdrop, we argue\nfor taking a design-inspired approach when encoding professional ethics into a\ncomputational form, so as to co-create codes of conduct for computational\npractice across a wide range of fields.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.18615,review,post_llm,2023,5,"{'ai_likelihood': 4.933940039740669e-06, 'text': 'Stronger Together: on the Articulation of Ethical Charters, Legal Tools,\n  and Technical Documentation in ML\n\n  The growing need for accountability of the people behind AI systems can be\naddressed by leveraging processes in three fields of study: ethics, law, and\ncomputer science. While these fields are often considered in isolation, they\nrely on complementary notions in their interpretation and implementation. In\nthis work, we detail this interdependence and motivate the necessary role of\ncollaborative governance tools in shaping a positive evolution of AI. We first\ncontrast notions of compliance in the ethical, legal, and technical fields; we\noutline both their differences and where they complement each other, with a\nparticular focus on the roles of ethical charters, licenses, and technical\ndocumentation in these interactions. We then focus on the role of values in\narticulating the synergies between the fields and outline specific mechanisms\nof interaction between them in practice. We identify how these mechanisms have\nplayed out in several open governance fora: an open collaborative workshop, a\nresponsible licensing initiative, and a proposed regulatory framework. By\nleveraging complementary notions of compliance in these three domains, we can\ncreate a more comprehensive framework for governing AI systems that jointly\ntakes into account their technical capabilities, their impact on society, and\nhow technical specifications can inform relevant regulations. Our analysis thus\nunderlines the necessity of joint consideration of the ethical, legal, and\ntechnical in AI ethics frameworks to be used on a larger scale to govern AI\nsystems and how the thinking in each of these areas can inform the others.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.18303,review,post_llm,2023,5,"{'ai_likelihood': 5.066394805908203e-06, 'text': 'New Era of Artificial Intelligence in Education: Towards a Sustainable\n  Multifaceted Revolution\n\n  The recent high performance of ChatGPT on several standardized academic tests\nhas thrust the topic of artificial intelligence (AI) into the mainstream\nconversation about the future of education. As deep learning is poised to shift\nthe teaching paradigm, it is essential to have a clear understanding of its\neffects on the current education system to ensure sustainable development and\ndeployment of AI-driven technologies at schools and universities. This research\naims to investigate the potential impact of AI on education through review and\nanalysis of the existing literature across three major axes: applications,\nadvantages, and challenges. Our review focuses on the use of artificial\nintelligence in collaborative teacher--student learning, intelligent tutoring\nsystems, automated assessment, and personalized learning. We also report on the\npotential negative aspects, ethical issues, and possible future routes for AI\nimplementation in education. Ultimately, we find that the only way forward is\nto embrace the new technology, while implementing guardrails to prevent its\nabuse.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.08157,regular,post_llm,2023,5,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""Algorithmic Pluralism: A Structural Approach To Equal Opportunity\n\n  We present a structural approach toward achieving equal opportunity in\nsystems of algorithmic decision-making called algorithmic pluralism.\nAlgorithmic pluralism describes a state of affairs in which no set of\nalgorithms severely limits access to opportunity, allowing individuals the\nfreedom to pursue a diverse range of life paths. To argue for algorithmic\npluralism, we adopt Joseph Fishkin's theory of bottlenecks, which focuses on\nthe structure of decision-points that determine how opportunities are\nallocated. The theory contends that each decision-point or bottleneck limits\naccess to opportunities with some degree of severity and legitimacy. We extend\nFishkin's structural viewpoint and use it to reframe existing systemic concerns\nabout equal opportunity in algorithmic decision-making, such as patterned\ninequality and algorithmic monoculture. In proposing algorithmic pluralism, we\nargue for the urgent priority of alleviating severe bottlenecks in algorithmic\ndecision-making. We contend that there must be a pluralism of opportunity\navailable to many different individuals in order to promote equal opportunity\nin a systemic way. We further show how this framework has several implications\nfor system design and regulation through current debates about equal\nopportunity in algorithmic hiring.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.0284,review,post_llm,2023,5,"{'ai_likelihood': 4.76837158203125e-06, 'text': ""Making Sense of Machine Learning: Integrating Youth's Conceptual,\n  Creative, and Critical Understandings of AI\n\n  Understanding how youth make sense of machine learning and how learning about\nmachine learning can be supported in and out of school is more relevant than\never before as young people interact with machine learning powered applications\neveryday; while connecting with friends, listening to music, playing games, or\nattending school. In this symposium, we present different perspectives on\nunderstanding how learners make sense of machine learning in their everyday\nlives, how sensemaking of machine learning can be supported in and out of\nschool through the construction of applications, and how youth critically\nevaluate machine learning powered systems. We discuss how sensemaking of\nmachine learning applications involves the development and integration of\nconceptual, creative, and critical understandings that are increasingly\nimportant to prepare youth to participate in the world.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.14358,review,post_llm,2023,5,"{'ai_likelihood': 2.1557013193766277e-05, 'text': 'Shall androids dream of genocides? How generative AI can change the\n  future of memorialization of mass atrocities\n\n  The memorialization of mass atrocities such as war crimes and genocides\nfacilitates the remembrance of past suffering, honors those who resisted the\nperpetrators, and helps prevent the distortion of historical facts. Digital\ntechnologies have transformed memorialization practices by enabling less\ntop-down and more creative approaches to remember mass atrocities. At the same\ntime, they may also facilitate the spread of denialism and distortion, attempt\nto justify past crimes and attack the dignity of victims. The emergence of\ngenerative forms of artificial intelligence (AI), which produce textual and\nvisual content, has the potential to revolutionize the field of memorialization\neven further. AI can identify patterns in training data to create new\nnarratives for representing and interpreting mass atrocities - and do so in a\nfraction of the time it takes for humans. The use of generative AI in this\ncontext raises numerous questions: For example, can the paucity of training\ndata on mass atrocities distort how AI interprets some atrocity-related\ninquiries? How important is the ability to differentiate between human- and\nAI-made content concerning mass atrocities? Can AI-made content be used to\npromote false information concerning atrocities? This article addresses these\nand other questions by examining the opportunities and risks associated with\nusing generative AIs for memorializing mass atrocities. It also discusses\nrecommendations for AIs integration in memorialization practices to steer the\nuse of these technologies toward a more ethical and sustainable direction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.05962,review,post_llm,2023,5,"{'ai_likelihood': 1.1788474188910592e-05, 'text': ""A Comprehensive Picture of Factors Affecting User Willingness to Use\n  Mobile Health Applications\n\n  Mobile health (mHealth) applications have become increasingly valuable in\npreventive healthcare and in reducing the burden on healthcare organizations.\nThe aim of this paper is to investigate the factors that influence user\nacceptance of mHealth apps and identify the underlying structure that shapes\nusers' behavioral intention. An online study that employed factorial survey\ndesign with vignettes was conducted, and a total of 1,669 participants from\neight countries across four continents were included in the study. Structural\nequation modeling was employed to quantitatively assess how various factors\ncollectively contribute to users' willingness to use mHealth apps. The results\nindicate that users' digital literacy has the strongest impact on their\nwillingness to use them, followed by their online habit of sharing personal\ninformation. Users' concerns about personal privacy only had a weak impact.\nFurthermore, users' demographic background, such as their country of residence,\nage, ethnicity, and education, has a significant moderating effect. Our\nfindings have implications for app designers, healthcare practitioners, and\npolicymakers. Efforts are needed to regulate data collection and sharing and\npromote digital literacy among the general population to facilitate the\nwidespread adoption of mHealth apps.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.18825,regular,post_llm,2023,5,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Instrumental genesis through interdisciplinary collaboration --\n  reflections on the emergence of a visualisation framework for video\n  annotation data\n\n  Instrumental genesis through interdisciplinary collaboration-reflections on\nthe emergence of a visualisation framework for video annotation data XML This\npaper presents, discusses and reflects on the development of a visualization\nframework for the analysis of the temporal dynamics of audiovisual\nexpressivity. The main focus lies on the instrumental genesis process (Rabardel\n1995; Longchamp 2012)-a concept trying to express and analyze the co-evolution\nof instruments and the practices they make possible-underlying this\ndevelopment. It is described through the collaboration and communication\nprocesses between computer science scholars and humanities scholars in finding\nnew ways of visualizing complex datasets for exploration and presentation in\nthe realm of film-studies research. It draws on the outcome and concrete usage\nof the visualizations in publications and presentations of a research group,\nthe AdAproject, that investigates the audiovisual rhetorics of affect in\naudiovisual media on the financial crisis (2007-). These film analyses are\nbased on theoretical assumptions on the process of film-viewing, the relation\nof the viewer's perception and the temporally unfolding audiovisual images, and\na methodical approach that draws on 'steps' in the research process such as\nsegmentation, description and qualification, called eMAEX (Kappelhoff et al.\n2011-2016) to reconstruct these experiential figurations (Bakels et al. 2020a,\n2020b). The main focus of this paper is the process of iterative development of\nvisualizations as interactive interfaces generated with the open-source\nsoftware Advene, that were an integral part of the research process. In this\nregard, the timeline visualization is not only of interest for visual\nargumentation in (digital) humanities publications, but also for the creation\nof annotations as well as the exploration of this data. In the first part of\nthe paper we describe this interdisciplinary collaboration as instrumental\ngenesis on a general level-as an evolving and iterative process. In the second\npart we focus on the specific challenge of designing a visualization framework\nfor the temporal dynamics of audiovisual aesthetics. Lastly we zoom out by\nreflecting on experiences and insights that might be of interest for the wider\ndigital humanities community.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.01776,review,post_llm,2023,5,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Taxonomizing and Measuring Representational Harms: A Look at Image\n  Tagging\n\n  In this paper, we examine computational approaches for measuring the\n""fairness"" of image tagging systems, finding that they cluster into five\ndistinct categories, each with its own analytic foundation. We also identify a\nrange of normative concerns that are often collapsed under the terms\n""unfairness,"" ""bias,"" or even ""discrimination"" when discussing problematic\ncases of image tagging. Specifically, we identify four types of\nrepresentational harms that can be caused by image tagging systems, providing\nconcrete examples of each. We then consider how different computational\nmeasurement approaches map to each of these types, demonstrating that there is\nnot a one-to-one mapping. Our findings emphasize that no single measurement\napproach will be definitive and that it is not possible to infer from the use\nof a particular measurement approach which type of harm was intended to be\nmeasured. Lastly, equipped with this more granular understanding of the types\nof representational harms that can be caused by image tagging systems, we show\nthat attempts to mitigate some of these types of harms may be in tension with\none another.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.00821,regular,post_llm,2023,5,"{'ai_likelihood': 1.0, 'text': 'Empowering Learner-Centered Instruction: Integrating ChatGPT Python API\n  and Tinker Learning for Enhanced Creativity and Problem-Solving Skills\n\n  The ChatGPT Python API plays a crucial role in promoting Learner-Centered\nInstruction (LCI) and aligns with the principles of Tinker Learning, allowing\nstudents to discover their learning strategies. LCI emphasizes the importance\nof active, hands-on learning experiences and encourages students to take\nresponsibility for their learning journey. By integrating the ChatGPT Python\nAPI into the educational process, students can explore various resources,\ngenerate new ideas, and create content in a more personalized manner. This\ninnovative approach enables students to engage with the learning material\ndeeper, fostering a sense of ownership and motivation. As they work through the\nCreative Learning Spiral, students develop essential skills such as critical\nthinking, problem-solving, and creativity. The ChatGPT Python API is a valuable\ntool for students to explore different solutions, evaluate alternatives, and\nmake informed decisions, all while encouraging self-directed learning. In\nTinker Learning environments, the integration of ChatGPT Python API empowers\nstudents to experiment and iterate, allowing them to find the most effective\nlearning strategies that cater to their individual needs and preferences. This\npersonalized approach helps students to become more confident in their\nabilities, leading to tremendous academic success and long-term skill\ndevelopment. By leveraging the capabilities of the ChatGPT Python API,\neducational institutions can create a more engaging, supportive, and dynamic\nlearning environment. This approach aligns with the principles of\nLearner-Centered Instruction and Tinker Learning, promoting a culture of\ncuriosity, exploration, and creativity among students while preparing them for\nthe challenges of the fast-paced, ever-changing world.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.060577392578125, 'GPT4': 0.73583984375, 'CLAUDE': 0.00014853477478027344, 'GOOGLE': 0.165771484375, 'OPENAI_O_SERIES': 0.0002942085266113281, 'DEEPSEEK': 2.777576446533203e-05, 'GROK': 1.7285346984863281e-06, 'NOVA': 1.7940998077392578e-05, 'OTHER': 0.03729248046875, 'HUMAN': 4.76837158203125e-07}}"
2305.18828,regular,post_llm,2023,5,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Decision Support to Crowdsourcing for Annotation and Transcription of\n  Ancient Documents: The RECITAL Workshop\n\n  In the 18th century in Paris, only two public theatres could officially\nperform comedies: the Com{\\'e}die-Fran{\\c c}aise, and the\nCom{\\'e}die-Italienne. The latter was much less well known. By studying a\ncentury of accounting registers, we aim to learn more about its successful\nplays, its actors, musicians, set designers, and all the small trades necessary\nfor its operation, its administration, logistics and finances. To this end, we\nemploy a mass of untapped and unpublished resources, the 27,544 pages of 63\ndaily registers available at the Biblioth{\\`e}que Nationale de France (BnF).\nAnd we take a decidedly fresh look at emerging forms of creation and changes in\nthe entertainmenteconomy. We developed the crowdsourcing platform RECITAL to\ncollect and index the data from theregisters, following an emerging trend in\nDigital Humanities. RECITAL is built upon the ScribeAPI framework and it offers\na fully-fledged web application to classify the pages, annotate with marks and\ntags, transcribe the indexed marks and even to verify the previous transcripts.\nWe also describe a multi-level data model and to develop a series of monitoring\nanddecision tools to support crowdsourced data management up to their\ndefinitive form.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.12537,regular,post_llm,2023,5,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'Word differences in news media of lower and higher peace countries\n  revealed by natural language processing and machine learning\n\n  Language is both a cause and a consequence of the social processes that lead\nto conflict or peace. Hate speech can mobilize violence and destruction. What\nare the characteristics of peace speech that reflect and support the social\nprocesses that maintain peace? This study used existing peace indices, machine\nlearning, and on-line, news media sources to identify the words most associated\nwith lower-peace versus higher-peace countries. As each peace index measures\ndifferent social properties, there is little consensus on the numerical values\nof these indices. There is however greater consensus with these indices for the\ncountries that are at the extremes of lower-peace and higher-peace. Therefore,\na data driven approach was used to find the words most important in\ndistinguishing lower-peace and higher-peace countries. Rather than assuming a\ntheoretical framework that predicts which words are more likely in lower-peace\nand higher-peace countries, and then searching for those words in news media,\nin this study, natural language processing and machine learning were used to\nidentify the words that most accurately classified a country as lower-peace or\nhigher-peace. Once the machine learning model was trained on the word\nfrequencies from the extreme lower-peace and higher-peace countries, that model\nwas also used to compute a quantitative peace index for these and other\nintermediate-peace countries. The model successfully yielded a quantitative\npeace index for intermediate-peace countries that was in between that of the\nlower-peace and higher-peace, even though they were not in the training set.\nThis study demonstrates how natural language processing and machine learning\ncan help to generate new quantitative measures of social systems, which in this\nstudy, were linguistic differences resulting in a quantitative index of peace\nfor countries at different levels of peacefulness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.193,regular,post_llm,2023,5,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'A Study on the Impact of Gender, Employment Status and Academic\n  Discipline on Cyber Hygiene: A Case Study of University of Nigeria, Nsukka\n\n  The COVID19 pandemic has helped amplify the importance of Cyber Hygiene. As\nthe reliance on the Internet and IT services increased during the pandemic.\nThis in turn has introduced a new wave of criminal activities such as\ncybercrimes. With the emergent of COVID19 which lead to increase in\ncyberattacks incidents, the pattern and sophistication, there is an urgent need\nto carry out an exploratory study to find out users level of cyber-hygiene\nknowledge and culture based on gender, employment status and academic\ndiscipline. Above this, with many organisations providing for dual mode work\npattern or remote and in-person as the pandemic subsides, this study remains\nvery relevant and hence the aim to investigate the cyber hygiene knowledge and\ncompliance of university students and employees of the University of Nigeria,\nNsukka (UNN). In addition, it attempts to verify the relationship between\ndemographics such as gender, employment status and academic discipline on cyber\nhygiene culture among students and employees. The sample population is made of\nemployees and students of UNN, where the employees are either academic staff or\nnon-academic staff. The sample size consisted of three hundred and sixteen\n(316) participants, one hundred and eight-seven (187) of whom were females and\none hundred and twenty-nine (129) were males. The results offer some useful\ninsight on cyber hygiene practices at the university.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.1125,review,post_llm,2023,5,"{'ai_likelihood': 8.17908181084527e-06, 'text': 'Towards Intersectional Moderation: An Alternative Model of Moderation\n  Built on Care and Power\n\n  Shortcomings of current models of moderation have driven policy makers,\nscholars, and technologists to speculate about alternative models of content\nmoderation. While alternative models provide hope for the future of online\nspaces, they can fail without proper scaffolding. Community moderators are\nroutinely confronted with similar issues and have therefore found creative ways\nto navigate these challenges. Learning more about the decisions these\nmoderators make, the challenges they face, and where they are successful can\nprovide valuable insight into how to ensure alternative moderation models are\nsuccessful.\n  In this study, I perform a collaborative ethnography with moderators of\nr/AskHistorians, a community that uses an alternative moderation model,\nhighlighting the importance of accounting for power in moderation. Drawing from\nBlack feminist theory, I call this ""intersectional moderation."" I focus on\nthree controversies emblematic of r/AskHistorians\' alternative model of\nmoderation: a disagreement over a moderation decision; a collaboration to fight\nracism on Reddit; and a period of intense turmoil and its impact on policy.\nThrough this evidence I show how volunteer moderators navigated multiple layers\nof power through care work. To ensure the successful implementation of\nintersectional moderation, I argue that designers should support\ndecision-making processes and policy makers should account for the impact of\nthe sociotechnical systems in which moderators work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.1532,review,post_llm,2023,5,"{'ai_likelihood': 0.9951171875, 'text': ""METU Students' college life satisfaction\n\n  The research was conducted to identify the factors that influence college\nstudents' satisfaction with their college experience. Firstly, the study was\nfocused on the literature review to determine relevant factors that have been\npreviously studied in the literature. Then, the survey analysis examined three\nmain independent factors that have been found to be related to college\nstudents' satisfaction: Major Satisfaction, Social Self-Efficacy, and Academic\nPerformance. The findings of the study suggested that the most important factor\naffecting students' satisfaction with their college experience is their\nsatisfaction with their chosen major. This means that students who are\nsatisfied with the major they have chosen are more likely to be overall\nsatisfied with their college experience. It's worth noting that, while the\nstudy found that major satisfaction is the most crucial factor, it doesn't mean\nthat other factors such as Social Self-Efficacy, Academic Performance, and\nCampus Life Satisfaction are not important. Based on these findings, it is\nrecommend that students prioritize their major satisfaction when making college\nchoices in order to maximize their overall satisfaction with their college\nexperience.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.96826171875, 'GPT4': 0.0016832351684570312, 'CLAUDE': 0.00010341405868530273, 'GOOGLE': 0.025848388671875, 'OPENAI_O_SERIES': 8.022785186767578e-05, 'DEEPSEEK': 1.2516975402832031e-06, 'GROK': 1.055002212524414e-05, 'NOVA': 1.6868114471435547e-05, 'OTHER': 0.004032135009765625, 'HUMAN': 8.910894393920898e-05}}"
2305.07762,regular,post_llm,2023,5,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Impacts of Differential Privacy on Fostering more Racially and\n  Ethnically Diverse Elementary Schools\n\n  In the face of increasingly severe privacy threats in the era of data and AI,\nthe US Census Bureau has recently adopted differential privacy, the de facto\nstandard of privacy protection for the 2020 Census release. Enforcing\ndifferential privacy involves adding carefully calibrated random noise to\nsensitive demographic information prior to its release. This change has the\npotential to impact policy decisions like political redistricting and other\nhigh-stakes practices, partly because tremendous federal funds and resources\nare allocated according to datasets (like Census data) released by the US\ngovernment. One under-explored yet important application of such data is the\nredrawing of school attendance boundaries to foster less demographically\nsegregated schools. In this study, we ask: how differential privacy might\nimpact diversity-promoting boundaries in terms of resulting levels of\nsegregation, student travel times, and school switching requirements?\nSimulating alternative boundaries using differentially-private student counts\nacross 67 Georgia districts, we find that increasing data privacy requirements\ndecreases the extent to which alternative boundaries might reduce segregation\nand foster more diverse and integrated schools, largely by reducing the number\nof students who would switch schools under boundary changes. Impacts on travel\ntimes are minimal. These findings point to a privacy-diversity tradeoff local\neducational policymakers may face in forthcoming years, particularly as\ncomputational methods are increasingly poised to facilitate attendance boundary\nredrawings in the pursuit of less segregated schools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.15996,regular,post_llm,2023,5,"{'ai_likelihood': 2.814663781060113e-06, 'text': 'Pro-f-quiz: increasing the PROductivity of Feedback through activating\n  QUIZzes\n\n  Feedback beyond the grade is an important part of the learning process.\nHowever, because of the large student groups, many teachers in higher education\nare faced with practicalities such as the limited time to prepare and\ncommunicate the feedback to individual students. We have set up an experiment,\ntitled Pro-f-quiz, in which over two years 236 students participated and in\nwhich the feedback is communicated through an online quiz that activates the\nstudents to reflect upon their solution. The system can be set up in a very\nlimited time compared to booking individual time slots. The results show that\napproximately 85% of the students appreciate the approach with 60% indicating\nthat they reflect more intensively about their work than when the feedback is\ntransmitted traditionally. Moreover, the grade of the students participating in\nthe project was substantially higher than students not participating.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.01764,regular,post_llm,2023,5,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Data Science in an Agent-Based Simulation World\n\n  In data science education, the importance of learning to solve real-world\nproblems has been argued. However, there are two issues with this approach: (1)\nit is very costly to prepare multiple real-world problems (using real data)\naccording to the learning objectives, and (2) the learner must suddenly tackle\ncomplex real-world problems immediately after learning from a textbook using\nideal data. To solve these issues, this paper proposes data science teaching\nmaterial that uses agent-based simulation (ABS). The proposed teaching material\nconsists of an ABS model and an ABS story. To solve issue 1, the scenario of\nthe problem can be changed according to the learning objectives by setting the\nappropriate parameters of the ABS model. To solve issue 2, the difficulty level\nof the tasks can be adjusted by changing the description in the ABS story. We\nshow that, by using this teaching material, the learner can simulate the\ntypical tasks performed by a data scientist in a step-by-step manner (causal\ninference, data understanding, hypothesis building, data collection, data\nwrangling, data analysis, and hypothesis testing). The teaching material\ndescribed in this paper focuses on causal inference as the learning objectives\nand infectious diseases as the model theme for ABS, but ABS is used as a model\nto reproduce many types of social phenomena, and its range of expression is\nextremely wide. Therefore, we expect that the proposed teaching material will\ninspire the construction of teaching material for various objectives in data\nscience education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.07208,review,post_llm,2023,5,"{'ai_likelihood': 2.4769041273328996e-05, 'text': 'Making Differential Privacy Work for Census Data Users\n\n  The U.S. Census Bureau collects and publishes detailed demographic data about\nAmericans which are heavily used by researchers and policymakers. The Bureau\nhas recently adopted the framework of differential privacy in an effort to\nimprove confidentiality of individual census responses. A key output of this\nprivacy protection system is the Noisy Measurement File (NMF), which is\nproduced by adding random noise to tabulated statistics. The NMF is critical to\nunderstanding any errors introduced in the data, and performing valid\nstatistical inference on published census data. Unfortunately, the current\nrelease format of the NMF is difficult to access and work with. We describe the\nprocess we use to transform the NMF into a usable format, and provide\nrecommendations to the Bureau for how to release future versions of the NMF.\nThese changes are essential for ensuring transparency of privacy measures and\nreproducibility of scientific research built on census data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.02835,regular,post_llm,2023,5,"{'ai_likelihood': 2.615981631808811e-06, 'text': ""Designing Bugs or Doing Another Project: Effects on Secondary Students'\n  Self-Beliefs in Computer Science\n\n  Debugging, finding and fixing bugs in code, is a heterogeneous process that\nshapes novice learners' self-beliefs and motivation in computing. Our Debugging\nby Design intervention (DbD) provocatively puts students in control over bugs\nby having them collaborate on designing creative buggy projects during an\nelectronic textiles unit in an introductory computing course. We implemented\nDbD virtually in eight classrooms with two teachers in public schools with\nhistorically marginalized populations, using a quasi-experimental design. Data\nfrom this study included post-activity results from a validated survey\ninstrument (N=144). For all students, project completion correlated with\nincreased computer science creative expression and e-textiles coding\nself-efficacy. In the comparison classes, project completion correlated with\nreduced programming anxiety, problem-solving competency beliefs, and\nprogramming self-concept. In DbD classes, project completion is uniquely\ncorrelated with increased fascination with design and programming growth\nmindset. In the discussion, we consider the relative benefits of DbD versus\nother open-ended projects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.01786,review,post_llm,2023,5,"{'ai_likelihood': 3.65244017706977e-05, 'text': 'Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\n\n  In the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.10369,review,post_llm,2023,5,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Sustainability in Computing Education: A Systematic Literature Review\n\n  Research shows that the global society as organized today, with our current\ntechnological and economic system, is impossible to sustain. We are living in\nthe Anthropocene, an era in which human activities in highly industrialized\ncountries are responsible for overshooting several planetary boundaries, with\npoorer communities contributing least to the problems but being impacted the\nmost. At the same time, technical and economic gains fail to provide society at\nlarge with equal opportunities and improved quality of life. This paper\ndescribes approaches taken in computing education to address the issue of\nsustainability. It presents results of a systematic review of literature on\nsustainability in computing education. From a set of 572 publications extracted\nfrom six large digital libraries plus snowballing, we distilled and analyzed\nthe 90 relevant primary studies. Using an inductive and deductive thematic\nanalysis, we study 1) conceptions of sustainability, computing, and education,\n2) implementations of sustainability in computing education, and 3) research on\nsustainability in computing education. We present a framework capturing\nlearning objectives and outcomes as well as pedagogical methods for\nsustainability in computing education. These results can be mapped to existing\nstandards and curricula in future work. We find that only a few of the articles\nengage with the challenges as calling for drastic systemic change, along with\nradically new understandings of computing and education. We suggest that future\nresearch should connect to the substantial body of critical theory such as\nfeminist theory of science and technology. Existing research on sustainability\nin computing education may be considered as rather immature as the majority of\narticles are experience reports with limited empirical research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.166,regular,post_llm,2023,5,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Temporal Evolution of Risk Behavior in a Disease Spread Simulation\n\n  Human behavior is a dynamic process that evolves with experience.\nUnderstanding the evolution of individual's risk propensity is critical to\ndesign public health interventions to propitiate the adoption of better\nbiosecurity protocols and thus, prevent the transmission of an infectious\ndisease. Using an experimental game that simulates the spread of a disease in a\nnetwork of porcine farms, we measure how learning from experience affects the\nrisk aversion of over $1000$ players. We used a fully automated approach to\nsegment the players into 4 categories based on the temporal trends of their\ngame plays and compare the outcomes of their overall game performance. We found\nthat the risk tolerant group is $50\\%$ more likely to incur an infection than\nthe risk averse one. We also find that while all individuals decrease the\namount of time it takes to make decisions as they become more experienced at\nthe game, we find a group of players with constant decision strategies who\nrapidly decrease their time to make a decision and a second context-aware\ndecision group that contemplates longer before decisions while presumably\nperforming a real-time risk assessment. The behavioral strategies employed by\nplayers in this simulated setting could be used in the future as an early\nwarning signal to identify undesirable biosecurity-related risk aversion\npreferences, or changes in behavior, which may allow for targeted interventions\nto help mitigate them.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.01773,review,post_llm,2023,5,"{'ai_likelihood': 0.31819661458333337, 'text': ""Voluminous yet Vacuous? Semantic Capital in an Age of Large Language\n  Models\n\n  Large Language Models (LLMs) have emerged as transformative forces in the\nrealm of natural language processing, wielding the power to generate human-like\ntext. However, despite their potential for content creation, they carry the\nrisk of eroding our Semantic Capital (SC) - the collective knowledge within our\ndigital ecosystem - thereby posing diverse social epistemic challenges. This\npaper explores the evolution, capabilities, and limitations of these models,\nwhile highlighting ethical concerns they raise. The study contribution is\ntwo-fold: first, it is acknowledged that, withstanding the challenges of\ntracking and controlling LLM impacts, it is necessary to reconsider our\ninteraction with these AI technologies and the narratives that form public\nperception of them. It is argued that before achieving this goal, it is\nessential to confront a potential deontological tipping point in an increasing\nAI-driven infosphere. This goes beyond just adhering to AI ethical norms or\nregulations and requires understanding the spectrum of social epistemic risks\nLLMs might bring to our collective SC. Secondly, building on Luciano Floridi's\ntaxonomy for SC risks, those are mapped within the functionality and\nconstraints of LLMs. By this outlook, we aim to protect and enrich our SC while\nfostering a collaborative environment between humans and AI that augments human\nintelligence rather than replacing it.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.01959,review,post_llm,2023,5,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Putting collective intelligence to the enforcement of the Digital\n  Services Act\n\n  While underlying the many ways to build strong cooperation settings between\nregulators and CSOs, this report focuses on making concrete recommendations for\nthe design of an efficient and influential expert group with the European\nCommission. The creation of an expert group finds its roots in article 64 and\nrecital 137 of the DSA which require the Commission to develop Union expertise\nand capabilities. Once established, the experts of this group will be able to\nbring evidence-based information directly to the Commission and specific\nexpertise on the protection of fundamental rights and the safety of users\nonline. By instituting an expert group, the Commission will not only benefit\nfrom valuable expert knowledge but will also demonstrate its willingness to put\nin place an efficient enforcement system based on collective intelligence.\nAside from the establishment of an expert group, other cumulative mechanisms\nwill also help the DSA's enforcement to thrive. Civil society organisations\nshould, for instance, consider organising regular crowdsourcing events to\ndeep-dive and analyse the data published by entities covered by the\ntransparency obligations. As it has done in the past, the Commission can\nsponsor these events and be a direct beneficiary of their results. Another way\nfor civil society organisations to bring information to the Regulator is by\nlegal action, including by making complaints to the regulators.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.14182,review,post_llm,2023,5,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Ethics in conversation: Building an ethics assurance case for autonomous\n  AI-enabled voice agents in healthcare\n\n  The deployment and use of AI systems should be both safe and broadly\nethically acceptable. The principles-based ethics assurance argument pattern is\none proposal in the AI ethics landscape that seeks to support and achieve that\naim. The purpose of this argument pattern or framework is to structure\nreasoning about, and to communicate and foster confidence in, the ethical\nacceptability of uses of specific real-world AI systems in complex\nsocio-technical contexts. This paper presents the interim findings of a case\nstudy applying this ethics assurance framework to the use of Dora, an AI-based\ntelemedicine system, to assess its viability and usefulness as an approach. The\ncase study process to date has revealed some of the positive ethical impacts of\nthe Dora platform, as well as unexpected insights and areas to prioritise for\nevaluation, such as risks to the frontline clinician, particularly in respect\nof clinician autonomy. The ethics assurance argument pattern offers a practical\nframework not just for identifying issues to be addressed, but also to start to\nconstruct solutions in the form of adjustments to the distribution of benefits,\nrisks and constraints on human autonomy that could reduce ethical disparities\nacross affected stakeholders. Though many challenges remain, this research\nrepresents a step in the direction towards the development and use of safe and\nethically acceptable AI systems and, ideally, a shift towards more\ncomprehensive and inclusive evaluations of AI systems in general.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.01137,review,post_llm,2023,5,"{'ai_likelihood': 0.95849609375, 'text': 'Toward Learning Societies for Digital Aging\n\n  The global aging population presents significant challenges for societies\nworldwide, particularly in an increasingly digitalized era. The Learning\nSociety is crucial in preparing different societies and their people to address\nthese challenges effectively. This paper extends this concept and proposes a\nnew conceptual framework, Learning Societies for Digital Aging, empowering all\nmembers across various sectors from different ages to acquire and develop the\nnecessary knowledge, skills, and competencies to navigate and thrive in an\nincreasingly digital world. It presents seven guiding principles for developing\nthis conceptual framework: 1) Centering Humanistic Values, 2) Embracing\nDigital, 3) Cultivating Learning Societies, 4) Advancing Inclusiveness, 5)\nTaking Holistic Approaches, 6) Encouraging Global Knowledge Sharing, and 7)\nFostering Adaptability. By integrating these guiding principles into the\ndesign, implementation, and evaluation of formal, nonformal, and informal\nlearning opportunities for people of all ages, stakeholders can contribute to\ncreating and nurturing learning societies that cater to aging populations in\nthe digital world. This paper aims to provide a foundation for further research\nand action toward building more inclusive, adaptive, and supportive learning\nenvironments that address the challenges of digital aging and foster more\nempathetic, informed, and prepared societies for the future of aging.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0091705322265625, 'GPT4': 0.21923828125, 'CLAUDE': 0.004383087158203125, 'GOOGLE': 0.75341796875, 'OPENAI_O_SERIES': 0.0025177001953125, 'DEEPSEEK': 4.76837158203125e-05, 'GROK': 5.543231964111328e-06, 'NOVA': 5.0127506256103516e-05, 'OTHER': 0.0011501312255859375, 'HUMAN': 0.01023101806640625}}"
2305.16821,review,post_llm,2023,5,"{'ai_likelihood': 0.9755859375, 'text': 'Transitioning towards fit-for-purpose Public Health Surveillance Systems\n\n  The COVID-19 pandemic has exposed several weaknesses in the public health\ninfrastructure, including supply chain mechanisms and public health ICT\nsystems. The expansion of testing and contact tracing has been key to\nidentifying and isolating infected individuals, as well as tracking and\ncontaining the spread of the virus. Digital technologies, such as telemedicine\nand virtual consultations, have experienced a surge in demand to provide\nmedical support while minimizing the risk of transmission and infection. The\npandemic has made it clear that cooperation, information sharing, and\ncommunication among stakeholders are crucial in making the right decisions and\npreventing future outbreaks. Redesigning public health systems for effective\nmanagement of outbreaks should include five key elements: disease surveillance\nand early warning systems, contact tracing and case management, data analytics\nand visualization, communication and education, and telemedicine. As the world\nnavigates the COVID-19 pandemic, healthcare ICT systems will play an\nincreasingly important role in the future of healthcare delivery. In a post\nCOVID-19 world, several ICT strategies should be implemented to improve the\nquality, efficiency, and accessibility of healthcare services, including the\nexpansion of telemedicine, data analytics and population health management,\ninteroperability, and cybersecurity. Overall, this report summarises the\nimportance of early detection and rapid response, international cooperation and\ncoordination, clear and consistent communication, investing in public health\nsystems and emergency preparedness, digital technology and telemedicine, and\nequity and social determinants of health. These lessons demonstrate the need\nfor better preparedness and planning for future crises and the importance of\naddressing underlying issues to create a more resilient and accessible digital\ninfrastructure.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.9677734375, 'GPT4': 0.00753021240234375, 'CLAUDE': 4.678964614868164e-05, 'GOOGLE': 0.022216796875, 'OPENAI_O_SERIES': 5.799531936645508e-05, 'DEEPSEEK': 1.430511474609375e-06, 'GROK': 3.5762786865234375e-07, 'NOVA': 1.7285346984863281e-06, 'OTHER': 0.0014591217041015625, 'HUMAN': 0.0008325576782226562}}"
2305.03267,regular,post_llm,2023,5,"{'ai_likelihood': 3.0795733133951826e-06, 'text': ""Forecasting Inter-Destination Tourism Flow via a Hybrid Deep Learning\n  Model\n\n  Tourists often go to multiple tourism destinations in one trip. The volume of\ntourism flow between tourism destinations, also referred to as ITF\n(Inter-Destination Tourism Flow) in this paper, is commonly used for tourism\nmanagement on tasks like the classification of destinations' roles and\nvisitation pattern mining. However, the ITF is hard to get due to the\nlimitation of data collection techniques and privacy issues. It is difficult to\nunderstand how the volume of ITF is influenced by features of the\nmulti-attraction system. To address these challenges, we utilized multi-source\ndatasets and proposed a graph-based hybrid deep learning model to predict the\nITF. The model makes use of both the explicit features of individual tourism\nattractions and the implicit features of the interactions between multiple\nattractions. Experiments on ITF data extracted from crowdsourced tourists'\ntravel notes about the city of Beijing verified the usefulness of the proposed\nmodel. Besides, we analyze how different features of tourism attractions\ninfluence the volume of ITF with explainable AI techniques. Results show that\npopularity, quality and distance are the main three influential factors. Other\nfeatures like coordinates will also exert an influence in different ways. The\npredicted ITF data can be further used for various downstream tasks in tourism\nmanagement. The research also deepens the understanding of tourists' visiting\nchoice in a tourism system consisting of multiple attractions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.0242,review,post_llm,2023,5,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Beyond case studies: Teaching data science critique and ethics through\n  sociotechnical surveillance studies\n\n  Ethics have become an urgent concern for data science research, practice, and\ninstruction in the wake of growing critique of algorithms and systems showing\nthat they reinforce structural oppression. There has been increasing desire on\nthe part of data science educators to craft curricula that speak to these\ncritiques, yet much ethics education remains individualized, focused on\nspecific cases, or too abstract and unapplicable. We synthesized some of the\nmost popular critical data science works and designed a data science ethics\ncourse that spoke to the social phenomena at the root of critical data studies\n-- theories of oppression, social systems, power, history, and change --\nthrough analysis of a pressing sociotechnical system: surveillance systems.\nThrough analysis of student reflections and final projects, we determined that\nat the conclusion of the semester, all students had developed critical analysis\nskills that allowed them to investigate surveillance systems of their own and\nidentify their benefits, harms, main proponents, those who resist them, and\ntheir interplay with social systems, all while considering dimensions of race,\nclass, gender, and more. We argue that this type of instruction -- directly\nteaching data science ethics alongside social theory -- is a crucial next step\nfor the field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.19526,regular,post_llm,2023,5,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""The competent Computational Thinking test (cCTt): a valid, reliable and\n  gender-fair test for longitudinal CT studies in grades 3-6\n\n  The introduction of computing education into curricula worldwide requires\nmulti-year assessments to evaluate the long-term impact on learning. However,\nno single Computational Thinking (CT) assessment spans primary school, and no\ngroup of CT assessments provides a means of transitioning between instruments.\nThis study therefore investigated whether the competent CT test (cCTt) could\nevaluate learning reliably from grades 3 to 6 (ages 7-11) using data from 2709\nstudents. The psychometric analysis employed Classical Test Theory, Item\nResponse Theory, Measurement Invariance analyses which include Differential\nItem Functioning, normalised z-scoring, and PISA's methodology to establish\nproficiency levels. The findings indicate that the cCTt is valid, reliable and\ngender-fair for grades 3-6, although more complex items would be beneficial for\ngrades 5-6. Grade-specific proficiency levels are provided to help tailor\ninterventions, with a normalised scoring system to compare students across and\nbetween grades, and help establish transitions between instruments. To improve\nthe utility of CT assessments among researchers, educators and practitioners,\nthe findings emphasise the importance of i) developing and validating\ngender-fair, grade-specific, instruments aligned with students' cognitive\nmaturation, and providing ii) proficiency levels, and iii) equivalency scales\nto transition between assessments. To conclude, the study provides insight into\nthe design of longitudinal developmentally appropriate assessments and\ninterventions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.07399,review,post_llm,2023,5,"{'ai_likelihood': 5.7948960198296445e-06, 'text': ""Conceptualizing A Multi-Sided Platform For Cloud Computing Resource\n  Trading\n\n  Cost-effective and responsible use of cloud computing resources (CCR) is on\nthe business agenda of many companies. Despite this strategic goal, two\ngeopolitical strategy decisions mainly influence the continuous existence of\novercapacity: Europe's General Data Protection Regulation and the US's Cloud\nAct. Given the circumstances, a typical data center produces approximately 30%\novercapacity annually. This overcapacity has severe environmental and economic\nconsequences. Our work addresses this overcapacity by proposing a multi-sided\nplatform for CCR trading. We initiate our research by conducting a literature\nreview to explore the existing body of knowledge which indicates a lack of\nrecent and evaluated platform design knowledge for CCR trading. We address this\nresearch gap by deriving design requirements and design principles. We\ninstantiate and evaluate the design knowledge in a respective platform\nframework. Thus, we contribute to research and practice by deriving and\nevaluating design knowledge and proposing a platform framework.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.14086,regular,post_llm,2023,5,"{'ai_likelihood': 1.0, 'text': ""Predicting Survey Response with Quotation-based Modeling: A Case Study\n  on Favorability towards the United States\n\n  The acquisition of survey responses is a crucial component in conducting\nresearch aimed at comprehending public opinion. However, survey data collection\ncan be arduous, time-consuming, and expensive, with no assurance of an adequate\nresponse rate. In this paper, we propose a pioneering approach for predicting\nsurvey responses by examining quotations using machine learning. Our\ninvestigation focuses on evaluating the degree of favorability towards the\nUnited States, a topic of interest to many organizations and governments. We\nleverage a vast corpus of quotations from individuals across different\nnationalities and time periods to extract their level of favorability. We\nemploy a combination of natural language processing techniques and machine\nlearning algorithms to construct a predictive model for survey responses. We\ninvestigate two scenarios: first, when no surveys have been conducted in a\ncountry, and second when surveys have been conducted but in specific years and\ndo not cover all the years. Our experimental results demonstrate that our\nproposed approach can predict survey responses with high accuracy. Furthermore,\nwe provide an exhaustive analysis of the crucial features that contributed to\nthe model's performance. This study has the potential to impact survey research\nin the field of data science by substantially decreasing the cost and time\nrequired to conduct surveys while simultaneously providing accurate predictions\nof public opinion.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.96142578125, 'GPT4': 0.019866943359375, 'CLAUDE': 1.4007091522216797e-05, 'GOOGLE': 0.0168609619140625, 'OPENAI_O_SERIES': 4.220008850097656e-05, 'DEEPSEEK': 5.364418029785156e-07, 'GROK': 7.152557373046875e-07, 'NOVA': 1.8477439880371094e-06, 'OTHER': 0.0018978118896484375, 'HUMAN': 6.377696990966797e-06}}"
2306.00227,review,post_llm,2023,5,"{'ai_likelihood': 3.182225757175022e-05, 'text': 'From human-centered to social-centered artificial intelligence:\n  Assessing ChatGPT\'s impact through disruptive events\n\n  Large language models (LLMs) and dialogue agents represent a significant\nshift in artificial intelligence (AI) research, particularly with the recent\nrelease of the GPT family of models. ChatGPT\'s generative capabilities and\nversatility across technical and creative domains led to its widespread\nadoption, marking a departure from more limited deployments of previous AI\nsystems. While society grapples with the emerging cultural impacts of this new\nsocietal-scale technology, critiques of ChatGPT\'s impact within machine\nlearning research communities have coalesced around its performance or other\nconventional safety evaluations relating to bias, toxicity, and\n""hallucination."" We argue that these critiques draw heavily on a particular\nconceptualization of the ""human-centered"" framework, which tends to cast\natomized individuals as the key recipients of technology\'s benefits and\ndetriments. In this article, we direct attention to another dimension of LLMs\nand dialogue agents\' impact: their effects on social groups, institutions, and\naccompanying norms and practices. By analyzing ChatGPT\'s social impact through\na social-centered framework, we challenge individualistic approaches in AI\ndevelopment and contribute to ongoing debates around the ethical and\nresponsible deployment of AI systems. We hope this effort will call attention\nto more comprehensive and longitudinal evaluation tools (e.g., including more\nethnographic analyses and participatory approaches) and compel technologists to\ncomplement human-centered thinking with social-centered approaches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.19552,review,post_llm,2023,5,"{'ai_likelihood': 2.5762452019585504e-05, 'text': ""Investigating Gender Euphoria and Dysphoria on TikTok: Characterization\n  and Comparison\n\n  With the emergence of short video-sharing platforms, engagement with social\nmedia sites devoted to opinion and knowledge dissemination has rapidly\nincreased. Among these platforms, TikTok is one of the most popular globally\nand has become the platform of choice for transgender and nonbinary\nindividuals, who have formed a large community to mobilize personal experience\nand exchange information. The knowledge produced in online spaces can influence\nthe ways in which people understand and experience their own gender and\ntransitions, as they hear about others and weigh experiential and medical\nknowledge against their own. This paper extends current research and past\ninterview methods on gender euphoria and gender dysphoria to analyze what and\nhow online communities on TikTok discuss these two types of gender experiences.\nOur findings indicate that gender euphoria and gender dysphoria are differently\ndescribed in online TikTok spaces. These findings indicate similarities in the\nwords used to describe gender dysphoria as well as gender euphoria in both the\ncomments of videos and content creators' hashtags. Finally, our results show\nthat gender euphoria is described in more similar terms between transfeminine\nand transmasculine experiences than gender dysphoria, which appears to be more\ndifferentiated by gendering experience and transition goals. We hope this paper\ncan provide insights for future research on understanding transgender and\nnonbinary individuals in online communities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.1216,regular,post_llm,2023,5,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Park visitation and walkshed demographics in the United States\n\n  A large and growing body of research demonstrates the value of local parks to\nmental and physical well-being. Recently, researchers have begun using passive\ndigital data sources to investigate equity in usage; exactly who is benefiting\nfrom parks? Early studies suggest that park visitation differs according to\ndemographic features, and that the demographic composition of a park's\nsurrounding neighborhood may be related to the utilization a park receives.\nEmploying a data set of park visitations generated by observations of roughly\n50 million mobile devices in the US in 2019, we assess the ability of the\ndemographic composition of a park's walkshed to predict its yearly visitation.\nPredictive models are constructed using Support Vector Regression, LASSO,\nElastic Net, and Random Forests. Surprisingly, our results suggest that the\ndemographic composition of a park's walkshed demonstrates little to no utility\nfor predicting visitation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.07153,review,post_llm,2023,5,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Towards best practices in AGI safety and governance: A survey of expert\n  opinion\n\n  A number of leading AI companies, including OpenAI, Google DeepMind, and\nAnthropic, have the stated goal of building artificial general intelligence\n(AGI) - AI systems that achieve or exceed human performance across a wide range\nof cognitive tasks. In pursuing this goal, they may develop and deploy AI\nsystems that pose particularly significant risks. While they have already taken\nsome measures to mitigate these risks, best practices have not yet emerged. To\nsupport the identification of best practices, we sent a survey to 92 leading\nexperts from AGI labs, academia, and civil society and received 51 responses.\nParticipants were asked how much they agreed with 50 statements about what AGI\nlabs should do. Our main finding is that participants, on average, agreed with\nall of them. Many statements received extremely high levels of agreement. For\nexample, 98% of respondents somewhat or strongly agreed that AGI labs should\nconduct pre-deployment risk assessments, dangerous capabilities evaluations,\nthird-party model audits, safety restrictions on model usage, and red teaming.\nUltimately, our list of statements may serve as a helpful foundation for\nefforts to develop best practices, standards, and regulations for AGI labs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.1521,regular,post_llm,2023,5,"{'ai_likelihood': 8.609559800889758e-06, 'text': 'Detecting disparities in police deployments using dashcam data\n\n  Large-scale policing data is vital for detecting inequity in police behavior\nand policing algorithms. However, one important type of policing data remains\nlargely unavailable within the United States: aggregated police deployment data\ncapturing which neighborhoods have the heaviest police presences. Here we show\nthat disparities in police deployment levels can be quantified by detecting\npolice vehicles in dashcam images of public street scenes. Using a dataset of\n24,803,854 dashcam images from rideshare drivers in New York City, we find that\npolice vehicles can be detected with high accuracy (average precision 0.82, AUC\n0.99) and identify 233,596 images which contain police vehicles. There is\nsubstantial inequality across neighborhoods in police vehicle deployment\nlevels. The neighborhood with the highest deployment levels has almost 20 times\nhigher levels than the neighborhood with the lowest. Two strikingly different\ntypes of areas experience high police vehicle deployments - 1) dense,\nhigher-income, commercial areas and 2) lower-income neighborhoods with higher\nproportions of Black and Hispanic residents. We discuss the implications of\nthese disparities for policing equity and for algorithms trained on policing\ndata.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.01708,regular,post_llm,2023,5,"{'ai_likelihood': 1.5497207641601562e-05, 'text': 'Exploring Xenophobic Events through GDELT Data Analysis\n\n  This study explores xenophobic events related to refugees and migration using\nthe GDELT 2.0 database and APIs through visualizations. We conducted two case\nstudies -- the first being an analysis of refugee-related news following the\ndeath of a two-year-old Syrian boy, Alan Kurdi, and the second a surge in news\narticles in March 2021 based on the data obtained from GDELT API. In addition\nto the two case studies, we present a discussion of our exploratory data\nanalysis steps and the challenges encountered while working with GDELT data and\nits tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.07575,regular,post_llm,2023,5,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""The Progression of Disparities within the Criminal Justice System:\n  Differential Enforcement and Risk Assessment Instruments\n\n  Algorithmic risk assessment instruments (RAIs) increasingly inform\ndecision-making in criminal justice. RAIs largely rely on arrest records as a\nproxy for underlying crime. Problematically, the extent to which arrests\nreflect overall offending can vary with the person's characteristics. We\nexamine how the disconnect between crime and arrest rates impacts RAIs and\ntheir evaluation. Our main contribution is a method for quantifying this bias\nvia estimation of the amount of unobserved offenses associated with particular\ndemographics. These unobserved offenses are then used to augment real-world\narrest records to create part real, part synthetic crime records. Using this\ndata, we estimate that four currently deployed RAIs assign 0.5--2.8 percentage\npoints higher risk scores to Black individuals than to White individuals with a\nsimilar \\emph{arrest} record, but the gap grows to 4.5--11.0 percentage points\nwhen we match on the semi-synthetic \\emph{crime} record. We conclude by\ndiscussing the potential risks around the use of RAIs, highlighting how they\nmay exacerbate existing inequalities if the underlying disparities of the\ncriminal justice system are not taken into account. In light of our findings,\nwe provide recommendations to improve the development and evaluation of such\ntools.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.13238,review,post_llm,2023,5,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""The Dimensions of Data Labor: A Road Map for Researchers, Activists, and\n  Policymakers to Empower Data Producers\n\n  Many recent technological advances (e.g. ChatGPT and search engines) are\npossible only because of massive amounts of user-generated data produced\nthrough user interactions with computing systems or scraped from the web (e.g.\nbehavior logs, user-generated content, and artwork). However, data producers\nhave little say in what data is captured, how it is used, or who it benefits.\nOrganizations with the ability to access and process this data, e.g. OpenAI and\nGoogle, possess immense power in shaping the technology landscape. By\nsynthesizing related literature that reconceptualizes the production of data\nfor computing as ``data labor'', we outline opportunities for researchers,\npolicymakers, and activists to empower data producers in their relationship\nwith tech companies, e.g advocating for transparency about data reuse, creating\nfeedback channels between data producers and companies, and potentially\ndeveloping mechanisms to share data's revenue more broadly. In doing so, we\ncharacterize data labor with six important dimensions - legibility, end-use\nawareness, collaboration requirement, openness, replaceability, and livelihood\noverlap - based on the parallels between data labor and various other types of\nlabor in the computing literature.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.08108,review,post_llm,2023,5,"{'ai_likelihood': 1.2583202785915798e-06, 'text': ""Integrating GIS into Hong Kong Secondary School Geography Curriculum\n\n  Hong Kong' senior geography curriculum has included GIS since the early\n2000s. However, GIS in secondary schools does not play a significant role in\nHong Kong secondary geography education. Analyzing GIS benefits by literature\nreview, it is believed that GIS should be included in both the senior and\njunior geography curriculum. Moreover, the literature review indicates that\nwithout clear instruction from the Hong Kong Education Bureau (EDB), low\npreparedness of Hong Kong geography teachers, and unsupportive attitudes from\nacademia and textbook publishers, GIS cannot be implemented in secondary\nschools of Hong Kong. Therefore, suggestions are made for the EDB, geography\nteachers, academia and textbook publishers to facilitate GIS involvement in\nsenior and junior geography curriculums. The EDB can develop clear guidelines\nfor teachers, academia and textbook publishers' references, and offer\nstudent-centered GIS educational courses for teachers. It is important for\nteachers to be prepared for advanced GIS technology and to even learn along\nwith students. Academics and textbook publishers can provide free GIS maps\ntargeted at Hong Kong' junior and senior geography curriculums. Although the\nreport provides brief information towards the GIS implementation in Hong Kong\ngeography education, it can inspire new ideas from other scholars to facilitate\nthe usage of GIS in Hong Kong secondary school geography teaching.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.186,review,post_llm,2023,5,"{'ai_likelihood': 5.9107939402262374e-05, 'text': ""Despertando o Interesse de Mulheres para os Cursos em STEM\n\n  This article presents initiatives aimed at promoting female participation in\nSTEM fields, with the goal of encouraging more women to pursue careers in these\nareas. One of these initiatives is the Em\\'ilias - Arma\\c{c}\\~ao em Bits\nProject, which organizes workshops in schools. Additionally, a podcast has been\ncreated to foster interaction between young people and professionals in the\nfield of computing, while also contributing to the establishment of female role\nmodels in the industry. The results of these initiatives have been promising,\nas 70.6% of the students who participated in the workshops expressed an\ninterest in computing. Furthermore, according to Spotify, the podcast's\naudience consists of 53% females, 44% males, and 3% unspecified, indicating\nthat it has successfully reached a female demographic.\n  Resumo. Este artigo apresenta iniciativas que t\\^em como objetivo promover a\nparticipa\\c{c}\\~ao das mulheres nas \\'areas de STEM, buscando encorajar mais\nmulheres a seguirem carreiras nesses campos. O Projeto Em\\'ilias -\nArma\\c{c}\\~ao em Bits desenvolve oficinas nas escolas e tamb\\'em um podcast,\npromovendo a intera\\c{c}\\~ao entre jovens e profissionais da \\'area de\ncomputa\\c{c}\\~ao, al\\'em de contribuir para a forma\\c{c}\\~ao de modelos\nfemininos nesse campo. Os resultados demonstraram que 70,6% das estudantes\ndemonstraram interesse pela computa\\c{c}\\~ao ap\\'os participarem das oficinas.\nEm rela\\c{c}\\~ao aos ouvintes do podcast, dados do Spotify indicaram que 53% do\np\\'ublico se identifica como feminino, 44% como masculino, e 3% n\\~ao\nespecificaram o g\\^enero, o que mostra que o podcast tem alcan\\c{c}ado um\np\\'ublico feminino.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.08326,review,post_llm,2023,5,"{'ai_likelihood': 1.0, 'text': ""Learner-Centered Analysis in Educational Metaverse Environments:\n  Exploring Value Exchange Systems through Natural Interaction and Text Mining\n\n  This paper explores the potential developments of self-directed learning in\nthe metaverse in response to Education 4.0 and the Fourth Industrial\nRevolution. It highlights the importance of education keeping up with\ntechnological advancements and adopting learner-centered approaches.\nAdditionally, it focuses on exploring value exchange systems through natural\ninteraction, text mining, and analysis. The metaverse concept extends beyond\nextended reality (XR) technologies, encompassing digital avatars and shared\necological value. The role of educators in exploring new technologies and\nleveraging text-mining techniques to enhance learning efficiency is emphasized.\nThe metaverse is presented as a platform for value exchange, necessitating\nmeaningful and valuable content to attract users. Integrating virtual and\nreal-world experiences within the metaverse offers practical applications and\ncontributes to its essence. This paper sheds light on the metaverse's potential\nto create a learner-centered educational environment and adapt to the evolving\nlandscape of Education 4.0. Its findings, supported by text mining analysis,\ncontribute to understanding the metaverse's role in shaping education in the\nFourth Industrial Revolution.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.3525390625, 'GPT4': 0.01445770263671875, 'CLAUDE': 0.00124359130859375, 'GOOGLE': 0.62109375, 'OPENAI_O_SERIES': 0.0008063316345214844, 'DEEPSEEK': 5.257129669189453e-05, 'GROK': 1.9669532775878906e-06, 'NOVA': 0.00011938810348510742, 'OTHER': 0.00992584228515625, 'HUMAN': 3.874301910400391e-06}}"
2305.183,review,post_llm,2023,5,"{'ai_likelihood': 2.1192762586805556e-06, 'text': ""Investigation of how social media influenced the endsars protests in\n  Lagos, Nigeria\n\n  The research assessed the role of social media in the unfolding of the\nEndSARS demonstrations in Nigeria. The study was necessary given the persistent\ncall by governments for the strict regulation of social medium platforms. This\nis given governments' claim that social media is being misused. The study,\nhowever, reveals that social media use is determined by users' social\nexperiences, including those caused by governments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.17038,review,post_llm,2023,5,"{'ai_likelihood': 2.8477774726019966e-06, 'text': ""Frontier AI developers need an internal audit function\n\n  This article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.05733,review,post_llm,2023,5,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Algorithms as Social-Ecological-Technological Systems: an Environmental\n  Justice Lens on Algorithmic Audits\n\n  This paper reframes algorithmic systems as intimately connected to and part\nof social and ecological systems, and proposes a first-of-its-kind methodology\nfor environmental justice-oriented algorithmic audits. How do we consider\nenvironmental and climate justice dimensions of the way algorithmic systems are\ndesigned, developed, and deployed? These impacts are inherently emergent and\ncan only be understood and addressed at the level of relations between an\nalgorithmic system and the social (including institutional) and ecological\ncomponents of the broader ecosystem it operates in. As a result, we claim that\nin absence of an integral ontology for algorithmic systems, we cannot do\njustice to the emergent nature of broader environmental impacts of algorithmic\nsystems and their underlying computational infrastructure. We propose to define\nalgorithmic systems as ontologically indistinct from\nSocial-Ecological-Technological Systems (SETS), framing emergent implications\nas couplings between social, ecological, and technical components of the\nbroader fabric in which algorithms are integrated and operate. We draw upon\nprior work on SETS analysis as well as emerging themes in the literature and\npractices of Environmental Justice (EJ) to conceptualize and assess algorithmic\nimpact. We then offer three policy recommendations to help establish a\nSETS-based EJ approach to algorithmic audits: (1) broaden the inputs and\nopen-up the outputs of an audit, (2) enable meaningful access to redress, and\n(3) guarantee a place-based and relational approach to the process of\nevaluating impact. We operationalize these as a qualitative framework of\nquestions for a spectrum of stakeholders. Doing so, this article aims to\ninspire stronger and more frequent interactions across policymakers,\nresearchers, practitioners, civil society, and grassroots communities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.18159,review,post_llm,2023,5,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'The Misuse of AUC: What High Impact Risk Assessment Gets Wrong\n\n  When determining which machine learning model best performs some high impact\nrisk assessment task, practitioners commonly use the Area under the Curve (AUC)\nto defend and validate their model choices. In this paper, we argue that the\ncurrent use and understanding of AUC as a model performance metric\nmisunderstands the way the metric was intended to be used. To this end, we\ncharacterize the misuse of AUC and illustrate how this misuse negatively\nmanifests in the real world across several risk assessment domains. We locate\nthis disconnect in the way the original interpretation of AUC has shifted over\ntime to the point where issues pertaining to decision thresholds, class\nbalance, statistical uncertainty, and protected groups remain unaddressed by\nAUC-based model comparisons, and where model choices that should be the purview\nof policymakers are hidden behind the veil of mathematical rigor. We conclude\nthat current model validation practices involving AUC are not robust, and often\ninvalid.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.09059,review,post_llm,2023,5,"{'ai_likelihood': 9.5367431640625e-06, 'text': 'Response to ""The digital pound: a new form of money for households and\n  businesses""\n\n  This document constitutes a response to a Consultation Paper published by the\nBank of England and HM Treasury, ""The digital pound: a new form of money for\nhouseholds and businesses?"", the latest document in a series that includes\n""Central Bank Digital Currency: opportunities, challenges and design"" in 2020\nand ""New forms of digital money"" in 2021. The Consultation Paper concerns the\nadoption of central bank digital currency (CBDC) for retail use in the United\nKingdom by the Bank of England. We shall address the consultation questions\ndirectly in the third section of this document.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.01185,review,post_llm,2023,5,"{'ai_likelihood': 1.0, 'text': 'The AI Revolution in Education: Will AI Replace or Assist Teachers in\n  Higher Education?\n\n  This paper explores the potential of artificial intelligence (AI) in higher\neducation, specifically its capacity to replace or assist human teachers. By\nreviewing relevant literature and analysing survey data from students and\nteachers, the study provides a comprehensive perspective on the future role of\neducators in the face of advancing AI technologies. Findings suggest that\nalthough some believe AI may eventually replace teachers, the majority of\nparticipants argue that human teachers possess unique qualities, such as\ncritical thinking, creativity, and emotions, which make them irreplaceable. The\nstudy also emphasizes the importance of social-emotional competencies developed\nthrough human interactions, which AI technologies cannot currently replicate.\nThe research proposes that teachers can effectively integrate AI to enhance\nteaching and learning without viewing it as a replacement. To do so, teachers\nneed to understand how AI can work well with teachers and students while\navoiding potential pitfalls, develop AI literacy, and address practical issues\nsuch as data protection, ethics, and privacy. The study reveals that students\nvalue and respect human teachers, even as AI becomes more prevalent in\neducation. The study also introduces a roadmap for students, teachers, and\nuniversities. This roadmap serves as a valuable guide for refining teaching\nskills, fostering personal connections, and designing curriculums that\neffectively balance the strengths of human educators with AI technologies. The\nfuture of education lies in the synergy between human teachers and AI. By\nunderstanding and refining their unique qualities, teachers, students, and\nuniversities can effectively navigate the integration of AI, ensuring a\nwell-rounded and impactful learning experience.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.006427764892578125, 'GPT4': 0.70654296875, 'CLAUDE': 2.8192996978759766e-05, 'GOOGLE': 0.285400390625, 'OPENAI_O_SERIES': 3.457069396972656e-05, 'DEEPSEEK': 9.5367431640625e-07, 'GROK': 2.384185791015625e-07, 'NOVA': 1.6689300537109375e-06, 'OTHER': 0.001399993896484375, 'HUMAN': 5.269050598144531e-05}}"
2305.04008,review,post_llm,2023,5,"{'ai_likelihood': 2.615981631808811e-06, 'text': ""Algorithmic Bias, Generalist Models,and Clinical Medicine\n\n  The technical landscape of clinical machine learning is shifting in ways that\ndestabilize pervasive assumptions about the nature and causes of algorithmic\nbias. On one hand, the dominant paradigm in clinical machine learning is narrow\nin the sense that models are trained on biomedical datasets for particular\nclinical tasks such as diagnosis and treatment recommendation. On the other\nhand, the emerging paradigm is generalist in the sense that general-purpose\nlanguage models such as Google's BERT and PaLM are increasingly being adapted\nfor clinical use cases via prompting or fine-tuning on biomedical datasets.\nMany of these next-generation models provide substantial performance gains over\nprior clinical models, but at the same time introduce novel kinds of\nalgorithmic bias and complicate the explanatory relationship between\nalgorithmic biases and biases in training data. This paper articulates how and\nin what respects biases in generalist models differ from biases in prior\nclinical models, and draws out practical recommendations for algorithmic bias\nmitigation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2305.18717,review,post_llm,2023,5,"{'ai_likelihood': 7.417466905381945e-06, 'text': ""Student Usage of Q&A Forums: Signs of Discomfort?\n\n  Q&A forums are widely used in large classes to provide scalable support. In\naddition to offering students a space to ask questions, these forums aim to\ncreate a community and promote engagement. Prior literature suggests that the\nway students participate in Q&A forums varies and that most students do not\nactively post questions or engage in discussions. Students may display\ndifferent participation behaviours depending on their comfort levels in the\nclass. This paper investigates students' use of a Q&A forum in a CS1 course. We\nalso analyze student opinions about the forum to explain the observed\nbehaviour, focusing on students' lack of visible participation (lurking,\nanonymity, private posting). We analyzed forum data collected in a CS1 course\nacross two consecutive years and invited students to complete a survey about\nperspectives on their forum usage. Despite a small cohort of highly engaged\nstudents, we confirmed that most students do not actively read or post on the\nforum. We discuss students' reasons for the low level of engagement and\nbarriers to participating visibly. Common reasons include fearing a lack of\nknowledge and repercussions from being visible to the student community.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.02751,regular,post_llm,2023,6,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""An Adapted Cascade Model to Scale Primary School Digital Education\n  Curricular Reforms and Teacher Professional Development Programs\n\n  Many countries struggle to effectively introduce Digital Education (DE) to\nall K-12 students as they lack adequately trained teachers. While cascade\nmodels of in-service teacher-professional development (PD) can rapidly deploy\nPD-programs through multiple levels of trainers to reach all teachers, they\nsuffer from many limitations and are often ineffective. We therefore propose an\nadapted cascade model to deploy a primary school DE teacher-PD program\nthroughout an administrative region. The model relies on teacher-trainers who\n(i) are active teachers in the region, (ii) have a prolonged trainer-PD with\nexperts who piloted the teacher-PD program to acquire adult-trainer and\nDE-related competences, and (iii) are supported by the experts throughout the\ndeployment. To validate the deployment model we used data from 14\nteacher-trainers, the 700 teachers they trained, and 350 teachers trained by\nexperts. The teacher-trainer findings demonstrate that the adapted cascade\nmodel effectively addresses most cascade models' limitations. The\nteacher-related findings further validate the adapted cascade model in terms of\nperception, motivation and adoption which are at least equivalent to those\nobtained with the experts. To conclude, the adapted cascade model is an\neffective means of spreading primary school DE PD-programs at a large scale and\ncan be used in other DE reforms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.01815,regular,post_llm,2023,6,"{'ai_likelihood': 0.4397243923611111, 'text': 'Prototyping the use of Large Language Models (LLMs) for adult learning\n  content creation at scale\n\n  As Large Language Models (LLMs) and other forms of Generative AI permeate\nvarious aspects of our lives, their application for learning and education has\nprovided opportunities and challenges. This paper presents an investigation\ninto the use of LLMs in asynchronous course creation, particularly within the\ncontext of adult learning, training and upskilling. We developed a course\nprototype leveraging an LLM, implementing a robust human-in-the-loop process to\nensure the accuracy and clarity of the generated content. Our research\nquestions focus on the feasibility of LLMs to produce high-quality adult\nlearning content with reduced human involvement. Initial findings indicate that\ntaking this approach can indeed facilitate faster content creation without\ncompromising on accuracy or clarity, marking a promising advancement in the\nfield of Generative AI for education. Despite some limitations, the study\nunderscores the potential of LLMs to transform the landscape of learning and\neducation, necessitating further research and nuanced discussions about their\nstrategic and ethical use in learning design.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.03686,review,post_llm,2023,6,"{'ai_likelihood': 1.0, 'text': ""A review of dental informatics : current trends and future directions\n\n  Dental informatics is a rapidly evolving field that combines dentistry with\ninformation technology to improve oral health care delivery, research, and\neducation. Electronic health records (EHRs), telehealth, digital imaging, and\nother digital tools have revolutionised how dental professionals diagnose,\ntreat, and manage oral health conditions. In this review article, we will\nexplore dental informatics's current trends and future directions, focusing on\nits impact on clinical practice, research, and education. We will also discuss\nthe challenges and opportunities associated with implementing dental\ninformatics and highlight fundamental research studies and innovations in the\nfield.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1397705078125, 'GPT4': 0.001178741455078125, 'CLAUDE': 0.0026836395263671875, 'GOOGLE': 0.5234375, 'OPENAI_O_SERIES': 0.0003879070281982422, 'DEEPSEEK': 1.4066696166992188e-05, 'GROK': 3.4689903259277344e-05, 'NOVA': 0.00020444393157958984, 'OTHER': 0.33251953125, 'HUMAN': 4.231929779052734e-06}}"
2306.06773,regular,post_llm,2023,6,"{'ai_likelihood': 3.837876849704319e-05, 'text': ""Gamified Crowdsourcing as a Novel Approach to Lung Ultrasound Dataset\n  Labeling\n\n  Study Objective: Machine learning models have advanced medical image\nprocessing and can yield faster, more accurate diagnoses. Despite a wealth of\navailable medical imaging data, high-quality labeled data for model training is\nlacking. We investigated whether a gamified crowdsourcing platform enhanced\nwith inbuilt quality control metrics can produce lung ultrasound clip labels\ncomparable to those from clinical experts.\n  Methods: 2,384 lung ultrasound clips were retrospectively collected from 203\npatients. Six lung ultrasound experts classified 393 of these clips as having\nno B-lines, one or more discrete B-lines, or confluent B-lines to create two\nsets of reference standard labels (195 training set clips and 198 test set\nclips). Sets were respectively used to A) train users on a gamified\ncrowdsourcing platform, and B) compare concordance of the resulting crowd\nlabels to the concordance of individual experts to reference standards.\n  Results: 99,238 crowdsourced opinions on 2,384 lung ultrasound clips were\ncollected from 426 unique users over 8 days. On the 198 test set clips, mean\nlabeling concordance of individual experts relative to the reference standard\nwas 85.0% +/- 2.0 (SEM), compared to 87.9% crowdsourced label concordance\n(p=0.15). When individual experts' opinions were compared to reference standard\nlabels created by majority vote excluding their own opinion, crowd concordance\nwas higher than the mean concordance of individual experts to reference\nstandards (87.4% vs. 80.8% +/- 1.6; p<0.001).\n  Conclusion: Crowdsourced labels for B-line classification via a gamified\napproach achieved expert-level quality. Scalable, high-quality labeling\napproaches may facilitate training dataset creation for machine learning model\ndevelopment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.06749,review,post_llm,2023,6,"{'ai_likelihood': 9.271833631727431e-07, 'text': ""Implementing AI Ethics: Making Sense of the Ethical Requirements\n\n  Society's increasing dependence on Artificial Intelligence (AI) and\nAI-enabled systems require a more practical approach from software engineering\n(SE) executives in middle and higher-level management to improve their\ninvolvement in implementing AI ethics by making ethical requirements part of\ntheir management practices. However, research indicates that most work on\nimplementing ethical requirements in SE management primarily focuses on\ntechnical development, with scarce findings for middle and higher-level\nmanagement. We investigate this by interviewing ten Finnish SE executives in\nmiddle and higher-level management to examine how they consider and implement\nethical requirements. We use ethical requirements from the European Union (EU)\nTrustworthy Ethics guidelines for Trustworthy AI as our reference for ethical\nrequirements and an Agile portfolio management framework to analyze\nimplementation. Our findings reveal a general consideration of privacy and data\ngovernance ethical requirements as legal requirements with no other\nconsideration for ethical requirements identified. The findings also show\npracticable consideration of ethical requirements as technical robustness and\nsafety for implementation as risk requirements and societal and environmental\nwell-being for implementation as sustainability requirements. We examine a\npractical approach to implementing ethical requirements using the ethical risk\nrequirements stack employing the Agile portfolio management framework.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.13696,regular,post_llm,2023,6,"{'ai_likelihood': 3.741847144232856e-06, 'text': ""Improving City Life via Legitimate and Participatory Policy-making: A\n  Data-driven Approach in Switzerland\n\n  This paper introduces a novel data-driven approach to address challenges\nfaced by city policymakers concerning the distribution of public funds.\nProviding budgeting processes for improving quality of life based on objective\n(data-driven) evidence has been so far a missing element in policy-making. This\npaper focuses on a case study of 1,204 citizens in the city of Aarau,\nSwitzerland, and analyzes survey data containing insightful indicators that can\nimpact the legitimacy of decision-making. Our approach is twofold. On the one\nhand, we aim to optimize the legitimacy of policymakers' decisions by\nidentifying the level of investment in neighborhoods and projects that offer\nthe greatest return in legitimacy. To do so, we introduce a new\ncontext-independent legitimacy metric for policymakers. This metric allows us\nto distinguish decisive vs. indecisive collective preferences for neighborhoods\nor projects on which to invest, enabling policymakers to prioritize impactful\nbottom-up consultations and participatory initiatives (e.g., participatory\nbudgeting). The metric also allows policymakers to identify the optimal number\nof investments in various project sectors and neighborhoods (in terms of\nlegitimacy gain). On the other hand, we aim to offer guidance to policymakers\nconcerning which satisfaction and participation factors influence citizens'\nquality of life through an accurate classification model and an evaluation of\nrelocations. By doing so, policymakers may be able to further refine their\nstrategy, making targeted investments with significant benefits to citizens'\nquality of life. These findings are expected to provide transformative insights\nfor practicing direct democracy in Switzerland and a blueprint for\npolicy-making to adopt worldwide.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.04461,review,post_llm,2023,6,"{'ai_likelihood': 1.0397699144151476e-05, 'text': ""An Interpretive Framework for Narrower Immunity Under Section 230 of the\n  Communications Decency Act\n\n  Almost all courts to interpret Section 230 of the Communications Decency Act\nhave construed its ambiguously worded immunity provision broadly, shielding\nInternet intermediaries from tort liability so long as they are not the literal\nauthors of offensive content. Although this broad interpretation effects the\nbasic goals of the statute, it ignores several serious textual difficulties and\nmistakenly extends protection too far by immunizing even direct participants in\ntortuous conduct.\n  This analysis, which examines the text and history of Section 230 in light of\ntwo strains of pre-Internet vicarious liability defamation doctrine, concludes\nthat the immunity provision of Section 230, though broad, was not intended to\nabrogate entirely traditional common law notions of vicarious liability. Some\nbases of vicarious liability remain, and their continuing validity both\nexplains the textual puzzles courts have faced in applying Section 230 and\nundergirds the push by a small minority of courts to narrow the section's\nimmunity provision.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05598,regular,post_llm,2023,6,"{'ai_likelihood': 1.8543667263454863e-06, 'text': 'Enclosed Loops: How open source communities become datasets\n\n  Centralization in code hosting and package management in the 2010s created\nfundamental shifts in the social arrangements of open source ecosystems. In a\nregime of centralized open source, platform effects can both empower and\ndetract from communities depending on underlying technical implementations and\ngovernance mechanisms. In this paper we examine Dependabot, Crater and Copilot\nas three nascent tools whose existence is predicated on centralized software at\nscale. Open source ecosystems are maintained by positive feedback loops between\ncommunity members and their outputs. This mechanism is guided by community\nstandards that foreground notions of accountability and transparency. On one\nhand, software at scale supports positive feedback loops of exchange among\necosystem stakeholders: community members (developers), users, and projects. On\nthe other, software at scale becomes a commodity to be leveraged and\nexpropriated. We perform a comparative analysis of attributes across the three\ntools and evaluate their goals, values, and norms. We investigate these\nfeedback loops and their sociotechnical effects on open source communities. We\ndemonstrate how the values embedded in each case study may diverge from the\nfoundational ethos of open communities as they are motivated by, and respond to\nthe platform effects, corporate capture, and centralization of open source\ninfrastructure. Our analysis finds that these tools embed values that are\nreflective of different modes of development - some are transparent and\naccountable, and others are not. In doing so, certain tools may have feedback\nmechanisms that extend communities. Others threaten and damage communities\nability to reproduce themselves.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.06573,regular,post_llm,2023,6,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'Ghosting the Machine: Judicial Resistance to a Recidivism Risk\n  Assessment Instrument\n\n  Recidivism risk assessment instruments are presented as an \'evidence-based\'\nstrategy for criminal justice reform - a way of increasing consistency in\nsentencing, replacing cash bail, and reducing mass incarceration. In practice,\nhowever, AI-centric reforms can simply add another layer to the sluggish,\nlabyrinthine machinery of bureaucratic systems and are met with internal\nresistance. Through a community-informed interview-based study of 23 criminal\njudges and other criminal legal bureaucrats in Pennsylvania, I find that judges\noverwhelmingly ignore a recently-implemented sentence risk assessment\ninstrument, which they disparage as ""useless,"" ""worthless,"" ""boring,"" ""a waste\nof time,"" ""a non-thing,"" and simply ""not helpful."" I argue that this algorithm\naversion cannot be accounted for by individuals\' distrust of the tools or\nautomation anxieties, per the explanations given by existing scholarship.\nRather, the instrument\'s non-use is the result of an interplay between three\norganizational factors: county-level norms about pre-sentence investigation\nreports; alterations made to the instrument by the Pennsylvania Sentencing\nCommission in response to years of public and internal resistance; and problems\nwith how information is disseminated to judges. These findings shed new light\non the important role of organizational influences on professional resistance\nto algorithms, which helps explain why algorithm-centric reforms can fail to\nhave their desired effect. This study also contributes to an\nempirically-informed argument against the use of risk assessment instruments:\nthey are resource-intensive and have not demonstrated positive on-the-ground\nimpacts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.10039,review,post_llm,2023,6,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Improved Web Accessibility Evaluation of Open Learning Contents for\n  Individuals with Learning Disabilities\n\n  Web content should be accessible to normal and disabled communities on\nelectronic devices. The Web Accessibility Initiative (WAI) has created standard\nguidelines called Web Content Accessibility Guidelines (WCAG). Mobile Web Best\nPractice (MWBP) is also proposed by WAI for accessibility of websites on\ndesktop computers and mobile devices like smartphones, tablets, iPads, iPhones,\nand iPods. Educational Resources that provide free licensed learning content\nare used to test the WCAG. The disabled community also has equal rights to gain\naccess to these learning materials through electronic devices. The main purpose\nof this research is to evaluate these selected open educational learning\nmaterials for individuals with only learning disabilities. This research\nprovides several recommendations to improve the accessibility level of the\nLearning Management Systems. Future research includes developing a more\naccessible learning management system with minimized or no accessibility\nerrors. Disability includes physical impairments, mental disorders, lack of\ncognition, learning and emotional disability. Some individuals have multiple\ndisorders. Learning disabilities are one of them. People have difficulty\nlearning because of an unknown factor or low intelligence quotient (IQ).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05372,review,post_llm,2023,6,"{'ai_likelihood': 0.97802734375, 'text': 'Towards FATE in AI for Social Media and Healthcare: A Systematic Review\n\n  As artificial intelligence (AI) systems become more prevalent, ensuring\nfairness in their design becomes increasingly important. This survey focuses on\nthe subdomains of social media and healthcare, examining the concepts of\nfairness, accountability, transparency, and ethics (FATE) within the context of\nAI. We explore existing research on FATE in AI, highlighting the benefits and\nlimitations of current solutions, and provide future research directions. We\nfound that statistical and intersectional fairness can support fairness in\nhealthcare on social media platforms, and transparency in AI is essential for\naccountability. While solutions like simulation, data analytics, and automated\nsystems are widely used, their effectiveness can vary, and keeping up-to-date\nwith the latest research is crucial.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.70068359375, 'GPT4': 0.00836944580078125, 'CLAUDE': 0.00101470947265625, 'GOOGLE': 0.271728515625, 'OPENAI_O_SERIES': 0.0002884864807128906, 'DEEPSEEK': 3.528594970703125e-05, 'GROK': 4.088878631591797e-05, 'NOVA': 3.159046173095703e-05, 'OTHER': 0.016021728515625, 'HUMAN': 0.0020294189453125}}"
2306.01365,regular,post_llm,2023,6,"{'ai_likelihood': 4.900826348198785e-06, 'text': 'Generation of Probabilistic Synthetic Data for Serious Games: A Case\n  Study on Cyberbullying\n\n  Synthetic data generation has been a growing area of research in recent\nyears. However, its potential applications in serious games have not been\nthoroughly explored. Advances in this field could anticipate data modelling and\nanalysis, as well as speed up the development process. To try to fill this gap\nin the literature, we propose a simulator architecture for generating\nprobabilistic synthetic data for serious games based on interactive narratives.\nThis architecture is designed to be generic and modular so that it can be used\nby other researchers on similar problems. To simulate the interaction of\nsynthetic players with questions, we use a cognitive testing model based on the\nItem Response Theory framework. We also show how probabilistic graphical models\n(in particular Bayesian networks) can be used to introduce expert knowledge and\nexternal data into the simulation. Finally, we apply the proposed architecture\nand methods in a use case of a serious game focused on cyberbullying. We\nperform Bayesian inference experiments using a hierarchical model to\ndemonstrate the identifiability and robustness of the generated data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.08959,review,post_llm,2023,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Statutory Professions in AI governance and their consequences for\n  explainable AI\n\n  Intentional and accidental harms arising from the use of AI have impacted the\nhealth, safety and rights of individuals. While regulatory frameworks are being\ndeveloped, there remains a lack of consensus on methods necessary to deliver\nsafe AI. The potential for explainable AI (XAI) to contribute to the\neffectiveness of the regulation of AI is being increasingly examined.\nRegulation must include methods to ensure compliance on an ongoing basis,\nthough there is an absence of practical proposals on how to achieve this. For\nXAI to be successfully incorporated into a regulatory system, the individuals\nwho are engaged in interpreting/explaining the model to stakeholders should be\nsufficiently qualified for the role. Statutory professionals are prevalent in\ndomains in which harm can be done to the health, safety and rights of\nindividuals. The most obvious examples are doctors, engineers and lawyers.\nThose professionals are required to exercise skill and judgement and to defend\ntheir decision making process in the event of harm occurring. We propose that a\nstatutory profession framework be introduced as a necessary part of the AI\nregulatory framework for compliance and monitoring purposes. We will refer to\nthis new statutory professional as an AI Architect (AIA). This AIA would be\nresponsible to ensure the risk of harm is minimised and accountable in the\nevent that harms occur. The AIA would also be relied on to provide appropriate\ninterpretations/explanations of XAI models to stakeholders. Further, in order\nto satisfy themselves that the models have been developed in a satisfactory\nmanner, the AIA would require models to have appropriate transparency.\nTherefore it is likely that the introduction of an AIA system would lead to an\nincrease in the use of XAI to enable AIA to discharge their professional\nobligations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.10037,review,post_llm,2023,6,"{'ai_likelihood': 3.493494457668728e-05, 'text': 'Legal and ethical considerations regarding the use of ChatGPT in\n  education\n\n  Artificial intelligence has evolved enormously over the last two decades,\nbecoming mainstream in different scientific domains including education, where\nso far, it is mainly utilized to enhance administrative and intelligent\ntutoring systems services and academic support. ChatGPT, an artificial\nintelligence-based chatbot, developed by OpenAI and released in November 2022,\nhas rapidly gained attention from the entire international community for its\nimpressive performance in generating comprehensive, systematic, and informative\nhuman-like responses to user input through natural language processing.\nInevitably, it has also rapidly posed several challenges, opportunities, and\npotential issues and concerns raised regarding its use across various\nscientific disciplines. This paper aims to discuss the legal and ethical\nimplications arising from this new technology, identify potential use cases,\nand enrich our understanding of Generative AI, such as ChatGPT, and its\ncapabilities in education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.08418,regular,post_llm,2023,6,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Welcome to the Dark Side: Analyzing the Revenue Flows of Fraud in the Online Ad Ecosystem\n\nThe online advertising market has recently reached the 500 billion dollar mark. To accommodate the need to match a user with the highest bidder at a fraction of a second, it has moved towards a complex, automated and often opaque model that involves numerous agents and intermediaries. Stimulated by the lack of transparency, but also the enormous potential profits, bad actors have found ways to circumvent restrictions, and generate substantial revenue that can support websites with objectionable or even illegal content.\n  In this work, we evaluate transparency Web standards and show how shady actors take advantage of gaps to absorb ad revenues while putting the brand safety of advertisers in danger. We collect and study a large corpus of thousands of websites and show how ad transparency standards can be abused by bad actors to obscure ad revenue flows. We show how identifier pooling can redirect ad revenues from reputable domains to notorious domains serving objectionable content, and that the phenomenon is underestimated by previous studies by a factor of 15. Finally, we publish a Web monitoring service that enhances the transparency of supply chains and business relationships between publishers and ad networks.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.09145,review,post_llm,2023,6,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'Artificial intelligence adoption in the physical sciences, natural\n  sciences, life sciences, social sciences and the arts and humanities: A\n  bibliometric analysis of research publications from 1960-2021\n\n  Analysing historical patterns of artificial intelligence (AI) adoption can\ninform decisions about AI capability uplift, but research to date has provided\na limited view of AI adoption across various fields of research. In this study\nwe examine worldwide adoption of AI technology within 333 fields of research\nduring 1960-2021. We do this by using bibliometric analysis with 137 million\npeer-reviewed publications captured in The Lens database. We define AI using a\nlist of 214 phrases developed by expert working groups at the Organisation for\nEconomic Cooperation and Development (OECD). We found that 3.1 million of the\n137 million peer-reviewed research publications during the entire period were\nAI-related, with a surge in AI adoption across practically all research fields\n(physical science, natural science, life science, social science and the arts\nand humanities) in recent years. The diffusion of AI beyond computer science\nwas early, rapid and widespread. In 1960 14% of 333 research fields were\nrelated to AI (many in computer science), but this increased to cover over half\nof all research fields by 1972, over 80% by 1986 and over 98% in current times.\nWe note AI has experienced boom-bust cycles historically: the AI ""springs"" and\n""winters"". We conclude that the context of the current surge appears different,\nand that interdisciplinary AI application is likely to be sustained.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.14857,regular,post_llm,2023,6,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Metapopulation Graph Neural Networks: Deep Metapopulation Epidemic\n  Modeling with Human Mobility\n\n  Epidemic prediction is a fundamental task for epidemic control and\nprevention. Many mechanistic models and deep learning models are built for this\ntask. However, most mechanistic models have difficulty estimating the\ntime/region-varying epidemiological parameters, while most deep learning models\nlack the guidance of epidemiological domain knowledge and interpretability of\nprediction results. In this study, we propose a novel hybrid model called\nMepoGNN for multi-step multi-region epidemic forecasting by incorporating Graph\nNeural Networks (GNNs) and graph learning mechanisms into Metapopulation SIR\nmodel. Our model can not only predict the number of confirmed cases but also\nexplicitly learn the epidemiological parameters and the underlying epidemic\npropagation graph from heterogeneous data in an end-to-end manner. The\nmulti-source epidemic-related data and mobility data of Japan are collected and\nprocessed to form the dataset for experiments. The experimental results\ndemonstrate our model outperforms the existing mechanistic models and deep\nlearning models by a large margin. Furthermore, the analysis on the learned\nparameters illustrate the high reliability and interpretability of our model\nand helps better understanding of epidemic spread. In addition, a mobility\ngeneration method is presented to address the issue of unavailable mobility\ndata, and the experimental results demonstrate effectiveness of the generated\nmobility data as an input to our model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.17682,review,post_llm,2023,6,"{'ai_likelihood': 2.2517310248480903e-06, 'text': 'ADS Standardization Landscape: Making Sense of its Status and of the\n  Associated Research Questions\n\n  Automated Driving Systems (ADS) hold great potential to increase safety,\nmobility, and equity. However, without public acceptance, none of these\npromises can be fulfilled. To engender public trust, many entities in the ADS\ncommunity participate in standards development organizations (SDOs) with the\ngoal of enhancing safety for the entire industry through a collaborative\napproach. The breadth and depth of the ADS safety standardization landscape is\nvast and constantly changing, as often is the case for novel technologies in\nrapid evolution. The pace of development of the ADS industry makes it hard for\nthe public and interested parties to keep track of ongoing SDO efforts,\nincluding the topics touched by each standard and the committees addressing\neach topic, as well as make sense of the wealth of documentation produced.\nTherefore, the authors present here a simplified framework for abstracting and\norganizing the current landscape of ADS safety standards into high-level, long\nterm themes. This framework is then utilized to develop and organize associated\nresearch questions that have not yet reached widely adopted industry positions,\nalong with identifying potential gaps where further research and\nstandardization is needed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.02327,regular,post_llm,2023,6,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'Agency and legibility for artists through Experiential AI\n\n  Experiential AI is an emerging research field that addresses the challenge of\nmaking AI tangible and explicit, both to fuel cultural experiences for\naudiences, and to make AI systems more accessible to human understanding. The\ncentral theme is how artists, scientists and other interdisciplinary actors can\ncome together to understand and communicate the functionality of AI, ML and\nintelligent robots, their limitations, and consequences, through informative\nand compelling experiences. It provides an approach and methodology for the\narts and tangible experiences to mediate between impenetrable computer code and\nhuman understanding, making not just AI systems but also their values and\nimplications more transparent, and therefore accountable. In this paper, we\nreport on an empirical case study of an experiential AI system designed for\ncreative data exploration of a user-defined dimension, to enable creators to\ngain more creative control over the AI process. We discuss how experiential AI\ncan increase legibility and agency for artists, and how the arts can provide\ncreative strategies and methods which can add to the toolbox for human-centred\nXAI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.07527,regular,post_llm,2023,6,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Discrimination through Image Selection by Job Advertisers on Facebook\n\n  Targeted advertising platforms are widely used by job advertisers to reach\npotential employees; thus issues of discrimination due to targeting that have\nsurfaced have received widespread attention. Advertisers could misuse targeting\ntools to exclude people based on gender, race, location and other protected\nattributes from seeing their job ads. In response to legal actions, Facebook\ndisabled the ability for explicit targeting based on many attributes for some\nad categories, including employment. Although this is a step in the right\ndirection, prior work has shown that discrimination can take place not just due\nto the explicit targeting tools of the platforms, but also due to the impact of\nthe biased ad delivery algorithm. Thus, one must look at the potential for\ndiscrimination more broadly, and not merely through the lens of the explicit\ntargeting tools.\n  In this work, we propose and investigate the prevalence of a new means for\ndiscrimination in job advertising, that combines both targeting and delivery --\nthrough the disproportionate representation or exclusion of people of certain\ndemographics in job ad images. We use the Facebook Ad Library to demonstrate\nthe prevalence of this practice through: (1) evidence of advertisers running\nmany campaigns using ad images of people of only one perceived gender, (2)\nsystematic analysis for gender representation in all current ad campaigns for\ntruck drivers and nurses, (3) longitudinal analysis of ad campaign image use by\ngender and race for select advertisers. After establishing that the\ndiscrimination resulting from a selective choice of people in job ad images,\ncombined with algorithmic amplification of skews by the ad delivery algorithm,\nis of immediate concern, we discuss approaches and challenges for addressing\nit.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.00635,review,post_llm,2023,6,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Experiential AI: A transdisciplinary framework for legibility and agency\n  in AI\n\n  Experiential AI is presented as a research agenda in which scientists and\nartists come together to investigate the entanglements between humans and\nmachines, and an approach to human-machine learning and development where\nknowledge is created through the transformation of experience. The paper\ndiscusses advances and limitations in the field of explainable AI; the\ncontribution the arts can offer to address those limitations; and methods to\nbring creative practice together with emerging technology to create rich\nexperiences that shed light on novel socio-technical systems, changing the way\nthat publics, scientists and practitioners think about AI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.00292,review,post_llm,2023,6,"{'ai_likelihood': 0.00017404556274414062, 'text': 'Sustainable AI Regulation\n\n  Current proposals for AI regulation, in the EU and beyond, aim to spur AI\nthat is trustworthy (e.g., AI Act) and accountable (e.g., AI Liability) What is\nmissing, however, is a robust regulatory discourse and roadmap to make AI, and\ntechnology more broadly, environmentally sustainable. This paper aims to take\nfirst steps to fill this gap. The ICT sector contributes up to 3.9 percent of\nglobal greenhouse gas (GHG) emissions-more than global air travel at 2.5\npercent. The carbon footprint and water consumption of AI, especially\nlarge-scale generative models like GPT-4, raise significant sustainability\nconcerns. The paper is the first to assess how current and proposed technology\nregulations, including EU environmental law, the General Data Protection\nRegulation (GDPR), and the AI Act, could be adjusted to better account for\nenvironmental sustainability. The GDPR, for instance, could be interpreted to\nlimit certain individual rights like the right to erasure if these rights\nsignificantly conflict with broader sustainability goals. In a second step, the\npaper suggests a multi-faceted approach to achieve sustainable AI regulation.\nIt advocates for transparency mechanisms, such as disclosing the GHG footprint\nof AI systems, as laid out in the proposed EU AI Act. However, sustainable AI\nregulation must go beyond mere transparency. The paper proposes a regulatory\ntoolkit comprising co-regulation, sustainability-by-design principles,\nrestrictions on training data, and consumption caps, including integration into\nthe EU Emissions Trading Scheme. Finally, the paper argues that this regulatory\ntoolkit could serve as a blueprint for regulating other high-emission\ntechnologies and infrastructures like blockchain, Metaverse applications, and\ndata centers. The framework aims to cohesively address the crucial dual\nchallenges of our era: digital transformation and climate change mitigation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.0082,regular,post_llm,2023,6,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""How are Primary School Computer Science Curricular Reforms Contributing\n  to Equity? Impact on Student Learning, Perception of the Discipline, and\n  Gender Gaps\n\n  Early exposure to Computer Science (CS) for all is critical to broaden\nparticipation and promote equity in the field. But how does introducting CS\ninto primary school curricula impact learning, perception, and gaps between\ngroups of students? We investigate a CS-curricular reform and teacher\nProfessional Development (PD) program from an equity standpoint by applying\nhierarchical regression and structural equation modelling on student learning\nand perception data from three studies with respectively 1384, 2433 & 1644\ngrade 3-6 students (ages 7-11) and their 83, 142 & 95 teachers. Regarding\nlearning, exposure to CS instruction appears to contribute to closing the\nperformance gap between low-achieving and high-achieving students, as well as\npre-existing gender gaps. Despite a lack of direct influence of what was taught\non student learning, there is no impact of teachers' demographics or motivation\non student learning, with teachers' perception of the CS-PD positively\ninfluencing learning. Regarding perception, students perceive CS and its\nteaching tools (robotics, tablets) positively, and even more so when they\nperceive a role model close to them as doing CS. Nonetheless gender differences\nexist all around with boys perceiving CS more positively than girls despite\naccess to CS education. However, access to CS-education affects boys and girls\ndifferently: larger gender gaps are closing (namely those related to robotics),\nwhile smaller gaps are increasing (namely those related to CS and tablets). To\nconclude, our findings highlight how a CS curricular reform impacts learning,\nperception, and equity and supports the importance of i) early introductions to\nCS for all, ii) preparing teachers to teach CS all the while removing the\ninfluence of teacher demographics and motivation on student outcomes, and iii)\nhaving developmentally appropriate activities that signal to all groups of\nstudents.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.04752,regular,post_llm,2023,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Data coverage, richness, and quality of OpenStreetMap for special\n  interest tags: wayside crosses -- a case study\n\n  Volunteered Geographic Information projects like OpenStreetMap which allow\naccessing and using the raw data, are a treasure trove for investigations -\ne.g. cultural topics, urban planning, or accessibility of services. Among the\nconcerns are the reliability and accurateness of the data. While it was found\nthat for mainstream topics, like roads or museums, the data completeness and\naccuracy is very high, especially in the western world, this is not clear for\nniche topics. Furthermore, many of the analyses are almost one decade old in\nwhich the OpenStreetMap-database grew to over nine billion elements.\n  Based on OpenStreetMap-data of wayside crosses and other cross-like objects\nregional cultural differences and prevalence of the types within Europe,\nGermany and Bavaria are investigated. For Bavaria, internally and by comparing\nto an official dataset and other proxies the data completeness, logical\nconsistency, positional, temporal, and thematic accuracy is assessed.\nSubsequently, the usability for the specific case and to generalize for the use\nof OpenStreetMap data for niche topics.\n  It is estimated that about one sixth to one third of the crosses located\nwithin Bavaria are recorded in the database and positional accuracy is better\nthan 50 metres in most cases. In addition, linguistic features of the\ninscriptions, the usage of building materials, dates of erection and other\ndetails deducible from the dataset are discussed. It is found that data quality\nand coverage for niche topics exceeds expectations but varies strongly by\nregion and should not be trusted without thorough dissection of the dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05884,review,post_llm,2023,6,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Social interactions mediated by the Internet and the Big-Five: a\n  cross-country analysis\n\n  This study analyzes the possible relationship between personality traits, in\nterms of Big Five (extraversion, agreeableness, responsibility, emotional\nstability and openness to experience), and social interactions mediated by\ndigital platforms in different socioeconomic and cultural contexts. We\nconsidered data from a questionnaire and the experience of using a chatbot, as\na mean of requesting and offering help, with students from 4 universities:\nUniversity of Trento (Italy), the National University of Mongolia, the School\nof Economics of London (United Kingdom) and the Universidad Cat\\'olica Nuestra\nSe\\~nora de la Asunci\\'on (Paraguay). The main findings confirm that\npersonality traits may influence social interactions and active participation\nin groups. Therefore, they should be taken into account to enrich the\nrecommendation of matching algorithms between people who ask for help and\npeople who could respond not only on the basis of their knowledge and skills.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.07443,review,post_llm,2023,6,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Accountability Infrastructure: How to implement limits on platform\n  optimization to protect population health\n\n  Attention capitalism has generated design processes and product development\ndecisions that prioritize platform growth over all other considerations. To the\nextent limits have been placed on these incentives, interventions have\nprimarily taken the form of content moderation. While moderation is important\nfor what we call ""acute harms,"" societal-scale harms -- such as negative\neffects on mental health and social trust -- require new forms of institutional\ntransparency and scientific investigation, which we group under the term\naccountability infrastructure.\n  This is not a new problem. In fact, there are many conceptual lessons and\nimplementation approaches for accountability infrastructure within the history\nof public health. After reviewing these insights, we reinterpret the societal\nharms generated by technology platforms through reference to public health. To\nthat end, we present a novel mechanism design framework and practical\nmeasurement methods for that framework. The proposed approach is iterative and\nbuilt into the product design process, and is applicable for both\ninternally-motivated (i.e. self regulation by companies) and\nexternally-motivated (i.e. government regulation) interventions for a range of\nsocietal problems, including mental health.\n  We aim to help shape a research agenda of principles for the design of\nmechanisms around problem areas on which there is broad consensus and a firm\nbase of support. We offer constructive examples and discussion of potential\nimplementation methods related to these topics, as well as several new data\nillustrations for potential effects of exposure to online content.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05122,regular,post_llm,2023,6,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Can AI Moderate Online Communities?\n\n  The task of cultivating healthy communication in online communities becomes\nincreasingly urgent, as gaming and social media experiences become\nprogressively more immersive and life-like. We approach the challenge of\nmoderating online communities by training student models using a large language\nmodel (LLM). We use zero-shot learning models to distill and expand datasets\nfollowed by a few-shot learning and a fine-tuning approach, leveraging\nopen-access generative pre-trained transformer models (GPT) from OpenAI. Our\npreliminary findings suggest, that when properly trained, LLMs can excel in\nidentifying actor intentions, moderating toxic comments, and rewarding positive\ncontributions. The student models perform above-expectation in non-contextual\nassignments such as identifying classically toxic behavior and perform\nsufficiently on contextual assignments such as identifying positive\ncontributions to online discourse. Further, using open-access models like\nOpenAI's GPT we experience a step-change in the development process for what\nhas historically been a complex modeling task. We contribute to the information\nsystem (IS) discourse with a rapid development framework on the application of\ngenerative AI in content online moderation and management of culture in\ndecentralized, pseudonymous communities by providing a sample model suite of\nindustrial-ready generative AI models based on open-access LLMs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.07223,regular,post_llm,2023,6,"{'ai_likelihood': 0.00021444426642523872, 'text': 'Wise in Vaccine Allocation\n\n  The paper uses machine learning and mathematical modeling to predict future\nvaccine distribution and solve the problem of allocating vaccines to different\ntypes of hospitals. They collected data and analyzed it, finding factors such\nas nearby residents, transportation, and medical personnel that impact\ndistribution. They used the results to create a model and allocate vaccines to\ncentral and community hospitals and health centers in Hangzhou Gongshu District\nand Harbin Daoli District based on the model. They provide an explanation for\nthe vaccine distribution based on their model and conclusions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.09759,regular,post_llm,2023,6,"{'ai_likelihood': 1.1987156338161893e-05, 'text': ""A research infrastructure for generating and sharing diversity-aware\n  data\n\n  The intensive flow of personal data associated with the trend of\ncomputerizing aspects of people's diversity in their daily lives is associated\nwith issues concerning not only people protection and their trust in new\ntechnologies, but also bias in the analysis of data and problems in their\nmanagement and reuse. Faced with a complex problem, the strategies adopted,\nincluding technologies and services, often focus on individual aspects, which\nare difficult to integrate into a broader framework, which can be of effective\nsupport for researchers and developers. Therefore, we argue for the development\nof an end-to-end research infrastructure (RI) that enables trustworthy\ndiversity-aware data within a citizen science community.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.01917,review,post_llm,2023,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Building a Credible Case for Safety: Waymo's Approach for the\n  Determination of Absence of Unreasonable Risk\n\n  This paper presents an overview of Waymo's approach to building a reliable\ncase for safety - a novel and thorough blueprint for use by any company\nbuilding fully autonomous driving systems. A safety case for fully autonomous\noperations is a formal way to explain how a company determines that an AV\nsystem is safe enough to be deployed on public roads without a human driver,\nand it includes evidence to support that determination. It involves an\nexplanation of the system, the methodologies used to develop it, the metrics\nused to validate it and the actual results of validation tests. Yet, in order\nto develop a worthwhile safety case, it is first important to understand what\nmakes one credible and well crafted, and align on evaluation criteria. This\npaper helps enabling such alignment by providing foundational thinking into not\nonly how a system is determined to be ready for deployment but also into\njustifying that the set of acceptance criteria employed in such determination\nis sufficient and that their evaluation (and associated methods) is credible.\nThe publication is structured around three complementary perspectives on safety\nthat build upon content published by Waymo since 2020: a layered approach to\nsafety; a dynamic approach to safety; and a credible approach to safety. The\nproposed approach is methodology-agnostic, so that anyone in the space could\nemploy portions or all of it.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05669,review,post_llm,2023,6,"{'ai_likelihood': 3.4769376118977866e-06, 'text': ""More than programming? The impact of AI on work and skills\n\n  This chapter explores the ways in which organisational readiness and\nscientific advances in Artificial Intelligence have been affecting the demand\nfor skills and their training in Australia and other nations leading in the\npromotion, use or development of AI. The consensus appears that having adequate\nnumbers of qualified data scientists and machine learning experts is critical\nfor meeting the challenges ahead. The chapter asks what this may mean for\nAustralia's education and training system, what needs to be taught and learned,\nand whether technical skills are all that matter.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.10338,regular,post_llm,2023,6,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Trauma lurking in the shadows: A Reddit case study of mental health\n  issues in online posts about Childhood Sexual Abuse\n\n  Childhood Sexual Abuse (CSA) is a menace to society and has long-lasting\neffects on the mental health of the survivors. From time to time CSA survivors\nare haunted by various mental health issues in their lifetime. Proper care and\nattention towards CSA survivors facing mental health issues can drastically\nimprove the mental health conditions of CSA survivors. Previous works\nleveraging online social media (OSM) data for understanding mental health\nissues haven't focused on mental health issues in individuals with CSA\nbackground. Our work fills this gap by studying Reddit posts related to CSA to\nunderstand their mental health issues. Mental health issues such as depression,\nanxiety, and Post-Traumatic Stress Disorder (PTSD) are most commonly observed\nin posts with CSA background. Observable differences exist between posts\nrelated to mental health issues with and without CSA background. Keeping this\ndifference in mind, for identifying mental health issues in posts with CSA\nexposure we develop a two-stage framework. The first stage involves classifying\nposts with and without CSA background and the second stage involves recognizing\nmental health issues in posts that are classified as belonging to CSA\nbackground. The top model in the first stage is able to achieve accuracy and\nf1-score (macro) of 96.26% and 96.24%. and in the second stage, the top model\nreports hamming score of 67.09%. Content Warning: Reader discretion is\nrecommended as our study tackles topics such as child sexual abuse,\nmolestation, etc.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.13952,review,post_llm,2023,6,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Artificial intelligence and biological misuse: Differentiating risks of\n  language models and biological design tools\n\n  As advancements in artificial intelligence (AI) propel progress in the life\nsciences, they may also enable the weaponisation and misuse of biological\nagents. This article differentiates two classes of AI tools that could pose\nsuch biosecurity risks: large language models (LLMs) and biological design\ntools (BDTs). LLMs, such as GPT-4 and its successors, might provide dual-use\ninformation and thus remove some barriers encountered by historical biological\nweapons efforts. As LLMs are turned into multi-modal lab assistants and\nautonomous science tools, this will increase their ability to support\nnon-experts in performing laboratory work. Thus, LLMs may in particular lower\nbarriers to biological misuse. In contrast, BDTs will expand the capabilities\nof sophisticated actors. Concretely, BDTs may enable the creation of pandemic\npathogens substantially worse than anything seen to date and could enable forms\nof more predictable and targeted biological weapons. In combination, the\nconvergence of LLMs and BDTs could raise the ceiling of harm from biological\nagents and could make them broadly accessible. A range of interventions would\nhelp to manage risks. Independent pre-release evaluations could help understand\nthe capabilities of models and the effectiveness of safeguards. Options for\ndifferentiated access to such tools should be carefully weighed with the\nbenefits of openly releasing systems. Lastly, essential for mitigating risks\nwill be universal and enhanced screening of gene synthesis products.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.13667,regular,post_llm,2023,6,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Ghost Booking as a New Philanthropy Channel: A Case Study on\n  Ukraine-Russia Conflict\n\n  The term ghost booking has recently emerged as a new way to conduct\nhumanitarian acts during the conflict between Russia and Ukraine in 2022. The\nphenomenon describes the events where netizens donate to Ukrainian citizens\nthrough no-show bookings on the Airbnb platform. Impressively, the social\nfundraising act that used to be organized on donation-based crowdfunding\nplatforms is shifted into a sharing economy platform market and thus gained\nmore visibility. Although the donation purpose is clear, the motivation of\ndonors in selecting a property to book remains concealed. Thus, our study aims\nto explore peer-to-peer donation behavior on a platform that was originally\nintended for economic exchanges, and further identifies which platform\nattributes effectively drive donation behaviors. We collect over 200K guest\nreviews from 16K Airbnb property listings in Ukraine by employing two\ncollection methods (screen scraping and HTML parsing). Then, we distinguish\nghost bookings among guest reviews. Our analysis uncovers the relationship\nbetween ghost booking behavior and the platform attributes, and pinpoints\nseveral attributes that influence ghost booking. Our findings highlight that\ndonors incline to credible properties explicitly featured with humanitarian\nneeds, i.e., the hosts in penury.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.01479,review,post_llm,2023,6,"{'ai_likelihood': 0.022057427300347224, 'text': ""Reconciling Governmental Use of Online Targeting With Democracy\n\n  The societal and epistemological implications of online targeted advertising\nhave been scrutinized by AI ethicists, legal scholars, and policymakers alike.\nHowever, the government's use of online targeting and its consequential\nsocio-political ramifications remain under-explored from a critical\nsocio-technical standpoint. This paper investigates the socio-political\nimplications of governmental online targeting, using a case study of the UK\ngovernment's application of such techniques for public policy objectives. We\nargue that this practice undermines democratic ideals, as it engenders three\nprimary concerns -- Transparency, Privacy, and Equality -- that clash with\nfundamental democratic doctrines and values. To address these concerns, the\npaper introduces a preliminary blueprint for an AI governance framework that\nharmonizes governmental use of online targeting with certain democratic\nprinciples. Furthermore, we advocate for the creation of an independent,\nnon-governmental regulatory body responsible for overseeing the process and\nmonitoring the government's use of online targeting, a critical measure for\npreserving democratic values.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.13141,regular,post_llm,2023,6,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""On Hate Scaling Laws For Data-Swamps\n\n  `Scale the model, scale the data, scale the GPU-farms' is the reigning\nsentiment in the world of generative AI today. While model scaling has been\nextensively studied, data scaling and its downstream impacts remain under\nexplored. This is especially of critical importance in the context of\nvisio-linguistic datasets whose main source is the World Wide Web, condensed\nand packaged as the CommonCrawl dump. This large scale data-dump, which is\nknown to have numerous drawbacks, is repeatedly mined and serves as the\ndata-motherlode for large generative models. In this paper, we: 1) investigate\nthe effect of scaling datasets on hateful content through a comparative audit\nof the LAION-400M and LAION-2B-en, containing 400 million and 2 billion samples\nrespectively, and 2) evaluate the downstream impact of scale on\nvisio-linguistic models trained on these dataset variants by measuring racial\nbias of the models trained on them using the Chicago Face Dataset (CFD) as a\nprobe. Our results show that 1) the presence of hateful content in datasets,\nwhen measured with a Hate Content Rate (HCR) metric on the inferences of the\nPysentimiento hate-detection Natural Language Processing (NLP) model, increased\nby nearly $12\\%$ and 2) societal biases and negative stereotypes were also\nexacerbated with scale on the models we evaluated. As scale increased, the\ntendency of the model to associate images of human faces with the `human being'\nclass over 7 other offensive classes reduced by half. Furthermore, for the\nBlack female category, the tendency of the model to associate their faces with\nthe `criminal' class doubled, while quintupling for Black male faces. We\npresent a qualitative and historical analysis of the model audit results,\nreflect on our findings and its implications for dataset curation practice, and\nclose with a summary of our findings and potential future work to be done in\nthis area.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.17298,regular,post_llm,2023,6,"{'ai_likelihood': 2.7815500895182294e-06, 'text': ""Tube2Vec: Social and Semantic Embeddings of YouTube Channels\n\n  Research using YouTube data often explores social and semantic dimensions of\nchannels and videos. Typically, analyses rely on laborious manual annotation of\ncontent and content creators, often found by low-recall methods such as keyword\nsearch. Here, we explore an alternative approach, using latent representations\n(embeddings) obtained via machine learning. Using a large dataset of YouTube\nlinks shared on Reddit; we create embeddings that capture social sharing\nbehavior, video metadata (title, description, etc.), and YouTube's video\nrecommendations. We evaluate these embeddings using crowdsourcing and existing\ndatasets, finding that recommendation embeddings excel at capturing both social\nand semantic dimensions, although social-sharing embeddings better correlate\nwith existing partisan scores. We share embeddings capturing the social and\nsemantic dimensions of 44,000 YouTube channels for the benefit of future\nresearch on YouTube: https://github.com/epfl-dlab/youtube-embeddings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.16267,regular,post_llm,2023,6,"{'ai_likelihood': 1.1920928955078125e-06, 'text': ""Automated Questions About Learners' Own Code Help to Detect Fragile\n  Knowledge\n\n  Students are able to produce correctly functioning program code even though\nthey have a fragile understanding of how it actually works. Questions derived\nautomatically from individual exercise submissions (QLC) can probe if and how\nwell the students understand the structure and logic of the code they just\ncreated. Prior research studied this approach in the context of the first\nprogramming course. We replicate the study on a follow-up programming course\nfor engineering students which contains a recap of general concepts in CS1. The\ntask was the classic rainfall problem which was solved by 90% of the students.\nThe QLCs generated from each passing submission were kept intentionally simple,\nyet 27% of the students failed in at least one of them. Students who struggled\nwith questions about their own program logic had a lower median for overall\ncourse points than students who answered correctly.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.13685,regular,post_llm,2023,6,"{'ai_likelihood': 0.005048116048177084, 'text': ""A Game-Based Learning Application to Help Learners to Practice\n  Mathematical Patterns and Structures\n\n  Purpose - The purpose of this study is to develop a game-based mobile\napplication to help learners practice mathematical patterns and structures.\n  Method - The study followed a mixed-method research design and prototyping\nmethodology to guide the study in developing the mobile application. An\ninstrument based on the Octalysis framework was developed as an evaluation tool\nfor the study.\n  Results - The study developed a mobile application based on the Octalysis\nframework. The application has fully achieved all its intended features based\non the rating provided by the students and IT experts.\n  Conclusion - The study successfully developed a mobile learning application\nfor mathematical patterns and structures. By incorporating GBL principles and\nthe Octalysis framework, the app achieved its intended features and received\npositive evaluations from students and IT experts. This highlights the\npotential of the app in promoting mathematical learning.\n  Recommendations - This study recommends that the application be further\nenhanced to include other topics. Incorporating other game-based principles and\napproaches like timed questions and the difficulty level is also worth\npursuing. Actual testing for end-users is also needed to verify the\napplication's effectiveness.\n  Practical Implications - Successful development of a game-based mobile app\nfor practicing mathematical patterns and structures can transform education\ntechnology by engaging learners and enhancing their experience. This study\nprovides valuable insights for future researchers developing similar\napplications, highlighting the potential to revolutionize traditional\napproaches and create an interactive learning environment for improving\nmathematical abilities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.08171,review,post_llm,2023,6,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""The aesthetics of cyber security: How do users perceive them?\n\n  While specific aesthetic philosophies may differ across cultures, all human\nsocieties have used aesthetics to support communication and learning. Within\nthe fields of usability and usable security, aesthetics have been deployed for\nsuch diverse purposes as enhancing students' e-learning experiences and\noptimising user interface design. In this paper, we seek to understand how\nindividual users perceive the visual assets that accompany cyber security\ninformation, and how these visual assets and user perceptions underwrite a\ndistinct \\emph{cyber security aesthetic}. We ask, (1) What constitutes cyber\nsecurity aesthetics, from the perspective of an individual user? and (2) How\nmight these aesthetics affect users' perceived self-efficacy as they informally\nlearn cyber security precepts? To begin answering these questions, we compile\nan image-set from cyber security web articles and analyse the distinct visual\nproperties and sentiments of these images.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.09753,regular,post_llm,2023,6,"{'ai_likelihood': 1.6854868994818793e-05, 'text': 'A context model for collecting diversity-aware data\n\n  Diversity-aware data are essential for a robust modeling of human behavior in\ncontext. In addition, being the human behavior of interest for numerous\napplications, data must also be reusable across domain, to ensure diversity of\ninterpretations. Current data collection techniques allow only a partial\nrepresentation of the diversity of people and often generate data that is\ndifficult to reuse. To fill this gap, we propose a data collection methodology,\nwithin a hybrid machine-artificial intelligence approach, and its related\ndataset, based on a comprehensive ontological notion of context which enables\ndata reusability. The dataset has a sample of 158 participants and is collected\nvia the iLog smartphone application. It contains more than 170 GB of subjective\nand objective data, which comes from 27 smartphone sensors that are associated\nwith 168,095 self-reported annotations on the participants context. The dataset\nis highly reusable, as demonstrated by its diverse applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.066,review,post_llm,2023,6,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Enabling Spatial Digital Twins: Technologies, Challenges, and Future\n  Research Directions\n\n  A Digital Twin (DT) is a virtual replica of a physical object or system,\ncreated to monitor, analyze, and optimize its behavior and characteristics. A\nSpatial Digital Twin (SDT) is a specific type of digital twin that emphasizes\nthe geospatial aspects of the physical entity, incorporating precise location\nand dimensional attributes for a comprehensive understanding within its spatial\nenvironment. The current body of research on SDTs primarily concentrates on\nanalyzing their potential impact and opportunities within various application\ndomains. As building an SDT is a complex process and requires a variety of\nspatial computing technologies, it is not straightforward for practitioners and\nresearchers of this multi-disciplinary domain to grasp the underlying details\nof enabling technologies of the SDT. In this paper, we are the first to\nsystematically analyze different spatial technologies relevant to building an\nSDT in layered approach (starting from data acquisition to visualization). More\nspecifically, we present the key components of SDTs into four layers of\ntechnologies: (i) data acquisition; (ii) spatial database management \\& big\ndata analytics systems; (iii) GIS middleware software, maps \\& APIs; and (iv)\nkey functional components such as visualizing, querying, mining, simulation and\nprediction. Moreover, we discuss how modern technologies such as AI/ML,\nblockchains, and cloud computing can be effectively utilized in enabling and\nenhancing SDTs. Finally, we identify a number of research challenges and\nopportunities in SDTs. This work serves as an important resource for SDT\nresearchers and practitioners as it explicitly distinguishes SDTs from\ntraditional DTs, identifies unique applications, outlines the essential\ntechnological components of SDTs, and presents a vision for their future\ndevelopment along with the challenges that lie ahead.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.10971,regular,post_llm,2023,6,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Quantitative dynamics of design thinking and creativity perspectives in\n  company context\n\n  This study is intended to provide in-depth insights into how design thinking\nand creativity issues are understood and possibly evolve in the course of\ndesign discussions in a company context. For that purpose, we use the seminar\ntranscripts of the Design Thinking Research Symposium 12 (DTRS12) dataset\n""Tech-centred Design Thinking: Perspectives from a Rising Asia,"" which are\nprimarily concerned with how Korean companies implement design thinking and\nwhat role designers currently play. We employed a novel method of information\nprocessing based on constructed dynamic semantic networks to investigate the\nseminar discussions according to company representatives and company size. We\ncompared the quantitative dynamics in two seminars: the first involved\nmanagerial representatives of four companies, and the second involved\nspecialized designers and management of a design center of single company. On\nthe basis of dynamic semantic networks, we quantified the changes in four\nsemantic measures -- abstraction, polysemy, information content, and pairwise\nword similarity -- in chronologically reconstructed individual design-thinking\nprocesses. Statistical analyses show that design thinking in the seminar with\nfour companies, exhibits significant differences in the dynamics of\nabstraction, polysemy, and information content, compared to the seminar with\nthe design center of single company. Both the decrease in polysemy and\nabstraction and the increase in information content in the individual\ndesign-thinking processes in the seminar with four companies indicate that\ndesign managers are focused on more concrete design issues, with more\ninformation and less ambiguous content to the final design product. By\ncontrast, specialized designers manifest more abstract thinking and appear to\nexhibit a slightly higher level of divergence in their design processes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.00891,review,post_llm,2023,6,"{'ai_likelihood': 9.702311621771918e-06, 'text': ""Why They're Worried: Examining Experts' Motivations for Signing the\n  'Pause Letter'\n\n  This paper presents perspectives on the state of AI, as held by a sample of\nexperts. These experts were early signatories of the recent open letter from\nFuture of Life, which calls for a pause on advanced AI development. Utmost\neffort was put into accurately representing the perspectives of our\ninterviewees, and they have all read and approved of their representation.\nHowever, no paper could offer a perfect portrayal of their position. We feel\nconfident in what opinions we do put forward, but we do not hold them tightly.\nIn such dynamic times, we feel that no one should be resolved in their\nexpectations for AI and its future.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05033,regular,post_llm,2023,6,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Cash, Credibility, and Conversion: The Influence of Synthetic Media on\n  Investment Behavior\n\n  Prior to November of 2022, the topic of synthetic media was largely buried\nwithin academic journals, constrained to conversations about national security,\nand often fundamentally misunderstood. The release of ChatGPT, however, has\naccelerated discourse on the societal impacts of synthetic media. This study\nfirst highlights several gaps within existing literature on synthetic media,\nstructuring the impact potential and limitations of synthetic media threats\nwithin a theoretical framework. Second, it identifies financial information\nenvironments as prime candidates for future disruption via synthetic text\nmodalities, proposing an experimental survey for measuring the influential\npower of synthetic financial text on global investment communities. Rather than\nmerely assessing the ability of survey participants to distinguish genuine from\nsynthetic text, the experiment contained within this study measures synthetic\nmedia influence by observing its ability to manipulate belief via a series of\nbehavioral variables. The results indicate that synthetic text can\nsignificantly shift investor sentiment away from what it might otherwise have\nbeen under truthful information conditions. Furthermore, synthetic financial\ntext demonstrated a unique ability to ""convert"" investors, inspiring extreme\nchanges in outlook about a company compared to genuine financial texts. This\ntrend should inspire concern within the global financial community,\nparticularly given the historical vulnerability of equity markets to investor\nsentiment shocks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.12082,review,post_llm,2023,6,"{'ai_likelihood': 0.00034146838717990456, 'text': ""The Importance of Education for Technological Development and the Role\n  of Internet-Based Learning in Education\n\n  In today's world, many technologically advanced countries have realized that\nreal power lies not in physical strength but in educated minds. As a result,\nevery country has embarked on restructuring its education system to meet the\ndemands of technology. As a country in the midst of these developments, we\ncannot remain indifferent to this transformation in education. In the\nInformation Age of the 21st century, rapid access to information is crucial for\nthe development of individuals and societies. To take our place among the\nknowledge societies in a world moving rapidly towards globalization, we must\nclosely follow technological innovations and meet the requirements of\ntechnology. This can be achieved by providing learning opportunities to anyone\ninterested in acquiring education in their area of interest. This study focuses\non the advantages and disadvantages of internet-based learning compared to\ntraditional teaching methods, the importance of computer usage in\ninternet-based learning, negative factors affecting internet-based learning,\nand the necessary recommendations for addressing these issues. In today's\nworld, it is impossible to talk about education without technology or\ntechnology without education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.13886,regular,post_llm,2023,6,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Exploring Programming Task Creation of Primary School Teachers in\n  Training\n\n  Introducing computational thinking in primary school curricula implies that\nteachers have to prepare appropriate lesson material. Typically this includes\ncreating programming tasks, which may overwhelm primary school teachers with\nlacking programming subject knowledge. Inadequate resulting example code may\nnegatively affect learning, and students might adopt bad programming habits or\nmisconceptions. To avoid this problem, automated program analysis tools have\nthe potential to help scaffolding task creation processes. For example, static\nprogram analysis tools can automatically detect both good and bad code\npatterns, and provide hints on improving the code. To explore how teachers\ngenerally proceed when creating programming tasks, whether tool support can\nhelp, and how it is perceived by teachers, we performed a pre-study with 26 and\na main study with 59 teachers in training and the LitterBox static analysis\ntool for Scratch. We find that teachers in training (1) often start with\nbrainstorming thematic ideas rather than setting learning objectives, (2) write\ncode before the task text, (3) give more hints in their task texts and create\nfewer bugs when supported by LitterBox, and (4) mention both positive aspects\nof the tool and suggestions for improvement. These findings provide an improved\nunderstanding of how to inform teacher training with respect to support needed\nby teachers when creating programming tasks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.05768,review,post_llm,2023,6,"{'ai_likelihood': 3.1458006964789497e-06, 'text': ""Range Anxiety Among Battery Electric Vehicle Users: Both Distance and\n  Waiting Time Matter\n\n  Range anxiety is a major concern of battery electric vehicles (BEVs) users or\npotential users. Previous work has explored the influential factors of\ndistance-related range anxiety. However, time-related range anxiety has rarely\nbeen explored. The time cost when charging or waiting to charge the BEVs can\nnegatively impact BEV users' experience. As a preliminary attempt, this survey\nstudy investigated time-related anxiety by observing BEV users' charging\ndecisions in scenarios when both battery level and time cost are of concern. We\ncollected and analyzed responses from 217 BEV users in mainland China. The\nresults revealed that time-related anxiety exists and could affect users'\ncharging decisions. Further, users' charging decisions can be a result of the\ntrade-off between distance-related and time-related anxiety, and can be\nmoderated by several external factors (e.g., regions and individual\ndifferences). The findings can support the optimization of charge station\ndistribution and EV charge recommendation algorithms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.06901,regular,post_llm,2023,6,"{'ai_likelihood': 3.543164994981554e-06, 'text': 'Engaging Engineering Teams Through Moral Imagination: A Bottom-Up\n  Approach for Responsible Innovation and Ethical Culture Change in Technology\n  Companies\n\n  We propose a ""Moral Imagination"" methodology to facilitate a culture of\nresponsible innovation for engineering and product teams in technology\ncompanies. Our approach has been operationalized over the past two years at\nGoogle, where we have conducted over 50 workshops with teams across the\norganization. We argue that our approach is a crucial complement to existing\nformal and informal initiatives for fostering a culture of ethical awareness,\ndeliberation, and decision-making in technology design such as company\nprinciples, ethics and privacy review procedures, and compliance controls. We\ncharacterize some of the distinctive benefits of our methodology for the\ntechnology sector in particular.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2306.03351,review,post_llm,2023,6,"{'ai_likelihood': 0.005204942491319445, 'text': 'The current opportunities and challenges of Web 3.0\n\n  With recent advancements in AI and 5G technologies,as well as the nascent\nconcepts of blockchain and metaverse,a new revolution of the Internet,known as\nWeb 3.0,is emerging. Given its significant potential impact on the internet\nlandscape and various professional sectors,Web 3.0 has captured considerable\nattention from both academic and industry circles. This article presents an\nexploratory analysis of the opportunities and challenges associated with Web\n3.0. Firstly, the study evaluates the technical differences between Web 1.0,\nWeb 2.0, and Web 3.0, while also delving into the unique technical architecture\nof Web 3.0. Secondly, by reviewing current literature, the article highlights\nthe current state of development surrounding Web 3.0 from both economic and\ntechnological perspective. Thirdly, the study identifies numerous research and\nregulatory obstacles that presently confront Web 3.0 initiatives. Finally, the\narticle concludes by providing a forward-looking perspective on the potential\nfuture growth and progress of Web 3.0 technology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.03194,review,post_llm,2023,7,"{'ai_likelihood': 0.017632378472222224, 'text': 'Cryptoart: Ethical Challenges of the NFT Revolution\n\n  The digital transformation of the art world has become a revolution for the\nsector. Cryptoart, based on non-fungible tokens (NFT), is attracting the\nattention of artists, collectors and enthusiasts for its ability to tokenise\nany element that can be sold as art in the digital market. That means it is\nable to become a scarce resource and an economic asset by encapsulating the\nmarket value of a piece of digital art, which may or may not have a reference\nin the real world. This study will delve into the ethical aspects underlying\nwhat is known as the NFT Revolution, particularly impacts related to the abuse\nor destruction of cultural heritage, speculation and the generation of economic\nbubbles and environmental unsustainability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.1579,regular,post_llm,2023,7,"{'ai_likelihood': 1.953707800971137e-06, 'text': ""Unlikely Organizers: The Rise of Labor Activism Among Professionals in the U.S. Technology Industry\n\n  Tech workers -- professional workers in the technology industry including software engineers, product managers, UX designers, etc. -- are not normally associated with labor activism. Yet, since 2017, we have seen a significant rise in labor actions among this group. Using an original dataset, we demonstrate how, in the case of tech workers, periods of intense workplace social activism preceded later periods of heightened labor activism. Regression analysis confirms that participation in social activism increases the likelihood of labor activism six months to one year later at the same company. This finding extends Fantasia's cultures of solidarity argument to professional workers. We find that organizing emerges out of collective action and ensuing conflict with management: first, tech workers, guided by their professional interest in socially beneficial work, engage in workplace social activism. This generates solidarity among employee-participants but also creates conflict with management and leads to the emergence of labor activism among professionals."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.05167,regular,post_llm,2023,7,"{'ai_likelihood': 0.06666395399305555, 'text': 'A non-custodial wallet for digital currency: design challenges and opportunities\n\nCentral Bank Digital Currency (CBDC) is a novel form of money that could be issued and regulated by central banks, offering benefits such as programmability, security, and privacy. However, the design of a CBDC system presents numerous technical and social challenges. This paper presents the design and prototype of a non-custodial wallet, a device that enables users to store and spend CBDC in various contexts. To address the challenges of designing a CBDC system, we conducted a series of workshops with internal and external stakeholders, using methods such as storytelling, metaphors, and provotypes to communicate CBDC concepts, elicit user feedback and critique, and incorporate normative values into the technical design. We derived basic guidelines for designing CBDC systems that balance technical and social aspects, and reflect user needs and values. Our paper contributes to the CBDC discourse by demonstrating a practical example of how CBDC could be used in everyday life and by highlighting the importance of a user-centred approach.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.00323,regular,post_llm,2023,7,"{'ai_likelihood': 2.3941198984781902e-05, 'text': ""RUI: A Web-based Road Updates Information System using Google Maps API\n\n  Knowing the current situation on every road in an area is still difficult to\nanticipate. Commuters, riders, and drivers are still dependent on road\nsituations from a local news agency to be well informed and be updated on\npossible road updates such as vehicular accidents, government road and bridge\nprojects/construction, and other related road obstructions. To give solutions\nregarding road updates, a web-based roads update information system has been\ndeveloped that uses Google Maps API allowing people to view and be notified of\nthe real-time updates of the road situation of a specific area. This paper\ndiscusses the main system functionalities, including sub-systems and modules of\nthe system, the research approach and methodology, which is the Agile Model,\nand its impact on disseminating road information and its status. The project\nhas been evaluated using ISO 25010. Based on the evaluation result, the project\nhas been rated 4.21, signifying an excellent performance based on qualitative\ndescription through a Likert scale descriptive interpretation. The project has\nbeen running and hosted on the world wide web and is expected to expand its\ncoverage area from its origin country to the rest of the world. Based on the\ninitial findings of the study, the respondents agreed that the developed web\nsystem was functional and a massive help to commuters, riders, and people who\ntravel a lot. The system's overall effectiveness and performance were excellent\nbased on the criteria set by ISO/IEC 25010. It is recommended for future\ndevelopment to expand the coverage of the road updates, if possible, including\nthe entire Philippine archipelago for long-drive commuters and drivers to be\nmore updated in terms of road updates. Also, include the use of mobile\napplications for more user-friendly design and interactions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.13882,regular,post_llm,2023,7,"{'ai_likelihood': 1.556343502468533e-06, 'text': ""Human Culture: A History Irrelevant and Predictable Experience\n\n  Human culture research has witnessed an opportunity of revolution thanks to\nthe big data and social network revolution. Websites such as Douban.com,\nGoodreads.com, Pandora and IMDB become the new gold mine for cultural\nresearchers. In 2021 and 2022, the author of this paper invented 2 data-free\nrecommender systems for AI cold-start problem. The algorithms can recommend\ncultural and commercial products to users without reference to users' past\npreferences. The social implications of the new inventions are human cultural\ntastes can be predicted very precisely without any information related to human\nindividuals. In this paper, we analyze the AI technologies and its cultural\nimplications together with other AI algorithms. We show that human culture is\n(mostly) a history irrelevant and predictable experience.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.08823,review,post_llm,2023,7,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Risk assessment at AGI companies: A review of popular risk assessment\n  techniques from other safety-critical industries\n\n  Companies like OpenAI, Google DeepMind, and Anthropic have the stated goal of\nbuilding artificial general intelligence (AGI) - AI systems that perform as\nwell as or better than humans on a wide variety of cognitive tasks. However,\nthere are increasing concerns that AGI would pose catastrophic risks. In light\nof this, AGI companies need to drastically improve their risk management\npractices. To support such efforts, this paper reviews popular risk assessment\ntechniques from other safety-critical industries and suggests ways in which AGI\ncompanies could use them to assess catastrophic risks from AI. The paper\ndiscusses three risk identification techniques (scenario analysis, fishbone\nmethod, and risk typologies and taxonomies), five risk analysis techniques\n(causal mapping, Delphi technique, cross-impact analysis, bow tie analysis, and\nsystem-theoretic process analysis), and two risk evaluation techniques\n(checklists and risk matrices). For each of them, the paper explains how they\nwork, suggests ways in which AGI companies could use them, discusses their\nbenefits and limitations, and makes recommendations. Finally, the paper\ndiscusses when to conduct risk assessments, when to use which technique, and\nhow to use any of them. The reviewed techniques will be obvious to risk\nmanagement professionals in other industries. And they will not be sufficient\nto assess catastrophic risks from AI. However, AGI companies should not skip\nthe straightforward step of reviewing best practices from other industries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.02045,regular,post_llm,2023,7,"{'ai_likelihood': 1.0, 'text': ""The Role of ChatGPT in Democratizing Data Science: An Exploration of\n  AI-facilitated Data Analysis in Telematics\n\n  The realm of data science, once reserved for specialists, is undergoing a\nrevolution with the rapid emergence of generative AI, particularly through\ntools like ChatGPT. This paper posits ChatGPT as a pivotal bridge, drastically\nlowering the steep learning curve traditionally associated with complex data\nanalysis. By generating intuitive data narratives and offering real-time\nassistance, ChatGPT democratizes the field, enabling a wider audience to glean\ninsights from intricate datasets. A notable illustration of this transformative\npotential is provided through the examination of a synthetically generated\ntelematics dataset, wherein ChatGPT aids in distilling complex patterns and\ninsights. However, the journey to democratization is not without its hurdles.\nThe paper delves into challenges presented by such AI, from potential biases in\nanalysis to ChatGPT's limited reasoning capabilities. While the promise of a\ndemocratized data science landscape beckons, it is imperative to approach this\ntransition with caution, cognizance, and an ever-evolving understanding of the\ntool's capabilities and constraints.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.800060272216797e-05, 'GPT4': 0.685546875, 'CLAUDE': 5.412101745605469e-05, 'GOOGLE': 0.31396484375, 'OPENAI_O_SERIES': 5.9604644775390625e-06, 'DEEPSEEK': 0.0002796649932861328, 'GROK': 6.258487701416016e-06, 'NOVA': 3.159046173095703e-06, 'OTHER': 0.00011014938354492188, 'HUMAN': 1.2516975402832031e-06}}"
2307.05543,review,post_llm,2023,7,"{'ai_likelihood': 0.9951171875, 'text': 'Typology of Risks of Generative Text-to-Image Models\n\n  This paper investigates the direct risks and harms associated with modern\ntext-to-image generative models, such as DALL-E and Midjourney, through a\ncomprehensive literature review. While these models offer unprecedented\ncapabilities for generating images, their development and use introduce new\ntypes of risk that require careful consideration. Our review reveals\nsignificant knowledge gaps concerning the understanding and treatment of these\nrisks despite some already being addressed. We offer a taxonomy of risks across\nsix key stakeholder groups, inclusive of unexplored issues, and suggest future\nresearch directions. We identify 22 distinct risk types, spanning issues from\ndata bias to malicious use. The investigation presented here is intended to\nenhance the ongoing discourse on responsible model development and deployment.\nBy highlighting previously overlooked risks and gaps, it aims to shape\nsubsequent research and governance initiatives, guiding them toward the\nresponsible, secure, and ethically conscious evolution of text-to-image models.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00027370452880859375, 'GPT4': 0.984375, 'CLAUDE': 0.0005521774291992188, 'GOOGLE': 0.0138092041015625, 'OPENAI_O_SERIES': 7.253885269165039e-05, 'DEEPSEEK': 6.22868537902832e-05, 'GROK': 7.033348083496094e-06, 'NOVA': 7.212162017822266e-06, 'OTHER': 3.331899642944336e-05, 'HUMAN': 0.0006017684936523438}}"
2308.02677,review,post_llm,2023,7,"{'ai_likelihood': 4.967053731282553e-06, 'text': 'Metaverse for Industry 5.0 in NextG Communications: Potential\n  Applications and Future Challenges\n\n  With the advent of new technologies and endeavors for automation in almost\nall day-to-day activities, the recent discussions on the metaverse life have a\ngreater expectation. Furthermore, we are in the era of the fifth industrial\nrevolution, where machines and humans collaborate to maximize productivity with\nthe effective utilization of human intelligence and other resources. Hence,\nIndustry 5.0 in the metaverse may have tremendous technological integration for\na more immersive experience and enhanced communication.These technological\namalgamations are suitable for the present environment and entirely different\nfrom the previous perception of virtual technologies. This work presents a\ncomprehensive review of the applications of the metaverse in Industry 5.0\n(so-called industrial metaverse). In particular, we first provide a preliminary\nto the metaverse and industry 5.0 and discuss key enabling technologies of the\nindustrial metaverse, including virtual and augmented reality, 3D modeling,\nartificial intelligence, edge computing, digital twin, blockchain, and 6G\ncommunication networks. This work then explores diverse metaverse applications\nin Industry 5.0 vertical domains like Society 5.0, agriculture, supply chain\nmanagement, healthcare, education, and transportation. A number of research\nprojects are presented to showcase the conceptualization and implementation of\nthe industrial metaverse. Furthermore, various challenges in realizing the\nindustrial metaverse, feasible solutions, and future directions for further\nresearch have been presented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.04781,regular,post_llm,2023,7,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Demonstrations of the Potential of AI-based Political Issue Polling\n\n  Political polling is a multi-billion dollar industry with outsized influence\non the societal trajectory of the United States and nations around the world.\nHowever, it has been challenged by factors that stress its cost, availability,\nand accuracy. At the same time, artificial intelligence (AI) chatbots have\nbecome compelling stand-ins for human behavior, powered by increasingly\nsophisticated large language models (LLMs). Could AI chatbots be an effective\ntool for anticipating public opinion on controversial issues to the extent that\nthey could be used by campaigns, interest groups, and polling firms? We have\ndeveloped a prompt engineering methodology for eliciting human-like survey\nresponses from ChatGPT, which simulate the response to a policy question of a\nperson described by a set of demographic factors, and produce both an ordinal\nnumeric response score and a textual justification. We execute large scale\nexperiments, querying for thousands of simulated responses at a cost far lower\nthan human surveys. We compare simulated data to human issue polling data from\nthe Cooperative Election Study (CES). We find that ChatGPT is effective at\nanticipating both the mean level and distribution of public opinion on a\nvariety of policy issues such as abortion bans and approval of the US Supreme\nCourt, particularly in their ideological breakdown (correlation typically\n>85%). However, it is less successful at anticipating demographic-level\ndifferences. Moreover, ChatGPT tends to overgeneralize to new policy issues\nthat arose after its training data was collected, such as US support for\ninvolvement in the war in Ukraine. Our work has implications for our\nunderstanding of the strengths and limitations of the current generation of AI\nchatbots as virtual publics or online listening platforms, future directions\nfor LLM development, and applications of AI tools to the political domain.\n(Abridged)\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.1001,regular,post_llm,2023,7,"{'ai_likelihood': 3.2782554626464844e-06, 'text': ""Connecting Beliefs, Mindsets, Anxiety, and Self-Efficacy in Computer\n  Science Learning: An Instrument for Capturing Secondary School Students'\n  Self-Beliefs\n\n  Background and Context: Few instruments exist to measure students' CS\nengagement and learning especially in areas where coding happens with creative,\nproject-based learning and in regard to students' self-beliefs about computing.\nObjective: We introduce the CS Interests and Beliefs Inventory (CSIBI), an\ninstrument designed for novice secondary students learning by designing\nprojects (particularly with physical computing). The inventory contains\nsubscales on beliefs on problem solving competency, fascination in design,\nvalue of CS, creative expression, and beliefs about context-specific CS\nabilities alongside programming mindsets and outcomes. We explain the creation\nof the instrument and attend to the role of mindsets as mediators of\nself-beliefs and how CSIBI may be adapted to other K-12 project-based learning\nsettings. Method: We administered the instrument to 303 novice CS secondary\nstudents who largely came from historically marginalized backgrounds (gender,\nethnicity, and socioeconomic status). We assessed the nine-factor structure for\nthe 32-item instrument using confirmatory factor analysis and tested the\nhypothesized model of mindsets as mediators with structural equation modeling.\nFindings: We confirmed the nine factor structure of CSIBI and found significant\npositive correlations across factors. The structural model results showed that\nproblem solving competency beliefs and CS creative expression promoted\nprogramming growth mindset, which subsequently fostered students' programming\nself-concept. Implications: We validated an instrument to measure secondary\nstudents' self-beliefs in CS that fills several gaps in K-12 CS measurement\ntools by focusing on contexts of learning by designing. CSIBI can be easily\nadapted to other learning by designing computing education contexts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.01311,review,post_llm,2023,7,"{'ai_likelihood': 9.040037790934246e-06, 'text': 'Autonomous Vehicles for All?\n\n  The traditional build-and-expand approach is not a viable solution to keep\nroadway traffic rolling safely, so technological solutions, such as Autonomous\nVehicles (AVs), are favored. AVs have considerable potential to increase the\ncarrying capacity of roads, ameliorate the chore of driving, improve safety,\nprovide mobility for those who cannot drive, and help the environment. However,\nthey also raise concerns over whether they are socially responsible, accounting\nfor issues such as fairness, equity, and transparency. Regulatory bodies have\nfocused on AV safety, cybersecurity, privacy, and legal liability issues, but\nhave failed to adequately address social responsibility. Thus, existing AV\ndevelopers do not have to embed social responsibility factors in their\nproprietary technology. Adverse bias may therefore occur in the development and\ndeployment of AV technology. For instance, an artificial intelligence-based\npedestrian detection application used in an AV may, in limited lighting\nconditions, be biased to detect pedestrians who belong to a particular racial\ndemographic more efficiently compared to pedestrians from other racial\ndemographics. Also, AV technologies tend to be costly, with a unique hardware\nand software setup which may be beyond the reach of lower-income people. In\naddition, data generated by AVs about their users may be misused by third\nparties such as corporations, criminals, or even foreign governments. AVs\npromise to dramatically impact labor markets, as many jobs that involve driving\nwill be made redundant. We argue that the academic institutions, industry, and\ngovernment agencies overseeing AV development and deployment must act\nproactively to ensure that AVs serve all and do not increase the digital divide\nin our society.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.02492,regular,post_llm,2023,7,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Influence of the algorithm's reliability and transparency in the user's\n  decision-making process\n\n  Algorithms have been becoming increasingly relevant for various\ndecision-making processes in the forms of Decision Support Systems or\nDecision-making systems in areas such as Criminal-Justice systems, Job\nApplication Filtering, Medicine, and Healthcare to name a few. It is crucial\nfor these algorithms to be fair and for the users to have confidence in these\ndecisions, especially in the above contexts, because they have a high impact on\nsociety. We conduct an online empirical study with 61 participants to find out\nhow the change in transparency and reliability of an algorithm which determines\nthe probability of lesions being melanoma could impact users' decision-making\nprocess, as well as the confidence in the decisions made by the algorithm. The\nresults indicate that people show at least moderate confidence in the decisions\nof the algorithm even when the reliability is bad. However, they would not\nblindly follow the algorithm's wrong decisions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.07903,review,post_llm,2023,7,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'The science of fake news\n\n  Fake news emerged as an apparent global problem during the 2016 U.S.\nPresidential election. Addressing it requires a multidisciplinary effort to\ndefine the nature and extent of the problem, detect fake news in real time, and\nmitigate its potentially harmful effects. This will require a better\nunderstanding of how the Internet spreads content, how people process news, and\nhow the two interact. We review the state of knowledge in these areas and\ndiscuss two broad potential mitigation strategies: better enabling individuals\nto identify fake news, and intervention within the platforms to reduce the\nattention given to fake news. The cooperation of Internet platforms (especially\nFacebook, Google, and Twitter) with researchers will be critical to\nunderstanding the scale of the issue and the effectiveness of possible\ninterventions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.02916,regular,post_llm,2023,7,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""The impact of an employee's psychological contract breach on compliance\n  with information security policies: intrinsic and extrinsic motivation\n\n  Despite the rapid rise in social engineering attacks, not all employees are\nas compliant with information security policies (ISPs) to the extent that\norganisations expect them to be. ISP non-compliance is caused by a variety of\npsychological motivation. This study investigates the effect of psychological\ncontract breach (PCB) of employees on ISP compliance intention (ICI) by\ndividing them into intrinsic and extrinsic motivation using the theory of\nplanned behaviour (TPB) and the general deterrence theory (GDT). Data analysis\nfrom UK employees (\\textit{n=206}) showed that the higher the PCB, the lower\nthe ICI. The study also found that PCBs significantly reduced intrinsic\nmotivation (attitude and perceived fairness) for ICI, whereas PCBs did not\nmoderate the relationship between extrinsic motivation (sanction severity and\nsanctions certainty) and ICI. As a result, this study successfully addresses\nthe risks of PCBs in the field of IS security and proposes effective solutions\nfor employees with high PCBs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.09979,regular,post_llm,2023,7,"{'ai_likelihood': 0.002360873752170139, 'text': ""From Ukraine to the World: Using LinkedIn Data to Monitor Professional\n  Migration from Ukraine\n\n  Highly skilled professionals' forced migration from Ukraine was triggered by\nthe conflict in Ukraine in 2014 and amplified by the Russian invasion in 2022.\nHere, we utilize LinkedIn estimates and official refugee data from the World\nBank and the United Nations Refugee Agency, to understand which are the main\npull factors that drive the decision-making process of the host country. We\nidentify an ongoing and escalating exodus of educated individuals, largely\ndrawn to Poland and Germany, and underscore the crucial role of pre-existing\nnetworks in shaping these migration flows. Key findings include a strong\ncorrelation between LinkedIn's estimates of highly educated Ukrainian displaced\npeople and official UN refugee statistics, pointing to the significance of\nprior relationships with Ukraine in determining migration destinations. We\ntrain a series of multilinear regression models and the SHAP method revealing\nthat the existence of a support network is the most critical factor in choosing\na destination country, while distance is less important. Our main findings show\nthat the migration patterns of Ukraine's highly skilled workforce, and their\nimpact on both the origin and host countries, are largely influenced by\npreexisting networks and communities. This insight can inform strategies to\ntackle the economic challenges posed by this loss of talent and maximize the\nbenefits of such migration for both Ukraine and the receiving nations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.04699,review,post_llm,2023,7,"{'ai_likelihood': 6.291601392957899e-07, 'text': ""International Institutions for Advanced AI\n\n  International institutions may have an important role to play in ensuring\nadvanced AI systems benefit humanity. International collaborations can unlock\nAI's ability to further sustainable development, and coordination of regulatory\nefforts can reduce obstacles to innovation and the spread of benefits.\nConversely, the potential dangerous capabilities of powerful and\ngeneral-purpose AI systems create global externalities in their development and\ndeployment, and international efforts to further responsible AI practices could\nhelp manage the risks they pose. This paper identifies a set of governance\nfunctions that could be performed at an international level to address these\nchallenges, ranging from supporting access to frontier AI systems to setting\ninternational safety standards. It groups these functions into four\ninstitutional models that exhibit internal synergies and have precedents in\nexisting organizations: 1) a Commission on Frontier AI that facilitates expert\nconsensus on opportunities and risks from advanced AI, 2) an Advanced AI\nGovernance Organization that sets international standards to manage global\nthreats from advanced models, supports their implementation, and possibly\nmonitors compliance with a future governance regime, 3) a Frontier AI\nCollaborative that promotes access to cutting-edge AI, and 4) an AI Safety\nProject that brings together leading researchers and engineers to further AI\nsafety research. We explore the utility of these models and identify open\nquestions about their viability.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.12549,regular,post_llm,2023,7,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Estimating Time to Clear Pendency of Cases in High Courts in India using\n  Linear Regression\n\n  Indian Judiciary is suffering from burden of millions of cases that are lying\npending in its courts at all the levels. The High Court National Judicial Data\nGrid (HC-NJDG) indexes all the cases pending in the high courts and publishes\nthe data publicly. In this paper, we analyze the data that we have collected\nfrom the HC-NJDG portal on 229 randomly chosen days between August 31, 2017 to\nMarch 22, 2020, including these dates. Thus, the data analyzed in the paper\nspans a period of more than two and a half years. We show that: 1) the pending\ncases in most of the high courts is increasing linearly with time. 2) the case\nload on judges in various high courts is very unevenly distributed, making\njudges of some high courts hundred times more loaded than others. 3) for some\nhigh courts it may take even a hundred years to clear the pendency cases if\nproper measures are not taken.\n  We also suggest some policy changes that may help clear the pendency within a\nfixed time of either five or fifteen years. Finally, we find that the rate of\ninstitution of cases in high courts can be easily handled by the current\nsanctioned strength. However, extra judges are needed only to clear earlier\nbacklogs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.15768,regular,post_llm,2023,7,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""DARSAN: A Decentralized Review System Suitable for NFT Marketplaces\n\n  We introduce DARSAN, a decentralized review system designed for Non-Fungible\nToken (NFT) marketplaces, to address the challenge of verifying the quality of\nhighly resalable products with few verified buyers by incentivizing unbiased\nreviews. DARSAN works by iteratively selecting a group of reviewers (called\n``experts'') who are likely to both accurately predict the objective popularity\nand assess some subjective quality of the assets uniquely associated with NFTs.\nThe system consists of a two-phased review process: a ``pre-listing'' phase\nwhere only experts can review the product, and a ``pre-sale'' phase where any\nreviewer on the system can review the product. Upon completion of the sale,\nDARSAN distributes incentives to the participants and selects the next\ngeneration of experts based on the performance of both experts and non-expert\nreviewers. We evaluate DARSAN through simulation and show that, once\nbootstrapped with an initial set of appropriately chosen experts, DARSAN favors\nhonest reviewers and improves the quality of the expert pool over time without\nany external intervention even in the presence of potentially malicious\nparticipants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.02665,review,post_llm,2023,7,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""A design theory for transparency of information privacy practices\n\n  The rising diffusion of information systems (IS) throughout society poses an\nincreasingly serious threat to privacy as a social value. One approach to\nalleviating this threat is to establish transparency of information privacy\npractices (TIPP) so that consumers can better understand how their information\nis processed. However, the design of transparency artifacts (eg, privacy\nnotices) has clearly not followed this approach, given the ever-increasing\nvolume of information processing. Hence, consumers face a situation where they\ncannot see the 'forest for the trees' when aiming to ascertain whether\ninformation processing meets their privacy expectations. A key problem is that\noverly comprehensive information presentation results in information overload\nand is thus counterproductive for establishing TIPP. We depart from the extant\ndesign logic of transparency artifacts and develop a theoretical foundation\n(TIPP theory) for transparency artifact designs useful for establishing TIPP\nfrom the perspective of privacy as a social value. We present TIPP theory in\ntwo parts to capture the sociotechnical interplay. The first part translates\nabstract knowledge on the IS artifact and privacy into a description of social\nsubsystems of transparency artifacts, and the second part conveys prescriptive\ndesign knowledge in form of a corresponding IS design theory. TIPP theory\nestablishes a bridge from the complexity of the privacy concept to a metadesign\nfor transparency artifacts that is useful to establish TIPP in any IS. In\nessence, transparency artifacts must accomplish more than offering\ncomprehensive information; they must also be adaptive to the current\ninformation needs of consumers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.01756,regular,post_llm,2023,7,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""Identifying Professional Photographers Through Image Quality and\n  Aesthetics in Flickr\n\n  In our generation, there is an undoubted rise in the use of social media and\nspecifically photo and video sharing platforms. These sites have proved their\nability to yield rich data sets through the users' interaction which can be\nused to perform a data-driven evaluation of capabilities. Nevertheless, this\nstudy reveals the lack of suitable data sets in photo and video sharing\nplatforms and evaluation processes across them. In this way, our first\ncontribution is the creation of one of the largest labelled data sets in Flickr\nwith the multimodal data which has been open sourced as part of this\ncontribution. Predicated on these data, we explored machine learning models and\nconcluded that it is feasible to properly predict whether a user is a\nprofessional photographer or not based on self-reported occupation labels and\nseveral feature representations out of the user, photo and crowdsourced sets.\nWe also examined the relationship between the aesthetics and technical quality\nof a picture and the social activity of that picture. Finally, we depicted\nwhich characteristics differentiate professional photographers from\nnon-professionals. As far as we know, the results presented in this work\nrepresent an important novelty for the users' expertise identification which\nresearchers from various domains can use for different applications.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.03196,review,post_llm,2023,7,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""ChatGPT is not a pocket calculator -- Problems of AI-chatbots for\n  teaching Geography\n\n  The recent success of large language models and AI chatbots such as ChatGPT\nin various knowledge domains has a severe impact on teaching and learning\nGeography and GIScience. The underlying revolution is often compared to the\nintroduction of pocket calculators, suggesting analogous adaptations that\nprioritize higher-level skills over other learning content. However, using\nChatGPT can be fraudulent because it threatens the validity of assessments. The\nsuccess of such a strategy therefore rests on the assumption that lower-level\nlearning goals are substitutable by AI, and supervision and assessments can be\nrefocused on higher-level goals. Based on a preliminary survey on ChatGPT's\nquality in answering questions in Geography and GIScience, we demonstrate that\nthis assumption might be fairly naive, and effective control in assessments and\nsupervision is required.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.0491,regular,post_llm,2023,7,"{'ai_likelihood': 0.00014543533325195312, 'text': 'Medical Misinformation in AI-Assisted Self-Diagnosis: Development of a\n  Method (EvalPrompt) for Analyzing Large Language Models\n\n  Rapid integration of large language models (LLMs) in health care is sparking\nglobal discussion about their potential to revolutionize health care quality\nand accessibility. At a time when improving health care quality and access\nremains a critical concern for countries worldwide, the ability of these models\nto pass medical examinations is often cited as a reason to use them for medical\ntraining and diagnosis. However, the impact of their inevitable use as a\nself-diagnostic tool and their role in spreading healthcare misinformation has\nnot been evaluated. This study aims to assess the effectiveness of LLMs,\nparticularly ChatGPT, from the perspective of an individual self-diagnosing to\nbetter understand the clarity, correctness, and robustness of the models. We\npropose the comprehensive testing methodology evaluation of LLM prompts\n(EvalPrompt). This evaluation methodology uses multiple-choice medical\nlicensing examination questions to evaluate LLM responses. We use open-ended\nquestions to mimic real-world self-diagnosis use cases, and perform sentence\ndropout to mimic realistic self-diagnosis with missing information. Human\nevaluators then assess the responses returned by ChatGPT for both experiments\nfor clarity, correctness, and robustness. The results highlight the modest\ncapabilities of LLMs, as their responses are often unclear and inaccurate. As a\nresult, medical advice by LLMs should be cautiously approached. However,\nevidence suggests that LLMs are steadily improving and could potentially play a\nrole in healthcare systems in the future. To address the issue of medical\nmisinformation, there is a pressing need for the development of a comprehensive\nself-diagnosis dataset. This dataset could enhance the reliability of LLMs in\nmedical applications by featuring more realistic prompt styles with minimal\ninformation across a broader range of medical fields.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.10903,regular,post_llm,2023,7,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'VoteLab: A Modular and Adaptive Experimentation Platform for Online\n  Collective Decision Making\n\n  Digital democracy and new forms for direct digital participation in policy\nmaking gain unprecedented momentum. This is particularly the case for\npreferential voting methods and decision-support systems designed to promote\nfairer, more inclusive and legitimate collective decision-making processes in\ncitizens assemblies, participatory budgeting and elections. However, a\nsystematic human experimentation with different voting methods is cumbersome\nand costly. This paper introduces VoteLab, an open-source and\nthoroughly-documented platform for modular and adaptive design of voting\nexperiments. It supports to visually and interactively build reusable campaigns\nwith a choice of different voting methods, while voters can easily respond to\nsubscribed voting questions on a smartphone. A proof-of-concept with four\nvoting methods and questions on COVID-19 in an online lab experiment have been\nused to study the consistency of voting outcomes. It demonstrates the\ncapability of VoteLab to support rigorous experimentation of complex voting\nscenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.01918,review,post_llm,2023,7,"{'ai_likelihood': 5.3313043382432725e-06, 'text': 'Computational Reproducibility in Computational Social Science\n\n  Replication crises have shaken the scientific landscape during the last\ndecade. As potential solutions, open science practices were heavily discussed\nand have been implemented with varying success in different disciplines. We\nargue that computational-x disciplines such as computational social science,\nare also susceptible for the symptoms of the crises, but in terms of\nreproducibility. We expand the binary definition of reproducibility into a tier\nsystem which allows increasing levels of reproducibility based on external\nverfiability to counteract the practice of open-washing. We provide solutions\nfor barriers in Computational Social Science that hinder researchers from\nobtaining the highest level of reproducibility, including the use of alternate\ndata sources and considering reproducibility proactively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.15846,review,post_llm,2023,7,"{'ai_likelihood': 1.0, 'text': 'Education 5.0: Requirements, Enabling Technologies, and Future\n  Directions\n\n  We are currently in a post-pandemic era in which life has shifted to a\ndigital world. This has affected many aspects of life, including education and\nlearning. Education 5.0 refers to the fifth industrial revolution in education\nby leveraging digital technologies to eliminate barriers to learning, enhance\nlearning methods, and promote overall well-being. The concept of Education 5.0\nrepresents a new paradigm in the field of education, one that is focused on\ncreating a learner-centric environment that leverages the latest technologies\nand teaching methods. This paper explores the key requirements of Education 5.0\nand the enabling technologies that make it possible, including artificial\nintelligence, blockchain, and virtual and augmented reality. We analyze the\npotential impact of these technologies on the future of education, including\ntheir ability to improve personalization, increase engagement, and provide\ngreater access to education. Additionally, we examine the challenges and\nethical considerations associated with Education 5.0 and propose strategies for\naddressing these issues. Finally, we offer insights into future directions for\nthe development of Education 5.0, including the need for ongoing research,\ncollaboration, and innovation in the field. Overall, this paper provides a\ncomprehensive overview of Education 5.0, its requirements, enabling\ntechnologies, and future directions, and highlights the potential of this new\nparadigm to transform education and improve learning outcomes for students.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.443359375, 'GPT4': 0.0007867813110351562, 'CLAUDE': 0.0003910064697265625, 'GOOGLE': 0.03900146484375, 'OPENAI_O_SERIES': 5.40614128112793e-05, 'DEEPSEEK': 1.5556812286376953e-05, 'GROK': 7.152557373046875e-06, 'NOVA': 4.589557647705078e-05, 'OTHER': 0.51611328125, 'HUMAN': 6.258487701416016e-06}}"
2307.10615,regular,post_llm,2023,7,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Analyzing HC-NJDG Data to Understand the Pendency in High Courts in\n  India\n\n  Indian Judiciary is suffering from burden of millions of cases that are lying\npending in its courts at all the levels. In this paper, we analyze the data\nthat we have collected on the pendency of 24 high courts in the Republic of\nIndia as they were made available on High Court NJDG (HC-NJDG). We collected\ndata on 73 days beginning August 31, 2017 to December 26, 2018, including these\ndays. Thus, the data collected by us spans a period of almost sixteen months.\nWe have analyzed various statistics available on the NJDG portal for High\nCourts, including but not limited to the number of judges in each high court,\nthe number of cases pending in each high court, cases that have been pending\nfor more than 10 years, cases filed, listed and disposed, cases filed by women\nand senior citizens, etc. Our results show that: 1) statistics as important as\nthe number of judges in high courts have serious errors on NJDG (Fig. 1, 2, 10,\n11, Table V). 2) pending cases in most of the high courts are increasing rather\nthan decreasing (Fig. 3, 13). 3) regular update of HC-NJDG is required for it\nto be useful. Data related to some high courts is not being updated regularly\nor is updated erroneously on the portal (Fig. 14). 4) there is a huge\ndifference in terms of average load of cases on judges of different high courts\n(Fig. 6). 5) if all the high courts operate at their approved strength of\njudges, then for most of the high courts pendency can be nullified within 20\nyears from now (Fig. 21, 22). 6) the pending cases filed by women and senior\ncitizens are disproportionately low, they together constitute less than 10% of\nthe total pending cases (Fig. 23 - 27) 7) a better scheduling process for\npreparing causelists in courts can help reducing the number of pending cases in\nthe High Courts (Fig. 29). 8) some statistics are not well defined (Fig. 31).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.11412,review,post_llm,2023,7,"{'ai_likelihood': 6.126032935248481e-06, 'text': 'Hybrid deliberation: Citizen dialogues in a post-pandemic era\n\n  This report first provides a brief review of various forms of dialogue-based\nparticipation, e.g., Citizen Assembly, Citizen Lottery, Citizen Jury,\nDeliberative Polling, and Participatory Budgeting. Challenges associated with\nthese long-lasting practices are identified and hybrid deliberation is proposed\nas a concept to address the challenges. The report then analyzes six leading\nexamples of digital or hybrid formats of citizen dialogues. Through the\ncomparison of the cases, the report concludes about the hurdles/risks, success\nfactors/opportunities, and best practices for a complementary use of digital\nand analogue participation formats. Hybrid deliberation is proposed to be the\nfuture direction for dialogue-based participation that involves masses and\ngenerates high-quality outcomes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.11662,regular,post_llm,2023,7,"{'ai_likelihood': 0.87548828125, 'text': ""BlockCampus: A Blockchain-Based DApp for enhancing Student Engagement\n  and Reward Mechanisms in an Academic Community for E-JUST University\n\n  In today's digital age, online communities have become an integral part of\nour lives, fostering collaboration, knowledge sharing, and community\nengagement. Higher education institutions, in particular, can greatly benefit\nfrom dedicated platforms that facilitate academic discussions and provide\nincentives for active participation. This research paper presents a\ncomprehensive study and implementation of a decentralized application (DApp)\nleveraging the blockchain technology to address these needs specifically for\nE-JUST (Egypt-Japan University of Science and Technology) students and academic\nstaff.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.016876220703125, 'GPT4': 0.2274169921875, 'CLAUDE': 0.1871337890625, 'GOOGLE': 0.422119140625, 'OPENAI_O_SERIES': 0.0570068359375, 'DEEPSEEK': 0.0017576217651367188, 'GROK': 0.0002779960632324219, 'NOVA': 0.0013113021850585938, 'OTHER': 0.04132080078125, 'HUMAN': 0.044769287109375}}"
2307.16296,review,post_llm,2023,7,"{'ai_likelihood': 1.0, 'text': 'Integrating Information Technology in Healthcare: Recent Developments,\n  Challenges, and Future Prospects for Urban and Regional Health\n\n  The use of technology in healthcare has become increasingly popular in recent\nyears, with the potential to improve how healthcare is delivered, patient\noutcomes, and cost-effectiveness. This review paper provides an overview of how\ntechnology has been used in healthcare, particularly in cities and for\npersonalized medicine. The paper discusses different ways technology is being\nused in healthcare, such as electronic health records, telemedicine, remote\nmonitoring, medical imaging, wearable devices, and artificial intelligence. It\nalso looks at the challenges and problems that come with using technology in\nhealthcare, such as keeping patient data private and secure, making sure\ndifferent technology systems can work together, and ensuring patients are\ncomfortable using technology. In addition, the paper explores the potential of\ntechnology in healthcare, including improving how easily patients can get care,\nthe quality of care they receive, and the cost of care. It also talks about how\ntechnology can help personalize care to individual patients. Finally, the paper\nsummarizes the main points, makes recommendations for healthcare providers and\npolicymakers, and suggests directions for future research. Overall, this review\nshows how technology can be used to improve healthcare, while also\nacknowledging the challenges that come with using technology in this way.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.8974609375, 'GPT4': 0.00152587890625, 'CLAUDE': 0.0004818439483642578, 'GOOGLE': 0.019012451171875, 'OPENAI_O_SERIES': 9.489059448242188e-05, 'DEEPSEEK': 1.0311603546142578e-05, 'GROK': 1.0728836059570312e-06, 'NOVA': 6.508827209472656e-05, 'OTHER': 0.08154296875, 'HUMAN': 1.0728836059570312e-06}}"
2307.11464,regular,post_llm,2023,7,"{'ai_likelihood': 3.1789143880208335e-06, 'text': 'Supporting Post-disaster Recovery with Agent-based Modeling in\n  Multilayer Socio-physical Networks\n\n  The examination of post-disaster recovery (PDR) in a socio-physical system\nenables us to elucidate the complex relationships between humans and\ninfrastructures. Although existing studies have identified many patterns in the\nPDR process, they fall short of describing how individual recoveries contribute\nto the overall recovery of the system. To enhance the understanding of\nindividual return behavior and the recovery of point-of-interests (POIs), we\npropose an agent-based model (ABM), called PostDisasterSim. We apply the model\nto analyze the recovery of five counties in Texas following Hurricane Harvey in\n2017. Specifically, we construct a three-layer network comprising the human\nlayer, the social infrastructure layer, and the physical infrastructure layer,\nusing mobile phone location data and POI data. Based on prior studies and a\nhousehold survey, we develop the ABM to simulate how evacuated individuals\nreturn to their homes, and social and physical infrastructures recover. By\nimplementing the ABM, we unveil the heterogeneity in recovery dynamics in terms\nof agent types, housing types, household income levels, and geographical\nlocations. Moreover, simulation results across nine scenarios quantitatively\ndemonstrate the positive effects of social and physical infrastructure\nimprovement plans. This study can assist disaster scientists in uncovering\nnuanced recovery patterns and policymakers in translating policies like\nresource allocation into practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.11119,regular,post_llm,2023,7,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'From computational ethics to morality: how decision-making algorithms\n  can help us understand the emergence of moral principles, the existence of an\n  optimal behaviour and our ability to discover it\n\n  This paper adds to the efforts of evolutionary ethics to naturalize morality\nby providing specific insights derived from a computational ethics view. We\npropose a stylized model of human decision-making, which is based on\nReinforcement Learning, one of the most successful paradigms in Artificial\nIntelligence. After the main concepts related to Reinforcement Learning have\nbeen presented, some particularly useful parallels are drawn that can\nilluminate evolutionary accounts of ethics. Specifically, we investigate the\nexistence of an optimal policy (or, as we will refer to, objective ethical\nprinciples) given the conditions of an agent. In addition, we will show how\nthis policy is learnable by means of trial and error, supporting our hypotheses\non two well-known theorems in the context of Reinforcement Learning. We\nconclude by discussing how the proposed framework can be enlarged to study\nother potentially interesting areas of human behavior from a formalizable\nperspective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.1665,regular,post_llm,2023,7,"{'ai_likelihood': 0.00010722213321261936, 'text': 'ChatGPT for Teaching and Learning: An Experience from Data Science\n  Education\n\n  ChatGPT, an implementation and application of large language models, has\ngained significant popularity since its initial release. Researchers have been\nexploring ways to harness the practical benefits of ChatGPT in real-world\nscenarios. Educational researchers have investigated its potential in various\nsubjects, e.g., programming, mathematics, finance, clinical decision support,\netc. However, there has been limited attention given to its application in data\nscience education. This paper aims to bridge that gap by utilizing ChatGPT in a\ndata science course, gathering perspectives from students, and presenting our\nexperiences and feedback on using ChatGPT for teaching and learning in data\nscience education. The findings not only distinguish data science education\nfrom other disciplines but also uncover new opportunities and challenges\nassociated with incorporating ChatGPT into the data science curriculum.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.09753,regular,post_llm,2023,7,"{'ai_likelihood': 2.1060307820638023e-05, 'text': 'Unmaking AI Imagemaking: A Methodological Toolkit for Critical\n  Investigation\n\n  AI image models are rapidly evolving, disrupting aesthetic production in many\nindustries. However, understanding of their underlying archives, their logic of\nimage reproduction, and their persistent biases remains limited. What kind of\nmethods and approaches could open up these black boxes? In this paper, we\nprovide three methodological approaches for investigating AI image models and\napply them to Stable Diffusion as a case study. Unmaking the ecosystem analyzes\nthe values, structures, and incentives surrounding the model\'s production.\nUnmaking the data analyzes the images and text the model draws upon, with their\nattendant particularities and biases. Unmaking the output analyzes the model\'s\ngenerative results, revealing its logics through prompting, reflection, and\niteration. Each mode of inquiry highlights particular ways in which the image\nmodel captures, ""understands,"" and recreates the world. This accessible\nframework supports the work of critically investigating generative AI image\nmodels and paves the way for more socially and politically attuned analyses of\ntheir impacts in the world.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.06743,regular,post_llm,2023,7,"{'ai_likelihood': 3.2120280795627172e-06, 'text': 'Smart Cities and Digital Twins in Lower Austria\n\n  Smart city solutions require innovative governance approaches together with\nthe smart use of technology, such as digital twins, by city managers and\npolicymakers to manage the big societal challenges. The project Smart Cities\naNd Digital Twins in Lower Austria (SCiNDTiLA) extends the state of the art of\nresearch in several contributing disciplines and uses the foundations of\ncomplexity theory and computational social science methods to develop a\ndigital-twin-based smart city model. The project will also apply a novel\ntransdisciplinary process to conceptualise sustainable smart cities and\nvalidate the smart city generic model. The outcomes will be translated into a\nroadmap highlighting methodologies, guidelines and policy recommendations for\ntackling societal challenges in smart cities with a focus on rescaling the\nentire framework to be transferred to regions, smaller towns and non-urban\nenvironments, such as rural areas and smart villages, in ways that fit the\nrespective local governance, ethical and operational capacity context.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.06563,review,post_llm,2023,7,"{'ai_likelihood': 0.03780788845486111, 'text': 'Money: Who Has a Stake in the Most Value-Centric Common Design Material?\n\n  Money is more than just a numeric value. It embodies trust and moral gravity,\nand it offers flexible ways to transact. However, the emergence of Central Bank\nDigital Currency (CBDC) is set to bring about a drastic change in the future of\nmoney. This paper invites designers to reflect on their role in shaping\nmaterial and immaterial monetary change. In this rapidly changing landscape,\ndesign could be instrumental in uncovering and showcasing the diverse values\nthat money holds for different stakeholders. Understanding these diversities\ncould promote a more equitable and inclusive financial, social, and global\nlandscape within emergent forms of cash-like digital currency. Without such\nconsideration, certain forms of money we have come to know could disappear,\nalong with the values people hold upon them. We report on semi-structured\ninterviews with stakeholders who have current knowledge or involvement in the\nemerging field of Central Bank Digital Currency (CBDC). Our research indicates\nthat this new form of money presents both challenges and opportunities for\ndesigners. Specifically, we emphasise the potential for Central Bank Digital\nCurrency (CBDC) to either positively or negatively reform values through its\ndesign. By considering time, reflecting present values, and promoting inclusion\nin its deployment, we can strive to ensure that Central Bank Digital Currency\n(CBDC) represents the diverse needs and perspectives of its users.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2307.05842,review,post_llm,2023,7,"{'ai_likelihood': 0.99755859375, 'text': 'The Butterfly Effect in Artificial Intelligence Systems: Implications\n  for AI Bias and Fairness\n\n  The Butterfly Effect, a concept originating from chaos theory, underscores\nhow small changes can have significant and unpredictable impacts on complex\nsystems. In the context of AI fairness and bias, the Butterfly Effect can stem\nfrom a variety of sources, such as small biases or skewed data inputs during\nalgorithm development, saddle points in training, or distribution shifts in\ndata between training and testing phases. These seemingly minor alterations can\nlead to unexpected and substantial unfair outcomes, disproportionately\naffecting underrepresented individuals or groups and perpetuating pre-existing\ninequalities. Moreover, the Butterfly Effect can amplify inherent biases within\ndata or algorithms, exacerbate feedback loops, and create vulnerabilities for\nadversarial attacks. Given the intricate nature of AI systems and their\nsocietal implications, it is crucial to thoroughly examine any changes to\nalgorithms or input data for potential unintended consequences. In this paper,\nwe envision both algorithmic and empirical strategies to detect, quantify, and\nmitigate the Butterfly Effect in AI systems, emphasizing the importance of\naddressing these challenges to promote fairness and ensure responsible AI\ndevelopment.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0031452178955078125, 'GPT4': 0.93212890625, 'CLAUDE': 0.0017614364624023438, 'GOOGLE': 0.061065673828125, 'OPENAI_O_SERIES': 0.0004241466522216797, 'DEEPSEEK': 3.898143768310547e-05, 'GROK': 2.9206275939941406e-06, 'NOVA': 1.817941665649414e-05, 'OTHER': 0.0012884140014648438, 'HUMAN': 0.0001634359359741211}}"
2308.05038,regular,post_llm,2023,8,"{'ai_likelihood': 7.953908708360461e-05, 'text': 'Xenophobic Events vs. Refugee Population -- Using GDELT to Identify\n  Countries with Disproportionate Coverage\n\n  In this preliminary study, we used the Global Database of Events, Language,\nand Tone (GDELT) database to examine xenophobic events reported in the media\nduring 2022. We collected a dataset of 2,778 unique events and created a\nchoropleth map illustrating the frequency of events scaled by the refugee\npopulation\'s proportion in each host country. We identified the top 10\ncountries with the highest scaled event frequencies among those with more than\n50,000 refugees. Contrary to the belief that hosting a significant number of\nforced migrants results in higher xenophobic incidents, our findings indicate a\npotential connection to political factors. We also categorized the 20 root\nevent codes in the CAMEO event data as either ""Direct"" or ""Indirect"". Almost\n90% of the events related to refugees in 2022 were classified as ""Indirect"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.0728,regular,post_llm,2023,8,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Distributed Governance: a Principal-Agent Approach to Data Governance --\n  Part 1 Background & Core Definitions\n\n  To address the need for regulating digital technologies without hampering\ninnovation or pre-digital transformation regulatory frameworks, we provide a\nmodel to evolve Data governance toward Information governance and precise the\nrelation between these two terms. This model bridges digital and non-digital\ninformation exchange. By considering the question of governed data usage\nthrough the angle of the Principal-Agent problem, we build a distributed\ngovernance model based on Autonomous Principals defined as entities capable of\nchoice, therefore capable of exercising a transactional sovereignty. Extending\nthe legal concept of the privacy sphere to a functional equivalent in the\ndigital space leads to the construction of a digital self to which rights and\naccountability can be attached. Ecosystems, defined as communities of\nautonomous principals bound by a legitimate authority, provide the basis of\ninteracting structures of increasing complexity endowed with a self-replicating\nproperty that mirrors physical world governance systems. The model proposes a\ngovernance concept for multi-stakeholder information systems operating across\njurisdictions. Using recent software engineering advances in decentralised\nauthentication and semantics, we provide a framework, Dynamic Data Economy to\ndeploy a distributed governance model embedding checks and balance between\nhuman and technological governance. Domain specific governance models are left\nfor further publications. Similarly, the technical questions related to the\nconnection between a digital-self and its physical world controller (e.g\nbiometric binding) will be treated in upcoming publications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.02961,regular,post_llm,2023,8,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""A Study of China's Censorship and Its Evasion Through the Lens of Online\n  Gaming\n\n  For the past 20 years, China has increasingly restricted the access of minors\nto online games using addiction prevention systems (APSes). At the same time,\nand through different means, i.e., the Great Firewall of China (GFW), it also\nrestricts general population access to the international Internet. This paper\nstudies how these restrictions impact young online gamers, and their evasion\nefforts. We present results from surveys (n = 2,415) and semi-structured\ninterviews (n = 35) revealing viable commonly deployed APS evasion techniques\nand APS vulnerabilities. We conclude that the APS does not work as designed,\neven against very young online game players, and can act as a censorship\nevasion training ground for tomorrow's adults, by familiarization with and\nnormalization of general evasion techniques, and desensitization to their\ndangers. Findings from these studies may further inform developers of\ncensorship-resistant systems about the perceptions and evasion strategies of\ntheir prospective users, and help design tools that leverage services and\nplatforms popular among the censored audience.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.04602,regular,post_llm,2023,8,"{'ai_likelihood': 0.0009822845458984375, 'text': 'NSF RESUME HPC Workshop: High-Performance Computing and Large-Scale Data\n  Management in Service of Epidemiological Modeling\n\n  The NSF-funded Robust Epidemic Surveillance and Modeling (RESUME) project\nsuccessfully convened a workshop entitled ""High-performance computing and\nlarge-scale data management in service of epidemiological modeling"" at the\nUniversity of Chicago on May 1-2, 2023. This was part of a series of workshops\ndesigned to foster sustainable and interdisciplinary co-design for predictive\nintelligence and pandemic prevention. The event brought together 31 experts in\nepidemiological modeling, high-performance computing (HPC), HPC workflows, and\nlarge-scale data management to develop a shared vision for capabilities needed\nfor computational epidemiology to better support pandemic prevention. Through\nthe workshop, participants identified key areas in which HPC capabilities could\nbe used to improve epidemiological modeling, particularly in supporting public\nhealth decision-making, with an emphasis on HPC workflows, data integration,\nand HPC access. The workshop explored nascent HPC workflow and large-scale data\nmanagement approaches currently in use for epidemiological modeling and sought\nto draw from approaches used in other domains to determine which practices\ncould be best adapted for use in epidemiological modeling. This report\ndocuments the key findings and takeaways from the workshop.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.00032,regular,post_llm,2023,8,"{'ai_likelihood': 6.291601392957899e-06, 'text': 'Blockchain Based Open Network in Technology Intermediation\n\n  Blockchain technology is developing using in reliable applications which can\nbe designed to achieve decentralization and trustless. Based on the open\nnetwork innovation theory, this paper proposes a technical intermediary\nmanagement idea based on blockchain technology to improve the efficiency of\ntechnology intermediaries, providing accurate, reliable information and cutting\ncost for the market. This study demonstrates the advantage of blockchain to\ntechnology intermediaries. First, on a specific level, it can provide openness,\ntransparency, decentralization and anonymity services. Second, the current\nindustrial innovation elements are analyzed. blockchain improve the efficiency\nof technology intermediary, prevent risks and to make up for the shortcomings\nof traditional intermediaries. It has revolutionized the traditional technology\nintermediary. As this happens, it can revolutionize traditional technology\nintermediaries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.01163,review,post_llm,2023,8,"{'ai_likelihood': 5.927350785997179e-06, 'text': 'Stakeholder-in-the-Loop Fair Decisions: A Framework to Design Decision\n  Support Systems in Public and Private Organizations\n\n  Due to the opacity of machine learning technology, there is a need for\nexplainability and fairness in the decision support systems used in public or\nprivate organizations. Although the criteria for appropriate explanations and\nfair decisions change depending on the values of those who are affected by the\ndecisions, there is a lack of discussion framework to consider the appropriate\noutputs for each stakeholder. In this paper, we propose a discussion framework\nthat we call ""stakeholder-in-the-loop fair decisions."" This is proposed to\nconsider the requirements for appropriate explanations and fair decisions. We\nidentified four stakeholders that need to be considered to design accountable\ndecision support systems and discussed how to consider the appropriate outputs\nfor each stakeholder by referring to our works. By clarifying the\ncharacteristics of specific stakeholders in each application domain and\nintegrating the stakeholders\' values into outputs that all stakeholders agree\nupon, decision support systems can be designed as systems that ensure\naccountable decision makings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.04454,regular,post_llm,2023,8,"{'ai_likelihood': 0.24943033854166669, 'text': 'Sustainable development-oriented campus bike-sharing site evaluation\n  model: A case study of Henan Polytechnic University\n\n  Promoting sustainable transportation options is increasingly crucial in the\npursuit of environmentally friendly and efficient campus mobility systems.\nAmong these options, bike-sharing programs have garnered substantial attention\nfor their capacity to mitigate traffic congestion, decrease carbon emissions,\nand enhance overall campus sustainability. However, improper selection of\nbike-sharing sites has led to the growing problems of unsustainable practices\nin campus, including the disorderly parking and indiscriminate placement of\nbike-sharing. To this end, this paper proposes a novel sustainable\ndevelopment-oriented campus bike-sharing site evaluation model integrating the\nimproved Delphi and fuzzy comprehensive evaluation approaches. Fourteen\nevaluation metrics are firstly selected from four dimensions: the user\nfeatures, implementation and usage characteristics of parking spots,\nenvironmental sustainability, and social sustainability, through the\ncombination of expert experience and the improved Delphi method. Then, the\nanalytic hierarchy process and the entropy weight method are employed to\ndetermine the weights of the evaluation indices, ensuring a robust and\nobjective assessment framework. The fuzzy comprehensive evaluation method is\nfinally implemented to evaluate the quality of location selection. South Campus\nof Henan Polytechnic University is selected as a case study using the proposed\nevaluation system. This work contributes to the existing body of knowledge by\npresenting a comprehensive location selection evaluation system for campus\nbike-sharing, informed by the principles of sustainable development.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12343,regular,post_llm,2023,8,"{'ai_likelihood': 2.5828679402669272e-06, 'text': ""Integrating Competency-Based Education in Interactive Learning Systems\n\n  Artemis is an interactive learning system that organizes courses, hosts\nlecture content and interactive exercises, conducts exams, and creates\nautomatic assessments with individual feedback. Research shows that students\nhave unique capabilities, previous experiences, and expectations. However, the\ncourse content on current learning systems, including Artemis, is not tailored\nto a student's competencies. The main goal of this paper is to describe how to\nmake Artemis capable of competency-based education and provide individual\ncourse content based on the unique characteristics of every student. We show\nhow instructors can define relations between competencies to create a\ncompetency relation graph, how Artemis measures and visualizes the student's\nprogress toward mastering a competency, and how the progress can generate a\npersonalized learning path for students that recommends relevant learning\nresources. Finally, we present the results of a user study regarding the\nusability of the newly designed competency visualization and give an outlook on\npossible improvements and future visions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.15309,regular,post_llm,2023,8,"{'ai_likelihood': 4.238552517361111e-06, 'text': ""Understanding the Privacy Risks of Popular Search Engine Advertising\n  Systems\n\n  We present the first extensive measurement of the privacy properties of the\nadvertising systems used by privacy-focused search engines. We propose an\nautomated methodology to study the impact of clicking on search ads on three\npopular private search engines which have advertising-based business models:\nStartPage, Qwant, and DuckDuckGo, and we compare them to two dominant\ndata-harvesting ones: Google and Bing. We investigate the possibility of third\nparties tracking users when clicking on ads by analyzing first-party storage,\nredirection domain paths, and requests sent before, when, and after the clicks.\nOur results show that privacy-focused search engines fail to protect users'\nprivacy when clicking ads. Users' requests are sent through redirectors on 4%\nof ad clicks on Bing, 86% of ad clicks on Qwant, and 100% of ad clicks on\nGoogle, DuckDuckGo, and StartPage. Even worse, advertising systems collude with\nadvertisers across all search engines by passing unique IDs to advertisers in\nmost ad clicks. These IDs allow redirectors to aggregate users' activity on\nads' destination websites in addition to the activity they record when users\nare redirected through them. Overall, we observe that both privacy-focused and\ntraditional search engines engage in privacy-harming behaviors allowing\ncross-site tracking, even in privacy-enhanced browsers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.00405,review,post_llm,2023,8,"{'ai_likelihood': 1.7186005910237632e-05, 'text': ""Who benefits from altmetrics? The effect of team gender composition on\n  the link between online visibility and citation impact\n\n  Online science dissemination has quickly become crucial in promoting\nscholars' work. Recent literature has demonstrated a lack of visibility for\nwomen's research, where women's articles receive fewer academic citations than\nmen's. The informetric and scientometric community has briefly examined\ngender-based inequalities in online visibility. However, the link between\nonline sharing of scientific work and citation impact for teams with different\ngender compositions remains understudied. Here we explore whether online\nvisibility is helping women overcome the gender-based citation penalty. Our\nanalyses cover the three broad research areas of Computer Science, Engineering,\nand Social Sciences, which have different gender representation, adoption of\nonline science dissemination practices, and citation culture. We create a\nquasi-experimental setting by applying Coarsened Exact Matching, which enables\nus to isolate the effects of team gender composition and online visibility on\nthe number of citations. We find that online visibility positively affects\ncitations across research areas, while team gender composition interacts\ndifferently with visibility in these research areas. Our results provide\nessential insights into gendered citation patterns and online visibility,\ninviting informed discussions about decreasing the citation gap.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.06943,review,post_llm,2023,8,"{'ai_likelihood': 0.970703125, 'text': 'Current Status and Trends of Engineering Entrepreneurship Education in\n  Australian Universities\n\n  This research sheds light on the present and future landscape of Engineering\nEntrepreneurship Education (EEE) by exploring varied approaches and models\nadopted in Australian universities, evaluating program effectiveness, and\noffering recommendations for curriculum enhancement. While EEE programs have\nbeen in existence for over two decades, their efficacy remains underexplored.\nUsing a multi-method approach encompassing self-reflection, scoping review,\nsurveys, and interviews, this study addresses key research questions regarding\nthe state, challenges, trends, and effectiveness of EEE. Findings reveal\nchallenges like resource limitations and propose solutions such as experiential\nlearning and industry partnerships. These insights underscore the importance of\ntailored EEE and inform teaching strategies and curriculum development,\nbenefiting educators and policymakers worldwide.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.003932952880859375, 'GPT4': 0.64013671875, 'CLAUDE': 0.00278472900390625, 'GOOGLE': 0.306396484375, 'OPENAI_O_SERIES': 0.0037326812744140625, 'DEEPSEEK': 0.018218994140625, 'GROK': 8.195638656616211e-05, 'NOVA': 0.00027632713317871094, 'OTHER': 0.0218353271484375, 'HUMAN': 0.0026454925537109375}}"
2309.12352,review,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': ""An overview of research on human-centered design in the development of\n  artificial general intelligence\n\n  Abstract: This article offers a comprehensive analysis of Artificial General\nIntelligence (AGI) development through a humanistic lens. Utilizing a wide\narray of academic and industry resources, it dissects the technological and\nethical complexities inherent in AGI's evolution. Specifically, the paper\nunderlines the societal and individual implications of AGI and argues for its\nalignment with human values and interests.\n  Purpose: The study aims to explore the role of human-centered design in AGI's\ndevelopment and governance.\n  Design/Methodology/Approach: Employing content analysis and literature\nreview, the research evaluates major themes and concepts in human-centered\ndesign within AGI development. It also scrutinizes relevant academic studies,\ntheories, and best practices.\n  Findings: Human-centered design is imperative for ethical and sustainable\nAGI, emphasizing human dignity, privacy, and autonomy. Incorporating values\nlike empathy, ethics, and social responsibility can significantly influence\nAGI's ethical deployment. Talent development is also critical, warranting\ninterdisciplinary initiatives.\n  Research Limitations/Implications: There is a need for additional empirical\nstudies focusing on ethics, social responsibility, and talent cultivation\nwithin AGI development.\n  Practical Implications: Implementing human-centered values in AGI development\nenables ethical and sustainable utilization, thus promoting human dignity,\nprivacy, and autonomy. Moreover, a concerted effort across industry, academia,\nand research sectors can secure a robust talent pool, essential for AGI's\nstable advancement.\n  Originality/Value: This paper contributes original research to the field by\nhighlighting the necessity of a human-centered approach in AGI development, and\ndiscusses its practical ramifications.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.512901306152344e-05, 'GPT4': 0.99755859375, 'CLAUDE': 0.0, 'GOOGLE': 0.00257110595703125, 'OPENAI_O_SERIES': 2.980232238769531e-07, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 3.039836883544922e-06, 'HUMAN': 0.0}}"
2309.12353,regular,post_llm,2023,8,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'How Beaufort, Neumann and Gates met? Subject integration with\n  spreadsheeting\n\n  Computational thinking should be the fourth fundamental skill, along with\nreading, writing, and arithmetic (3R). To reach the level where computational\nthinking skills, especially digital problem solving have their own schemata,\nthere is a long way to go. In the present paper, a novel approach is detailed\nto support subject integration and building digital schemata, on the well-known\nBeaufort scale. The conversion of a traditional, paper-based problem and a data\nretrieval process are presented within the frame of a Grade 8 action research\nstudy. It is found that both students content knowledge and their digital\nskills developed more efficiently than in traditional course book and\ndecontextualized digital environments. Furthermore, the method presented here\ncan be adapted to any paper-based problems whose solutions would be more\neffective in a digital environment and which offer various forms for building\nschemata both in the subject matter and informatics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.06963,regular,post_llm,2023,8,"{'ai_likelihood': 0.00013828277587890625, 'text': 'The Future of Cybersecurity in Southeast Asia along the Maritime Silk\n  Road\n\n  This paper proposes an analysis of the prospects of the cyber security\nindustry and educational ecosystems in four Southeast Asian countries, namely\nVietnam, Singapore, Malaysia, and Indonesia, which are along the Maritime Silk\nRoad, by using two novel metrics: the ""Cybersecurity Education Prospects Index""\n(CEPI) and the ""Cybersecurity Industry Prospects Index"" (CIPI). The CEPI\nevaluates the state of cybersecurity education by assessing the availability\nand quality of cybersecurity degrees together with their ability to attract new\nstudents. On the other hand, the CIPI measures the potential for the\ncybersecurity industry\'s growth and development by assessing the talent pool\nneeded to build and sustain its growth. Ultimately, this study emphasizes the\nvital importance of a healthy cybersecurity ecosystem where education is\nresponsible for supporting the industry to ensure the security and reliability\nof commercial operations in these countries against a complex and evolving\ncyber threat landscape.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.08673,review,post_llm,2023,8,"{'ai_likelihood': 5.066394805908203e-06, 'text': ""Freedom of Speech and AI Output\n\n  Is the output of generative AI entitled to First Amendment protection? We're\ninclined to say yes. Even though current AI programs are of course not people\nand do not themselves have constitutional rights, their speech may potentially\nbe protected because of the rights of the programs' creators. But beyond that,\nand likely more significantly, AI programs' speech should be protected because\nof the rights of their users-both the users' rights to listen and their rights\nto speak. In this short Article, we sketch the outlines of this analysis.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.01218,review,post_llm,2023,8,"{'ai_likelihood': 2.9802322387695312e-06, 'text': 'The Sequence Matters in Learning -- A Systematic Literature Review\n\n  Describing and analysing learner behaviour using sequential data and analysis\nis becoming more and more popular in Learning Analytics. Nevertheless, we found\na variety of definitions of learning sequences, as well as choices regarding\ndata aggregation and the methods implemented for analysis. Furthermore,\nsequences are used to study different educational settings and serve as a base\nfor various interventions. In this literature review, the authors aim to\ngenerate an overview of these aspects to describe the current state of using\nsequence analysis in educational support and learning analytics. The 74\nincluded articles were selected based on the criteria that they conduct\nempirical research on an educational environment using sequences of learning\nactions as the main focus of their analysis. The results enable us to highlight\ndifferent learning tasks where sequences are analysed, identify data mapping\nstrategies for different types of sequence actions, differentiate techniques\nbased on purpose and scope, and identify educational interventions based on the\noutcomes of sequence analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.07008,regular,post_llm,2023,8,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""Minimizing Polarization in Noisy Leader-Follower Opinion Dynamics\n\n  The operation of creating edges has been widely applied to optimize relevant\nquantities of opinion dynamics. In this paper, we consider a problem of\npolarization optimization for the leader-follower opinion dynamics in a noisy\nsocial network with $n$ nodes and $m$ edges, where a group $Q$ of $q$ nodes are\nleaders, and the remaining $n-q$ nodes are followers. We adopt the popular\nleader-follower DeGroot model, where the opinion of every leader is identical\nand remains unchanged, while the opinion of every follower is subject to white\nnoise. The polarization is defined as the steady-state variance of the\ndeviation of each node's opinion from leaders' opinion, which equals one half\nof the effective resistance $\\mathcal{R}_Q$ between the node group $Q$ and all\nother nodes. Concretely, we propose and study the problem of minimizing\n$\\mathcal{R}_Q$ by adding $k$ new edges with each incident to a node in $Q$. We\nshow that the objective function is monotone and supermodular. We then propose\na simple greedy algorithm with an approximation factor $1-1/e$ that\napproximately solves the problem in $O((n-q)^3)$ time. To speed up the\ncomputation, we also provide a fast algorithm to compute\n$(1-1/e-\\eps)$-approximate effective resistance $\\mathcal{R}_Q$, the running\ntime of which is $\\Otil (mk\\eps^{-2})$ for any $\\eps>0$, where the $\\Otil\n(\\cdot)$ notation suppresses the ${\\rm poly} (\\log n)$ factors. Extensive\nexperiment results show that our second algorithm is both effective and\nefficient.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.16151,review,post_llm,2023,8,"{'ai_likelihood': 5.364418029785156e-06, 'text': 'Automatic assessment of text-based responses in post-secondary\n  education: A systematic review\n\n  Text-based open-ended questions in academic formative and summative\nassessments help students become deep learners and prepare them to understand\nconcepts for a subsequent conceptual assessment. However, grading text-based\nquestions, especially in large courses, is tedious and time-consuming for\ninstructors. Text processing models continue progressing with the rapid\ndevelopment of Artificial Intelligence (AI) tools and Natural Language\nProcessing (NLP) algorithms. Especially after breakthroughs in Large Language\nModels (LLM), there is immense potential to automate rapid assessment and\nfeedback of text-based responses in education. This systematic review adopts a\nscientific and reproducible literature search strategy based on the PRISMA\nprocess using explicit inclusion and exclusion criteria to study text-based\nautomatic assessment systems in post-secondary education, screening 838 papers\nand synthesizing 93 studies. To understand how text-based automatic assessment\nsystems have been developed and applied in education in recent years, three\nresearch questions are considered. All included studies are summarized and\ncategorized according to a proposed comprehensive framework, including the\ninput and output of the system, research motivation, and research outcomes,\naiming to answer the research questions accordingly. Additionally, the typical\nstudies of automated assessment systems, research methods, and application\ndomains in these studies are investigated and summarized. This systematic\nreview provides an overview of recent educational applications of text-based\nassessment systems for understanding the latest AI/NLP developments assisting\nin text-based assessments in higher education. Findings will particularly\nbenefit researchers and educators incorporating LLMs such as ChatGPT into their\neducational activities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12335,review,post_llm,2023,8,"{'ai_likelihood': 0.9755859375, 'text': 'The Impact of Live Polling Quizzes on Student Engagement and Performance\n  in Computer Science Lectures\n\n  Prior to the COVID-19 pandemic, the adoption of live polling and real-time\nfeedback tools gained traction in higher education to enhance student\nengagement and learning outcomes. Integrating live polling activities has been\nshown to boost attention, participation, and understanding of course materials.\nHowever, recent changes in learning behaviours due to the pandemic necessitate\na reevaluation of these active learning technologies. In this context, our\nstudy focuses on the Computer Science (CS) domain, investigating the impact of\nLive Polling Quizzes (LPQs) in undergraduate CS lectures. These quizzes\ncomprise fact-based, formally defined questions with clear answers, aiming to\nenhance engagement, learning outcomes, and overall perceptions of the course\nmodule. A survey was conducted among 70 undergraduate CS students, attending CS\nmodules with and without LPQs. The results revealed that while LPQs contributed\nto higher attendance, other factors likely influenced attendance rates more\nsignificantly. LPQs were generally viewed positively, aiding comprehension and\nmaintaining student attention and motivation. However, careful management of\nLPQ frequency is crucial to prevent overuse for some students and potential\nreduced motivation. Clear instructions for using the polling software were also\nhighlighted as essential.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.896257400512695e-05, 'GPT4': 0.00963592529296875, 'CLAUDE': 0.0001957416534423828, 'GOOGLE': 0.986328125, 'OPENAI_O_SERIES': 0.0026187896728515625, 'DEEPSEEK': 6.282329559326172e-05, 'GROK': 2.384185791015625e-07, 'NOVA': 2.2649765014648438e-06, 'OTHER': 0.00010412931442260742, 'HUMAN': 0.0007681846618652344}}"
2308.08005,review,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': 'Navigating the complex nexus: cybersecurity in political landscapes\n\n  Cybersecurity in politics has emerged as a critical and intricate realm\nintersecting technology, governance, and international relations. In this\ninterconnected digital context, political entities confront unparalleled\nchallenges in securing sensitive data, upholding democratic procedures, and\ncountering cyber threats. This study delves into the multifaceted landscape of\npolitical cybersecurity, examining the evolving landscape of cyberattacks,\ntheir impact on political stability, and strategies for bolstering digital\nresilience. The intricate interplay between state-sponsored hacking,\ndisinformation campaigns, and eroding public trust underscores the imperative\nfor robust cybersecurity measures to safeguard political system integrity.\nThrough an extensive exploration of real-world case studies, policy frameworks,\nand collaborative initiatives, this research illuminates the intricate network\nof technological vulnerabilities, geopolitical dynamics, and ethical concerns\nthat shape the dynamic evolution of cybersecurity in politics. Amidst evolving\ndigital landscapes, the imperative for agile and preemptive cybersecurity\nstrategies is paramount for upholding the stability and credibility of\npolitical institutions.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.28044319152832e-05, 'GPT4': 0.0091094970703125, 'CLAUDE': 0.00689697265625, 'GOOGLE': 0.98046875, 'OPENAI_O_SERIES': 0.0030517578125, 'DEEPSEEK': 4.57763671875e-05, 'GROK': 0.0, 'NOVA': 0.00014853477478027344, 'OTHER': 3.445148468017578e-05, 'HUMAN': 4.76837158203125e-07}}"
2308.1092,review,post_llm,2023,8,"{'ai_likelihood': 0.0, 'text': 'Addressing Knowledge Leakage Risk caused by the use of mobile devices in\n  Australian Organizations\n\n  Information and knowledge leakage has become a significant security risk to\nAustralian organizations. Each security incident in Australian business cost an\naverage US$\\$$2.8 million. Furthermore, Australian organisations spend the\nsecond most worldwide (US$\\$$1.2 million each on average) on investigating and\nassessing information breaches. The leakage of sensitive organizational\ninformation occurs through different avenues, such as social media, cloud\ncomputing and mobile devices. In this study, we (1) analyze the knowledge\nleakage risk (KLR) caused by the use of mobile devices in knowledge-intensive\nAustralian organizations, (2) present a conceptual research model to explain\nthe determinants that influence KLR through the use of mobile devices grounded\nin the literature, (3) conduct interviews with security and knowledge managers\nto understand what strategies they use to mitigate KLR caused by the use of\nmobile devices and (4) use content analysis and the conceptual model to frame\nthe preliminary findings from the interviews. Keywords: Knowledge leakage,\nmobile devices, mobile contexts, knowledge leakage risk\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.14954,review,post_llm,2023,8,"{'ai_likelihood': 0.0008153915405273438, 'text': 'Transitioning ECP Software Technology into a Foundation for Sustainable\n  Research Software\n\n  Research software plays a crucial role in advancing scientific knowledge, but\nensuring its sustainability, maintainability, and long-term viability is an\nongoing challenge. The Sustainable Research Software Institute (SRSI) Model has\nbeen designed to address the concerns, and presents a comprehensive framework\ndesigned to promote sustainable practices in the research software community.\nHowever the SRSI Model does not address the transitional requirements for the\nExascale Computing Project (ECP) Software Technology (ECP-ST) focus area\nspecifically. This white paper provides an overview and detailed description of\nhow ECP-ST will transition into the SRSI in a compressed time frame that a)\nmeets the needs of the ECP end-of-technical-activities deadline; and b) ensures\nthe continuity of the sustainability efforts that are already underway.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12349,review,post_llm,2023,8,"{'ai_likelihood': 6.4240561591254345e-06, 'text': ""On the culture of open access: the Sci-hub paradox\n\n  Shadow libraries, also known as ''pirate libraries'', are online collections\nof copyrighted publications that have been made available for free without the\npermission of the copyright holders. They have gradually become key players of\nscientific knowledge dissemination, despite their illegality in most countries\nof the world. Many publishers and scientist-editors decry such libraries for\ntheir copyright infringement and loss of publication usage information, while\nsome scholars and institutions support them, sometimes in a roundabout way, for\ntheir role in reducing inequalities of access to knowledge, particularly in\nlow-income countries. Although there is a wealth of literature on shadow\nlibraries, none of this have focused on its potential role in knowledge\ndissemination, through the open access movement. Here we analyze how shadow\nlibraries can affect researchers' citation practices, highlighting some\ncounter-intuitive findings about their impact on the Open Access Citation\nAdvantage (OACA). Based on a large randomized sample, this study first shows\nthat OA publications, including those in fully OA journals, receive more\ncitations than their subscription-based counterparts do. However, the OACA has\nslightly decreased over the seven last years. The introduction of a distinction\nbetween those accessible or not via the Scihub platform among\nsubscription-based suggest that the generalization of its use cancels the\npositive effect of OA publishing. The results show that publications in fully\nOA journals are victims of the success of Sci-hub. Thus, paradoxically,\nalthough Sci-hub may seem to facilitate access to scientific knowledge, it\nnegatively affects the OA movement as a whole, by reducing the comparative\nadvantage of OA publications in terms of visibility for researchers. The\ndemocratization of the use of Sci-hub may therefore lead to a vicious cycle,\nhindering efforts to develop full OA strategies without proposing a credible\nand sustainable alternative model for the dissemination of scientific\nknowledge.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.00215,regular,post_llm,2023,8,"{'ai_likelihood': 6.65585199991862e-06, 'text': 'From Talent Shortage to Workforce Excellence in the CHIPS Act Era:\n  Harnessing Industry 4.0 Paradigms for a Sustainable Future in Domestic Chip\n  Production\n\n  The CHIPS Act is driving the U.S. towards a self-sustainable future in\ndomestic chip production. Decades of outsourced manufacturing, assembly,\ntesting, and packaging has diminished the workforce ecosystem, imposing major\nlimitations on semiconductor companies racing to build new fabrication sites as\npart of the CHIPS Act. In response, a systemic alliance between academic\ninstitutions, the industry, government, various consortiums, and organizations\nhas emerged to establish a pipeline to educate and onboard the next generation\nof talent. Establishing a stable and continuous flow of talent requires\nsignificant time investments and comes with no guarantees, particularly\nfactoring in the low workplace desirability in current fabrication houses for\nU.S workforce. This paper will explore the feasibility of two paradigms of\nIndustry 4.0, automation and Augmented Reality(AR)/Virtual Reality(VR), to\ncomplement ongoing workforce development efforts and optimize workplace\ndesirability by catalyzing core manufacturing processes and effectively\nenhancing the education, onboarding, and professional realms-all with promising\ncapabilities amid the ongoing talent shortage and trajectory towards advanced\npackaging.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.06148,regular,post_llm,2023,8,"{'ai_likelihood': 3.940529293484158e-06, 'text': ""Tapping into Privacy: A Study of User Preferences and Concerns on\n  Trigger-Action Platforms\n\n  The Internet of Things (IoT) devices are rapidly increasing in popularity,\nwith more individuals using Internet-connected devices that continuously\nmonitor their activities. This work explores privacy concerns and expectations\nof end-users related to Trigger-Action platforms (TAPs) in the context of the\nInternet of Things (IoT). TAPs allow users to customize their smart\nenvironments by creating rules that trigger actions based on specific events or\nconditions. As personal data flows between different entities, there is a\npotential for privacy concerns. In this study, we aimed to identify the privacy\nfactors that impact users' concerns and preferences for using IoT TAPs. To\naddress this research objective, we conducted three focus groups with 15\nparticipants and we extracted nine themes related to privacy factors using\nthematic analysis. Our participants particularly prefer to have control and\ntransparency over the automation and are concerned about unexpected data\ninferences, risks and unforeseen consequences for themselves and for bystanders\nthat are caused by the automation. The identified privacy factors can help\nresearchers derive predefined and selectable profiles of privacy permission\nsettings for IoT TAPs that represent the privacy preferences of different types\nof users as a basis for designing usable privacy controls for IoT TAPs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.1234,regular,post_llm,2023,8,"{'ai_likelihood': 4.437234666612413e-06, 'text': 'Security for Children in the Digital Society -- A Rights-based and\n  Research Ethics Approach\n\n  In this position paper, we present initial perspectives and research results\nfrom the project ""SIKID - Security for Children in the Digital World."" The\nproject is situated in a German context with a focus on European frameworks for\nthe development of Artificial Intelligence and the protection of children from\nsecurity risks arising in the course of algorithm-mediated online\ncommunication. The project strengthens networks of relevant stakeholders,\nexplores regulatory measures and informs policy makers, and develops a\nchildren\'s rights approach to questions of security for children online while\nalso developing a research ethics approach for conducting research with\nchildren on online harms such as cybergrooming and sexual violence against\nchildren.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.11529,review,post_llm,2023,8,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Redistricting for Proportionality\n\n  American democracy is currently heavily reliant on plurality in single-member\ndistricts, or PSMD, as a system of election. But public perceptions of fairness\nare often keyed to partisan proportionality, or the degree of congruence\nbetween each party\'s share of the the vote and its share of representation.\nPSMD has not tended to secure proportional outcomes historically, partially due\nto gerrymandering, where line-drawers intentionally extract more advantage for\ntheir side. But it is now increasingly clear that even blind PSMD is frequently\ndisproportional, and in unpredictable ways that depend on local political\ngeography. In this paper we consider whether it is feasible to bring PSMD into\nalignment with a proportionality norm by targeting proportional outcomes in the\ndesign and selection of districts. We do this mainly through a close\nexamination of the ""Freedom to Vote Test,"" a redistricting reform proposed in\ndraft legislation in 2021. We find that applying the test with a\nproportionality target makes for sound policy: it performs well in legal\nbattleground states and has a workable exception to handle edge cases where\nproportionality is out of reach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12336,review,post_llm,2023,8,"{'ai_likelihood': 4.006756676567926e-06, 'text': ""The E.U.'s Artificial Intelligence Act: An Ordoliberal Assessment\n\n  In light of the rise of generative AI and recent debates about the\nsocio-political implications of large-language models and chatbots, this\narticle investigates the E.U.'s Artificial Intelligence Act (AIA), the world's\nfirst major attempt by a government body to address and mitigate the\npotentially negative impacts of AI technologies. The article critically\nanalyzes the AIA from a distinct economic ethics perspective, i.e.,\nordoliberalism 2.0 - a perspective currently lacking in the academic\nliterature. It evaluates, in particular, the AIA's ordoliberal strengths and\nweaknesses and proposes reform measures that could be taken to strengthen the\nAIA.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.14914,regular,post_llm,2023,8,"{'ai_likelihood': 0.99267578125, 'text': 'Exploring the combined effects of major fuel technologies, eco-routing,\n  and eco-driving for sustainable traffic decarbonization in downtown Toronto\n\n  As global efforts to combat climate change intensify, transitioning to\nsustainable transportation is crucial. This study explores decarbonization\nstrategies for urban traffic in downtown Toronto through microsimulation,\nevaluating the environmental and economic impacts of vehicle technologies,\ntraffic management strategies (eco-routing), and driving behaviours\n(eco-driving). The study analyzes 140 decarbonization scenarios involving\ndifferent fuel types, Connected and Automated Vehicle (CAV) penetration rates,\nand anticipatory routing strategies. Using transformer-based prediction models,\nwe forecast Greenhouse Gas (GHG) and Nitrogen Oxides (NOx) emissions, along\nwith average speed and travel time. The key findings show that 100% Battery\nElectric Vehicles (BEVs) reduce GHG emissions by 75%, but face challenges\nrelated to cost and infrastructure. Hybrid Electric Vehicles (HEVs) achieve GHG\nreductions of 35-40%, while e-fuels result in modest reductions of 5%.\nIntegrating CAVs with anticipatory routing strategies significantly reduces GHG\nemissions. Additionally, eco-driving practices and eco-routing strategies have\na notable impact on NOx emissions and travel time. By incorporating a\ncomprehensive cost analysis, the study offers valuable insights into the\neconomic feasibility of these strategies. The findings provide practical\nguidance for policymakers and stakeholders in developing effective\ndecarbonization policies and supporting sustainable transportation systems.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0011386871337890625, 'GPT4': 0.0245361328125, 'CLAUDE': 0.003223419189453125, 'GOOGLE': 0.96484375, 'OPENAI_O_SERIES': 0.00379180908203125, 'DEEPSEEK': 0.0001302957534790039, 'GROK': 9.5367431640625e-07, 'NOVA': 2.4974346160888672e-05, 'OTHER': 0.0021152496337890625, 'HUMAN': 0.00017940998077392578}}"
2308.06921,regular,post_llm,2023,8,"{'ai_likelihood': 3.907415601942274e-06, 'text': ""CodeHelp: Using Large Language Models with Guardrails for Scalable\n  Support in Programming Classes\n\n  Computing educators face significant challenges in providing timely support\nto students, especially in large class settings. Large language models (LLMs)\nhave emerged recently and show great promise for providing on-demand help at a\nlarge scale, but there are concerns that students may over-rely on the outputs\nproduced by these models. In this paper, we introduce CodeHelp, a novel\nLLM-powered tool designed with guardrails to provide on-demand assistance to\nprogramming students without directly revealing solutions. We detail the design\nof the tool, which incorporates a number of useful features for instructors,\nand elaborate on the pipeline of prompting strategies we use to ensure\ngenerated outputs are suitable for students. To evaluate CodeHelp, we deployed\nit in a first-year computer and data science course with 52 students and\ncollected student interactions over a 12-week period. We examine students'\nusage patterns and perceptions of the tool, and we report reflections from the\ncourse instructor and a series of recommendations for classroom use. Our\nfindings suggest that CodeHelp is well-received by students who especially\nvalue its availability and help with resolving errors, and that for instructors\nit is easy to deploy and complements, rather than replaces, the support that\nthey provide to students.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.02681,regular,post_llm,2023,8,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'MARTA Reach: Piloting an On-Demand Multimodal Transit System in Atlanta\n\n  This paper reports on the results of the six-month pilot MARTA Reach, which\naimed to demonstrate the potential value of On-Demand Multimodal Transit\nSystems (ODMTS) in the city of Atlanta, Georgia. ODMTS take a transit-centric\nview by integrating on-demand services and traditional fixed routes in order to\naddress the first/last mile problem. ODMTS combine fixed routes and on-demand\nshuttle services by design (not as an after-thought) into a transit system that\noffers a door-to-door multimodal service with fully integrated operations and\nfare structure. The paper fills a knowledge gap, i.e., the understanding of the\nimpact, benefits, and challenges of deploying ODMTS in a city as complex as\nAtlanta, Georgia. The pilot was deployed in four different zones with limited\ntransit options, and used on-demand shuttles integrated with the overall\ntransit system to address the first/last mile problem. The paper describes the\ndesign and operations of the pilot, and presents the results in terms of\nridership, quality of service, trip purposes, alternative modes of\ntransportation, multimodal nature of trips, challenges encountered, and cost\nestimates. The main findings of the pilot are that Reach offered a highly\nvalued service that performed a large number of trips that would have otherwise\nbeen served by ride-hailing companies, taxis, or personal cars. Moreover, the\nwide majority of Reach trips were multimodal, with connections to rail being\nmost prominent.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.14782,regular,post_llm,2023,8,"{'ai_likelihood': 6.4240561591254345e-06, 'text': 'Helping Fact-Checkers Identify Fake News Stories Shared through Images\n  on WhatsApp\n\n  WhatsApp has introduced a novel avenue for smartphone users to engage with\nand disseminate news stories. The convenience of forming interest-based groups\nand seamlessly sharing content has rendered WhatsApp susceptible to the\nexploitation of misinformation campaigns. While the process of fact-checking\nremains a potent tool in identifying fabricated news, its efficacy falters in\nthe face of the unprecedented deluge of information generated on the Internet\ntoday. In this work, we explore automatic ranking-based strategies to propose a\n""fakeness score"" model as a means to help fact-checking agencies identify fake\nnews stories shared through images on WhatsApp. Based on the results, we design\na tool and integrate it into a real system that has been used extensively for\nmonitoring content during the 2018 Brazilian general election. Our experimental\nevaluation shows that this tool can reduce by up to 40% the amount of effort\nrequired to identify 80% of the fake news in the data when compared to current\nmechanisms practiced by the fact-checking agencies for the selection of news\nstories to be checked.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.01586,regular,post_llm,2023,8,"{'ai_likelihood': 2.0000669691297742e-05, 'text': ""Analyzing Bank Account Information of Nominees and Scammers\n\n  Nowadays, people heavily rely on the Internet for various activities, such as\ne-commerce (e.g., online shopping) and online banking. While online\ntransactions are practical, they also provide scammers with a new way to\nexploit unsuspecting individuals. This study and investigation utilized data\nfrom ChaladOhn, a website designed and developed by academics and policemen.\nThe data covered the period from February 2022 to January 2023. After analyzing\nand investigating, the results reveal that the total losses amounted to over\n3,100 million Thai Baht, with each case incurring losses of less than 10\nmillion. Furthermore, the investigation discovered the involvement of the top\ntwo banks in the market, KB*** and BB*, in the fraud. These banks accounted\nfor: 1) 28.2% and 16.0% of the total number of scam accounts, 2) 25.6% and\n20.5% of the total transactions, and 3) 35.7% and 14.9% of the total losses\nfrom the victims as recorded in the database, respectively. Considering the\nanticipated deterioration of this issue, it is crucial to inform regulators and\nrelevant organizations about the investigation's findings. This will enable the\ndevelopment, suggestion, and implementation of an efficient solution to address\nthe rapidly increasing number of online scam cases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.05458,regular,post_llm,2023,8,"{'ai_likelihood': 1.2715657552083334e-05, 'text': 'Inter-Rater Reliability is Individual Fairness\n\n  In this note, a connection between inter-rater reliability and individual\nfairness is established. It is shown that inter-rater reliability is a special\ncase of individual fairness, a notion of fairness requiring that similar people\nare treated similarly.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.02678,review,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': 'Ethical Considerations and Policy Implications for Large Language\n  Models: Guiding Responsible Development and Deployment\n\n  This paper examines the ethical considerations and implications of large\nlanguage models (LLMs) in generating content. It highlights the potential for\nboth positive and negative uses of generative AI programs and explores the\nchallenges in assigning responsibility for their outputs. The discussion\nemphasizes the need for proactive ethical frameworks and policy measures to\nguide the responsible development and deployment of LLMs.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.447509765625, 'GPT4': 0.002201080322265625, 'CLAUDE': 0.0435791015625, 'GOOGLE': 0.491455078125, 'OPENAI_O_SERIES': 0.00036072731018066406, 'DEEPSEEK': 6.318092346191406e-05, 'GROK': 1.6093254089355469e-06, 'NOVA': 8.082389831542969e-05, 'OTHER': 0.01462554931640625, 'HUMAN': 0.00014579296112060547}}"
2308.10244,review,post_llm,2023,8,"{'ai_likelihood': 0.9658203125, 'text': ""Navigating the acceptance of implementing business intelligence in\n  organizations: A system dynamics approach\n\n  The rise of information technology has transformed the business landscape,\nwith organizations increasingly relying on information systems to collect and\nstore vast amounts of data. To stay competitive, businesses must harness this\ndata to make informed decisions that optimize their actions in response to the\nmarket. Business intelligence (BI) is an approach that enables organizations to\nleverage data-driven insights for better decision-making, but implementing BI\ncomes with its own set of challenges. Accordingly, understanding the key\nfactors that contribute to successful implementation is crucial.\n  This study examines the factors affecting the implementation of BI projects\nby analyzing the interactions between these factors using system dynamics\nmodeling. The research draws on interviews with five BI experts and a review of\nthe background literature to identify effective implementation strategies.\nSpecifically, the study compares traditional and self-service implementation\napproaches and simulates their respective impacts on organizational acceptance\nof BI. The results show that the two approaches were equally effective in\ngenerating organizational acceptance until the twenty-fifth month of\nimplementation, after which the self-service strategy generated significantly\nhigher levels of acceptance than the traditional strategy. In fact, after 60\nmonths, the self-service approach was associated with a 30% increase in\norganizational acceptance over the traditional approach. The paper also\nprovides recommendations for increasing the acceptance of BI in both\nimplementation strategies. Overall, this study underscores the importance of\nidentifying and addressing key factors that impact BI implementation success,\noffering practical guidance to organizations seeking to leverage the power of\nBI in today's competitive business environment.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.84375, 'GPT4': 0.10015869140625, 'CLAUDE': 0.00140380859375, 'GOOGLE': 0.049041748046875, 'OPENAI_O_SERIES': 0.0005593299865722656, 'DEEPSEEK': 5.27501106262207e-05, 'GROK': 3.516674041748047e-06, 'NOVA': 1.6808509826660156e-05, 'OTHER': 0.00392913818359375, 'HUMAN': 0.0012807846069335938}}"
2308.0589,review,post_llm,2023,8,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'A Study of the Landscape of Privacy Policies of Smart Devices\n\n  As the adoption of smart devices continues to permeate all aspects of our\nlives, user privacy concerns have become more pertinent than ever. Privacy\npolicies outline the data handling practices of these devices. Prior work in\nthe domains of websites and mobile apps has shown that privacy policies are\nrarely read and understood by users. In these domains, automatic analysis of\nprivacy policies has been shown to help give users appropriate insights.\nHowever, there is a lack of such an analysis in the domain of smart device\nprivacy policies. This paper presents a comprehensive study of the landscape of\nprivacy policies of smart devices. We introduce a methodology that addresses\nthe unique challenges of smart devices, by finding information about them,\ntheir manufacturers, and their privacy policies on the Web. Our methodology\nutilizes state-of-the-art analysis techniques to assess readability and privacy\nof smart device policies and compares it policies of e-commerce websites and\nmobile applications. Overall, we analyzed 4,556 smart devices, 2,211\nmanufacturers, and 819 privacy policies. Despite smart devices having access to\nmore intrusive data about their users (using sensors such as cameras and\nmicrophones), more than 1,167 of the analyzed manufacturers did not have\npolicies available. The study highlights that significant improvement is\nrequired on communicating the data management practices of smart devices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.04449,review,post_llm,2023,8,"{'ai_likelihood': 4.927317301432292e-05, 'text': 'The Disparate Impacts of College Admissions Policies on Asian American\n  Applicants\n\n  There is debate over whether Asian American students are admitted to\nselective colleges and universities at lower rates than white students with\nsimilar academic qualifications. However, there have been few empirical\ninvestigations of this issue, in large part due to a dearth of data. Here we\npresent the results from analyzing 685,709 applications from Asian American and\nwhite students to a subset of selective U.S. institutions over five application\ncycles, beginning with the 2015-2016 cycle. The dataset does not include\nadmissions decisions, and so we construct a proxy based in part on enrollment\nchoices. Based on this proxy, we estimate the odds that Asian American\napplicants were admitted to at least one of the schools we consider were 28%\nlower than the odds for white students with similar test scores, grade-point\naverages, and extracurricular activities. The gap was particularly pronounced\nfor students of South Asian descent (49% lower odds). We trace this pattern in\npart to two factors. First, many selective colleges openly give preference to\nthe children of alumni, and we find that white applicants were substantially\nmore likely to have such legacy status than Asian applicants, especially South\nAsian applicants. Second, after adjusting for observed student characteristics,\nthe institutions we consider appear less likely to admit students from\ngeographic regions with relatively high shares of applicants who are Asian. We\nhope these results inform ongoing discussions on the equity of college\nadmissions policies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.00862,review,post_llm,2023,8,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Confidence-Building Measures for Artificial Intelligence: Workshop\n  Proceedings\n\n  Foundation models could eventually introduce several pathways for undermining\nstate security: accidents, inadvertent escalation, unintentional conflict, the\nproliferation of weapons, and the interference with human diplomacy are just a\nfew on a long list. The Confidence-Building Measures for Artificial\nIntelligence workshop hosted by the Geopolitics Team at OpenAI and the Berkeley\nRisk and Security Lab at the University of California brought together a\nmultistakeholder group to think through the tools and strategies to mitigate\nthe potential risks introduced by foundation models to international security.\nOriginating in the Cold War, confidence-building measures (CBMs) are actions\nthat reduce hostility, prevent conflict escalation, and improve trust between\nparties. The flexibility of CBMs make them a key instrument for navigating the\nrapid changes in the foundation model landscape. Participants identified the\nfollowing CBMs that directly apply to foundation models and which are further\nexplained in this conference proceedings: 1. crisis hotlines 2. incident\nsharing 3. model, transparency, and system cards 4. content provenance and\nwatermarks 5. collaborative red teaming and table-top exercises and 6. dataset\nand evaluation sharing. Because most foundation model developers are\nnon-government entities, many CBMs will need to involve a wider stakeholder\ncommunity. These measures can be implemented either by AI labs or by relevant\ngovernment actors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.08362,review,post_llm,2023,8,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Functional Consistency across Retail Central Bank Digital Currency and\n  Commercial Bank Money\n\n  Central banks are actively exploring central bank digital currencies (CBDCs)\nby conducting research, proofs of concept and pilots. However, adoption of a\nretail CBDC can risk fragmenting both payments markets and retail deposits if\nthe retail CBDC and commercial bank money do not have common operational\ncharacteristics. In this paper we focus on a potential UK retail CBDC - the\n""digital pound"" - and the Bank of England\'s ""platform model"". We first explore\nhow the concept of functional consistency could mitigate the risk of\nfragmentation. We next identify the common operational characteristics that are\nrequired to achieve functional consistency across all forms of regulated retail\ndigital money. We identify four design options based on the provision of these\ncommon operational characteristics by the central bank, payment interface\nproviders, technical service providers or a financial market infrastructure. We\nnext identify architecturally significant use cases and select key capabilities\nthat support these use cases and the common operational characteristics. We\nevaluate the suitability of the design options to provide these key\ncapabilities and draw insights. We conclude that no single design option could\nprovide functional consistency across digital pounds and commercial bank money\nand, instead, a complete solution would need to combine the suitable design\noption(s) for each key capability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.12276,regular,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': 'GAIDE: A Framework for Using Generative AI to Assist in Course Content\n  Development\n\n  This paper introduces ""GAIDE: Generative AI for Instructional Development and\nEducation,"" a novel framework for using Generative AI (GenAI) to enhance\neducational content creation. GAIDE stands out by offering a practical approach\nfor educators to produce diverse, engaging, and academically rigorous\nmaterials. It integrates GenAI into curriculum design, easing the workload of\ninstructors and elevating material quality. With GAIDE, we present a distinct,\nadaptable model that harnesses technological progress in education, marking a\nstep towards more efficient instructional development. Motivated by the demand\nfor innovative educational content and the rise of GenAI use among students,\nthis research tackles the challenge of adapting and integrating technology into\nteaching. GAIDE aims to streamline content development, encourage the creation\nof dynamic materials, and demonstrate GenAI\'s utility in instructional design.\nThe framework is grounded in constructivist learning theory and TPCK,\nemphasizing the importance of integrating technology in a manner that\ncomplements pedagogical goals and content knowledge. Our approach aids\neducators in crafting effective GenAI prompts and guides them through\ninteractions with GenAI tools, both of which are critical for generating\nhigh-quality, contextually appropriate content. Initial evaluations indicate\nGAIDE reduces time and effort in content creation, without compromising on the\nbreadth or depth of the content. Moreover, the use of GenAI has shown promise\nin deterring conventional cheating methods, suggesting a positive impact on\nacademic integrity and student engagement.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.7881393432617188e-06, 'GPT4': 1.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0001952648162841797, 'OPENAI_O_SERIES': 3.814697265625e-06, 'DEEPSEEK': 5.364418029785156e-07, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 5.960464477539063e-08}}"
2308.00645,review,post_llm,2023,8,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Comparability of Automated Vehicle Crash Databases\n\n  Introduction: This paper reviewed current driving automation (DA) and\nbaseline human-driven crash databases and evaluated their comparability.\nMethod: Five sources of DA crash data and three sources of human-driven crash\ndata were reviewed for consistency of inclusion criteria, scope of coverage,\nand potential sources of bias. Alternative methods to determine vehicle\nautomation capability using vehicle identification number (VIN) from\nstate-maintained crash records were also explored. Conclusions: Evaluated data\nsets used incompatible or nonstandard minimum crash severity thresholds,\ncomplicating crash rate comparisons. The most widely-used standard was\n""police-reportable crash,"" which itself has different reporting thresholds\namong jurisdictions. Although low- and no-damage crashes occur at greater\nfrequencies and have more statistical power, they were not consistently\nreported for automated vehicles. Crash data collection can be improved through\ncollection of driving automation exposure data, widespread collection of crash\ndata form electronic data recorders, and standardization of crash definitions.\nPractical Applications: Researchers and DA developers may use this analysis to\nconduct more thorough and accurate evaluations of driving automation crash\nrates. Lawmakers and regulators may use these findings as evidence to enhance\ndata collection efforts, both internally and via new rules regarding electronic\ndata recorders.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.02573,review,post_llm,2023,8,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""A Critical Take on Privacy in a Datafied Society\n\n  Privacy is an increasingly feeble constituent of the present datafied world\nand apparently the reason for that is clear: powerful actors worked to invade\neveryone's privacy for commercial and surveillance purposes. The existence of\nthose actors and their agendas is undeniable, but the explanation is overly\nsimplistic and contributed to create a narrative that tends to preserve the\nstatus quo. In this essay, I analyze several facets of the lack of online\nprivacy and idiosyncrasies exhibited by privacy advocates, together with\ncharacteristics of the industry mostly responsible for the datafication process\nand why its asserted high effectiveness should be openly inquired. Then I\ndiscuss of possible effects of datafication on human behavior, the prevalent\nmarket-oriented assumption at the base of online privacy, and some emerging\nadaptation strategies. In the last part, the regulatory approach to online\nprivacy is considered. The EU's GDPR is praised as the reference case of modern\nprivacy regulations, but the same success hinders critical aspects that also\nemerged, from the quirks of the institutional decision process, to the flaws of\nthe informed consent principle. A glimpse on the likely problematic future is\nprovided with a discussion on privacy related aspects of EU, UK, and China's\nproposed generative AI policies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.12955,regular,post_llm,2023,8,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'A new framework for global data regulation\n\n  Under the current regulatory framework for data protections, the protection\nof human rights writ large and the corresponding outcomes are regulated largely\nindependently from the data and tools that both threaten those rights and are\nneeded to protect them. This separation between tools and the outcomes they\ngenerate risks overregulation of the data and tools themselves when not linked\nto sensitive use cases. In parallel, separation risks under-regulation if the\ndata can be collected and processed under a less-restrictive framework, but\nused to drive an outcome that requires additional sensitivity and restrictions.\nA new approach is needed to support differential protections based on the\ngenuinely high-risk use cases within each sector. Here, we propose a regulatory\nframework designed to apply not to specific data or tools themselves, but to\nthe outcomes and rights that are linked to the use of these data and tools in\ncontext. This framework is designed to recognize, address, and protect a broad\nrange of human rights, including privacy, and suggests a more flexible approach\nto policy making that is aligned with current engineering tools and practices.\nWe test this framework in the context of open banking and describe how current\nprivacy-enhancing technologies and other engineering strategies can be applied\nin this context and that of contract tracing applications. This approach for\ndata protection regulations more effectively builds on existing engineering\ntools and protects the wide range of human rights defined by legislation and\nconstitutions around the globe.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12326,review,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': 'Enhancing interoperability among health information systems in low- and\n  middle- income countries: a review of challenges and strategies\n\n  The review article aims to provide an overview of the challenges and\nstrategies for enhancing interoperability among health information systems in\nlow- and middle- income countries (LMICs). Achieving interoperability in LMICs\npresents unique challenges due to various factors, such as limited resources,\nfragmented health information systems, and diverse health IT infrastructure.\nThe methodology involves conducting a comprehensive literature review,\nsynthesising findings, identifying challenges and strategies, analysing and\ninterpreting results, and writing and finalising the article. The article\nhighlights that the interoperability challenges include a lack of\nstandardisation, fragmented systems, limited resources, and data privacy\nconcerns. The article proposes strategies to enhance interoperability in LMICs,\nsuch as standardisation of data formats and protocols, consolidation of health\ninformation systems, investment in health IT infrastructure, and capacity\nbuilding of health IT professionals in LMICs. The article aims to provide\ninsights into the current state and potential strategies for enhancing\ninteroperability among health information systems in LMICs, intending to\nimprove healthcare delivery and outcomes in these KEYWORDS Interoperability,\nHealth information systems, low and middle-income countries (LMICs),\nchallenges, strategies, standardisation\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.372802734375, 'GPT4': 0.0055999755859375, 'CLAUDE': 0.0009493827819824219, 'GOOGLE': 0.5703125, 'OPENAI_O_SERIES': 0.00013709068298339844, 'DEEPSEEK': 0.00020456314086914062, 'GROK': 2.7418136596679688e-06, 'NOVA': 0.0011548995971679688, 'OTHER': 0.0487060546875, 'HUMAN': 2.562999725341797e-06}}"
2308.12876,regular,post_llm,2023,8,"{'ai_likelihood': 1.2351406945122613e-05, 'text': 'The Impact of De-Identification on Single-Year-of-Age Counts in the U.S.\n  Census\n\n  In 2020, the U.S. Census Bureau transitioned from data swapping to\ndifferential privacy (DP) in its approach to de-identifying decennial census\ndata. This decision has faced considerable criticism from data users,\nparticularly due to concerns about the accuracy of DP. We compare the relative\nimpacts of swapping and DP on census data, focusing on the use case of school\nplanning, where single-year-of-age population counts (i.e., the number of\nfour-year-olds in the district) are used to estimate the number of incoming\nstudents and make resulting decisions surrounding faculty, classrooms, and\nfunding requests. We examine these impacts for school districts of varying\npopulation sizes and age distributions.\n  Our findings support the use of DP over swapping for single-year-of-age\ncounts; in particular, concerning behaviors associated with DP (namely, poor\nbehavior for smaller districts) occur with swapping mechanisms as well. For the\nschool planning use cases we investigate, DP provides comparable, if not\nimproved, accuracy over swapping, while offering other benefits such as\nimproved transparency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.15979,regular,post_llm,2023,8,"{'ai_likelihood': 8.56320063273112e-05, 'text': 'Fine-Grained Socioeconomic Prediction from Satellite Images with\n  Distributional Adjustment\n\n  While measuring socioeconomic indicators is critical for local governments to\nmake informed policy decisions, such measurements are often unavailable at\nfine-grained levels like municipality. This study employs deep learning-based\npredictions from satellite images to close the gap. We propose a method that\nassigns a socioeconomic score to each satellite image by capturing the\ndistributional behavior observed in larger areas based on the ground truth. We\ntrain an ordinal regression scoring model and adjust the scores to follow the\ncommon power law within and across regions. Evaluation based on official\nstatistics in South Korea shows that our method outperforms previous models in\npredicting population and employment size at both the municipality and grid\nlevels. Our method also demonstrates robust performance in districts with\nuneven development, suggesting its potential use in developing countries where\nreliable, fine-grained data is scarce.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.15668,regular,post_llm,2023,8,"{'ai_likelihood': 9.5367431640625e-06, 'text': 'Intersectional Inquiry, on the Ground and in the Algorithm\n\n  This article makes two key contributions to methodological debates in\nautomation research. First, we argue for and demonstrate how methods in this\nfield must account for intersections of social difference, such as race, class,\nethnicity, culture, and disability, in more nuanced ways. Second, we consider\nthe complexities of bringing together computational and qualitative methods in\nan intersectional methodological approach while also arguing that in their\nrespective subjects (machines and human subjects) and conceptual scope they\nenable a specific dialogue on intersectionality and automation to be\narticulated. We draw on field reflections from a project that combines an\nanalysis of intersectional bias in language models with findings from a\ncommunity workshop on the frustrations and aspirations produced through\nengagement with everyday AI-driven technologies in the context of care.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.11541,review,post_llm,2023,8,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Refugee status determination: how cooperation with machine learning\n  tools can lead to more justice\n\n  Previous research on refugee status adjudications has shown that prediction\nof the outcome of an application can be derived from very few features with\nsatisfactory accuracy. Recent research work has achieved between 70 and 90%\naccuracy using text analytics on various legal fields among which refugee\nstatus determination. Some studies report predictions derived from the judge\nidentity only. Additionally most features used for prediction are\nnon-substantive and external features ranging from news reports, date and time\nof the hearing or weather. On the other hand, literature shows that noise is\nubiquitous in human judgments and significantly affects the outcome of\ndecisions. It has been demonstrated that noise is a significant factor\nimpacting legal decisions. We use the term ""noise"" in the sense described by D.\nKahneman, as a measure of how human beings are unavoidably influenced by\nexternal factors when making a decision. In the context of refugee status\ndetermination, it means for instance that two judges would take different\ndecisions when presented with the same application. This article explores ways\nthat machine learning can help reduce noise in refugee law decision making. We\nare not suggesting that this proposed methodology should be exclusive from\nother approaches to improve decisions such as training of decision makers,\nskills acquisition or judgment aggregation, but rather that it is a path worth\nexploring. We investigate how artificial intelligence and specifically\ndata-driven applications can be used to benefit all parties involved in refugee\nstatus adjudications. We specifically look at decisions taken in Canada and in\nthe United States. Our research aims at reducing arbitrariness and unfairness\nthat derive from noisy decisions, based on the assumption that if two cases or\napplications are alike they should be treated in the same way and induce the\nsame outcome.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.16707,regular,post_llm,2023,8,"{'ai_likelihood': 0.5214843749999999, 'text': ""Causal Analysis of First-Year Course Approval Delays in an Engineering\n  Major Through Inference Techniques\n\n  The study addresses the problem of delays in the approval of first-year\ncourses in the Civil Engineering Major at the National University of Tucum\\'an,\nArgentina. Students take an average of 5 years to pass these subjects. Using\nthe DoWhy and Causal Discovery Toolbox tools, we looked to identify the\nunderlying causes of these delays. The analysis revealed that the regulatory\nstructure of the program and the evaluation methods play a crucial role in this\ndelay. Specifically, the accumulation of regular subjects without passing a\nfinal exam was identified as a key factor. These findings can guide\ninterventions to improve student success rates and the effectiveness of the\neducation system in general.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.254150390625, 'GPT4': 0.0085601806640625, 'CLAUDE': 0.0305633544921875, 'GOOGLE': 0.56884765625, 'OPENAI_O_SERIES': 0.001308441162109375, 'DEEPSEEK': 0.00043487548828125, 'GROK': 0.00011610984802246094, 'NOVA': 0.0012826919555664062, 'OTHER': 0.059295654296875, 'HUMAN': 0.0751953125}}"
2308.03118,review,post_llm,2023,8,"{'ai_likelihood': 0.22867838541666669, 'text': ""Level of Awareness of PSU Bayambang Campus Students towards E learning\n  Technologies\n\n  The study assesses the awareness of PSU Bayambang Campus students regarding\ne-learning technologies. A Quantitative Research Approach was used, gathering\ndata through a demographic questionnaire and ICT Resources assessment. The\nsurvey measured students' familiarity and knowledge of existing e-learning\ntechnologies. Around 52.50% of respondents were familiar with e learning\nconcepts, but their exposure and utilization levels need consideration.\nTechnology, Support, and Users were identified as key factors influencing\nstudent awareness. Implementation can be improved through policies and resource\nprovision. The researchers recommend integrating e learning policies, providing\nICT Resources and Infrastructure, and offering training for students and\nteachers. This research serves as a guide for policy design, enhancing the\nUniversity's learning process and facilitating better learning and interaction.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.00154,review,post_llm,2023,8,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Learning From Peers: A Survey of Perception and Utilization of Online\n  Peer Support Among Informal Dementia Caregivers\n\n  Informal dementia caregivers are those who care for a person living with\ndementia (PLWD) without receiving payment (e.g., family members, friends, or\nother unpaid caregivers). These informal caregivers are subject to substantial\nmental, physical, and financial burdens. Online communities enable these\ncaregivers to exchange caregiving strategies and communicate experiences with\nother caregivers whom they generally do not know in real life. Research has\ndemonstrated the benefits of peer support in online communities, but they are\nlimited in focusing merely on caregivers who are already online users. In this\npaper, we designed and administered a survey to investigate the perception and\nutilization of online peer support from 140 informal dementia caregivers (with\n100 online-community caregivers). Our findings show that the behavior to access\nany online community is only significantly associated with their belief in the\nvalue of online peer support (p = 0.006). Moreover, 33 (83%) of the 40\nnon-online-community caregivers had a belief score above 24, a score assigned\nwhen a neutral option is selected for each belief question. The reasons most\narticulated for not accessing any online community were no time to do so (14;\n10%), and insufficient online information searching skills (9; 6%). Our\nfindings suggest that online peer support is valuable, but practical strategies\nare needed to assist informal dementia caregivers who have limited time or\nsearching skills.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12344,review,post_llm,2023,8,"{'ai_likelihood': 0.9853515625, 'text': 'Exploring IoT in Smart Cities: Practices, Challenges and Way Forward\n\n  The rise of Internet of things (IoT) technology has revolutionized urban\nliving, offering immense potential for smart cities in which smart home, smart\ninfrastructure, and smart industry are essential aspects that contribute to the\ndevelopment of intelligent urban ecosystems. The integration of smart home\ntechnology raises concerns regarding data privacy and security, while smart\ninfrastructure implementation demands robust networking and interoperability\nsolutions. Simultaneously, deploying IoT in industrial settings faces\nchallenges related to scalability, standardization, and data management. This\nresearch paper offers a systematic literature review of published research in\nthe field of IoT in smart cities including 55 relevant primary studies that\nhave been published in reputable journals and conferences. This extensive\nliterature review explores and evaluates various aspects of smart home, smart\ninfrastructure, and smart industry and the challenges like security and\nprivacy, smart sensors, interoperability and standardization. We provide a\nunified perspective, as we seek to enhance the efficiency and effectiveness of\nsmart cities while overcoming security concerns. It then explores their\npotential for collective integration and impact on the development of smart\ncities. Furthermore, this study addresses the challenges associated with each\ncomponent individually and explores their combined impact on enhancing urban\nefficiency and sustainability. Through a comprehensive analysis of security\nconcerns, this research successfully integrates these IoT components in a\nunified approach, presenting a holistic framework for building smart cities of\nthe future. Integrating smart home, smart infrastructure, and smart industry,\nthis research highlights the significance of an integrated approach in\ndeveloping smart cities.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.006072998046875, 'GPT4': 0.3525390625, 'CLAUDE': 0.00019657611846923828, 'GOOGLE': 0.63232421875, 'OPENAI_O_SERIES': 0.002307891845703125, 'DEEPSEEK': 8.046627044677734e-06, 'GROK': 9.5367431640625e-07, 'NOVA': 3.993511199951172e-06, 'OTHER': 0.0003190040588378906, 'HUMAN': 0.006282806396484375}}"
2308.04933,regular,post_llm,2023,8,"{'ai_likelihood': 2.615981631808811e-06, 'text': ""You Are How You Walk: Quantifying Privacy Risks in Step Count Data\n\n  Wearable devices have gained huge popularity in today's world. These devices\ncollect large-scale health data from their users, such as heart rate and step\ncount data, that is privacy sensitive, however it has not yet received the\nnecessary attention in the academia. In this paper, we perform the first\nsystematic study on quantifying privacy risks stemming from step count data. In\nparticular, we propose two attacks including attribute inference for gender,\nage and education and temporal linkability. We demonstrate the severity of the\nprivacy attacks by performing extensive evaluation on a real life dataset and\nderive key insights. We believe our results can serve as a step stone for\nderiving a privacy-preserving ecosystem for wearable devices in the future.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.14953,review,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': 'An Open Community-Driven Model For Sustainable Research Software:\n  Sustainable Research Software Institute\n\n  Research software plays a crucial role in advancing scientific knowledge, but\nensuring its sustainability, maintainability, and long-term viability is an\nongoing challenge. To address these concerns, the Sustainable Research Software\nInstitute (SRSI) Model presents a comprehensive framework designed to promote\nsustainable practices in the research software community. This white paper\nprovides an in-depth overview of the SRSI Model, outlining its objectives,\nservices, funding mechanisms, collaborations, and the significant potential\nimpact it could have on the research software community. It explores the wide\nrange of services offered, diverse funding sources, extensive collaboration\nopportunities, and the transformative influence of the SRSI Model on the\nresearch software landscape\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0164794921875, 'GPT4': 0.08807373046875, 'CLAUDE': 0.0142822265625, 'GOOGLE': 0.66015625, 'OPENAI_O_SERIES': 0.21337890625, 'DEEPSEEK': 0.003719329833984375, 'GROK': 0.00011348724365234375, 'NOVA': 0.0024166107177734375, 'OTHER': 0.0010776519775390625, 'HUMAN': 0.0004978179931640625}}"
2308.08028,regular,post_llm,2023,8,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""A Graph Analysis of the Impact of COVID-19 on Emergency Housing Shelter\n  Access Patterns\n\n  This paper investigates how COVID-19 disrupted emergency housing shelter\naccess patterns in Calgary, Canada and what aspects of these changes persist to\nthe present day. This analysis will utilize aggregated shelter access data for\nover 40,000 individuals from seven major urban shelters dating from 2018 to the\npresent. A graph theoretic approach will be used to examine the journeys of\nindividuals between shelters before, during and after the COVID-19 lockdown\nperiod. This approach treats shelters as nodes in a graph and a person's\ntransition between shelter as an arrow or edge between nodes. This perspective\nis used to create both timeline and network diagrams that visualize shelter use\nand the flow of people between shelters. Statistical results are also presented\nthat illustrate the differences between the cohorts of people who only used\nshelter pre/post-lockdown, people who stayed in shelter during lockdown and\npeople who used shelter for the first time during lockdown. The results\ndemonstrate not only how a complex system of care responded to the pandemic but\nalso the characteristics of the people most likely to continue to rely on that\nsystem during an emergency.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.07938,regular,post_llm,2023,8,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Computer Aided Design and Grading for an Electronic Functional\n  Programming Exam\n\n  Electronic exams (e-exams) have the potential to substantially reduce the\neffort required for conducting an exam through automation. Yet, care must be\ntaken to sacrifice neither task complexity nor constructive alignment nor\ngrading fairness in favor of automation. To advance automation in the design\nand fair grading of (functional programming) e-exams, we introduce the\nfollowing: A novel algorithm to check Proof Puzzles based on finding correct\nsequences of proof lines that improves fairness compared to an existing, edit\ndistance based algorithm; an open-source static analysis tool to check source\ncode for task relevant features by traversing the abstract syntax tree; a\nhigher-level language and open-source tool to specify regular expressions that\nmakes creating complex regular expressions less error-prone. Our findings are\nembedded in a complete experience report on transforming a paper exam to an\ne-exam. We evaluated the resulting e-exam by analyzing the degree of automation\nin the grading process, asking students for their opinion, and critically\nreviewing our own experiences. Almost all tasks can be graded automatically at\nleast in part (correct solutions can almost always be detected as such), the\nstudents agree that an e-exam is a fitting examination format for the course\nbut are split on how well they can express their thoughts compared to a paper\nexam, and examiners enjoy a more time-efficient grading process while the point\ndistribution in the exam results was almost exactly the same compared to a\npaper exam.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12348,review,post_llm,2023,8,"{'ai_likelihood': 4.404120975070529e-06, 'text': 'ChatGPT impacts in programming education: A recent literature overview\n  that debates ChatGPT responses\n\n  This paper aims at a brief overview of the main impact of ChatGTP in the\nscientific field of programming and learning/education in computer science. It\nlists, covers and documents from the literature the major issues that have been\nidentified for this topic, such as applications, advantages and limitations,\nethical issues raised. Answers to the above questions were solicited from\nChatGPT itself, the responses were collected, and then the recent literature\nwas surveyed to determine whether or not the responses are supported. The paper\nends with a short discussion on what is expected to happen in the near future.\nA future that can be extremely promising if humanity manages to have AI as a\nproper ally and partner, with distinct roles and specific rules of cooperation\nand interaction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.12258,review,post_llm,2023,8,"{'ai_likelihood': 0.04950629340277778, 'text': 'Innovating Computer Programming Pedagogy: The AI-Lab Framework for\n  Generative AI Adoption\n\n  Over the last year, the ascent of Generative AI (GenAI) has raised concerns\nabout its impact on core skill development, such as problem-solving and\nalgorithmic thinking, in Computer Science students. Preliminary anonymous\nsurveys show that at least 48.5% of our students use GenAI for homework. With\nthe proliferation of these tools, the academic community must contemplate the\nappropriate role of these tools in education. Neglecting this might culminate\nin a phenomenon we term the ""Junior-Year Wall,"" where students struggle in\nadvanced courses due to prior over-dependence on GenAI. Instead of discouraging\nGenAI use, which may unintentionally foster covert usage, our research seeks to\nanswer: ""How can educators guide students\' interactions with GenAI to preserve\ncore skill development during their foundational academic years?""\n  We introduce ""AI-Lab,"" a pedagogical framework for guiding students in\neffectively leveraging GenAI within core collegiate programming courses. This\nframework accentuates GenAI\'s benefits and potential as a pedagogical\ninstrument. By identifying and rectifying GenAI\'s errors, students enrich their\nlearning process. Moreover, AI-Lab presents opportunities to use GenAI for\ntailored support such as topic introductions, detailed examples, corner case\nidentification, rephrased explanations, and debugging assistance. Importantly,\nthe framework highlights the risks of GenAI over-dependence, aiming to\nintrinsically motivate students towards balanced usage. This approach is\npremised on the idea that mere warnings of GenAI\'s potential failures may be\nmisconstrued as instructional shortcomings rather than genuine tool\nlimitations.\n  Additionally, AI-Lab offers strategies for formulating prompts to elicit\nhigh-quality GenAI responses. For educators, AI-Lab provides mechanisms to\nexplore students\' perceptions of GenAI\'s role in their learning experience.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.04819,review,post_llm,2023,8,"{'ai_likelihood': 9.040037790934246e-06, 'text': '""This (Smart) Town Ain\'t Big Enough"": Smart Small Towns and Digital\n  Twins for Sustainable Urban and Regional Development\n\n  One of the major challenges today lies in the creation of governance concepts\nfor regional development that not only promote growth but, at the same time,\nensure promotion of inclusiveness, fairness, and resilience. Digital twins can\nsupport policymakers in developing smart, sustainable solutions for cities and\nregions and, therefore, urban and non-urban environments. The project SCiNDTiLA\n(Smart Cities aNd Digital Twins in Lower Austria) aims to define the\nstate-of-the-art in the field of smart cities, identify interdependencies,\ncritical components and stakeholders, and provide a roadmap for smart cities\nwith application to both smaller-scale urban and non-urban environments.\nSCiNDTiLA uses the foundations of complexity theory and computational social\nscience methods to model Austrian towns and regions as smart cities/regions and\nthus as systems of socio-technical interaction to guide policy decision-making\ntoward sustainable development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2308.14576,regular,post_llm,2023,8,"{'ai_likelihood': 1.0, 'text': ""Turnkey Technology: A Powerful Tool for Cyber Warfare\n\n  Turnkey technology has emerged as a game-changing tool in cyber warfare,\noffering state and non-state actors unprecedented access to advanced cyber\ncapabilities. The advantages of turnkey technology include rapid deployment and\nadaptability, lower costs and resource requirements, the democratization of\ncyber warfare capabilities, and enhanced offensive and defensive strategies.\nHowever, turnkey technology also introduces significant risks, such as the\nproliferation of cyber weapons, ethical considerations, potential collateral\ndamage, escalation of conflicts, and legal ramifications. This paper provides a\nunique perspective on the implications of turnkey technology in cyber warfare,\nhighlighting its advantages, risks, and challenges, as well as the potential\nstrategies for mitigating these concerns. The research's novelty lies in\nexamining real-world examples and proposing a multifaceted approach to address\nthe challenges associated with turnkey technology in cyber warfare. This\napproach focuses on developing effective cybersecurity measures, establishing\ninternational norms and regulations, promoting responsible use and development\nof turnkey technology, and enhancing global cooperation on cyber warfare\nissues. By adopting this accountable and balanced approach, governments,\norganizations, and the international community can work together to create a\nmore secure and stable digital environment, leveraging the benefits of turnkey\ntechnology while minimizing the associated risks and challenges.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.04925537109375, 'GPT4': 0.0285797119140625, 'CLAUDE': 0.2017822265625, 'GOOGLE': 0.380126953125, 'OPENAI_O_SERIES': 0.0001455545425415039, 'DEEPSEEK': 1.9371509552001953e-05, 'GROK': 3.4570693969726562e-06, 'NOVA': 0.00011789798736572266, 'OTHER': 0.340087890625, 'HUMAN': 5.793571472167969e-05}}"
2309.06927,regular,post_llm,2023,9,"{'ai_likelihood': 3.841188218858507e-06, 'text': ""OMOD: An open-source tool for creating disaggregated mobility demand\n  based on OpenStreetMap\n\n  In this paper, we introduce the OpenStreetMap Mobility Demand Generator\n(OMOD), a new open-source activity-based mobility demand generation tool. OMOD\ncreates a population of agents and detailed daily activity schedules that state\nwhat activities each agent plans to conduct, where, and for how long. The\ntemporal aspect of the output is wholly disaggregated, while the spatial aspect\nis given on the level of individual buildings. In contrast to other existing\nmodels, OMOD is freely available, open-source, works out-of-the-box, can be\napplied to any region on earth, and only requires freely available\nOpenStreetMap (OSM) data from the user. With OMOD, it is easy for non-experts\nto create realistic mobility demand, which can be used in transportation\nstudies, energy system modeling, communications system research, et cetera.\nOMOD uses a data-driven approach to generate mobility demand that has been\ncalibrated with household travel survey data. This paper describes OMOD's\narchitecture and validates the model for three cities ranging from 200,000 to\n2.5 million inhabitants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.00374,regular,post_llm,2023,9,"{'ai_likelihood': 3.1458006964789497e-06, 'text': 'Coordinated pausing: An evaluation-based coordination scheme for\n  frontier AI developers\n\n  As artificial intelligence (AI) models are scaled up, new capabilities can\nemerge unintentionally and unpredictably, some of which might be dangerous. In\nresponse, dangerous capabilities evaluations have emerged as a new risk\nassessment tool. But what should frontier AI developers do if sufficiently\ndangerous capabilities are in fact discovered? This paper focuses on one\npossible response: coordinated pausing. It proposes an evaluation-based\ncoordination scheme that consists of five main steps: (1) Frontier AI models\nare evaluated for dangerous capabilities. (2) Whenever, and each time, a model\nfails a set of evaluations, the developer pauses certain research and\ndevelopment activities. (3) Other developers are notified whenever a model with\ndangerous capabilities has been discovered. They also pause related research\nand development activities. (4) The discovered capabilities are analyzed and\nadequate safety precautions are put in place. (5) Developers only resume their\npaused activities if certain safety thresholds are reached. The paper also\ndiscusses four concrete versions of that scheme. In the first version, pausing\nis completely voluntary and relies on public pressure on developers. In the\nsecond version, participating developers collectively agree to pause under\ncertain conditions. In the third version, a single auditor evaluates models of\nmultiple developers who agree to pause if any model fails a set of evaluations.\nIn the fourth version, developers are legally required to run evaluations and\npause if dangerous capabilities are discovered. Finally, the paper discusses\nthe desirability and feasibility of our proposed coordination scheme. It\nconcludes that coordinated pausing is a promising mechanism for tackling\nemerging risks from frontier AI models. However, a number of practical and\nlegal obstacles need to be overcome, especially how to avoid violations of\nantitrust law.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.08266,regular,post_llm,2023,9,"{'ai_likelihood': 1.546409395005968e-05, 'text': 'Enterprise Architecture as an Enabler for a Government Business\n  Ecosystem: Experiences from Finland\n\n  Public sector procurement units in the field of ICT suffer from siloed,\napplication-specific architectures, where each system operates in isolation\nfrom others. As a consequence, similar or even identical data is maintained in\nseveral different databases hosted by different organizations. Such problems\nare caused by the lack of standard guidelines and practices that would result\nin interoperable systems instead of overlapping ones. In the Finnish public\nsector, enterprise architecture (EA) is a mandatory requirement so that an\necosystem can be formed to overcome the above problems. However, the adoption\nrates are low, and the focus is often on technology rather than processes and\npractices. This study investigates the use of EA and its potential in Finnish\nprocurement units through semi-structured interviews. Five procurement units\nand four vendors participated in the study, and altogether 12 interviews took\nplace.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.07055,regular,post_llm,2023,9,"{'ai_likelihood': 1.0, 'text': 'Unraveling the Geography of Infection Spread: Harnessing Super-Agents\n  for Predictive Modeling\n\n  Our study presents an intermediate-level modeling approach that bridges the\ngap between complex Agent-Based Models (ABMs) and traditional compartmental\nmodels for infectious diseases. We introduce ""super-agents"" to simulate\ninfection spread in cities, reducing computational complexity while retaining\nindividual-level interactions. This approach leverages real-world mobility data\nand strategic geospatial tessellations for efficiency. Voronoi Diagram\ntessellations, based on specific street network locations, outperform standard\nCensus Block Group tessellations, and a hybrid approach balances accuracy and\nefficiency. Benchmarking against existing ABMs highlights key optimizations.\nThis research improves disease modeling in urban areas, aiding public health\nstrategies in scenarios requiring geographic specificity and high computational\nefficiency.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00021219253540039062, 'GPT4': 0.8076171875, 'CLAUDE': 0.025146484375, 'GOOGLE': 0.0225372314453125, 'OPENAI_O_SERIES': 0.0034084320068359375, 'DEEPSEEK': 0.1387939453125, 'GROK': 5.936622619628906e-05, 'NOVA': 5.5789947509765625e-05, 'OTHER': 0.0020904541015625, 'HUMAN': 7.927417755126953e-05}}"
2309.01964,review,post_llm,2023,9,"{'ai_likelihood': 1.1622905731201172e-05, 'text': ""Gender Inequalities: Women Researchers Require More Knowledge in\n  Specific and Experimental Topics\n\n  Gender inequalities in science have long been observed globally. Studies have\ndemonstrated it through survey data or published literature, focusing on the\ninterests of subjects or authors; few, however, examined the manifestation of\ngender inequalities on researchers' knowledge status. This study analyzes the\nrelationship between regional and gender identities, topics, and knowledge\nstatus while revealing the female labor division in science and scientific\nresearch using online Q&A from researchers. We find that gender inequalities\nare merged with both regional-specific characteristics and global common\npatterns. Women's field and topic distribution within fields are influenced by\nregions, yet the prevalent topics are consistent in all regions. Women are more\ninvolved in specific topics, particularly topics about experiments with weaker\nlevels of knowledge and they are of less assistance. To promote inequality in\nscience, the scientific community should pay more attention to reducing the\nknowledge gap and encourage women to work on unexplored topics and areas.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.11625,review,post_llm,2023,9,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Legitimate Interest is the New Consent -- Large-Scale Measurement and\n  Legal Compliance of IAB Europe TCF Paywalls\n\n  Cookie paywalls allow visitors of a website to access its content only after\nthey make a choice between paying a fee or accept tracking. European Data\nProtection Authorities (DPAs) recently issued guidelines and decisions on\npaywalls lawfulness, but it is yet unknown whether websites comply with them.\nWe study in this paper the prevalence of cookie paywalls on the top one million\nwebsites using an automatic crawler. We identify 431 cookie paywalls, all using\nthe Transparency and Consent Framework (TCF). We then analyse the data these\npaywalls communicate through the TCF, and in particular, the legal grounds and\nthe purposes used to collect personal data. We observe that cookie paywalls\nextensively rely on legitimate interest legal basis systematically conflated\nwith consent. We also observe a lack of correlation between the presence of\npaywalls and legal decisions or guidelines by DPAs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.01919,regular,post_llm,2023,9,"{'ai_likelihood': 0.06079779730902778, 'text': ""Towards Understanding of Deepfake Videos in the Wild\n\n  Deepfakes have become a growing concern in recent years, prompting\nresearchers to develop benchmark datasets and detection algorithms to tackle\nthe issue. However, existing datasets suffer from significant drawbacks that\nhamper their effectiveness. Notably, these datasets fail to encompass the\nlatest deepfake videos produced by state-of-the-art methods that are being\nshared across various platforms. This limitation impedes the ability to keep\npace with the rapid evolution of generative AI techniques employed in\nreal-world deepfake production. Our contributions in this IRB-approved study\nare to bridge this knowledge gap from current real-world deepfakes by providing\nin-depth analysis. We first present the largest and most diverse and recent\ndeepfake dataset (RWDF-23) collected from the wild to date, consisting of 2,000\ndeepfake videos collected from 4 platforms targeting 4 different languages span\ncreated from 21 countries: Reddit, YouTube, TikTok, and Bilibili. By expanding\nthe dataset's scope beyond the previous research, we capture a broader range of\nreal-world deepfake content, reflecting the ever-evolving landscape of online\nplatforms. Also, we conduct a comprehensive analysis encompassing various\naspects of deepfakes, including creators, manipulation strategies, purposes,\nand real-world content production methods. This allows us to gain valuable\ninsights into the nuances and characteristics of deepfakes in different\ncontexts. Lastly, in addition to the video content, we also collect viewer\ncomments and interactions, enabling us to explore the engagements of internet\nusers with deepfake content. By considering this rich contextual information,\nwe aim to provide a holistic understanding of the {evolving} deepfake\nphenomenon and its impact on online platforms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12885,regular,post_llm,2023,9,"{'ai_likelihood': 1.1854701571994357e-05, 'text': 'Do Digital Jobs Need an Image Filter? Factors Contributing to Negative\n  Attitudes\n\n  The rapid expansion of high-speed internet has led to the emergence of new\ndigital jobs, such as digital influencers, fitness models, and adult models who\nshare content on subscription-based social media platforms. Across two\nexperiments involving 1,002 participants, we combined theories from both social\npsychology and information systems to investigate perceptions of digital jobs\ncompared to matched established jobs, and predictors of attitudes toward\ndigital jobs (e.g., symbolic threat, contact, perceived usefulness). We found\nthat individuals in digital professions were perceived as less favorably and as\nless hard-working than those in matched established jobs. Digital jobs were\nalso regarded as more threatening to societal values and less useful. The\nrelation between job type and attitudes toward these jobs was partially\nmediated by contact with people working in these jobs, perceived usefulness,\nperception of hard-working, and symbolic threat. These effects were consistent\nacross openness to new experiences, attitudes toward digitalization, political\norientation, and age. Among the nine jobs examined, lecturers were perceived as\nthe most favorable, while adult models were viewed least favorably. Overall,\nour findings demonstrate that integrating theories from social psychology and\ninformation systems can enhance our understanding of how attitudes are formed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.14098,regular,post_llm,2023,9,"{'ai_likelihood': 0.98291015625, 'text': 'Computer Science Framework to Teach Community-Based Environmental\n  Literacy and Data Literacy to Diverse Students\n\n  This study introduces an integrated curriculum designed to empower\nunderrepresented students by combining environmental literacy, data literacy,\nand computer science. The framework promotes environmental awareness, data\nliteracy, and civic engagement using a culturally sustaining approach. This\nintegrated curriculum is embedded with resources to support language\ndevelopment, technology skills, and coding skills to accommodate the diverse\nneeds of students. To evaluate the effectiveness of this curriculum, we\nconducted a pilot study in a 5th-grade special education classroom with\nmultilingual Latinx students. During the pilot, students utilized Scratch, a\nblock-based coding language, to create interactive projects that showcased\nlocally collected data, which they used to communicate environmental challenges\nand propose solutions to community leaders. This approach allowed students to\nengage with environmental literacy at a deeper level, harnessing their\ncreativity and community knowledge in the digital learning environment.\nMoreover, this curriculum equipped students with the skills to critically\nanalyze political and socio-cultural factors impacting environmental\nsustainability. Students not only gained knowledge within the classroom but\nalso applied their learning to address real environmental issues within their\ncommunity. The results of the pilot study underscore the efficacy of this\nintegrated approach.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00089263916015625, 'GPT4': 0.6875, 'CLAUDE': 6.22868537902832e-05, 'GOOGLE': 0.306884765625, 'OPENAI_O_SERIES': 0.0019006729125976562, 'DEEPSEEK': 6.633996963500977e-05, 'GROK': 3.838539123535156e-05, 'NOVA': 6.854534149169922e-06, 'OTHER': 5.650520324707031e-05, 'HUMAN': 0.0027923583984375}}"
2309.08133,review,post_llm,2023,9,"{'ai_likelihood': 3.0795733133951826e-06, 'text': 'Talkin\' \'Bout AI Generation: Copyright and the Generative-AI Supply\n  Chain\n\n  ""Does generative AI infringe copyright?"" is an urgent question. It is also a\ndifficult question, for two reasons. First, ""generative AI"" is not just one\nproduct from one company. It is a catch-all name for a massive ecosystem of\nloosely related technologies, including conversational text chatbots like\nChatGPT, image generators like Midjourney and DALL-E, coding assistants like\nGitHub Copilot, and systems that compose music and create videos. These systems\nbehave differently and raise different legal issues. The second problem is that\ncopyright law is notoriously complicated, and generative-AI systems manage to\ntouch on a great many corners of it: authorship, similarity, direct and\nindirect liability, fair use, and licensing, among much else. These issues\ncannot be analyzed in isolation, because there are connections everywhere.\n  In this Article, we aim to bring order to the chaos. To do so, we introduce\nthe generative-AI supply chain: an interconnected set of stages that transform\ntraining data (millions of pictures of cats) into generations (a new,\npotentially never-seen-before picture of a cat that has never existed).\nBreaking down generative AI into these constituent stages reveals all of the\nplaces at which companies and users make choices that have copyright\nconsequences. It enables us to trace the effects of upstream technical designs\non downstream uses, and to assess who in these complicated sociotechnical\nsystems bears responsibility for infringement when it happens. Because we\nengage so closely with the technology of generative AI, we are able to shed\nmore light on the copyright questions. We do not give definitive answers as to\nwho should and should not be held liable. Instead, we identify the key\ndecisions that courts will need to make as they grapple with these issues, and\npoint out the consequences that would likely flow from different liability\nregimes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12354,regular,post_llm,2023,9,"{'ai_likelihood': 5.265076955159506e-06, 'text': 'Enhancing E-Learning System Through Learning Management System (LMS)\n  Technologies: Reshape The Learner Experience\n\n  This paper aims to determine how the LMS Web portal application reshapes the\nlearner experience through the developed E-Learning Management System using\nData Mining Algorithm.\n  The methodology that the researchers used is descriptive research involving\nthe interpretation of the meaning or significance of what is described. Gather\ndata from questionnaires, surveys, observations concerned with the study, and\nthe chi-square formula for the statistical treatment of data.\n  The findings of the study, the extent that LMS Web portal application\nreshapes the learner experience in terms of the following variables with the\nAverage Weighted Mean (AWM): Flexible engagement of Learners in any device is\nhighly satisfied; Personalize learning tracker is highly satisfied;\nCollaborating with the Learning Expert is highly satisfied; Provides\nuser-friendly Teaching Tools is satisfied; Evident Learner Progress and\nInvolvement and is satisfied.\n  In the final analysis, this E-Learning System can fit any educational needs\nas follows: chat, virtual classes, supportive resources for the students,\nindividual and group monitoring, and assessment using LMS as maximum\nefficiency. Moreover, this platform can be used to deliver hybrid learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.10869,regular,post_llm,2023,9,"{'ai_likelihood': 7.086329989963108e-06, 'text': ""SOS TUTORIA UC: A Diversity-Aware Application for Tutor Recommendation\n  Based on Competence and Personality\n\n  SOS TUTORIA UC is a student connection application aimed at facilitating\nacademic assistance between students through external tutoring outside of the\napplication. To achieve this, a responsive web application was designed and\nimplemented, integrated with the WeNet platform, which provides various\nservices for user management and user recommendation algorithms. This study\npresents the development and validation of the experience in the application by\nevaluating the importance of incorporating the dimension of personality traits,\naccording to the Big Five model, in the process of recommending students for\nacademic tutoring. The goal is to provide support for students to find others\nwith greater knowledge and with a personality that is \\'different\\',\n\\'similar\\' or \\'indifferent\\' to their own preferences for receiving academic\nassistance on a specific topic. The integration with the WeNet platform was\nsuccessful in terms of components, and the results of the recommendation system\ntesting were positive but have room for improvement.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.15876,review,post_llm,2023,9,"{'ai_likelihood': 2.5497542487250434e-06, 'text': ""Older LGBT+ and Blockchain in Healthcare: A Value Sensitive Design\n  Perspective\n\n  Most algorithms deployed in healthcare do not consider gender and sex despite\nthe effect they have on individuals' health differences. Missing these\ndimensions in healthcare information systems is a point of concern, as\nneglecting these aspects will inevitably perpetuate existing biases, produce\nfar from optimal results, and may generate diagnosis errors. An\noften-overlooked community with distinct care values and needs are LGBT+ older\nadults, which has traditionally been under-surveyed in healthcare and\ntechnology design. This paper investigates the implications of missing gender\nand sex considerations in distributed ledger technologies for LGBT+ older\nadults. By using the value sensitive design methodology, our contribution shows\nthat many value meanings dear to marginalized communities are not considered in\nthe design of the blockchain, such as LGBT+ older adults' interpretations of\ntrust, privacy, and security. By highlighting the LGBT+ older population\nvalues, our contribution alerts us to the potential discriminatory implications\nof these technologies, which do not consider the gender and sex differences of\nmarginalized, silent populations. Focusing on one community throughout - LGBT+\nolder adults - we emphasize the need for a holistic, value sensitive design\napproach for the development of ledger technologies for healthcare, including\nthe values of everyone within the healthcare ecosystem.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.0044,regular,post_llm,2023,9,"{'ai_likelihood': 3.1391779581705734e-05, 'text': ""Yet another Improvement of Plantard Arithmetic for Faster Kyber on\n  Low-end 32-bit IoT Devices\n\n  This paper presents another improved version of Plantard arithmetic that\ncould speed up Kyber implementations on two low-end 32-bit IoT platforms (ARM\nCortex-M3 and RISC-V) without SIMD extensions. Specifically, we further enlarge\nthe input range of the Plantard arithmetic without modifying its computation\nsteps. After tailoring the Plantard arithmetic for Kyber's modulus, we show\nthat the input range of the Plantard multiplication by a constant is at least\n2.14 times larger than the original design in TCHES2022. Then, two optimization\ntechniques for efficient Plantard arithmetic on Cortex-M3 and RISC-V are\npresented. We show that the Plantard arithmetic supersedes both Montgomery and\nBarrett arithmetic on low-end 32-bit platforms. With the enlarged input range\nand the efficient implementation of the Plantard arithmetic on these platforms,\nwe propose various optimization strategies for NTT/INTT. We minimize or\nentirely eliminate the modular reduction of coefficients in NTT/INTT by taking\nadvantage of the larger input range of the proposed Plantard arithmetic on\nlow-end 32-bit platforms. Furthermore, we propose two memory optimization\nstrategies that reduce 23.50% to 28.31% stack usage for the speed-version Kyber\nimplementation when compared to its counterpart on Cortex-M4. The proposed\noptimizations make the speed-version implementation more feasible on low-end\nIoT devices. Thanks to the aforementioned optimizations, our NTT/INTT\nimplementation shows considerable speedups compared to the state-of-the-art\nwork. Overall, we demonstrate the applicability of the speed-version Kyber\nimplementation on memory-constrained IoT platforms and set new speed records\nfor Kyber on these platforms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.09562,other,post_llm,2023,9,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Training Students' Abstraction Skills Around a CAF\\'E 2.0\n\n  Shaping first year students' mind to help them master abstraction skills is\nas crucial as it is challenging. Although abstraction is a key competence in\nproblem-solving (in particular in STEM disciplines), students are often found\nto rush that process because they find it hard and do not get any direct\noutcome out of it. They prefer to invest their efforts directly in a concrete\nground, rather than using abstraction to create a solution.\n  To overcome that situation, in the context of our CS1 course, we implemented\na tool called CAF\\'E 2.0. It allows students to actively and regularly practice\n(thanks to a longitudinal activity) their abstraction skills through a\ngraphical programming methodology. Moreover, further than reviewing students'\nfinal implementation, CAF\\'E 2.0 produces a personalized feedback on how\nstudents modeled their solution, and on how consistent it is with their final\ncode. This paper describes CAF\\'E 2.0 in a general setting and also provides a\nconcrete example in our CS1 course context. This paper also assesses students'\ninteraction with CAF\\'E 2.0 through perception and participation data. Finally,\nwe explain how CAF\\'E 2.0 could extended in another context than a CS1 course.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.04179,regular,post_llm,2023,9,"{'ai_likelihood': 2.185503641764323e-05, 'text': 'Less Power for More Learning: Restricting OCaml Features for Effective\n  Teaching\n\n  We present a framework for sandboxing and restricting features of the OCaml\nprogramming language to effectively automate the grading of programming\nexercises, scaling to hundreds of submissions. We describe how to disable\nlanguage and library features that should not be used to solve a given\nexercise. We present an overview of an implementation of a mock IO system to\nallow testing of IO-related exercises in a controlled environment. Finally, we\ndetail a number of security considerations to ensure submitted code remains\nsandboxed, allowing automatic grading to be trusted without manual\nverification. The source code of our implementation is publicly available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.06196,regular,post_llm,2023,9,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Cookiescanner: An Automated Tool for Detecting and Evaluating GDPR\n  Consent Notices on Websites\n\n  The enforcement of the GDPR led to the widespread adoption of consent\nnotices, colloquially known as cookie banners. Studies have shown that many\nwebsite operators do not comply with the law and track users prior to any\ninteraction with the consent notice, or attempt to trick users into giving\nconsent through dark patterns. Previous research has relied on manually curated\nfilter lists or automated detection methods limited to a subset of websites,\nmaking research on GDPR compliance of consent notices tedious or limited. We\npresent \\emph{cookiescanner}, an automated scanning tool that detects and\nextracts consent notices via various methods and checks if they offer a decline\noption or use color diversion. We evaluated cookiescanner on a random sample of\nthe top 10,000 websites listed by Tranco. We found that manually curated filter\nlists have the highest precision but recall fewer consent notices than our\nkeyword-based methods. Our BERT model achieves high precision for English\nnotices, which is in line with previous work, but suffers from low recall due\nto insufficient candidate extraction. While the automated detection of decline\noptions proved to be challenging due to the dynamic nature of many sites,\ndetecting instances of different colors of the buttons was successful in most\ncases. Besides systematically evaluating our various detection techniques, we\nhave manually annotated 1,000 websites to provide a ground-truth baseline,\nwhich has not existed previously. Furthermore, we release our code and the\nannotated dataset in the interest of reproducibility and repeatability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12358,review,post_llm,2023,9,"{'ai_likelihood': 1.2616316477457683e-05, 'text': 'Modeling Digital Twin Data and Architecture: A Building Guide with\n  FIWARE as Enabling Technology\n\n  The use of Digital Twins in the industry has become a growing trend in recent\nyears, allowing to improve the lifecycle of any process by taking advantage of\nthe relationship between the physical and the virtual world. Existing\nliterature formulates several challenges for building Digital Twins, as well as\nsome proposals for overcoming them. However, in the vast majority of the cases,\nthe architectures and technologies presented are strongly bounded to the domain\nwhere the Digital Twins are applied. This article proposes the FIWARE\nEcosystem, combining its catalog of components and its Smart Data Models, as a\nsolution for the development of any Digital Twin. We also provide a use case to\nshowcase how to use FIWARE for building Digital Twins through a complete\nexample of a Parking Digital Twin. We conclude that the FIWARE Ecosystem\nconstitutes a real reference option for developing DTs in any domain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.14876,review,post_llm,2023,9,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'APPRAISE: a governance framework for innovation with AI systems\n\n  As artificial intelligence (AI) systems increasingly impact society, the EU\nArtificial Intelligence Act (AIA) is the first serious legislative attempt to\ncontain the harmful effects of AI systems. This paper proposes a governance\nframework for AI innovation. The framework bridges the gap between strategic\nvariables and responsible value creation, recommending audit as an enforcement\nmechanism. Strategic variables include, among others, organization size,\nexploration versus exploitation -, and build versus buy dilemmas. The proposed\nframework is based on primary and secondary research; the latter describes four\npressures that organizations innovating with AI experience. Primary research\nincludes an experimental setup, using which 34 organizations in the Netherlands\nare surveyed, followed up by 2 validation interviews. The survey measures the\nextent to which organizations coordinate technical elements of AI systems to\nultimately comply with the AIA. The validation interviews generated additional\nin-depth insights and provided root causes. The moderating effect of the\nstrategic variables is tested and found to be statistically significant for\nvariables such as organization size. Relevant insights from primary and\nsecondary research are eventually combined to propose the APPRAISE framework.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.0626,regular,post_llm,2023,9,"{'ai_likelihood': 0.95849609375, 'text': ""College Dropout Factors: An Analysis with LightGBM and Shapley's\n  Cooperative Game Theory\n\n  This study was based on data analysis of academic histories of civil\nengineering students at FACET-UNT. Our main objective was to determine the\nacademic performance variables that have a significant impact on the dropout of\nthe career. To do this, we implemented a correlation model using LightGBM\n(Barbier et al., 2016; Ke et al., 2017; Shi et al., 2022). We use this model to\nidentify the key variables that influence the probability of student dropout.\n  In addition, we use game theory to interpret the results obtained.\nSpecifically, we use the SHAP library (Lundberg et al., 2018, 2020; Lundberg &\nLee, 2017) in Python to calculate the Shapley numbers.\n  The results of our study revealed the most important variables that influence\nthe dropout from the civil engineering career. Significant differences were\nidentified in terms of age, time spent in studies, and academic performance,\nwhich includes the number of courses passed and the number of exams taken.\nThese results may be useful to develop more effective student retention\nstrategies and improve academic success in this discipline.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.27685546875, 'GPT4': 0.004093170166015625, 'CLAUDE': 0.0148773193359375, 'GOOGLE': 0.67333984375, 'OPENAI_O_SERIES': 0.0005812644958496094, 'DEEPSEEK': 2.3543834686279297e-05, 'GROK': 4.4345855712890625e-05, 'NOVA': 0.0005316734313964844, 'OTHER': 0.015960693359375, 'HUMAN': 0.01342010498046875}}"
2309.11239,review,post_llm,2023,9,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Data-Driven Analysis of Gender Fairness in the Software Engineering\n  Academic Landscape\n\n  Gender bias in education gained considerable relevance in the literature over\nthe years. However, while the problem of gender bias in education has been\nwidely addressed from a student perspective, it is still not fully analysed\nfrom an academic point of view. In this work, we study the problem of gender\nbias in academic promotions (i.e., from Researcher to Associated Professor and\nfrom Associated to Full Professor) in the informatics (INF) and software\nengineering (SE) Italian communities. In particular, we first conduct a\nliterature review to assess how the problem of gender bias in academia has been\naddressed so far. Next, we describe a process to collect and preprocess the INF\nand SE data needed to analyse gender bias in Italian academic promotions.\nSubsequently, we apply a formal bias metric to these data to assess the amount\nof bias and look at its variation over time. From the conducted analysis, we\nobserve how the SE community presents a higher bias in promotions to Associate\nProfessors and a smaller bias in promotions to Full Professors compared to the\noverall INF community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.11253,review,post_llm,2023,9,"{'ai_likelihood': 1.986821492513021e-07, 'text': ""Data Exfiltration by Hotjar Revisited\n\n  Session replay scripts allow website owners to record the interaction of each\nweb site visitor and aggregate the interaction to reveal the interests and\nproblems of the visitors. However, previous research identified such techniques\nas privacy intrusive. This position paper updates the information on data\ncollection by Hotjar. It revisits the previous findings to detect and describe\nthe changes. The default policy to gather inputs changed; the recording script\ngathers only information from explicitly allowed input elements. Nevertheless,\nHotjar does record content reflecting users' behaviour outside input HTML\nelements. Even though we propose changes that would prevent the leakage of the\nreflected content, we argue that such changes will most likely not appear in\npractice. The paper discusses improvements in handling TLS. Not only do web\npage operators interact with Hotjar through encrypted connections, but Hotjar\nscripts do not work on sites not protected by TLS. Hotjar respects the Do Not\nTrack signal; however, users need to connect to Hotjar even in the presence of\nthe Do Not Track setting. Worse, malicious web operators can trick Hotjar into\nrecording sessions of users with the active Do Not Track setting. Finally, we\npropose and motivate the extension of GDPR Art. 25 obligations to processors.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.07651,regular,post_llm,2023,9,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Methodologies for Selection of Optimal Sites for Renewable Energy Under\n  a Diverse Set of Constraints and Objectives\n\n  In this paper, we present methodologies for optimal selection for renewable\nenergy sites under a different set of constraints and objectives. We consider\ntwo different models for the site-selection problem - coarse-grained and\nfine-grained, and analyze them to find solutions. We consider multiple\ndifferent ways to measure the benefits of setting up a site. We provide\napproximation algorithms with a guaranteed performance bound for two different\nbenefit metrics with the coarse-grained model. For the fine-grained model, we\nprovide a technique utilizing Integer Linear Program to find the optimal\nsolution. We present the results of our extensive experimentation with\nsynthetic data generated from sparsely available real data from solar farms in\nArizona.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12355,review,post_llm,2023,9,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Role of ICT Innovation in Perpetuating the Myth of Techno-Solutionism\n\n  Innovation in Information and Communication Technology has become one of the\nkey economic drivers of our technology dependent world. In popular notion, the\ntech industry or how ICT is often known has become synonymous to all\ntechnologies that drive modernity. Digital technologies have become so\npervasive that it is hard to imagine new technology developments that are not\ntotally or partially influenced by ICT innovations. Furthermore, the pace of\ninnovation in ICT sector over the last few decades has been unprecedented in\nhuman history. In this paper we argue that, not only ICT had a tremendous\nimpact on the way we communicate and produce but this innovation paradigm has\ncrucially shaped collective expectations and imagination about what technology\nmore broadly can actually deliver. These expectations have often crystalised\ninto a widespread acceptance, among general public and policy makers, of\ntechnosolutionism. This is a belief that technology not restricted to ICT alone\ncan solve all problems humanity is facing from poverty and inequality to\necosystem loss and climate change. In this paper we show the many impacts of\nrelentless ICT innovation. The spectacular advances in this sector, coupled\nwith corporate power that benefits from them have facilitated the uptake by\ngovernments and industries of an uncritical narrative of techno-optimist that\nneglects the complexity of the wicked problems that affect the present and\nfuture of humanity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.00328,regular,post_llm,2023,9,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Deployment Corrections: An incident response framework for frontier AI\n  models\n\n  A comprehensive approach to addressing catastrophic risks from AI models\nshould cover the full model lifecycle. This paper explores contingency plans\nfor cases where pre-deployment risk management falls short: where either very\ndangerous models are deployed, or deployed models become very dangerous.\n  Informed by incident response practices from industries including\ncybersecurity, we describe a toolkit of deployment corrections that AI\ndevelopers can use to respond to dangerous capabilities, behaviors, or use\ncases of AI models that develop or are detected after deployment. We also\nprovide a framework for AI developers to prepare and implement this toolkit.\n  We conclude by recommending that frontier AI developers should (1) maintain\ncontrol over model access, (2) establish or grow dedicated teams to design and\nmaintain processes for deployment corrections, including incident response\nplans, and (3) establish these deployment corrections as allowable actions with\ndownstream users. We also recommend frontier AI developers, standard-setting\norganizations, and regulators should collaborate to define a standardized\nindustry-wide approach to the use of deployment corrections in incident\nresponse.\n  Caveat: This work applies to frontier AI models that are made available\nthrough interfaces (e.g., API) that provide the AI developer or another\nupstream party means of maintaining control over access (e.g., GPT-4 or\nClaude). It does not apply to management of catastrophic risk from open-source\nmodels (e.g., BLOOM or Llama-2), for which the restrictions we discuss are\nlargely unenforceable.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.09775,regular,post_llm,2023,9,"{'ai_likelihood': 6.4240561591254345e-06, 'text': ""ArxNet Model and Data: Building Social Networks from Image Archives\n\n  A corresponding explosion in digital images has accompanied the rapid\nadoption of mobile technology around the world. People and their activities are\nroutinely captured in digital image and video files. By their very nature,\nthese images and videos often portray social and professional connections.\nIndividuals in the same picture are often connected in some meaningful way. Our\nresearch seeks to identify and model social connections found in images using\nmodern face detection technology and social network analysis. The proposed\nmethods are then demonstrated on the public image repository associated with\nthe 2022 Emmy's Award Presentation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.05626,regular,post_llm,2023,9,"{'ai_likelihood': 9.602970547146268e-06, 'text': ""L'origine de l'objectif est-elle importante? Effets motivationnels\n  d'objectifs autod{\\'e}finis en production\n\n  Only 21% of employees consider themselves engaged at work. Moreover,\ndisengagement has been shown to be even more problematic when work is\nrepetitive in nature. Lack of engagement has been linked to variety of negative\noutcomes for employees and companies (e.g., turnover, absenteeism, well-being,\nsafety incidents, productivity). Gamification, i.e., integrating game elements\ninto work systems, has been successfully used to increase engagement and\nmotivation, even when work tasks were mundane and repetitive. In the current\nstudy, we focused on a commonly used game element, goal setting, through the\nlens of self-determination theory and goal-setting theory. We argue that goals\ngiven by an external source (e.g., company, experimenter) produce extrinsic\nmotivation, which improves engagement and performance only in the short term.\nWe posit that self-set goals lead to more autonomous motivation, and therefore\nlong-term engagement and performance. One hundred two participants completed a\nrepetitive material-handling task in one of three conditions (assigned goal,\nself-set goal, no goal). Results showed that perceived autonomy (autonomous\nmotivation) and performance were best when goals were self-set. Engagement,\nhowever, was equal between self-set and assigned goals. The results indicate\nthat self-set goals have the greatest potential to generate long-term positive\noutcomes both for employees and companies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.00505,regular,post_llm,2023,9,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Rural Access Index: A global study\n\n  The Rural Access Index (RAI), one of the UN Sustainable Development Goal\nindicators (SDG 9.1.1), represents the proportion of the rural population\nresiding within 2 km of all-season roads. It reflects the accessibility of\nrural residents to transportation services and could provide guidance for the\nimprovement of road infrastructure. The primary deficiencies in assessing the\nRAI include the limited studying area, its incomplete meaning and the absence\nof correlation analysis with other influencing factors. To address these\nissues, this study proposes the ""Not-served Rural Population (NSRP)"" as a\ncomplementary indicator to RAI. Utilizing multi-source open data, we analysed\nthe spatial patterns of RAI and NSRP indicators for 203 countries and then\nexplored the correlation between these 2 indicators and other 10 relevant\nfactors. The main findings are as follows: 1) North America, Europe, and\nOceania exhibit relatively high RAI values (>80%) and low NSRP values (<1\nmillion). In contrast, African regions have relatively low RAI values (<40%)\nand high NSRP values (>5 million). There is a negative correlation between RAI\nand NSRP. 2) There is spatial autocorrelation and significant imbalances in the\ndistribution of these two indicators. 3) The RAI exhibit a positive correlation\nwith the factors showing levels of the development of countries such as GDP,\neducation, indicating that improving the road infrastructure could reduce the\npoverty rates and enhance access to education. And in contrast with RAI, NSRP\nexhibit the completely negative correlations with these factors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.16789,review,post_llm,2023,9,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Extensible Consent Management Architectures for Data Trusts\n\n  Sensitive personal information of individuals and non-personal information of\norganizations or communities often needs to be legitimately exchanged among\ndifferent stakeholders, to provide services, maintain public health, law and\norder, and so on. While such exchanges are necessary, they also impose enormous\nprivacy and security challenges. Data protection laws like GDPR for personal\ndata and Indian Non-personal data protection draft specify conditions and the\n\\textit{legal capacity} in which personal and non-personal information can be\nsolicited and disseminated further. But there is a dearth of formalisms for\nspecifying legal capacities and jurisdictional boundaries, so that open-ended\nexchange of such data can be implemented. This paper proposes an extensible\nframework for consent management in Data Trusts in which data can flow across a\nnetwork through ""role tunnels"" established based on corresponding legal\ncapacities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.00129,regular,post_llm,2023,9,"{'ai_likelihood': 9.887748294406467e-05, 'text': ""ILB: Graph Neural Network Enabled Emergency Demand Response Program For\n  Electricity\n\n  Demand Response (DR) programs have become a crucial component of smart\nelectricity grids as they shift the flexibility of electricity consumption from\nsupply to demand in response to the ever-growing demand for electricity. In\nparticular, in times of crisis, an emergency DR program is required to manage\nunexpected spikes in energy demand. In this paper, we propose the\nIncentive-Driven Load Balancer (ILB), a program designed to efficiently manage\ndemand and response during crisis situations. By offering incentives to\nflexible households likely to reduce demand, the ILB facilitates effective\ndemand reduction and prepares them for unexpected events. To enable ILB, we\nintroduce a two-step machine learning-based framework for participant\nselection, which employs a graph-based approach to identify households capable\nof easily adjusting their electricity consumption. This framework utilizes two\nGraph Neural Networks (GNNs): one for pattern recognition and another for\nhousehold selection. Through extensive experiments on household-level\nelectricity consumption in California, Michigan, and Texas, we demonstrate the\nILB program's significant effectiveness in supporting communities during\nemergencies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.10681,review,post_llm,2023,9,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Social Interactions Mediated by the Internet and the Big- Five: a\n  Cross-Country Analysis\n\n  This study analyzes the possible relationship between personality traits, in\nterms of Big Five (extraversion, agreeableness, responsibility, emotional\nstability and openness to experience), and social interactions mediated by\ndigital platforms in different socioeconomic and cultural contexts. We\nconsidered data from a questionnaire and the experience of using a chatbot, as\na mean of requesting and offering help, with students from 4 universities:\nUniversity of Trento (Italy), the National University of Mongolia, the School\nof Economics of London (United Kingdom) and the Universidad Cat\\'olica Nuestra\nSe\\~nora de la Asunci\\'on (Paraguay). The main findings confirm that\npersonality traits may influence social interactions and active participation\nin groups. Therefore, they should be taken into account to enrich the\nrecommendation of matching algorithms between people who ask for help and\npeople who could respond not only on the basis of their knowledge and skills.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.10088,review,post_llm,2023,9,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Analyzing the Endeavours of the Supreme Court of India to Transcribe and\n  Translate Court Arguments in Light of the Proposed EU AI Act\n\n  The Supreme Court of India has been a pioneer in using ICT in courts through\nits e-Courts project in India. Yet another leap, its recent project, Design,\nDevelopment, and Implementation of Artificial Intelligence (AI) solution, tools\nfor transcribing arguments and Court proceedings at Supreme Court of India, has\npotential to impact the way AI algorithms are designed in India, and not just\nfor this particular project. In this paper, we evaluate the endeavours of the\nSupreme Court of India in light of the state of AI technology as well as the\nattempts to regulate AI. We argue that since the project aims to transcribe and\ntranslate the proceedings of the constitutional benches of the Supreme Court,\nit has potential to impact rule of law in the country. Hence, we place this\napplication in High Risk AI as per the provisions to the proposed EU AI Act. We\nprovide some guidelines on the approach to transcribe and translate making the\nmaximum use of AI in the Supreme Court of India without running into the\ndangers it may pose.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.00939,review,post_llm,2023,9,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Data Repurposing through Compatibility: A Computational Perspective\n\n  Reuse of data in new contexts beyond the purposes for which it was originally\ncollected has contributed to technological innovation and reducing the consent\nburden on data subjects. One of the legal mechanisms that makes such reuse\npossible is purpose compatibility assessment. In this paper, I offer an\nin-depth analysis of this mechanism through a computational lens. I moreover\nconsider what should qualify as repurposing apart from using data for a\ncompletely new task, and argue that typical purpose formulations are an\nimpediment to meaningful repurposing. Overall, the paper positions\ncompatibility assessment as a constructive practice beyond an ineffective\nstandard.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.06262,regular,post_llm,2023,9,"{'ai_likelihood': 0.9853515625, 'text': ""Exploring the Causal Relationship between Walkability and Affective\n  Walking Experience: Evidence from 7 Major Tertiary Education Campuses in\n  China\n\n  This study investigates the causal relationship between campus walkability\nand the emotional walking experiences of students, with a focus on their mental\nwell-being. Using data from 697 participants across seven Chinese tertiary\neducation campuses, the study employs a counterfactual analysis to estimate the\nimpact of campus walkability on students' walking experiences. The analysis\nreveals that students living in campuses with improved walkability are 9.75%\nmore likely to have positive walking experiences compared to those without\nwalkability renovations. While walking attitude is strongly correlated with\nwalking experiences, the study emphasizes the significance of objective factors\nsuch as campus surroundings and the availability of walking spaces in\ninfluencing the walking experience. Geographical features, including campus\nwalkability improvements, have the most substantial impact, and this effect\nvaries across different subsets of respondents. These findings underscore the\nimportance of considering specific subsets and geographical features when\nassessing the impact of walkability improvements on the walking experience. In\nconclusion, the study provides compelling evidence of a causal link between\nimproved campus walkability and enhanced emotional walking experiences among\nstudents, suggesting the need for further research on mediating factors and\ncultural variations affecting student mental health on various Chinese\ncampuses.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.185302734375, 'GPT4': 0.11688232421875, 'CLAUDE': 0.001224517822265625, 'GOOGLE': 0.68701171875, 'OPENAI_O_SERIES': 0.0002942085266113281, 'DEEPSEEK': 2.1755695343017578e-05, 'GROK': 5.125999450683594e-06, 'NOVA': 1.6450881958007812e-05, 'OTHER': 0.008087158203125, 'HUMAN': 0.0011043548583984375}}"
2309.06607,review,post_llm,2023,9,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'An Empirical Analysis of Racial Categories in the Algorithmic Fairness\n  Literature\n\n  Recent work in algorithmic fairness has highlighted the challenge of defining\nracial categories for the purposes of anti-discrimination. These challenges are\nnot new but have previously fallen to the state, which enacts race through\ngovernment statistics, policies, and evidentiary standards in\nanti-discrimination law. Drawing on the history of state race-making, we\nexamine how longstanding questions about the nature of race and discrimination\nappear within the algorithmic fairness literature. Through a content analysis\nof 60 papers published at FAccT between 2018 and 2020, we analyze how race is\nconceptualized and formalized in algorithmic fairness frameworks. We note that\ndiffering notions of race are adopted inconsistently, at times even within a\nsingle analysis. We also explore the institutional influences and values\nassociated with these choices. While we find that categories used in\nalgorithmic fairness work often echo legal frameworks, we demonstrate that\nvalues from academic computer science play an equally important role in the\nconstruction of racial categories. Finally, we examine the reasoning behind\ndifferent operationalizations of race, finding that few papers explicitly\ndescribe their choices and even fewer justify them. We argue that the\nconstruction of racial categories is a value-laden process with significant\nsocial and political consequences for the project of algorithmic fairness. The\nwidespread lack of justification around the operationalization of race reflects\ninstitutional norms that allow these political decisions to remain obscured\nwithin the backstage of knowledge production.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.13057,regular,post_llm,2023,9,"{'ai_likelihood': 1.3642840915256076e-05, 'text': 'The Return on Investment in AI Ethics: A Holistic Framework\n\n  We propose a Holistic Return on Ethics (HROE) framework for understanding the\nreturn on organizational investments in artificial intelligence (AI) ethics\nefforts. This framework is useful for organizations that wish to quantify the\nreturn for their investment decisions. The framework identifies the direct\neconomic returns of such investments, the indirect paths to return through\nintangibles associated with organizational reputation, and real options\nassociated with capabilities. The holistic framework ultimately provides\norganizations with the competency to employ and justify AI ethics investments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12362,review,post_llm,2023,9,"{'ai_likelihood': 1.0, 'text': 'Personalization, Cognition, and Gamification-based Programming Language\n  Learning: A State-of-the-Art Systematic Literature Review\n\n  Programming courses in computing science are important because they are often\nthe first introduction to computer programming for many students. Many\nuniversity students are overwhelmed with the information they must learn for an\nintroductory course. The current teacher-lecturer model of learning commonly\nemployed in university lecture halls often results in a lack of motivation and\nparticipation in learning. Personalized gamification is a pedagogical approach\nthat combines gamification and personalized learning to motivate and engage\nstudents while addressing individual differences in learning. This approach\nintegrates gamification and personalized learning strategies to inspire and\ninvolve students while addressing their unique learning needs and differences.\nA comprehensive literature search was conducted by including 81 studies that\nwere analyzed based on their research design, intervention, outcome measures,\nand quality assessment. The findings suggest that personalized gamification can\nenhance student cognition in programming courses by improving motivation,\nengagement, and learning outcomes. However, the effectiveness of personalized\ngamification varies depending on various factors, such as the type of\ngamification elements used, the degree of personalization, and the\ncharacteristics of the learners. This paper provides insights into designing\nand implementing effective personalized gamification interventions in\nprogramming courses. The findings could inform educational practitioners and\nresearchers in programming education about the potential benefits of\npersonalized gamification and its implications for educational practice.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.014434814453125, 'GPT4': 0.0001506805419921875, 'CLAUDE': 1.8596649169921875e-05, 'GOOGLE': 0.98291015625, 'OPENAI_O_SERIES': 4.3272972106933594e-05, 'DEEPSEEK': 4.470348358154297e-06, 'GROK': 1.0728836059570312e-06, 'NOVA': 1.6570091247558594e-05, 'OTHER': 0.002471923828125, 'HUMAN': 1.1265277862548828e-05}}"
2309.02142,regular,post_llm,2023,9,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Characteristics of ChatGPT users from Germany: implications for the digital divide from web tracking data\n\nA major challenge of our time is reducing disparities in access to and effective use of digital technologies, with recent discussions highlighting the role of AI in exacerbating the digital divide. We examine user characteristics that predict usage of the AI-powered conversational agent ChatGPT. We combine behavioral and survey data in a web tracked sample of N = 1376 German citizens to investigate differences in ChatGPT activity (usage, visits, and adoption) during the first 11 months from the launch of the service (November 30, 2022). Guided by a model of technology acceptance (UTAUT-2), we examine the role of socio-demographics commonly associated with the digital divide in ChatGPT activity and explore further socio-political attributes identified via stability selection in Lasso regressions. We confirm that lower age and higher education affect ChatGPT usage, but do not find that gender or income do. We find full-time employment and more children to be barriers to ChatGPT activity. Using a variety of social media was positively associated with ChatGPT activity. In terms of political variables, political knowledge and political self-efficacy as well as some political behaviors such as voting, debating political issues online and offline and political action online were all associated with ChatGPT activity, with online political debating and political self-efficacy negatively so. Finally, need for cognition and communication skills such as writing, attending meetings, or giving presentations, were also associated with ChatGPT engagement, though chairing/organizing meetings was negatively associated. Our research informs efforts to address digital disparities and promote digital literacy among underserved populations by presenting implications, recommendations, and discussions on ethical and social issues of our findings.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09228,regular,post_llm,2023,9,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Does Starting Deep Learning Homework Earlier Improve Grades?\n\n  Intuitively, students who start a homework assignment earlier and spend more\ntime on it should receive better grades on the assignment. However, existing\nliterature on the impact of time spent on homework is not clear-cut and comes\nmostly from K-12 education. It is not clear that these prior studies can inform\ncoursework in deep learning due to differences in demographics, as well as the\ncomputational time needed for assignments to be completed. We study this\nproblem in a post-hoc study of three semesters of a deep learning course at the\nUniversity of Maryland, Baltimore County (UMBC), and develop a hierarchical\nBayesian model to help make principled conclusions about the impact on student\nsuccess given an approximate measure of the total time spent on the homework,\nand how early they submitted the assignment. Our results show that both\nsubmitting early and spending more time positively relate with final grade.\nSurprisingly, the value of an additional day of work is apparently equal across\nstudents, even when some require less total time to complete an assignment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.00196,regular,post_llm,2023,9,"{'ai_likelihood': 1.9404623243543836e-05, 'text': 'A Comparative Study of Reference Reliability in Multiple Language\n  Editions of Wikipedia\n\n  Information presented in Wikipedia articles must be attributable to reliable\npublished sources in the form of references. This study examines over 5 million\nWikipedia articles to assess the reliability of references in multiple language\neditions. We quantify the cross-lingual patterns of the perennial sources list,\na collection of reliability labels for web domains identified and\ncollaboratively agreed upon by Wikipedia editors. We discover that some sources\n(or web domains) deemed untrustworthy in one language (i.e., English) continue\nto appear in articles in other languages. This trend is especially evident with\nsources tailored for smaller communities. Furthermore, non-authoritative\nsources found in the English version of a page tend to persist in other\nlanguage versions of that page. We finally present a case study on the Chinese,\nRussian, and Swedish Wikipedias to demonstrate a discrepancy in reference\nreliability across cultures. Our finding highlights future challenges in\ncoordinating global knowledge on source reliability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.10085,regular,post_llm,2023,9,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Evaluating the Impact of ChatGPT on Exercises of a Software Security\n  Course\n\n  Along with the development of large language models (LLMs), e.g., ChatGPT,\nmany existing approaches and tools for software security are changing. It is,\ntherefore, essential to understand how security-aware these models are and how\nthese models impact software security practices and education. In exercises of\na software security course at our university, we ask students to identify and\nfix vulnerabilities we insert in a web application using state-of-the-art\ntools. After ChatGPT, especially the GPT-4 version of the model, we want to\nknow how the students can possibly use ChatGPT to complete the exercise tasks.\nWe input the vulnerable code to ChatGPT and measure its accuracy in\nvulnerability identification and fixing. In addition, we investigated whether\nChatGPT can provide a proper source of information to support its outputs.\nResults show that ChatGPT can identify 20 of the 28 vulnerabilities we inserted\nin the web application in a white-box setting, reported three false positives,\nand found four extra vulnerabilities beyond the ones we inserted. ChatGPT makes\nnine satisfactory penetration testing and fixing recommendations for the ten\nvulnerabilities we want students to fix and can often point to related sources\nof information.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.05627,regular,post_llm,2023,9,"{'ai_likelihood': 1.0861290825737848e-05, 'text': 'Understanding the Difference between Office Presence and Co-presence in\n  Team Member Interactions\n\n  Although the public health emergency related to the coronavirus disease 2019\n(COVID-19) pandemic has officially ended, many software developers still work\npartly from home. Agile teams that coordinate their office time foster a sense\nof unity, collaboration, and cohesion among team members. In contrast, teams\nwith limited co-presence may experience challenges in establishing\npsychological safety and developing a cohesive and inclusive team culture,\npotentially hindering effective communication, knowledge sharing, and trust\nbuilding. Therefore, the effect of agile team members not being co-located\ndaily must be investigated. We explore the co-presence patterns of 17 agile\nteams in a large agile telecommunications company whose employees work partly\nfrom home. Based on office access card data, we found significant variation in\nco-presence practices. Some teams exhibited a coordinated approach, ensuring\nteam members are simultaneously present at the office. However, other teams\ndemonstrated fragmented co-presence, with only small subgroups of members\nmeeting in person and the remainder rarely interacting with their team members\nface-to-face. Thus, high average office presence in the team does not\nnecessarily imply that team members meet often in person at the office. In\ncontrast, non-coordinated teams may have both high average office presence and\nlow frequency of in-person interactions among the members. Our results suggest\nthat the promotion of mere office presence without coordinated co-presence is\nbased on a false assumption that good average attendance levels guarantee\nfrequent personal interactions. These findings carry important implications for\nresearch on long-term team dynamics and practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.02814,regular,post_llm,2023,9,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Can Telematics Improve Driving Style? The Use of Behavioural Data in\n  Motor Insurance\n\n  Motor insurance can use telematics data not only to understand the individual\ndriving style, but also to implement innovative coaching strategies that feed\nback to the drivers, through an app, the aggregated information extracted from\nthe data. The purpose is to encourage an improvement in their driving style.\nPrecondition for this improvement is that drivers are digitally engaged, that\nis, they interact with the app. Our hypothesis is that the effectiveness of\ncurrent experimentations depends on the integration of two distinct types of\nbehavioural data: behavioural data on driving style and behavioural data on\nusers' interaction with the app. Based on the empirical investigation of the\ndataset of a company selling a telematics motor insurance policy, our research\nshows that there is a correlation between engagement with the app and\nimprovement of driving style, but the analysis must distinguish different\ngroups of users with different driving abilities, and take into account time\ndifferences. Our findings contribute to clarify the methodological challenges\nthat must be addressed when exploring engagement and coaching effectiveness in\nproactive insurance policies. We conclude by discussing the possibility and\ndifficulties of tracking and using second-order behavioural data related to\npolicyholder engagement with the app.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.04997,review,post_llm,2023,9,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Gender Bias in Multimodal Models: A Transnational Feminist Approach\n  Considering Geographical Region and Culture\n\n  Deep learning based visual-linguistic multimodal models such as Contrastive\nLanguage Image Pre-training (CLIP) have become increasingly popular recently\nand are used within text-to-image generative models such as DALL-E and Stable\nDiffusion. However, gender and other social biases have been uncovered in these\nmodels, and this has the potential to be amplified and perpetuated through AI\nsystems. In this paper, we present a methodology for auditing multimodal models\nthat consider gender, informed by concepts from transnational feminism,\nincluding regional and cultural dimensions. Focusing on CLIP, we found evidence\nof significant gender bias with varying patterns across global regions. Harmful\nstereotypical associations were also uncovered related to visual cultural cues\nand labels such as terrorism. Levels of gender bias uncovered within CLIP for\ndifferent regions aligned with global indices of societal gender equality, with\nthose from the Global South reflecting the highest levels of gender bias.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12359,review,post_llm,2023,9,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Open access in Europe: a national and regional comparison\n\n  Open access to scientific publications has progressively become a key issue\nfor European policy makers, resulting in concrete measures by the different\ncountry members to promote its development. The aim of paper is, after\nproviding a quick overview of OA policies in Europe, to carry out a comparative\nstudy of OA practices within European countries, using data from the Web of\nScience (WoS) database. This analysis is based on two indicators: the OA share\nthat illustrates the evolution over time, and the normalized OA indicator\n(NOAI) that allows spatial comparisons, taking into account disciplinary\nstructures of countries. Results show a general trend towards the development\nof OA over time as expected, but with large disparities between countries,\ndepending on how early they begin taking measures in favor of OA. While it is\npossible to stress the importance of policy and its influence on open access at\ncountry level, this does not appear to be the case at the regional level. There\nis not much variability between regions, within the same country, in terms of\nopen access indicators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.12468,regular,post_llm,2023,9,"{'ai_likelihood': 1.778205235799154e-05, 'text': 'Generativism: the new hybrid\n\n  Generative Artificial Intelligence (GenAI) in Education has in a few short\nmonths moved from being the topic of discussion around speculative education\nfutures to a very concrete reality. It is clear that the future of education,\nas all industries, is collaboration with GenAI. GenAI attributes make it well\nsuited for social and constructivist approaches to learning that value\ncollaboration, community and the construction of knowledge and skills through\nactive learning. This article presents an approach to designing education in\ncollaboration with GenAI, based on digital education frameworks adapted for\nthis new hybrid of the AI age.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.08428,regular,post_llm,2023,9,"{'ai_likelihood': 0.3342013888888889, 'text': 'Virtual Harassment, Real Understanding: Using a Serious Game and\n  Bayesian Networks to Study Cyberbullying\n\n  Cyberbullying among minors is a pressing concern in our digital society,\nnecessitating effective prevention and intervention strategies. Traditional\ndata collection methods often intrude on privacy and yield limited insights.\nThis study explores an innovative approach, employing a serious game - designed\nwith purposes beyond entertainment - as a non-intrusive tool for data\ncollection and education. In contrast to traditional correlation-based\nanalyses, we propose a causality-based approach using Bayesian Networks to\nunravel complex relationships in the collected data and quantify result\nuncertainties. This robust analytical tool yields interpretable outcomes,\nenhances transparency in assumptions, and fosters open scientific discourse.\nPreliminary pilot studies with the serious game show promising results,\nsurpassing the informative capacity of traditional demographic and\npsychological questionnaires, suggesting its potential as an alternative\nmethodology. Additionally, we demonstrate how our approach facilitates the\nexamination of risk profiles and the identification of intervention strategies\nto mitigate this cybercrime. We also address research limitations and potential\nenhancements, considering the noise and variability of data in social studies\nand video games. This research advances our understanding of cyberbullying and\nshowcase the potential of serious games and causality-based approaches in\nstudying complex social issues.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2309.10678,regular,post_llm,2023,9,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Dialogues with algorithms\n\n  In this short paper we focus on human in the loop for rule-based software\nused for law enforcement. For example, one can think of software that computes\nfines like tachograph software, software that prepares evidence like DNA\nsequencing software or social profiling software to patrol in high-risk zones,\namong others. An important difference between a legal human agent and a\nsoftware application lies in possible dialogues. A human agent can be\ninterrogated to motivate her decisions. Often such dialogues with software are\nat the best extremely hard but mostly impossible. We observe that the absence\nof a dialogue can sincerely violate civil rights and legal principles like, for\nexample, Transparency or Contestability. Thus, possible dialogues with legal\nalgorithms are at the least highly desirable. Futuristic as this may sound, we\nobserve that in various realms of formal methods, such dialogues are easily\nobtainable. However, this triggers the usual tension between the expressibility\nof the dialogue language and the feasibility of the corresponding computations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10726,review,post_llm,2023,10,"{'ai_likelihood': 6.258487701416016e-06, 'text': ""Cost/benefit analysis model for implementing virtual reality in\n  construction companies\n\n  Immersive technologies (ImT), like Virtual Reality (VR), have several\npotential applications in the construction industry. However, the absence of a\ncost-benefit analysis discourages construction decision-makers from\nimplementing these technologies. In this study, we proposed a primary model for\nconducting a cost-benefit analysis for implementing virtual reality in\nconstruction companies. The cost and benefit factors were identified through a\nliterature review and considered input variables for the model, and then using\nsynthetic data, a Monte Carlo simulation was performed to generate a\ndistribution of outcome. Given the uncertainty in input parameters, this\ndistribution reflected the potential range of total net benefit. Considering\nsynthetic data and input factors obtained only through literature and\nassumptions, VR implementation could be a promising decision based on the\nresults. This study's results would benefit decision-makers in construction\ncompanies about the costs and benefits of implementing VR and other researchers\ninterested in this field.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.07888,regular,post_llm,2023,10,"{'ai_likelihood': 3.344482845730252e-06, 'text': 'Viability of Mobile Forms for Population Health Surveys in Low Resource\n  Areas\n\n  Population health surveys are an important tool to effectively allocate\nlimited resources in low resource communities. In such an environment, surveys\nare often done by local population with pen and paper. Data thus collected is\ndifficult to tabulate and analyze. We conducted a series of interviews and\nexperiments in the Philippines to assess if mobile forms can be a viable and\nmore efficient survey method. We first conducted pilot interviews and found 60%\nof the local surveyors actually preferred mobile forms over paper. We then\nbuilt a software that can generate mobile forms that are easy to use, capable\nof working offline, and able to track key metrics such as time to complete\nquestions. Our mobile form was field tested in three locations in the\nPhilippines with 33 surveyors collecting health survey responses from 266\nsubjects. The percentage of surveyors preferring mobile forms increased to 76%\nafter just using the form a few times. The results demonstrate our mobile form\nis a viable method to conduct large scale population health surveys in a low\nresource environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.03192,review,post_llm,2023,10,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""Generative AI in the Classroom: Can Students Remain Active Learners?\n\n  Generative Artificial Intelligence (GAI) can be seen as a double-edged weapon\nin education. Indeed, it may provide personalized, interactive and empowering\npedagogical sequences that could favor students' intrinsic motivation, active\nengagement and help them have more control over their learning. But at the same\ntime, other GAI properties such as the lack of uncertainty signalling even in\ncases of failure (particularly with Large Language Models (LLMs)) could lead to\nopposite effects, e.g. over-estimation of one's own competencies, passiveness,\nloss of curious and critical-thinking sense, etc.\n  These negative effects are due in particular to the lack of a pedagogical\nstance in these models' behaviors. Indeed, as opposed to standard pedagogical\nactivities, GAI systems are often designed to answers users' inquiries easily\nand conveniently, without asking them to make an effort, and without focusing\non their learning process and/or outcomes.\n  This article starts by outlining some of these opportunities and challenges\nsurrounding the use of GAI in education, with a focus on the effects on\nstudents' active learning strategies and related metacognitive skills. Then, we\npresent a framework for introducing pedagogical transparency in GAI-based\neducational applications. This framework presents 1) training methods to\ninclude pedagogical principles in the models, 2) methods to ensure controlled\nand pedagogically-relevant interactions when designing activities with GAI and\n3) educational methods enabling students to acquire the relevant skills to\nproperly benefit from the use of GAI in their learning activities\n(meta-cognitive skills, GAI litteracy).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.19303,regular,post_llm,2023,10,"{'ai_likelihood': 5.364418029785156e-06, 'text': ""Extracting user needs with Chat-GPT for dialogue recommendation\n\n  Large-scale language models (LLMs), such as ChatGPT, are becoming\nincreasingly sophisticated and exhibit human-like capabilities, playing an\nessential role in assisting humans in a variety of everyday tasks. An important\napplication of AI is interactive recommendation systems that respond to human\ninquiries and make recommendations tailored to the user. In most conventional\ninteractive recommendation systems, the language model is used only as a\ndialogue model, and there is a separate recommendation system. This is due to\nthe fact that the language model used as a dialogue system does not have the\ncapability to serve as a recommendation system. Therefore, we will realize the\nconstruction of a dialogue system with recommendation capability by using\nOpenAI's Chat-GPT, which has a very high inference capability as a dialogue\nsystem and the ability to generate high-quality sentences, and verify the\neffectiveness of the system.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.1532,review,post_llm,2023,10,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Conflating point of interest (POI) data: A systematic review of matching\n  methods\n\n  Point of interest (POI) data provide digital representations of places in the\nreal world, and have been increasingly used to understand human-place\ninteractions, support urban management, and build smart cities. Many POI\ndatasets have been developed, which often have different geographic coverages,\nattribute focuses, and data quality. From time to time, researchers may need to\nconflate two or more POI datasets in order to build a better representation of\nthe places in the study areas. While various POI conflation methods have been\ndeveloped, there lacks a systematic review, and consequently, it is difficult\nfor researchers new to POI conflation to quickly grasp and use these existing\nmethods. This paper fills such a gap. Following the protocol of Preferred\nReporting Items for Systematic Reviews and Meta-Analyses (PRISMA), we conduct a\nsystematic review by searching through three bibliographic databases using\nreproducible syntax to identify related studies. We then focus on a main step\nof POI conflation, i.e., POI matching, and systematically summarize and\ncategorize the identified methods. Current limitations and future opportunities\nare discussed afterwards. We hope that this review can provide some guidance\nfor researchers interested in conflating POI datasets for their research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.1074,regular,post_llm,2023,10,"{'ai_likelihood': 1.1490450965033638e-05, 'text': ""Evaluation of Crowdsourced Data on Unplowed Roads\n\n  Transportation agencies routinely collect weather data to support maintenance\nactivities. With the proliferation of smartphones, many agencies have begun\nusing crowdsourced data in operations. This study evaluates a novel unplowed\nroads dataset from the largest crowdsourced transportation data provider Waze.\nUser-reported unplowed roads in Virginia were compared to national and state\nweather data for accuracy, and found 81% of reports were near known snow\nevents, with false positives occurring at a regular rate of approximately 10\nper day statewide. Reports were largely located on primary roads, limiting the\nusefulness for transportation agencies who may be most concerned with poorly\nmonitored secondary roads. An effort to encourage unplowed road reporting in\nWaze through targeted messages on social media did not increase participation.\nLow reporting may be due to the feature's novelty, recent mild winters, or\nCOVID-19 school and business closures.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.05432,regular,post_llm,2023,10,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Emergency Financing Tokens\n\n  We propose a novel payment mechanism for use by victims of large-scale\nconflict or natural disasters to conduct critical economic transactions and\nrebuild damaged infrastructure in the absence of both cash and traditional\nelectronic payment mechanisms linked to bank accounts, such as debit cards or\nwire transfers. Claimants shall receive electronic tokens that can be used to\npay registered businesses, such as purveyors of food and other basic goods,\nproviders of essential services, and contractors to carry out construction\ntasks. The system shall be based upon the scalable architecture for retail\npayments described in our earlier work, which provides both strong privacy for\nconsumers and strong compliance enforcement for recipients of funds. The system\nshall be designed to achieve three main objectives. First, tokens issued to\nclaimants would be held directly by the claimants themselves, not via\nintermediaries, to avoid the risk of failure or subversion of asset custodians.\nSecond, transactions shall not be traceable to the identity of the claimants,\nthus mitigating the risk that claimants can be pressured by service providers\nor other parties to reveal information that can be used to exploit them. Third,\nbusinesses and service providers that receive tokens shall be subject to\nrigorous compliance procedures upon redemption for cash or bank deposits, thus\nensuring that only legitimate businesses or service providers can receive value\nfrom tokens, that token transfers will embed the identities of any recipients\nbeyond the initial claimant, and that tax obligations shall be met at the time\nof redemption.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.18451,regular,post_llm,2023,10,"{'ai_likelihood': 1.3444158766004775e-05, 'text': ""Fusion of the Power from Citations: Enhance your Influence by\n  Integrating Information from References\n\n  Influence prediction plays a crucial role in the academic community. The\namount of scholars' influence determines whether their work will be accepted by\nothers. Most existing research focuses on predicting one paper's citation count\nafter a period or identifying the most influential papers among the massive\ncandidates, without concentrating on an individual paper's negative or positive\nimpact on its authors. Thus, this study aims to formulate the prediction\nproblem to identify whether one paper can increase scholars' influence or not,\nwhich can provide feedback to the authors before they publish their papers.\nFirst, we presented the self-adapted ACC (Average Annual Citation Counts)\nmetric to measure authors' impact yearly based on their annual published\npapers, paper citation counts, and contributions in each paper. Then, we\nproposed the RD-GAT (Reference-Depth Graph Attention Network) model to\nintegrate heterogeneous graph information from different depth of references by\nassigning attention coefficients on them. Experiments on AMiner dataset\ndemonstrated that the proposed ACC metrics could represent the authors\ninfluence effectively, and the RD-GAT model is more efficiently on the academic\ncitation network, and have stronger robustness against the overfitting problem\ncompared with the baseline models. By applying the framework in this work,\nscholars can identify whether their papers can improve their influence in the\nfuture.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10727,review,post_llm,2023,10,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Unveiling the Potential of Big Data Analytics for Transforming Higher\n  Education in Bangladesh; Needs, Prospects, and Challenges\n\n  Big Data Analytics has gained tremendous momentum in many sectors worldwide.\nBig Data has substantial influence in the field of Learning Analytics that may\nallow academic institutions to better understand the learners needs and\nproactively address them. Hence, it is essential to understand Big Data and its\napplication. With the capability of Big Data to find a broad understanding of\nthe scientific decision making process, Big Data Analytics (BDA) can be a piece\nof the answer to accomplishing Bangladesh Higher Education (BHE) objectives.\nThis paper reviews the capacity of BDA, considers possible applications in BHE,\ngives an insight into how to improve the quality of education or uncover\nadditional values from the data generated by educational institutions, and\nlastly, identifies needs and difficulties, opportunities, and some frameworks\nto probable implications about the BDA in BHE sector.\n  Keywords; Big Data Analytics, Learning Analytics, Quality of Education,\nChallenges, Higher Education, Bangladesh\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.03252,review,post_llm,2023,10,"{'ai_likelihood': 2.506706449720595e-05, 'text': ""Exploring age-related patterns in internet access: Insights from a\n  secondary analysis of New Zealand survey data\n\n  About thirty years ago, when the Internet started to be commercialised,\naccess to the medium became a topic of research and debate. Up-to-date evidence\nabout key predictors, such age, is crucial because of the Internet's\never-changing nature and the challenges associated with gaining access to it.\nThis paper aims to give an overview of New Zealand's Internet access trends and\nhow they relate to age. It is based on secondary analysis of data from a larger\nonline panel survey with 1,001 adult respondents. The Chi-square test of\nindependence and Cramer's V were used in the analysis. The study provides new\nevidence to understand the digital divide. Specifically, it uncovers a growing\ndisparity in the quality of Internet connectivity. Even though fibre is the\nmost common type of broadband connection at home, older adults are less likely\nto have it and more likely to use wireless broadband, which is a slower\nconnection type. Additionally, a sizable majority of people in all age\ncategories have favourable opinions on the Internet. Interestingly, this was\nmore prevalent among older people, although they report an increased concern\nabout the security of their personal information online. The implications of\nthe results are discussed and some directions for future research are proposed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.16984,regular,post_llm,2023,10,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""Patterns of Student Help-Seeking When Using a Large Language\n  Model-Powered Programming Assistant\n\n  Providing personalized assistance at scale is a long-standing challenge for\ncomputing educators, but a new generation of tools powered by large language\nmodels (LLMs) offers immense promise. Such tools can, in theory, provide\non-demand help in large class settings and be configured with appropriate\nguardrails to prevent misuse and mitigate common concerns around learner\nover-reliance. However, the deployment of LLM-powered tools in authentic\nclassroom settings is still rare, and very little is currently known about how\nstudents will use them in practice and what type of help they will seek. To\naddress this, we examine students' use of an innovative LLM-powered tool that\nprovides on-demand programming assistance without revealing solutions directly.\nWe deployed the tool for 12 weeks in an introductory computer and data science\ncourse ($n = 52$), collecting more than 2,500 queries submitted by students\nthroughout the term. We manually categorized all student queries based on the\ntype of assistance sought, and we automatically analyzed several additional\nquery characteristics. We found that most queries requested immediate help with\nprogramming assignments, whereas fewer requests asked for help on related\nconcepts or for deepening conceptual understanding. Furthermore, students often\nprovided minimal information to the tool, suggesting this is an area in which\ntargeted instruction would be beneficial. We also found that students who\nachieved more success in the course tended to have used the tool more\nfrequently overall. Lessons from this research can be leveraged by programming\neducators and institutions who plan to augment their teaching with emerging\nLLM-powered tools.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.06778,review,post_llm,2023,10,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""How Knowledge Workers Think Generative AI Will (Not) Transform Their\n  Industries\n\n  Generative AI is expected to have transformative effects in multiple\nknowledge industries. To better understand how knowledge workers expect\ngenerative AI may affect their industries in the future, we conducted\nparticipatory research workshops for seven different industries, with a total\nof 54 participants across three US cities. We describe participants'\nexpectations of generative AI's impact, including a dominant narrative that cut\nacross the groups' discourse: participants largely envision generative AI as a\ntool to perform menial work, under human review. Participants do not generally\nanticipate the disruptive changes to knowledge industries currently projected\nin common media and academic narratives. Participants do however envision\ngenerative AI may amplify four social forces currently shaping their\nindustries: deskilling, dehumanization, disconnection, and disinformation. We\ndescribe these forces, and then we provide additional detail regarding\nattitudes in specific knowledge industries. We conclude with a discussion of\nimplications and research challenges for the HCI community.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.20488,regular,post_llm,2023,10,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'NaijaCoder: Participatory Design for Early Algorithms Education in the\n  Global South\n\n  The majority of Nigerian high schoolers have little to no exposure to the\nbasics of algorithms and programming. We believe this trajectory should change\nas programming offers these students, especially those from indigent\nbackgrounds, an opportunity to learn profitable skills and ignite their\npassions for problem-solving and critical thinking.\n  NaijaCoder is an organization that is dedicated to organizing a free,\nintensive summer program in Nigeria to teach the basics of algorithms and\ncomputer programming to high schoolers. However, the adoption of computer\nscience curriculum has been especially challenging in countries in the global\nsouth that face unique challenges -- such as unstable power supply, internet\nservice, and price volatility. We design a curriculum that is more conducive to\nthe local environment while incorporating rigorous thinking and preparation.\nUsing basic survey designs, we elicit feedback, from the students, designed to\nfurther improve and iterate on our curriculum.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.19754,regular,post_llm,2023,10,"{'ai_likelihood': 8.17908181084527e-06, 'text': 'Rail journey cost calculator for Great Britain\n\n  Accessibility of different places, such as hospitals or areas with jobs, is\nimportant in understanding transportation systems, urban environments, and\npotential inequalities in what services and opportunities different people can\nreach. Often, research in this area is framed around the question of whether\npeople living in an area are able to reach certain destinations within a\nprespecified time frame. However, the cost of such journeys, and whether they\nare affordable, is often omitted or not considered to the same level. Here, we\npresent a Python package and an associated data set which allows to analyse the\ncost of train journeys in Great Britain. We present the original data set we\nused to construct this, the Python package we developed to analyse it, and the\noutput data set which we generated. We envisage our work to allow researchers,\npolicy makers, and other stakeholders, to investigate questions around the cost\nof train journeys, any geographical or social inequalities arising from this,\nand how the transport system could be improved.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.15977,regular,post_llm,2023,10,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'The Conspiracy Money Machine: Uncovering Telegram\'s Conspiracy Channels and their Profit Model\n\nIn recent years, major social media platforms have implemented increasingly strict moderation policies, resulting in bans and restrictions on conspiracy theory-related content. To circumvent these restrictions, conspiracy theorists are turning to alternatives, such as Telegram, where they can express and spread their views with fewer limitations. Telegram offers channels, virtual rooms where only administrators can broadcast messages, and a more permissive content policy. These features have created the perfect breeding ground for a complex ecosystem of conspiracy channels.\n  In this paper, we illuminate this ecosystem. First, we propose an approach to detect conspiracy channels. Then, we discover that conspiracy channels can be clustered into four distinct communities comprising over 17,000 channels. Finally, we uncover the ""Conspiracy Money Machine,"" revealing how most conspiracy channels actively seek to profit from their subscribers. We find conspiracy theorists leverage e-commerce platforms to sell questionable products or lucratively promote them through affiliate links. Moreover, we observe that conspiracy channels use donation and crowdfunding platforms to raise funds for their campaigns. We determine that this business involves hundreds of thousands of donors and generates a turnover of almost $71 million.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.14111,regular,post_llm,2023,10,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""A Student-Dominant View of the Readiness to use Metaverse in Education:\n  The TRI-F Framework\n\n  This paper reports on students' readiness for using Metaverse for education\nin a university in a developing country facing infrastructure and poverty\nchallenges. Covid-19 forced many universities to adopt a hybrid approach to\nteaching and supervision. While online meeting technologies have become\ncommonplace, there is a lack of the connectedness of face-to-face meetings, for\nwhich Metaverse is promoted as a solution. We pose the question as to the level\nof readiness of students to use Metaverse technologies. Thematic analysis of\nstudents' self-reflections on their experience of supervision in a 2D virtual\nworld revealed the usefulness of the technology readiness index model, from\nwhich an extension to the model was proposed to include facilitators for the\napplication of the technology that may mediate the motivators and inhibitors\nwhen assessing readiness to use Metaverse in education settings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.19334,regular,post_llm,2023,10,"{'ai_likelihood': 1.394086413913303e-05, 'text': ""Scalable Two-Minute Feedback: Digital, Lecture-Accompanying Survey as a\n  Continuous Feedback Instrument\n\n  Detailed feedback on courses and lecture content is essential for their\nimprovement and also serves as a tool for reflection. However, feedback methods\nare often only used sporadically, especially in mass courses, because\ncollecting and analyzing feedback in a timely manner is often a challenge for\nteachers. Moreover, the current situation of the students or the changing\nworkload during the semester are usually not taken into account either. For a\nholistic investigation, the article used a digital survey format as formative\nfeedback which attempts to measure student stress in a quantitative part and to\naddress the participants' reflection in a qualitative part, as well as to\ncollect general suggestions for improvement (based on the so-called One-Minute\nPaper) at two educational institutions. The feedback during the semester is\nevaluated qualitatively and discussed on a meta-level and special features\n(e.g. reflections on student work ethic or other courses) are addressed. The\nresults show a low, but constant rate of feedback. Responses mostly cover\ntopics of the lecture content or organizational aspects and were intensively\nused to report issues within the lecture. In addition, artificial intelligence\n(AI) support in the form of a large language model was tested and showed\npromising results in summarizing the open-ended responses for the teacher.\nFinally, the experiences from the lecturers are reflected upon and the results\nas well as possibilities for improvement are discussed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.18792,review,post_llm,2023,10,"{'ai_likelihood': 2.8477774726019966e-06, 'text': ""Privacy as Contextual Integrity in Online Proctoring Systems in Higher\n  Education: A Scoping Review\n\n  Privacy is one of the key challenges to the adoption and implementation of\nonline proctoring systems in higher education. To better understand this\nchallenge, we adopt privacy as contextual integrity theory to conduct a scoping\nreview of 17 papers. The results show different types of students' personal and\nsensitive information are collected and disseminated; this raises considerable\nprivacy concerns. As well as the governing principles including transparency\nand fairness, consent and choice, information minimization, accountability, and\ninformation security and accuracy have been identified to address privacy\nproblems. This study notifies a need to clarify how these principles should be\nimplemented and sustained, and what privacy concerns and actors they relate to.\nFurther, it calls for the need to clarify the responsibility of key actors in\nenacting and sustaining responsible adoption and use of OPS in higher\neducation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.11643,regular,post_llm,2023,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Opinion Change or Differential Turnout: Changing Opinions on the Austin\n  Police Department in a Budget Feedback Process\n\n  In 2020 the tragic murder of George Floyd at the hands of law enforcement\nignited and intensified nationwide protests, demanding changes in police\nfunding and allocation. This happened during a budgeting feedback exercise\nwhere residents of Austin, Texas were invited to share opinions on the budgets\nof various city service areas, including the Police Department, on an online\nplatform designed by our team. Daily responses increased by a hundredfold and\nresponses registered after the ""exogenous shock"" overwhelmingly advocated for\nreducing police funding. This opinion shift far exceeded what we observed in 14\nother Participatory Budgeting elections on our Participatory Budgeting\nPlatform, and can\'t be explained by shifts in the respondent demographics.\nAnalysis of the results from an Austin budgetary feedback exercise in 2021 and\na follow-up survey indicates that the opinion shift from 2020 persisted, with\nthe opinion gap on police funding widening. We conclude that there was an\nactual change of opinion regarding police funding. This study not only sheds\nlight on the enduring impact of the 2020 events and protests on public opinion,\nbut also showcases the value of analysis of clustered opinions as a tool in the\nevaluation toolkit of survey organizers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.02756,regular,post_llm,2023,10,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Modeling of Annual and Daily Electricity Demand of Retrofitted Heat\n  Pumps based on Gas Smart Meter Data\n\n  Currently, gas furnaces are common heating systems in Europe. Due to the\nefforts for decarbonizing the complete energy sector, heat pumps should\ncontinuously replace existing gas furnaces. At the same time, the\nelectrification of the heating sector represents a significant challenge for\nthe power grids and their operators. Thus, new approaches are required to\nestimate the additional electricity demand to operate heat pumps. The\nelectricity required by a heat pump to produce a given amount of heat depends\non the Seasonal Performance Factor (SPF), which is hard to model in theory due\nto many influencing factors and hard to measure in reality as the heat produced\nby a heat pump is usually not measured. Therefore, we show in this paper that\ncollected smart meter data forms an excellent data basis on building level for\nmodeling heat demand and the SPF. We present a novel methodology to estimate\nthe mean SPF based on an unpaired dataset of heat pump electricity and gas\nconsumption data taken from buildings within the same city by comparing the\ndistributions using the Jensen-Shannon Divergence (JSD). Based on a real-world\ndataset, we evaluate this novel method by predicting the electricity demand\nrequired if all gas furnaces in a city were replaced by heat pumps and briefly\nhighlight possible use cases.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.00152,regular,post_llm,2023,10,"{'ai_likelihood': 5.828009711371528e-06, 'text': ""Developing a Tool to Automate Extensions to Support a Flexible Extension\n  Policy\n\n  In this work, we present the development of an automated extension tool to\nassist educators and increase the success and well-being of students by\nimplementing flexible extension policies. Flexible extension policies\nmaterialize in many ways, yet there are similarities in students' interactions\nwith them; students tend to request multi-day long extensions repeatedly. In\ncourses with hundreds or potentially thousands of students, providing a system\nto support this extension request demand is not possible given most currently\navailable resources and limited staff. As such, a tool is necessary to help\nautomate flexible extension processes. The development of this tool should\nreduce staff load while increasing individualized student support, which can be\nused in varying ways for different extension policies. Our research questions\nare: RQ1: Does the extension tool reduce barriers and stigma around asking for\nassistance? RQ2: Does the tool lessen the wait time between requesting and\nreceiving an extension, and how does the tool improve students' learning\nexperience in the course? These questions will help inform us about how an\nautomated tool for flexible extensions helps support growing course sizes and\nstudents who may not otherwise receive the support they need for their success\nand well-being in the course.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.01977,regular,post_llm,2023,10,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Experiences with Research Processes in an Undergraduate Theory of\n  Computing Course\n\n  Theory of computing (ToC) courses are a staple in many undergraduate CS\ncurricula as they lay the foundation of why CS is important to students.\nAlthough not a stated goal, an inevitable outcome of the course is enhancing\nthe students' technical reading and writing abilities as it often contains\nformal reasoning and proof writing. Separately, many undergraduate students are\ninterested in performing research, but often lack these abilities. Based on\nthis observation, we emulated a common research environment within our ToC\ncourse by creating a mock conference assignment, where students (in groups)\nboth wrote a technical paper solving an assigned problem and (individually)\nanonymously refereed other groups' papers. In this paper we discuss the details\nof this assignment and our experiences, and conclude with reflections and\nfuture work about similar courses.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.11806,review,post_llm,2023,10,"{'ai_likelihood': 3.980265723334419e-05, 'text': ""Hierarchical accompanying and inhibiting patterns on the spatial\n  arrangement of taxis' local hotspots\n\n  The spatial arrangement of taxi hotspots indicates their inherent\ndistribution relationships, reflecting their spatial organization structure,\nand has received attention in urban studies. Previous studies have primarily\nexplored large-scale hotspots through visual analysis or simple indices, which\ntypically spans hundreds or even thousands of meters. However, the spatial\narrangement patterns of small-scale hotspots representing specific popular\npick-up and drop-off locations have been largely overlooked. In this study, we\nquantitatively examine the spatial arrangement of local hotspots in Wuhan and\nBeijing, China, using taxi trajectory data. Local hotspots are small-scale\nhotspots with the highest density near the center. Their optimal radius is\nadaptively calculated based on the data, which is 90 m * 90 m and 110 m * 110 m\nin Wuhan and Beijing, respectively. Popular hotspots are typically surrounded\nby less popular ones, although regions with many popular hotspots inhibit the\npresence of less popular ones. These configurations are termed as hierarchical\naccompanying and inhibiting patterns. Finally, inspired by both patterns, a\nKNN-based model is developed to describe these relationships and successfully\nreproduce the spatial distribution of less popular hotspots based on the most\npopular ones. These insights enhance our understanding of local urban\nstructures and support urban planning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.20183,review,post_llm,2023,10,"{'ai_likelihood': 6.033314598931207e-05, 'text': 'Thriving in a Pandemic: Lessons Learned from a Resilient University\n  Program Seen Through the CoI Lens\n\n  In March 2020, college campuses underwent a sudden transformation to online\nlearning due to the COVID-19 outbreak. To understand the impact of COVID-19 on\nstudents\' expectations, this study conducted a three-year survey from ten core\ncourses within the Project Management Center for Excellence at the University\nof Maryland. The study involved two main steps: 1) a statistical analysis to\nevaluate students\' expectations regarding ""student,"" ""class,"" ""instructor,"" and\n""effort;"" and 2) a lexical salience-valence analysis (LSVA) through the lens of\nthe Community of Inquiry (CoI) framework to show the changes of students\'\nexpectations. The results revealed that students\' overall evaluations\nmaintained relatively consistent amid the COVID-19 teaching period. However,\nthere were significant shifts of the student expectations toward Cognitive,\nSocial and Teaching Presence course elements based on LSVA results. Also, clear\ndifferences emerged between under-graduates and graduates in their expectations\nand preferences in course design and delivery. These insights provide practical\nrecommendations for course instructors in designing effective online courses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.04097,regular,post_llm,2023,10,"{'ai_likelihood': 6.861156887478298e-05, 'text': 'Impact of Gender on the Evaluation of Security Decisions\n\n  Security decisions are made by human analysts under uncertain conditions\nwhich leaves room for bias judgement. However, little is known about how\ndemographics like gender and education impact these judgments. We conducted an\nempirical study to investigate their influence on security decision\nevaluations, addressing this knowledge gap.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.15847,regular,post_llm,2023,10,"{'ai_likelihood': 2.410676744249132e-05, 'text': ""A Novel Method for Analysing Racial Bias: Collection of Person Level\n  References\n\n  Long term exposure to biased content in literature or media can significantly\ninfluence people's perceptions of reality, leading to the development of\nimplicit biases that are difficult to detect and address (Gerbner 1998). In\nthis study, we propose a novel method to analyze the differences in\nrepresentation between two groups and use it examine the representation of\nAfrican Americans and White Americans in books between 1850 to 2000 with the\nGoogle Books dataset (Goldberg and Orwant 2013). By developing better tools to\nunderstand differences in representation, we aim to contribute to the ongoing\nefforts to recognize and mitigate biases. To improve upon the more common\nphrase based (men, women, white, black, etc) methods to differentiate context\n(Tripodi et al. 2019, Lucy; Tadimeti, and Bamman 2022), we propose collecting a\ncomprehensive list of historically significant figures and using their names to\nselect relevant context. This novel approach offers a more accurate and nuanced\nmethod for detecting implicit biases through reducing the risk of selection\nbias. We create group representations for each decade and analyze them in an\naligned semantic space (Hamilton, Leskovec, and Jurafsky 2016). We further\nsupport our results by assessing the time adjusted toxicity (Bassignana,\nBasile, and Patti 2018) in the context for each group and identifying the\nsemantic axes (Lucy, Tadimeti, and Bamman 2022) that exhibit the most\nsignificant differences between the groups across decades. We support our\nmethod by showing that our proposed method can capture known socio political\nchanges accurately and our findings indicate that while the relative number of\nAfrican American names mentioned in books have increased over time, the context\nsurrounding them remains more toxic than white Americans.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.17533,review,post_llm,2023,10,"{'ai_likelihood': 8.960564931233725e-05, 'text': 'Decoding The Digital Fuku: Deciphering Colonial Legacies to Critically\n  Assess ChatGPT in Dominican Education\n\n  Educational disparities within the Dominican Republic (DR) have long-standing\norigins rooted in economic, political, and social inequity. Addressing these\nchallenges has necessarily called for capacity building with respect to\neducational materials, high-quality instruction, and structural resourcing.\nGenerative AI tools like ChatGPT have begun to pique the interest of Dominican\neducators due to their perceived potential to bridge these educational gaps.\nHowever, a substantial body of AI fairness literature has documented ways AI\ndisproportionately reinforces power dynamics reflective of jurisdictions\ndriving AI development and deployment policies, collectively termed the AI\nGlobal North. As such, indiscriminate adoption of this technology for DR\neducation, even in part, risks perpetuating forms of digital coloniality.\nTherefore, this paper centers embracing AI-facilitated educational reform by\ncritically examining how AI-driven tools like ChatGPT in DR education may\nreplicate facets of digital colonialism. We provide a concise overview of\n20th-century Dominican education reforms following the 1916 US occupation.\nThen, we employ identified neocolonial aspects historically shaping Dominican\neducation to interrogate the perceived advantages of ChatGPT for contemporary\nDominican education, as outlined by a Dominican scholar. This work invites AI\nGlobal North & South developers, stakeholders, and Dominican leaders alike to\nexercise a relational contextualization of data-centric epistemologies like\nChatGPT to reap its transformative benefits while remaining vigilant of\nsafeguarding Dominican digital sovereignty.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10741,review,post_llm,2023,10,"{'ai_likelihood': 8.675787183973525e-06, 'text': 'Data Equity: Foundational Concepts for Generative AI\n\n  This briefing paper focuses on data equity within foundation models, both in\nterms of the impact of Generative AI (genAI) on society and on the further\ndevelopment of genAI tools. GenAI promises immense potential to drive digital\nand social innovation, such as improving efficiency, enhancing creativity and\naugmenting existing data. GenAI has the potential to democratize access and\nusage of technologies. However, left unchecked, it could deepen inequities.\nWith the advent of genAI significantly increasing the rate at which AI is\ndeployed and developed, exploring frameworks for data equity is more urgent\nthan ever. The goals of the briefing paper are threefold: to establish a shared\nvocabulary to facilitate collaboration and dialogue; to scope initial concerns\nto establish a framework for inquiry on which stakeholders can focus; and to\nshape future development of promising technologies. The paper represents a\nfirst step in exploring and promoting data equity in the context of genAI. The\nproposed definitions, framework and recommendations are intended to proactively\nshape the development of promising genAI technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.08133,regular,post_llm,2023,10,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Multi Level Dense Layer Neural Network Model for Housing Price\n  Prediction\n\n  Predicting the price of a house remains a challenging issue that needs to be\naddressed. Research has attempted to establish a model with different methods\nand algorithms to predict the housing price, from the traditional hedonic model\nto a neural network algorithm. However, many existing algorithms in the\nliterature are proposed without any finetuning and customization in the model.\nIn this paper, the author attempted to propose a novel neural network-based\nmodel to improve the performance of housing price prediction. Inspired by the\nmodular neural network, the proposed model consists of a three-level neural\nnetwork that is capable to process information in parallel. The author compared\nseveral state-of-the-art algorithms available in the literature on the Boston\nhousing dataset to evaluate the effectiveness of the proposed model. The\nresults show that the proposed model provides better accuracy and outperforms\nexisting algorithms in different evaluation metrics. The code for the\nimplementation is available\nhttps://github.com/wijayarobert/MultiLevelDenseLayerNN\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.00949,regular,post_llm,2023,10,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Unveiling Technorelief: Enhancing Neurodiverse Collaboration with Media\n  Capabilities\n\n  As the workforce settles into flexible work arrangements, researchers have\nfocused on the collaborative and psychological consequences of the shift. While\nnearly a fifth of the world\'s population is estimated to be neurodivergent, the\nimplications of remote collaboration on the cognitive, sensory, and\nsocio-affective experiences of autistic workers are poorly understood. Prior\nliterature suggests that information and communication technologies (ICTs)\nintroduce major psychological stressors. Theoretically, these stressors ought\nto be exceptionally straining considering autistic traits $\\unicode{x2013}$\nyet, studies describe a strong attraction to ICTs. We thus ask: how do digital\ntechnologies alleviate autistic workers\' experiences of their collaborative\nwork environment? Thirty-three interviews were conducted to address this\nquestion. Findings suggest that digital media present capabilities that filter\ninput from the environment, turning it into a virtual stage that lets workers\n""time out"". The resulting ""technorelief"" enables autistic workers to tune into\ntheir perceptions and regain control of their collaborative experiences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.12696,regular,post_llm,2023,10,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Protection from Evil and Good: The Differential Effects of Page\n  Protection on Wikipedia Article Quality\n\n  Wikipedia, the Web\'s largest encyclopedia, frequently faces content disputes\nor malicious users seeking to subvert its integrity. Administrators can\nmitigate such disruptions by enforcing ""page protection"" that selectively\nlimits contributions to specific articles to help prevent the degradation of\ncontent. However, this practice contradicts one of Wikipedia\'s fundamental\nprinciples$-$that it is open to all contributors$-$and may hinder further\nimprovement of the encyclopedia. In this paper, we examine the effect of page\nprotection on article quality to better understand whether and when page\nprotections are warranted. Using decade-long data on page protections from the\nEnglish Wikipedia, we conduct a quasi-experimental study analyzing pages that\nreceived ""requests for page protection""$-$written appeals submitted by\nWikipedia editors to administrators to impose page protections. We match pages\nthat indeed received page protection with similar pages that did not and\nquantify the causal effect of the interventions on a well-established measure\nof article quality. Our findings indicate that the effect of page protection on\narticle quality depends on the characteristics of the page prior to the\nintervention: high-quality articles are affected positively as opposed to\nlow-quality articles that are impacted negatively. Subsequent analysis suggests\nthat high-quality articles degrade when left unprotected, whereas low-quality\narticles improve. Overall, with our study, we outline page protections on\nWikipedia and inform best practices on whether and when to protect an article.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.00828,regular,post_llm,2023,10,"{'ai_likelihood': 0.0007534027099609375, 'text': 'A Model for Calculating Cost of Applying Electronic Governance and\n  Robotic Process Automation to a Distributed Management System\n\n  Electronic Governance (eGov) and Robotic Process Automation (RPA) are two\ntechnological advancements that have the potential to revolutionize the way\norganizations manage their operations. When applied to Distributed Management\n(DM), these technologies can further enhance organizational efficiency and\neffectiveness. In this brief article, we present a mathematical model for\ncalculating the cost of accomplishing a task by applying eGov and RPA in a DM\nsystem. This model is one of the first of its kind, and is expected to spark\nfurther research on cost analysis for organizational efficiency given the\nunprecedented advancements in electronic and automation technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.08349,review,post_llm,2023,10,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Performativity and Prospective Fairness\n\n  Deploying an algorithmically informed policy is a significant intervention in\nthe structure of society. As is increasingly acknowledged, predictive\nalgorithms have performative effects: using them can shift the distribution of\nsocial outcomes away from the one on which the algorithms were trained.\nAlgorithmic fairness research is usually motivated by the worry that these\nperformative effects will exacerbate the structural inequalities that gave rise\nto the training data. However, standard retrospective fairness methodologies\nare ill-suited to predict these effects. They impose static fairness\nconstraints that hold after the predictive algorithm is trained, but before it\nis deployed and, therefore, before performative effects have had a chance to\nkick in. However, satisfying static fairness criteria after training is not\nsufficient to avoid exacerbating inequality after deployment. Addressing the\nfundamental worry that motivates algorithmic fairness requires explicitly\ncomparing the change in relevant structural inequalities before and after\ndeployment. We propose a prospective methodology for estimating this\npost-deployment change from pre-deployment data and knowledge about the\nalgorithmic policy. That requires a strategy for distinguishing between, and\naccounting for, different kinds of performative effects. In this paper, we\nfocus on the algorithmic effect on the causally downstream outcome variable.\nThroughout, we are guided by an application from public administration: the use\nof algorithms to (1) predict who among the recently unemployed will stay\nunemployed for the long term and (2) targeting them with labor market programs.\nWe illustrate our proposal by showing how to predict whether such policies will\nexacerbate gender inequalities in the labor market.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.06475,review,post_llm,2023,10,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Approaches to the Algorithmic Allocation of Public Resources: A\n  Cross-disciplinary Review\n\n  Allocation of scarce resources is a recurring challenge for the public\nsector: something that emerges in areas as diverse as healthcare, disaster\nrecovery, and social welfare. The complexity of these policy domains and the\nneed for meeting multiple and sometimes conflicting criteria has led to\nincreased focus on the use of algorithms in this type of decision. However,\nlittle engagement between researchers across these domains has happened,\nmeaning a lack of understanding of common problems and techniques for\napproaching them. Here, we performed a cross disciplinary literature review to\nunderstand approaches taken for different areas of algorithmic allocation\nincluding healthcare, organ transplantation, homelessness, disaster relief, and\nwelfare. We initially identified 1070 papers by searching the literature, then\nsix researchers went through them in two phases of screening resulting in 176\nand 75 relevant papers respectively. We then analyzed the 75 papers from the\nlenses of optimization goals, techniques, interpretability, flexibility, bias,\nethical considerations, and performance. We categorized approaches into\nhuman-oriented versus resource-oriented perspective, and individual versus\naggregate and identified that 76% of the papers approached the problem from a\nhuman perspective and 60% from an aggregate level using optimization\ntechniques. We found considerable potential for performance gains, with\noptimization techniques often decreasing waiting times and increasing success\nrate by as much as 50%. However, there was a lack of attention to responsible\ninnovation: only around one third of the papers considered ethical issues in\nchoosing the optimization goals while just a very few of them paid attention to\nthe bias issues. Our work can serve as a guide for policy makers and\nresearchers wanting to use an algorithm for addressing a resource allocation\nproblem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.01278,regular,post_llm,2023,10,"{'ai_likelihood': 3.013345930311415e-06, 'text': 'Open and Linked Data Model for Carbon Footprint Scenarios\n\n  Carbon footprint quantification is key to well-informed decision making over\ncarbon reduction potential, both for individuals and for companies. Many carbon\nfootprint case studies for products and services have been circulated recently.\nDue to the complex relationships within each scenario, however, the underlying\nassumptions often are difficult to understand. Also, re-using and adapting a\nscenario to local or individual circumstances is not a straightforward task. To\novercome these challenges, we propose an open and linked data model for carbon\nfootprint scenarios which improves data quality and transparency by design. We\ndemonstrate the implementation of our idea with a web-based data interpreter\nprototype.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.01673,regular,post_llm,2023,10,"{'ai_likelihood': 0.12091742621527778, 'text': ""A Versatile Data Fabric for Advanced IoT-Based Remote Health Monitoring\n\n  This paper presents a data-centric and security-focused data fabric designed\nfor digital health applications. With the increasing interest in digital health\nresearch, there has been a surge in the volume of Internet of Things (IoT) data\nderived from smartphones, wearables, and ambient sensors. Managing this vast\namount of data, encompassing diverse data types and varying time scales, is\ncrucial. Moreover, compliance with regulatory and contractual obligations is\nessential. The proposed data fabric comprises an architecture and a toolkit\nthat facilitate the integration of heterogeneous data sources, across different\nenvironments, to provide a unified view of the data in dashboards. Furthermore,\nthe data fabric supports the development of reusable and configurable data\nintegration components, which can be shared as open-source or inner-source\nsoftware. These components are used to generate data pipelines that can be\ndeployed and scheduled to run either in the cloud or on-premises. Additionally,\nwe present the implementation of our data fabric in a home-based telemonitoring\nresearch project involving older adults, conducted in collaboration with the\nUniversity of California, San Diego (UCSD). The study showcases the streamlined\nintegration of data collected from various IoT sensors and mobile applications\nto create a unified view of older adults' health for further analysis and\nresearch.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.07314,regular,post_llm,2023,10,"{'ai_likelihood': 6.258487701416016e-06, 'text': 'Adaptive and Gamified Learning Paths with Polyglot and .NET Interactive\n\n  The digital age is changing the role of educators and pushing for a paradigm\nshift in the education system as a whole. Growing demand for general and\nspecialized education inside and outside classrooms is at the heart of this\nrising trend. In modern, heterogeneous learning environments, the\none-size-fits-all approach is proven to be fundamentally flawed.\nIndividualization through adaptivity is, therefore, crucial to nurture\nindividual potential and address accessibility needs and neurodiversity. By\nformalizing a learning framework that takes into account all these different\naspects, we aim to define and implement an open, content-agnostic, and\nextensible platform to design and consume adaptive and gamified learning\nexperiences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.05476,review,post_llm,2023,10,"{'ai_likelihood': 4.56637806362576e-05, 'text': 'Revitalizing education through ict: a short overview of japan\'s current\n  landscape\n\n  The domain of Information and Communication Technology (ICT) education has\ngarnered significant consideration in recent times. However, several challenges\nare inherent to this area of study, including monetary expense, temporal\nfactors, pedagogical environment, teacher training programs, incentive,\nsyllabus design, and health-related concerns. This paper presents an analysis\nof the difficulties encountered in the realm of ICT education in Japan, taking\ninto account ten different perspectives. A peer-reviewed article of this\nPreprint also exists ""Fujita, T. (2023). REVITALIZING EDUCATION THROUGH ICT: A\nSHORT OVERVIEW OF JAPAN\'S CURRENT LANDSCAPE. European Journal of Social\nSciences Studies, 8(5).""\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.20435,review,post_llm,2023,10,"{'ai_likelihood': 0.47688802083333337, 'text': 'Assessing the Sustainability and Trustworthiness of Federated Learning Models\n\nArtificial intelligence (AI) increasingly influences critical decision-making across sectors. Federated Learning (FL), as a privacy-preserving collaborative AI paradigm, not only enhances data protection but also holds significant promise for intelligent network management, including distributed monitoring, adaptive control, and edge intelligence. Although the trustworthiness of FL systems has received growing attention, the sustainability dimension remains insufficiently explored, despite its importance for scalable real-world deployment. To address this gap, this work introduces sustainability as a distinct pillar within a comprehensive trustworthy FL taxonomy, consistent with AI-HLEG guidelines. This pillar includes three key aspects: hardware efficiency, federation complexity, and the carbon intensity of energy sources. Experiments using the FederatedScope framework under diverse scenarios, including varying participants, system complexity, hardware, and energy configurations, validate the practicality of the approach. Results show that incorporating sustainability into FL evaluation supports environmentally responsible deployment, enabling more efficient, adaptive, and trustworthy network services and management AI models.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.19201,review,post_llm,2023,10,"{'ai_likelihood': 1.0066562228732639e-05, 'text': 'Open Problems in DAOs\n\n  Decentralized autonomous organizations (DAOs) are a new, rapidly-growing\nclass of organizations governed by smart contracts. Here we describe how\nresearchers can contribute to the emerging science of DAOs and other\ndigitally-constituted organizations. From granular privacy primitives to\nmechanism designs to model laws, we identify high-impact problems in the DAO\necosystem where existing gaps might be tackled through a new data set or by\napplying tools and ideas from existing research fields such as political\nscience, computer science, economics, law, and organizational science. Our\nrecommendations encompass exciting research questions as well as promising\nbusiness opportunities. We call on the wider research community to join the\nglobal effort to invent the next generation of organizations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.02982,regular,post_llm,2023,10,"{'ai_likelihood': 1.8311871422661676e-05, 'text': 'Are LLMs Useful in the Poorest Schools? TheTeacher.AI in Sierra Leone\n\n  Education systems in developing countries have few resources to serve large,\npoor populations. How might generative AI integrate into classrooms? This paper\nintroduces an AI chatbot designed to assist teachers in Sierra Leone with\nprofessional development to improve their instruction. We describe initial\nfindings from early implementation across 122 schools and 193 teachers, and\nanalyze its use with qualitative observations and by analyzing queries.\nTeachers use the system for lesson planning, classroom management, and subject\nmatter. Usage is sustained over the school year, and a subset of teachers use\nthe system more regularly. We draw conclusions from these findings about how\ngenerative AI systems can be integrated into school systems in low income\ncountries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10732,regular,post_llm,2023,10,"{'ai_likelihood': 1.0, 'text': ""White Paper: The Generative Education (GenEd) Framework\n\n  The Generative Education (GenEd) Framework explores the transition from Large\nLanguage Models (LLMs) to Large Multimodal Models (LMMs) in education,\nenvisioning a harmonious relationship between AI and educators to enhance\nlearning experiences. This paper delves into the potential of LMMs to create\npersonalized, interactive, and emotionally-aware learning environments. Through\naddressing the Two-Sigma problem and the introduction of a conceptual product\nnamed Harmony, the narrative emphasizes educator development, adapting policy\nframeworks, and fostering cross-sector collaboration to realize the envisioned\nAI-enhanced education landscape. The discussion underscores the urgency for\nproactive adaptation amidst AI's evolution, offering a pragmatic roadmap to\nnavigate the technical, ethical, and policy intricacies of integrating AI in\neducation.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0015125274658203125, 'GPT4': 0.8623046875, 'CLAUDE': 0.0038776397705078125, 'GOOGLE': 0.07757568359375, 'OPENAI_O_SERIES': 0.01172637939453125, 'DEEPSEEK': 0.021026611328125, 'GROK': 0.0027942657470703125, 'NOVA': 0.0033435821533203125, 'OTHER': 0.0157623291015625, 'HUMAN': 1.329183578491211e-05}}"
2310.17475,regular,post_llm,2023,10,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Analytical model for large-scale design of sidewalk delivery robot\n  systems\n\n  With the rise in demand for local deliveries and e-commerce, robotic\ndeliveries are being considered as efficient and sustainable solutions.\nHowever, the deployment of such systems can be highly complex due to numerous\nfactors involving stochastic demand, stochastic charging and maintenance needs,\ncomplex routing, etc. We propose a model that uses continuous approximation\nmethods for evaluating service trade-offs that consider the unique\ncharacteristics of large-scale sidewalk delivery robot systems used to serve\nonline food deliveries. The model captures both the initial cost and the\noperation cost of the delivery system and evaluates the impact of constraints\nand operation strategies on the deployment. By minimizing the system cost,\nvariables related to the system design can be determined. First, the\nminimization problem is formulated based on a homogeneous area, and the optimal\nsystem cost can be derived as a closed-form expression. By evaluating the\nexpression, relationships between variables and the system cost can be directly\nobtained. We then apply the model in neighborhoods in New York City to evaluate\nthe cost of deploying the sidewalk delivery robot system in a real-world\nscenario. The results shed light on the potential of deploying such a system in\nthe future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.0934,regular,post_llm,2023,10,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Geo-knowledge-guided GPT models improve the extraction of location\n  descriptions from disaster-related social media messages\n\n  Social media messages posted by people during natural disasters often contain\nimportant location descriptions, such as the locations of victims. Recent\nresearch has shown that many of these location descriptions go beyond simple\nplace names, such as city names and street names, and are difficult to extract\nusing typical named entity recognition (NER) tools. While advanced machine\nlearning models could be trained, they require large labeled training datasets\nthat can be time-consuming and labor-intensive to create. In this work, we\npropose a method that fuses geo-knowledge of location descriptions and a\nGenerative Pre-trained Transformer (GPT) model, such as ChatGPT and GPT-4. The\nresult is a geo-knowledge-guided GPT model that can accurately extract location\ndescriptions from disaster-related social media messages. Also, only 22\ntraining examples encoding geo-knowledge are used in our method. We conduct\nexperiments to compare this method with nine alternative approaches on a\ndataset of tweets from Hurricane Harvey. Our method demonstrates an over 40%\nimprovement over typically used NER approaches. The experiment results also\nshow that geo-knowledge is indispensable for guiding the behavior of GPT\nmodels. The extracted location descriptions can help disaster responders reach\nvictims more quickly and may even save lives.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.1726,review,post_llm,2023,10,"{'ai_likelihood': 6.9207615322536894e-06, 'text': 'Socially Beneficial Metaverse: Framework, Technologies, Applications,\n  and Challenges\n\n  In recent years, the maturation of emerging technologies such as Virtual\nReality, Digital twins, and Blockchain has accelerated the realization of the\nmetaverse. As a virtual world independent of the real world, the metaverse will\nprovide users with a variety of virtual activities that bring great convenience\nto society. In addition, the metaverse can facilitate digital twins, which\noffers transformative possibilities for the industry. Thus, the metaverse has\nattracted the attention of the industry, and a huge amount of capital is about\nto be invested. However, the development of the metaverse is still in its\ninfancy and little research has been undertaken so far. We describe the\ndevelopment of the metaverse. Next, we introduce the architecture of the\nsocially beneficial metaverse (SB-Metaverse) and we focus on the technologies\nthat support the operation of SB-Metaverse. In addition, we also present the\napplications of SB-Metaverse. Finally, we discuss several challenges faced by\nSB-Metaverse which must be addressed in the future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.13625,review,post_llm,2023,10,"{'ai_likelihood': 2.7815500895182294e-06, 'text': 'Oversight for Frontier AI through a Know-Your-Customer Scheme for\n  Compute Providers\n\n  To address security and safety risks stemming from highly capable artificial\nintelligence (AI) models, we propose that the US government should ensure\ncompute providers implement Know-Your-Customer (KYC) schemes. Compute - the\ncomputational power and infrastructure required to train and run these AI\nmodels - is emerging as a node for oversight. KYC, a standard developed by the\nbanking sector to identify and verify client identity, could provide a\nmechanism for greater public oversight of frontier AI development and close\nloopholes in existing export controls. Such a scheme has the potential to\nidentify and warn stakeholders of potentially problematic and/or sudden\nadvancements in AI capabilities, build government capacity for AI regulation,\nand allow for the development and implementation of more nuanced and targeted\nexport controls. Unlike the strategy of limiting access to AI chip purchases,\nregulating the digital access to compute offers more precise controls, allowing\nregulatory control over compute quantities, as well as the flexibility to\nsuspend access at any time. To enact a KYC scheme, the US government will need\nto work closely with industry to (1) establish a dynamic threshold of compute\nthat effectively captures high-risk frontier model development, while\nminimizing imposition on developers not engaged in frontier AI; (2) set\nrequirements and guidance for compute providers to keep records and report\nhigh-risk entities; (3) establish government capacity that allows for\nco-design, implementation, administration and enforcement of the scheme; and\n(4) engage internationally to promote international alignment with the scheme\nand support its long-term efficacy. While the scheme will not address all AI\nrisks, it complements proposed solutions by allowing for a more precise and\nflexible approach to controlling the development of frontier AI models and\nunwanted AI proliferation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.13595,review,post_llm,2023,10,"{'ai_likelihood': 3.245141771104601e-06, 'text': ""The History and Risks of Reinforcement Learning and Human Feedback\n\n  Reinforcement learning from human feedback (RLHF) has emerged as a powerful\ntechnique to make large language models (LLMs) easier to use and more\neffective. A core piece of the RLHF process is the training and utilization of\na model of human preferences that acts as a reward function for optimization.\nThis approach, which operates at the intersection of many stakeholders and\nacademic disciplines, remains poorly understood. RLHF reward models are often\ncited as being central to achieving performance, yet very few descriptors of\ncapabilities, evaluations, training methods, or open-source models exist. Given\nthis lack of information, further study and transparency is needed for learned\nRLHF reward models. In this paper, we illustrate the complex history of\noptimizing preferences, and articulate lines of inquiry to understand the\nsociotechnical context of reward models. In particular, we highlight the\nontological differences between costs, rewards, and preferences at stake in\nRLHF's foundations, related methodological tensions, and possible research\ndirections to improve general understanding of how reward models function.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.06056,regular,post_llm,2023,10,"{'ai_likelihood': 1.9636419084337023e-05, 'text': 'An Automated Tool to Detect Suicidal Susceptibility from Social Media\n  Posts\n\n  The World Health Organization (WHO) estimated that approximately 1.4 million\nindividuals worldwide died by suicide in 2022. This figure indicates that one\nperson died by suicide every 20 s during the year. Globally, suicide is the\ntenth-leading cause of death, while it is the second-leading cause of death\namong young people aged 15329 years. In 2022, it was estimated that\napproximately 10.5 million suicide attempts would occur. The WHO suggests that\nalong with each completed suicide attempt, many individuals attempt suicide.\nToday, social media is a place in which people share their feelings. Thus,\nsocial media can help us understand the thoughts and possible actions of\nindividuals. This study leverages this advantage and focuses on developing an\nautomated model to use information from social media to determine whether\nsomeone is contemplating self-harm. This model is based on the Suicidal-ELECTRA\nmodel. We collected datasets of social media posts, processed them, and used\nthem to train and fiune-tune our model. Evaluation of the refined model with a\ntesting dataset consistently yielded outstanding results. The model had an\nimpressive accuracy rate of 93% and commendable F1 score of 0.93. Additionally,\nwe developed an application programming interface that seamlessly integrated\nour tool with third-party platforms, enhancing its implementation potential to\naddress the concern of rising suicide rates.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.04997,review,post_llm,2023,10,"{'ai_likelihood': 1.5033615960015191e-05, 'text': 'Unmasking Biases and Navigating Pitfalls in the Ophthalmic Artificial\n  Intelligence Lifecycle: A Review\n\n  Over the past two decades, exponential growth in data availability,\ncomputational power, and newly available modeling techniques has led to an\nexpansion in interest, investment, and research in Artificial Intelligence (AI)\napplications. Ophthalmology is one of many fields that seek to benefit from AI\ngiven the advent of telemedicine screening programs and the use of ancillary\nimaging. However, before AI can be widely deployed, further work must be done\nto avoid the pitfalls within the AI lifecycle. This review article breaks down\nthe AI lifecycle into seven steps: data collection; defining the model task;\ndata pre-processing and labeling; model development; model evaluation and\nvalidation; deployment; and finally, post-deployment evaluation, monitoring,\nand system recalibration and delves into the risks for harm at each step and\nstrategies for mitigating them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.04824,review,post_llm,2023,10,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'PaperCard for Reporting Machine Assistance in Academic Writing\n\n  Academic writing process has benefited from various technological\ndevelopments over the years including search engines, automatic translators,\nand editing tools that review grammar and spelling mistakes. They have enabled\nhuman writers to become more efficient in writing academic papers, for example\nby helping with finding relevant literature more effectively and polishing\ntexts. While these developments have so far played a relatively assistive role,\nrecent advances in large-scale language models (LLMs) have enabled LLMs to play\na more major role in the writing process, such as coming up with research\nquestions and generating key contents. This raises critical questions\nsurrounding the concept of authorship in academia. ChatGPT, a\nquestion-answering system released by OpenAI in November 2022, has demonstrated\na range of capabilities that could be utilised in producing academic papers.\nThe academic community will have to address relevant pressing questions,\nincluding whether Artificial Intelligence (AI) should be merited authorship if\nit made significant contributions in the writing process, or whether its use\nshould be restricted such that human authorship would not be undermined. In\nthis paper, we aim to address such questions, and propose a framework we name\n""PaperCard"", a documentation for human authors to transparently declare the use\nof AI in their writing process.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10728,regular,post_llm,2023,10,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Improving Feedback from Automated Reviews of Student Spreadsheets\n\n  Spreadsheets are one of the most widely used tools for end users. As a\nresult, spreadsheets such as Excel are now included in many curricula. However,\ndigital solutions for assessing spreadsheet assignments are still scarce in the\nteaching context. Therefore, we have developed an Intelligent Tutoring System\n(ITS) to review students' Excel submissions and provide individualized feedback\nautomatically. Although the lecturer only needs to provide one reference\nsolution, the students' submissions are analyzed automatically in several ways:\nvalue matching, detailed analysis of the formulas, and quality assessment of\nthe solution. To take the students' learning level into account, we have\ndeveloped feedback levels for an ITS that provide gradually more information\nabout the error by using one of the different analyses. Feedback at a higher\nlevel has been shown to lead to a higher percentage of correct submissions and\nwas also perceived as well understandable and helpful by the students.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.04628,review,post_llm,2023,10,"{'ai_likelihood': 1.8543667263454863e-06, 'text': ""(Re)framing Built Heritage through the Machinic Gaze\n\n  Built heritage has been both subject and product of a gaze that has been\nsustained through moments of colonial fixation on ruins and monuments,\ntechnocratic examination and representation, and fetishisation by aglobal\ntourist industry. We argue that the recent proliferation of machine learning\nand vision technologies create new scopic regimes for heritage: storing and\nretrieving existing images from vast digital archives, and further imparting\ntheir own distortions upon its visual representation. We introduce the term\n`machinic gaze' to conceptualise the reconfiguration of heritage representation\nvia AI models. To explore how this gaze reframes heritage, we deploy an\nimage-text-image pipeline that reads, interprets, and resynthesizes images of\nseveral UNESCO World Heritage Sites. Employing two concepts from media studies\n-- heteroscopia and anamorphosis -- we describe the reoriented perspective that\nmachine vision systems introduce. We propose that the machinic gaze highlights\nthe artifice of the human gaze and its underlying assumptions and practices\nthat combine to form established notions of heritage.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.01711,regular,post_llm,2023,10,"{'ai_likelihood': 4.470348358154297e-06, 'text': 'Learning Class-Specific Spectral Patterns to Improve Deep Learning Based\n  Scene-Level Fire Smoke Detection from Multi-Spectral Satellite Imagery\n\n  Detecting fire smoke is crucial for the timely identification of early\nwildfires using satellite imagery. However, the spatial and spectral similarity\nof fire smoke to other confounding aerosols, such as clouds and haze, often\nconfuse even the most advanced deep-learning (DL) models. Nonetheless, these\naerosols also present distinct spectral characteristics in some specific bands,\nand such spectral patterns are useful for distinguishing the aerosols more\naccurately. Early research tried to derive various threshold values from the\nreflectance and brightness temperature in specific spectral bands to\ndifferentiate smoke and cloud pixels. However, such threshold values were\ndetermined based on domain knowledge and are hard to generalise. In addition,\nsuch threshold values were manually derived from specific combinations of bands\nto infer spectral patterns, making them difficult to employ in deep-learning\nmodels. In this paper, we introduce a DL module called input amplification\n(InAmp) which is designed to enable DL models to learn class-specific spectral\npatterns automatically from multi-spectral satellite imagery and improve the\nfire smoke detection accuracy. InAmp can be conveniently integrated with\ndifferent DL architectures. We evaluate the effectiveness of the InAmp module\non different Convolutional neural network (CNN) architectures using two\nsatellite imagery datasets: USTC_SmokeRS, derived from Moderate Resolution\nImaging Spectroradiometer (MODIS) with three spectral bands; and Landsat_Smk,\nderived from Landsat 5/8 with six spectral bands. Our experimental results\ndemonstrate that the InAmp module improves the fire smoke detection accuracy of\nthe CNN models. Additionally, we visualise the spectral patterns extracted by\nthe InAmp module using test imagery and demonstrate that the InAmp module can\neffectively extract class-specific spectral patterns.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10744,review,post_llm,2023,10,"{'ai_likelihood': 1.0331471761067709e-05, 'text': ""Advancing a Model of Students' Intentional Persistence in Machine\n  Learning and Artificial Intelligence\n\n  Machine Learning (ML) and Artificial Intelligence (AI) are powering the\napplications we use, the decisions we make, and the decisions made about us. We\nhave seen numerous examples of non-equitable outcomes, from facial recognition\nalgorithms to recidivism algorithms, when they are designed without diversity\nin mind. Thus, we must take action to promote diversity among those in this\nfield. A critical step in this work is understanding why some students who\nchoose to study ML/AI later leave the field. While the persistence of diverse\npopulations has been studied in engineering, there is a lack of research\ninvestigating factors that influence persistence in ML/AI. In this work, we\npresent the advancement of a model of intentional persistence in ML/AI by\nsurveying students in ML/AI courses. We examine persistence across demographic\ngroups, such as gender, international student status, student loan status, and\nvisible minority status. We investigate independent variables that distinguish\nML/AI from other STEM fields, such as the varying emphasis on non-technical\nskills, the ambiguous ethical implications of the work, and the highly\ncompetitive and lucrative nature of the field. Our findings suggest that\nshort-term intentional persistence is associated with academic enrollment\nfactors such as major and level of study. Long-term intentional persistence is\ncorrelated with measures of professional role confidence. Unique to our study,\nwe show that wanting your work to have a positive social benefit is a negative\npredictor of long-term intentional persistence, and women generally care more\nabout this. We provide recommendations to educators to meaningfully discuss\nML/AI ethics in classes and encourage the development of interpersonal skills\nto help increase diversity in the field.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.17888,review,post_llm,2023,10,"{'ai_likelihood': 6.02669186062283e-06, 'text': ""Large Language Models as Subpopulation Representative Models: A Review\n\n  Of the many commercial and scientific opportunities provided by large\nlanguage models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and\nAnthropic's Claude), one of the more intriguing applications has been the\nsimulation of human behavior and opinion. LLMs have been used to generate human\nsimulcra to serve as experimental participants, survey respondents, or other\nindependent agents, with outcomes that often closely parallel the observed\nbehavior of their genuine human counterparts. Here, we specifically consider\nthe feasibility of using LLMs to estimate subpopulation representative models\n(SRMs). SRMs could provide an alternate or complementary way to measure public\nopinion among demographic, geographic, or political segments of the population.\nHowever, the introduction of new technology to the socio-technical\ninfrastructure does not come without risk. We provide an overview of behavior\nelicitation techniques for LLMs, and a survey of existing SRM implementations.\nWe offer frameworks for the analysis, development, and practical implementation\nof LLMs as SRMs, consider potential risks, and suggest directions for future\nwork.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.08532,regular,post_llm,2023,10,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Platform for generating medical datasets for machine learning in public\n  health\n\n  Currently, there are many difficulties regarding the interoperability of\nmedical data and related population data sources. These complications get in\nthe way of the generation of high-quality data sets at city, region and\nnational levels. Moreover, the collection of datasets within large medical\ncenters is feasible due to own IT departments whereas the collection of raw\nmedical data from multiple organizations is a more complicated process. In\nthese circumstances, the most appropriate option is to develop digital products\nbased on microservice architecture. Because of this approach, it is possible to\nensure the multimodality of the system, the flexibility of the interface and\nthe internal system approach, when interconnected elements behave as a whole,\ndemonstrating behavior different from the behavior when working independently.\nThese conditions allow, in turn, to ensure the maximum number and\nrepresentativeness of the resulting data sets. This paper demonstrates a\nconcept of the platform for a sustainable generation of quality and reliable\nsets of multimodal medical data. It collects data from different external\nsources, harmonizes it using a special service, anonymizes harmonized data, and\nlabels processed data. The proposed system aims to be a promising solution to\nthe improvement of medical data quality for machine learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.02521,review,post_llm,2023,10,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Who Audits the Auditors? Recommendations from a field scan of the\n  algorithmic auditing ecosystem\n\n  AI audits are an increasingly popular mechanism for algorithmic\naccountability; however, they remain poorly defined. Without a clear\nunderstanding of audit practices, let alone widely used standards or regulatory\nguidance, claims that an AI product or system has been audited, whether by\nfirst-, second-, or third-party auditors, are difficult to verify and may\nexacerbate, rather than mitigate, bias and harm. To address this knowledge gap,\nwe provide the first comprehensive field scan of the AI audit ecosystem. We\nshare a catalog of individuals (N=438) and organizations (N=189) who engage in\nalgorithmic audits or whose work is directly relevant to algorithmic audits;\nconduct an anonymous survey of the group (N=152); and interview industry\nleaders (N=10). We identify emerging best practices as well as methods and\ntools that are becoming commonplace, and enumerate common barriers to\nleveraging algorithmic audits as effective accountability mechanisms. We\noutline policy recommendations to improve the quality and impact of these\naudits, and highlight proposals with wide support from algorithmic auditors as\nwell as areas of debate. Our recommendations have implications for lawmakers,\nregulators, internal company policymakers, and standards-setting bodies, as\nwell as for auditors. They are: 1) require the owners and operators of AI\nsystems to engage in independent algorithmic audits against clearly defined\nstandards; 2) notify individuals when they are subject to algorithmic\ndecision-making systems; 3) mandate disclosure of key components of audit\nfindings for peer review; 4) consider real-world harm in the audit process,\nincluding through standardized harm incident reporting and response mechanisms;\n5) directly involve the stakeholders most likely to be harmed by AI systems in\nthe algorithmic audit process; and 6) formalize evaluation and, potentially,\naccreditation of algorithmic auditors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2310.04739,review,post_llm,2023,10,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Link, user-centred designer: Game characters as transcendent models\n\n  Games allow us to construct and explore identities and offer us role models,\ngood and bad. Game characters are a reflection of us -- players and creators\nalike -- or could be. But do games also encode identities, values, and\norientations that transcend diegetic categories and player self-insertion? I\nexplore the notion of game characters as conduits of transcendent models\nthrough the case study of Link from the Legend of Zelda series. I propose that\ndesigners embed tacit, nondiegetic patterns of praxis and complex value models,\nsuch as user-centred design, when crafting the embodiment of characters in\ngameplay, even unawares.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09518,regular,post_llm,2023,11,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""From GPT-3 to GPT-4: On the Evolving Efficacy of LLMs to Answer\n  Multiple-choice Questions for Programming Classes in Higher Education\n\n  We explore the evolving efficacy of three generative pre-trained transformer\n(GPT) models in generating answers for multiple-choice questions (MCQ) from\nintroductory and intermediate Python programming courses in higher education.\nWe focus on the differences in capabilities of the models prior to the release\nof ChatGPT (Nov '22), at the time of the release, and today (i.e., Aug '23).\nRecent studies have established that the abilities of the OpenAI's GPT models\nto handle assessments originally designed for humans keep increasing as the\nnewer more capable models are released. However, the qualitative differences in\nthe capabilities and limitations of these models to reason about and/or analyze\nprogramming MCQs have been under-explored. We evaluated three OpenAI's GPT\nmodels on formative and summative MCQ assessments from three Python courses\n(530 questions) focusing on the qualitative differences in the evolving\nefficacy of the subsequent models. This study provides further evidence and\ninsight into the trajectory of the current developments where there already\nexists a technology that can be utilized by students to collect passing scores,\nwith no effort whatsoever, on what today counts as viable programming knowledge\nand skills assessments. This study could be leveraged by educators and\ninstitutions to better understand the recent technological developments in\norder to adapt the design of programming assessments as well as to fuel the\nnecessary discussions into how assessments in future programming classes should\nbe updated.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.04327,regular,post_llm,2023,11,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Promoting Rural Entrepreneurship through Technology: A Case Study using\n  Productivity Enhancing Technology Experience Kits (PETE-Kits)\n\n  Contribution: Case study of a rural-focused educational program with two\ncomponents: 1) introducing high school students and teachers to Smart and\nConnected Technologies (SCTs) that can be used to solve local problems; 2)\nengaging the local community in supporting local technology-driven\nentrepreneurship.\n  Background: Rural communities typically lag behind in terms of participation\nin the digital economy, and use of technology in general. Yet they often have\nthe most to gain, due to high rates of self-employment and lower private-sector\njob opportunities.\n  Research Questions: Can a broadly-scoped rural technology education program\nlead to improvements in 1) student and teacher SCT awareness, 2) SCT skills, 3)\naspirations for future SCT use directed toward entrepreneurship and overall\ncommunity wellbeing?\n  Methodology: Our multidisciplinary team used a mixed-methods approach to\nengage a rural high school robotics team as well as the local community. Over\nthe course of one year, students took part in hands-on-training with SCTs\n(""PETE-Kits"" and associated curriculum) and brainstormed entrepreneurial\nprojects via ideation events. Community members were involved at the beginning\nand end of the project, including judging a ""shark-tank"" style event where\nstudent business ideas using SCT were presented.\n  Findings: Results from student pre / post activity assessments suggest that\nthe program was effective at increasing comfort with technology and combining\ntechnical skills with entrepreneurial opportunities. Post surveys from\ncommunity members, including teachers, demonstrated clear support for the\nprogram and an appreciation of how SCTs / digital skills could benefit the\nlocal economy and wellbeing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10748,regular,post_llm,2023,11,"{'ai_likelihood': 8.17908181084527e-06, 'text': 'An international treaty to implement a global compute cap for advanced artificial intelligence\n\nThis paper presents an international treaty to reduce risks from the development of advanced artificial intelligence (AI). The main provision of the treaty is a global compute cap: a ban on the development of AI systems above an agreed-upon computational resource threshold. The treaty also proposes the development and testing of emergency response plans, negotiations to establish an international agency to enforce the treaty, the establishment of new communication channels and whistleblower protections, and a commitment to avoid an AI arms race. We hope this treaty serves as a useful template for global leaders as they implement governance regimes to protect civilization from the dangers of advanced artificial intelligence.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.0201,review,post_llm,2023,11,"{'ai_likelihood': 8.27842288547092e-06, 'text': 'A cast of thousands: How the IDEAS Productivity project has advanced\n  software productivity and sustainability\n\n  Computational and data-enabled science and engineering are revolutionizing\nadvances throughout science and society, at all scales of computing. For\nexample, teams in the U.S. DOE Exascale Computing Project have been tackling\nnew frontiers in modeling, simulation, and analysis by exploiting unprecedented\nexascale computing capabilities-building an advanced software ecosystem that\nsupports next-generation applications and addresses disruptive changes in\ncomputer architectures. However, concerns are growing about the productivity of\nthe developers of scientific software, its sustainability, and the\ntrustworthiness of the results that it produces. Members of the IDEAS project\nserve as catalysts to address these challenges through fostering software\ncommunities, incubating and curating methodologies and resources, and\ndisseminating knowledge to advance developer productivity and software\nsustainability. This paper discusses how these synergistic activities are\nadvancing scientific discovery-mitigating technical risks by building a firmer\nfoundation for reproducible, sustainable science at all scales of computing,\nfrom laptops to clusters to exascale and beyond.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.17345,regular,post_llm,2023,11,"{'ai_likelihood': 8.940696716308594e-06, 'text': 'Perancangan UI/UX Aplikasi Sistem Informasi Layanan Administrasi dalam\n  Perspektif Psikologi Menggunakan Metode Prototype\n\n  Bina Darma University student administration services are still carried out\nconventionally. Students meet the lecturer to ask the lecturer to sign their\nadministrative documents. However, cases of forged signatures still occur at\nBina Darma University. This problem can cause material loss and is included in\nthe category of criminal offense. The aim of this research is to design an\nAdministrative Services Information System (SILASTRI) interface by applying\ncolor psychology theory, Gestalt principles with a good user experience.\nSILASTRI is designed to support student administration services at Bina Darma\nUniversity. Data collection through observation, distributing questionnaires\nand literature study. This research uses a prototype method which consists of\ncommunication, quick plan, modeling quick design, construction of prototype and\ndeployment delivery & feedback. The prototype method proves technical\nfeasibility and validates the usability of the user interface display by\nestimating the software so that if there are deficiencies they can be corrected\nimmediately. Based on the results of usability testing using Maze, which was\ntested by 70 respondents, the Maze usability value was 89 and the SUS\ncalculation value was 88, which is in the good category. Therefore, it can be\nconcluded that the UI/UX design of the SILASTRI application by applying a\npsychological perspective has an interface and user experience that is well\nreceived by users. The results of this testing and evaluation prove that the\nSILASTRI display design is ready to be developed into an application.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09529,regular,post_llm,2023,11,"{'ai_likelihood': 1.0, 'text': 'TransCrimeNet: A Transformer-Based Model for Text-Based Crime Prediction\n  in Criminal Networks\n\n  This paper presents TransCrimeNet, a novel transformer-based model for\npredicting future crimes in criminal networks from textual data. Criminal\nnetwork analysis has become vital for law enforcement agencies to prevent\ncrimes. However, existing graph-based methods fail to effectively incorporate\ncrucial textual data like social media posts and interrogation transcripts that\nprovide valuable insights into planned criminal activities. To address this\nlimitation, we develop TransCrimeNet which leverages the representation\nlearning capabilities of transformer models like BERT to extract features from\nunstructured text data. These text-derived features are fused with graph\nembeddings of the criminal network for accurate prediction of future crimes.\nExtensive experiments on real-world criminal network datasets demonstrate that\nTransCrimeNet outperforms previous state-of-the-art models by 12.7\\% in F1\nscore for crime prediction. The results showcase the benefits of combining\ntextual and graph-based features for actionable insights to disrupt criminal\nenterprises.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0728836059570312e-06, 'GPT4': 4.285573959350586e-05, 'CLAUDE': 0.99951171875, 'GOOGLE': 1.233816146850586e-05, 'OPENAI_O_SERIES': 0.00011163949966430664, 'DEEPSEEK': 0.00020647048950195312, 'GROK': 3.5762786865234375e-07, 'NOVA': 1.7881393432617188e-07, 'OTHER': 1.1920928955078125e-06, 'HUMAN': 3.5762786865234375e-07}}"
2311.1804,review,post_llm,2023,11,"{'ai_likelihood': 2.7484363979763457e-06, 'text': ""Evaluating Trustworthiness of AI-Enabled Decision Support Systems:\n  Validation of the Multisource AI Scorecard Table (MAST)\n\n  The Multisource AI Scorecard Table (MAST) is a checklist tool based on\nanalytic tradecraft standards to inform the design and evaluation of\ntrustworthy AI systems. In this study, we evaluate whether MAST is associated\nwith people's trust perceptions in AI-enabled decision support systems\n(AI-DSSs). Evaluating trust in AI-DSSs poses challenges to researchers and\npractitioners. These challenges include identifying the components,\ncapabilities, and potential of these systems, many of which are based on the\ncomplex deep learning algorithms that drive DSS performance and preclude\ncomplete manual inspection. We developed two interactive, AI-DSS test\nenvironments using the MAST criteria. One emulated an identity verification\ntask in security screening, and another emulated a text summarization system to\naid in an investigative reporting task. Each test environment had one version\ndesigned to match low-MAST ratings, and another designed to match high-MAST\nratings, with the hypothesis that MAST ratings would be positively related to\nthe trust ratings of these systems. A total of 177 subject matter experts were\nrecruited to interact with and evaluate these systems. Results generally show\nhigher MAST ratings for the high-MAST conditions compared to the low-MAST\ngroups, and that measures of trust perception are highly correlated with the\nMAST ratings. We conclude that MAST can be a useful tool for designing and\nevaluating systems that will engender high trust perceptions, including AI-DSS\nthat may be used to support visual screening and text summarization tasks.\nHowever, higher MAST ratings may not translate to higher joint performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.01838,review,post_llm,2023,11,"{'ai_likelihood': 1.11593140496148e-05, 'text': 'When fairness is an abstraction: Equity and AI in Swedish compulsory\n  education\n\n  Artificial intelligence experts often question whether AI is fair. They view\nfairness as a property of AI systems rather than of sociopolitical and economic\nsystems. This paper emphasizes the need to be fair in the social, political,\nand economic contexts within which an educational system operates and uses AI.\nTaking Swedish decentralized compulsory education as the context, this paper\nexamines whether and how the use of AI envisaged by national authorities and\nedtech companies exacerbates unfairness. A qualitative content analysis of\nselected Swedish policy documents and edtech reports was conducted using the\nconcept of relevant social groups to understand how different groups view the\nrisks and benefits of AI for fairness. Three groups that view efficiency as a\nkey value of AI are identified, and interpreted as economical, pedagogical and\naccessibility-related. By separating fairness from social justice, this paper\nchallenges the notion of fairness as the formal equality of opportunities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.05656,review,post_llm,2023,11,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Combating Misinformation in the Age of LLMs: Opportunities and\n  Challenges\n\n  Misinformation such as fake news and rumors is a serious threat on\ninformation ecosystems and public trust. The emergence of Large Language Models\n(LLMs) has great potential to reshape the landscape of combating\nmisinformation. Generally, LLMs can be a double-edged sword in the fight. On\nthe one hand, LLMs bring promising opportunities for combating misinformation\ndue to their profound world knowledge and strong reasoning abilities. Thus, one\nemergent question is: how to utilize LLMs to combat misinformation? On the\nother hand, the critical challenge is that LLMs can be easily leveraged to\ngenerate deceptive misinformation at scale. Then, another important question\nis: how to combat LLM-generated misinformation? In this paper, we first\nsystematically review the history of combating misinformation before the advent\nof LLMs. Then we illustrate the current efforts and present an outlook for\nthese two fundamental questions respectively. The goal of this survey paper is\nto facilitate the progress of utilizing LLMs for fighting misinformation and\ncall for interdisciplinary efforts from different stakeholders for combating\nLLM-generated misinformation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.06957,regular,post_llm,2023,11,"{'ai_likelihood': 0.0012758043077256945, 'text': ""Simulating Public Administration Crisis: A Novel Generative Agent-Based\n  Simulation System to Lower Technology Barriers in Social Science Research\n\n  This article proposes a social simulation paradigm based on the GPT-3.5 large\nlanguage model. It involves constructing Generative Agents that emulate human\ncognition, memory, and decision-making frameworks, along with establishing a\nvirtual social system capable of stable operation and an insertion mechanism\nfor standardized public events. The project focuses on simulating a township\nwater pollution incident, enabling the comprehensive examination of a virtual\ngovernment's response to a specific public administration event. Controlled\nvariable experiments demonstrate that the stored memory in generative agents\nsignificantly influences both individual decision-making and social networks.\n  The Generative Agent-Based Simulation System introduces a novel approach to\nsocial science and public administration research. Agents exhibit personalized\ncustomization, and public events are seamlessly incorporated through natural\nlanguage processing. Its high flexibility and extensive social interaction\nrender it highly applicable in social science investigations. The system\neffectively reduces the complexity associated with building intricate social\nsimulations while enhancing its interpretability.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.158,review,post_llm,2023,11,"{'ai_likelihood': 0.003115336100260417, 'text': 'Public sentiment analysis and topic modeling regarding ChatGPT in mental\n  health on Reddit: Negative sentiments increase over time\n\n  In order to uncover users\' attitudes towards ChatGPT in mental health, this\nstudy examines public opinions about ChatGPT in mental health discussions on\nReddit. Researchers used the bert-base-multilingual-uncased-sentiment\ntechniques for sentiment analysis and the BERTopic model for topic modeling. It\nwas found that overall, negative sentiments prevail, followed by positive ones,\nwith neutral sentiments being the least common. The prevalence of negative\nemotions has increased over time. Negative emotions encompass discussions on\nChatGPT providing bad mental health advice, debates on machine vs. human value,\nthe fear of AI, and concerns about Universal Basic Income (UBI). In contrast,\npositive emotions highlight ChatGPT\'s effectiveness in counseling, with\nmentions of keywords like ""time"" and ""wallet."" Neutral discussions center\naround private data concerns. These findings shed light on public attitudes\ntoward ChatGPT in mental health, potentially contributing to the development of\ntrustworthy AI in mental health from the public perspective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.039,review,post_llm,2023,11,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Why Fair Automated Hiring Systems Breach EU Non-Discrimination Law\n\n  Employment selection processes that use automated hiring systems based on\nmachine learning are becoming increasingly commonplace. Meanwhile, concerns\nabout algorithmic direct and indirect discrimination that result from such\nsystems are front-and-center, and the technical solutions provided by the\nresearch community often systematically deviate from the principle of equal\ntreatment to combat disparate or adverse impacts on groups based on protected\nattributes. Those technical solutions are now being used in commercially\navailable automated hiring systems, potentially engaging in real-world\ndiscrimination. Algorithmic fairness and algorithmic non-discrimination are not\nthe same. This article examines a conflict between the two: whether such hiring\nsystems are compliant with EU non-discrimination law.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09969,review,post_llm,2023,11,"{'ai_likelihood': 0.00018941031561957466, 'text': ""Examining bias perpetuation in academic search engines: an algorithm\n  audit of Google and Semantic Scholar\n\n  Researchers rely on academic Web search engines to find scientific sources,\nbut search engine mechanisms may selectively present content that aligns with\nbiases embedded in queries. This study examines whether confirmation biased\nqueries prompted into Google Scholar and Semantic Scholar will yield results\naligned with a query's bias. Six queries (topics across health and technology\ndomains such as vaccines, Internet use) were analyzed for disparities in search\nresults. We confirm that biased queries (targeting benefits or risks) affect\nsearch results in line with bias, with technology-related queries displaying\nmore significant disparities. Overall, Semantic Scholar exhibited fewer\ndisparities than Google Scholar. Topics rated as more polarizing did not\nconsistently show more disparate results. Academic search results that\nperpetuate confirmation bias have strong implications for both researchers and\ncitizens searching for evidence. More research is needed to explore how\nscientific inquiry and academic search engines interact.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.01254,review,post_llm,2023,11,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Human participants in AI research: Ethics and transparency in practice\n\n  In recent years, research involving human participants has been critical to\nadvances in artificial intelligence (AI) and machine learning (ML),\nparticularly in the areas of conversational, human-compatible, and cooperative\nAI. For example, roughly 9% of publications at recent AAAI and NeurIPS\nconferences indicate the collection of original human data. Yet AI and ML\nresearchers lack guidelines for ethical research practices with human\nparticipants. Fewer than one out of every four of these AAAI and NeurIPS papers\nconfirm independent ethical review, the collection of informed consent, or\nparticipant compensation. This paper aims to bridge this gap by examining the\nnormative similarities and differences between AI research and related fields\nthat involve human participants. Though psychology, human-computer interaction,\nand other adjacent fields offer historic lessons and helpful insights, AI\nresearch presents several distinct considerations$\\unicode{x2014}$namely,\nparticipatory design, crowdsourced dataset development, and an expansive role\nof corporations$\\unicode{x2014}$that necessitate a contextual ethics framework.\nTo address these concerns, this manuscript outlines a set of guidelines for\nethical and transparent practice with human participants in AI and ML research.\nOverall, this paper seeks to equip technical researchers with practical\nknowledge for their work, and to position them for further dialogue with social\nscientists, behavioral researchers, and ethicists.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.15473,other,post_llm,2023,11,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'M\\=aori algorithmic sovereignty: idea, principles, and use\n\n  Due to the emergence of data-driven technologies in Aotearoa New Zealand that\nuse M\\=aori data, there is a need for values-based frameworks to guide thinking\naround balancing the tension between the opportunities these create, and the\ninherent risks that these technologies can impose. Algorithms can be framed as\na particular use of data, therefore data frameworks that currently exist can be\nextended to include algorithms. M\\=aori data sovereignty principles are\nwell-known and are used by researchers and government agencies to guide the\nculturally appropriate use of M\\=aori data. Extending these principles to fit\nthe context of algorithms, and re-working the underlying sub-principles to\naddress issues related to responsible algorithms from a M\\=aori perspective\nleads to the M\\=aori algorithmic sovereignty principles. We define this idea,\npresent the updated principles and subprinciples, and highlight how these can\nbe used to decolonise algorithms currently in use, and argue that these ideas\ncould potentially be used to developed Indigenised algorithms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.06671,review,post_llm,2023,11,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'Guideline for the Production of Digital Rights Management (DRM)\n\n  Multiple news sources over the years have reported on the problematic effects\nof Digital Rights Management, yet there are no reforms for DRM development,\nsimply removal. The issues are well-known to the public, frequently repeated\neven when addressed: impact on the software and to the devices that run them.\nYet few, if any, have discussed it in recent years, especially with the intent\nof eliminating the shown issues. This study reviews Digital Rights Management\nas a general topic, including the various forms it can take, the current laws\nthat affect DRM, and the current public reception and responses. This study\ndescribes the different types of DRM in general terms and then lists both\npositive and negative examples.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.17318,regular,post_llm,2023,11,"{'ai_likelihood': 2.4967723422580297e-05, 'text': ""Impact of Indoor Mobility Behavior on the Respiratory Infectious\n  Diseases Transmission Trends\n\n  The importance of indoor human mobility in the transmission dynamics of\nrespiratory infectious diseases has been acknowledged. Previous studies have\npredominantly addressed a single type of mobility behavior such as queueing and\na series of behaviors under specific scenarios. However, these studies ignore\nthe abstraction of mobility behavior in various scenes and the critical\nexamination of how these abstracted behaviors impact disease propagation. To\naddress these problems, this study considers people's mobility behaviors in a\ngeneral scenario, abstracting them into two main categories: crowding behavior,\nrelated to the spatial aspect, and stopping behavior, related to the temporal\naspect. Accordingly, this study investigates their impacts on disease spreading\nand the impact of individual spatio-temporal distribution resulting from these\nmobility behaviors on epidemic transmission. First, a point of interest (POI)\nmethod is introduced to quantify the crowding-related spatial POI factors\n(i.e., the number of crowdings and the distance between crowdings) and\nstopping-related temporal POI factors (i.e., the number of stoppings and the\nduration of each stopping). Besides, a personal space determined with Voronoi\ndiagrams is used to construct the individual spatio-temporal distribution\nfactor. Second, two indicators (i.e., the daily number of new cases and the\naverage exposure risk of people) are applied to quantify epidemic transmission.\nThese indicators are derived from a fundamental model which accurately predicts\ndisease transmission between moving individuals. Third, a set of 200 indoor\nscenarios is constructed and simulated to help determine variable values.\nConcurrently, the influences and underlying mechanisms of these behavioral\nfactors on disease transmission are examined using structural equation modeling\nand causal inference modeling......\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.06961,regular,post_llm,2023,11,"{'ai_likelihood': 0.98046875, 'text': ""Empowering Learning: Standalone, Browser-Only Courses for Seamless\n  Education\n\n  Massive Open Online Courses (MOOCs) have transformed the educational\nlandscape, offering scalable and flexible learning opportunities, particularly\nin data-centric fields like data science and artificial intelligence.\nIncorporating AI and data science into MOOCs is a potential means of enhancing\nthe learning experience through adaptive learning approaches. In this context,\nwe introduce PyGlide, a proof-of-concept open-source MOOC delivery system that\nunderscores autonomy, transparency, and collaboration in maintaining course\ncontent. We provide a user-friendly, step-by-step guide for PyGlide,\nemphasizing its distinct advantage of not requiring any local software\ninstallation for students. Highlighting its potential to enhance accessibility,\ninclusivity, and the manageability of course materials, we showcase PyGlide's\npractical application in a continuous integration pipeline on GitHub. We\nbelieve that PyGlide charts a promising course for the future of open-source\nMOOCs, effectively addressing crucial challenges in online education.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0007452964782714844, 'GPT4': 0.83349609375, 'CLAUDE': 6.35981559753418e-05, 'GOOGLE': 0.1640625, 'OPENAI_O_SERIES': 0.00018477439880371094, 'DEEPSEEK': 0.00015842914581298828, 'GROK': 8.52346420288086e-06, 'NOVA': 7.748603820800781e-06, 'OTHER': 0.000583648681640625, 'HUMAN': 0.0006985664367675781}}"
2311.09934,regular,post_llm,2023,11,"{'ai_likelihood': 2.5232632954915367e-05, 'text': 'Echo Chambers within the Russo-Ukrainian War: The Role of Bipartisan\n  Users\n\n  The ongoing Russia-Ukraine war has been extensively discussed on social\nmedia. One commonly observed problem in such discussions is the emergence of\necho chambers, where users are rarely exposed to opinions outside their\nworldview. Prior literature on this topic has assumed that such users hold a\nsingle consistent view. However, recent work has revealed that complex topics\n(such as the war) often trigger bipartisanship among certain people. With this\nin mind, we study the presence of echo chambers on Twitter related to the\nRusso-Ukrainian war. We measure their presence and identify an important subset\nof bipartisan users who vary their opinions during the invasion. We explore the\nrole they play in the communications graph and identify features that\ndistinguish them from remaining users. We conclude by discussing their\nimportance and how they can improve the quality of discourse surrounding the\nwar.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.15125,regular,post_llm,2023,11,"{'ai_likelihood': 2.2517310248480903e-06, 'text': 'Using Assignment Incentives to Reduce Student Procrastination and\n  Encourage Code Review Interactions\n\n  Procrastination causes student stress, reduced learning and performance, and\nresults in very busy help sessions immediately before deadlines. A key\nchallenge is encouraging students to complete assignments earlier rather than\nwaiting until right before the deadline, so the focus becomes on the learning\nobjectives rather than just meeting deadlines. This work presents an incentive\nsystem encouraging students to complete assignments many days before deadlines.\nCompleted assignments are code reviewed by staff for correctness and providing\nfeedback, which results in more student-instructor interactions and may help\nreduce student use of generative AI. The incentives result in a change in\nstudent behavior with 45% of assignments completed early and 30% up to 4 days\nbefore the deadline. Students receive real-time feedback with no increase in\nmarking time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10039,review,post_llm,2023,11,"{'ai_likelihood': 0.06398518880208334, 'text': 'Data Silos A Roadblock for AIOps\n\n  Using artificial intelligence to manage IT operations, also known as AIOps,\nis a trend that has attracted a lot of interest and anticipation in recent\nyears. The challenge in IT operations is to run steady-state operations without\ndisruption as well as support agility"" can be rephrased as ""IT operations face\nthe challenge of maintaining steady-state operations while also supporting\nagility [11]. AIOps assists in bridging the gap between the demand for IT\noperations and the ability of humans to meet that demand. However, it is not\neasy to apply AIOps in current organizational settings. Data Centralization is\na major obstacle for adopting AIOps, according to a recent survey by Cisco [1].\nThe survey, which involved 8,161 senior business leaders from organizations\nwith more than 500 employees, found that 81% of them acknowledged that their\ndata was scattered across different silos within their organizations. This\npaper illustrates the topic of data silos, their causes, consequences, and\nsolutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09452,review,post_llm,2023,11,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'Keep the Future Human: Why and How We Should Close the Gates to AGI and\n  Superintelligence, and What We Should Build Instead\n\n  Dramatic advances in artificial intelligence over the past decade (for\nnarrow-purpose AI) and the last several years (for general-purpose AI) have\ntransformed AI from a niche academic field to the core business strategy of\nmany of the world\'s largest companies, with hundreds of billions of dollars in\nannual investment in the techniques and technologies for advancing AI\'s\ncapabilities. We now come to a critical juncture. As the capabilities of new AI\nsystems begin to match and exceed those of humans across many cognitive\ndomains, humanity must decide: how far do we go, and in what direction? This\nessay argues that we should keep the future human by closing the ""gates"" to\nsmarter-than-human, autonomous, general-purpose AI -- sometimes called ""AGI"" --\nand especially to the highly-superhuman version sometimes called\n""superintelligence."" Instead, we should focus on powerful, trustworthy AI tools\nthat can empower individuals and transformatively improve human societies\'\nabilities to do what they do best.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14718,regular,post_llm,2023,11,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Demonstrative Evidence and the Use of Algorithms in Jury Trials\n\n  We investigate how the use of bullet comparison algorithms and demonstrative\nevidence may affect juror perceptions of reliability, credibility, and\nunderstanding of expert witnesses and presented evidence. The use of\nstatistical methods in forensic science is motivated by a lack of scientific\nvalidity and error rate issues present in many forensic analysis methods. We\nexplore what our study says about how this type of forensic evidence is\nperceived in the courtroom where individuals unfamiliar with advanced\nstatistical methods are asked to evaluate results in order to assess guilt. In\nthe course of our initial study, we found that individuals overwhelmingly\nprovided high Likert scale ratings in reliability, credibility, and\nscientificity regardless of experimental condition. This discovery of scale\ncompression - where responses are limited to a few values on a larger scale,\ndespite experimental manipulations - limits statistical modeling but provides\nopportunities for new experimental manipulations which may improve future\nstudies in this area.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.04579,regular,post_llm,2023,11,"{'ai_likelihood': 0.76806640625, 'text': 'Text Finder Application for Android\n\n  A Text Finder, an android application that utilizes Optical Character\nRecognition (OCR) technology with the help of Google Cloud Vision API to\nextract text from images taken with the device camera or from existing images\nin the users phone. The extracted text can be saved to the device storage where\nall previous extracts can be easily accessed on a user-friendly interface. The\napplication also features editing, deletion and sharing options for the\nextracted text. The user interface is user-friendly, making the application\naccessible to students, professional and organizations for a variety of\npurposes, including document scanning, data entry, and information retrieval.\nManual extraction of text by typing or writing from images can be very\ntime-consuming and can be prone to errors. This application is an efficient and\nsimple solution for extracted texts and organizing important information from\nthe photos. This paper describes the technical details of the OCR technology\nand Googles ML Kit Text Recognition API used in the application, as well as the\ndesign, implementation and evaluation of the application in terms of\nperformance and accuracy. The research also explores the key objectives and\nbenefits of Text Finder, such as reducing the time and effort required and\nincreasing the efficiency of document-based tasks.\n', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.77587890625, 'GPT4': 0.155517578125, 'CLAUDE': 0.000728607177734375, 'GOOGLE': 0.04937744140625, 'OPENAI_O_SERIES': 0.0002701282501220703, 'DEEPSEEK': 1.2934207916259766e-05, 'GROK': 5.7220458984375e-06, 'NOVA': 7.3909759521484375e-06, 'OTHER': 0.0015420913696289062, 'HUMAN': 0.0167694091796875}}"
2311.06187,regular,post_llm,2023,11,"{'ai_likelihood': 1.6954210069444445e-05, 'text': ""A Note on Tesla's Revised Safety Report Crash Rates\n\n  Between June 2018 and December 2022, Tesla released quarterly safety reports\nciting average miles between crashes for Tesla vehicles. Prior to March 2021,\ncrash rates were categorized as 1) with their SAE Level 2 automated driving\nsystem Autopilot engaged, 2) without Autopilot but with active safety features\nsuch as automatic emergency braking, and 3) without Autopilot and without\nactive safety features. In January 2022, Tesla revised past reports to reflect\ntheir new categories of with and without Autopilot engaged, in addition to\nmaking small adjustments based on recently discovered double counting of\nreports and excluding previously recorded crashes that did not meet their\nthresholds of airbag or active safety restraint activation. The revisions are\nheavily biased towards no-active-safety-features$\\unicode{x2014}$a surprising\nresult given prior research showing that drivers predominantly keep most active\nsafety features enabled. As Tesla's safety reports represent the only national\nsource of Level 2 advanced driver assistance system crash rates, clarification\nof their methods is essential for researchers and regulators. This note\ndescribes the changes and considers possible explanations for the\ndiscrepancies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14241,regular,post_llm,2023,11,"{'ai_likelihood': 0.020311143663194444, 'text': 'How We Manage an Army of Teaching Assistants: Experience Report on\n  Scaling a CS1 Course\n\n  A considerable increase in enrollment numbers poses major challenges in\ncourse management, such as fragmented information sharing, inefficient\nmeetings, and poor understanding of course activities among a large team of\nteaching assistants. To address these challenges, we restructured the course,\ndrawing inspiration from successful management and educational practices. We\ndeveloped an organized, three-tier structure for teams, each led by an\nexperienced Lead TA. We also formed five functional teams, each focusing on a\nspecific area of responsibility: communication, content, ""lost student""\nsupport, plagiarism, and scheduling. In addition, we updated our recruitment\nmethod for undergraduate TAs, following a model similar to the one used in the\nsoftware industry, while also deciding to mentor Lead TAs in place of\ntraditional training. Our experiences, lessons learned, and future plans for\nenhancement have been detailed in this experience report. We emphasize the\nvalue of using management techniques in dealing with large-scale course\nhandling and invite cooperation to improve the implementation of these\nstrategies, inviting other institutions to consider and adapt this approach,\ntailoring it to their specific needs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14903,regular,post_llm,2023,11,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Code Generation Based Grading: Evaluating an Auto-grading Mechanism for\n  ""Explain-in-Plain-English"" Questions\n\n  Comprehending and elucidating the purpose of code is often cited as being a\nkey learning objective within introductory programming courses. To address this\nobjective ``Explain-in-Plain-English\'\' questions, in which students are shown a\nsegment of code and asked to provide an abstract description of the code\'s\npurpose, have been adopted. However, given EiPE questions require a natural\nlanguage response, they often require manual grading which is time-consuming\nfor course staff and delays feedback for students. With the advent of large\nlanguage models (LLMs) capable of generating code, responses to EiPE questions\ncan be used to generate code segments, the correctness of which can then be\neasily verified using test cases. We refer to this approach as ""Code Generation\nBased Grading"" (CGBG) and in this paper we explore its agreement with human\ngraders using EiPE responses from past exams in an introductory programming\ncourse taught in Python. Overall, we find that CGBG achieves moderate agreement\nwith human graders with the primary area of disagreement being its leniency\nwith respect to low-level and line-by-line descriptions of code.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09173,review,post_llm,2023,11,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Design Theory for Societal Digital Transformation: The Case of Digital\n  Global Health\n\n  With societal challenges, including but not limited to human development,\nequity, social justice, and climate change, societal-level digital\ntransformation (SDT) is of imminent relevance and theoretical interest. While\nbuilding on local-level efforts, societal-level transformation is a nonlinear\nextension of the local level. Unfortunately, academic discourse on digital\ntransformation has largely left SDT unaccounted for. Drawing on more than 25\nyears of intensive, interventionist research engagement with the digital\ntransformation of public healthcare information management and delivery in more\nthan 80 countries in the Global South, we contribute to theorizing SDT in the\nform of a design theory consisting of six interconnected design principles.\nThese design principles articulate the interplay and tensions of accommodating\nover time increased diversity and flexibility in digital solutions, while\nsimultaneously connecting local, national, and regional/ global efforts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.16585,regular,post_llm,2023,11,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Sorting Out New York City's Trash Problem\n\n  To reduce waste and improve public health and sanitation in New York City,\ninnovative policies tailored to the city's unique urban landscape are\nnecessary. The first program we propose is the Dumpster and Compost\nAccessibility Program. This program is affordable and utilizes dumpsters placed\nnear fire hydrants to keep waste off the street without eliminating parking\nspaces. It also includes legal changes and the provision of compost bins to\nsingle/two-family households, which together will increase composting rates.\nThe second program is the Pay-As-You-Throw Program. This requires New Yorkers\nliving in single/two-family households to purchase stickers for each refuse bag\nthey have collected by the city, incentivizing them to sort out compostable\nwaste and recyclables. We conduct a weighted multi-objective optimization to\ndetermine the optimal sticker price based on the City's priorities. Roughly in\nproportion to the price, this program will increase diversion rates and\ndecrease the net costs to New York City's Department of Sanitation. In\nconjunction, these two programs will improve NYC's diversion rates, eliminate\ngarbage bags from the streets, and potentially save New York City money.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.10244,regular,post_llm,2023,11,"{'ai_likelihood': 1.0, 'text': ""JediCode -- A Gamefied Approach to Competitive Coding\n\n  JediCode (name inspired from Star Wars) pioneers a transformative approach to\ncompetitive coding by infusing the challenge with gamified elements. This\nplatform reimagines coding competitions, integrating real-time leaderboards,\nsynchronized challenges, and random matchmaking, creating an engaging, dynamic,\nand friendly atmosphere. This paper explores JediCode's innovative features and\narchitecture, shedding light on its user-centric design and powerful execution\nservice. By embracing gamification, JediCode not only elevates the thrill of\ncoding challenges but also fosters a sense of community, inspiring programmers\nto excel while enjoying the process.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.514787673950195e-05, 'GPT4': 0.9453125, 'CLAUDE': 0.00015974044799804688, 'GOOGLE': 0.051971435546875, 'OPENAI_O_SERIES': 0.0002512931823730469, 'DEEPSEEK': 0.0002187490463256836, 'GROK': 1.6033649444580078e-05, 'NOVA': 2.0444393157958984e-05, 'OTHER': 0.002147674560546875, 'HUMAN': 5.543231964111328e-06}}"
2311.18345,review,post_llm,2023,11,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Situating the social issues of image generation models in the model life\n  cycle: a sociotechnical approach\n\n  The race to develop image generation models is intensifying, with a rapid\nincrease in the number of text-to-image models available. This is coupled with\ngrowing public awareness of these technologies. Though other generative AI\nmodels--notably, large language models--have received recent critical attention\nfor the social and other non-technical issues they raise, there has been\nrelatively little comparable examination of image generation models. This paper\nreports on a novel, comprehensive categorization of the social issues\nassociated with image generation models. At the intersection of machine\nlearning and the social sciences, we report the results of a survey of the\nliterature, identifying seven issue clusters arising from image generation\nmodels: data issues, intellectual property, bias, privacy, and the impacts on\nthe informational, cultural, and natural environments. We situate these social\nissues in the model life cycle, to aid in considering where potential issues\narise, and mitigation may be needed. We then compare these issue clusters with\nwhat has been reported for large language models. Ultimately, we argue that the\nrisks posed by image generation models are comparable in severity to the risks\nposed by large language models, and that the social impact of image generation\nmodels must be urgently considered.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.0006,review,post_llm,2023,11,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Open data ecosystems: what models to co-create service innovations in\n  smart cities?\n\n  While smart cities are recently providing open data, how to organise the\ncollective creation of data, knowledge and related products and services\nproduced from this collective resource, still remains to be thought. This paper\naims at gathering the literature review on open data ecosystems to tackle the\nfollowing research question: what models can be imagined to stimulate the\ncollective co-creation of services between smart cities' stakeholders acting as\nproviders and users of open data? Such issue is currently at stake in many\nmunicipalities such as Lisbon which decided to position itself as a platform\n(O'Reilly, 2010) in the local digital ecosystem. With the implementation of its\nCity Operation Center (COI), Lisbon's municipality provides an Information\nInfrastructure (Bowker et al., 2009) to many different types of actors such as\ntelecom companies, municipalities, energy utilities or transport companies.\nThrough this infrastructure, Lisbon encourages such actors to gather, integrate\nand release heterogeneous datasets and tries to orchestrate synergies among\nthem so data-driven solution to urban problems can emerge (Carvalho and Vale,\n2018). The remaining question being: what models for the municipalities such as\nLisbon to lean on so as to drive this cutting-edge type of service innovation?\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.09932,regular,post_llm,2023,11,"{'ai_likelihood': 8.344650268554688e-06, 'text': 'The Communication GSC System with Energy Harvesting Nodes aided by\n  Opportunistic Routing\n\n  In this paper, a cooperative communication network based on energy-harvesting\n(EH) decode-and-forward (DF) relays is proposed. For relay nodes, there is\nharvest-storage-use (HSU) structure in this system. And energy can be obtained\nfrom the surrounding environment through energy buffering. In order to improve\nthe performance of the communication system, the opportunistic routing\nalgorithm and the generalized selection combining (GSC) algorithm are adopted\nin this communication system. In addition, from discrete-time continuous-state\nspace Markov chain model (DCSMC), a theoretical expression of the energy\nlimiting distribution stored in infinite buffers is derived. Through using the\nprobability distribution and state transition matrix, the theoretical\nexpressions of system outage probability, throughput and time cost of per\npacket are obtained. Through the simulation verification, the theoretical\nresults are in good agreement with the simulation results.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.01372,regular,post_llm,2023,11,"{'ai_likelihood': 2.9802322387695312e-06, 'text': 'Data-Augmented and Retrieval-Augmented Context Enrichment in Chinese\n  Media Bias Detection\n\n  With the increasing pursuit of objective reports, automatically understanding\nmedia bias has drawn more attention in recent research. However, most of the\nprevious work examines media bias from Western ideology, such as the left and\nright in the political spectrum, which is not applicable to Chinese outlets.\nBased on the previous lexical bias and informational bias structure, we refine\nit from the Chinese perspective and go one step further to craft data with 7\nfine-grained labels. To be specific, we first construct a dataset with Chinese\nnews reports about COVID-19 which is annotated by our newly designed system,\nand then conduct substantial experiments on it to detect media bias. However,\nthe scale of the annotated data is not enough for the latest deep-learning\ntechnology, and the cost of human annotation in media bias, which needs a lot\nof professional knowledge, is too expensive. Thus, we explore some context\nenrichment methods to automatically improve these problems. In Data-Augmented\nContext Enrichment (DACE), we enlarge the training data; while in\nRetrieval-Augmented Context Enrichment (RACE), we improve information retrieval\nmethods to select valuable information and integrate it into our models to\nbetter understand bias. Extensive experiments are conducted on both our dataset\nand an English dataset BASIL. Our results show that both methods outperform our\nbaselines, while the RACE methods are more efficient and have more potential.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14696,review,post_llm,2023,11,"{'ai_likelihood': 1.0, 'text': 'Navigating information and uncertainty: A fuzzy logic model to approach\n  transparency, democracy and social wellbeing\n\n  In the digital age of information overload and uncertainty, the authors\npropose the tDTSW model based on fuzzy logic to navigate governance\ncomplexities. This model transcends binary thinking, analyzes democracy,\ntransparency, and social well-being, highlighting their roles in just societies\nthrough case studies. It addresses challenges like capitalism, sustainability,\ngender equality, and education in modern democracies, emphasizing their\ninterplay for positive change. ""Navigating Information and Uncertainty""\nintroduces fuzzy logic, offering a structured approach. It calls for collective\nefforts to create equitable, sustainable, and just societies, inviting readers\nto shape a brighter future.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.006565093994140625, 'GPT4': 0.173095703125, 'CLAUDE': 0.0009145736694335938, 'GOOGLE': 0.43408203125, 'OPENAI_O_SERIES': 0.0523681640625, 'DEEPSEEK': 0.016448974609375, 'GROK': 0.00012874603271484375, 'NOVA': 0.01165771484375, 'OTHER': 0.3046875, 'HUMAN': 7.30752944946289e-05}}"
2311.14704,regular,post_llm,2023,11,"{'ai_likelihood': 3.725290298461914e-05, 'text': ""An\\'alise e modelagem de jogos digitais: relato de uma experi\\^encia\n  educacional utilizando metodologias ativas em um grupo multidisciplinar\n\n  The traditional teaching of software engineering is focused on technical\nskills. Active strategies, where students experience content and interact with\nreality, are effective. The market demands new skills in the digital\ntransformation, dealing with the complexity of modeling businesses and the\ninterconnection between people, systems, and technologies. The transition to\nactive methodologies, such as Problem-Based Learning (PBL), brings real market\nscenarios into the classroom. This article reports on the experience in the\ncourse, presenting concepts and results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.02476,regular,post_llm,2023,11,"{'ai_likelihood': 2.1027194129096137e-05, 'text': ""Forecasting Success of Computer Science Professors and Students Based on\n  Their Academic and Personal Backgrounds\n\n  After completing their undergraduate studies, many computer science (CS)\nstudents apply for competitive graduate programs in North America. Their\nlong-term goal is often to be hired by one of the big five tech companies or to\nbecome a faculty member. Therefore, being aware of the role of admission\ncriteria may help them choose the best path towards their goals. In this paper,\nwe analyze the influence of students' previous universities on their chances of\nbeing accepted to prestigious North American universities and returning to\nacademia as professors in the future. Our findings demonstrate that the ranking\nof their prior universities is a significant factor in achieving their goals.\nWe then illustrate that there is a bias in the undergraduate institutions of\nstudents admitted to the top 25 computer science programs. Finally, we employ\nmachine learning models to forecast the success of professors at these\nuniversities. We achieved an RMSE of 7.85 for this prediction task.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14891,review,post_llm,2023,11,"{'ai_likelihood': 1.5232298109266494e-06, 'text': ""Simpson's Paradox and Lagging Progress in Completion Trends of\n  Underrepresented Students in Computer Science\n\n  It is imperative for the Computer Science (CS) community to ensure active\nparticipation and success of students from diverse backgrounds. This work\ncompares CS to other areas of study with respect to success of students from\nthree underrepresented groups: Women, Black and Hispanic or Latino. Using a\ndata-driven approach, we show that trends of success over the years for\nunderrepresented groups in CS are lagging behind other disciplines. Completion\nof CS programs by Black students in particular shows an alarming regression in\nthe years 2011 through 2019. This national level decline is most concentrated\nin the Southeast of the United States and seems to be driven mostly by a small\nnumber of institutes that produce a large number of graduates. We strongly\nbelieve that more data-driven studies in this area are necessary to make\nprogress towards a more equitable and inclusive CS community. Without an\nunderstanding of underlying dynamics, policy makers and practitioners will be\nunable to make informed decisions about how and where to allocate resources to\naddress the problem.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.11388,review,post_llm,2023,11,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Machine Culture\n\n  The ability of humans to create and disseminate culture is often credited as\nthe single most important factor of our success as a species. In this\nPerspective, we explore the notion of machine culture, culture mediated or\ngenerated by machines. We argue that intelligent machines simultaneously\ntransform the cultural evolutionary processes of variation, transmission, and\nselection. Recommender algorithms are altering social learning dynamics.\nChatbots are forming a new mode of cultural transmission, serving as cultural\nmodels. Furthermore, intelligent machines are evolving as contributors in\ngenerating cultural traits--from game strategies and visual art to scientific\nresults. We provide a conceptual framework for studying the present and\nanticipated future impact of machines on cultural evolution, and present a\nresearch agenda for the study of machine culture.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.01693,regular,post_llm,2023,11,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Enhancing Computer Science Education with Pair Programming and Problem\n  Solving Studios\n\n  This study examines the adaptation of the problem-solving studio to computer\nscience education by combining it with pair programming. Pair programming is a\nsoftware engineering practice in industry, but has seen mixed results in the\nclassroom. Recent research suggests that pair programming has promise and\npotential to be an effective pedagogical tool, however what constitutes good\ninstructional design and implementation for pair programming in the classroom\nis not clear. We developed a framework for instructional design for pair\nprogramming by adapting the problem-solving studio (PSS), a pedagogy originally\nfrom biomedical engineering. PSS involves teams of students solving open-ended\nproblems with real-time feedback given by the instructor. Notably, PSS uses\nproblems of adjustable difficulty to keep students of all levels engaged and\nfunctioning within the zone of proximal development. The course structure has\nthree stages, first starting with demonstration, followed by a PSS session,\nthen finishing with a debrief. We studied the combination of PSS and pair\nprogramming in a CS1 class over three years. Surveys of the students report a\nhigh level of engagement, learning, and motivation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.024,review,post_llm,2023,11,"{'ai_likelihood': 0.0002002716064453125, 'text': 'From Plate to Production: Artificial Intelligence in Modern\n  Consumer-Driven Food Systems\n\n  Global food systems confront the urgent challenge of supplying sustainable,\nnutritious diets in the face of escalating demands. The advent of Artificial\nIntelligence (AI) is bringing in a personal choice revolution, wherein\nAI-driven individual decisions transform food systems from dinner tables, to\nthe farms, and back to our plates. In this context, AI algorithms refine\npersonal dietary choices, subsequently shaping agricultural outputs, and\npromoting an optimized feedback loop from consumption to cultivation.\nInitially, we delve into AI tools and techniques spanning the food supply\nchain, and subsequently assess how AI subfields$\\unicode{x2013}$encompassing\nmachine learning, computer vision, and speech recognition$\\unicode{x2013}$are\nharnessed within the AI-enabled Food System (AIFS) framework, which\nincreasingly leverages Internet of Things, multimodal sensors and real-time\ndata exchange. We spotlight the AIFS framework, emphasizing its fusion of AI\nwith technologies such as digitalization, big data analytics, biotechnology,\nand IoT extensively used in modern food systems in every component. This\nparadigm shifts the conventional ""farm to fork"" narrative to a cyclical\n""consumer-driven farm to fork"" model for better achieving sustainable,\nnutritious diets. This paper explores AI\'s promise and the intrinsic challenges\nit poses within the food domain. By championing stringent AI governance,\nuniform data architectures, and cross-disciplinary partnerships, we argue that\nAI, when synergized with consumer-centric strategies, holds the potential to\nsteer food systems toward a sustainable trajectory. We furnish a comprehensive\nsurvey for the state-of-the-art in diverse facets of food systems, subsequently\npinpointing gaps and advocating for the judicious and efficacious deployment of\nemergent AI methodologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.01156,regular,post_llm,2023,11,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Several Consequences of Optimality\n\n  Rationality is frequently associated with making the best possible decisions.\nIt\'s widely acknowledged that humans, as rational beings, have limitations in\ntheir decision-making capabilities. Nevertheless, recent advancements in\nfields, such as, computing, science and technology, combined with the\navailability of vast amounts of data, have sparked optimism that these\ndevelopments could potentially expand the boundaries of human bounded\nrationality through the augmentation of machine intelligence. In this paper,\nfindings from a computational model demonstrated that when an increasing number\nof agents independently strive to achieve global optimality, facilitated by\nimproved computing power, etc., they indirectly accelerated the occurrence of\nthe ""tragedy of the commons"" by depleting shared resources at a faster rate.\nFurther, as agents achieve optimality, there is a drop in information entropy\namong the solutions of the agents. Also, clear economic divide emerges among\nagents. Considering, two groups, one as producer and the other (the group\nagents searching for optimality) as consumer of the highest consumed resource,\nthe consumers seem to gain more than the producers. Thus, bounded rationality\ncould be seen as boon to sustainability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.16905,regular,post_llm,2023,11,"{'ai_likelihood': 4.5564439561631945e-05, 'text': 'LLM generated responses to mitigate the impact of hate speech\n\nIn this study, we explore the use of Large Language Models (LLMs) to counteract hate speech. We conducted the first real-life A/B test assessing the effectiveness of LLM-generated counter-speech. During the experiment, we posted 753 automatically generated responses aimed at reducing user engagement under tweets that contained hate speech toward Ukrainian refugees in Poland.\n  Our work shows that interventions with LLM-generated responses significantly decrease user engagement, particularly for original tweets with at least ten views, reducing it by over 20%. This paper outlines the design of our automatic moderation system, proposes a simple metric for measuring user engagement and details the methodology of conducting such an experiment. We discuss the ethical considerations and challenges in deploying generative AI for discourse moderation.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.15377,review,post_llm,2023,11,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'Increased Compute Efficiency and the Diffusion of AI Capabilities\n\n  Training advanced AI models requires large investments in computational\nresources, or compute. Yet, as hardware innovation reduces the price of compute\nand algorithmic advances make its use more efficient, the cost of training an\nAI model to a given performance falls over time - a concept we describe as\nincreasing compute efficiency. We find that while an access effect increases\nthe number of actors who can train models to a given performance over time, a\nperformance effect simultaneously increases the performance available to each\nactor. This potentially enables large compute investors to pioneer new\ncapabilities, maintaining a performance advantage even as capabilities diffuse.\nSince large compute investors tend to develop new capabilities first, it will\nbe particularly important that they share information about their AI models,\nevaluate them for emerging risks, and, more generally, make responsible\ndevelopment and release decisions. Further, as compute efficiency increases,\ngovernments will need to prepare for a world where dangerous AI capabilities\nare widely available - for instance, by developing defenses against harmful AI\nmodels or by actively intervening in the diffusion of particularly dangerous\ncapabilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.07807,regular,post_llm,2023,11,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""Creation of a CS1 Course with Modern C++ Principles\n\n  Best practices in programming need to be emphasized in a CS1 course as bad\nstudent habits persist if not reinforced well. The C++ programming language,\nalthough a relatively old language, has been regularly updated with new\nversions since 2011, on the pace of once every three years. Each new version\ncontains important features that make the C++ language more complex for\nbackwards compatibility, but often introduce new features to make common use\ncases simpler to implement. This poster contains experiences in designing a CS1\ncourse that uses the C++ programming language that incorporates ``modern''\nversions of the language from the start, as well as recent conferences about\nthe language. Our goals were to prevent many common bad habits among C++\nprogrammers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.1135,review,post_llm,2023,11,"{'ai_likelihood': 1.6887982686360678e-06, 'text': ""An Alternative to Regulation: The Case for Public AI\n\n  Can governments build AI? In this paper, we describe an ongoing effort to\ndevelop ``public AI'' -- publicly accessible AI models funded, provisioned, and\ngoverned by governments or other public bodies. Public AI presents both an\nalternative and a complement to standard regulatory approaches to AI, but it\nalso suggests new technical and policy challenges. We present a roadmap for how\nthe ML research community can help shape this initiative and support its\nimplementation, and how public AI can complement other responsible AI\ninitiatives.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.03449,regular,post_llm,2023,11,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""Into the LAIONs Den: Investigating Hate in Multimodal Datasets\n\n  'Scale the model, scale the data, scale the compute' is the reigning\nsentiment in the world of generative AI today. While the impact of model\nscaling has been extensively studied, we are only beginning to scratch the\nsurface of data scaling and its consequences. This is especially of critical\nimportance in the context of vision-language datasets such as LAION. These\ndatasets are continually growing in size and are built based on large-scale\ninternet dumps such as the Common Crawl, which is known to have numerous\ndrawbacks ranging from quality, legality, and content. The datasets then serve\nas the backbone for large generative models, contributing to the\noperationalization and perpetuation of harmful societal and historical biases\nand stereotypes. In this paper, we investigate the effect of scaling datasets\non hateful content through a comparative audit of two datasets: LAION-400M and\nLAION-2B. Our results show that hate content increased by nearly 12% with\ndataset scale, measured both qualitatively and quantitatively using a metric\nthat we term as Hate Content Rate (HCR). We also found that filtering dataset\ncontents based on Not Safe For Work (NSFW) values calculated based on images\nalone does not exclude all the harmful content in alt-text. Instead, we found\nthat trace amounts of hateful, targeted, and aggressive text remain even when\ncarrying out conservative filtering. We end with a reflection and a discussion\nof the significance of our results for dataset curation and usage in the AI\ncommunity. Code and the meta-data assets curated in this paper are publicly\navailable at https://github.com/vinayprabhu/hate_scaling. Content warning: This\npaper contains examples of hateful text that might be disturbing, distressing,\nand/or offensive.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.04559,review,post_llm,2023,11,"{'ai_likelihood': 3.168980280558269e-05, 'text': 'Individual and gender inequality in computer science: A career study of\n  cohorts from 1970 to 2000\n\n  Inequality prevails in science. Individual inequality means that most perish\nquickly and only a few are successful, while gender inequality implies that\nthere are differences in achievements for women and men. Using large-scale\nbibliographic data and following a computational approach, we study the\nevolution of individual and gender inequality for cohorts from 1970 to 2000 in\nthe whole field of computer science as it grows and becomes a team-based\nscience. We find that individual inequality in productivity (publications)\nincreases over a scholar\'s career but is historically invariant, while\nindividual inequality in impact (citations), albeit larger, is stable across\ncohorts and careers. Gender inequality prevails regarding productivity, but\nthere is no evidence for differences in impact. The Matthew Effect is shown to\naccumulate advantages to early achievements and to become stronger over the\ndecades, indicating the rise of a ""publish or perish"" imperative. Only some\nauthors manage to reap the benefits that publishing in teams promises. The\nMatthew Effect then amplifies initial differences and propagates the gender\ngap. Women continue to fall behind because they continue to be at a higher risk\nof dropping out for reasons that have nothing to do with early-career\nachievements or social support. Our findings suggest that mentoring programs\nfor women to improve their social-networking skills can help to reduce gender\ninequality.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.12365,regular,post_llm,2023,11,"{'ai_likelihood': 6.887647840711806e-06, 'text': 'Designing Problem Sessions for Algorithmic Subjects to Boost Student\n  Confidence\n\n  In this paper, we describe how we changed the structure of problem sessions\nin an algorithmic subject, in order to improve student confidence. The subject\nin question is taught to very large cohorts of (around 900) students, though\nour approach can be applied more broadly. We reflect on our experiences over a\nnumber of years, including during the pandemic, and show that by adding clear\nsectioning indicating the style of the questions and by including simple\nwarm-up questions, student engagement and confidence improves, while making the\nteaching activities of our teaching assistants easier to manage.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.07792,regular,post_llm,2023,11,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Western, Religious or Spiritual: An Evaluation of Moral Justification in\n  Large Language Models\n\n  The increasing success of Large Language Models (LLMs) in variety of tasks\nlead to their widespread use in our lives which necessitates the examination of\nthese models from different perspectives. The alignment of these models to\nhuman values is an essential concern in order to establish trust that we have\nsafe and responsible systems. In this paper, we aim to find out which values\nand principles are embedded in LLMs in the process of moral justification. For\nthis purpose, we come up with three different moral perspective categories:\nWestern tradition perspective (WT), Abrahamic tradition perspective (AT), and\nSpiritualist/Mystic tradition perspective (SMT). In two different experiment\nsettings, we asked models to choose principles from the three for suggesting a\nmoral action and evaluating the moral permissibility of an action if one tries\nto justify an action on these categories, respectively. Our experiments\nindicate that tested LLMs favors the Western tradition moral perspective over\nothers. Additionally, we observe that there potentially exists an\nover-alignment towards religious values represented in the Abrahamic Tradition,\nwhich causes models to fail to recognize an action is immoral if it is\npresented as a ""religious-action"". We believe that these results are essential\nin order to direct our attention in future efforts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.06207,review,post_llm,2023,11,"{'ai_likelihood': 0.98779296875, 'text': 'Vox Populi, Vox ChatGPT: Large Language Models, Education and Democracy\n\n  In the era of generative AI and specifically large language models (LLMs),\nexemplified by ChatGPT, the intersection of artificial intelligence and human\nreasoning has become a focal point of global attention. Unlike conventional\nsearch engines, LLMs go beyond mere information retrieval, entering into the\nrealm of discourse culture. Its outputs mimic well-considered, independent\nopinions or statements of facts, presenting a pretense of wisdom. This paper\nexplores the potential transformative impact of LLMs on democratic societies.\nIt delves into the concerns regarding the difficulty in distinguishing\nChatGPT-generated texts from human output. The discussion emphasizes the\nessence of authorship, rooted in the unique human capacity for reason - a\nquality indispensable for democratic discourse and successful collaboration\nwithin free societies. Highlighting the potential threats to democracy, this\npaper presents three arguments: the Substitution argument, the Authenticity\nargument, and the Facts argument. These arguments highlight the potential risks\nthat are associated with an overreliance on LLMs. The central thesis posits\nthat widespread deployment of LLMs may adversely affect the fabric of a\ndemocracy if not comprehended and addressed proactively and properly. In\nproposing a solution, we advocate for an emphasis on education as a means to\nmitigate risks. We suggest cultivating thinking skills in children, fostering\ncoherent thought formulation, and distinguishing between machine-generated\noutput and genuine, i.e. human, reasoning. The focus should be on responsible\ndevelopment and usage of LLMs, with the goal of augmenting human capacities in\nthinking, deliberating and decision-making rather than substituting them.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00014197826385498047, 'GPT4': 0.94873046875, 'CLAUDE': 1.1742115020751953e-05, 'GOOGLE': 0.050262451171875, 'OPENAI_O_SERIES': 1.811981201171875e-05, 'DEEPSEEK': 1.1920928955078125e-06, 'GROK': 5.960464477539062e-07, 'NOVA': 5.960464477539062e-07, 'OTHER': 5.304813385009766e-06, 'HUMAN': 0.0010194778442382812}}"
2311.06477,review,post_llm,2023,11,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Report of the 1st Workshop on Generative AI and Law\n\n  This report presents the takeaways of the inaugural Workshop on Generative AI\nand Law (GenLaw), held in July 2023. A cross-disciplinary group of\npractitioners and scholars from computer science and law convened to discuss\nthe technical, doctrinal, and policy challenges presented by law for Generative\nAI, and by Generative AI for law, with an emphasis on U.S. law in particular.\nWe begin the report with a high-level statement about why Generative AI is both\nimmensely significant and immensely challenging for law. To meet these\nchallenges, we conclude that there is an essential need for 1) a shared\nknowledge base that provides a common conceptual language for experts across\ndisciplines; 2) clarification of the distinctive technical capabilities of\ngenerative-AI systems, as compared and contrasted to other computer and AI\nsystems; 3) a logical taxonomy of the legal issues these systems raise; and, 4)\na concrete research agenda to promote collaboration and knowledge-sharing on\nemerging issues at the intersection of Generative AI and law. In this report,\nwe synthesize the key takeaways from the GenLaw workshop that begin to address\nthese needs. All of the listed authors contributed to the workshop upon which\nthis report is based, but they and their organizations do not necessarily\nendorse all of the specific claims in this report.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.07803,review,post_llm,2023,11,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""Designing Theory of Computing Backwards\n\n  The design of any technical Computer Science course must involve its context\nwithin the institution's CS program, but also incorporate any new material that\nis relevant and appropriately accessible to students. In many institutions,\ntheory of computing (ToC) courses within undergraduate CS programs are often\nplaced near the end of the program, and have a very common structure of\nbuilding off previous sections of the course. The central question behind any\nsuch course is ``What are the limits of computers?'' for various types of\ncomputational models. However, what is often intuitive for students about what\na ``computer'' is--a Turing machine--is taught at the end of the course, which\nnecessitates motivation for earlier models. This poster contains our\nexperiences in designing a ToC course that teaches the material effectively\n``backwards,'' with pedagogic motivation of instead asking the question ``What\nsuitable restrictions can we place on computers to make their problems\ntractable?'' We also give recommendations for future course design.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14689,regular,post_llm,2023,11,"{'ai_likelihood': 5.771716435750326e-05, 'text': ""Analyze Factors Influencing Drivers' Cell Phone Online Ride-hailing\n  Software Using While driving: A Case Study in China\n\n  The road safety of traffic is greatly affected by the driving performance of\nonline ride-hailing, which has become an increasingly popular travel option for\nmany people. Little attention has been paid to the fact that the use of cell\nphone online ride-hailing software by drivers to accept orders while driving is\none of the causes of traffic accidents involving online ride-hailing. This\npaper, adopting the extended theory of planned behavior, investigates the\nfactors that factors influencing the behavior of Chinese online ride-hailing\ndrivers cell phone ride-hailing software usage to accept orders while driving.\nResults showed that attitudes, subjective norms, and perceived behavioral\ncontrol have a significant and positive effect on behavioral intentions.\nBehavioral intention is most strongly influenced by attitude. There is no\ndirect and significant impact of group norms on behavioral intention.\nNonetheless, group norms exert a substantial and beneficial influence on\nattitude, subjective norms, and perceived behavioral control. This study has\ndiscovered, through a mediating effect test, that attitude, subjective norm,\nand perceived behavioral control play a mediating and moderating role in the\nimpact of group norm on behavioral intention. These findings can offer\ntheoretical guidance to relevant departments in developing effective measures\nfor promoting safe driving among online ride-hailing drivers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2311.14678,review,post_llm,2023,11,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'Data-driven recommendations for enhancing real-time natural hazard\n  warnings, communication, and response\n\n  The effectiveness and adequacy of natural hazard warnings hinges on the\navailability of data and its transformation into actionable knowledge for the\npublic. Real-time warning communication and emergency response therefore need\nto be evaluated from a data science perspective. However, there are currently\ngaps between established data science best practices and their application in\nsupporting natural hazard warnings. This Perspective reviews existing\ndata-driven approaches that underpin real-time warning communication and\nemergency response, highlighting limitations in hazard and impact forecasts.\nFour main themes for enhancing warnings are emphasised: (i) applying\nbest-practice principles in visualising hazard forecasts, (ii) data\nopportunities for more effective impact forecasts, (iii) utilising data for\nmore localised forecasts, and (iv) improving data-driven decision-making using\nuncertainty. Motivating examples are provided from the extensive flooding\nexperienced in Australia in 2022. This Perspective shows the capacity for\nimproving the efficacy of natural hazard warnings using data science, and the\ncollaborative potential between the data science and natural hazards\ncommunities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.11505,regular,post_llm,2023,12,"{'ai_likelihood': 3.5100513034396704e-06, 'text': ""Biscari Network. Tutti gli uomini del principe\n\n  Thanks to its heterogeneity, the Biscari Archive, one of the most\nrepresentative family's archives in Sicily, in a new digital historical study,\nbecame a valuable set of computable data that can lead historians to\nreconstruct the history of the city of Catania and Sicily. Ignazio Paterno'\nCastello and his wife Anna, princes of Biscari, were the promoters of the\ncity's reconstruction after the 1693 earthquake, both politically and\nculturally. How could the digital historical methodology fulfil the traditional\nHistoriography gap about how this noble family built its mighty? As we know,\nHumanities cannot easily be encapsulated in a few understandable numbers and\nnames. However, historians, boosting Artificial Intelligence, such as\nTranskribus, and applying Historical Networks Analysis could help computers\ninfer computable meaning from the digitised historical primary source. The\nTuring Machine became the most powerful tool to help historians understand what\nhappened in the Past and identify the actors in cities and places' cultural and\npolitical renewal.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.11289,regular,post_llm,2023,12,"{'ai_likelihood': 0.001994238959418403, 'text': 'Learning to Generate Pseudo Personal Mobility\n\n  The importance of personal mobility data is widely recognized in various\nfields. However, the utilization of real personal mobility data raises privacy\nconcerns. Therefore, it is crucial to generate pseudo personal mobility data\nthat accurately reflects real-world mobility patterns while safeguarding user\nprivacy. Nevertheless, existing methods for generating pseudo mobility data,\nsuch as mechanism-based and deep-learning-based approaches, have limitations in\ncapturing sufficient individual heterogeneity. To address these gaps, taking\npseudo-person(avatar) as ground-zero, a novel individual-based human mobility\ngenerator called GeoAvatar has been proposed - which considers individual\nheterogeneity in spatial and temporal decision-making, incorporates demographic\ncharacteristics, and provides interpretability. Our method utilizes a deep\ngenerative model to simulate heterogeneous individual life patterns, a reliable\nlabeler for inferring individual demographic characteristics, and a Bayesian\napproach for generating spatial choices. Through our method, we have achieved\nthe generation of heterogeneous individual human mobility data without\naccessing individual-level personal information, with good quality - we\nevaluated the proposed method based on physical features, activity patterns,\nand spatial-temporal characteristics, demonstrating its good performance,\ncompared to mechanism-based modeling and black-box deep learning approaches.\nFurthermore, this method maintains extensibility for broader applications,\nmaking it a promising paradigm for generating human mobility data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.09359,review,post_llm,2023,12,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Children, Parents, and Misinformation on Social Media\n\n  Children encounter misinformation on social media in a similar capacity as\ntheir parents. Unlike their parents, children are an exceptionally vulnerable\npopulation because their cognitive abilities and emotional regulation are still\nmaturing, rendering them more susceptible to misinformation and falsehoods\nonline. Yet, little is known about children\'s experience with misinformation as\nwell as what their parents think of the misinformation\'s effect on child\ndevelopment. To answer these questions, we combined a qualitative survey of\nparents (n=87) with semi-structured interviews of both parents and children\n(n=12). We found that children usually encounter deep fakes, memes with\npolitical context, or celebrity/influencer rumors on social media. Children\nrevealed they ""ask Siri"" whether a social media video or post is true or not\nbefore they search on Google or ask their parents about it. Parents expressed\ndiscontent that their children are impressionable to misinformation, stating\nthat the burden falls on them to help their children develop critical thinking\nskills for navigating falsehoods on social media. Here, the majority of parents\nfelt that schools should also teach these skills as well as media literacy to\ntheir children. Misinformation, according to both parents and children affects\nthe family relationships especially with grandparents with different political\nviews than theirs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10059,review,post_llm,2023,12,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'A collection of principles for guiding and evaluating large language\n  models\n\n  Large language models (LLMs) demonstrate outstanding capabilities, but\nchallenges remain regarding their ability to solve complex reasoning tasks, as\nwell as their transparency, robustness, truthfulness, and ethical alignment. In\nthis preliminary study, we compile a set of core principles for steering and\nevaluating the reasoning of LLMs by curating literature from several relevant\nstrands of work: structured reasoning in LLMs, self-evaluation/self-reflection,\nexplainability, AI system safety/security, guidelines for human critical\nthinking, and ethical/regulatory guidelines for AI. We identify and curate a\nlist of 220 principles from literature, and derive a set of 37 core principles\norganized into seven categories: assumptions and perspectives, reasoning,\ninformation and evidence, robustness and security, ethics, utility, and\nimplications. We conduct a small-scale expert survey, eliciting the subjective\nimportance experts assign to different principles and lay out avenues for\nfuture work beyond our preliminary results. We envision that the development of\na shared model of principles can serve multiple purposes: monitoring and\nsteering models at inference time, improving model behavior during training,\nand guiding human evaluation of model reasoning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.14804,review,post_llm,2023,12,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Using large language models to promote health equity\n\n  Advances in large language models (LLMs) have driven an explosion of interest\nabout their societal impacts. Much of the discourse around how they will impact\nsocial equity has been cautionary or negative, focusing on questions like ""how\nmight LLMs be biased and how would we mitigate those biases?"" This is a vital\ndiscussion: the ways in which AI generally, and LLMs specifically, can entrench\nbiases have been well-documented. But equally vital, and much less discussed,\nis the more opportunity-focused counterpoint: ""what promising applications do\nLLMs enable that could promote equity?"" If LLMs are to enable a more equitable\nworld, it is not enough just to play defense against their biases and failure\nmodes. We must also go on offense, applying them positively to equity-enhancing\nuse cases to increase opportunities for underserved groups and reduce societal\ndiscrimination. There are many choices which determine the impact of AI, and a\nfundamental choice very early in the pipeline is the problems we choose to\napply it to. If we focus only later in the pipeline -- making LLMs marginally\nmore fair as they facilitate use cases which intrinsically entrench power -- we\nwill miss an important opportunity to guide them to equitable impacts. Here, we\nhighlight the emerging potential of LLMs to promote equity by presenting four\nnewly possible, promising research directions, while keeping risks and\ncautionary points in clear view.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.12338,review,post_llm,2023,12,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Smart Connected Farms and Networked Farmers to Tackle Climate Challenges\n  Impacting Agricultural Production\n\n  To meet the grand challenges of agricultural production including climate\nchange impacts on crop production, a tight integration of social science,\ntechnology and agriculture experts including farmers are needed. There are\nrapid advances in information and communication technology, precision\nagriculture and data analytics, which are creating a fertile field for the\ncreation of smart connected farms (SCF) and networked farmers. A network and\ncoordinated farmer network provides unique advantages to farmers to enhance\nfarm production and profitability, while tackling adverse climate events. The\naim of this article is to provide a comprehensive overview of the state of the\nart in SCF including the advances in engineering, computer sciences, data\nsciences, social sciences and economics including data privacy, sharing and\ntechnology adoption.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.03901,regular,post_llm,2023,12,"{'ai_likelihood': 3.2782554626464844e-06, 'text': 'Redrawing the 2012 map of the Maryland congressional districts\n\n  Gerrymandering is the practice of drawing biased electoral maps that\nmanipulate the voter population to gain an advantage. The most recent time\ngerrymandering became an issue was 2019 when the U.S. Federal Supreme Court\ndecided that the court does not have the authority to dictate how to draw the\ndistrict map and state legislators are the ones who should come up with an\nelectoral district plan. We solve the political districting problem and redraw\nthe 2012 map of Maryland congressional districts which raised the issue in\n2019.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.04616,regular,post_llm,2023,12,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Can apparent bystanders distinctively shape an outcome? Global south\n  countries and global catastrophic risk-focused governance of artificial\n  intelligence\n\n  Increasingly, there is well-grounded concern that through perpetual\nscaling-up of computation power and data, current deep learning techniques will\ncreate highly capable artificial intelligence that could pursue goals in a\nmanner that is not aligned with human values. In turn, such AI could have the\npotential of leading to a scenario in which there is serious global-scale\ndamage to human wellbeing. Against this backdrop, a number of researchers and\npublic policy professionals have been developing ideas about how to govern AI\nin a manner that reduces the chances that it could lead to a global\ncatastrophe. The jurisdictional focus of a vast majority of their assessments\nso far has been the United States, China, and Europe. That preference seems to\nreveal an assumption underlying most of the work in this field: That global\nsouth countries can only have a marginal role in attempts to govern AI\ndevelopment from a global catastrophic risk -focused perspective. Our paper\nsets out to undermine this assumption. We argue that global south countries\nlike India and Singapore (and specific coalitions) could in fact be fairly\nconsequential in the global catastrophic risk-focused governance of AI. We\nsupport our position using 4 key claims. 3 are constructed out of the current\nways in which advanced foundational AI models are built and used while one is\nconstructed on the strategic roles that global south countries and coalitions\nhave historically played in the design and use of multilateral rules and\ninstitutions. As each claim is elaborated, we also suggest some ways through\nwhich global south countries can play a positive role in designing,\nstrengthening and operationalizing global catastrophic risk-focused AI\ngovernance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10225,regular,post_llm,2023,12,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""Customizing Large Language Models for Business Context: Framework and\n  Experiments\n\n  The advent of Large Language Models (LLMs) has ushered in a new era for\ndesign science in Information Systems, demanding a paradigm shift in tailoring\nLLMs design for business contexts. We propose and test a novel framework to\ncustomize LLMs for general business contexts that aims to achieve three\nfundamental objectives simultaneously: (1) aligning conversational patterns,\n(2) integrating in-depth domain knowledge, and (3) embodying theory-driven soft\nskills and core principles. We design methodologies that combine\ndomain-specific theory with Supervised Fine Tuning (SFT) to achieve these\nobjectives simultaneously. We instantiate our proposed framework in the context\nof medical consultation. Specifically, we carefully construct a large volume of\nreal doctors' consultation records and medical knowledge from multiple\nprofessional databases. Additionally, drawing on medical theory, we identify\nthree soft skills and core principles of human doctors: professionalism,\nexplainability, and emotional support, and design approaches to integrate these\ntraits into LLMs. We demonstrate the feasibility of our framework using online\nexperiments with thousands of real patients as well as evaluation by domain\nexperts and consumers. Experimental results show that the customized LLM model\nsubstantially outperforms untuned base model in medical expertise as well as\nconsumer satisfaction and trustworthiness, and it substantially reduces the gap\nbetween untuned LLMs and human doctors, elevating LLMs to the level of human\nexperts. Additionally, we delve into the characteristics of textual\nconsultation records and adopt interpretable machine learning techniques to\nidentify what drives the performance gain. Finally, we showcase the practical\nvalue of our model through a decision support system designed to assist human\ndoctors in a lab experiment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.04777,regular,post_llm,2023,12,"{'ai_likelihood': 7.881058586968316e-06, 'text': 'Inclusive Online Learning in Australia: Barriers and Enablers\n\n  While the pandemic highlighted the critical role technology plays in\nchildren\'s lives, not all Australian children have reliable access to\ntechnology. This situation exacerbates educational disadvantage for children\nwho are already amongst our nation\'s most vulnerable. In this research project,\nwe carried out a pilot project with three schools in Western Australia,\nconducting a series of workshops and interviews with students, parents, school\nstaff members, and teachers. Drawing on rich empirical material, we identify\nkey barriers and enablers for digitally inclusive online learning at the\nindividual, interpersonal, organizational, and infrastructural levels. Of\nparticular importance is that technology is only part of this story - an array\nof social, environmental, and skills ""infrastructure"" is needed to facilitate\ninclusive online learning. Building on this finding, we ran a Digital Inclusion\nStudio to address this holistic set of issues with strongly positive feedback\nfrom participants. We conclude with a set of recommendations for stakeholders\n(parents, schools, government agencies) who wish to support more digitally\ninclusive learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.05629,regular,post_llm,2023,12,"{'ai_likelihood': 0.0004182921515570747, 'text': ""Enhancing Situational Awareness in Surveillance: Leveraging Data\n  Visualization Techniques for Machine Learning-based Video Analytics Outcomes\n\n  The pervasive deployment of surveillance cameras produces a massive volume of\ndata, requiring nuanced interpretation. This study thoroughly examines data\nrepresentation and visualization techniques tailored for AI surveillance data\nwithin current infrastructures. It delves into essential data metrics, methods\nfor situational awareness, and various visualization techniques, highlighting\ntheir potential to enhance safety and guide urban development. This study is\nbuilt upon real-world research conducted in a community college environment,\nutilizing eight cameras over eight days. This study presents tools like the\nOccupancy Indicator, Statistical Anomaly Detection, Bird's Eye View, and\nHeatmaps to elucidate pedestrian behaviors, surveillance, and public safety.\nGiven the intricate data from smart video surveillance, such as bounding boxes\nand segmented images, we aim to convert these computer vision results into\nintuitive visualizations and actionable insights for stakeholders, including\nlaw enforcement, urban planners, and social scientists. The results emphasize\nthe crucial impact of visualizing AI surveillance data on emergency handling,\npublic health protocols, crowd control, resource distribution, predictive\nmodeling, city planning, and informed decision-making.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.02093,review,post_llm,2023,12,"{'ai_likelihood': 6.159146626790365e-06, 'text': ""Cultural Differences in Students' Privacy Concerns in Learning Analytics\n  across Germany, South Korea, Spain, Sweden, and the United States\n\n  Applications of learning analytics (LA) can raise concerns from students\nabout their privacy in higher education contexts. Developing effective\nprivacy-enhancing practices requires a systematic understanding of students'\nprivacy concerns and how they vary across national and cultural dimensions. We\nconducted a survey study with established instruments to measure privacy\nconcerns and cultural values for university students in five countries\n(Germany, South Korea, Spain, Sweden, and the United States; N = 762). The\nresults show that students generally trusted institutions with their data and\ndisclosed information as they perceived the risks to be manageable even though\nthey felt somewhat limited in their ability to control their privacy. Across\nthe five countries, German and Swedish students stood out as the most trusting\nand least concerned, especially compared to US students who reported greater\nperceived risk and less control. Students in South Korea and Spain responded\nsimilarly on all five privacy dimensions (perceived privacy risk, perceived\nprivacy control, privacy concerns, trusting beliefs, and non-self-disclosure\nbehavior), despite their significant cultural differences. Culture measured at\nthe individual level affected the antecedents and outcomes of privacy concerns\nmore than country-level culture. Perceived privacy risk and privacy control\nincrease with power distance. Trusting beliefs increase with a desire for\nuncertainty avoidance and lower masculinity. Non-self-disclosure behaviors rise\nwith power distance and masculinity, and decrease with more uncertainty\navoidance. Thus, cultural values related to trust in institutions, social\nequality and risk-taking should be considered when developing privacy-enhancing\npractices and policies in higher education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.12817,review,post_llm,2023,12,"{'ai_likelihood': 2.9272503323025175e-05, 'text': ""Research on the Development of Blockchain-based Distributed Intelligent\n  Healthcare Industry -- A Policy Analysis Perspective\n\n  As a pivotal innovation in digital infrastructure, blockchain ledger\ntechnology catalyzes the development of nascent business paradigms and\napplications globally. Utilizing Rothwell and Zegveld's taxonomy of twelve\ninnovation policy tools, this study offers a nuanced comparison of domestic\nblockchain policies, dissecting supply, environment, and demand-driven policy\ndimensions to distill prevailing strategic orientations towards blockchain\nhealthcare adoption. The findings indicate that blockchain technology has seen\nrapid growth in the healthcare industry. However, a certain misalignment exists\nbetween the corporate and policy layers in terms of supply and demand. While\ncompanies focus more on technological applications, existing policies are\ngeared towards regulations and governance. Government emphasis lies on legal\nsupervision through environmental policies, aiming to guide the standardization\nand regulation of blockchain technology. This maintains a balance between\nencouraging innovation and market and legal regulatory order, thereby providing\na reference for the development of the distributed intelligent healthcare\nindustry in our country.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.00629,review,post_llm,2023,12,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'The Ecosystem of Trust (EoT): Enabling effective deployment of\n  autonomous systems through collaborative and trusted ecosystems\n\n  Ecosystems are ubiquitous but trust within them is not guaranteed. Trust is\nparamount because stakeholders within an ecosystem must collaborate to achieve\ntheir objectives. With the twin transitions, digital transformation to go in\nparallel with green transition, accelerating the deployment of autonomous\nsystems, trust has become even more critical to ensure that the deployed\ntechnology creates value. To address this need, we propose an ecosystem of\ntrust approach to support deployment of technology by enabling trust among and\nbetween stakeholders, technologies and infrastructures, institutions and\ngovernance, and the artificial and natural environments in an ecosystem. The\napproach can help the stakeholders in the ecosystem to create, deliver, and\nreceive value by addressing their concerns and aligning their objectives. We\npresent an autonomous, zero emission ferry as a real world use case to\ndemonstrate the approach from a stakeholder perspective. We argue that\nassurance, defined as grounds for justified confidence originated from evidence\nand knowledge, is a prerequisite to enable the approach. Assurance provides\nevidence and knowledge that are collected, analysed, and communicated in a\nsystematic, targeted, and meaningful way. Assurance can enable the approach to\nhelp successfully deploy technology by ensuring that risk is managed, trust is\nshared, and value is created.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.0001,review,post_llm,2023,12,"{'ai_likelihood': 0.025431315104166668, 'text': 'Going Viral: An Analysis of Advertising of Technology Products on TikTok\n\n  Social media has transformed the advertising landscape, becoming an essential\ntool for reaching and connecting with consumers. Its sharing and engagement\nfeatures amplify brand exposure, while its cost-effective options provide\nbusinesses with flexible advertising solutions. TikTok is a more recent social\nmedia platform that has gained popularity for advertising, particularly in the\nrealm of e-commerce, due to its large user base and viral nature. TikTok had\n1.2 billion monthly active users in Q4 2021, generating an estimated $4.6\nbillion revenue in 2021. Virality can lead to a massive increase in brand\nexposure, reaching a vast audience that may not have been accessible through\ntraditional marketing efforts alone. Advertisements for technological products\nare an example of such viral ads that are abundant on TikTok. The goal of this\nthesis is to understand how creators, community activity, and the\nrecommendation algorithm influence the virality of advertisements for\ntechnology products on TikTok. The study analyzes various aspects of virality,\nincluding sentiment analysis, content characteristics, and the role of\ninfluencers. It employs data scraping and natural language processing tools to\nanalyze metadata from 2,000 TikTok posts and 274,651, offering insights into\nthe nuances of viral tech product advertising on TikTok.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.04714,regular,post_llm,2023,12,"{'ai_likelihood': 1.0232130686442058e-05, 'text': ""The Potential Impact of AI Innovations on U.S. Occupations\n\n  An occupation is comprised of interconnected tasks, and it is these tasks,\nnot occupations themselves, that are affected by AI. To evaluate how tasks may\nbe impacted, previous approaches utilized manual annotations or coarse-grained\nmatching. Leveraging recent advancements in machine learning, we replace\ncoarse-grained matching with more precise deep learning approaches. Introducing\nthe AI Impact (AII) measure, we employ Deep Learning Natural Language\nProcessing to automatically identify AI patents that may impact various\noccupational tasks at scale. Our methodology relies on a comprehensive dataset\nof 17,879 task descriptions and quantifies AI's potential impact through\nanalysis of 24,758 AI patents filed with the United States Patent and Trademark\nOffice (USPTO) between 2015 and 2022. Our results reveal that some occupations\nwill potentially be impacted, and that impact is intricately linked to specific\nskills. These include not only routine tasks (codified as a series of steps),\nas previously thought, but also non-routine ones (e.g., diagnosing health\nconditions, programming computers, and tracking flight routes). However, AI's\nimpact on labour is limited by the fact that some of the occupations affected\nare augmented rather than replaced (e.g., neurologists, software engineers, air\ntraffic controllers), and the sectors affected are experiencing labour\nshortages (e.g., IT, Healthcare, Transport).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10819,regular,post_llm,2023,12,"{'ai_likelihood': 0.005484686957465278, 'text': 'Satellite Data Shows Resilience of Tigrayan Farmers in Crop Cultivation\n  During Civil War\n\n  The Tigray War was an armed conflict that took place primarily in the Tigray\nregion of northern Ethiopia from November 3, 2020 to November 2, 2022. Given\nthe importance of agriculture in Tigray to livelihoods and food security,\ndetermining the impact of the war on cultivated area is critical. However,\nquantifying this impact was difficult due to restricted movement within and\ninto the region and conflict-driven insecurity and blockages. Using satellite\nimagery and statistical area estimation techniques, we assessed changes in crop\ncultivation area in Tigray before and during the war. Our findings show that\ncultivated area was largely stable between 2020-2021 despite the widespread\nimpacts of the war. We estimated $1,132,000\\pm133,000$ hectares of cultivation\nin pre-war 2020 compared to $1,217,000 \\pm 132,000$ hectares in wartime 2021.\nComparing changes inside and outside of a 5 km buffer around conflict events,\nwe found a slightly higher upper confidence limit of cropland loss within the\nbuffer (0-3%) compared to outside the buffer (0-1%). Our results support other\nreports that despite widespread war-related disruptions, Tigrayan farmers were\nlargely able to sustain cultivation. Our study demonstrates the capability of\nremote sensing combined with machine learning and statistical techniques to\nprovide timely, transparent area estimates for monitoring food security in\nregions inaccessible due to conflict.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.05241,review,post_llm,2023,12,"{'ai_likelihood': 1.0, 'text': 'Contra generative AI detection in higher education assessments\n\n  This paper presents a critical analysis of generative Artificial Intelligence\n(AI) detection tools in higher education assessments. The rapid advancement and\nwidespread adoption of generative AI, particularly in education, necessitates a\nreevaluation of traditional academic integrity mechanisms. We explore the\neffectiveness, vulnerabilities, and ethical implications of AI detection tools\nin the context of preserving academic integrity. Our study synthesises insights\nfrom various case studies, newspaper articles, and student testimonies to\nscrutinise the practical and philosophical challenges associated with AI\ndetection. We argue that the reliance on detection mechanisms is misaligned\nwith the educational landscape, where AI plays an increasingly widespread role.\nThis paper advocates for a strategic shift towards robust assessment methods\nand educational policies that embrace generative AI usage while ensuring\nacademic integrity and authenticity in assessments.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.000759124755859375, 'GPT4': 0.96044921875, 'CLAUDE': 0.0005879402160644531, 'GOOGLE': 0.0372314453125, 'OPENAI_O_SERIES': 0.0007681846618652344, 'DEEPSEEK': 3.057718276977539e-05, 'GROK': 1.2576580047607422e-05, 'NOVA': 0.00016069412231445312, 'OTHER': 0.0001552104949951172, 'HUMAN': 6.54458999633789e-05}}"
2312.1423,regular,post_llm,2023,12,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Views on AI aren\'t binary -- they\'re plural\n\n  Recent developments in AI have brought broader attention to tensions between\ntwo overlapping communities, ""AI Ethics"" and ""AI Safety."" In this article we\n(i) characterize this false binary, (ii) argue that a simple binary is not an\naccurate model of AI discourse, and (iii) provide concrete suggestions for how\nindividuals can help avoid the emergence of us-vs-them conflict in the broad\ncommunity of people working on AI development and governance. While we focus on\n""AI Ethics"" an ""AI Safety,"" the general lessons apply to related tensions,\nincluding those between accelerationist (""e/acc"") and cautious stances on AI\ndevelopment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.1262,review,post_llm,2023,12,"{'ai_likelihood': 3.675619761149089e-06, 'text': '""It Can Relate to Real Lives"": Attitudes and Expectations in Justice-Centered Data Structures & Algorithms for Non-Majors\n\nPrior work has argued for a more justice-centered approach to postsecondary computing education by emphasizing ethics, identity, and political vision. In this experience report, we examine how postsecondary students of diverse gender and racial identities experience a justice-centered Data Structures and Algorithms designed for undergraduate non-computer science majors. Through a quantitative and qualitative analysis of two quarters of student survey data collected at the start and end of each quarter, we report on student attitudes and expectations.\n  Across the class, we found a significant increase in the following attitudes: computing confidence and sense of belonging. While women, non-binary, and other students not identifying as men (WNB+) also increased in these areas, they still reported significantly lower confidence and sense of belonging than men at the end of the quarter. Black, Latinx, Middle Eastern and North African, Native American, and Pacific Islander (BLMNPI) students had no significant differences compared to white and Asian students.\n  We also analyzed end-of-quarter student self-reflections on their fulfillment of expectations prior to taking the course. While the majority of students reported a positive overall sentiment about the course and many students specifically appreciated the justice-centered approach, some desired more practice with program implementation and interview preparation. We discuss implications for practice and articulate a political vision for holding both appreciation for computing ethics and a desire for professional preparation together through iterative design.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.10897,regular,post_llm,2023,12,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Transformations in the Time of The Transformer\n\n  Foundation models offer a new opportunity to redesign existing systems and\nworkflows with a new AI first perspective. However, operationalizing this\nopportunity faces several challenges and tradeoffs. The goal of this article is\nto offer an organizational framework for making rational choices as enterprises\nstart their transformation journey towards an AI first organization. The\nchoices provided are holistic, intentional and informed while avoiding\ndistractions. The field may appear to be moving fast, but there are core\nfundamental factors that are relatively more slow moving. We focus on these\ninvariant factors to build the logic of the argument.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.00337,regular,post_llm,2023,12,"{'ai_likelihood': 6.788306766086155e-06, 'text': 'Dynamic Matrix of Extremisms and Terrorism (DMET): A Continuum Approach\n  Towards Identifying Different Degrees of Extremisms\n\n  We propose to extend the current binary understanding of terrorism (versus\nnon-terrorism) with a Dynamic Matrix of Extremisms and Terrorism (DMET). DMET\nconsiders the whole ecosystem of content and actors that can contribute to a\ncontinuum of extremism (e.g., right-wing, left-wing, religious, separatist,\nsingle-issue). It organizes levels of extremisms by varying degrees of\nideological engagement and the presence of violence identified (e.g., partisan,\nfringe, violent extremism, terrorism) based on cognitive and behavioral cues\nand group dynamics. DMET is globally applicable due to its comprehensive\nconceptualization of the levels of extremisms. It is also dynamic, enabling\niterative mapping with the region- and time-specific classifications of\nextremist actors. Once global actors recognize DMET types and their distinct\ncharacteristics, they can comprehensively analyze the profiles of extremist\nactors (e.g., individuals, groups, movements), track these respective actors\nand their activities (e.g., social media content) over time, and launch\ntargeted counter activities (e.g. de-platforming, content moderation, or\nredirects to targeted CVE narratives).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.08659,review,post_llm,2023,12,"{'ai_likelihood': 0.0017823113335503472, 'text': 'Generative AI and Its Educational Implications\n\n  We discuss the implications of generative AI on education across four\ncritical sections: the historical development of AI in education, its\ncontemporary applications in learning, societal repercussions, and strategic\nrecommendations for researchers. We propose ways in which generative AI can\ntransform the educational landscape, primarily via its ability to conduct\nassessment of complex cognitive performances and create personalized content.\nWe also address the challenges of effective educational tool deployment, data\nbias, design transparency, and accurate output verification. Acknowledging the\nsocietal impact, we emphasize the need for updating curricula, redefining\ncommunicative trust, and adjusting to transformed social norms. We end by\noutlining the ways in which educational stakeholders can actively engage with\ngenerative AI, develop fluency with its capacities and limitations, and apply\nthese insights to steer educational practices in a rapidly advancing digital\nlandscape.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.15,review,post_llm,2023,12,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""The Impact of Cloaking Digital Footprints on User Privacy and\n  Personalization\n\n  Our online lives generate a wealth of behavioral records -'digital\nfootprints'- which are stored and leveraged by technology platforms. This data\ncan be used to create value for users by personalizing services. At the same\ntime, however, it also poses a threat to people's privacy by offering a highly\nintimate window into their private traits (e.g., their personality, political\nideology, sexual orientation). Prior work has proposed a potential remedy: The\ncloaking of users' footprints. That is, platforms could allow users to hide\nportions of their digital footprints from predictive algorithms to avoid\nundesired inferences. While such an approach has been shown to offer privacy\nprotection in the moment, there are two open questions. First, it remains\nunclear how well cloaking performs over time. As people constantly leave new\ndigital footprints, the algorithm might regain the ability to predict\npreviously cloaked traits. Second, cloaking digital footprints to avoid one\nundesirable inference may degrade the performance of models for other,\ndesirable inferences (e.g., those driving desired personalized content). In the\nlight of these research gaps, our contributions are twofold: 1) We propose a\nnovel cloaking strategy that conceals 'metafeatures' (automatically generated\nhigher-level categories) and compares its effectiveness against existing\ncloaking approaches, and 2) we test the spill-over effects of cloaking one\ntrait on the accuracy of inferences on other traits. A key finding is that the\neffectiveness of cloaking degrades over times, but the rate at which it\ndegrades is significantly smaller when cloaking metafeatures rather than\nindividual footprints. In addition, our findings reveal the expected trade-off\nbetween privacy and personalization: Cloaking an undesired trait also partially\nconceals other desirable traits.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10092,regular,post_llm,2023,12,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Introspecting the Happiness amongst University Students using Machine\n  Learning\n\n  Happiness underlines the intuitive constructs of a specified population based\non positive psychological outcomes. It is the cornerstone of the cognitive\nskills and exploring university student's happiness has been the essence of the\nresearchers lately. In this study, we have analyzed the university student's\nhappiness and its facets using statistical distribution charts; designing\nresearch questions. Furthermore, regression analysis, machine learning, and\nclustering algorithms were applied on the world happiness dataset and\nuniversity student's dataset for training and testing respectively. Philosophy\nwas the happiest department while Sociology the saddest; average happiness\nscore being 2.8 and 2.44 respectively. Pearson coefficient of correlation was\n0.74 for Health. Predicted happiness score was 5.2 and the goodness of model\nfit was 51%. train and test error being 0.52, 0.47 respectively. On a\nConfidence Interval(CI) of 5% p-value was least for Campus Environment(CE) and\nUniversity Reputation(UR) and maximum for Extra-curricular Activities(ECA) and\nWork Balance(WB) (i.e. 0.184 and 0.228 respectively). RF with Clustering got\nthe highest accuracy(89%) and F score(0.98) and the least error(17.91%), hence\nturned out to be best for our study\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.07565,regular,post_llm,2023,12,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'On the Use of Smart Hybrid Contracts to Provide Flexibility in\n  Algorithmic Governance\n\n  The use of computer technology to automate the enforcement of law is a\npromising alternative to simplify bureaucratic procedures. However, careless\nautomation might result in an inflexible and dehumanise law enforcement system\ndriven by algorithms that do not account for the particularities of individuals\nor minorities. In this paper, we argue that hybrid smart contracts deployed to\nmonitor rather than to blindly enforce regulations can be used to add\nflexibility. Enforcement is a suitable alternative only when prevention is\nstrictly necessary; however, we argue that in many situations a corrective\napproach based on monitoring is more flexible and suitable. To add more\nflexibility, the hybrid smart contract can be programmed to stop to request the\nintervention of a human or of a group of them when human judgement is needed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.07789,regular,post_llm,2023,12,"{'ai_likelihood': 2.317958407931858e-06, 'text': ""Impact of Computer-Based Assessments on the Science's Ranks of Secondary\n  Students\n\n  This study reports the impact of examining either with digital or paper-based\ntests in science subjects taught across the second-ary level. With our method,\nwe compare the percentile ranking scores of two cohorts earned in computer- and\npaper-based teacher-made assessments to find signals of a testing mode effect.\nIt was found that overall, at cohort and gender levels, pupils were\nrank-ordered equivalently in both testing modes. Furthermore, females and\ntop-achieving pupils were the two subgroups where the differences between modes\nwere smaller. The practical implications of these findings are discussed from\nthe lens of a case study and the doubt about whether regular schools could\nafford to deliver high-stakes computer-based tests.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.08237,review,post_llm,2023,12,"{'ai_likelihood': 0.9365234375, 'text': ""From Brussels Effect to Gravity Assists: Understanding the Evolution of\n  the GDPR-Inspired Personal Information Protection Law in China\n\n  This paper explores the evolution of China's Personal Information Protection\nLaw (PIPL) and situates it within the context of global data protection\ndevelopment. It draws inspiration from the theory of 'Brussels Effect' and\nprovides a critical account of its application in non-Western jurisdictions,\ntaking China as a prime example. Our objective is not to provide a comparative\ncommentary on China's legal development but to illuminate the intricate\ndynamics between the Chinese law and the EU's GDPR. We argue that the\ntrajectory of China's Personal Information Protection Law calls into question\nthe applicability of the Brussels Effect: while the GDPR's imprint on the PIPL\nis evident, a deeper analysis unveils China's nuanced, non-linear adoption that\ndiverges from many assumptions of the Brussels Effect and similar theories. The\nevolution of the GDPR-inspired PIPL is not as a straightforward outcome of the\nBrussels Effect but as a nuanced, intricate interplay of external influence and\ndomestic dynamics. We introduce a complementary theory of 'gravity assist',\nwhich portrays China's strategic instrumentalisation of the GDPR as a template\nto shape its unique data protection landscape. Our theoretical framework\nhighlights how China navigates through a patchwork of internal considerations,\ninternational standards, and strategic choices, ultimately sculpting a data\nprotection regime that has a similar appearance to the GDPR but aligns with its\ndistinct political, cultural and legal landscape. With a detailed historical\nand policy analysis of the PIPL, coupled with reasonable speculations on its\nfuture avenues, our analysis presents a pragmatic, culturally congruent\napproach to legal development in China. It signals a trajectory that, while\npotentially converging at a principled level, is likely to diverge\nsignificantly in practice [...]\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.3245811462402344e-05, 'GPT4': 0.99951171875, 'CLAUDE': 5.662441253662109e-06, 'GOOGLE': 0.00015783309936523438, 'OPENAI_O_SERIES': 5.960464477539062e-07, 'DEEPSEEK': 9.5367431640625e-07, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 0.0004858970642089844}}"
2312.10551,regular,post_llm,2023,12,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Predicting Regional Road Transport Emissions From Satellite Imagery\n\n  This paper presents a novel two-part pipeline for monitoring progress towards\nthe UN Sustainable Development Goals (SDG's) related to Climate Action and\nSustainable Cities and Communities. The pipeline consists of two main parts:\nthe first part takes a raw satellite image of a motorway section and produces\ntraffic count predictions for count sites within the image; the second part\ntakes these predicted traffic counts and other variables to produce estimates\nof Local Authority (LA) motorway Average Annual Daily Traffic (AADT) and\nGreenhouse Gas (GHG) emissions on a per vehicle type basis. We also provide\nflexibility to the pipeline by implementing a novel method for estimating\nemissions when data on AADT per vehicle type or/and live vehicle speeds are not\navailable. Finally, we extend the pipeline to also estimate LA A-Roads and\nminor roads AADT and GHG emissions. We treat the 2017 year as training and 2018\nas the test year. Results show that it is possible to predict AADT and GHG\nemissions from satellite imagery, with motorway test year $R^2$ values of 0.92\nand 0.78 respectively, and for A-roads' $R^2$ values of 0.94 and 0.98. This\nend-to-end two-part pipeline builds upon and combines previous research in road\ntransportation traffic flows, speed estimation from satellite imagery, and\nemissions estimation, providing new contributions and insights into these\nareas.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.05415,review,post_llm,2023,12,"{'ai_likelihood': 8.040004306369358e-05, 'text': 'The Role of Generative AI in Global Diplomatic Practices: A Strategic\n  Framework\n\n  As Artificial Intelligence (AI) transforms the domain of diplomacy in the\n21st century, this research addresses the pressing need to evaluate the\ndualistic nature of these advancements, unpacking both the challenges they pose\nand the opportunities they offer. It has been almost a year since the launch of\nChatGPT by OpenAI that revolutionised various work domains with its\ncapabilities. The scope of application of these capabilities to diplomacy is\nyet to be fully explored or understood. Our research objective is to\nsystematically examine the current discourse on Digital and AI Diplomacy, thus\ninforming the development of a comprehensive framework for the role of\nGenerative AI in modern diplomatic practices. Through the systematic analysis\nof 230 scholarly articles, we identified a spectrum of opportunities and\nchallenges, culminating in a strategic framework that captures the multifaceted\nconcepts for integration of Generative AI, setting a course for future research\nand innovation in diplomacy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.06707,review,post_llm,2023,12,"{'ai_likelihood': 0.00579833984375, 'text': ""Exploring Public's Perception of Safety and Video Surveillance\n  Technology: A Survey Approach\n\n  Addressing public safety effectively requires incorporating diverse\nstakeholder perspectives, particularly those of the community, which are often\nunderrepresented compared to other stakeholders. This study presents a\ncomprehensive analysis of the community's general public safety concerns, their\nview of existing surveillance technologies, and their perception of AI-driven\nsolutions for enhancing safety in urban environments, focusing on Charlotte,\nNC. Through a survey approach, including in-person surveys conducted in August\nand September 2023 with 410 participants, this research investigates\ndemographic factors such as age, gender, ethnicity, and educational level to\ngain insights into public perception and concerns toward public safety and\npossible solutions. Based on the type of dependent variables, we utilized\ndifferent statistical and significance analyses, such as logit regression and\nordinal logistic regression, to explore the effects of demographic factors on\nthe various dependent variables. Our results reveal demographic differences in\npublic safety concerns. Younger females tend to feel less secure yet trust\nexisting video surveillance systems, whereas older, educated individuals are\nmore concerned about violent crimes in malls. Additionally, attitudes towards\nAI-driven surveillance differ: older Black individuals demonstrate support for\nit despite having concerns about data privacy, while educated females show a\ntendency towards skepticism.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.15383,review,post_llm,2023,12,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""SoK: Technical Implementation and Human Impact of Internet Privacy\n  Regulations\n\n  Growing recognition of the potential for exploitation of personal data and of\nthe shortcomings of prior privacy regimes has led to the passage of a multitude\nof new online privacy regulations. Some of these laws -- notably the European\nUnion's General Data Protection Regulation (GDPR) and the California Consumer\nPrivacy Act (CCPA) -- have been the focus of large bodies of research by the\ncomputer science community, while others have received less attention. In this\nwork, we analyze a set of Internet privacy and data protection regulations\ndrawn from around the world -- both those that have frequently been studied by\ncomputer scientists and those that have not -- and develop a taxonomy of rights\ngranted and obligations imposed by these laws. We then leverage this taxonomy\nto systematize 270 technical research papers published in computer science\nvenues that investigate the impact of these laws and explore how technical\nsolutions can complement legal protections. Finally, we analyze the results in\nthis space through an interdisciplinary lens and make recommendations for\nfuture work at the intersection of computer science and legal privacy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.14561,regular,post_llm,2023,12,"{'ai_likelihood': 1.0, 'text': 'Traffic Reconstruction and Analysis of Natural Driving Behaviors at\n  Unsignalized Intersections\n\n  This paper explores the intricacies of traffic behavior at unsignalized\nintersections through the lens of a novel dataset, combining manual video data\nlabeling and advanced traffic simulation in SUMO. This research involved\nrecording traffic at various unsignalized intersections in Memphis, TN, during\ndifferent times of the day. After manually labeling video data to capture\nspecific variables, we reconstructed traffic scenarios in the SUMO simulation\nenvironment. The output data from these simulations offered a comprehensive\nanalysis, including time-space diagrams for vehicle movement, travel time\nfrequency distributions, and speed-position plots to identify bottleneck\npoints. This approach enhances our understanding of traffic dynamics, providing\ncrucial insights for effective traffic management and infrastructure\nimprovements.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.002956390380859375, 'GPT4': 0.845703125, 'CLAUDE': 0.0011510848999023438, 'GOOGLE': 0.043731689453125, 'OPENAI_O_SERIES': 0.0015735626220703125, 'DEEPSEEK': 0.002262115478515625, 'GROK': 0.0310516357421875, 'NOVA': 0.004146575927734375, 'OTHER': 0.0672607421875, 'HUMAN': 0.00014519691467285156}}"
2312.14424,review,post_llm,2023,12,"{'ai_likelihood': 1.1656019422743056e-05, 'text': ""Lost in the Logistical Funhouse: Speculative Design as Synthetic Media\n  Enterprise\n\n  From the deployment of chatbots as procurement negotiators by corporations\nsuch as Walmart to autonomous agents providing 'differentiated chat' for\nmanaging overbooked flights, synthetic media are making the world of logistics\ntheir 'natural' habitat. Here the coordination of commodities, parts and labour\ndesign the problems and produce the training sets from which 'solutions' can be\nsynthesised. But to what extent might synthetic media, surfacing via\nproto-platforms such as MidJourney and OpenAI and apps such as Eleven Labs and\nD:ID, be understood as logistical media? This paper details synthetic media\nexperiments with 'ChatFOS', a GPT-based bot tasked with developing a logistics\ndesign business. Using its prompt-generated media outputs, we assemble a\nsimulation and parody of AI's emerging functionalities within logistical\nworlds. In the process, and with clunky 'human-in-the-loop' stitching, we\nillustrate how large language models become media routers or switches,\ngoverning production of image prompts, website code, promotional copy, and\ninvestor pitch scenarios. Together these elements become links chained together\nin media ensembles such as the corporate website or the promotional video,\nfuelling the fictive logistics visualisation company we have 'founded'. The\nprocesses and methods of producing speculative scenarios via ChatFOS lead us to\nconsider how synthetic media might be re-positioned as logistical media. Our\nexperiments probe the ways in which the media of logistics and the logistics of\nmedia are increasingly enfolded. We ask: what can a (practice-based)\narticulation of this double-becoming of logistics and synthetic mediality tell\nus about the politics and aesthetics of contemporary computation and capital?\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.17197,review,post_llm,2023,12,"{'ai_likelihood': 0.01761542426215278, 'text': 'Navigating the Research Landscape of Decentralized Autonomous\n  Organizations: A Research Note and Agenda\n\n  This note and agenda serve as a cause for thought for scholars interested in\nresearching Decentralized Autonomous Organizations (DAOs), addressing both the\nopportunities and challenges posed by this phenomenon. It covers key aspects of\ndata retrieval, data selection criteria, issues in data reliability and\nvalidity such as governance token pricing complexities, discrepancy in\ntreasuries, Mainnet and Testnet data, understanding the variety of DAO types\nand proposal categories, airdrops affecting governance, and the Sybil problem.\nThe agenda aims to equip scholars with the essential knowledge required to\nconduct nuanced and rigorous academic studies on DAOs by illuminating these\nvarious aspects and proposing directions for future research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.12915,review,post_llm,2023,12,"{'ai_likelihood': 4.238552517361111e-06, 'text': 'Survey on Multi-Document Summarization: Systematic Literature Review\n\n  In this era of information technology, abundant information is available on\nthe internet in the form of web pages and documents on any given topic. Finding\nthe most relevant and informative content out of these huge number of\ndocuments, without spending several hours of reading has become a very\nchallenging task. Various methods of multi-document summarization have been\ndeveloped to overcome this problem. The multi-document summarization methods\ntry to produce high-quality summaries of documents with low redundancy. This\nstudy conducts a systematic literature review of existing methods for\nmulti-document summarization methods and provides an in-depth analysis of\nperformance achieved by these methods. The findings of the study show that more\neffective methods are still required for getting higher accuracy of these\nmethods. The study also identifies some open challenges that can gain the\nattention of future researchers of this domain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.0061,regular,post_llm,2023,12,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'A High School Camp on Algorithms and Coding in Jamaica\n\n  This is a report on JamCoders, a four-week long computer-science camp for\nhigh school students in Jamaica. The camp teaches college-level coding and\nalgorithms, and targets academically excellent students in grades 9--11 (ages\n14--17). Qualitative assessment shows that the camp was, in general terms, a\nsuccess. We reflect on the background and academic structure of the camp and\nshare key takeaways on designing and operating a successful camp. We analyze\ndata collected before, during and after the camp and map the effects of\ndemographic differences on student performance in camp. We conclude with a\ndiscussion on possible improvements on our approach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.11697,review,post_llm,2023,12,"{'ai_likelihood': 1.635816362169054e-05, 'text': ""A risk-based approach to assessing liability risk for AI-driven harms\n  considering EU liability directive\n\n  Artificial intelligence can cause inconvenience, harm, or other unintended\nconsequences in various ways, including those that arise from defects or\nmalfunctions in the AI system itself or those caused by its use or misuse.\nResponsibility for AI harms or unintended consequences must be addressed to\nhold accountable the people who caused such harms and ensure that victims\nreceive compensation for any damages or losses they may have sustained.\nHistorical instances of harm caused by AI have led to European Union\nestablishing an AI Liability Directive. The directive aims to lay down a\nuniform set of rules for access to information, delineate the duty and level of\ncare required for AI development and use, and clarify the burden of proof for\ndamages or harms caused by AI systems, establishing broader protection for\nvictims. The future ability of provider to contest a product liability claim\nwill depend on good practices adopted in designing, developing, and maintaining\nAI systems in the market. This paper provides a risk-based approach to\nexamining liability for AI-driven injuries. It also provides an overview of\nexisting liability approaches, insights into limitations and complexities in\nthese approaches, and a detailed self-assessment questionnaire to assess the\nrisk associated with liability for a specific AI system from a provider's\nperspective.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.08161,regular,post_llm,2023,12,"{'ai_likelihood': 6.159146626790365e-06, 'text': 'The mentor-child paradigm for individuals with autism spectrum disorders\n\n  Our aim is to analyze the relevance of the mentor-child paradigm with a robot\nfor individuals with Autism Spectrum Disorders, and the adaptations required.\nThis method could allow a more reliable evaluation of the socio-cognitive\nabilities of individuals with autism, which may have been underestimated due to\npragmatic factors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.0309,regular,post_llm,2023,12,"{'ai_likelihood': 4.9802992078993055e-05, 'text': 'Critiquing Computing Artifacts through Programming Satirical Python\n  Scripts\n\n  Computing artifacts tend to exclude marginalized students, so we must create\nnew methods to critique and change them. We studied the potential for\n""satirical programming"" to critique artifacts as part of culturally responsive\ncomputing (CRC) pedagogy. We conducted a one-hour session for three different\nBPC programs (N=51). We showed an example of a satirical Python script and\ntaught elements of Python to create a script. Our findings suggest this method\nis a promising CRC pedagogical approach: 50% of marginalized students worked\ntogether to create a satirical script, and 80% enjoyed translating their\n""glitches"" into satirical Python scripts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.03924,review,post_llm,2023,12,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Integrating Traditional CS Class Activities with Computing for Social\n  Good, Ethics, and Communication and Leadership Skills\n\n  Software and information technologies are becoming increasingly integrated\nand pervasive in human society and range from automated decision making and\nsocial media and entertainment, to running critical social and physical\ninfrastructures like government programs, utilities, and financial\ninstitutions. As a result, there is a growing awareness of the need to develop\nprofessionals who will harness these technologies in fair and inclusive ways\nand use them to address global issues like health, water management, poverty,\nand human rights. In this regard, many academic researchers have expressed the\nneed to complement traditional teaching of CS technical skills with computer\nand information ethics (computing for social good), as well as communication\nand leadership skills. In this paper, we describe our goals and some possible\nclass activities we have developed and refined over the past few years with\nencouraging results, to help CS students understand the potential uses of\ncomputing for social good. In these carefully planned project assignments, we\nseamlessly integrate traditional approaches to develop technical skills with\nbroader professional responsibility and soft skills. We then discuss the\nlessons learned from these activities and briefly outline future plans.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.17107,regular,post_llm,2023,12,"{'ai_likelihood': 5.596213870578343e-06, 'text': 'The Intelligence College in Europe (ICE): An Effort to Create a European\n  Intelligence Community\n\n  In fulfilling the European security commitment, the actors of the so-called\n""Intelligence Community"" play a central role. They provide political and\nmilitary decision-makers with important analyses and information. The\nIntelligence College in Europe (ICE) is the first entity to offer professional\nintelligence training as well as postgraduate level academic education in\nintelligence and security studies at a pan-European level. In developing its\npostgraduate provision, ICE has benefited from the experience of the German\nMaster of Intelligence and Security Studies (MISS), which is a joint effort of\nthe University of the Bundeswehr Munich and the Department of Intelligence at\nthe Federal University of Administrative Sciences in Berlin. As a main\ncontribution of this paper, the module Counterterrorism (adapted from the MISS)\nis examined in more detail as a case study of how postgraduate modules can be\nmodified to speak to a pan-European audience of intelligence professionals.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.06361,regular,post_llm,2023,12,"{'ai_likelihood': 3.443823920355903e-06, 'text': 'Visions Of Destruction: Exploring Human Impact on Nature by Navigating\n  the Latent Space of a Diffusion Model via Gaze\n\n  This paper discusses the artwork ""Visions of Destruction"", with a primary\nconceptual focus on the Anthropocene, which is communicated through audience\ninteraction and generative AI as artistic research methods. Gaze-based\ninteraction transitions the audience from mere observers to agents of landscape\ntransformation, fostering a profound, on-the-edge engagement with pressing\nissues such as climate change and planetary destruction. The paper looks into\nearly references of interactive art history that deploy eye-tracking as a\nmethod for audience interaction, and presents recent AI-aided artworks that\ndemonstrate interactive latent space navigation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.09354,review,post_llm,2023,12,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Older Adults' Experiences with Misinformation on Social Media\n\n  Older adults habitually encounter misinformation on social media, but there\nis little knowledge about their experiences with it. In this study, we combined\na qualitative survey (n=119) with in-depth interviews (n=21) to investigate how\nolder adults in America conceptualize, discern, and contextualize social media\nmisinformation. As misinformation on social media in the past was driven\ntowards influencing voting outcomes, we were particularly interested to\napproach our study from a voting intention perspective. We found that 62% of\nthe participants intending to vote Democrat saw a manipulative political\npurpose behind the spread of misinformation while only 5% of those intending to\nvote Republican believed misinformation has a political dissent purpose.\nRegardless of the voting intentions, most participants relied on source\nheuristics combined with fact-checking to discern truth from misinformation on\nsocial media. The biggest concern about the misinformation, among all the\nparticipants, was that it increasingly leads to biased reasoning influenced by\npersonal values and feelings instead of reasoning based on objective evidence.\nThe participants intending to vote Democrat were in 74% of the cases concerned\nthat misinformation will cause escalation of extremism in the future, while\nthose intending to vote Republican, were undecided, or planned to abstain were\nconcerned that misinformation will further erode the trust in democratic\ninstitutions, specifically in the context of public health and free and fair\nelections. During our interviews, we found that 63% of the participants who\nintended to vote Republican, were fully aware and acknowledged that Republican\nor conservative voices often time speak misinformation, even though they are\nclosely aligned to their political ideology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.05377,review,post_llm,2023,12,"{'ai_likelihood': 8.351273006863064e-05, 'text': ""The impact of generative artificial intelligence on socioeconomic\n  inequalities and policy making\n\n  Generative artificial intelligence has the potential to both exacerbate and\nameliorate existing socioeconomic inequalities. In this article, we provide a\nstate-of-the-art interdisciplinary overview of the potential impacts of\ngenerative AI on (mis)information and three information-intensive domains:\nwork, education, and healthcare. Our goal is to highlight how generative AI\ncould worsen existing inequalities while illuminating how AI may help mitigate\npervasive social problems. In the information domain, generative AI can\ndemocratize content creation and access, but may dramatically expand the\nproduction and proliferation of misinformation. In the workplace, it can boost\nproductivity and create new jobs, but the benefits will likely be distributed\nunevenly. In education, it offers personalized learning, but may widen the\ndigital divide. In healthcare, it might improve diagnostics and accessibility,\nbut could deepen pre-existing inequalities. In each section we cover a specific\ntopic, evaluate existing research, identify critical gaps, and recommend\nresearch directions, including explicit trade-offs that complicate the\nderivation of a priori hypotheses. We conclude with a section highlighting the\nrole of policymaking to maximize generative AI's potential to reduce\ninequalities while mitigating its harmful effects. We discuss strengths and\nweaknesses of existing policy frameworks in the European Union, the United\nStates, and the United Kingdom, observing that each fails to fully confront the\nsocioeconomic challenges we have identified. We propose several concrete\npolicies that could promote shared prosperity through the advancement of\ngenerative AI. This article emphasizes the need for interdisciplinary\ncollaborations to understand and address the complex challenges of generative\nAI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.08039,review,post_llm,2023,12,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Safeguarding the safeguards: How best to promote AI alignment in the\n  public interest\n\n  AI alignment work is important from both a commercial and a safety lens. With\nthis paper, we aim to help actors who support alignment efforts to make these\nefforts as effective as possible, and to avoid potential adverse effects. We\nbegin by suggesting that institutions that are trying to act in the public\ninterest (such as governments) should aim to support specifically alignment\nwork that reduces accident or misuse risks. We then describe four problems\nwhich might cause alignment efforts to be counterproductive, increasing\nlarge-scale AI risks. We suggest mitigations for each problem. Finally, we make\na broader recommendation that institutions trying to act in the public interest\nshould think systematically about how to make their alignment efforts as\neffective, and as likely to be beneficial, as possible.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.09457,review,post_llm,2023,12,"{'ai_likelihood': 1.0, 'text': ""Empowering Africa: An In-depth Exploration of the Adoption of Artificial\n  Intelligence Across the Continent\n\n  This paper explores the dynamic landscape of Artificial Intelligence (AI)\nadoption in Africa, analysing its varied applications in addressing\nsocio-economic challenges and fostering development. Examining the African AI\necosystem, the study considers regional nuances, cultural factors, and\ninfrastructural constraints shaping the deployment of AI solutions. Case\nstudies in healthcare, agriculture, finance, and education highlight AI's\ntransformative potential for efficiency, accessibility, and inclusivity. The\npaper emphasizes indigenous AI innovations and international collaborations\ncontributing to a distinct African AI ecosystem. Ethical considerations,\nincluding data privacy and algorithmic bias, are addressed alongside policy\nframeworks supporting responsible AI implementation. The role of governmental\nbodies, regulations, and private sector partnerships is explored in creating a\nconducive AI development environment. Challenges such as digital literacy gaps\nand job displacement are discussed, with proposed strategies for mitigation. In\nconclusion, the paper provides a nuanced understanding of AI in Africa,\ncontributing to sustainable development discussions and advocating for an\ninclusive and ethical AI ecosystem on the continent.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.4955482482910156e-05, 'GPT4': 0.1553955078125, 'CLAUDE': 0.0005121231079101562, 'GOOGLE': 0.82421875, 'OPENAI_O_SERIES': 0.019378662109375, 'DEEPSEEK': 0.0004150867462158203, 'GROK': 6.556510925292969e-07, 'NOVA': 2.002716064453125e-05, 'OTHER': 2.759695053100586e-05, 'HUMAN': 5.960464477539062e-07}}"
2312.10076,review,post_llm,2023,12,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'A Framework for Exploring the Consequences of AI-Mediated Enterprise\n  Knowledge Access and Identifying Risks to Workers\n\n  Organisations generate vast amounts of information, which has resulted in a\nlong-term research effort into knowledge access systems for enterprise\nsettings. Recent developments in artificial intelligence, in relation to large\nlanguage models, are poised to have significant impact on knowledge access.\nThis has the potential to shape the workplace and knowledge in new and\nunanticipated ways. Many risks can arise from the deployment of these types of\nAI systems, due to interactions between the technical system and organisational\npower dynamics.\n  This paper presents the Consequence-Mechanism-Risk framework to identify\nrisks to workers from AI-mediated enterprise knowledge access systems. We have\ndrawn on wide-ranging literature detailing risks to workers, and categorised\nrisks as being to worker value, power, and wellbeing. The contribution of our\nframework is to additionally consider (i) the consequences of these systems\nthat are of moral import: commodification, appropriation, concentration of\npower, and marginalisation, and (ii) the mechanisms, which represent how these\nconsequences may take effect in the system. The mechanisms are a means of\ncontextualising risk within specific system processes, which is critical for\nmitigation. This framework is aimed at helping practitioners involved in the\ndesign and deployment of AI-mediated knowledge access systems to consider the\nrisks introduced to workers, identify the precise system mechanisms that\nintroduce those risks and begin to approach mitigation. Future work could apply\nthis framework to other technological systems to promote the protection of\nworkers and other groups.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.15948,review,post_llm,2023,12,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'How digital will the future be? Analysis of prospective scenarios\n\nWith the climate change context, many prospective studies, generally encompassing all areas of society, imagine possible futures to expand the range of options. The role of digital technologies within these possible futures is rarely specifically targeted. Which digital technologies and methodologies do these studies envision in a world that has mitigated and adapted to climate change? In this paper, we propose a typology for scenarios to survey digital technologies and their applications in 14 prospective studies and their corresponding 35 future scenarios. Our finding is that all the scenarios consider digital technology to be present in the future. We observe that only a few of them question our relationship with digital technology and all aspects related to its materiality, and none of the general studies envision breakthroughs concerning technologies used today. Our result demonstrates the lack of a systemic view of information and communication technologies. We therefore argue for new prospective studies to envision the future of ICT.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10198,regular,post_llm,2023,12,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Expert-Level Annotation Quality Achieved by Gamified Crowdsourcing for\n  B-line Segmentation in Lung Ultrasound\n\n  Accurate and scalable annotation of medical data is critical for the\ndevelopment of medical AI, but obtaining time for annotation from medical\nexperts is challenging. Gamified crowdsourcing has demonstrated potential for\nobtaining highly accurate annotations for medical data at scale, and we\ndemonstrate the same in this study for the segmentation of B-lines, an\nindicator of pulmonary congestion, on still frames within point-of-care lung\nultrasound clips. We collected 21,154 annotations from 214 annotators over 2.5\ndays, and we demonstrated that the concordance of crowd consensus segmentations\nwith reference standards exceeds that of individual experts with the same\nreference standards, both in terms of B-line count (mean squared error 0.239\nvs. 0.308, p<0.05) as well as the spatial precision of B-line annotations (mean\nDice-H score 0.755 vs. 0.643, p<0.05). These results suggest that\nexpert-quality segmentations can be achieved using gamified crowdsourcing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.0118,regular,post_llm,2023,12,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'A Comparative Analysis of Text-to-Image Generative AI Models in\n  Scientific Contexts: A Case Study on Nuclear Power\n\n  In this work, we propose and assess the potential of generative artificial\nintelligence (AI) to generate public engagement around potential clean energy\nsources. Such an application could increase energy literacy -- an awareness of\nlow-carbon energy sources among the public therefore leading to increased\nparticipation in decision-making about the future of energy systems. We explore\nthe use of generative AI to communicate technical information about low-carbon\nenergy sources to the general public, specifically in the realm of nuclear\nenergy. We explored 20 AI-powered text-to-image generators and compared their\nindividual performances on general and scientific nuclear-related prompts. Of\nthese models, DALL-E, DreamStudio, and Craiyon demonstrated promising\nperformance in generating relevant images from general-level text related to\nnuclear topics. However, these models fall short in three crucial ways: (1)\nthey fail to accurately represent technical details of energy systems; (2) they\nreproduce existing biases surrounding gender and work in the energy sector; and\n(3) they fail to accurately represent indigenous landscapes -- which have\nhistorically been sites of resource extraction and waste deposition for energy\nindustries. This work is performed to motivate the development of specialized\ngenerative tools and their captions to improve energy literacy and effectively\nengage the public with low-carbon energy sources.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.13126,regular,post_llm,2023,12,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Generative agents in the streets: Exploring the use of Large Language\n  Models (LLMs) in collecting urban perceptions\n\n  Evaluating the surroundings to gain understanding, frame perspectives, and\nanticipate behavioral reactions is an inherent human trait. However, these\ncontinuous encounters are diverse and complex, posing challenges to their study\nand experimentation. Researchers have been able to isolate environmental\nfeatures and study their effect on human perception and behavior. However, the\nresearch attempts to replicate and study human behaviors with proxies, such as\nby integrating virtual mediums and interviews, have been inconsistent. Large\nlanguage models (LLMs) have recently been unveiled as capable of contextual\nunderstanding and semantic reasoning. These models have been trained on large\namounts of text and have evolved to mimic believable human behavior. This study\nexplores the current advancements in Generative agents powered by LLMs with the\nhelp of perceptual experiments. The experiment employs Generative agents to\ninteract with the urban environments using street view images to plan their\njourney toward specific goals. The agents are given virtual personalities,\nwhich make them distinguishable. They are also provided a memory database to\nstore their thoughts and essential visual information and retrieve it when\nneeded to plan their movement. Since LLMs do not possess embodiment, nor have\naccess to the visual realm, and lack a sense of motion or direction, we\ndesigned movement and visual modules that help agents gain an overall\nunderstanding of surroundings. The agents are further employed to rate the\nsurroundings they encounter based on their perceived sense of safety and\nliveliness. As these agents store details in their memory, we query the\nfindings to get details regarding their thought processes. Overall, this study\nexperiments with current AI developments and their potential in simulated human\nbehavior in urban environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.11914,regular,post_llm,2023,12,"{'ai_likelihood': 1.457002427842882e-06, 'text': ""The Role of Likes: How Online Feedback Impacts Users' Mental Health\n\n  Social media usage has been shown to have both positive and negative\nconsequences for users' mental health. Several studies indicated that peer\nfeedback plays an important role in the relationship between social media use\nand mental health. In this research, we analyse the impact of receiving online\nfeedback on users' emotional experience, social connectedness and self-esteem.\nIn an experimental study, we let users interact with others on a Facebook-like\nsystem over the course of a week while controlling for the amount of positive\nreactions they receive from their peers. We find that experiencing little to no\nreaction from others does not only elicit negative emotions and stress amongst\nusers, but also induces low levels of self-esteem. In contrast, receiving much\npositive online feedback, evokes feelings of social connectedness and reduces\noverall loneliness. On a societal level, our study can help to better\nunderstand the mechanisms through which social media use impacts mental health\nin a positive or negative way. On a methodological level, we provide a new\nopen-source tool for designing and conducting social media experiments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.14408,regular,post_llm,2023,12,"{'ai_likelihood': 3.907415601942274e-06, 'text': ""Extended p-median problems for balancing service efficiency and equality\n\n  This article deals with the location problem for balancing the service\nefficiency and equality. In public service systems, some individuals may\nexperience envy if they have to travel longer distances to access services\ncompared to others. This envy can be simplified by comparing an individual's\ntravel distance to a service facility against a threshold distance. Four\nextended p-median problems are proposed, utilizing the total travel distance\nand total envy to balance service efficiency and spatial equality. The new\nobjective function is designed to be inequity-averse and exhibits several\nanalytical properties that pertain to both service efficiency and equality. The\nextended problems were extensively tested on two sets of benchmark instances\nand one set of geographical instances. The experimentation shows that the\nequality measures, such as the standard deviation, mean absolute deviation, and\nGini coefficient between travel distances, can be substantially improved by\nslightly increasing the travel distance. Additionally, the advantages of the\nproposed problems were validated through Pareto optimality analysis and\ncomparisons with other location problems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.10901,review,post_llm,2023,12,"{'ai_likelihood': 0.0032552083333333335, 'text': ""Enabling Technologies for Web 3.0: A Comprehensive Survey\n\n  Web 3.0 represents the next stage of Internet evolution, aiming to empower\nusers with increased autonomy, efficiency, quality, security, and privacy. This\nevolution can potentially democratize content access by utilizing the latest\ndevelopments in enabling technologies. In this paper, we conduct an in-depth\nsurvey of enabling technologies in the context of Web 3.0, such as blockchain,\nsemantic web, 3D interactive web, Metaverse, Virtual reality/Augmented reality,\nInternet of Things technology, and their roles in shaping Web 3.0. We commence\nby providing a comprehensive background of Web 3.0, including its concept,\nbasic architecture, potential applications, and industry adoption.\nSubsequently, we examine recent breakthroughs in IoT, 5G, and blockchain\ntechnologies that are pivotal to Web 3.0 development. Following that, other\nenabling technologies, including AI, semantic web, and 3D interactive web, are\ndiscussed. Utilizing these technologies can effectively address the critical\nchallenges in realizing Web 3.0, such as ensuring decentralized identity,\nplatform interoperability, data transparency, reducing latency, and enhancing\nthe system's scalability. Finally, we highlight significant challenges\nassociated with Web 3.0 implementation, emphasizing potential solutions and\nproviding insights into future research directions in this field.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.00205,review,post_llm,2023,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""Consumer Manipulation via Online Behavioral Advertising\n\n  Online behavioral advertising (OBA) has a significant role in the digital\neconomy. It allows advertisers to target consumers categorized according to\ntheir algorithmically inferred interests based on their behavioral data. As\nAlphabet and Meta gatekeep the Internet with their digital platforms and\nchannel most of the consumer attention online, they are best placed to execute\nOBA and earn profits far exceeding fair estimations. There are increasing\nconcerns that gatekeepers achieve such profitability at the expense of\nconsumers, advertisers, and publishers who are dependent on their services to\naccess the Internet. In particular, some claim that OBA systematically exploits\nconsumers' decision-making vulnerabilities, creating internet infrastructure\nand relevant markets that optimize for consumer manipulation.\n  Intuitively, consumer manipulation via OBA comes in tension with the ideal of\nconsumer autonomy in liberal democracies. Nevertheless, academia has largely\noverlooked this phenomenon and instead has primarily focused on privacy and\ndiscrimination concerns of OBA. This article redirects academic discourse and\nregulatory focus on consumer manipulation via OBA. In doing so, first, this\narticle elaborates on how OBA works. Second, it constructs an analytic\nframework for understanding manipulation. Third, it applies the theory of\nmanipulation to OBA. As a result, this article illustrates the extent to which\nOBA leads to consumer manipulation. Crucially, this article is purely analytic\nand avoids normative evaluation of consumer manipulation via OBA. Evaluating\nconsumer manipulation harms of OBA is an equally important but separate task\nand is pursued in another publication.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2312.10833,regular,post_llm,2023,12,"{'ai_likelihood': 0.00012245443132188584, 'text': 'AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?\n\n  This study delves into the pervasive issue of gender issues in artificial\nintelligence (AI), specifically within automatic scoring systems for\nstudent-written responses. The primary objective is to investigate the presence\nof gender biases, disparities, and fairness in generally targeted training\nsamples with mixed-gender datasets in AI scoring outcomes. Utilizing a\nfine-tuned version of BERT and GPT-3.5, this research analyzes more than 1000\nhuman-graded student responses from male and female participants across six\nassessment items. The study employs three distinct techniques for bias\nanalysis: Scoring accuracy difference to evaluate bias, mean score gaps by\ngender (MSG) to evaluate disparity, and Equalized Odds (EO) to evaluate\nfairness. The results indicate that scoring accuracy for mixed-trained models\nshows an insignificant difference from either male- or female-trained models,\nsuggesting no significant scoring bias. Consistently with both BERT and\nGPT-3.5, we found that mixed-trained models generated fewer MSG and\nnon-disparate predictions compared to humans. In contrast, compared to humans,\ngender-specifically trained models yielded larger MSG, indicating that\nunbalanced training data may create algorithmic models to enlarge gender\ndisparities. The EO analysis suggests that mixed-trained models generated more\nfairness outcomes compared with gender-specifically trained models.\nCollectively, the findings suggest that gender-unbalanced data do not\nnecessarily generate scoring bias but can enlarge gender disparities and reduce\nscoring fairness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.10896,review,post_llm,2023,12,"{'ai_likelihood': 0.08660210503472222, 'text': ""Responsible AI Governance: A Systematic Literature Review\n\n  As artificial intelligence transforms a wide range of sectors and drives\ninnovation, it also introduces complex challenges concerning ethics,\ntransparency, bias, and fairness. The imperative for integrating Responsible AI\n(RAI) principles within governance frameworks is paramount to mitigate these\nemerging risks. While there are many solutions for AI governance, significant\nquestions remain about their effectiveness in practice. Addressing this\nknowledge gap, this paper aims to examine the existing literature on AI\nGovernance. The focus of this study is to analyse the literature to answer key\nquestions: WHO is accountable for AI systems' governance, WHAT elements are\nbeing governed, WHEN governance occurs within the AI development life cycle,\nand HOW it is executed through various mechanisms like frameworks, tools,\nstandards, policies, or models. Employing a systematic literature review\nmethodology, a rigorous search and selection process has been employed. This\neffort resulted in the identification of 61 relevant articles on the subject of\nAI Governance. Out of the 61 studies analysed, only 5 provided complete\nresponses to all questions. The findings from this review aid research in\nformulating more holistic and comprehensive Responsible AI (RAI) governance\nframeworks. This study highlights important role of AI governance on various\nlevels specially organisational in establishing effective and responsible AI\npractices. The findings of this study provides a foundational basis for future\nresearch and development of comprehensive governance models that align with RAI\nprinciples.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.02404,regular,post_llm,2024,1,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks\n\n  Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01646,review,post_llm,2024,1,"{'ai_likelihood': 9.040037790934246e-06, 'text': 'Recommendations for public action towards sustainable generative AI\n  systems\n\n  Growing awareness of the environmental impact of digital technologies has led\nto several isolated initiatives to promote sustainable practices. However,\ndespite these efforts, the environmental footprint of generative AI,\nparticularly in terms of greenhouse gas emissions and water consumption,\nremains considerable. This contribution first presents the components of this\nenvironmental footprint, highlighting the massive CO2 emissions and water\nconsumption associated with training large language models, thus underlining\nthe need to rethink learning and inference methods. The paper also explores the\nfactors and characteristics of models that have an influence on their\nenvironmental footprint and demonstrates the existence of solutions to reduce\nit, such as using more efficient processors or optimising the energy\nperformance of data centres. The potentially harmful effects of AI on the\nplanet and its ecosystem have made environmental protection one of the founding\nprinciples of AI ethics at international and European levels. However, this\nrecognition has not yet translated into concrete measures to address it.To\naddress this issue, our contribution puts forward twelve pragmatic\nrecommendations for public action to promote sustainable generative AI, in\nparticular by building a long-term strategy to achieve carbon neutrality for AI\nmodels, encouraging international cooperation to set common standards,\nsupporting scientific research and developing appropriate legal and regulatory\nframeworks.This paper seeks to inform the members of the Interministerial\nCommittee on Generative AI about the environmental challenges of this\ntechnology by providing a brief review of the scientific literature on the\nsubject and proposing concrete recommendations of public policy actions to\nreconcile technological innovation with the need to protect our environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.0165,review,post_llm,2024,1,"{'ai_likelihood': 4.8345989651150175e-06, 'text': 'Effect of trip attributes on ridehailing driver trip request acceptance\n\n  A generalized additive mixed model was estimated to investigate the factors\nthat impact ridehailing driver trip request acceptance choices, relying on 200\nresponses from a stated preference survey in Seattle, US. Several policy\nrecommendations were proposed to promote trip request acceptance based on\nridehailing drivers willingness to accept compensation for undesired trip\nfeatures. The findings could be useful for transportation agencies to improve\nridehailing service efficiency, better fulfill urban mobility needs, and reduce\nenvironmental burden.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.0909,review,post_llm,2024,1,"{'ai_likelihood': 1.0927518208821615e-05, 'text': 'Understanding the concerns and choices of public when using large\n  language models for healthcare\n\n  Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.03324,review,post_llm,2024,1,"{'ai_likelihood': 0.23369683159722224, 'text': 'Requirements for a Career in Information Security: A Comprehensive\n  Review\n\n  This research paper adopts a methodology by conducting a thorough literature\nreview to uncover the essential prerequisites for achieving a prosperous career\nin the field of Information Security (IS). The primary objective is to increase\npublic awareness regarding the diverse opportunities available in the\nInformation Security (IS) field. The initial search involved scouring four\nprominent academic databases using the specific keywords ""cybersecurity"" and\n""skills,"" resulting in the identification of a substantial corpus of 1,520\narticles. After applying rigorous screening criteria, a refined set of 31\nrelevant papers was selected for further analysis. Thematic analysis was\nconducted on these studies to identify and delineate the crucial knowledge and\nskills that an IS professional should possess. The research findings emphasize\nthe significant time investment required for individuals to acquire the\nnecessary technical proficiency in the cybersecurity domain. Furthermore, the\nstudy recognizes the existence of gender-related obstacles for women pursuing\ncybersecurity careers due to the field\'s unique requirements. It suggests that\nfemales can potentially overcome these barriers by initially entering the\nprofession at lower levels and subsequently advancing based on individual\ncircumstances.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.08899,review,post_llm,2024,1,"{'ai_likelihood': 0.0007761849297417535, 'text': 'Landscape of Generative AI in Global News: Topics, Sentiments, and\n  Spatiotemporal Analysis\n\n  Generative AI has exhibited considerable potential to transform various\nindustries and public life. The role of news media coverage of generative AI is\npivotal in shaping public perceptions and judgments about this significant\ntechnological innovation. This paper provides in-depth analysis and rich\ninsights into the temporal and spatial distribution of topics, sentiment, and\nsubstantive themes within global news coverage focusing on the latest emerging\ntechnology --generative AI. We collected a comprehensive dataset of news\narticles (January 2018 to November 2023, N = 24,827). For topic modeling, we\nemployed the BERTopic technique and combined it with qualitative coding to\nidentify semantic themes. Subsequently, sentiment analysis was conducted using\nthe RoBERTa-base model. Analysis of temporal patterns in the data reveals\nnotable variability in coverage across key topics--business, corporate\ntechnological development, regulation and security, and education--with spikes\nin articles coinciding with major AI developments and policy discussions.\nSentiment analysis shows a predominantly neutral to positive media stance, with\nthe business-related articles exhibiting more positive sentiment, while\nregulation and security articles receive a reserved, neutral to negative\nsentiment. Our study offers a valuable framework to investigate global news\ndiscourse and evaluate news attitudes and themes related to emerging\ntechnologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.14908,review,post_llm,2024,1,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""A Framework for Assurance Audits of Algorithmic Systems\n\n  An increasing number of regulations propose AI audits as a mechanism for\nachieving transparency and accountability for artificial intelligence (AI)\nsystems. Despite some converging norms around various forms of AI auditing,\nauditing for the purpose of compliance and assurance currently lacks\nagreed-upon practices, procedures, taxonomies, and standards. We propose the\ncriterion audit as an operationalizable compliance and assurance external audit\nframework. We model elements of this approach after financial auditing\npractices, and argue that AI audits should similarly provide assurance to their\nstakeholders about AI organizations' ability to govern their algorithms in ways\nthat mitigate harms and uphold human values. We discuss the necessary\nconditions for the criterion audit and provide a procedural blueprint for\nperforming an audit engagement in practice. We illustrate how this framework\ncan be adapted to current regulations by deriving the criteria on which bias\naudits can be performed for in-scope hiring algorithms, as required by the\nrecently effective New York City Local Law 144 of 2021. We conclude by offering\na critical discussion on the benefits, inherent limitations, and implementation\nchallenges of applying practices of the more mature financial auditing industry\nto AI auditing where robust guardrails against quality assurance issues are\nonly starting to emerge. Our discussion -- informed by experiences in\nperforming these audits in practice -- highlights the critical role that an\naudit ecosystem plays in ensuring the effectiveness of audits.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01645,review,post_llm,2024,1,"{'ai_likelihood': 0.10213216145833334, 'text': 'Recent Innovations in Footwear Sensors: Role of Smart Footwear in\n  Healthcare -- A Survey\n\n  Smart shoes have ushered in a new era of personalised health monitoring and\nassistive technology. The shoe leverages technologies such as Bluetooth for\ndata collection and wireless transmission and incorporates features such as GPS\ntracking, obstacle detection, and fitness tracking. This article provides an\noverview of the current state of smart shoe technology, highlighting the\nintegration of advanced sensors for health monitoring, energy harvesting,\nassistive features for the visually impaired, and deep learning for data\nanalysis. The study discusses the potential of smart footwear in medical\napplications, particularly for patients with diabetes, and the ongoing research\nin this field. Current footwear challenges are also discussed, including\ncomplex construction, poor fit, comfort, and high cost.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.05826,review,post_llm,2024,1,"{'ai_likelihood': 0.0001426537831624349, 'text': 'Crumbled Cookie Exploring E-commerce Websites Cookie Policies with Data\n  Protection Regulations\n\n  Despite stringent data protection regulations such as the General Data\nProtection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and\nother country-specific regulations, many websites continue to use cookies to\ntrack user activities. Recent studies have revealed several data protection\nviolations, resulting in significant penalties, especially for multinational\ncorporations. Motivated by the question of why these data protection violations\ncontinue to occur despite strong data protection regulations, we examined 360\npopular e-commerce websites in multiple countries to analyze whether they\ncomply with regulations to protect user privacy from a cookie perspective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.04454,regular,post_llm,2024,1,"{'ai_likelihood': 6.291601392957899e-07, 'text': 'Character comes from practice: longitudinal practice-based ethics\n  training in data science\n\n  In this chapter, we propose a non-traditional RCR training in data science\nthat is grounded into a virtue theory framework. First, we delineate the\napproach in more theoretical detail, by discussing how the goal of RCR training\nis to foster the cultivation of certain moral abilities. We specify the nature\nof these abilities: while the ideal is the cultivation of virtues, the limited\nspace allowed by RCR modules can only facilitate the cultivation of superficial\nabilities or proto-virtues, which help students to familiarize with moral and\npolitical issues in the data science environment. Third, we operationalize our\napproach by stressing that (proto-)virtue acquisition (like skill acquisition)\noccurs through the technical and social tasks of daily data science activities,\nwhere these repetitive tasks provide the opportunities to develop\n(proto-)virtue capacity and to support the development of ethically robust data\nsystems. Finally, we discuss a concrete example of how this approach has been\nimplemented. In particular, we describe how this method is applied to teach\ndata ethics to students participating in the CODATA-RDA Data Science Summer\nSchools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.14252,regular,post_llm,2024,1,"{'ai_likelihood': 5.3279929690890844e-05, 'text': 'On mission Twitter Profiles: A Study of Selective Toxic Behavior\n\n  The argument for persistent social media influence campaigns, often funded by\nmalicious entities, is gaining traction. These entities utilize instrumented\nprofiles to disseminate divisive content and disinformation, shaping public\nperception. Despite ample evidence of these instrumented profiles, few\nidentification methods exist to locate them in the wild. To evade detection and\nappear genuine, small clusters of instrumented profiles engage in unrelated\ndiscussions, diverting attention from their true goals. This strategic thematic\ndiversity conceals their selective polarity towards certain topics and fosters\npublic trust.\n  This study aims to characterize profiles potentially used for influence\noperations, termed \'on-mission profiles,\' relying solely on thematic content\ndiversity within unlabeled data. Distinguishing this work is its focus on\ncontent volume and toxicity towards specific themes. Longitudinal data from\n138K Twitter or X, profiles and 293M tweets enables profiling based on theme\ndiversity. High thematic diversity groups predominantly produce toxic content\nconcerning specific themes, like politics, health, and news classifying them as\n\'on-mission\' profiles.\n  Using the identified ``on-mission"" profiles, we design a classifier for\nunseen, unlabeled data. Employing a linear SVM model, we train and test it on\nan 80/20% split of the most diverse profiles. The classifier achieves a\nflawless 100% accuracy, facilitating the discovery of previously unknown\n``on-mission"" profiles in the wild.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.17512,regular,post_llm,2024,1,"{'ai_likelihood': 6.159146626790365e-06, 'text': 'A Cradle-to-Gate Life Cycle Analysis of Bitcoin Mining Equipment Using\n  Sphera LCA and ecoinvent Databases\n\n  Bitcoin mining is regularly pointed out for its massive energy consumption\nand associated greenhouse gas emissions, hence contributing significantly to\nclimate change. However, most studies ignore the environmental impacts of\nproducing mining equipment, which is problematic given the short lifespan of\nsuch highly specific hardware. In this study, we perform a cradle-to-gate life\ncycle assessment (LCA) of dedicated Bitcoin mining equipment, considering their\nspecific architecture. Our results show that the application-specific\nintegrated circuit designed for Bitcoin mining is the main contributor to\nproduction-related impacts. This observation applies to most impact categories,\nincluding the global warming potential. In addition, this finding stresses out\nthe necessity to carefully consider the specificity of the hardware. By\ncomparing these results with several usage scenarios, we also demonstrate that\nthe impacts of producing this type of equipment can be significant (up to 80%\nof the total life cycle impacts), depending on the sources of electricity\nsupply for the use phase. Therefore, we highlight the need to consider the\nproduction phase when assessing the environmental impacts of Bitcoin mining\nhardware. To test the validity of our results, we use the Sphera LCA and\necoinvent databases for the background modeling of our system. Surprisingly, it\nleads to results with variations of up to 4 orders of magnitude for\ntoxicity-related indicators, despite using the same foreground modeling. This\ndatabase mismatch phenomenon, already identified in previous studies, calls for\nbetter understanding, consideration and discussion of environmental impacts in\nthe field of electronics, going well beyond climate change indicators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.16268,regular,post_llm,2024,1,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'A.I. In All The Wrong Places\n\n  This text describes experiences gained across a two-year test period during\nwhich two generations of Generative Artificial Intelligence (A.I.) systems were\nincorporated into an interdisciplinary, university level course on A.I. for art\nand design practices. The text uses the results from the courses to reflect on\nnew opportunities for generative systems in art and design, while considering\ntraps and limits.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01671,review,post_llm,2024,1,"{'ai_likelihood': 0.95361328125, 'text': 'Learning Analytics Dashboards for Advisors -- A Systematic Literature\n  Review\n\n  Learning Analytics Dashboard for Advisors is designed to provide data-driven\ninsights and visualizations to support advisors in their decision-making\nregarding student academic progress, engagement, targeted support, and overall\nsuccess. This study explores the current state of the art in learning analytics\ndashboards, focusing on specific requirements for advisors. By examining\nexisting literature and case studies, this research investigates the key\nfeatures and functionalities essential for an effective learning analytics\ndashboard tailored to advisor needs. This study also aims to provide a\ncomprehensive understanding of the landscape of learning analytics dashboards\nfor advisors, offering insights into the advancements, opportunities, and\nchallenges in their development by synthesizing the current trends from a total\nof 21 research papers used for analysis. The findings will contribute to the\ndesign and implementation of new features in learning analytics dashboards that\nempower advisors to provide proactive and individualized support, ultimately\nfostering student retention and academic success.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1717529296875, 'GPT4': 0.161865234375, 'CLAUDE': 0.0241851806640625, 'GOOGLE': 0.62939453125, 'OPENAI_O_SERIES': 0.0007143020629882812, 'DEEPSEEK': 9.554624557495117e-05, 'GROK': 1.6927719116210938e-05, 'NOVA': 0.00015270709991455078, 'OTHER': 0.004756927490234375, 'HUMAN': 0.007144927978515625}}"
2401.15229,review,post_llm,2024,1,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Evolving AI Risk Management: A Maturity Model based on the NIST AI Risk\n  Management Framework\n\n  Researchers, government bodies, and organizations have been repeatedly\ncalling for a shift in the responsible AI community from general principles to\ntangible and operationalizable practices in mitigating the potential\nsociotechnical harms of AI. Frameworks like the NIST AI RMF embody an emerging\nconsensus on recommended practices in operationalizing sociotechnical harm\nmitigation. However, private sector organizations currently lag far behind this\nemerging consensus. Implementation is sporadic and selective at best. At worst,\nit is ineffective and can risk serving as a misleading veneer of trustworthy\nprocesses, providing an appearance of legitimacy to substantively harmful\npractices. In this paper, we provide a foundation for a framework for\nevaluating where organizations sit relative to the emerging consensus on\nsociotechnical harm mitigation best practices: a flexible maturity model based\non the NIST AI RMF.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.02191,regular,post_llm,2024,1,"{'ai_likelihood': 0.00011278523339165583, 'text': 'Characterizing Fake News Targeting Corporations\n\n  Misinformation proliferates in the online sphere, with evident impacts on the\npolitical and social realms, influencing democratic discourse and posing risks\nto public health and safety. The corporate world is also a prime target for\nfake news dissemination. While recent studies have attempted to characterize\ncorporate misinformation and its effects on companies, their findings often\nsuffer from limitations due to qualitative or narrative approaches and a narrow\nfocus on specific industries. To address this gap, we conducted an analysis\nutilizing social media quantitative methods and crowd-sourcing studies to\ninvestigate corporate misinformation across a diverse array of industries\nwithin the S\\&P 500 companies. Our study reveals that corporate misinformation\nencompasses topics such as products, politics, and societal issues. We\ndiscovered companies affected by fake news also get reputable news coverage but\nless social media attention, leading to heightened negativity in social media\ncomments, diminished stock growth, and increased stress mentions among employee\nreviews. Additionally, we observe that a company is not targeted by fake news\nall the time, but there are particular times when a critical mass of fake news\nemerges. These findings hold significant implications for regulators, business\nleaders, and investors, emphasizing the necessity to vigilantly monitor the\nescalating phenomenon of corporate misinformation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.14462,review,post_llm,2024,1,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'AI auditing: The Broken Bus on the Road to AI Accountability\n\n  One of the most concrete measures to take towards meaningful AI\naccountability is to consequentially assess and report the systems\' performance\nand impact. However, the practical nature of the ""AI audit"" ecosystem is\nmuddled and imprecise, making it difficult to work through various concepts and\nmap out the stakeholders involved in the practice. First, we taxonomize current\nAI audit practices as completed by regulators, law firms, civil society,\njournalism, academia, consulting agencies. Next, we assess the impact of audits\ndone by stakeholders within each domain. We find that only a subset of AI audit\nstudies translate to desired accountability outcomes. We thus assess and\nisolate practices necessary for effective AI audit results, articulating the\nobserved connections between AI audit design, methodology and institutional\ncontext on its effectiveness as a meaningful mechanism for accountability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.00136,regular,post_llm,2024,1,"{'ai_likelihood': 3.311369154188368e-06, 'text': ""Transdisciplinary Multi Modal Approach to Knowledge\n\n  The muti-modal or multi-sensorial perception of nature is presented in this\narticle as part of research devoted to inclusive tools developed in the\nframework of User Centered Design. This proposal shows that it is possible to\nwork in a transdisciplinary way, establishing feedback not only between\ndesigners and final users, but also between humans and computers, to reduce\nerrors and co-design the resources according to personal needs. As part of the\npresent research, we present the basis for a new accessible software, sonoUno,\nwhich was designed with the user in mind from the beginning, and we propose a\ntraining activity to enhance the user's capacities, expand the detection of\ndifferent kinds of natural signals, and improve the comprehension of the Human\nComputer Interfaces, opening new windows to the sciences for diverse\npopulations, not only in education and outreach but also in research. Some\nexamples of the exploitation of these new devices and tools are also presented.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.1456,review,post_llm,2024,1,"{'ai_likelihood': 0.6850585937499999, 'text': 'The Role of Intelligent Transportation Systems and Artificial\n  Intelligence in Energy Efficiency and Emission Reduction\n\n  Despite the technological advancements in the transportation sector, the\nindustry continues to grapple with increasing energy consumption and vehicular\nemissions, which intensify environmental degradation and climate change. The\ninefficient management of traffic flow, the underutilization of transport\nnetwork interconnectivity, and the limited implementation of artificial\nintelligence (AI)-driven predictive models pose significant challenges to\nachieving energy efficiency and emission reduction. Thus, there is a timely and\ncritical need for an integrated, sophisticated approach that leverages\nintelligent transportation systems (ITSs) and AI for energy conservation and\nemission reduction. In this paper, we explore the role of ITSs and AI in future\nenhanced energy and emission reduction (EER). More specifically, we discuss the\nimpact of sensors at different levels of ITS on improving EER. We also\ninvestigate the potential networking connections in ITSs and provide an\nillustration of how they improve EER. Finally, we discuss potential AI services\nfor improved EER in the future. The findings discussed in this paper will\ncontribute to the ongoing discussion about the vital role of ITSs and AI\napplications in addressing the challenges associated with achieving energy\nsavings and emission reductions in the transportation sector. Additionally, it\nwill provide insights for policymakers and industry professionals to enable\nthem to develop policies and implementation plans for the integration of ITSs\nand AI technologies in the transportation sector.\n', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.196044921875, 'GPT4': 0.65966796875, 'CLAUDE': 9.03010368347168e-05, 'GOOGLE': 0.1051025390625, 'OPENAI_O_SERIES': 0.0002846717834472656, 'DEEPSEEK': 6.198883056640625e-06, 'GROK': 1.0132789611816406e-06, 'NOVA': 7.927417755126953e-06, 'OTHER': 0.00022172927856445312, 'HUMAN': 0.038421630859375}}"
2401.08866,review,post_llm,2024,1,"{'ai_likelihood': 6.063116921318902e-05, 'text': 'Foundation Models in Augmentative and Alternative Communication:\n  Opportunities and Challenges\n\n  Augmentative and Alternative Communication (AAC) are essential techniques\nthat help people with communication disabilities. AAC demonstrates its\ntransformative power by replacing spoken language with symbol sequences.\nHowever, to unlock its full potential, AAC materials must adhere to specific\ncharacteristics, placing the onus on educators to create custom-tailored\nmaterials and symbols. This paper introduces AMBRA (Pervasive and Personalized\nAugmentative and Alternative Communication based on Federated Learning and\nGenerative AI), an open platform that aims to leverage the capabilities of\nfoundation models to tackle many AAC issues, opening new opportunities (but\nalso challenges) for AI-enhanced AAC. We thus present a compelling vision--a\nroadmap towards a more inclusive society. By leveraging the capabilities of\nmodern technologies, we aspire to not only transform AAC but also guide the way\ntoward a world where communication knows no bounds.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.14438,regular,post_llm,2024,1,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'From the Fair Distribution of Predictions to the Fair Distribution of\n  Social Goods: Evaluating the Impact of Fair Machine Learning on Long-Term\n  Unemployment\n\n  Deploying an algorithmically informed policy is a significant intervention in\nsociety. Prominent methods for algorithmic fairness focus on the distribution\nof predictions at the time of training, rather than the distribution of social\ngoods that arises after deploying the algorithm in a specific social context.\nHowever, requiring a ""fair"" distribution of predictions may undermine efforts\nat establishing a fair distribution of social goods. First, we argue that\naddressing this problem requires a notion of prospective fairness that\nanticipates the change in the distribution of social goods after deployment.\nSecond, we provide formal conditions under which this change is identified from\npre-deployment data. That requires accounting for different kinds of\nperformative effects. Here, we focus on the way predictions change policy\ndecisions and, consequently, the causally downstream distribution of social\ngoods. Throughout, we are guided by an application from public administration:\nthe use of algorithms to predict who among the recently unemployed will remain\nunemployed in the long term and to target them with labor market programs.\nThird, using administrative data from the Swiss public employment service, we\nsimulate how such algorithmically informed policies would affect gender\ninequalities in long-term unemployment. When risk predictions are required to\nbe ""fair"" according to statistical parity and equality of opportunity,\ntargeting decisions are less effective, undermining efforts to both lower\noverall levels of long-term unemployment and to close the gender gap in\nlong-term unemployment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01658,review,post_llm,2024,1,"{'ai_likelihood': 1.933839586046007e-05, 'text': ""Untersuchung der Wirkung von Data Storytelling auf das Datenverstaendnis\n  von Dashboard-Nutzern\n\n  With the increasing use of big data and business analytics, data storytelling\nhas gained popularity as an effective means of communicating analytical\ninsights to audiences to support decision making and improve business\nperformance. However, there is little empirical evidence on the impact of data\nstorytelling on data understanding. This study validates the concept of data\nstorytelling as a construct in terms of its impact on users' data\nunderstanding. Based on empirical data analysis, the results of this study show\nthat data storytelling competence is positively associated with organizational\nperformance, which is partly due to the quality of the decision is conveyed.\nThese results provide a theoretical basis for further investigation of\npotential antecedents and consequences of data storytelling.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.08572,review,post_llm,2024,1,"{'ai_likelihood': 6.390942467583551e-06, 'text': 'The illusion of artificial inclusion\n\n  Human participants play a central role in the development of modern\nartificial intelligence (AI) technology, in psychological science, and in user\nresearch. Recent advances in generative AI have attracted growing interest to\nthe possibility of replacing human participants in these domains with AI\nsurrogates. We survey several such ""substitution proposals"" to better\nunderstand the arguments for and against substituting human participants with\nmodern generative AI. Our scoping review indicates that the recent wave of\nthese proposals is motivated by goals such as reducing the costs of research\nand development work and increasing the diversity of collected data. However,\nthese proposals ignore and ultimately conflict with foundational values of work\nwith human participants: representation, inclusion, and understanding. This\npaper critically examines the principles and goals underlying human\nparticipation to help chart out paths for future work that truly centers and\nempowers participants.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.13142,review,post_llm,2024,1,"{'ai_likelihood': 3.0133459303114152e-05, 'text': 'Unsocial Intelligence: an Investigation of the Assumptions of AGI\n  Discourse\n\n  Dreams of machines rivaling human intelligence have shaped the field of AI\nsince its inception. Yet, the very meaning of human-level AI or artificial\ngeneral intelligence (AGI) remains elusive and contested. Definitions of AGI\nembrace a diverse range of incompatible values and assumptions. Contending with\nthe fractured worldviews of AGI discourse is vital for critiques that pursue\ndifferent values and futures. To that end, we provide a taxonomy of AGI\ndefinitions, laying the ground for examining the key social, political, and\nethical assumptions they make. We highlight instances in which these\ndefinitions frame AGI or human-level AI as a technical topic and expose the\nvalue-laden choices being implicitly made. Drawing on feminist, STS, and social\nscience scholarship on the political and social character of intelligence in\nboth humans and machines, we propose contextual, democratic, and participatory\npaths to imagining future forms of machine intelligence. The development of\nfuture forms of AI must involve explicit attention to the values it encodes,\nthe people it includes or excludes, and a commitment to epistemic justice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.01291,review,post_llm,2024,1,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Generative AI is already widespread in the public sector\n\n  Generative AI has the potential to transform how public services are\ndelivered by enhancing productivity and reducing time spent on bureaucracy.\nFurthermore, unlike other types of artificial intelligence, it is a technology\nthat has quickly become widely available for bottom-up adoption: essentially\nanyone can decide to make use of it in their day to day work. But to what\nextent is generative AI already in use in the public sector? Our survey of 938\npublic service professionals within the UK (covering education, health, social\nwork and emergency services) seeks to answer this question. We find that use of\ngenerative AI systems is already widespread: 45% of respondents were aware of\ngenerative AI usage within their area of work, while 22% actively use a\ngenerative AI system. Public sector professionals were positive about both\ncurrent use of the technology and its potential to enhance their efficiency and\nreduce bureaucratic workload in the future. For example, those working in the\nNHS thought that time spent on bureaucracy could drop from 50% to 30% if\ngenerative AI was properly exploited, an equivalent of one day per week (an\nenormous potential impact). Our survey also found a high amount of trust (61%)\naround generative AI outputs, and a low fear of replacement (16%). While\nrespondents were optimistic overall, areas of concern included feeling like the\nUK is missing out on opportunities to use AI to improve public services (76%),\nand only a minority of respondents (32%) felt like there was clear guidance on\ngenerative AI usage in their workplaces. In other words, it is clear that\ngenerative AI is already transforming the public sector, but uptake is\nhappening in a disorganised fashion without clear guidelines. The UK's public\nsector urgently needs to develop more systematic methods for taking advantage\nof the technology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01689,review,post_llm,2024,1,"{'ai_likelihood': 0.002536773681640625, 'text': 'Predictive Health Analysis in Industry 5.0: A Scientometric and\n  Systematic Review of Motion Capture in Construction\n\n  In an era of rapid technological advancement, the rise of Industry 4.0 has\nprompted industries to pursue innovative improvements in their processes. As we\nadvance towards Industry 5.0, which focuses more on collaboration between\nhumans and intelligent systems, there is a growing requirement for better\nsensing technologies for healthcare and safety purposes. Consequently, Motion\nCapture (MoCap) systems have emerged as critical enablers in this technological\nevolution by providing unmatched precision and versatility in various\nworkplaces, including construction. As the construction workplace requires\nphysically demanding tasks, leading to work-related musculoskeletal disorders\n(WMSDs) and health issues, the study explores the increasing relevance of MoCap\nsystems within the concept of Industry 4.0 and 5.0. Despite the growing\nsignificance, there needs to be more comprehensive research, a scientometric\nreview that quantitatively assesses the role of MoCap systems in construction.\nOur study combines bibliometric, scientometric, and systematic review\napproaches to address this gap, analyzing articles sourced from the Scopus\ndatabase. A total of 52 papers were carefully selected from a pool of 962\npapers for a quantitative study using a scientometric approach and a\nqualitative, indepth examination. Results showed that MoCap systems are\nemployed to improve worker health and safety and reduce occupational\nhazards.The in-depth study also finds the most tested construction tasks are\nmasonry, lifting, training, and climbing, with a clear preference for\nmarkerless systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.02799,review,post_llm,2024,1,"{'ai_likelihood': 0.0726318359375, 'text': ""Missing Value Chain in Generative AI Governance China as an example\n\n  We examined the world's first regulation on Generative AI, China's\nProvisional Administrative Measures of Generative Artificial Intelligence\nServices, which came into effect in August 2023. Our assessment reveals that\nthe Measures, while recognizing the technical advances of generative AI and\nseeking to govern its full life cycle, presents unclear distinctions regarding\ndifferent roles in the value chain of Generative AI including upstream\nfoundation model providers and downstream deployers. The lack of distinction\nand clear legal status between different players in the AI value chain can have\nprofound consequences. It can lead to ambiguity in accountability, potentially\nundermining the governance and overall success of AI services.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.11335,regular,post_llm,2024,1,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Deception and Manipulation in Generative AI\n\n  Large language models now possess human-level linguistic abilities in many\ncontexts. This raises the concern that they can be used to deceive and\nmanipulate on unprecedented scales, for instance spreading political\nmisinformation on social media. In future, agentic AI systems might also\ndeceive and manipulate humans for their own ends. In this paper, first, I argue\nthat AI-generated content should be subject to stricter standards against\ndeception and manipulation than we ordinarily apply to humans. Second, I offer\nnew characterizations of AI deception and manipulation meant to support such\nstandards, according to which a statement is deceptive (manipulative) if it\nleads human addressees away from the beliefs (choices) they would endorse under\n``semi-ideal\'\' conditions. Third, I propose two measures to guard against AI\ndeception and manipulation, inspired by this characterization: ""extreme\ntransparency"" requirements for AI-generated content and defensive systems that,\namong other things, annotate AI-generated statements with contextualizing\ninformation. Finally, I consider to what extent these measures can protect\nagainst deceptive behavior in future, agentic AIs, and argue that non-agentic\ndefensive systems can provide an important layer of defense even against more\npowerful agentic systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.11892,review,post_llm,2024,1,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'AI, insurance, discrimination and unfair differentiation. An overview\n  and research agenda\n\n  Insurers underwrite risks: they calculate risks and decide on the insurance\nprice. Insurers seem captivated by two trends enabled by Artificial\nIntelligence (AI). First, insurers could use AI for analysing more and new\ntypes of data to assess risks more precisely: data-intensive underwriting.\nSecond, insurers could use AI to monitor the behaviour of individual consumers\nin real-time: behaviour-based insurance. For example, some car insurers offer a\ndiscount if the consumer agrees to being tracked by the insurer and drives\nsafely. While the two trends bring many advantages, they may also have\ndiscriminatory effects on society. This paper focuses on the following\nquestion. Which effects related to discrimination and unfair differentiation\nmay occur if insurers use data-intensive underwriting and behaviour-based\ninsurance?\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.0166,regular,post_llm,2024,1,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Integration of LaTeX formula in computer-based test application for\n  academic purposes\n\n  LaTeX is a free document preparation system that handles the typesetting of\nmathematical expressions smoothly and elegantly. It has become the standard\nformat for creating and publishing research articles in mathematics and many\nscientific fields. Computer-based testing (CBT) has become widespread in recent\nyears. Most establishments now use it to deliver assessments as an alternative\nto using the pen-paper method. To deliver an assessment, the examiner would\nfirst add a new exam or edit an existing exam using a CBT editor. Thus, the\nimplementation of CBT should comprise both support for setting and\nadministering questions. Existing CBT applications used in the academic space\nlacks the capacity to handle advanced formulas, programming codes, and tables,\nthereby resorting to converting them into images which takes a lot of time and\nstorage space. In this paper, we discuss how we solvde this problem by\nintegrating latex technology into our CBT applications. This enables seamless\nmanipulation and accurate rendering of tables, programming codes, and equations\nto increase readability and clarity on both the setting and administering of\nquestions platforms. Furthermore, this implementation has reduced drastically\nthe sizes of system resources allocated to converting tables, codes, and\nequations to images. Those in mathematics, statistics, computer science,\nengineering, chemistry, etc. will find this application useful.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01666,review,post_llm,2024,1,"{'ai_likelihood': 0.0004527303907606337, 'text': ""A Comprehensive Exploration of Personalized Learning in Smart Education:\n  From Student Modeling to Personalized Recommendations\n\n  With the development of artificial intelligence, personalized learning has\nattracted much attention as an integral part of intelligent education. China,\nthe United States, the European Union, and others have put forward the\nimportance of personalized learning in recent years, emphasizing the\nrealization of the organic combination of large-scale education and\npersonalized training. The development of a personalized learning system\noriented to learners' preferences and suited to learners' needs should be\naccelerated. This review provides a comprehensive analysis of the current\nsituation of personalized learning and its key role in education. It discusses\nthe research on personalized learning from multiple perspectives, combining\ndefinitions, goals, and related educational theories to provide an in-depth\nunderstanding of personalized learning from an educational perspective,\nanalyzing the implications of different theories on personalized learning, and\nhighlighting the potential of personalized learning to meet the needs of\nindividuals and to enhance their abilities. Data applications and assessment\nindicators in personalized learning are described in detail, providing a solid\ndata foundation and evaluation system for subsequent research. Meanwhile, we\nstart from both student modeling and recommendation algorithms and deeply\nanalyze the cognitive and non-cognitive perspectives and the contribution of\npersonalized recommendations to personalized learning. Finally, we explore the\nchallenges and future trajectories of personalized learning. This review\nprovides a multidimensional analysis of personalized learning through a more\ncomprehensive study, providing academics and practitioners with cutting-edge\nexplorations to promote continuous progress in the field of personalized\nlearning.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.1385,regular,post_llm,2024,1,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""PADTHAI-MM: Principles-based Approach for Designing Trustworthy,\n  Human-centered AI using MAST Methodology\n\n  Despite an extensive body of literature on trust in technology, designing\ntrustworthy AI systems for high-stakes decision domains remains a significant\nchallenge, further compounded by the lack of actionable design and evaluation\ntools. The Multisource AI Scorecard Table (MAST) was designed to bridge this\ngap by offering a systematic, tradecraft-centered approach to evaluating\nAI-enabled decision support systems. Expanding on MAST, we introduce an\niterative design framework called \\textit{Principles-based Approach for\nDesigning Trustworthy, Human-centered AI using MAST Methodology} (PADTHAI-MM).\nWe demonstrate this framework in our development of the Reporting Assistant for\nDefense and Intelligence Tasks (READIT), a research platform that leverages\ndata visualizations and natural language processing-based text analysis,\nemulating an AI-enabled system supporting intelligence reporting work. To\nempirically assess the efficacy of MAST on trust in AI, we developed two\ndistinct iterations of READIT for comparison: a High-MAST version, which\nincorporates AI contextual information and explanations, and a Low-MAST\nversion, akin to a ``black box'' system. This iterative design process, guided\nby stakeholder feedback and contemporary AI architectures, culminated in a\nprototype that was evaluated through its use in an intelligence reporting task.\nWe further discuss the potential benefits of employing the MAST-inspired design\nframework to address context-specific needs. We also explore the relationship\nbetween stakeholder evaluators' MAST ratings and three categories of\ninformation known to impact trust: \\textit{process}, \\textit{purpose}, and\n\\textit{performance}. Overall, our study supports the practical benefits and\ntheoretical validity for PADTHAI-MM as a viable method for designing trustable,\ncontext-specific AI systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.11981,regular,post_llm,2024,1,"{'ai_likelihood': 6.490283542209202e-06, 'text': 'Learning Analytics in Higher Education -- Exploring Students and\n  Teachers Expectations in Germany\n\n  Technology enhanced learning analytics has the potential to play a\nsignificant role in higher education in the future. Opinions and expectations\ntowards technology and learning analytics, thus, are vital to consider for\ninstitutional developments in higher education institutions. The Sheila\nframework offers instruments to yield exploratory knowledge about stakeholder\naspirations towards technology, such as learning analytics in higher education.\nThe sample of the study consists of students (N = 1169) and teachers (N = 497)\nat a higher education institution in Germany. Using self-report questionnaires,\nwe assessed students and teachers attitudes towards learning analytics in\nhigher education teaching, comparing ideal and expected circumstances. We\nreport results on the attitudes of students, teachers, as well as comparisons\nof the two groups and different disciplines. We discuss the results with regard\nto practical implications for the implementation and further developments of\nlearning analytics in higher education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.03996,review,post_llm,2024,1,"{'ai_likelihood': 1.0, 'text': ""Nigeria's ICT and Economic Sustainability in the Digital Age\n\n  Nigeria's remarkable information and communication technology (ICT) journey\nspans decades, playing a pivotal role in economic sustainability, especially as\nthe nation celebrates its Republic at Sixty. This paper provides an overview of\nNigeria's ICT journey, underscoring its central role in sustainable economic\nprosperity. We explore the potential of artificial intelligence, blockchain,\nand the Internet of Things (IoT), revealing the remarkable opportunities on the\nhorizon. We stress the urgency of achieving digital inclusivity, bridging the\nurban-rural gap, and reducing the technological divide, all of which are\ncritical as Nigeria marks its sixtieth year. We intend to prove the invaluable\nopportunities of ICT for policymakers, business leaders, and educational\ninstitutes as Nigeria looks towards enduring economic development in this\ndigital age. Specifically, we envision a dynamic landscape where emerging\ntechnologies are set to redefine industries, supercharge economic growth, and\nenhance the quality of life for every Nigerian.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00014030933380126953, 'GPT4': 0.94970703125, 'CLAUDE': 1.4543533325195312e-05, 'GOOGLE': 0.049957275390625, 'OPENAI_O_SERIES': 1.1324882507324219e-05, 'DEEPSEEK': 1.5616416931152344e-05, 'GROK': 1.0728836059570312e-06, 'NOVA': 1.1324882507324219e-06, 'OTHER': 2.4497509002685547e-05, 'HUMAN': 2.9206275939941406e-06}}"
2402.0167,review,post_llm,2024,1,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""A Scalable and Automated Framework for Tracking the likely Adoption of\n  Emerging Technologies\n\n  While new technologies are expected to revolutionise and become game-changers\nin improving the efficiencies and practises of our daily lives, it is also\ncritical to investigate and understand the barriers and opportunities faced by\ntheir adopters. Such findings can serve as an additional feature in the\ndecision-making process when analysing the risks, costs, and benefits of\nadopting an emerging technology in a particular setting. Although several\nstudies have attempted to perform such investigations, these approaches adopt a\nqualitative data collection methodology which is limited in terms of the size\nof the targeted participant group and is associated with a significant manual\noverhead when transcribing and inferring results. This paper presents a\nscalable and automated framework for tracking likely adoption and/or rejection\nof new technologies from a large landscape of adopters. In particular, a large\ncorpus of social media texts containing references to emerging technologies was\ncompiled. Text mining techniques were applied to extract sentiments expressed\ntowards technology aspects. In the context of the problem definition herein, we\nhypothesise that the expression of positive sentiment infers an increase in the\nlikelihood of impacting a technology user's acceptance to adopt, integrate,\nand/or use the technology, and negative sentiment infers an increase in the\nlikelihood of impacting the rejection of emerging technologies by adopters. To\nquantitatively test our hypothesis, a ground truth analysis was performed to\nvalidate that the sentiment captured by the text mining approach is comparable\nto the results given by human annotators when asked to label whether such texts\npositively or negatively impact their outlook towards adopting an emerging\ntechnology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.09145,regular,post_llm,2024,1,"{'ai_likelihood': 0.4172092013888889, 'text': ""Your blush gives you away: detecting hidden mental states with remote\n  photoplethysmography and thermal imaging\n\n  Multimodal emotion recognition techniques are increasingly essential for\nassessing mental states. Image-based methods, however, tend to focus\npredominantly on overt visual cues and often overlook subtler mental state\nchanges. Psychophysiological research has demonstrated that HR and skin\ntemperature are effective in detecting ANS activities, thereby revealing these\nsubtle changes. However, traditional HR tools are generally more costly and\nless portable, while skin temperature analysis usually necessitates extensive\nmanual processing. Advances in remote-PPG and automatic thermal ROI detection\nalgorithms have been developed to address these issues, yet their accuracy in\npractical applications remains limited. This study aims to bridge this gap by\nintegrating r-PPG with thermal imaging to enhance prediction performance.\nNinety participants completed a 20-minute questionnaire to induce cognitive\nstress, followed by watching a film aimed at eliciting moral elevation. The\nresults demonstrate that the combination of r-PPG and thermal imaging\neffectively detects emotional shifts. Using r-PPG alone, the prediction\naccuracy was 77% for cognitive stress and 61% for moral elevation, as\ndetermined by SVM. Thermal imaging alone achieved 79% accuracy for cognitive\nstress and 78% for moral elevation, utilizing a RF algorithm. An early fusion\nstrategy of these modalities significantly improved accuracies, achieving 87%\nfor cognitive stress and 83% for moral elevation using RF. Further analysis,\nwhich utilized statistical metrics and explainable machine learning methods\nincluding SHAP, highlighted key features and clarified the relationship between\ncardiac responses and facial temperature variations. Notably, it was observed\nthat cardiovascular features derived from r-PPG models had a more pronounced\ninfluence in data fusion, despite thermal imaging's higher predictive accuracy\nin unimodal analysis.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.13605,review,post_llm,2024,1,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Regulating AI-Based Remote Biometric Identification. Investigating the\n  Public Demand for Bans, Audits, and Public Database Registrations\n\n  AI is increasingly being used in the public sector, including public\nsecurity. In this context, the use of AI-powered remote biometric\nidentification (RBI) systems is a much-discussed technology. RBI systems are\nused to identify criminal activity in public spaces, but are criticised for\ninheriting biases and violating fundamental human rights. It is therefore\nimportant to ensure that such systems are developed in the public interest,\nwhich means that any technology that is deployed for public use needs to be\nscrutinised. While there is a consensus among business leaders, policymakers\nand scientists that AI must be developed in an ethical and trustworthy manner,\nscholars have argued that ethical guidelines do not guarantee ethical AI, but\nrather prevent stronger regulation of AI. As a possible counterweight, public\nopinion can have a decisive influence on policymakers to establish boundaries\nand conditions under which AI systems should be used -- if at all. However, we\nknow little about the conditions that lead to regulatory demand for AI systems.\nIn this study, we focus on the role of trust in AI as well as trust in law\nenforcement as potential factors that may lead to demands for regulation of AI\ntechnology. In addition, we explore the mediating effects of discrimination\nperceptions regarding RBI. We test the effects on four different use cases of\nRBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose\nof use (persecution of criminals vs. safeguarding public events) in a survey\namong German citizens. We found that German citizens do not differentiate\nbetween the different modes of application in terms of their demand for RBI\nregulation. Furthermore, we show that perceptions of discrimination lead to a\ndemand for stronger regulation, while trust in AI and trust in law enforcement\nlead to opposite effects in terms of demand for a ban on RBI systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.09591,regular,post_llm,2024,1,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""Bringing Social Computing to Secondary School Classrooms\n\n  Social computing is the study of how technology shapes human social\ninteractions. This topic has become increasingly relevant to secondary school\nstudents (ages 11--18) as more of young people's everyday social experiences\ntake place online, particularly with the continuing effects of the COVID-19\npandemic. However, social computing topics are rarely touched upon in existing\nmiddle and high school curricula. We seek to introduce concepts from social\ncomputing to secondary school students so they can understand how computing has\nwide-ranging social implications that touch upon their everyday lives, as well\nas think critically about both the positive and negative sides of different\nsocial technology designs.\n  In this report, we present a series of six lessons combining presentations\nand hands-on activities covering topics within social computing and detail our\nexperience teaching these lessons to approximately 1,405 students across 13\nmiddle and high schools in our local school district. We developed lessons\ncovering how social computing relates to the topics of Data Management,\nEncrypted Messaging, Human-Computer Interaction Careers, Machine Learning and\nBias, Misinformation, and Online Behavior. We found that 81.13% of students\nexpressed greater interest in the content of our lessons compared to their\ninterest in STEM overall. We also found from pre- and post-lesson comprehension\nquestions that 63.65% learned new concepts from the main activity. We release\nall lesson materials on a website for public use. From our experience, we\nobserved that students were engaged in these topics and found enjoyment in\nfinding connections between computing and their own lives.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.07386,regular,post_llm,2024,1,"{'ai_likelihood': 2.4007426367865668e-05, 'text': 'How do machines learn? Evaluating the AIcon2abs method\n\n  This study is an expansion of a previous work aiming to evaluate the\nAIcon2abs method (AI from Concrete to Abstract: Demystifying Artificial\nIntelligence to the general public), an innovative method aimed at increasing\nthe public (including children) understanding of machine learning (ML). The\napproach employs the WiSARD algorithm, a weightless neural network known for\nits simplicity, and user accessibility. WiSARD does not require Internet,\nmaking it ideal for non-technical users and resource-limited environments. This\nmethod enables participants to intuitively visualize and interact with ML\nprocesses through engaging, hands-on activities, as if they were the algorithms\nthemselves. The method allows users to intuitively visualize and understand the\ninternal processes of training and classification through practical activities.\nOnce WiSARDs functionality does not require an Internet connection, it can\nlearn effectively from a minimal dataset, even from a single example. This\nfeature enables users to observe how the machine improves its accuracy\nincrementally as it receives more data. Moreover, WiSARD generates mental\nimages representing what it has learned, highlighting essential features of the\nclassified data. AIcon2abs was tested through a six-hour remote course with 34\nBrazilian participants, including 5 children, 5 adolescents, and 24 adults.\nData analysis was conducted from two perspectives: a mixed-method\npre-experiment (including hypothesis testing), and a qualitative\nphenomenological analysis. Nearly all participants rated AIcon2abs positively,\nwith the results demonstrating a high degree of satisfaction in achieving the\nintended outcomes. This research was approved by the CEP-HUCFF-UFRJ Research\nEthics Committee.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.14533,review,post_llm,2024,1,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'My Future with My Chatbot: A Scenario-Driven, User-Centric Approach to\n  Anticipating AI Impacts\n\n  As a general purpose technology without a concrete pre-defined purpose,\npersonal chatbots can be used for a whole range of objectives, depending on the\npersonal needs, contexts, and tasks of an individual, and so potentially impact\na variety of values, people, and social contexts. Traditional methods of risk\nassessment are confronted with several challenges: the lack of a clearly\ndefined technology purpose, the lack of clearly defined values to orient on,\nthe heterogeneity of uses, and the difficulty of actively engaging citizens\nthemselves in anticipating impacts from the perspective of their individual\nlived realities. In this article, we leverage scenario writing at scale as a\nmethod for anticipating AI impact that is responsive to these challenges. The\nadvantages of the scenario method are its ability to engage individual users\nand stimulate them to consider how chatbots are likely to affect their reality\nand so collect different impact scenarios depending on the cultural and\nsocietal embedding of a heterogeneous citizenship. Empirically, we tasked 106\nUS-based participants to write short fictional stories about the future impact\n(whether desirable or undesirable) of AI-based personal chatbots on individuals\nand society and, in addition, ask respondents to explain why these impacts are\nimportant and how they relate to their values. In the analysis process, we map\nthose impacts and analyze them in relation to socio-demographic as well as\nAI-related attitudes of the scenario writers. We show that our method is\neffective in (1) identifying and mapping desirable and undesirable impacts of\nAI-based personal chatbots, (2) setting these impacts in relation to values\nthat are important for individuals, and (3) detecting socio-demographic and\nAI-attitude related differences of impact anticipation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.12423,regular,post_llm,2024,1,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Rank, Pack, or Approve: Voting Methods in Participatory Budgeting\n\n  Participatory budgeting is a popular method to engage residents in budgeting\ndecisions by local governments. The Stanford Participatory Budgeting platform\nis an online platform that has been used to engage residents in more than 150\nbudgeting processes. We present a data set with anonymized budget opinions from\nthese processes with K-approval, K-ranking or knapsack primary ballots. For a\nsubset of the voters, it includes paired votes with a different elicitation\nmethod in the same process. This presents a unique data set, as the voters,\nprojects and setting are all related to real-world decisions that the voters\nhave an actual interest in. With data from primary ballots we find that while\nballot complexity (number of projects to choose from, number of projects to\nselect and ballot length) is correlated with a higher median time spent by\nvoters, it is not correlated with a higher abandonment rate.\n  We use vote pairs with different voting methods to analyze the effect of\nvoting methods on the cost of selected projects, more comprehensively than was\npreviously possible. In most elections, voters selected significantly more\nexpensive projects using K-approval than using knapsack, although we also find\na small number of examples with a significant effect in the opposite direction.\nThis effect happens at the aggregate level as well as for individual voters,\nand is influenced both by the implicit constraints of the voting method and the\nexplicit constraints of the voting interface. Finally, we validate the use of\nK-ranking elicitation to offer a paper alternative for knapsack voting.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.06453,regular,post_llm,2024,1,"{'ai_likelihood': 1.2814998626708984e-05, 'text': ""Causally Aware Generative Adversarial Networks for Light Pollution\n  Control\n\n  Artificial light plays an integral role in modern cities, significantly\nenhancing human productivity and the efficiency of civilization. However,\nexcessive illumination can lead to light pollution, posing non-negligible\nthreats to economic burdens, ecosystems, and human health. Despite its critical\nimportance, the exploration of its causes remains relatively limited within the\nfield of artificial intelligence, leaving an incomplete understanding of the\nfactors contributing to light pollution and sustainable illumination planning\ndistant. To address this gap, we introduce a novel framework named Causally\nAware Generative Adversarial Networks (CAGAN). This innovative approach aims to\nuncover the fundamental drivers of light pollution within cities and offer\nintelligent solutions for optimal illumination resource allocation in the\ncontext of sustainable urban development. We commence by examining light\npollution across 33,593 residential areas in seven global metropolises. Our\nfindings reveal substantial influences on light pollution levels from various\nbuilding types, notably grasslands, commercial centers and residential\nbuildings as significant contributors. These discovered causal relationships\nare seamlessly integrated into the generative modeling framework, guiding the\nprocess of generating light pollution maps for diverse residential areas.\nExtensive experiments showcase CAGAN's potential to inform and guide the\nimplementation of effective strategies to mitigate light pollution. Our code\nand data are publicly available at\nhttps://github.com/zhangyuuao/Light_Pollution_CAGAN.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01718,review,post_llm,2024,1,"{'ai_likelihood': 5.125999450683594e-05, 'text': 'Business Models for Digitalization Enabled Energy Efficiency and\n  Flexibility in Industry: A Survey with Nine Case Studies\n\n  Digitalization is challenging in heavy industrial sectors, and many pi-lot\nprojects facing difficulties to be replicated and scaled. Case studies are\nstrong pedagogical vehicles for learning and sharing experience & knowledge,\nbut rarely available in the literature. Therefore, this paper conducts a survey\nto gather a diverse set of nine industry cases, which are subsequently\nsubjected to analysis using the business model canvas (BMC). The cases are\nsummarized and compared based on nine BMC components, and a Value of Business\nModel (VBM) evaluation index is proposed to assess the business potential of\nindustrial digital solutions. The results show that the main partners are\nindustry stakeholders, IT companies and academic institutes. Their key\nactivities for digital solutions include big-data analysis, machine learning\nalgorithms, digital twins, and internet of things developments. The value\npropositions of most cases are improving energy efficiency and enabling energy\nflexibility. Moreover, the technology readiness levels of six industrial\ndigital solutions are under level 7, indicating that they need further\nvalidation in real-world environments. Building upon these insights, this paper\nproposes six recommendations for future industrial digital solution\ndevelopment: fostering cross-sector collaboration, prioritizing comprehensive\ntesting and validation, extending value propositions, enhancing product\nadaptability, providing user-friendly platforms, and adopting transparent\nrecommendations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01667,regular,post_llm,2024,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': ""Students' accommodation allocation: A Multicriteria Decision Support\n  System\n\n  The social life of students at university has an impact on their educational\nsuccess. The allocation of accommodation is part of this aspect. This article\npresents our proposal to improve students' allocation accommodation. We aim to\nsupport university administrative departments for the selection of students for\nhousing. Therefore, we propose a decision support system based on\nmulti-criteria decision support methods. To calculate the weights of the\ncriteria, we use the AHP method. Then, to rank the students, AHP, Weighted Sum\nMethod and PROMETHEE methods are used. The aim is to find the most adequate\nmethod to rank the students. The result is achieved because the AHP is able to\ncalculate the weight of criteria and the AHP, SWM and PROMETHEE are able to\nrank the students.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.16863,review,post_llm,2024,1,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Enabling the Digital Democratic Revival: A Research Program for Digital\n  Democracy\n\n  This white paper outlines a long-term scientific vision for the development\nof digital-democracy technology. We contend that if digital democracy is to\nmeet the ambition of enabling a participatory renewal in our societies, then a\ncomprehensive multi-methods research effort is required that could, over the\nyears, support its development in a democratically principled, empirically and\ncomputationally informed way. The paper is co-authored by an international and\ninterdisciplinary team of researchers and arose from the Lorentz Center\nWorkshop on ``Algorithmic Technology for Democracy'' (Leiden, October 2022).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.03481,review,post_llm,2024,1,"{'ai_likelihood': 6.02669186062283e-06, 'text': 'A Large Language Model Supported Synthesis of Contemporary Academic\n  Integrity Research Trends\n\n  This paper reports on qualitative content analysis undertaken using ChatGPT,\na Large Language Model (LLM), to identify primary research themes in current\nacademic integrity research as well as the methodologies used to explore these\nareas. The analysis by the LLM identified 7 research themes and 13 key areas\nfor exploration. The outcomes from the analysis suggest that much contemporary\nresearch in the academic integrity field is guided by technology. Technology is\noften explored as potential way of preventing academic misconduct, but this\ncould also be a limiting factor when aiming to promote a culture of academic\nintegrity. The findings underscore that LLM led research may be option in the\nacademic integrity field, but that there is also a need for continued\ntraditional research. The findings also indicate that researchers and\neducational providers should continue to develop policy and operational\nframeworks for academic integrity. This will help to ensure that academic\nstandards are maintained across the wide range of settings that are present in\nmodern education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.17701,regular,post_llm,2024,1,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""Towards a low-cost universal access cloud framework to assess STEM\n  students\n\n  Government-imposed lockdowns have challenged academic institutions to\ntransition from traditional face-to-face education into hybrid or fully remote\nlearning models. This transition has focused on the technological challenge of\nguaranteeing the continuity of sound pedagogy and granting safe access to\nonline digital university services. However, a key requisite involves adapting\nthe evaluation process as well. In response to this need, the authors of this\npaper tailored and implemented a cloud deployment to provide universal access\nto online summative assessment of university students in a computer programming\ncourse that mirrored a traditional in-person monitored computer laboratory\nunder strictly controlled exam conditions. This deployment proved easy to\nintegrate with the university systems and many commercial proctoring tools.\nThis cloud deployment is not only a solution for extraordinary situations; it\ncan also be adapted daily for online collaborative coding assignments,\npractical lab sessions, formative assessments, and masterclasses where the\nstudents connect using their equipment. Connecting from home facilitates access\nto education for students with physical disabilities. It also allows\nparticipation with their students' own adapted equipment in the evaluation\nprocesses, simplifying assessment for those with hearing or visual impairments.\nIn addition to these benefits and the evident commitment to the safety rules,\nthis solution has proven cheaper and more flexible than on-premise equivalent\ninstallations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.00021,review,post_llm,2024,1,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Monitoring-Supported Value Generation for Managing Structures and\n  Infrastructure Systems\n\n  To maximize its value, the design, development and implementation of\nStructural Health Monitoring (SHM) should focus on its role in facilitating\ndecision support. In this position paper, we offer perspectives on the synergy\nbetween SHM and decision-making. We propose a classification of SHM use cases\naligning with various dimensions that are closely linked to the respective\ndecision contexts. The types of decisions that have to be supported by the SHM\nsystem within these settings are discussed along with the corresponding\nchallenges. We provide an overview of different classes of models that are\nrequired for integrating SHM in the decision-making process to support\nmanagement and operation and maintenance of structures and infrastructure\nsystems. Fundamental decision-theoretic principles and state-of-the-art methods\nfor optimizing maintenance and operational decision-making under uncertainty\nare briefly discussed. Finally, we offer a viewpoint on the appropriate course\nof action for quantifying, validating and maximizing the added value generated\nby SHM. This work aspires to synthesize the different perspectives of the SHM,\nPrognostic Health Management (PHM), and reliability communities, and deliver a\nroadmap towards monitoring-based decision support.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.03229,review,post_llm,2024,1,"{'ai_likelihood': 3.814697265625e-05, 'text': 'Autonomous Crowdsensing: Operating and Organizing Crowdsensing for\n  Sensing Automation\n\n  The precise characterization and modeling of Cyber-Physical-Social Systems\n(CPSS) requires more comprehensive and accurate data, which imposes heightened\ndemands on intelligent sensing capabilities. To address this issue,\nCrowdsensing Intelligence (CSI) has been proposed to collect data from CPSS by\nharnessing the collective intelligence of a diverse workforce. Our first and\nsecond Distributed/Decentralized Hybrid Workshop on Crowdsensing Intelligence\n(DHW-CSI) have focused on principles and high-level processes of organizing and\noperating CSI, as well as the participants, methods, and stages involved in\nCSI. This letter reports the outcomes of the latest DHW-CSI, focusing on\nAutonomous Crowdsensing (ACS) enabled by a range of technologies such as\ndecentralized autonomous organizations and operations, large language models,\nand human-oriented operating systems. Specifically, we explain what ACS is and\nexplore its distinctive features in comparison to traditional crowdsensing.\nMoreover, we present the ``6A-goal"" of ACS and propose potential avenues for\nfuture research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.15497,review,post_llm,2024,1,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Foregrounding Artist Opinions: A Survey Study on Transparency,\n  Ownership, and Fairness in AI Generative Art\n\n  Generative AI tools are used to create art-like outputs and sometimes aid in\nthe creative process. These tools have potential benefits for artists, but they\nalso have the potential to harm the art workforce and infringe upon artistic\nand intellectual property rights. Without explicit consent from artists,\nGenerative AI creators scrape artists' digital work to train Generative AI\nmodels and produce art-like outputs at scale. These outputs are now being used\nto compete with human artists in the marketplace as well as being used by some\nartists in their generative processes to create art. We surveyed 459 artists to\ninvestigate the tension between artists' opinions on Generative AI art's\npotential utility and harm. This study surveys artists' opinions on the utility\nand threat of Generative AI art models, fair practices in the disclosure of\nartistic works in AI art training models, ownership and rights of AI art\nderivatives, and fair compensation. Results show that a majority of artists\nbelieve creators should disclose what art is being used in AI training, that AI\noutputs should not belong to model creators, and express concerns about AI's\nimpact on the art workforce and who profits from their art. We hope the results\nof this work will further meaningful collaboration and alignment between the\nart community and Generative AI researchers and developers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01731,regular,post_llm,2024,1,"{'ai_likelihood': 2.7649932437472875e-05, 'text': 'Integration of Artificial Intelligence in Educational Measurement:\n  Efficacy of ChatGPT in Data Generation within the Scope of Item Response\n  Theory\n\n  The aim of this study is to investigate the effectiveness of ChatGPT 3.5 in\ndeveloping algorithms for data generation within the framework of Item Response\nTheory (IRT) using the R programming language. In this context, validity\nexaminations were conducted on data sets generated according to the\nTwo-Parameter Logistic Model (2PLM) with algorithms written by ChatGPT 3.5 and\nresearchers. These examinations considered whether the data sets met the IRT\nassumptions and the simulation conditions of the item parameters. As a result,\nit was determined that while ChatGPT 3.5 was quite successful in generating\ndata that met the IRT assumptions, it was less effective in meeting the\nsimulation conditions of the item parameters compared to the algorithm\ndeveloped by the researchers. In this regard, ChatGPT 3.5 is recommended as a\nuseful tool that researchers can use in developing data generation algorithms\nfor IRT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01701,review,post_llm,2024,1,"{'ai_likelihood': 1.7881393432617188e-06, 'text': ""Nie pozw\\'ol algorytmom rz\\k{a}dzi\\'c Twoim koszykiem: systemy\n  rekomendacyjne w dobie Omnibusa\n\n  The Omnibus Directive is an essential part of the European Union's New Deal\nfor Consumers. The Directive introduces new regulations in trade, including\ne-commerce, with the main goal being to increase transparency, fairness and\nconsumer protection. The authors critically draw attention to a significant\noversight in the Omnibus Directive, namely the lack of consideration of\nrecommendation systems. Recommendation engines can be a source of potentially\nharmful practices affecting consumers, hence the need for a directive\nextension. The proposals presented in this article include the introduction of\nethical supervision over recommendation systems to minimize the risk of\nnegative effects of their recommendations, as well as a clear explanation of\nthe criteria on which recommendations are made -- similar to search result\nrankings.\n  --\n  Dyrektywa Omnibus stanowi istotn\\k{a} cz\\k{e}\\'s\\'c Nowego {\\L}adu dla\nKonsument\\'ow (ang. \\emph{New Deal for Consumers}) Unii Europejskiej. Dyrektywa\nwprowadza nowe regulacje w handlu, w tym e-commerce, kt\\'orych g{\\l}\\'ownym\ncelem jest zwi\\k{e}kszenie przejrzysto\\'sci, uczciwo\\'sci i ochrony\nkonsument\\'ow. Autorzy krytycznie zwracaj\\k{a} uwag\\k{e} na istotne zaniedbanie\nw dyrektywie Omnibus, jakim jest brak uwzgl\\k{e}dnienia system\\'ow\nrekomendacyjnych. Silniki rekomendacyjne mog\\k{a} by\\'c \\'zr\\'od{\\l}em\npotencjalnie szkodliwych praktyk uderzaj\\k{a}cych w konsument\\'ow, st\\k{a}d\nniezb\\k{e}dne jest rozszerzenie dyrektywy. Propozycje przedstawione w\nniniejszym artykule obejmuj\\k{a} wprowadzenie etycznego nadzoru nad systemami\nrekomenduj\\k{a}cymi, aby zminimalizowa\\'c ryzyko negatywnych skutk\\'ow ich\nrekomendacji, a tak\\.ze jasne wyja\\'snienie kryteri\\'ow, na podstawie kt\\'orych\ndokonywane s\\k{a} rekomendacje -- analogicznie do ranking\\'ow wynik\\'ow\nwyszukiwania.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.12755,review,post_llm,2024,1,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Towards Risk Analysis of the Impact of AI on the Deliberate Biological\n  Threat Landscape\n\n  The perception that the convergence of biological engineering and artificial\nintelligence (AI) could enable increased biorisk has recently drawn attention\nto the governance of biotechnology and artificial intelligence. The 2023\nExecutive Order, Executive Order on the Safe, Secure, and Trustworthy\nDevelopment and Use of Artificial Intelligence, requires an assessment of how\nartificial intelligence can increase biorisk. Within this perspective,\nquantitative and qualitative frameworks for evaluating biorisk are presented.\nBoth frameworks are exercised using notional scenarios and their benefits and\nlimitations are then discussed. Finally, the perspective concludes by noting\nthat assessment and evaluation methodologies must keep pace with advances of AI\nin the life sciences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.13079,review,post_llm,2024,1,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of\n  Information Retrieval of Genocide-Related Information\n\n  The growing application of artificial intelligence (AI) in the field of\ninformation retrieval (IR) affects different domains, including cultural\nheritage. By facilitating organisation and retrieval of large volumes of\nheritage-related content, AI-driven IR systems inform users about a broad range\nof historical phenomena, including genocides (e.g. the Holocaust). However, it\nis currently unclear to what degree IR systems are capable of dealing with\nmultiple ethical challenges associated with the curation of genocide-related\ninformation. To address this question, this chapter provides an overview of\nethical challenges associated with the human curation of genocide-related\ninformation using a three-part framework inspired by Belmont criteria (i.e.\ncuration challenges associated with respect for individuals, beneficence and\njustice/fairness). Then, the chapter discusses to what degree the\nabove-mentioned challenges are applicable to the ways in which AI-driven IR\nsystems deal with genocide-related information and what can be the potential\nways of bridging AI and memory ethics in this context.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.01796,review,post_llm,2024,1,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Understanding engagement with platform safety technology for reducing\n  exposure to online harms\n\n  User facing 'platform safety technology' encompasses an array of tools\noffered by platforms to help people protect themselves from harm, for example\nallowing people to report content and unfollow or block other users. These\ntools are an increasingly important part of online safety: in the UK,\nlegislation has made it a requirement for large platforms to offer them.\nHowever, little is known about user engagement with such tools. We present\nfindings from a nationally representative survey of UK adults covering their\nawareness of and experiences with seven common safety technologies. We show\nthat experience of online harms is widespread, with 67% of people having seen\nwhat they perceived as harmful content online; 26% of people have also had at\nleast one piece of content removed by content moderation. Use of safety\ntechnologies is also high, with more than 80\\% of people having used at least\none. Awareness of specific tools is varied, with people more likely to be aware\nof 'post-hoc' safety tools, such as reporting, than preventative measures.\nHowever, satisfaction with safety technologies is generally low. People who\nhave previously seen online harms are more likely to use safety tools, implying\na 'learning the hard way' route to engagement. Those higher in digital literacy\nare also more likely to use some of these tools, raising concerns about the\naccessibility of these technologies to all users. Additionally, women are more\nlikely to engage in particular types of online 'safety work'. We discuss the\nimplications of our results for those seeking a safer online environment.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2401.07059,regular,post_llm,2024,1,"{'ai_likelihood': 3.22527355617947e-05, 'text': 'Classifying Proposals of Decentralized Autonomous Organizations Using\n  Large Language Models\n\n  Our study demonstrates the effective use of Large Language Models (LLMs) for\nautomating the classification of complex datasets. We specifically target\nproposals of Decentralized Autonomous Organizations (DAOs), as the\nclas-sification of this data requires the understanding of context and,\ntherefore, depends on human expertise, leading to high costs associated with\nthe task. The study applies an iterative approach to specify categories and\nfurther re-fine them and the prompt in each iteration, which led to an accuracy\nrate of 95% in classifying a set of 100 proposals. With this, we demonstrate\nthe po-tential of LLMs to automate data labeling tasks that depend on textual\ncon-text effectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.18463,regular,post_llm,2024,2,"{'ai_likelihood': 1.6921096377902562e-05, 'text': 'Understanding the Impact of AI Generated Content on Social Media: The\n  Pixiv Case\n\n  In the last two years, Artificial Intelligence Generated Content (AIGC) has\nreceived significant attention, leading to an anecdotal rise in the amount of\nAIGC being shared via social media platforms. The impact of AIGC and its\nimplications are of key importance to social platforms, e.g., regarding the\nimplementation of policies, community formation, and algorithmic design. Yet,\nto date, we know little about how the arrival of AIGC has impacted the social\nmedia ecosystem. To fill this gap, we present a comprehensive study of Pixiv,\nan online community for artists who wish to share and receive feedback on their\nillustrations. Pixiv hosts over 100 million artistic submissions and receives\nmore than 1 billion page views per month (as of 2023). Importantly, it allows\nboth human and AI generated content to be uploaded. Exploiting this, we perform\nthe first analysis of the impact that AIGC has had on the social media\necosystem, through the lens of Pixiv. Based on a dataset of 15.2 million posts\n(including 2.4 million AI-generated images), we measure the impact of AIGC on\nthe Pixiv community, as well as the differences between AIGC and\nhuman-generated content in terms of content creation and consumption patterns.\nOur results offer key insight to how AIGC is changing the dynamics of social\nmedia platforms like Pixiv.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.18545,regular,post_llm,2024,2,"{'ai_likelihood': 3.4769376118977866e-06, 'text': ""Crowdsourcing Dermatology Images with Google Search Ads: Creating a\n  Real-World Skin Condition Dataset\n\n  Background: Health datasets from clinical sources do not reflect the breadth\nand diversity of disease in the real world, impacting research, medical\neducation, and artificial intelligence (AI) tool development. Dermatology is a\nsuitable area to develop and test a new and scalable method to create\nrepresentative health datasets.\n  Methods: We used Google Search advertisements to invite contributions to an\nopen access dataset of images of dermatology conditions, demographic and\nsymptom information. With informed contributor consent, we describe and release\nthis dataset containing 10,408 images from 5,033 contributions from internet\nusers in the United States over 8 months starting March 2023. The dataset\nincludes dermatologist condition labels as well as estimated Fitzpatrick Skin\nType (eFST) and Monk Skin Tone (eMST) labels for the images.\n  Results: We received a median of 22 submissions/day (IQR 14-30). Female\n(66.72%) and younger (52% < age 40) contributors had a higher representation in\nthe dataset compared to the US population, and 32.6% of contributors reported a\nnon-White racial or ethnic identity. Over 97.5% of contributions were genuine\nimages of skin conditions. Dermatologist confidence in assigning a differential\ndiagnosis increased with the number of available variables, and showed a weaker\ncorrelation with image sharpness (Spearman's P values <0.001 and 0.01\nrespectively). Most contributions were short-duration (54% with onset < 7 days\nago ) and 89% were allergic, infectious, or inflammatory conditions. eFST and\neMST distributions reflected the geographical origin of the dataset. The\ndataset is available at github.com/google-research-datasets/scin .\n  Conclusion: Search ads are effective at crowdsourcing images of health\nconditions. The SCIN dataset bridges important gaps in the availability of\nrepresentative images of common skin conditions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.07339,regular,post_llm,2024,2,"{'ai_likelihood': 4.933940039740669e-06, 'text': 'Beyond the Headlines: Understanding Sentiments and Morals Impacting\n  Female Employment in Spain\n\n  After decades of improvements in the employment conditions of females in\nSpain, this process came to a sudden stop with the Great Spanish Recession of\n2008. In this contribution, we analyse a large longitudinal corpus of national\nand regional news outlets employing advanced Natural Language Processing\ntechniques to capture the valence of mentions of gender inequality expressed in\nthe Spanish press. The automatic analysis of the news articles does indeed\ncapture the known hardships faced by females in the Spanish labour market. Our\napproach can be straightforwardly generalised to other topics of interest.\nAssessing the sentiment and moral values expressed in the articles, we notice\nthat females are, in the majority of cases, concerned more than males when\nthere is a deterioration in the overall labour market conditions, based on\nnewspaper articles. This behaviour has been present in the entire period of\nstudy (2000--2022) and looked particularly pronounced during the economic\ncrisis of 2008 and the recent COVID-19 pandemic. Most of the time, this\nphenomenon looks to be more pronounced at the regional level, perhaps caused by\na significant focus on local labour markets rather than on aggregate statistics\nor because, in local contexts, females might suffer more from an isolation or\ndiscrimination condition. Our findings contribute to a deeper understanding of\nthe gender inequalities in Spain using alternative data, informing policymakers\nand stakeholders.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.08171,review,post_llm,2024,2,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Epistemic Power in AI Ethics Labor: Legitimizing Located Complaints\n\n  What counts as legitimate AI ethics labor, and consequently, what are the\nepistemic terms on which AI ethics claims are rendered legitimate? Based on 75\ninterviews with technologists including researchers, developers, open source\ncontributors, and activists, this paper explores the various epistemic bases\nfrom which AI ethics is discussed and practiced. In the context of outside\nattacks on AI ethics as an impediment to ""progress,"" I show how some AI ethics\npractices have reached toward authority from automation and quantification, and\nachieved some legitimacy as a result, while those based on richly embodied and\nsituated lived experience have not. This paper draws together the work of\nfeminist Anthropology and Science and Technology Studies scholars Diana\nForsythe and Lucy Suchman with the works of postcolonial feminist theorist Sara\nAhmed and Black feminist theorist Kristie Dotson to examine the implications of\ndominant AI ethics practices.\n  By entrenching the epistemic power of quantification, dominant AI ethics\npractices -- employing Model Cards and similar interventions -- risk\nlegitimizing AI ethics as a project in equal and opposite measure to which they\nmarginalize embodied lived experience as a legitimate part of the same project.\nIn response, I propose humble technical practices: quantified or technical\npractices which specifically seek to make their epistemic limits clear in order\nto flatten hierarchies of epistemic power.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.1207,review,post_llm,2024,2,"{'ai_likelihood': 4.76837158203125e-06, 'text': ""Ambivalence in stakeholders' views on connected and autonomous vehicles\n\n  Connected and autonomous vehicles (CAVs) are often discussed as a solution to\npressing issues of the current transport systems, including congestion, safety,\nsocial inclusion and ecological sustainability. Scientifically, there is\nagreement that CAVs may solve, but can also aggravate these issues, depending\non the specific CAV solution. In the current paper, we investigate the visions\nand worst-case scenarios of various stakeholders, including representatives of\npublic administrations, automotive original equipment manufacturers, insurance\ncompanies, public transportation service providers, mobility experts and\npoliticians. A qualitative analysis of 17 semi-structured interviews is\npresented. It reveals experts' ambivalence towards the introduction of CAVs,\nreflecting high levels of uncertainty about CAV consequences, including issues\nof efficiency, comfort and sustainability, and concerns about road co-users\nsuch as pedestrians and cyclists. Implications of the sluggishness of\npolicymakers to set boundary conditions and for the labor market are discussed.\nAn open debate between policymakers, citizens and other stakeholders on how to\nintroduce CAVs seems timely.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01551,regular,post_llm,2024,2,"{'ai_likelihood': 0.14119466145833334, 'text': 'Mapping acceptance: micro scenarios as a dual-perspective approach for\n  assessing public opinion and individual differences in technology perception\n\n  Understanding public perception of technology is crucial to aligning\nresearch, development, and governance of technology. This article introduces\nmicro scenarios as an integrative method to evaluate mental models and social\nacceptance across numerous technologies and concepts using a few single-item\nscales within a single comprehensive survey. This approach contrasts with\ntraditional methods that focus on detailed assessments of as few as one\nscenario. The data can be interpreted in two ways: Perspective (1): Average\nevaluations of each participant can be seen as individual differences,\nproviding reflexive measurements across technologies or topics. This helps in\nunderstanding how perceptions of technology relate to other personality\nfactors. Perspective (2): Average evaluations of each technology or topic can\nbe interpreted as technology attributions. This makes it possible to position\ntechnologies on visuo-spatial maps to simplify identification of critical\nissues, conduct comparative rankings based on selected criteria, and to analyze\nthe interplay between different attributions. This dual approach enables the\nmodeling of acceptance-relevant factors that shape public opinion. It offers a\nframework for researchers, technology developers, and policymakers to identify\npivotal factors for acceptance at both the individual and technology levels. I\nillustrate this methodology with examples from my research, provide practical\nguidelines, and include R code to enable others to conduct similar studies.\nThis paper aims to bridge the gap between technological advancement and\nsocietal perception, offering a tool for more informed decision-making in\ntechnology development and policy-making.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.08797,review,post_llm,2024,2,"{'ai_likelihood': 3.013345930311415e-06, 'text': 'Computing Power and the Governance of Artificial Intelligence\n\n  Computing power, or ""compute,"" is crucial for the development and deployment\nof artificial intelligence (AI) capabilities. As a result, governments and\ncompanies have started to leverage compute as a means to govern AI. For\nexample, governments are investing in domestic compute capacity, controlling\nthe flow of compute to competing countries, and subsidizing compute access to\ncertain sectors. However, these efforts only scratch the surface of how compute\ncan be used to govern AI development and deployment. Relative to other key\ninputs to AI (data and algorithms), AI-relevant compute is a particularly\neffective point of intervention: it is detectable, excludable, and\nquantifiable, and is produced via an extremely concentrated supply chain. These\ncharacteristics, alongside the singular importance of compute for cutting-edge\nAI models, suggest that governing compute can contribute to achieving common\npolicy objectives, such as ensuring the safety and beneficial use of AI. More\nprecisely, policymakers could use compute to facilitate regulatory visibility\nof AI, allocate resources to promote beneficial outcomes, and enforce\nrestrictions against irresponsible or malicious AI development and usage.\nHowever, while compute-based policies and technologies have the potential to\nassist in these areas, there is significant variation in their readiness for\nimplementation. Some ideas are currently being piloted, while others are\nhindered by the need for fundamental research. Furthermore, naive or poorly\nscoped approaches to compute governance carry significant risks in areas like\nprivacy, economic impacts, and centralization of power. We end by suggesting\nguardrails to minimize these risks from compute governance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.10968,regular,post_llm,2024,2,"{'ai_likelihood': 0.94384765625, 'text': 'Thermal Infrared Imaging to Evaluate Emotional Competences in Nursing\n  Students: A First Approach through a Case Study\n\n  During nursing studies, it is crucial to develop emotional skills for both\nacademic success and quality patient care. Utilizing technologies like\nthermography can be instrumental in nursing education to assess and enhance\nthese skills. The study aims to evaluate the effectiveness of thermography in\nmonitoring and improving the emotional skills of nursing students through a\ncase study approach. The case study involved exposing a student to various\nemotional stimuli, including videos and music, and measuring facial temperature\nchanges. These changes were recorded using a FLIR E6 camera across three\nphases: acclimatization, stimulus, and response. Environmental factors such as\ntemperature and humidity were also recorded. Distinct thermal responses were\nobserved for different emotions. For instance, during the acclimatization phase\nwith video stimuli, forehead temperatures varied between positive emotions\n(joy: 34.5\\textdegree C to 34.5\\textdegree C) and negative emotions (anger:\n36.1\\textdegree C to 35.1\\textdegree C). However, there was a uniform change in\ntemperature during both stimulus (joy: 34.7\\textdegree C to 35.0\\textdegree C,\nanger: 35.0\\textdegree C to 35.0\\textdegree C) and response phases (joy:\n35.0\\textdegree C to 35.0\\textdegree C, anger: 34.8\\textdegree C to\n35.0\\textdegree C). Music stimuli also induced varying thermal patterns (joy:\n34.2\\textdegree C to 33.9\\textdegree C to 33.4\\textdegree C, anger:\n33.8\\textdegree C to 33.4\\textdegree C to 33.8\\textdegree C).Thermography\nrevealed consistent thermal patterns in response to emotional stimuli, with the\nexception of the nose area, suggesting its suitability as a non-invasive,\nquantifiable, and accessible method for emotional skill training in nursing\neducation.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00760650634765625, 'GPT4': 0.4169921875, 'CLAUDE': 0.00067138671875, 'GOOGLE': 0.5576171875, 'OPENAI_O_SERIES': 0.01421356201171875, 'DEEPSEEK': 0.0003120899200439453, 'GROK': 8.338689804077148e-05, 'NOVA': 1.9431114196777344e-05, 'OTHER': 0.0015125274658203125, 'HUMAN': 0.0008387565612792969}}"
2402.00442,regular,post_llm,2024,2,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Responsible developments and networking research: a reflection beyond a\n  paper ethical statement\n\n  Several recent initiatives have proposed new directions for research\npractices and their operations in the computer science community, from updated\ncodes of conduct that clarify the use of AI-assisted tools to the inclusion of\nethical statements and the organization of working groups on the environmental\nfootprint of digitalization. In this position paper, we focus on the specific\ncase of networking research. We reflect on the technical realization of the\ncommunity and its incidence beyond techno-centric contributions. In particular,\nwe structure the discussion around two frameworks that were recently developed\nin different contexts to describe the sense of engagement and responsibilities\nto which the practitioner of a computing-related area may be confronted.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.18321,review,post_llm,2024,2,"{'ai_likelihood': 3.8080745273166235e-06, 'text': 'Privacy Policies and Consent Management Platforms: Growth and Users\'\n  Interactions over Time\n\n  In response to growing concerns about user privacy, legislators have\nintroduced new regulations and laws such as the General Data Protection\nRegulation (GDPR) and the California Consumer Privacy Act (CCPA) that force\nwebsites to obtain user consent before activating personal data collection,\nfundamental to providing targeted advertising. The cornerstone of this\nconsent-seeking process involves the use of Privacy Banners, the technical\nmechanism to collect users\' approval for data collection practices. Consent\nmanagement platforms (CMPs) have emerged as practical solutions to make it\neasier for website administrators to properly manage consent, allowing them to\noutsource the complexities of managing user consent and activating advertising\nfeatures.\n  This paper presents a detailed and longitudinal analysis of the evolution of\nCMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the\nHTTP Archive dataset, we provide insights into the growth, market share, and\ngeographical spread of CMPs. Noteworthy observations include the substantial\nimpact of GDPR on the proliferation of CMPs in Europe. Secondly, we analyse\nmillions of user interactions with a medium-sized CMP present in thousands of\nwebsites worldwide. We observe how even small changes in the design of Privacy\nBanners have a critical impact on the user\'s giving or denying their consent to\ndata collection. For instance, over 60% of users do not consent when offered a\nsimple ""one-click reject-all"" option. Conversely, when opting out requires more\nthan one click, about 90% of users prefer to simply give their consent. The\nmain objective is in fact to eliminate the annoying privacy banner rather the\nmake an informed decision. Curiously, we observe iOS users exhibit a higher\ntendency to accept cookies compared to Android users, possibly indicating\ngreater confidence in the privacy offered by Apple devices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.17861,review,post_llm,2024,2,"{'ai_likelihood': 6.4240561591254345e-06, 'text': ""Towards AI Accountability Infrastructure: Gaps and Opportunities in AI\n  Audit Tooling\n\n  Audits are critical mechanisms for identifying the risks and limitations of\ndeployed artificial intelligence (AI) systems. However, the effective execution\nof AI audits remains incredibly difficult, and practitioners often need to make\nuse of various tools to support their efforts. Drawing on interviews with 35 AI\naudit practitioners and a landscape analysis of 435 tools, we compare the\ncurrent ecosystem of AI audit tooling to practitioner needs. While many tools\nare designed to help set standards and evaluate AI systems, they often fall\nshort in supporting accountability. We outline challenges practitioners faced\nin their efforts to use AI audit tools and highlight areas for future tool\ndevelopment beyond evaluation -- from harms discovery to advocacy. We conclude\nthat the available resources do not currently support the full scope of AI\naudit practitioners' needs and recommend that the field move beyond tools for\njust evaluation and towards more comprehensive infrastructure for AI\naccountability.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.03551,regular,post_llm,2024,2,"{'ai_likelihood': 1.7318460676405164e-05, 'text': ""A retrospective analysis of Montana's 2020 congressional redistricting\n  map\n\n  The 2020 decennial census data resulted in an increase from one to two\ncongressional representatives in the state of Montana. The state underwent its\nredistricting process in 2021 in time for the November 2022 congressional\nelections, carving the state into two districts. This paper analyzes the\nredistricting process and compares the adopted congressional map to the space\nof all other possible maps. In particular, we look at the population deviation,\ncompactness and political outcomes of these maps. We also consider how well two\npopular sampling techniques, that sample from the space of possible maps,\napproximate the true distributions of these measures.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.1928,regular,post_llm,2024,2,"{'ai_likelihood': 1.7219119601779514e-06, 'text': 'Mobile Health Text Misinformation Identification Using Mobile Data\n  Mining\n\n  More than six million people died of the COVID-19 by April 2022. The heavy\ncasualties have put people on great and urgent alert and people try to find all\nkinds of information to keep them from being inflected by the coronavirus. This\nresearch tries to find out whether the mobile health text information sent to\npeoples devices is correct as smartphones becoming the major information source\nfor people. The proposed method uses various mobile information retrieval and\ndata mining technologies including lexical analysis, stopword elimination,\nstemming, and decision trees to classify the mobile health text information to\none of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv)\ndisinformative, and (v) neutral. Experiment results show the accuracy of the\nproposed method is above the threshold value 50 percentage, but is not optimal.\nIt is because the problem, mobile text misinformation identification, is\nintrinsically difficult.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14637,regular,post_llm,2024,2,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'SimGrade: Using Code Similarity Measures for More Accurate Human Grading\n\n  While the use of programming problems on exams is a common form of summative\nassessment in CS courses, grading such exam problems can be a difficult and\ninconsistent process. Through an analysis of historical grading patterns we\nshow that inaccurate and inconsistent grading of free-response programming\nproblems is widespread in CS1 courses. These inconsistencies necessitate the\ndevelopment of methods to ensure more fairer and more accurate grading. In\nsubsequent analysis of this historical exam data we demonstrate that graders\nare able to more accurately assign a score to a student submission when they\nhave previously seen another submission similar to it. As a result, we\nhypothesize that we can improve exam grading accuracy by ensuring that each\nsubmission that a grader sees is similar to at least one submission they have\npreviously seen. We propose several algorithms for (1) assigning student\nsubmissions to graders, and (2) ordering submissions to maximize the\nprobability that a grader has previously seen a similar solution, leveraging\ndistributed representations of student code in order to measure similarity\nbetween submissions. Finally, we demonstrate in simulation that these\nalgorithms achieve higher grading accuracy than the current standard random\nassignment process used for grading.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.1083,review,post_llm,2024,2,"{'ai_likelihood': 4.635916815863716e-06, 'text': 'Towards a Consensual Definition for Smart Tourism and Smart Tourism Tools\n\nSmart tourism (ST) stems from the concepts of e-tourism - focused on the digitalization of processes within the tourism industry, and digital tourism - also considering the digitalization within the tourist experience. The earlier ST references found regard ST Destinations and emerge from the development of Smart Cities.\n  Our initial literature review on the ST concept and Smart Tourism Tools (STT) revealed significant research uncertainties: ST is poorly defined and frequently linked to the concept of Smart Cities; different authors have different, sometimes contradictory, views on the goals of ST; STT claims are often only based on technological aspects, and their ""smartness"" is difficult to evaluate; often the term ""Smart"" describes developments fueled by cutting-edge technologies, which lose that status after a few years.\n  This chapter is part of the ongoing initiative to build an online observatory that provides a comprehensive view of STTs\' offerings in Europe, known as the European STT Observatory. To achieve this, the observatory requires methodologies and tools to evaluate ""smartness"" based on a sound definition of ST and STT, while also being able to adapt to technological advancements. In this chapter, we present the results of a participatory approach where we invited ST experts from around the world to help us achieve this level of soundness. Our goal is to make a valuable contribution to the discussion on the definition of ST and STT.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.12667,regular,post_llm,2024,2,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'Remote Possibilities: Where there is a WIL, is there a Way? AI Education\n  for Remote Learners in a New Era of Work-Integrated-Learning\n\n  Increasing diversity in educational settings is challenging in part due to\nthe lack of access to resources for non-traditional learners in remote\ncommunities. Post-pandemic platforms designed specifically for remote and\nhybrid learning -- supporting team-based collaboration online -- are positioned\nto bridge this gap. Our work combines the use of these new platforms with\nco-creation and collaboration tools for AI assisted remote\nWork-Integrated-Learning (WIL) opportunities, including efforts in community\nand with the public library system. This paper outlines some of our experiences\nto date, and proposes methods to further integrate AI education into\ncommunity-driven applications for remote WIL.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14649,review,post_llm,2024,2,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""The economic value of scientific software\n\n  Academic institutions and their staff use, adapt and create software. We're\nthinking of business tools used to carry out their mission: teaching management\n(Moodle) or subject teaching support (such as Maxima for formal calculus), for\nexample. We're talking about software resulting from research work, designed by\na researcher or a team as part of a research project (funded by ANR, Europe,\netc. or not) or as a research service for a third party. These projects can\nlast for decades (such as the Coq program proof assistant project, or the GPAC\nmultimedia content distribution platform).We discuss why this software is\nproduced, with what resources, the interest that institutions derive from it,\nwhat we call the ''valorization'' of software resulting from scientific\nresearch. The latter is multifaceted, as are the missions of scientific\ninstitutions: social value (contribution to the world heritage of knowledge),\nfinancial value (contracts), economic value (business creation), scientific\nvalue (publication), image value (visibility of the institution among target\naudiences: students, researchers, companies, prescribers).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.08101,review,post_llm,2024,2,"{'ai_likelihood': 8.881092071533203e-05, 'text': ""Auditing Work: Exploring the New York City algorithmic bias audit regime\n\n  In July 2023, New York City (NYC) initiated the first algorithm auditing\nsystem for commercial machine-learning systems. Local Law 144 (LL 144) mandates\nNYC-based employers using automated employment decision-making tools (AEDTs) in\nhiring to undergo annual bias audits conducted by an independent auditor. This\npaper examines lessons from LL 144 for other national algorithm auditing\nattempts. Through qualitative interviews with 16 experts and practitioners\nwithin the regime, we find that LL 144 has not effectively established an\nauditing regime. The law fails to clearly define key aspects, such as AEDTs and\nindependent auditors, leading auditors, AEDT vendors, and companies using AEDTs\nto define the law's practical implementation in ways that failed to protect job\napplicants. Contributing factors include the law's flawed transparency-driven\ntheory of change, industry lobbying narrowing the definition of AEDTs,\npractical and cultural challenges faced by auditors in accessing data, and wide\ndisagreement over what constitutes a legitimate auditor, resulting in four\ndistinct 'auditor roles.' We conclude with four recommendations for\npolicymakers seeking to create similar bias auditing regimes, emphasizing\nclearer definitions, metrics, and increased accountability. By exploring LL 144\nthrough the lens of auditors, our paper advances the evidence base around audit\nas an accountability mechanism, providing guidance for policymakers seeking to\ncreate similar regimes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.02831,review,post_llm,2024,2,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'What do we teach to engineering students: embedded ethics, morality, and\n  politics\n\n  In the past few years, calls for integrating ethics modules in engineering\ncurricula have multiplied. Despite this positive trend, a number of issues with\nthese embedded programs remains. First, learning goals are underspecified. A\nsecond limitation is the conflation of different dimensions under the same\nbanner, in particular confusion between ethics curricula geared towards\naddressing the ethics of individual conduct and curricula geared towards\naddressing ethics at the societal level. In this article, we propose a\ntripartite framework to overcome these difficulties. Our framework analytically\ndecomposes an ethics module into three dimensions. First, there is the ethical\ndimension, which pertains to the learning goals. Second, there is the moral\ndimension, which addresses the moral relevance of engineers conduct. Finally,\nthere is the political dimension, which scales up issues of moral relevance at\nthe civic level. All in all, our framework has two advantages. First, it\nprovides analytic clarity, i.e. it enables course instructors to locate ethical\ndilemmas in either the moral or political realm and to make use of the tools\nand resources from moral and political philosophy. Second, it depicts a\ncomprehensive ethical training, which enables students to both reason about\nmoral issues in the abstract, and to socially contextualize potential\nsolutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05551,review,post_llm,2024,2,"{'ai_likelihood': 2.37425168355306e-05, 'text': 'A Bibliometric View of AI Ethics Development\n\n  Artificial Intelligence (AI) Ethics is a nascent yet critical research field.\nRecent developments in generative AI and foundational models necessitate a\nrenewed look at the problem of AI Ethics. In this study, we perform a\nbibliometric analysis of AI Ethics literature for the last 20 years based on\nkeyword search. Our study reveals a three-phase development in AI Ethics,\nnamely an incubation phase, making AI human-like machines phase, and making AI\nhuman-centric machines phase. We conjecture that the next phase of AI ethics is\nlikely to focus on making AI more machine-like as AI matches or surpasses\nhumans intellectually, a term we coin as ""machine-like human"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14653,review,post_llm,2024,2,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""Between Copyright and Computer Science: The Law and Ethics of Generative\n  AI\n\n  Copyright and computer science continue to intersect and clash, but they can\ncoexist. The advent of new technologies such as digitization of visual and\naural creations, sharing technologies, search engines, social media offerings,\nand more challenge copyright-based industries and reopen questions about the\nreach of copyright law. Breakthroughs in artificial intelligence research,\nespecially Large Language Models that leverage copyrighted material as part of\ntraining models, are the latest examples of the ongoing tension between\ncopyright and computer science. The exuberance, rush-to-market, and edge\nproblem cases created by a few misguided companies now raises challenges to\ncore legal doctrines and may shift Open Internet practices for the worse. That\nresult does not have to be, and should not be, the outcome.\n  This Article shows that, contrary to some scholars' views, fair use law does\nnot bless all ways that someone can gain access to copyrighted material even\nwhen the purpose is fair use. Nonetheless, the scientific need for more data to\nadvance AI research means access to large book corpora and the Open Internet is\nvital for the future of that research. The copyright industry claims, however,\nthat almost all uses of copyrighted material must be compensated, even for\nnon-expressive uses. The Article's solution accepts that both sides need to\nchange. It is one that forces the computer science world to discipline its\nbehaviors and, in some cases, pay for copyrighted material. It also requires\nthe copyright industry to abandon its belief that all uses must be compensated\nor restricted to uses sanctioned by the copyright industry. As part of this\nre-balancing, the Article addresses a problem that has grown out of this clash\nand under theorized.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.06775,review,post_llm,2024,2,"{'ai_likelihood': 2.043114768134223e-05, 'text': 'Twenty Constructionist Things to Do with Artificial Intelligence and\n  Machine Learning\n\n  In this paper, we build on the 1971 memo ""Twenty Things to Do With a\nComputer"" by Seymour Papert and Cynthia Solomon and propose twenty\nconstructionist things to do with artificial intelligence and machine learning.\nSeveral proposals build on ideas developed in the original memo while others\nare new and address topics in science, mathematics, and the arts. In reviewing\nthe big themes, we notice a renewed interest in children\'s engagement not just\nfor technical proficiency but also to cultivate a deeper understanding of their\nown cognitive processes. Furthermore, the ideas stress the importance of\ndesigning personally relevant AI/ML applications, moving beyond isolated models\nand off-the-shelf datasets disconnected from their interests. We also\nacknowledge the social aspects of data production involved in making AI/ML\napplications. Finally, we highlight the critical dimensions necessary to\naddress potential harmful algorithmic biases and consequences of AI/ML\napplications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.05118,regular,post_llm,2024,2,"{'ai_likelihood': 0.006646050347222223, 'text': 'Enhancing Accessibility of Rural Populations through Vehicle-based\n  Services\n\n  Improving access to essential public services like healthcare and education\nis crucial for human development, particularly in rural Sub-Saharan Africa.\nHowever, limited reliable transportation and sparse public facilities present\nsignificant challenges. Mobile facilities like mobile clinics offer a\ncost-effective solution to enhance spatial accessibility for the rural\npopulation.Public authorities require detailed demand distribution data to\nallocate resources efficiently and maximize the impact of mobile facilities.\nThis includes determining optimal vehicle service stop locations and estimating\noperational costs. Our integrated approach utilizes GIS data and an\naccessibility scaling factor to assess spatial accessibility for rural\npopulations. We tailor demand structures to account for remote and underserved\npopulations. To reduce average travel distances to 5 km, we apply a clustering\nalgorithm and optimize vehicle service stop locations. In a case study in rural\nEthiopia, focusing on four key public services, our analysis demonstrates that\nmobile facilities can address 39-62\\% of unmet demand, even in areas with\nwidely dispersed populations. This approach aids decision-makers, including\nfleet operators, policymakers, and public authorities in Sub-Saharan Africa,\nduring project evaluation and planning for mobile facilities. By enhancing\nspatial accessibility and optimizing resource allocation, our methodology\ncontributes to the effective delivery of essential public services to\nunderserved populations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.05249,regular,post_llm,2024,2,"{'ai_likelihood': 1.2020270029703777e-05, 'text': ""Digital Distractions from the Point of View of Higher Education Students\n\n  Technology enables a more sustainable and universally accessible educational\nmodel. However, technology has brought a paradox into students' lives: it helps\nthem engage in learning activities, but it is also a source of distraction.\nDuring the academic year 2021-2022, the authors conducted a study focusing on\nclassroom distractions. One of the objectives was to identify the main digital\ndistractions from the point of view of students. The study was carried out at\nan engineering school, where technology is fully integrated in the classroom\nand in the academic routines of teachers and students. Discussions and surveys,\ncomplemented by a statistical study based on bivariate correlations, were used\nwith participating students (n = 105). Students considered digital distractions\nto have a significant impact on their performance in lab sessions. This\nperformance was mainly self-assessed as improvable. Contrary to other\ncontemporary research, the results were not influenced by the year of study of\nthe subject, as the issue is important regardless of the students' backgrounds.\nProfessors should implement strategies to raise students' awareness of the\nsignificant negative effects of digital distractions on their performance, as\nwell as to develop students' self-control skills. This is of vital importance\nfor the use of technology to be sustainable in the long-term.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.00096,review,post_llm,2024,2,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Future of Pandemic Prevention and Response CCC Workshop Report\n\n  This report summarizes the discussions and conclusions of a 2-day\nmultidisciplinary workshop that brought together researchers and practitioners\nin healthcare, computer science, and social sciences to explore what lessons\nwere learned and what actions, primarily in research, could be taken. One\nconsistent observation was that there is significant merit in thinking not only\nabout pandemic situations, but also about peacetime advances, as many\nhealthcare networks and communities are now in a perpetual state of crisis.\nAttendees discussed how the COVID-19 pandemic amplified gaps in our health and\ncomputing systems, and how current and future computing technologies could fill\nthese gaps and improve the trajectory of the next pandemic.\n  Three major computing themes emerged from the workshop: models, data, and\ninfrastructure. Computational models are extremely important during pandemics,\nfrom anticipating supply needs of hospitals, to determining the care capacity\nof hospital and social service providers, to projecting the spread of the\ndisease. Accurate, reliable models can save lives, and inform community leaders\non policy decisions. Health system users require accurate, reliable data to\nachieve success when applying models. This requires data and measurement\nstandardization across health care organizations, modernizing the data\ninfrastructure, and methods for ensuring data remains private while shared for\nmodel development, validation, and application. Finally, many health care\nsystems lack the data, compute, and communication infrastructures required to\nbuild models on their data, use those models in ordinary operations, or even to\nreliably access their data. Robust and timely computing research has the\npotential to better support healthcare works to save lives in times of crisis\n(e.g., pandemics) and today during relative peacetime.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14646,regular,post_llm,2024,2,"{'ai_likelihood': 0.0015322367350260417, 'text': 'Digital Twin for Wind Energy: Latest updates from the NorthWind project\n\n  NorthWind, a collaborative research initiative supported by the Research\nCouncil of Norway, industry stakeholders, and research partners, aims to\nadvance cutting-edge research and innovation in wind energy. The core mission\nis to reduce wind power costs and foster sustainable growth, with a key focus\non the development of digital twins. A digital twin is a virtual representation\nof physical assets or processes that uses data and simulators to enable\nreal-time forecasting, optimization, monitoring, control and informed\ndecision-making. Recently, a hierarchical scale ranging from 0 to 5 (0 -\nStandalone, 1 - Descriptive, 2 - Diagnostic, 3 - Predictive, 4 - Prescriptive,\n5 - Autonomous has been introduced within the NorthWind project to assess the\ncapabilities of digital twins. This paper elaborates on our progress in\nconstructing digital twins for wind farms and their components across various\ncapability levels.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05545,regular,post_llm,2024,2,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Unveiling the influence of behavioural, built environment and\n  socio-economic features on the spatial and temporal variability of bus use\n  using explainable machine learning\n\n  Understanding the variability of people's travel patterns is key to transport\nplanning and policy-making. However, to what extent daily transit use displays\ngeographic and temporal variabilities, and what are the contributing factors\nhave not been fully addressed. Drawing on smart card data in Beijing, China,\nthis study seeks to address these deficits by adopting new indices to capture\nthe spatial and temporal variability of bus use during peak hours and\ninvestigate their associations with relevant contextual features. Using\nexplainable machine learning, our findings reveal non-linear interaction\nbetween spatial and temporal variability and trip frequency. Furthermore,\ngreater distance to the urban centres (>10 kilometres) is associated with\nincreased spatial variability of bus use, while greater separation of trip\norigins and destinations from the subcentres reduces both spatial and temporal\nvariability. Higher availability of bus routes is linked to higher spatial\nvariability but lower temporal variability. Meanwhile, both lower and higher\nroad density is associated with higher spatial variability of bus use\nespecially in morning times. These findings indicate that different built\nenvironment features moderate the flexibility of travel time and locations.\nImplications are derived to inform more responsive and reliable operation and\nplanning of transit systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05558,review,post_llm,2024,2,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Ethical and Privacy Considerations with Location Based Data Research\n\n  Networking research, especially focusing on human mobility, has evolved\nsignificantly in the last two decades and now relies on collection and\nanalyzing larger datasets. The increasing sizes of datasets are enabled by\nlarger automated efforts to collect data as well as by scalable methods to\nanalyze and unveil insights, which was not possible many years ago. However,\nthis fast expansion and innovation in human-centric research often comes at a\ncost of privacy or ethics. In this work, we review a vast corpus of scientific\nwork on human mobility and how ethics and privacy were considered. We reviewed\na total of 118 papers, including 149 datasets on individual mobility. We\ndemonstrate that these ever growing collections, while enabling new and\ninsightful studies, have not all consistently followed a pre-defined set of\nguidelines regarding acceptable practices in data governance as well as how\ntheir research was communicated. We conclude with a series of discussions on\nhow data, privacy and ethics could be dealt within our community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.17393,regular,post_llm,2024,2,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Designing Chatbots to Support Victims and Survivors of Domestic Abuse\n\n  Objective: Domestic abuse cases have risen significantly over the last four\nyears, in part due to the COVID-19 pandemic and the challenges for victims and\nsurvivors in accessing support. In this study, we investigate the role that\nchatbots - Artificial Intelligence (AI) and rule-based - may play in supporting\nvictims/survivors in situations such as these or where direct access to help is\nlimited. Methods: Interviews were conducted with experts working in domestic\nabuse support services and organizations (e.g., charities, law enforcement) and\nthe content of websites of related support-service providers was collected.\nThematic content analysis was then applied to assess and extract insights from\nthe interview data and the content on victim-support websites. We also reviewed\npertinent chatbot literature to reflect on studies that may inform design\nprinciples and interaction patterns for agents used to support\nvictims/survivors. Results: From our analysis, we outlined a set of design\nconsiderations/practices for chatbots that consider potential use cases and\ntarget groups, dialog structure, personality traits that might be useful for\nchatbots to possess, and finally, safety and privacy issues that should be\naddressed. Of particular note are situations where AI systems (e.g., ChatGPT,\nCoPilot, Gemini) are not recommended for use, the value of conveying emotional\nsupport, the importance of transparency, and the need for a safe and\nconfidential space. Conclusion: It is our hope that these\nconsiderations/practices will stimulate debate among chatbots and AI developers\nand service providers and - for situations where chatbots are deemed\nappropriate for use - inspire efficient use of chatbots in the support of\nsurvivors of domestic abuse.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.02838,review,post_llm,2024,2,"{'ai_likelihood': 1.0, 'text': ""HAPI-FHIR Server Implementation to Enhancing Interoperability among\n  Primary Care Health Information Systems in Sri Lanka: Review of the Technical\n  Use Case\n\n  This review underscores the vital role of interoperability in digital health,\nadvocating for a standardized framework. It focuses on implementing a Fast\nHealthcare Interoperability Resources (FHIR) server, addressing technical,\nsemantic, and process challenges. FHIR's adaptability ensures uniformity within\nPrimary Care Health Information Systems, fostering interoperability. Patient\ndata management complexities highlight the pivotal role of semantic\ninteroperability in seamless patient care. FHIR standards enhance these\nefforts, offering multiple pathways for data search. The ADR-guided FHIR server\nimplementation systematically addresses challenges related to patient identity,\nbiometrics, and data security. The detailed development phases emphasize\narchitecture, API integration, and security. The concluding stages incorporate\nforward-looking approaches, including HHIMS Synthetic Dataset testing.\nEnvisioning FHIR integration as transformative, it anticipates a responsive\nhealthcare environment aligned with the evolving digital health landscape,\nensuring comprehensive, dynamic, and interconnected systems for efficient data\nexchange and access.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.7756900787353516e-05, 'GPT4': 0.06304931640625, 'CLAUDE': 0.00010663270950317383, 'GOOGLE': 0.92333984375, 'OPENAI_O_SERIES': 0.01055908203125, 'DEEPSEEK': 0.001728057861328125, 'GROK': 3.7550926208496094e-06, 'NOVA': 0.0008158683776855469, 'OTHER': 0.00029778480529785156, 'HUMAN': 2.0325183868408203e-05}}"
2402.10751,regular,post_llm,2024,2,"{'ai_likelihood': 1.9636419084337023e-05, 'text': 'Another Body in the World: Flusserian Freedom in Mixed Reality\n\n  In Flusserian view of media history, humans often misperceive the world\nprojected by media to be the world itself, leading to a loss of freedom. This\npaper examines Flusserian Freedom in the context of Mixed Reality (MR) and\nexplores how humans can recognize the obscuration of the world within the media\n(i.e., MR) and understand their relationship. The authors investigate the\nconcept of playing against apparatus and deliberately alienating the perception\nof the projected world through an artwork titled ""Surrealism Me."" This artwork\nenables the user to have another body within MR through interactive and\nimmersive experiences based on the definition of Sense of Embodiment. The\npurpose of this work is to raise awareness of the domination of media and to\napproach Flusserian freedom within contemporary technical arrangements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.15402,regular,post_llm,2024,2,"{'ai_likelihood': 2.122587627834744e-05, 'text': ""This Class Isn't Designed For Me: Recognizing Ableist Trends In Design\n  Education, And Redesigning For An Inclusive And Sustainable Future\n\n  Traditional and currently-prevalent pedagogies of design perpetuate ableist\nand exclusionary notions of what it means to be a designer. In this paper, we\ntrace such historically exclusionary norms of design education, and highlight\nmodern-day instances from our own experiences as design educators in such\nepistemologies. Towards imagining a more inclusive and sustainable future of\ndesign education, we present three case studies from our own experience as\ndesign educators in redesigning course experiences for blind and low-vision\n(BLV), deaf and hard-of-hearing (DHH) students, and students with other\ndisabilities. In documenting successful and unsuccessful practices, we imagine\nwhat a pedagogy of care in design education would look like.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.05764,regular,post_llm,2024,2,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Datastringer: easy dataset monitoring for journalists\n\n  We created a software enabling journalists to define a set of criteria they\nwould like to see applied regularly to a constantly-updated dataset, sending\nthem an alert when these criteria are met, thus signaling them that there may\nbe a story to write. The main challenges were to keep the product scalable and\npowerful, while making sure that it could be used by journalists who would not\npossess all the technical knowledge to exploit it fully. In order to do so, we\nhad to choose Javascript as our main language, as well as designing the code in\nsuch a way that it would allow re-usability and further improvements. This\nproject is a proof of concept being tested in a real-life environment, and will\nbe developed towards more and more accessibility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.07778,review,post_llm,2024,2,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Algorithmic Fairness and Color-blind Racism: Navigating the Intersection\n\n  Our focus lies at the intersection between two broader research perspectives:\n(1) the scientific study of algorithms and (2) the scholarship on race and\nracism. Many streams of research related to algorithmic fairness have been born\nout of interest at this intersection. We think about this intersection as the\nproduct of work derived from both sides. From (1) algorithms to (2) racism, the\nstarting place might be an algorithmic question or method connected to a\nconceptualization of racism. On the other hand, from (2) racism to (1)\nalgorithms, the starting place could be recognizing a setting where a legacy of\nracism is known to persist and drawing connections between that legacy and the\nintroduction of algorithms into this setting. In either direction, meaningful\ndisconnection can occur when conducting research at the intersection of racism\nand algorithms. The present paper urges collective reflection on research\ndirections at this intersection. Despite being primarily motivated by instances\nof racial bias, research in algorithmic fairness remains mostly disconnected\nfrom scholarship on racism. In particular, there has not been an examination\nconnecting algorithmic fairness discussions directly to the ideology of\ncolor-blind racism; we aim to fill this gap. We begin with a review of an\nessential account of color-blind racism then we review racial discourse within\nalgorithmic fairness research and underline significant patterns, shifts and\ndisconnects. Ultimately, we argue that researchers can improve the navigation\nof the landscape at the intersection by recognizing ideological shifts as such\nand iteratively re-orienting towards maintaining meaningful connections across\ninterdisciplinary lines.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.07908,regular,post_llm,2024,2,"{'ai_likelihood': 0.00023947821723090278, 'text': 'Adaptation of the Multi-Concept Multivariate Elo Rating System to\n  Medical Students Training Data\n\n  Accurate estimation of question difficulty and prediction of student\nperformance play key roles in optimizing educational instruction and enhancing\nlearning outcomes within digital learning platforms. The Elo rating system is\nwidely recognized for its proficiency in predicting student performance by\nestimating both question difficulty and student ability while providing\ncomputational efficiency and real-time adaptivity. This paper presents an\nadaptation of a multi concept variant of the Elo rating system to the data\ncollected by a medical training platform, a platform characterized by a vast\nknowledge corpus, substantial inter-concept overlap, a huge question bank with\nsignificant sparsity in user question interactions, and a highly diverse user\npopulation, presenting unique challenges. Our study is driven by two primary\nobjectives: firstly, to comprehensively evaluate the Elo rating systems\ncapabilities on this real-life data, and secondly, to tackle the issue of\nimprecise early stage estimations when implementing the Elo rating system for\nonline assessments. Our findings suggest that the Elo rating system exhibits\ncomparable accuracy to the well-established logistic regression model in\npredicting final exam outcomes for users within our digital platform.\nFurthermore, results underscore that initializing Elo rating estimates with\nhistorical data remarkably reduces errors and enhances prediction accuracy,\nespecially during the initial phases of student interactions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.11477,regular,post_llm,2024,2,"{'ai_likelihood': 2.006689707438151e-05, 'text': 'Cross-Cultural Differences in Mental Health Expressions on Social Media\n\n  Culture moderates the way individuals perceive and express mental distress.\nCurrent understandings of mental health expressions on social media, however,\nare predominantly derived from WEIRD (Western, Educated, Industrialized, Rich,\nand Democratic) contexts. To address this gap, we examine mental health posts\non Reddit made by individuals geolocated in India, to identify variations in\nsocial media language specific to the Indian context compared to users from\nWestern nations. Our experiments reveal significant psychosocial variations in\nemotions and temporal orientation. This study demonstrates the potential of\nsocial media platforms for identifying cross-cultural differences in mental\nhealth expressions (e.g. seeking advice in India vs seeking support by Western\nusers). Significant linguistic variations in online mental health-related\nlanguage emphasize the importance of developing precision-targeted\ninterventions that are culturally appropriate.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.02676,regular,post_llm,2024,2,"{'ai_likelihood': 5.298190646701389e-07, 'text': ""The Gig's Up: How ChatGPT Stacks Up Against Quora on Gig Economy\n  Insights\n\n  Generative AI is changing the way in which humans seek to find answers to\nquestions in different fields including on the gig economy and labour markets,\nbut there is limited information available about closely ChatGPT simulated\noutput matches that obtainable from existing question and answer platforms.\nThis paper uses ChatGPT as a research assistant to explore how far ChatGPT can\nreplicate Quora question and answers, using data from the gig economy as an\nindicative case study. The results from content analysis suggest that Quora is\nlikely to be asked questions from users looking to make money and answers are\nlikely to include personal experiences and examples. ChatGPT simulated versions\nare less personal and more concept-based, including considerations on\nemployment implications and labour rights. It appears therefore that generative\nAI simulates only part of what a human would want in their answers relating to\nthe gig economy. The paper proposes that a similar comparative methodology\nwould also be useful across other research fields to help in establishing the\nbest real world uses of generative AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.19233,regular,post_llm,2024,2,"{'ai_likelihood': 0.1723904079861111, 'text': ""Shared lightweight autonomous vehicles for urban food deliveries: A\n  simulation study\n\n  In recent years, the rapid growth of on-demand deliveries, especially in food\ndeliveries, has spurred the exploration of innovative mobility solutions. In\nthis context, lightweight autonomous vehicles have emerged as a potential\nalternative. However, their fleet-level behavior remains largely unexplored. To\naddress this gap, we have developed an agent-based model and an environmental\nimpact study assessing the fleet performance of lightweight autonomous food\ndelivery vehicles. This model explores critical factors such as fleet sizing,\nservice level, operational strategies, and environmental impacts. We have\napplied this model to a case study in Cambridge, MA, USA, where results\nindicate that there could be environmental benefits in replacing traditional\ncar-based deliveries with shared lightweight autonomous vehicle fleets. Lastly,\nwe introduce an interactive platform that offers a user-friendly means of\ncomprehending the model's performance and potential trade-offs, which can help\ninform decision-makers in the evolving landscape of food delivery innovation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.05731,regular,post_llm,2024,2,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Face Recognition: to Deploy or not to Deploy? A Framework for Assessing\n  the Proportional Use of Face Recognition Systems in Real-World Scenarios\n\n  Face recognition (FR) has reached a high technical maturity. However, its use\nneeds to be carefully assessed from an ethical perspective, especially in\nsensitive scenarios. This is precisely the focus of this paper: the use of FR\nfor the identification of specific subjects in moderately to densely crowded\nspaces (e.g. public spaces, sports stadiums, train stations) and law\nenforcement scenarios. In particular, there is a need to consider the trade-off\nbetween the need to protect privacy and fundamental rights of citizens as well\nas their safety. Recent Artificial Intelligence (AI) policies, notably the\nEuropean AI Act, propose that such FR interventions should be proportionate and\ndeployed only when strictly necessary. Nevertheless, concrete guidelines on how\nto address the concept of proportional FR intervention are lacking to date.\nThis paper proposes a framework to contribute to assessing whether an FR\nintervention is proportionate or not for a given context of use in the above\nmentioned scenarios. It also identifies the main quantitative and qualitative\nvariables relevant to the FR intervention decision (e.g. number of people in\nthe scene, level of harm that the person(s) in search could perpetrate,\nconsequences to individual rights and freedoms) and propose a 2D graphical\nmodel making it possible to balance these variables in terms of ethical cost vs\nsecurity gain. Finally, different FR scenarios inspired by real-world\ndeployments validate the proposed model. The framework is conceived as a simple\nsupport tool for decision makers when confronted with the deployment of an FR\nsystem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01908,regular,post_llm,2024,2,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Large language models that replace human participants can harmfully\n  misportray and flatten identity groups\n\n  Large language models (LLMs) are increasing in capability and popularity,\npropelling their application in new domains -- including as replacements for\nhuman participants in computational social science, user testing, annotation\ntasks, and more. In many settings, researchers seek to distribute their surveys\nto a sample of participants that are representative of the underlying human\npopulation of interest. This means in order to be a suitable replacement, LLMs\nwill need to be able to capture the influence of positionality (i.e., relevance\nof social identities like gender and race). However, we show that there are two\ninherent limitations in the way current LLMs are trained that prevent this. We\nargue analytically for why LLMs are likely to both misportray and flatten the\nrepresentations of demographic groups, then empirically show this on 4 LLMs\nthrough a series of human studies with 3200 participants across 16 demographic\nidentities. We also discuss a third limitation about how identity prompts can\nessentialize identities. Throughout, we connect each limitation to a pernicious\nhistory of epistemic injustice against the value of lived experiences that\nexplains why replacement is harmful for marginalized demographic groups.\nOverall, we urge caution in use cases where LLMs are intended to replace human\nparticipants whose identities are relevant to the task at hand. At the same\ntime, in cases where the benefits of LLM replacement are determined to outweigh\nthe harms (e.g., the goal is to supplement rather than fully replace, engaging\nhuman participants may cause them harm), we provide inference-time techniques\nthat we empirically demonstrate do reduce, but do not remove, these harms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.15398,regular,post_llm,2024,2,"{'ai_likelihood': 3.973642985026042e-06, 'text': ""An International and Multidisciplinary Teaching Experience with Real\n  Industrial Team Project Development\n\n  This paper presents the design, objectives, experiences, and results of an\ninternational cooperation project funded by the European Commission in the\ncontext of the Erasmus Intensive Programme (IP, for short) designed to improve\nstudents' curricula. An IP is a short programme of study (minimum 2 weeks) that\nbrings together university students and staff from at least three countries in\norder to encourage efficient and multinational teaching of specialist topics,\nwhich might otherwise not be taught at all. This project lasted for 6 years,\ncovering two different editions, each one with three year duration. This\nproject lasted for 6 years, covering two different editions, each one with\nthree year duration. The first edition, named SAVRO (Simulation and Virtual\nReality in Robotics for Industrial Assembly Processes) was held in the period\n2008-2010, with the participation of three Universities, namely the Universitat\nPolitecnica de Valencia (Spain), acting as IP coordinator, the Technische\nUniversitat Kaiserslautern (Germany), and the Universita degli Studi di Salerno\n(Italy). The Universite de Reims Champagne-Ardenne (France) participated as a\nnew partner in the subsequent edition (2011-2013) of the IP, renamed as HUMAIN\n(Human-Machine Interaction). Both editions of the teaching project were\ncharacterized by the same objectives and organizational aspects, aiming to\nprovide educational initiatives based on active teaching through collaborative\nworks between international institutions, involving industrial partners too.\nThe aim of the paper is to illustrate the best practices that characterized the\norganization of our experience as well as to present some general\nrecommendations and suggestions on how to devise computing academic curricula.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.0083,review,post_llm,2024,2,"{'ai_likelihood': 2.3179584079318578e-05, 'text': 'Common errors in Generative AI systems used for knowledge extraction in\n  the climate action domain\n\n  Large Language Models (LLMs) and, more specifically, the Generative\nPre-Trained Transformers (GPT) can help stakeholders in climate action explore\ndigital knowledge bases and extract and utilize climate action knowledge in a\nsustainable manner. However, LLMs are ""probabilistic models of knowledge bases""\nthat excel at generating convincing texts but cannot be entirely relied upon\ndue to the probabilistic nature of the information produced. This brief report\nillustrates the problem space with examples of LLM responses to some of the\nquestions of relevance to climate action.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.1676,review,post_llm,2024,2,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Integrating Dark Pattern Taxonomies\n\n  The problem of ``Dark Patterns"" in user interface/user experience (UI/UX)\ndesign has proven a difficult issue to tackle. Malicious and explotitative\ndesign has expanded to multiple domains in the past 10 years and which has in\nturn led to multiple taxonomies attempting to describe them. While these\ntaxonomies holds their own merit, and constitute unique contributions to the\nliterature, their usefulness as separate entities is limited. We believe that\nin order to make meaningful progress in regulating malicious interface design,\nwe must first form a globally harmonized system (GHS) for the classification\nand labeling of Dark Patterns. By leaning on network analysis tools and\nmethods, this paper synthesizes existing taxonomies and their elements through\nas a directed graph. In doing so, the interconnectedness of Dark patterns can\nbe more clearly revealed via community (cluster) detection. Ultimately, we hope\nthat this work can serve as the inspiration for the creation of a glyph-based\nGHS for the classification of Dark Patterns.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.01813,regular,post_llm,2024,2,"{'ai_likelihood': 5.563100179036459e-06, 'text': 'An Educational Tool for Learning about Social Media Tracking, Profiling,\n  and Recommendation\n\n  This paper introduces an educational tool for classroom use, based on\nexplainable AI (XAI), designed to demystify key social media mechanisms -\ntracking, profiling, and content recommendation - for novice learners. The tool\nprovides a familiar, interactive interface that resonates with learners\'\nexperiences with popular social media platforms, while also offering the means\nto ""peek under the hood"" and exposing basic mechanisms of datafication.\nLearners gain first-hand experience of how even the slightest actions, such as\npausing to view content, are captured and recorded in their digital footprint,\nand further distilled into a personal profile. The tool uses real-time\nvisualizations and verbal explanations to create a sense of immediacy: each\ntime the user acts, the resulting changes in their engagement history and their\nprofile are displayed in a visually engaging and understandable manner. This\npaper discusses the potential of XAI and educational technology in transforming\ndata and digital literacy education and in fostering the growth of children\'s\nprivacy and security mindsets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.11823,regular,post_llm,2024,2,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Identifying Periods of Cyclical Stress in University Students Using\n  Wearables In-the-Wild\n\n  University students encounter various forms of stress during their academic\njourney, including cyclical stress associated with final exams. Supporting\ntheir well-being means helping them manage their stress levels. In this study,\nwe used a wearable health-tracking ring on a cohort of 103 Japanese university\nstudents for up to 28 months in the wild. The study aimed to investigate\nwhether group-wide biomarkers of stress can be identified in a sample having\nsimilar daily schedules and whether these occurrences can be pinpointed to\nspecific periods of the academic year. We found population-wide increased\nstress markers during exams, New Year's, and job hunting season, a Japanese job\nmarket peculiarity. Our results highlight the available potential of\nunobtrusive, in-situ detection of the current mental state of university\nstudent populations using off-the-shelf wearables from noisy data, with\nsignificant implications for the well-being of the users. Our approach and\nmethod of analysis allows for monitoring the student body's stress level\nwithout singling out individuals and therefore represents a privacy-preserving\nmethod. This way, new and sudden stress increases can be recognized, which can\nhelp identify the stressor and inform the design and introduction of counter\nmeasures.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.16766,review,post_llm,2024,2,"{'ai_likelihood': 5.596213870578343e-06, 'text': 'The Paradox of Industrial Involvement in Engineering Higher Education\n\n  This paper discusses the importance of reflective and socially conscious\neducation in engineering schools, particularly within the EE/CS sector. While\nmost engineering disciplines have historically aligned themselves with the\ndemands of the technology industry, the lack of critical examination of\nindustry practices and their impact on justice, equality, and sustainability is\nself-evident. Today, the for-profit engineering/technology companies, some of\nwhich are among the largest in the world, also shape the narrative of\nengineering education and research in universities. As engineering graduates\nform the largest cohorts within STEM disciplines in Western countries, they\nbecome future professionals who will work, lead, or even establish companies in\nthis industry. Unfortunately, the curriculum within engineering education often\nlacks a deep understanding of social realities, an essential component of a\ncomprehensive university education. Here we establish this unusual connection\nwith the industry that has driven engineering higher education for several\ndecades and its obvious negative impacts to society. We analyse this nexus and\nhighlight the need for engineering schools to hold a more critical viewpoint.\nGiven the wealth and power of modern technology companies, particularly in the\nICT domain, questioning their techno-solutionism narrative is essential within\nthe institutes of higher education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05539,regular,post_llm,2024,2,"{'ai_likelihood': 0.9658203125, 'text': 'Insights from the Field: A Comprehensive Analysis of Industrial\n  Accidents in Plants and Strategies for Enhanced Workplace Safety\n\n  The study delves into 425 industrial incidents documented on Kaggle [1], all\nof which occurred in 12 separate plants in the South American region. By\nmeticulously examining this extensive dataset, we aim to uncover valuable\ninsights into the occurrence of accidents, identify recurring trends, and\nilluminate underlying causes. The implications of this analysis extend beyond\nmere statistical observation, offering organizations an opportunity to enhance\nsafety and health management practices. Our findings underscore the importance\nof addressing specific areas for improvement, empowering organizations to\nfortify safety measures, mitigate risks, and cultivate a secure working\nenvironment. We advocate for strategically applying statistical analysis and\ndata visualization techniques to leverage this wealth of information\neffectively. This approach facilitates the extraction of meaningful insights\nand empowers decision-makers to implement targeted improvements, fostering a\npreventive mindset, and promoting a safety culture within organizations. This\nresearch is a crucial resource for organizations committed to transforming data\ninto actionable strategies for accident prevention and creating a safer\nworkplace.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001747608184814453, 'GPT4': 0.003971099853515625, 'CLAUDE': 7.37309455871582e-05, 'GOOGLE': 0.99072265625, 'OPENAI_O_SERIES': 0.00399017333984375, 'DEEPSEEK': 2.7477741241455078e-05, 'GROK': 1.1920928955078125e-07, 'NOVA': 4.291534423828125e-06, 'OTHER': 0.0002791881561279297, 'HUMAN': 0.0008339881896972656}}"
2402.0434,regular,post_llm,2024,2,"{'ai_likelihood': 8.27842288547092e-06, 'text': 'Skills in computational thinking of engineering students of the first\n  school year\n\n  In this world of the digital era, in which we are living, one of the\nfundamental competences that students must acquire is the competence in\nComputational Thinking (CT). Although there is no general consensus on a formal\ndefinition, there is a general understanding of it as a set of skills and\nattitudes necessary for the resolution, with or without a computer, of problems\nthat may arise in any area of life. Measuring and evaluating which of the CT\nskills students have acquired is fundamental, and for this purpose, previously\nvalidated measuring instruments must be used. In this study, a previously\nvalidated instrument is applied to know if the new students in the Engineering\nDegrees of the University of the Basque Country have the following skills in\nCT: Critical Thinking, Algorithmic Thinking, Problem Solving, Cooperativity and\nCreativity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05549,review,post_llm,2024,2,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'A Scheduling Perspective on Modular Educational Systems in Europe\n\n  In modular educational systems, students are allowed to choose a part of\ntheir own curriculum themselves. This is typically done in the final class\nlevels which lead to maturity for university access. The rationale behind\nletting students choose their courses themselves is to enhance\nself-responsibility, improve student motivation, and allow a focus on specific\nareas of interest. A central instrument for bringing these systems to fruition\nis the timetable. However, scheduling the timetable in such systems can be an\nextremely challenging and time-consuming task. In this study, we present a\nframework for classifying modular educational systems in Europe that reflects\ndifferent degrees of freedom regarding student choices, and explore the\nconsequences from the perspective of scheduling a timetable that satisfies all\nrequirements from the organizational and the pedagogical perspective. For this\npurpose, we conducted interviews in Austria, Germany, Finland, Switzerland, the\nNetherlands, and Luxembourg and apply the framework to these educational\nsystems, finding that among them the Finnish system shows the highest degree of\nmodularity. After analyzing the consequences of modularity from the scheduling\nperspective, we assess the necessity for automated scheduling methods, which\nare central for realizing the potential and many benefits of modular education\nin practice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.09028,regular,post_llm,2024,2,"{'ai_likelihood': 2.7484363979763457e-06, 'text': 'Understanding Stress, Burnout, and Behavioral Patterns in Medical\n  Residents Using Large-scale Longitudinal Wearable Recordings\n\n  Medical residency training is often associated with physically intense and\nemotionally demanding tasks, requiring them to engage in extended working hours\nproviding complex clinical care. Residents are hence susceptible to negative\npsychological effects, including stress and anxiety, that can lead to decreased\nwell-being, affecting them achieving desired training outcomes. Understanding\nthe daily behavioral patterns of residents can guide the researchers to\nidentify the source of stress in residency training, offering unique\nopportunities to improve residency programs. In this study, we investigate the\nworkplace behavioral patterns of 43 medical residents across different stages\nof their training, using longitudinal wearable recordings collected over a\n3-week rotation. Specifically, we explore their ambulatory patterns, the\ncomputer access, and the interactions with mentors of residents. Our analysis\nreveals that residents showed distinct working behaviors in walking movement\npatterns and computer usage compared to different years in the program.\nMoreover, we identify that interaction patterns with mentoring doctors indicate\nstress, burnout, and job satisfaction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.1818,regular,post_llm,2024,2,"{'ai_likelihood': 5.496872795952691e-06, 'text': ""Human Simulacra: Benchmarking the Personification of Large Language\n  Models\n\n  Large language models (LLMs) are recognized as systems that closely mimic\naspects of human intelligence. This capability has attracted attention from the\nsocial science community, who see the potential in leveraging LLMs to replace\nhuman participants in experiments, thereby reducing research costs and\ncomplexity. In this paper, we introduce a framework for large language models\npersonification, including a strategy for constructing virtual characters' life\nstories from the ground up, a Multi-Agent Cognitive Mechanism capable of\nsimulating human cognitive processes, and a psychology-guided evaluation method\nto assess human simulations from both self and observational perspectives.\nExperimental results demonstrate that our constructed simulacra can produce\npersonified responses that align with their target characters. Our work is a\npreliminary exploration which offers great potential in practical applications.\nAll the code and datasets will be released, with the hope of inspiring further\ninvestigations. Our code and dataset are available at:\nhttps://github.com/hasakiXie123/Human-Simulacra.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.11333,regular,post_llm,2024,2,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and\n  Prejudice\n\n  Shame and pride are social emotions expressed across cultures to motivate and\nregulate people's thoughts, feelings, and behaviors. In this paper, we\nintroduce the first cross-cultural dataset of over 10k shame/pride-related\nexpressions, with underlying social expectations from ~5.4K Bollywood and\nHollywood movies. We examine how and why shame and pride are expressed across\ncultures using a blend of psychology-informed language analysis combined with\nlarge language models. We find significant cross-cultural differences in shame\nand pride expression aligning with known cultural tendencies of the USA and\nIndia -- e.g., in Hollywood, shame-expressions predominantly discuss self\nwhereas shame is expressed toward others in Bollywood. Women are more\nsanctioned across cultures and for violating similar social expectations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.1262,regular,post_llm,2024,2,"{'ai_likelihood': 4.106097751193576e-06, 'text': 'Are Large Language Models (LLMs) Good Social Predictors?\n\n  The prediction has served as a crucial scientific method in modern social\nstudies. With the recent advancement of Large Language Models (LLMs), efforts\nhave been made to leverage LLMs to predict the human features in social life,\nsuch as presidential voting. These works suggest that LLMs are capable of\ngenerating human-like responses. However, we find that the promising\nperformance achieved by previous studies is because of the existence of input\nshortcut features to the response. In fact, by removing these shortcuts, the\nperformance is reduced dramatically. To further revisit the ability of LLMs, we\nintroduce a novel social prediction task, Soc-PRF Prediction, which utilizes\ngeneral features as input and simulates real-world social study settings. With\nthe comprehensive investigations on various LLMs, we reveal that LLMs cannot\nwork as expected on social prediction when given general input features without\nshortcuts. We further investigate possible reasons for this phenomenon that\nsuggest potential ways to enhance LLMs for social prediction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.0447,review,post_llm,2024,2,"{'ai_likelihood': 1.0, 'text': 'Six Fallacies in Substituting Large Language Models for Human Participants\n\nCan AI systems like large language models (LLMs) replace human participants in behavioral and psychological research? Here I critically evaluate the ""replacement"" perspective and identify six interpretive fallacies that undermine its validity. These fallacies are: (1) equating token prediction with human intelligence, (2) treating LLMs as the average human, (3) interpreting alignment as explanation, (4) anthropomorphizing AI systems, (5) essentializing identities, and (6) substituting model data for human evidence. Each fallacy represents a potential misunderstanding about what LLMs are and what they can tell us about human cognition. The analysis distinguishes levels of similarity between LLMs and humans, particularly functional equivalence (outputs) versus mechanistic equivalence (processes), while highlighting both technical limitations (addressable through engineering) and conceptual limitations (arising from fundamental differences between statistical and biological intelligence). For each fallacy, specific safeguards are provided to guide responsible research practices. Ultimately, the analysis supports conceptualizing LLMs as pragmatic simulation tools--useful for role-play, rapid hypothesis testing, and computational modeling (provided their outputs are validated against human data)--rather than as replacements for human participants. This framework enables researchers to leverage language models productively while respecting the fundamental differences between machine intelligence and human thought.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.344650268554688e-07, 'GPT4': 0.00017833709716796875, 'CLAUDE': 0.9970703125, 'GOOGLE': 3.0994415283203125e-06, 'OPENAI_O_SERIES': 8.940696716308594e-07, 'DEEPSEEK': 0.00252532958984375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 2.384185791015625e-07, 'HUMAN': 2.9861927032470703e-05}}"
2402.04911,review,post_llm,2024,2,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'What Values Do ImageNet-trained Classifiers Enact?\n\n  We identify ""values"" as actions that classifiers take that speak to open\nquestions of significant social concern. Investigating a classifier\'s values\nbuilds on studies of social bias that uncover how classifiers participate in\nsocial processes beyond their creators\' forethought. In our case, this\nparticipation involves what counts as nutritious, what it means to be modest,\nand more. Unlike AI social bias, however, a classifier\'s values are not\nnecessarily morally loathsome. Attending to image classifiers\' values can\nfacilitate public debate and introspection about the future of society. To\nsubstantiate these claims, we report on an extensive examination of both\nImageNet training/validation data and ImageNet-trained classifiers with custom\ntesting data. We identify perceptual decision boundaries in 118 categories that\naddress open questions in society, and through quantitative testing of rival\ndatasets we find that ImageNet-trained classifiers enact at least 7 values\nthrough their perceptual decisions. To contextualize these results, we develop\na conceptual framework that integrates values, social bias, and accuracy, and\nwe describe a rhetorical method for identifying how context affects the values\nthat a classifier enacts. We also discover that classifier performance does not\nstraightforwardly reflect the proportions of subgroups in a training set. Our\nfindings bring a rich sense of the social world to ML researchers that can be\napplied to other domains beyond computer vision.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14654,review,post_llm,2024,2,"{'ai_likelihood': 0.47661675347222227, 'text': 'ChatGPT in Veterinary Medicine: A Practical Guidance of Generative\n  Artificial Intelligence in Clinics, Education, and Research\n\n  ChatGPT, the most accessible generative artificial intelligence (AI) tool,\noffers considerable potential for veterinary medicine, yet a dedicated review\nof its specific applications is lacking. This review concisely synthesizes the\nlatest research and practical applications of ChatGPT within the clinical,\neducational, and research domains of veterinary medicine. It intends to provide\nspecific guidance and actionable examples of how generative AI can be directly\nutilized by veterinary professionals without a programming background. For\npractitioners, ChatGPT can extract patient data, generate progress notes, and\npotentially assist in diagnosing complex cases. Veterinary educators can create\ncustom GPTs for student support, while students can utilize ChatGPT for exam\npreparation. ChatGPT can aid in academic writing tasks in research, but\nveterinary publishers have set specific requirements for authors to follow.\nDespite its transformative potential, careful use is essential to avoid\npitfalls like hallucination. This review addresses ethical considerations,\nprovides learning resources, and offers tangible examples to guide responsible\nimplementation. Carefully selected, up-to-date links to platforms that host\nlarge language models are provided for advanced readers with programming\ncapability. A table of key takeaways was provided to summarize this review. By\nhighlighting potential benefits and limitations, this review equips\nveterinarians, educators, and researchers to harness the power of ChatGPT\neffectively.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.00081,review,post_llm,2024,2,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'The Constitutions of Web3\n\n  The governance of online communities has been a critical issue since the\nfirst USENET groups, and a number of serious constitutions -- declarations of\ngoals, values, and rights -- have emerged since the mid-1990s. More recently,\ndecentralized autonomous organizations (DAOs) have begun to publish their own\nconstitutions, manifestos, and other governance documents. There are two unique\naspects to these documents: they (1) often govern significantly more resources\nthan previously-observed online communities, and (2) are used in conjunction\nwith smart contracts that can secure certain community rights and processes\nthrough code. In this article, we analyze 25 DAO constitutions, observe a\nnumber of common patterns, and provide a template and a set of recommendations\nto support the crafting and dissemination of future DAO constitutions. We\nconclude with a report on how our template and recommendations were then used\nwithin the actual constitutional drafting process of a major blockchain.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.0491,review,post_llm,2024,2,"{'ai_likelihood': 3.841188218858507e-06, 'text': 'Exploring responsible applications of Synthetic Data to advance Online\n  Safety Research and Development\n\n  The use of synthetic data provides an opportunity to accelerate online safety\nresearch and development efforts while showing potential for bias mitigation,\nfacilitating data storage and sharing, preserving privacy and reducing exposure\nto harmful content. However, the responsible use of synthetic data requires\ncaution regarding anticipated risks and challenges. This short report explores\nthe potential applications of synthetic data to the domain of online safety,\nand addresses the ethical challenges that effective use of the technology may\npresent.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2402.18835,review,post_llm,2024,2,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Envisioning the Applications and Implications of Generative AI for News\n  Media\n\n  This article considers the increasing use of algorithmic decision-support\nsystems and synthetic media in the newsroom, and explores how generative models\ncan help reporters and editors across a range of tasks from the conception of a\nnews story to its distribution. Specifically, we draw from a taxonomy of tasks\nassociated with news production, and discuss where generative models could\nappropriately support reporters, the journalistic and ethical values that must\nbe preserved within these interactions, and the resulting implications for\ndesign contributions in this area in the future. Our essay is relevant to\npractitioners and researchers as they consider using generative AI systems to\nsupport different tasks and workflows.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.12263,review,post_llm,2024,3,"{'ai_likelihood': 3.4769376118977866e-06, 'text': ""Fostering Inclusion: A Regional Initiative Uniting Communities to\n  Co-Design Assistive Technologies\n\n  People with disabilities often face discrimination and lack of access in all\nareas of society. While improving the affordability and appropriateness of\nassistive technologies can pave the way for easier participation and\nindependence, awareness and acceptance of disability as part of society are\ninevitable. The presented regional initiative strives to tackle these problems\nby bringing together people with disabilities, students, researchers, and\nassociations. During different lecture formats at the university, students\nco-design assistive technologies with people with disabilities. After one year\nin practice, we reflect on the initiative and its impact on assistive\ntechnology development and mitigation of ableism. We conducted and analyzed\nthirteen semi-structured interviews with participants and other involved\nstakeholders. Not all co-design projects were finished within the time of a\nlecture. Participants nevertheless appreciated the co-design approach and steps\nin the right direction as projects are continued in upcoming semesters.\nInterviewees highlighted the initiative's importance in raising awareness and\nbroadening knowledge regarding disability and internalized ableist assumptions\nfor those participating. We conclude that collaboration, continuity, and public\noutreach are most important to work towards tangible assistive technologies,\nbridging accessibility gaps, and fostering a more inclusive society.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.15759,regular,post_llm,2024,3,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Deep Learning Approach to Forecasting COVID-19 Cases in Residential\n  Buildings of Hong Kong Public Housing Estates: The Role of Environment and\n  Sociodemographics\n\n  Introduction: The current study investigates the complex association between\nCOVID-19 and the studied districts' socioecology (e.g. internal and external\nbuilt environment, sociodemographic profiles, etc.) to quantify their\ncontributions to the early outbreaks and epidemic resurgence of COVID-19.\nMethods: We aligned the analytic model's architecture with the hierarchical\nstructure of the resident's socioecology using a multi-headed hierarchical\nconvolutional neural network to structure the vast array of hierarchically\nrelated predictive features representing buildings' internal and external built\nenvironments and residents' sociodemographic profiles as model input. COVID-19\ncases accumulated in buildings across three adjacent districts in HK, both\nbefore and during HK's epidemic resurgence, were modeled. A forward-chaining\nvalidation was performed to examine the model's performance in forecasting\nCOVID-19 cases over the 3-, 7-, and 14-day horizons during the two months\nsubsequent to when the model for COVID-19 resurgence was built to align with\nthe forecasting needs in an evolving pandemic. Results: Different sets of\nfactors were found to be linked to the earlier waves of COVID-19 outbreaks\ncompared to the epidemic resurgence of the pandemic. Sociodemographic factors\nsuch as work hours, monthly household income, employment types, and the number\nof non-working adults or children in household populations were of high\nimportance to the studied buildings' COVID-19 case counts during the early\nwaves of COVID-19. Factors constituting one's internal built environment, such\nas the number of distinct households in the buildings, the number of distinct\nhouseholds per floor, and the number of floors, corridors, and lifts, had the\ngreatest unique contributions to the building-level COVID-19 case counts during\nepidemic resurgence.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14677,review,post_llm,2024,3,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""An Elemental Ethics for Artificial Intelligence: Water as Resistance\n  Within AI's Value Chain\n\n  Research and activism have increasingly denounced the problematic\nenvironmental record of the infrastructure and value chain underpinning\nArtificial Intelligence (AI). Water-intensive data centres, polluting mineral\nextraction and e-waste dumping are incontrovertibly part of AI's footprint. In\nthis article, I turn to areas affected by AI-fuelled environmental harm and\nidentify an ethics of resistance emerging from local activists, which I term\n'elemental ethics'. Elemental ethics interrogates the AI value chain's\nproblematic relationship with the elements that make up the world, critiques\nthe undermining of local and ancestral approaches to nature and reveals the\nvital and quotidian harms engendered by so-called intelligent systems. While\nthis ethics is emerging from grassroots and Indigenous groups, it echoes recent\ncalls from environmental philosophy to reconnect with the environment via the\nelements. In empirical terms, this article looks at groups in Chile resisting a\nGoogle data centre project in Santiago and lithium extraction (used for\nrechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert.\nAs I show, elemental ethics can complement top-down, utilitarian and\nquantitative approaches to AI ethics and sustainable AI as well as interrogate\nwhose lived experience and well-being counts in debates on AI extinction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.0707,review,post_llm,2024,3,"{'ai_likelihood': 3.63257196214464e-05, 'text': 'Retail Central Bank Digital Currency: Motivations, Opportunities, and\n  Mistakes\n\n  Nations around the world are conducting research into the design of central\nbank digital currency (CBDC), a new, digital form of money that would be issued\nby central banks alongside cash and central bank reserves. Retail CBDC would be\nused by individuals and businesses as form of money suitable for routine\ncommerce. An important motivating factor in the development of retail CBDC is\nthe decline of the popularity of central bank money for retail purchases and\nthe increasing use of digital money created by the private sector for such\npurposes. The debate about how retail CBDC would be designed and implemented\nhas led to many proposals, which have sparked considerable debate about\nbusiness models, regulatory frameworks, and the socio-technical role of money\nin general. Here, we present a critical analysis of the existing proposals. We\nexamine their motivations and themes, as well as their underlying assumptions.\nWe also offer a reflection of the opportunity that retail CBDC represents and\nsuggest a way forward in furtherance of the public interest.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14708,review,post_llm,2024,3,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Visualizing Progress in Broadening Participation in Computing: The Value\n  of Context\n\n  Concerns about representation in computing within the U.S. have driven\nnumerous activities to broaden participation. Assessment of the impact of these\nefforts and, indeed, a clear assessment of the actual ""problem"" being addressed\nare limited by the nature of the most common data analysis which looks at the\nrepresentation of each population as a percentage of the number of students\ngraduating with a degree in computing. This use of a single metric cannot\nadequately assess the impact of broadening participation efforts. First, this\napproach fails to account for changing demographics of the undergraduate\npopulation in terms of overall numbers and relative proportion of the Federally\ndesignated gender, race, and ethnicity groupings. A second issue is that the\nmajority of literature on broadening participation in computing (BPC) reports\ndata on gender or on race/ethnicity, omitting data on students\' intersectional\nidentities. This leads to an incorrect understanding of both the data and the\nchallenges we face as a field. In this paper we present several different\napproaches to tracking the impact of BPC efforts. We make three\nrecommendations: 1) cohort-based analysis should be used to accurately show\nstudent engagement in computing; 2) the field as a whole needs to adopt the\nnorm of always reporting intersectional data; 3) university demographic context\nmatters when looking at how well a CS department is doing to broaden\nparticipation in computing, including longitudinal analysis of university\ndemographic shifts that impact the local demographics of computing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.06076,regular,post_llm,2024,3,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Sistemas de informaci\\'on de salud en contextos extremos: Uso de tel\\'efonos m\\'oviles para combatir el sida en Uganda\n\nThe HIV/AIDS pandemic is a global issue that has unequally affected several countries. Due to the complexity of this condition and the human drama it represents to those most affected by it, several fields have contributed to solving or at least alleviating this situation, and the information systems (IS) field has not been absent from these efforts. With the importance of antiretroviral therapy (ART) as a starting point, several initiatives in the IS field have focused on ways to improve the adherence and effectiveness of this therapy: mobile phone reminders (for pill intake and appointments), and mobile interfaces between patients and health workers are popular contributions. However, many of these solutions have been difficult to implement or deploy in some countries in the Global South, which are among the most affected by this pandemic. This paper presents one such case. Using a case-study approach with an extreme-case selection technique, the paper studies an m-health system for HIV patients in the Kalangala region of Uganda. Using Heeks' design-reality gap model for data analysis, the paper shows that the rich interaction between social context and technology should be considered a central concern when designing or deploying such systems."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.1467,regular,post_llm,2024,3,"{'ai_likelihood': 5.265076955159506e-06, 'text': 'Electric Vehicle Enquiry (EVE) Pilot\n\n  This data paper presents the dataset from a study on the use of electric\nvehicles (EVs). This dataset covers the first dataset collected in this study:\nthe usage data from a Renault Zoe over 3 years. The process of collection of\nthe dataset, its treatment, and descriptions of all the included variables are\ndetailed. The collection of this dataset represents an iteration of\nparticipative research in the personal mobility domain as the dataset was\ncollected with low-cost commercially available equipment and open-source\nsoftware. Some of the challenges of providing the dataset are also discussed:\nthe most pertinent being the intermittent nature of data collection as an\nandroid phone and OBDII adapter were used to collect the dataset.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.01572,regular,post_llm,2024,3,"{'ai_likelihood': 0.00048769844902886287, 'text': ""Deeply Embedded Wages: Navigating Digital Payments in Data Work\n\n  Many of the world's workers rely on digital platforms for their income. In\nVenezuela, a nation grappling with extreme inflation and where most of the\nworkforce is self-employed, data production platforms for machine learning have\nemerged as a viable opportunity for many to earn a flexible income in US\ndollars. Platform workers are deeply interconnected within a vast network of\nfirms and entities that act as intermediaries for wage payments in digital\ncurrencies and its subsequent conversion to the national currency, the bolivar.\nPast research on embeddedness has noted that being intertwined in multi-tiered\nsocioeconomic networks of companies and individuals can offer significant\nrewards to social participants, while also connoting a particular set of\nlimitations. This paper furnishes qualitative evidence regarding how this deep\nembeddedness impacts platform workers in Venezuela. Given the backdrop of a\nnational crisis and rampant hyperinflation, the perks of receiving wages\nthrough various financial platforms include access to a more stable currency\nand the ability to save and invest outside the national financial system.\nHowever, relying on numerous digital and local intermediaries often diminishes\nincome due to transaction fees. Moreover, this introduces heightened financial\nrisks, particularly due to the unpredictable nature of cryptocurrencies as an\ninvestment. The over-reliance on external financial platforms erodes worker\nautonomy through power dynamics that lean in favor of the platforms that set\nthe transaction rules and prices. These findings present a multifaceted\nperspective on deep embeddedness in platform labor, highlighting how the\nrewards of financial intermediation often come at a substantial cost for the\nworkers in unstable situations, who are saddled with escalating financial\nrisks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.17405,review,post_llm,2024,3,"{'ai_likelihood': 0.01792060004340278, 'text': 'The recessionary pressures of generative AI: A threat to wellbeing\n\n  Generative Artificial Intelligence (AI) stands as a transformative force that\npresents a paradox; it offers unprecedented opportunities for productivity\ngrowth while potentially posing significant threats to economic stability and\nsocietal wellbeing. Many consider generative AI as akin to previous\ntechnological advancements, using historical precedent to argue that fears of\nwidespread job displacement are unfounded, while others contend that generative\nAI`s unique capacity to undertake non-routine cognitive tasks sets it apart\nfrom other forms of automation capital and presents a threat to the quality and\navailability of work that underpin stable societies. This paper explores the\nconditions under which both may be true. We posit the existence of an\nAI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of\nrecessionary pressures could be triggered, exacerbating social disparities,\nreducing social cohesion, heightening tensions, and requiring sustained\ngovernment intervention to maintain stability. To prevent this, the paper\nunderscores the urgent need for proactive policy responses, making\nrecommendations to reduce these risks through robust regulatory frameworks and\na new social contract characterised by progressive social and economic\npolicies. This approach aims to ensure a sustainable, inclusive, and resilient\neconomic future where human contribution to the economy is retained and\nintegrated with generative AI to enhance the Mental Wealth of nations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.08397,regular,post_llm,2024,3,"{'ai_likelihood': 1.3642840915256076e-05, 'text': ""Interactive environments for training children's curiosity through the\n  practice of metacognitive skills: a pilot study\n\n  Curiosity-driven learning has shown significant positive effects on students'\nlearning experiences and outcomes. But despite this importance, reports show\nthat children lack this skill, especially in formal educational settings. To\naddress this challenge, we propose an 8-session workshop that aims to enhance\nchildren's curiosity through training a set of specific metacognitive skills we\nhypothesize are involved in its process. Our workshop contains animated videos\npresenting declarative knowledge about curiosity and the said metacognitive\nskills as well as practice sessions to apply these skills during a\nreading-comprehension task, using a web platform designed for this study (e.g.\nexpressing uncertainty, formulating questions, etc). We conduct a pilot study\nwith 15 primary school students, aged between 8 and 10. Our first results show\na positive impact on children's metacognitive efficiency and their ability to\nexpress their curiosity through question-asking behaviors.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.11153,review,post_llm,2024,3,"{'ai_likelihood': 0.0015121036105685765, 'text': ""Technological Utilization in Remote Healthcare: Factors Influencing\n  Healthcare Professionals' Adoption and Use\n\n  With the increasing importance of remote healthcare monitoring in the\nhealthcare industry, it is essential to evaluate the usefulness and the ease of\nuse the technology brings in remote healthcare. With this research, we want to\nunderstand the perspective of healthcare professionals, their competencies in\nusing technology related to remote healthcare monitoring, and their trust and\nadoption of technology. In addition to these core factors, we introduce\nsustainability as a pivotal dimension in the Technology Acceptance Model,\nreflecting its importance in motivating and determining the use of remote\nhealthcare technology. The results suggest that the participants have a\npositive view towards the use of remote monitoring devices for telemedicine,\nbut have some concerns about security and privacy, and believe that network\ncoverage needs to improve in remote areas. However, advances in technology and\na focus on sustainable development can facilitate more effective and widespread\nadoption of remote monitoring devices in telemedicine.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14669,regular,post_llm,2024,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Large-Scale Evaluation of Mobility, Technology and Demand Scenarios in\n  the Chicago Region Using POLARIS\n\n  Rapid technological progress and innovation in the areas of vehicle\nconnectivity, automation and electrification, new modes of shared and\nalternative mobility, and advanced transportation system demand and supply\nmanagement strategies, have motivated numerous questions and studies regarding\nthe potential impact on key performance and equity metrics. Several of these\nareas of development may or may not have a synergistic outcome on the overall\nbenefits such as reduction in congestion and travel times. In this study, the\nuse of an end-to-end modeling workflow centered around an activity-based\nagent-based travel demand forecasting tool called POLARIS is explored to\nprovide insights on the effects of several different technology deployments and\noperational policies in combination for the Chicago region. The objective of\nthe research was to explore the direct impacts and observe any interactions\nbetween the various policy and technology scenarios to help better characterize\nand evaluate their potential future benefits. We analyze system outcome metrics\non mobility, energy and emissions, equity and environmental justice and overall\nefficiency for a scenario design of experiments that looks at combinations of\nsupply interventions (congestion pricing, transit expansion, tnc policy,\noff-hours freight policy, connected signal optimization) for different\npotential demand scenarios defined by e-commerce and on-demand delivery\nengagement, and market penetration of electric vehicles. We found different\ncombinations of strategies that can reduce overall travel times up to 7% and\nincrease system efficiency up to 53% depending on how various metrics are\nprioritized. The results demonstrate the importance of considering various\ninterventions jointly.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.09216,review,post_llm,2024,3,"{'ai_likelihood': 7.980399661593967e-06, 'text': ""Unlocking the Potential of Open Government Data: Exploring the\n  Strategic, Technical, and Application Perspectives of High-Value Datasets\n  Opening in Taiwan\n\n  Today, data has an unprecedented value as it forms the basis for data-driven\ndecision-making, including serving as an input for AI models, where the latter\nis highly dependent on the availability of the data. However, availability of\ndata in an open data format creates a little added value, where the value of\nthese data, i.e., their relevance to the real needs of the end user, is key.\nThis is where the concept of high-value dataset (HVD) comes into play, which\nhas become popular in recent years. Defining and opening HVD is an ongoing\nprocess consisting of a set of interrelated steps, the implementation of which\nmay vary from one country or region to another. Therefore, there has recently\nbeen a call to conduct research in a country or region setting considered to be\nof greatest national value. So far, only a few studies have been conducted at\nthe regional or national level, most of which consider only one step of the\nprocess, such as identifying HVD or measuring their impact. With this study, we\nanswer this call and examine the national case of Taiwan by exploring the\nentire lifecycle of HVD opening. The aim of the paper is to understand and\nevaluate the lifecycle of high-value dataset publishing in one of the world's\nleading producers of information and communication technology (ICT) products -\nTaiwan. To do this, we conduct a qualitative study with exploratory interviews\nwith representatives from government agencies in Taiwan responsible for HVD\nopening, exploring HVD opening lifecycle. As such, we examine (1) strategic\naspects related to the HVD determination process, (2) technical aspects, and\n(3) application aspects.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05205,review,post_llm,2024,3,"{'ai_likelihood': 0.8291015625, 'text': ""Interoperability of the Metaverse: A Digital Ecosystem Perspective\n  Review\n\n  The Metaverse is at the vanguard of the impending digital revolution, with\nthe potential to significantly transform industries and lifestyles. However, in\n2023, skepticism surfaced within industrial and academic spheres, raising\nconcerns that excitement may outpace actual technological progress.\nInteroperability, recognized as a major barrier to the Metaverse's full\npotential, is central to this debate. CoinMarketCap's report in February 2023\nindicated that of over 240 metaverse initiatives, most existed in isolation,\nunderscoring the interoperability challenge. Despite consensus on its critical\nrole, there is a research gap in exploring the impact on the Metaverse,\nsignificance, and developmental extent. Our study bridges this gap via a\nsystematic literature review and content analysis of the Web of Science (WoS)\nand Scopus databases, yielding 74 publications after a rigorous selection\nprocess. Interoperability, difficult to define due to varied contexts and lack\nof standardization, is central to the Metaverse, often seen as a digital\necosystem. Urs Gasser's framework, outlining technological, data, human, and\ninstitutional dimensions, systematically addresses interoperability\ncomplexities. Incorporating this framework, we dissect the literature for a\ncomprehensive Metaverse interoperability overview. Our study seeks to establish\nbenchmarks for future inquiries, navigating the complex field of Metaverse\ninteroperability studies and contributing to academic advancement.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0006341934204101562, 'GPT4': 0.4892578125, 'CLAUDE': 0.00017821788787841797, 'GOOGLE': 0.467041015625, 'OPENAI_O_SERIES': 0.0099639892578125, 'DEEPSEEK': 0.0012807846069335938, 'GROK': 3.2186508178710938e-06, 'NOVA': 0.00011962652206420898, 'OTHER': 0.002826690673828125, 'HUMAN': 0.028656005859375}}"
2403.08501,review,post_llm,2024,3,"{'ai_likelihood': 3.675619761149089e-06, 'text': 'Governing Through the Cloud: The Intermediary Role of Compute Providers\n  in AI Regulation\n\n  As jurisdictions around the world take their first steps toward regulating\nthe most powerful AI systems, such as the EU AI Act and the US Executive Order\n14110, there is a growing need for effective enforcement mechanisms that can\nverify compliance and respond to violations. We argue that compute providers\nshould have legal obligations and ethical responsibilities associated with AI\ndevelopment and deployment, both to provide secure infrastructure and to serve\nas intermediaries for AI regulation. Compute providers can play an essential\nrole in a regulatory ecosystem via four key capacities: as securers,\nsafeguarding AI systems and critical infrastructure; as record keepers,\nenhancing visibility for policymakers; as verifiers of customer activities,\nensuring oversight; and as enforcers, taking actions against rule violations.\nWe analyze the technical feasibility of performing these functions in a\ntargeted and privacy-conscious manner and present a range of technical\ninstruments. In particular, we describe how non-confidential information, to\nwhich compute providers largely already have access, can provide two key\ngovernance-relevant properties of a computational workload: its type-e.g.,\nlarge-scale training or inference-and the amount of compute it has consumed.\nUsing AI Executive Order 14110 as a case study, we outline how the US is\nbeginning to implement record keeping requirements for compute providers. We\nalso explore how verification and enforcement roles could be added to establish\na comprehensive AI compute oversight scheme. We argue that internationalization\nwill be key to effective implementation, and highlight the critical challenge\nof balancing confidentiality and privacy with risk mitigation as the role of\ncompute providers in AI regulation expands.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.08624,review,post_llm,2024,3,"{'ai_likelihood': 0.99755859375, 'text': 'Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding\n  the Development and Assessment of AI Systems\n\n  As artificial intelligence continues its unprecedented global expansion,\naccompanied by a proliferation of benefits, an increasing apprehension about\nthe privacy and security implications of AI-enabled systems emerges. The\npivotal question of effectively controlling AI development at both\njurisdictional and organizational levels has become a prominent theme in\ncontemporary discourse. While the European Parliament and Council have taken a\ndecisive step by reaching a political agreement on the EU AI Act, the first\ncomprehensive AI law, organizations still find it challenging to adapt to the\nfast-evolving AI landscape, lacking a universal tool for evaluating the privacy\nand security dimensions of their AI models and systems. In response to this\ncritical challenge, this study conducts a systematic literature review spanning\nthe years 2020 to 2023, with a primary focus on establishing a unified\ndefinition of key concepts in AI Ethics, particularly emphasizing the domains\nof privacy and security. Through the synthesis of knowledge extracted from the\nSLR, this study presents a conceptual framework tailored for privacy- and\nsecurity-aware AI systems. This framework is designed to assist diverse\nstakeholders, including organizations, academic institutions, and governmental\nbodies, in both the development and critical assessment of AI systems.\nEssentially, the proposed framework serves as a guide for ethical\ndecision-making, fostering an environment wherein AI is developed and utilized\nwith a strong commitment to ethical principles. In addition, the study unravels\nthe key issues and challenges surrounding the privacy and security dimensions,\ndelineating promising avenues for future research, thereby contributing to the\nongoing dialogue on the globalization and democratization of AI ethics.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.337860107421875e-05, 'GPT4': 0.99609375, 'CLAUDE': 1.6689300537109375e-06, 'GOOGLE': 0.0036144256591796875, 'OPENAI_O_SERIES': 3.74913215637207e-05, 'DEEPSEEK': 2.86102294921875e-06, 'GROK': 2.980232238769531e-07, 'NOVA': 1.7881393432617188e-07, 'OTHER': 9.000301361083984e-06, 'HUMAN': 4.506111145019531e-05}}"
2403.02768,review,post_llm,2024,3,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'An Empirical Analysis on the Use and Reporting of National Security\n  Letters\n\n  Government investigatory and surveillance powers are important tools for\nexamining crime and protecting public safety. However, since these tools must\nbe employed in secret, it can be challenging to identify abuses or changes in\nuse that could be of significant public interest. In this paper, we evaluate\nthis phenomenon in the context of National Security Letters (NSLs). NSLs are a\nform of legal process that empowers parts of the United States federal\ngovernment to request certain pieces of information for national security\npurposes. After initial concerns about the lack of public oversight, Congress\nworked to increase transparency by mandating government agencies to publish\naggregated statistics on the NSL usage and by allowing the private sector to\nreport information on NSLs in transparency reports. The implicit goal is that\nthese transparency mechanisms should deter large-scale abuse by making it\nvisible. We evaluate how well these mechanisms work by carefully analyzing the\nfull range of publicly available data related to NSL use. Our findings suggest\nthat they may not lead to the desired public scrutiny as we find published\ninformation requires significant manual effort to collect and parse data due to\nthe lack of structure and context. Moreover, we discovered mistakes\n(subsequently fixed after our reporting to the ODNI), which suggests a lack of\nactive auditing. Taken together, our case study of NSLs provides insights and\nsuggestions for the successful construction of transparency mechanisms that\nenable effective public auditing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.02931,regular,post_llm,2024,3,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Improving the quality of individual-level online information tracking:\n  challenges of existing approaches and introduction of a new content- and\n  long-tail sensitive academic solution\n\n  This article evaluates the quality of data collection in individual-level\ndesktop information tracking used in the social sciences and shows that the\nexisting approaches face sampling issues, validity issues due to the lack of\ncontent-level data and their disregard of the variety of devices and long-tail\nconsumption patterns as well as transparency and privacy issues. To overcome\nsome of these problems, the article introduces a new academic tracking\nsolution, WebTrack, an open source tracking tool maintained by a major European\nresearch institution. The design logic, the interfaces and the backend\nrequirements for WebTrack, followed by a detailed examination of strengths and\nweaknesses of the tool, are discussed. Finally, using data from 1185\nparticipants, the article empirically illustrates how an improvement in the\ndata collection through WebTrack leads to new innovative shifts in the\nprocessing of tracking data. As WebTrack allows collecting the content people\nare exposed to on more than classical news platforms, we can strongly improve\nthe detection of politics-related information consumption in tracking data with\nthe application of automated content analysis compared to traditional\napproaches that rely on the list-based identification of news.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.13536,regular,post_llm,2024,3,"{'ai_likelihood': 0.99755859375, 'text': ""Conceptualizing predictive conceptual model for unemployment rates in\n  the implementation of Industry 4.0: Exploring machine learning techniques\n\n  Although there are obstacles related to obtaining data, ensuring model\nprecision, and upholding ethical standards, the advantages of utilizing machine\nlearning to generate predictive models for unemployment rates in developing\nnations amid the implementation of Industry 4.0 (I4.0) are noteworthy. This\nresearch delves into the concept of utilizing machine learning techniques\nthrough a predictive conceptual model to understand and address factors that\ncontribute to unemployment rates in developing nations during the\nimplementation of I4.0. A thorough examination of the literature was carried\nout through a literature review to determine the economic and social factors\nthat have an impact on the unemployment rates in developing nations. The\nexamination of the literature uncovered that considerable influence on\nunemployment rates in developing nations is attributed to elements such as\neconomic growth, inflation, population increase, education levels, and\ntechnological progress. A predictive conceptual model was developed that\nindicates factors that contribute to unemployment in developing nations can be\naddressed by using techniques of machine learning like regression analysis and\nneural networks when adopting I4.0. The study's findings demonstrated the\neffectiveness of the proposed predictive conceptual model in accurately\nunderstanding and addressing unemployment rate factors within developing\nnations when deploying I4.0. The model serves a dual purpose of predicting\nfuture unemployment rates and tracking the advancement of reducing unemployment\nrates in emerging economies. By persistently conducting research and\nimprovements, decision-makers and enterprises can employ these patterns to\narrive at more knowledgeable judgments that can advance the growth of the\neconomy, generation of employment, and alleviation of poverty specifically in\nemerging nations.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.177490234375, 'GPT4': 0.315185546875, 'CLAUDE': 6.186962127685547e-05, 'GOOGLE': 0.49560546875, 'OPENAI_O_SERIES': 0.00804901123046875, 'DEEPSEEK': 6.318092346191406e-06, 'GROK': 4.172325134277344e-07, 'NOVA': 4.589557647705078e-06, 'OTHER': 0.00045108795166015625, 'HUMAN': 0.00324249267578125}}"
2403.17911,review,post_llm,2024,3,"{'ai_likelihood': 8.675787183973525e-06, 'text': 'Domain-Specific Evaluation Strategies for AI in Journalism\n\n  News organizations today rely on AI tools to increase efficiency and\nproductivity across various tasks in news production and distribution. These\ntools are oriented towards stakeholders such as reporters, editors, and\nreaders. However, practitioners also express reservations around adopting AI\ntechnologies into the newsroom, due to the technical and ethical challenges\ninvolved in evaluating AI technology and its return on investments. This is to\nsome extent a result of the lack of domain-specific strategies to evaluate AI\nmodels and applications. In this paper, we consider different aspects of AI\nevaluation (model outputs, interaction, and ethics) that can benefit from\ndomain-specific tailoring, and suggest examples of how journalistic\nconsiderations can lead to specialized metrics or strategies. In doing so, we\nlay out a potential framework to guide AI evaluation in journalism, such as\nseen in other disciplines (e.g. law, healthcare). We also consider directions\nfor future work, as well as how our approach might generalize to other domains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.17466,review,post_llm,2024,3,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Green HPC: An analysis of the domain based on Top500\n\n  The demand in computing power has never stopped growing over the years.\nToday, the performance of the most powerful systems exceeds the exascale and\nthe number of petascale systems continues to grow. Unfortunately, this growth\nalso goes hand in hand with ever-increasing energy costs, which in turn means a\nsignificant carbon footprint. In view of the environmental crisis, this paper\nintents to look at the often hidden issue of energy consumption of HPC systems.\nAs it is not easy to access the data of the constructors, we then consider the\nTop500 as the tip of the iceberg to identify the trends of the whole domain.The\nobjective of this work is to analyze Top500 and Green500 data from several\nperspectives in order to identify the dynamic of the domain regarding its\nenvironmental impact. The contributions are to take stock of the empirical laws\ngoverning the evolution of HPC computing systems both from the performance and\nenergy perspectives, to analyze the most relevant data for developing the\nperformance and energy efficiency of large-scale computing systems, to put\nthese analyses into perspective with effects and impacts (lifespan of the HPC\nsystems) and finally to derive a predictive model for the weight of HPC sector\nwithin the horizon 2030.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14896,review,post_llm,2024,3,"{'ai_likelihood': 0.99755859375, 'text': 'Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs\n  and Human Perception\n\n  The pervasive spread of misinformation and disinformation in social media\nunderscores the critical importance of detecting media bias. While robust Large\nLanguage Models (LLMs) have emerged as foundational tools for bias prediction,\nconcerns about inherent biases within these models persist. In this work, we\ninvestigate the presence and nature of bias within LLMs and its consequential\nimpact on media bias detection. Departing from conventional approaches that\nfocus solely on bias detection in media content, we delve into biases within\nthe LLM systems themselves. Through meticulous examination, we probe whether\nLLMs exhibit biases, particularly in political bias prediction and text\ncontinuation tasks. Additionally, we explore bias across diverse topics, aiming\nto uncover nuanced variations in bias expression within the LLM framework.\nImportantly, we propose debiasing strategies, including prompt engineering and\nmodel fine-tuning. Extensive analysis of bias tendencies across different LLMs\nsheds light on the broader landscape of bias propagation in language models.\nThis study advances our understanding of LLM bias, offering critical insights\ninto its implications for bias detection tasks and paving the way for more\nrobust and equitable AI systems\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0009636878967285156, 'GPT4': 0.49609375, 'CLAUDE': 0.0009236335754394531, 'GOOGLE': 0.458740234375, 'OPENAI_O_SERIES': 0.039947509765625, 'DEEPSEEK': 0.0023746490478515625, 'GROK': 1.1920928955078125e-06, 'NOVA': 0.0006418228149414062, 'OTHER': 4.112720489501953e-05, 'HUMAN': 0.00011259317398071289}}"
2403.09208,review,post_llm,2024,3,"{'ai_likelihood': 0.00025881661309136287, 'text': ""Older adults' safety and security online: A post-pandemic exploration of\n  attitudes and behaviors\n\n  Older adults' growing use of the internet and related technologies, further\naccelerated by the COVID-19 pandemic, has prompted not only a critical\nexamination of their behaviors and attitudes about online threats but also a\ngreater understanding of the roles of specific characteristics within this\npopulation group. Based on survey data and using descriptive and inferential\nstatistics, this empirical study delves into this matter. The behaviors and\nattitudes of a group of older adults aged 60 years and older (n=275) regarding\ndifferent dimensions of online safety and cybersecurity are investigated. The\nresults show that older adults report a discernible degree of concern about the\nsecurity of their personal information. Despite the varied precautions taken,\nmost of them do not know where to report online threats. What is more,\nregarding key demographics, the study found some significant differences in\nterms of gender and age group, but not disability status. This implies that\nolder adults do not seem to constitute a homogeneous group when it comes to\nattitudes and behaviors regarding safety and security online. The study\nconcludes that support systems should include older adults in the development\nof protective measures and acknowledge their diversity. The implications of the\nresults are discussed and some directions for future research are proposed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14699,review,post_llm,2024,3,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Digital Twins: How Far from Ideas to Twins?\n\n  As a bridge from virtuality to reality, Digital Twin has increased in\npopularity since proposed. Ideas have been proposed theoretical and practical\nfor digital twins. From theoretical perspective, digital twin is fusion of data\nmapping between modalities; from practical point of view, digital twin is\nscenario implementation based on the Internet of Things and models. From these\ntwo perspectives, we explore the researches from idea to realization of digital\ntwins and discuss thoroughly.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.06865,review,post_llm,2024,3,"{'ai_likelihood': 0.96826171875, 'text': ""On the Preservation of Africa's Cultural Heritage in the Age of\n  Artificial Intelligence\n\n  In this paper we delve into the historical evolution of data as a fundamental\nelement in communication and knowledge transmission. The paper traces the\nstages of knowledge dissemination from oral traditions to the digital era,\nhighlighting the significance of languages and cultural diversity in this\nprogression. It also explores the impact of digital technologies on memory,\ncommunication, and cultural preservation, emphasizing the need for promoting a\nculture of the digital (rather than a digital culture) in Africa and beyond.\nAdditionally, it discusses the challenges and opportunities presented by data\nbiases in AI development, underscoring the importance of creating diverse\ndatasets for equitable representation. We advocate for investing in data as a\ncrucial raw material for fostering digital literacy, economic development, and,\nabove all, cultural preservation in the digital age.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.2418212890625, 'GPT4': 0.0909423828125, 'CLAUDE': 0.00702667236328125, 'GOOGLE': 0.52587890625, 'OPENAI_O_SERIES': 0.0032749176025390625, 'DEEPSEEK': 0.0005159378051757812, 'GROK': 0.0005803108215332031, 'NOVA': 0.00148773193359375, 'OTHER': 0.12371826171875, 'HUMAN': 0.00482177734375}}"
2403.12774,review,post_llm,2024,3,"{'ai_likelihood': 0.93896484375, 'text': 'Is open source software culture enough to make AI a common ?\n\n  Language models (LM or LLM) are increasingly deployed in the field of\nartificial intelligence (AI) and its applications, but the question arises as\nto whether they can be a common resource managed and maintained by a community\nof users. Indeed, the dominance of private companies with exclusive access to\nmassive data and language processing resources can create inequalities and\nbiases in LM, as well as obstacles to innovation for those who do not have the\nsame resources necessary for their implementation. In this contribution, we\nexamine the concept of the commons and its relevance for thinking about LM. We\nhighlight the potential benefits of treating the data and resources needed to\ncreate LMs as commons, including increased accessibility, equity, and\ntransparency in the development and use of AI technologies. Finally, we present\na case study centered on the Hugging Face platform, an open-source platform for\ndeep learning designed to encourage collaboration and sharing among AI\ndesigners.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.88525390625, 'GPT4': 0.005641937255859375, 'CLAUDE': 0.005451202392578125, 'GOOGLE': 0.08123779296875, 'OPENAI_O_SERIES': 9.638071060180664e-05, 'DEEPSEEK': 1.4007091522216797e-05, 'GROK': 2.2113323211669922e-05, 'NOVA': 2.8371810913085938e-05, 'OTHER': 0.0129241943359375, 'HUMAN': 0.009490966796875}}"
2406.11846,review,post_llm,2024,3,"{'ai_likelihood': 2.317958407931858e-06, 'text': ""Lifecycle of a sub-metered tertiary multi-use (GreEn-ER) building's open\n  energy data: from resource mobilisation to data re-usability\n\n  The proliferation of sensors in buildings has given us access to more data\nthan before. To shepherd this rise in data, many open data lifecycles have been\nproposed over the past decade. However, many of the proposed lifecycles do not\nreflect the necessary complexity in the built environment. In this paper, we\npresent a new open data lifecycle model: Open Energy Data Lifecycle (OPENDAL).\nOPENDAL builds on the key themes in more popular lifecycles and looks to extend\nthem by better accounting for the information flows between cycles and the\ninteractions between stakeholders around the data. These elements are included\nin the lifecycle in a bid to increase the reuse of published datasets. In\naddition, we apply the lifecycle model to the datasets from the GreEn-ER\nbuilding, a mixed-use education building in France. Different use cases of\nthese datasets are highlighted and discussed as a way to incentivise the use of\ndata by other individuals.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.19245,review,post_llm,2024,3,"{'ai_likelihood': 0.97314453125, 'text': ""The use of ChatGPT in higher education: The advantages and disadvantages\n\n  Higher education scholars are interested in an artificial intelligence (AI)\ntechnology called ChatGPT, which was developed by OpenAI. Whether ChatGPT can\nimprove learning is still a topic of debate among experts. This concise\noverview of the literature examines the application of ChatGPT in higher\neducation to comprehend and produce high-level instruction. By examining the\nessential literature, this study seeks to provide a thorough assessment of the\nadvantages and disadvantages of utilizing ChatGPT in higher education settings.\nBut it's crucial to consider both the positive and negative elements. For this\nrapid review, the researcher searched Google Scholar, Scopus, and others\nbetween January 2023 and July 2023 for prior research from various\npublications. These studies were examined. The study found that employing\nChatGPT in higher education is beneficial for a number of reasons. It can\nprovide individualized instruction, and prompt feedback, facilitate access to\nlearning, and promote student interaction. These benefits could improve the\nlearning environment and make it more fun for academics and students. The cons\nof ChatGPT are equally present. These problems include the inability to\ncomprehend emotions, the lack of social interaction chances, technological\nlimitations, and the dangers of depending too much on ChatGPT for higher\neducation. Higher education should combine ChatGPT with other teaching\ntechniques to provide students and lecturers with a comprehensive education.\nHowever, it is crucial to consider the positives, negatives, and moral issues\nbefore adopting ChatGPT in the classroom.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.05157470703125, 'GPT4': 0.0284576416015625, 'CLAUDE': 0.00011032819747924805, 'GOOGLE': 0.888671875, 'OPENAI_O_SERIES': 0.0035610198974609375, 'DEEPSEEK': 1.3887882232666016e-05, 'GROK': 5.304813385009766e-06, 'NOVA': 9.143352508544922e-05, 'OTHER': 0.00894927978515625, 'HUMAN': 0.0186309814453125}}"
2406.11849,review,post_llm,2024,3,"{'ai_likelihood': 1.0165903303358291e-05, 'text': 'What do we know about Computing Education in Africa? A Systematic Review\n  of Computing Education Research Literature\n\n  Noticeably, Africa is underrepresented in the computing education research\n(CER) community. However, there has been some effort from the researchers in\nthe region to contribute to the growing need for computing for all. To\nunderstand the body of works that emerged from the global south region and\ntheir area of focus in computing education, we conducted a systematic review of\nthe literature. This research investigates the prominent CER journals and\nconferences to discern the kind of research that has been published and how\nmuch contribution they have made to the growing field. Of the 68 selected\nstudies, 45 papers were from South Africa. The prominent aspect of computing in\nthe literature is programming, which accounts for 43%. We identified open areas\nfor research in the context and discussed the implication of our findings for\nthe development of CER in Africa.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.00367,regular,post_llm,2024,3,"{'ai_likelihood': 4.331270853678386e-05, 'text': ""SA-LSPL:Sequence-Aware Long- and Short- Term Preference Learning for\n  next POI recommendation\n\n  The next Point of Interest (POI) recommendation aims to recommend the next\nPOI for users at a specific time. As users' check-in records can be viewed as a\nlong sequence, methods based on Recurrent Neural Networks (RNNs) have recently\nshown good applicability to this task. However, existing methods often struggle\nto fully explore the spatio-temporal correlations and dependencies at the\nsequence level, and don't take full consideration for various factors\ninfluencing users' preferences. To address these issues, we propose a novel\napproach called Sequence-Aware Long- and Short-Term Preference Learning\n(SA-LSPL) for next-POI recommendation. We combine various information features\nto effectively model users' long-term preferences. Specifically, our proposed\nmodel uses a multi-modal embedding module to embed diverse check-in details,\ntaking into account both user's personalized preferences and social influences\ncomprehensively. Additionally, we consider explicit spatio-temporal\ncorrelations at the sequence level and implicit sequence dependencies.\nFurthermore, SA-LSPL learns the spatio-temporal correlations of consecutive and\nnon-consecutive visits in the current check-in sequence, as well as transition\ndependencies between categories, providing a comprehensive capture of user's\nshort-term preferences. Extensive experiments on two real-world datasets\ndemonstrate the superiority of SA-LSPL over state-of-the-art baseline methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.01292,review,post_llm,2024,3,"{'ai_likelihood': 5.728668636745877e-06, 'text': 'Autonomous Intelligent Systems: From Illusion of Control to Inescapable\n  Delusion\n\n  Autonomous systems, including generative AI, have been adopted faster than\nprevious digital innovations. Their impact on society might as well be more\nprofound, with a radical restructuring of the economy of knowledge and dramatic\nconsequences for social and institutional balances. Different attitudes to\ncontrol these systems have emerged rooted in the classical pillars of legal\nsystems, proprietary rights, and social responsibility. We show how an illusion\nof control might be guiding governments and regulators, while autonomous\nsystems might be driving us to inescapable delusion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.12838,regular,post_llm,2024,3,"{'ai_likelihood': 7.516807980007596e-06, 'text': 'How Spammers and Scammers Leverage AI-Generated Images on Facebook for\n  Audience Growth\n\n  Much of the research and discourse on risks from artificial intelligence (AI)\nimage generators, such as DALL-E and Midjourney, has centered around whether\nthey could be used to inject false information into political discourse. We\nshow that spammers and scammers - seemingly motivated by profit or clout, not\nideology - are already using AI-generated images to gain significant traction\non Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated\nimages to users who neither follow the Pages posting the images nor realize\nthat the images are AI-generated, highlighting the need for improved\ntransparency and provenance standards as AI models proliferate.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.13487,review,post_llm,2024,3,"{'ai_likelihood': 1.0, 'text': 'The future of generative AI chatbots in higher education\n\n  The integration of generative Artificial Intelligence (AI) chatbots in higher\neducation institutions (HEIs) is reshaping the educational landscape, offering\nopportunities for enhanced student support, and administrative and research\nefficiency. This study explores the future implications of generative AI\nchatbots in HEIs, aiming to understand their potential impact on teaching and\nlearning, and research processes. Utilizing a narrative literature review (NLR)\nmethodology, this study synthesizes existing research on generative AI chatbots\nin higher education from diverse sources, including academic databases and\nscholarly publications. The findings highlight the transformative potential of\ngenerative AI chatbots in streamlining administrative tasks, enhancing student\nlearning experiences, and supporting research activities. However, challenges\nsuch as academic integrity concerns, user input understanding, and resource\nallocation pose significant obstacles to the effective integration of\ngenerative AI chatbots in HEIs. This study underscores the importance of\nproactive measures to address ethical considerations, provide comprehensive\ntraining for stakeholders, and establish clear guidelines for the responsible\nuse of generative AI chatbots in higher education. By navigating these\nchallenges, and leveraging the benefits of generative AI technologies, HEIs can\nharness the full potential of generative AI chatbots to create a more\nefficient, effective, inclusive, and innovative educational environment.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.01214599609375, 'GPT4': 0.004108428955078125, 'CLAUDE': 0.0013265609741210938, 'GOOGLE': 0.97216796875, 'OPENAI_O_SERIES': 0.0001569986343383789, 'DEEPSEEK': 6.079673767089844e-06, 'GROK': 7.152557373046875e-07, 'NOVA': 3.331899642944336e-05, 'OTHER': 0.0102386474609375, 'HUMAN': 5.555152893066406e-05}}"
2403.10688,regular,post_llm,2024,3,"{'ai_likelihood': 0.0, 'text': 'Safer Digital Intimacy For Sex Workers And Beyond: A Technical Research\n  Agenda\n\n  Many people engage in digital intimacy: sex workers, their clients, and\npeople who create and share intimate content recreationally. With this intimacy\ncomes significant security and privacy risk, exacerbated by stigma. In this\narticle, we present a commercial digital intimacy threat model and 10 research\ndirections for safer digital intimacy\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.01095,regular,post_llm,2024,3,"{'ai_likelihood': 3.013345930311415e-06, 'text': 'Inevitable-Metaverse: A Novel Twitter Dataset for Public Sentiments on\n  Metaverse\n\n  Metaverse has emerged as a novel technology with the objective to merge the\nphysical world into the virtual world. This technology has seen a lot of\ninterest and investment in recent times from prominent organizations including\nFacebook which has changed its company name to Meta with the goal of being the\nleader in developing this technology. Although people in general are excited\nabout the prospects of metaverse due to potential use cases such as virtual\nmeetings and virtual learning environments, there are also concerns due to\npotential negative consequences. For instance, people are concerned about their\ndata privacy as well as spending a lot of their time on the metaverse leading\nto negative impacts in real life. Therefore, this research aims to further\ninvestigate the public sentiments regarding metaverse on social media. A total\nof 86565 metaverse-related tweets were used to perform lexicon-based sentiment\nanalysis. Furthermore, various machine and deep learning models with various\ntext features were utilized to predict the sentiment class. The BERT\ntransformer model was demonstrated to be the best at predicting the sentiment\ncategories with 92.6% accuracy and 0.91 F-measure on the test dataset. Finally,\nthe implications and future research directions were also discussed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14473,review,post_llm,2024,3,"{'ai_likelihood': 4.635916815863716e-06, 'text': 'The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on\n  Large Language Models (LLMs)\n\n  With the introduction of ChatGPT, Large Language Models (LLMs) have received\nenormous attention in healthcare. Despite their potential benefits, researchers\nhave underscored various ethical implications. While individual instances have\ndrawn much attention, the debate lacks a systematic overview of practical\napplications currently researched and ethical issues connected to them. Against\nthis background, this work aims to map the ethical landscape surrounding the\ncurrent stage of deployment of LLMs in medicine and healthcare. Electronic\ndatabases and preprint servers were queried using a comprehensive search\nstrategy. Studies were screened and extracted following a modified rapid review\napproach. Methodological quality was assessed using a hybrid approach. For 53\nrecords, a meta-aggregative synthesis was performed. Four fields of\napplications emerged and testify to a vivid exploration phase. Advantages of\nusing LLMs are attributed to their capacity in data analysis, personalized\ninformation provisioning, support in decision-making, mitigating information\nloss and enhancing information accessibility. However, we also identifies\nrecurrent ethical concerns connected to fairness, bias, non-maleficence,\ntransparency, and privacy. A distinctive concern is the tendency to produce\nharmful misinformation or convincingly but inaccurate content. A recurrent plea\nfor ethical guidance and human oversight is evident. Given the variety of use\ncases, it is suggested that the ethical guidance debate be reframed to focus on\ndefining what constitutes acceptable human oversight across the spectrum of\napplications. This involves considering diverse settings, varying potentials\nfor harm, and different acceptable thresholds for performance and certainty in\nhealthcare. In addition, a critical inquiry is necessary to determine the\nextent to which the current experimental use of LLMs is necessary and\njustified.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.08093,regular,post_llm,2024,3,"{'ai_likelihood': 9.404288397894966e-05, 'text': 'Preserving Automotive Heritage: A Blockchain-Based Solution for Secure\n  Documentation of Classic Cars Restoration\n\n  Classic automobiles are an important part of the automotive industry and\nrepresent the historical and technological achievements of certain eras.\nHowever, to be considered masterpieces, they must be maintained in pristine\ncondition or restored according to strict guidelines applied by expert\nservices. Therefore, all data about restoration processes and other relevant\ninformation about these vehicles must be rigorously documented to ensure their\nverifiability and immutability. Here, we report on our ongoing research to\nadequately provide such capabilities to the classic car ecosystem.\n  Using a design science research approach, we have developed a\nblockchain-based solution using Hyperledger Fabric that facilitates the proper\nrecording of classic car information, restoration procedures applied, and all\nrelated documentation by ensuring that this data is immutable and trustworthy\nwhile promoting collaboration between interested parties. This solution was\nvalidated and received positive feedback from various entities in the classic\ncar sector. The enhanced and secured documentation is expected to contribute to\nthe digital transformation of the classic car sector, promote authenticity and\ntrustworthiness, and ultimately increase the market value of classic cars.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.13073,regular,post_llm,2024,3,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'A Canary in the AI Coal Mine: American Jews May Be Disproportionately\n  Harmed by Intellectual Property Dispossession in Large Language Model\n  Training\n\n  Systemic property dispossession from minority groups has often been carried\nout in the name of technological progress. In this paper, we identify evidence\nthat the current paradigm of large language models (LLMs) likely continues this\nlong history. Examining common LLM training datasets, we find that a\ndisproportionate amount of content authored by Jewish Americans is used for\ntraining without their consent. The degree of over-representation ranges from\naround 2x to around 6.5x. Given that LLMs may substitute for the paid labor of\nthose who produced their training data, they have the potential to cause even\nmore substantial and disproportionate economic harm to Jewish Americans in the\ncoming years. This paper focuses on Jewish Americans as a case study, but it is\nprobable that other minority communities (e.g., Asian Americans, Hindu\nAmericans) may be similarly affected and, most importantly, the results should\nlikely be interpreted as a ""canary in the coal mine"" that highlights deep\nstructural concerns about the current LLM paradigm whose harms could soon\naffect nearly everyone. We discuss the implications of these results for the\npolicymakers thinking about how to regulate LLMs as well as for those in the AI\nfield who are working to advance LLMs. Our findings stress the importance of\nworking together towards alternative LLM paradigms that avoid both disparate\nimpacts and widespread societal harms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.12227,regular,post_llm,2024,3,"{'ai_likelihood': 1.1258655124240452e-06, 'text': ""Analyzing-Evaluating-Creating: Assessing Computational Thinking and\n  Problem Solving in Visual Programming Domains\n\n  Computational thinking (CT) and problem-solving skills are increasingly\nintegrated into K-8 school curricula worldwide. Consequently, there is a\ngrowing need to develop reliable assessments for measuring students'\nproficiency in these skills. Recent works have proposed tests for assessing\nthese skills across various CT concepts and practices, in particular, based on\nmulti-choice items enabling psychometric validation and usage in large-scale\nstudies. Despite their practical relevance, these tests are limited in how they\nmeasure students' computational creativity, a crucial ability when applying CT\nand problem solving in real-world settings. In our work, we have developed ACE,\na novel test focusing on the three higher cognitive levels in Bloom's Taxonomy,\ni.e., Analyze, Evaluate, and Create. ACE comprises a diverse set of 7x3\nmulti-choice items spanning these three levels, grounded in elementary\nblock-based visual programming. We evaluate the psychometric properties of ACE\nthrough a study conducted with 371 students in grades 3-7 from 10 schools.\nBased on several psychometric analysis frameworks, our results confirm the\nreliability and validity of ACE. Our study also shows a positive correlation\nbetween students' performance on ACE and performance on Hour of Code: Maze\nChallenge by Code.org.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14971,regular,post_llm,2024,3,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""Learners Teaching Novices: An Uplifting Alternative Assessment\n\n  We propose and carry-out a novel method of formative assessment called\nAssessment via Teaching (AVT), in which learners demonstrate their\nunderstanding of CS1 topics by tutoring more novice students. AVT has powerful\nbenefits over traditional forms of assessment: it is centered around service to\nothers and is highly rewarding for the learners who teach. Moreover, teaching\ngreatly improves the learners' own understanding of the material and has a huge\npositive impact on novices, who receive free 1:1 tutoring. Lastly, this form of\nassessment is naturally difficult to cheat -- a critical property for\nassessments in the era of large-language models.\n  We use AVT in a randomised control trial with learners in a CS1 course at an\nR1 university. The learners provide tutoring sessions to more novice students\ntaking a lagged online version of the same course. We show that learners who do\nan AVT session before the course exam performed 20 to 30 percentage points\nbetter than the class average on several questions. Moreover, compared to\nstudents who did a practice exam, the AVT learners enjoyed their experience\nmore and were twice as likely to study for their teaching session. We believe\nAVT is a scalable and uplifting method for formative assessment that could one\nday replace traditional exams.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.10279,regular,post_llm,2024,3,"{'ai_likelihood': 1.3808409372965496e-05, 'text': ""Emotion-Aware Multimodal Fusion for Meme Emotion Detection\n\n  The ever-evolving social media discourse has witnessed an overwhelming use of\nmemes to express opinions or dissent. Besides being misused for spreading\nmalcontent, they are mined by corporations and political parties to glean the\npublic's opinion. Therefore, memes predominantly offer affect-enriched insights\ntowards ascertaining the societal psyche. However, the current approaches are\nyet to model the affective dimensions expressed in memes effectively. They rely\nextensively on large multimodal datasets for pre-training and do not generalize\nwell due to constrained visual-linguistic grounding. In this paper, we\nintroduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We\nthen present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a\nnovel multimodal neural framework that (i) explicitly models emotion-enriched\nvisual cues, and (ii) employs an efficient cross-modal fusion via a gating\nmechanism. Our investigation establishes ALFRED's superiority over existing\nbaselines by 4.94% F1. Additionally, ALFRED competes strongly with previous\nbest approaches on the challenging Memotion task. We then discuss ALFRED's\ndomain-agnostic generalizability by demonstrating its dominance on two\nrecently-released datasets - HarMeme and Dank Memes, over other baselines.\nFurther, we analyze ALFRED's interpretability using attention maps. Finally, we\nhighlight the inherent challenges posed by the complex interplay of disparate\nmodality-specific cues toward meme analysis.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.06593,review,post_llm,2024,3,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Authorship and the Politics and Ethics of LLM Watermarks\n\n  Recently, watermarking schemes for large language models (LLMs) have been\nproposed to distinguish text generated by machines and by humans. The present\npaper explores philosophical, political, and ethical ramifications of\nimplementing and using watermarking schemes. A definition of authorship that\nincludes both machines (LLMs) and humans is proposed to serve as a backdrop. It\nis argued that private watermarks may provide private companies with sweeping\nrights to determine authorship, which is incompatible with traditional\nstandards of authorship determination. Then, possible ramifications of the\nso-called entropy dependence of watermarking mechanisms are explored. It is\nargued that entropy may vary for different, socially salient groups. This could\nlead to group dependent rates at which machine generated text is detected.\nSpecifically, groups more interested in low entropy text may face the challenge\nthat it is harder to detect machine generated text that is of interest to them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14674,review,post_llm,2024,3,"{'ai_likelihood': 0.0010654661390516493, 'text': 'Packaging Up Media Mix Modeling: An Introduction to Robyn\'s Open-Source\n  Approach\n\n  As privacy-centric changes reshape the digital advertising landscape,\ndeterministic attribution and measurement of advertising-related user behavior\nis increasingly constrained. In response, there has been a resurgence in the\nuse of traditional probabilistic measurement techniques, such as media and\nmarketing mix modeling (m/MMM), particularly among digital-first advertisers.\nHowever, small and midsize businesses often lack the resources to implement\nadvanced proprietary modeling systems, which require specialized expertise and\nsignificant team investments. To address this gap, marketing data scientists at\nMeta have developed the open-source computational package Robyn, designed to\nfacilitate the adoption of m/MMM for digital advertising measurement. This\narticle explores the computational components and design choices that underpin\nRobyn, emphasizing how it ""packages up"" m/MMM to promote organizational\nacceptance and mitigate common biases. As a widely adopted and actively\nmaintained open-source tool, Robyn is continually evolving. Consequently, the\nsolutions described here should not be seen as definitive or conclusive but as\nan outline of the pathways that the Robyn community has embarked on. This\narticle aims to provide a structured introduction to these evolving practices,\nencouraging feedback and dialogue to ensure that Robyn\'s development aligns\nwith the needs of the broader data science community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14686,regular,post_llm,2024,3,"{'ai_likelihood': 0.88037109375, 'text': ""Evaluating Pedagogical Incentives in Undergraduate Computing: A Mixed\n  Methods Approach Using Learning Analytics\n\n  In the context of higher education's evolving dynamics post-COVID-19, this\npaper assesses the impact of new pedagogical incentives implemented in a\nfirst-year undergraduate computing module at University College London. We\nemploy a mixed methods approach, combining learning analytics with qualitative\ndata, to evaluate the effectiveness of these incentives on increasing student\nengagement.\n  A longitudinal overview of resource interactions is mapped through Bayesian\nnetwork analysis of Moodle activity logs from 204 students. This analysis\nidentifies early resource engagement as a predictive indicator of continued\nengagement while also suggesting that the new incentives disproportionately\nbenefit highly engaged students. Focus group discussions complement this\nanalysis, providing insights into student perceptions of the pedagogical\nchanges and the module design. These qualitative findings underscore the\nchallenge of sustaining engagement through the new incentives and highlight the\nimportance of communication in blended learning environments.\n  Our paper introduces an interpretable and actionable model for student\nengagement, which integrates objective, data-driven analysis with students'\nperspectives. This model provides educators with a tool to evaluate and improve\ninstructional strategies. By demonstrating the effectiveness of our mixed\nmethods approach in capturing the intricacies of student behaviour in digital\nlearning environments, we underscore the model's potential to improve online\npedagogical practices across diverse educational settings.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.00011986494064331055, 'GPT4': 0.9375, 'CLAUDE': 0.00022399425506591797, 'GOOGLE': 0.058319091796875, 'OPENAI_O_SERIES': 0.0011425018310546875, 'DEEPSEEK': 4.863739013671875e-05, 'GROK': 4.76837158203125e-07, 'NOVA': 9.5367431640625e-07, 'OTHER': 7.325410842895508e-05, 'HUMAN': 0.0023365020751953125}}"
2403.15488,regular,post_llm,2024,3,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""Enhancing Students' Learning Process Through Self-Generated Tests\n\n  The use of new technologies in higher education has surprisingly emphasized\nstudents' tendency to adopt a passive behavior in class. Participation and\ninteraction of students are essential to improve academic results. This paper\ndescribes an educational experiment aimed at the promotion of students'\nautonomous learning by requiring them to generate test type questions related\nto the contents of the course. The main idea is to make the student feel part\nof the evaluation process by including students' questions in the evaluation\nexams. A set of applications running on our university online learning\nenvironment has been developed in order to provide both students and teachers\nwith the necessary tools for a good interaction between them. Questions\nuploaded by students are visible to every enrolled student as well as to each\ninvolved teacher. In this way, we enhance critical analysis skills, by solving\nand finding possible mistakes in the questions sent by their fellows. The\nexperiment was applied over 769 students from 12 different courses. Results\nshow that the students who have actively participated in the experiment have\nobtained better academic performance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.12717,review,post_llm,2024,3,"{'ai_likelihood': 5.198849572075738e-06, 'text': 'Survey of Methods, Resources, and Formats for Teaching Constraint\n  Programming\n\n  This paper provides an overview of the state of teaching for Constraint\nProgramming, based on a survey of the community for the 2023 Workshop on\nTeaching Constraint Programming at the CP 2023 conference in Toronto. The paper\npresents the results of the survey, as well as lists of books, video courses\nand other tutorial materials for teaching Constraint Programming. The paper\nserves as a single location for current and public information on course\nresources, topics, formats, and methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.19049,regular,post_llm,2024,3,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Power and Play: Investigating ""License to Critique"" in Teams\' AI Ethics\n  Discussions\n\n  Past work has sought to design AI ethics interventions--such as checklists or\ntoolkits--to help practitioners design more ethical AI systems. However, other\nwork demonstrates how these interventions may instead serve to limit critique\nto that addressed within the intervention, while rendering broader concerns\nillegitimate. In this paper, drawing on work examining how standards enact\ndiscursive closure and how power relations affect whether and how people raise\ncritique, we recruit three corporate teams, and one activist team, each with\nprior context working with one another, to play a game designed to trigger\nbroad discussion around AI ethics. We use this as a point of contrast to\ntrigger reflection on their teams\' past discussions, examining factors which\nmay affect their ""license to critique"" in AI ethics discussions. We then report\non how particular affordances of this game may influence discussion, and find\nthat the hypothetical context created in the game is unlikely to be a viable\nmechanism for real world change. We discuss how power dynamics within a group\nand notions of ""scope"" affect whether people may be willing to raise critique\nin AI ethics discussions, and discuss our finding that games are unlikely to\nenable direct changes to products or practice, but may be more likely to allow\nmembers to find critically-aligned allies for future collective action.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.10484,review,post_llm,2024,3,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Moodle Usability Assessment Methodology using the Universal Design for Learning perspective\n\nThe application of the Universal Design for Learning framework favors the creation of virtual educational environments for all. It requires developing accessible content, having a usable platform, and the use of flexible didactics and evaluations that promote constant student motivation. The present study aims to design a methodology to evaluate the usability of the Moodle platform based on the principles of Universal Design for Learning, recognizing the importance of accessibility, usability and the availability of Assistive Technologies. We developed and applied a methodology to assess the usability level of Moodle platforms, taking into consideration that they integrate Assistive Technologies or are used for MOOC contexts. We provide the results of a use case that assesses two instances for the respective Moodle v.2.x and v.3.x family versions. We employed the framework of mixed design research in order to assess a MOOC-type educational program devised under the principles of Universal Design for Learning. As a result of the assessment of Moodle v.2.x and v.3.x, we conclude that the platforms must improve some key elements (e.g. contrasting colors, incorporation of alternative text and links) in order to comply with international accessibility standards. With respect to usability, we can confirm that the principles and guidelines of Universal Design for Learning are applicable to MOOC-type Virtual Learning Environments, are positively valued by students, and have a positive impact on certification rates.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.00431,regular,post_llm,2024,3,"{'ai_likelihood': 0.99755859375, 'text': ""Robotic Process Automation as a Driver for Sustainable Innovation and\n  Entrepreneurship\n\n  Technological innovation plays a crucial role in driving economic growth and\ndevelopment. In this study, we investigate the extent to which technological\ninnovation contributes to a more sustainable future and fosters\nentrepreneurship. To examine this, we focus on robotic process automation (RPA)\nhighly relevant technology. We conducted a comprehensive analysis by examining\nthe usage of RPA and its impact on environmental, social, and governance (ESG)\nfactors. Our research involved gathering data from the 300 largest companies in\nterms of market capitalization. We assessed whether these companies used RPA\nand obtained their corresponding ESG ratings. To investigate the relationship\nbetween RPA and ESG, we employed a contingency table analysis, which involved\ncategorizing the data based on ESG ratings. We further used Pearson's\nChi-square Test of Independence to assess the impact of RPA on ESG. Our\nfindings revealed a statistically significant association between RPA and ESG\nratings, indicating their interconnection. The calculated value for Pearson's\nChi-square Test of Independence was 6.54, with a corresponding p-value of\n0.0381. This indicates that at a significance level of five percent, the RPA\nand ESG variables depend on each other. These results suggest that RPA,\nrepresentative of modern technologies, likely influences the achievement of a\nsustainable future and the promotion of entrepreneurship. In conclusion, our\nstudy provides empirical evidence supporting the notion that technological\ninnovations such as RPA have the potential to positively shape sustainability\nefforts and entrepreneurial endeavours.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00450897216796875, 'GPT4': 0.66259765625, 'CLAUDE': 0.00016450881958007812, 'GOOGLE': 0.327392578125, 'OPENAI_O_SERIES': 0.00485992431640625, 'DEEPSEEK': 2.86102294921875e-06, 'GROK': 1.7881393432617188e-07, 'NOVA': 1.3053417205810547e-05, 'OTHER': 0.00018072128295898438, 'HUMAN': 0.00019931793212890625}}"
2403.14986,regular,post_llm,2024,3,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style\n  Feedback in a Global Course\n\n  Teaching students how to write code that is elegant, reusable, and\ncomprehensible is a fundamental part of CS1 education. However, providing this\n""style feedback"" in a timely manner has proven difficult to scale. In this\npaper, we present our experience deploying a novel, real-time style feedback\ntool in Code in Place, a large-scale online CS1 course. Our tool is based on\nthe latest breakthroughs in large-language models (LLMs) and was carefully\ndesigned to be safe and helpful for students. We used our Real-Time Style\nFeedback tool (RTSF) in a class with over 8,000 diverse students from across\nthe globe and ran a randomized control trial to understand its benefits. We\nshow that students who received style feedback in real-time were five times\nmore likely to view and engage with their feedback compared to students who\nreceived delayed feedback. Moreover, those who viewed feedback were more likely\nto make significant style-related edits to their code, with over 79% of these\nedits directly incorporating their feedback. We also discuss the practicality\nand dangers of LLM-based tools for feedback, investigating the quality of the\nfeedback generated, LLM limitations, and techniques for consistency,\nstandardization, and safeguarding against demographic bias, all of which are\ncrucial for a tool utilized by students.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.09197,regular,post_llm,2024,3,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'MetroGNN: Metro Network Expansion with Reinforcement Learning\n\n  Selecting urban regions for metro network expansion to meet maximal\ntransportation demands is crucial for urban development, while computationally\nchallenging to solve. The expansion process relies not only on complicated\nfeatures like urban demographics and origin-destination (OD) flow but is also\nconstrained by the existing metro network and urban geography. In this paper,\nwe introduce a reinforcement learning framework to address a Markov decision\nprocess within an urban heterogeneous multi-graph. Our approach employs an\nattentive policy network that intelligently selects nodes based on information\ncaptured by a graph neural network. Experiments on real-world urban data\ndemonstrate that our proposed methodology substantially improve the satisfied\ntransportation demands by over 30\\% when compared with state-of-the-art\nmethods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14707,regular,post_llm,2024,3,"{'ai_likelihood': 7.523430718315972e-05, 'text': 'Information Fusion in Multimodal IoT Systems for physical activity level monitoring\n\nThis study exploits information fusion in IoT systems and uses a clustering method to identify similarities in behaviours and key characteristics within each cluster. This approach facilitates early detection of behaviour changes and provides a more in-depth understanding of behaviour routines for continuous health monitoring.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.1438,regular,post_llm,2024,3,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial\n\nThe development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.04717,review,post_llm,2024,3,"{'ai_likelihood': 0.95361328125, 'text': 'Literature Review of Current Sustainability Assessment Frameworks and\n  Approaches for Organizations\n\n  This systematic literature review explores sustainability assessment\nframeworks (SAFs) across diverse industries. The review focuses on SAF design\napproaches including the methods used for Sustainability Indicator (SI)\nselection, relative importance assessment, and interdependency analysis.\nVarious methods, including literature reviews, stakeholder interviews,\nquestionnaires, Pareto analysis, SMART approach, and adherence to\nsustainability standards, contribute to the complex SI selection process.\nFuzzy-AHP stands out as a robust technique for assessing relative SI\nimportance. While dynamic sustainability and performance indices are essential,\nmethods like DEMATEL, VIKOR, correlation analysis, and causal models for\ninterdependency assessment exhibit static limitations. The review presents\nstrengths and limitations of SAFs, addressing gaps in design approaches and\ncontributing to a comprehensive understanding. The insights of this review aim\nto benefit policymakers, administrators, leaders, and researchers, fostering\nsustainability practices. Future research recommendations include exploring\nmulti-criteria decision-making models and hybrid approaches, extending\nsustainability evaluation across organizational levels and supply chains.\nEmphasizing adaptability to industry specifics and dynamic global adjustments\nis proposed for holistic sustainability practices, further enhancing\norganizational sustainability.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00103759765625, 'GPT4': 0.04669189453125, 'CLAUDE': 0.0023651123046875, 'GOOGLE': 0.82080078125, 'OPENAI_O_SERIES': 0.11053466796875, 'DEEPSEEK': 0.0018205642700195312, 'GROK': 1.811981201171875e-05, 'NOVA': 0.00018095970153808594, 'OTHER': 0.0024261474609375, 'HUMAN': 0.01392364501953125}}"
2403.18188,regular,post_llm,2024,3,"{'ai_likelihood': 0.0007915496826171875, 'text': 'Integrating urban digital twins with cloud-based geospatial dashboards\n  for coastal resilience planning: A case study in Florida\n\n  Coastal communities are confronted with a growing incidence of\nclimate-induced flooding, necessitating adaptation measures for resilience. In\nthis paper, we introduce a framework that integrates an urban digital twin with\na geospatial dashboard to allow visualization of the vulnerabilities within\ncritical infrastructure across a range of spatial and temporal scales. The\nsynergy between these two technologies fosters heightened community awareness\nabout increased flood risks to establish a unified understanding, the\nfoundation for collective decision-making in adaptation plans. The paper also\nelucidates ethical considerations while developing the platform, including\nensuring accessibility, promoting transparency and equity, and safeguarding\nindividual privacy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.10356,regular,post_llm,2024,3,"{'ai_likelihood': 0.0007120768229166667, 'text': ""Understanding Stress: A Web Interface for Mental Arithmetic Tasks in a\n  Trier Social Stress Test\n\n  Stress is a dynamic process that reflects the responses of the brain.\nTraditional methods for measuring stress are often time-consuming and\nsusceptible to recall bias. To address this, we investigated changes in heart\nrate (HR) during the Trier Social Stress Test (TSST). Our study incorporated\nvarying levels of complexity in mental arithmetic problems. Participants' HR\nincreased during the Mental Arithmetic Task phase compared to baseline and\nresting stages, indicating that stress is reflected in HR.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.18107,review,post_llm,2024,3,"{'ai_likelihood': 0.99267578125, 'text': 'The Need for Climate Data Stewardship: 10 Tensions and Reflections\n  regarding Climate Data Governance\n\n  Datafication -- the increase in data generation and advancements in data\nanalysis -- offers new possibilities for governing and tackling worldwide\nchallenges such as climate change. However, employing new data sources in\npolicymaking carries various risks, such as exacerbating inequalities,\nintroducing biases, and creating gaps in access. This paper articulates ten\ncore tensions related to climate data and its implications for climate data\ngovernance, ranging from the diversity of data sources and stakeholders to\nissues of quality, access, and the balancing act between local needs and global\nimperatives. Through examining these tensions, the article advocates for a\nparadigm shift towards multi-stakeholder governance, data stewardship, and\nequitable data practices to harness the potential of climate data for public\ngood. It underscores the critical role of data stewards in navigating these\nchallenges, fostering a responsible data ecology, and ultimately contributing\nto a more sustainable and just approach to climate action and broader social\nissues.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0020351409912109375, 'GPT4': 0.8017578125, 'CLAUDE': 0.0009174346923828125, 'GOOGLE': 0.181396484375, 'OPENAI_O_SERIES': 0.006557464599609375, 'DEEPSEEK': 0.0016040802001953125, 'GROK': 0.0020751953125, 'NOVA': 6.99162483215332e-05, 'OTHER': 0.001224517822265625, 'HUMAN': 0.0021076202392578125}}"
2403.06144,regular,post_llm,2024,3,"{'ai_likelihood': 0.001762178209092882, 'text': 'Simulating Family Conversations using LLMs: Demonstration of Parenting\n  Styles\n\n  This study presents a framework for conducting psychological and linguistic\nresearch through simulated conversations using large language models (LLMs).\nThe proposed methodology offers significant advantages, particularly for\nsimulating human interactions involving potential unethical language or\nbehaviors that would be impermissible in traditional experiments with human\nparticipants. As a demonstration, we employed LLMs to simulate family\nconversations across four parenting styles (authoritarian, authoritative,\npermissive, and uninvolved). In general, we observed that the characteristics\nof the four parenting styles were portrayed in the simulated conversations.\nSeveral strategies could be used to improve the simulation quality, such as\nincluding context awareness, employing a few-shot prompting approach or\nfine-tuning models to cater to specific simulation requirements. Overall, this\nstudy introduces a promising methodology for conducting psychological and\nlinguistic research through simulated conversations, while acknowledging the\ncurrent limitations and proposing potential solutions for future refinement and\nimprovement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.07556,regular,post_llm,2024,3,"{'ai_likelihood': 5.894237094455296e-06, 'text': 'Community Driven Approaches to Research in Technology & Society CCC\n  Workshop Report\n\n  Based on our workshop activities, we outlined three ways in which research\ncan support community needs: (1) Mapping the ecosystem of both the players and\necosystem and harm landscapes, (2) Counter-Programming, which entails using the\nsame surveillance tools that communities are subjected to observe the entities\ndoing the surveilling, effectively protecting people from surveillance, and\nconducting ethical data collection to measure the impact of these technologies,\nand (3) Engaging in positive visions and tools for empowerment so that\ntechnology can bring good instead of harm.\n  In order to effectively collaborate on the aforementioned directions, we\noutlined seven important mechanisms for effective collaboration: (1) Never\nexpect free labor of community members, (2) Ensure goals are aligned between\nall collaborators, (3) Elevate community members to leadership positions, (4)\nUnderstand no group is a monolith, (5) Establish a common language, (6) Discuss\norganization roles and goals of the project transparently from the start, and\n(7) Enable a recourse for harm.\n  We recommend that anyone engaging in community-based research (1) starts with\ncommunity-defined solutions, (2) provides alternatives to digital\nservices/information collecting mechanisms, (3) prohibits harmful automated\nsystems, (4) transparently states any systems impact, (5) minimizes and\nprotects data, (6) proactively demonstrates a system is safe and beneficial\nprior to deployment, and (7) provides resources directly to community partners.\n  Throughout the recommendation section of the report, we also provide specific\nrecommendations for funding agencies, academic institutions, and individual\nresearchers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.03935,review,post_llm,2024,3,"{'ai_likelihood': 1.0, 'text': 'Demographic Dynamics and Artificial Intelligence: Challenges and\n  Opportunities in Europe and Africa for 2050\n\n  This paper explores the complex relationship between demographics and\nartificial intelligence (AI) advances in Europe and Africa, projecting into the\nyear 2050. The advancement of AI technologies has occurred at diverse rates,\nwith Africa lagging behind Europe. Moreover, the imminent economic consequences\nof demographic shifts require a more careful examination of immigration\npatterns, with Africa emerging as a viable labor pool for European countries.\nHowever, within these dynamics, questions are raised about the differences in\nAI proficiency between African immigrants and Europeans by 2050. This paper\nexamines demographic trends and AI developments to unravel insights into the\nmultifaceted challenges and opportunities that lie ahead in the realms of\ntechnology, the economy, and society as we look ahead to 2050.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00640869140625, 'GPT4': 0.71240234375, 'CLAUDE': 0.0009889602661132812, 'GOOGLE': 0.27783203125, 'OPENAI_O_SERIES': 0.00019168853759765625, 'DEEPSEEK': 5.078315734863281e-05, 'GROK': 2.2709369659423828e-05, 'NOVA': 0.00013375282287597656, 'OTHER': 0.001766204833984375, 'HUMAN': 0.00019085407257080078}}"
2403.04226,regular,post_llm,2024,3,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Disciplining Deliberation: A Sociotechnical Perspective on Machine\n  Learning Trade-offs\n\n  This paper examines two prominent formal trade-offs in artificial\nintelligence (AI) -- between predictive accuracy and fairness, and between\npredictive accuracy and interpretability. These trade-offs have become a\ncentral focus in normative and regulatory discussions as policymakers seek to\nunderstand the value tensions that can arise in the social adoption of AI\ntools. The prevailing interpretation views these formal trade-offs as directly\ncorresponding to tensions between underlying social values, implying\nunavoidable conflicts between those social objectives. In this paper, I\nchallenge that prevalent interpretation by introducing a sociotechnical\napproach to examining the value implications of trade-offs. Specifically, I\nidentify three key considerations -- validity and instrumental relevance,\ncompositionality, and dynamics -- for contextualizing and characterizing these\nimplications. These considerations reveal that the relationship between model\ntrade-offs and corresponding values depends on critical choices and\nassumptions. Crucially, judicious sacrifices in one model property for another\ncan, in fact, promote both sets of corresponding values. The proposed\nsociotechnical perspective thus shows that we can and should aspire to higher\nepistemic and ethical possibilities than the prevalent interpretation suggests,\nwhile offering practical guidance for achieving those outcomes. Finally, I draw\nout the broader implications of this perspective for AI design and governance,\nhighlighting the need to broaden normative engagement across the AI lifecycle,\ndevelop legal and auditing tools sensitive to sociotechnical considerations,\nand rethink the vital role and appropriate structure of interdisciplinary\ncollaboration in fostering a responsible AI workforce.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.13446,regular,post_llm,2024,3,"{'ai_likelihood': 0.06632486979166667, 'text': ""IndiTag: An Online Media Bias Analysis System Using Fine-Grained Bias Indicators\n\nIn the age of information overload and polarized discourse, understanding media bias has become imperative for informed decision-making and fostering a balanced public discourse. However, without the experts' analysis, it is hard for the readers to distinguish bias from the news articles. This paper presents IndiTag, an innovative online media bias analysis system that leverages fine-grained bias indicators to dissect and distinguish bias in digital content. IndiTag offers a novel approach by incorporating large language models, bias indicators, and vector database to detect and interpret bias automatically. Complemented by a user-friendly interface facilitating automated bias analysis for readers, IndiTag offers a comprehensive platform for in-depth bias examination. We demonstrate the efficacy and versatility of IndiTag through experiments on four datasets encompassing news articles from diverse platforms. Furthermore, we discuss potential applications of IndiTag in fostering media literacy, facilitating fact-checking initiatives, and enhancing the transparency and accountability of digital media platforms. IndiTag stands as a valuable tool in the pursuit of fostering a more informed, discerning, and inclusive public discourse in the digital age. We release an online system for end users and the source code is available at https://github.com/lylin0/IndiTag."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.05221,review,post_llm,2024,3,"{'ai_likelihood': 1.1258655124240452e-06, 'text': ""Understanding Hybrid Spaces: Designing a Spacetime Model to Represent\n  Dynamic Topologies of Hybrid Spaces\n\n  This paper develops a spatiotemporal model for the visualization of dynamic\ntopologies of hybrid spaces. The visualization of spatiotemporal data is a\nwell-known problem, for example in digital twins in urban planning. There is\nalso a lack of a basic ontology for understanding hybrid spaces. The developed\nspatiotemporal model has three levels: a level of places and media types, a\nlevel of perception and a level of time and interaction. Existing concepts and\ntypes of representation of hybrid spaces are presented. The space-time model is\ntested on the basis of an art exhibition. Two hypotheses guide the accompanying\nonline survey: (A) there are correlations between media use (modality), the\nparticipants' interactions (creativity) and their perception (understanding of\nart) and (B) individual parameters (demographic data, location and situation,\nindividual knowledge) influence perception (understanding of art). The range,\nthe number of interactions and the response rate were also evaluated.\n  The online survey generally showed a positive correlation between media use\n(modality) and individual activity (creativity). However, due to the low\nparticipation rate ($P_{TN} = 14$), the survey is unfortunately not very\nrepresentative. Various dynamic topologies of hybrid spaces were successfully\nvisualized. The joint representation of real and virtual places and media types\nconveys a new basic understanding of place, range and urban density.\nRelationships between modality, Mobility and communicative interaction become\nvisible. The current phenomenon of multilocality has been successfully mapped.\nThe space-time model enables more precise class and structure formation, for\nexample in the development of digital twins. Dynamic topologies of hybrid\nspaces, such as in social media, at events or in urban development, can thus be\nbetter represented and compared.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14717,review,post_llm,2024,3,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Individual and Product-Related Antecedents of Electronic Word-of-Mouth\n\n  This research investigates the antecedents of positive and negative\nelectronic word-of-mouth (eWOM) propensity, as well as the impact of eWOM\npropensity on the intention to repurchase the product. Two types of eWOM\npredictors were considered: product related variables and personal factors. The\ndata were collected through an online survey conducted on a sample of 335\nRomanian subjects, and the analysis method was Structural Equation Modeling.\nOur findings show that personal factors - social media usage behavior,\nmarketing mavenism and need to evaluate - are the most important antecedents of\nthe intention to write product reviews and comments online, either positive or\nnegative. From the product related factors, only brand trust influences the\npropensity to provide eWOM. Furthermore, both positive and negative eWOM\nintentions are associated with the repurchase intention.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.03339,regular,post_llm,2024,3,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'An Online Approach to Solving Public Transit Stationing and Dispatch\n  Problem\n\n  Public bus transit systems provide critical transportation services for large\nsections of modern communities. On-time performance and maintaining the\nreliable quality of service is therefore very important. Unfortunately,\ndisruptions caused by overcrowding, vehicular failures, and road accidents\noften lead to service performance degradation. Though transit agencies keep a\nlimited number of vehicles in reserve and dispatch them to relieve the affected\nroutes during disruptions, the procedure is often ad-hoc and has to rely on\nhuman experience and intuition to allocate resources (vehicles) to affected\ntrips under uncertainty. In this paper, we describe a principled approach using\nnon-myopic sequential decision procedures to solve the problem and decide (a)\nif it is advantageous to anticipate problems and proactively station transit\nbuses near areas with high-likelihood of disruptions and (b) decide if and\nwhich vehicle to dispatch to a particular problem. Our approach was developed\nin partnership with the Metropolitan Transportation Authority for a mid-sized\ncity in the USA and models the system as a semi-Markov decision problem (solved\nas a Monte-Carlo tree search procedure) and shows that it is possible to obtain\nan answer to these two coupled decision problems in a way that maximizes the\noverall reward (number of people served). We sample many possible futures from\ngenerative models, each is assigned to a tree and processed using root\nparallelization. We validate our approach using 3 years of data from our\npartner agency. Our experiments show that the proposed framework serves 2% more\npassengers while reducing deadhead miles by 40%.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.15634,regular,post_llm,2024,3,"{'ai_likelihood': 8.410877651638455e-06, 'text': 'An Interactive Decision-Support Dashboard for Optimal Hospital Capacity\n  Management\n\n  Data-driven optimization models have the potential to significantly improve\nhospital capacity management, particularly during demand surges, when effective\nallocation of capacity is most critical and challenging. However, integrating\nmodels into existing processes in a way that provides value requires\nrecognizing that hospital administrators are ultimately responsible for making\ncapacity management decisions, and carefully building trustworthy and\naccessible tools for them. In this study, we develop an interactive,\nuser-friendly, electronic dashboard for informing hospital capacity management\ndecisions during surge periods. The dashboard integrates real-time hospital\ndata, predictive analytics, and optimization models. It allows hospital\nadministrators to interactively customize parameters, enabling them to explore\na range of scenarios, and provides real-time updates on recommended optimal\ndecisions. The dashboard was created through a participatory design process,\ninvolving hospital administrators in the development team to ensure practical\nutility, trustworthiness, transparency, explainability, and usability. We\nsuccessfully deployed our dashboard within the Johns Hopkins Health System\nduring the height of the COVID-19 pandemic, addressing the increased need for\ntools to inform hospital capacity management. It was used on a daily basis,\nwith results regularly communicated to hospital leadership. This study\ndemonstrates the practical application of a prospective, data-driven,\ninteractive decision-support tool for hospital system capacity management.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.09798,review,post_llm,2024,3,"{'ai_likelihood': 1.0, 'text': ""Comparing Rationality Between Large Language Models and Humans: Insights\n  and Open Questions\n\n  This paper delves into the dynamic landscape of artificial intelligence,\nspecifically focusing on the burgeoning prominence of large language models\n(LLMs). We underscore the pivotal role of Reinforcement Learning from Human\nFeedback (RLHF) in augmenting LLMs' rationality and decision-making prowess. By\nmeticulously examining the intricate relationship between human interaction and\nLLM behavior, we explore questions surrounding rationality and performance\ndisparities between humans and LLMs, with particular attention to the Chat\nGenerative Pre-trained Transformer. Our research employs comprehensive\ncomparative analysis and delves into the inherent challenges of irrationality\nin LLMs, offering valuable insights and actionable strategies for enhancing\ntheir rationality. These findings hold significant implications for the\nwidespread adoption of LLMs across diverse domains and applications,\nunderscoring their potential to catalyze advancements in artificial\nintelligence.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0008263587951660156, 'GPT4': 0.068115234375, 'CLAUDE': 0.0016994476318359375, 'GOOGLE': 0.908203125, 'OPENAI_O_SERIES': 0.000186920166015625, 'DEEPSEEK': 1.6629695892333984e-05, 'GROK': 4.76837158203125e-07, 'NOVA': 5.3763389587402344e-05, 'OTHER': 0.0208587646484375, 'HUMAN': 6.562471389770508e-05}}"
2403.07082,regular,post_llm,2024,3,"{'ai_likelihood': 3.351105584038629e-05, 'text': 'Exploring the Impact of ChatGPT on Student Interactions in\n  Computer-Supported Collaborative Learning\n\n  The growing popularity of generative AI, particularly ChatGPT, has sparked\nboth enthusiasm and caution among practitioners and researchers in education.\nTo effectively harness the full potential of ChatGPT in educational contexts,\nit is crucial to analyze its impact and suitability for different educational\npurposes. This paper takes an initial step in exploring the applicability of\nChatGPT in a computer-supported collaborative learning (CSCL) environment.\nUsing statistical analysis, we validate the shifts in student interactions\nduring an asynchronous group brainstorming session by introducing ChatGPT as an\ninstantaneous question-answering agent.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.00728,review,post_llm,2024,3,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Investigating Youths' Everyday Understanding of Machine Learning\n  Applications: a Knowledge-in-Pieces Perspective\n\n  Despite recent calls for including artificial intelligence (AI) literacy in\nK-12 education, not enough attention has been paid to studying youths' everyday\nknowledge about machine learning (ML). Most research has examined how youths\nattribute intelligence to AI/ML systems. Other studies have centered on youths'\ntheories and hypotheses about ML highlighting their misconceptions and how\nthese may hinder learning. However, research on conceptual change shows that\nyouths may not have coherent theories about scientific phenomena and instead\nhave knowledge pieces that can be productive for formal learning. We\ninvestigate teens' everyday understanding of ML through a knowledge-in-pieces\nperspective. Our analyses reveal that youths showed some understanding that ML\napplications learn from training data and that applications recognize patterns\nin input data and depending on these provide different outputs. We discuss how\nthese findings expand our knowledge base and implications for the design of\ntools and activities to introduce youths to ML.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.15475,review,post_llm,2024,3,"{'ai_likelihood': 7.616149054633247e-06, 'text': 'Large language models can help boost food production, but be mindful of\n  their risks\n\n  Coverage of ChatGPT-style large language models (LLMs) in the media has\nfocused on their eye-catching achievements, including solving advanced\nmathematical problems and reaching expert proficiency in medical examinations.\nBut the gradual adoption of LLMs in agriculture, an industry which touches\nevery human life, has received much less public scrutiny. In this short\nperspective, we examine risks and opportunities related to more widespread\nadoption of language models in food production systems. While LLMs can\npotentially enhance agricultural efficiency, drive innovation, and inform\nbetter policies, challenges like agricultural misinformation, collection of\nvast amounts of farmer data, and threats to agricultural jobs are important\nconcerns. The rapid evolution of the LLM landscape underscores the need for\nagricultural policymakers to think carefully about frameworks and guidelines\nthat ensure the responsible use of LLMs in food production before these\ntechnologies become so ingrained that policy intervention becomes challenging.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.03869,review,post_llm,2024,3,"{'ai_likelihood': 0.2639431423611111, 'text': 'Digitality as a ""longue dur\\`ee"" historical phenomenon\n\n  The digital age introduced the Digital Ecological Niche (DEN),\nrevolutionizing human interactions. The advent of Digital History (DHy) has\nmarked a methodological shift in historical studies, tracing its roots to\nBabbage and Lovelace\'s 19th-century work on ""coding"" as a foundational\ncommunication process, fostering a new interaction paradigm between humans and\nmachines, termed ""person2persons2machines."" This evolution, through\ndigitization and informatization, builds upon ancient coding practices but was\nsignificantly advanced by Babbage and Lovelace\'s contributions to mathematical\nlinguistic systems, laying the groundwork for Computer Science. This field,\ncentral to 20th-century mainframe interaction through programming languages and\nformalization, situates Digital History within a broader historical context.\nHere, coding and mathematical methodologies empower historians with advanced\ntechnologies for historical data preservation and analysis. Nonetheless, the\nextent to which computation and Turing machines can fully understand and\ninterpret history remains a subject of debate.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.19095,regular,post_llm,2024,3,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Purposeful remixing with generative AI: Constructing designer voice in\n  multimodal composing\n\n  Voice, the discursive construction of the writer's identity, has been\nextensively studied and theorized in composition studies. In multimodal\nwriting, students are able to mobilize both linguistic and non linguistic\nresources to express their real or imagined identities. But at the same time,\nwhen students are limited to choose from available online resources, their\nvoices might be compromised due to the incompatibility between their authorial\nintentions and the existing materials. This study, therefore, investigates\nwhether the use of generative AI tools could help student authors construct a\nmore consistent voice in multimodal writing. In this study, we have designed a\nphoto essay assignment where students recount a story in the form of photo\nessays and prompt AI image generating tools to create photos for their\nstorytelling. Drawing on interview data, written reflection, written\nannotation, and multimodal products from seven focal participants, we have\nidentified two remixing practices, through which students attempted to\nestablish a coherent and unique voice in writing. The study sheds light on the\nintentional and discursive nature of multimodal writing with AI as afforded by\nthe technological flexibility, while also highlighting the practical and\nethical challenges that could be attributed to students insufficient prompt and\nmultimodal literacy and the innate limitations of AI systems. This study\nprovides important implications for incorporating AI tools in designing\nmultimodal writing tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.03061,review,post_llm,2024,3,"{'ai_likelihood': 7.2187847561306425e-06, 'text': ""When Industry meets Trustworthy AI: A Systematic Review of AI for\n  Industry 5.0\n\n  Industry is at the forefront of adopting new technologies, and the process\nfollowed by the adoption has a significant impact on the economy and society.\nIn this work, we focus on analysing the current paradigm in which industry\nevolves, making it more sustainable and Trustworthy. In Industry 5.0,\nArtificial Intelligence (AI), among other technology enablers, is used to build\nservices from a sustainable, human-centric and resilient perspective. It is\ncrucial to understand those aspects that can bring AI to industry, respecting\nTrustworthy principles by collecting information to define how it is\nincorporated in the early stages, its impact, and the trends observed in the\nfield. In addition, to understand the challenges and gaps in the transition\nfrom Industry 4.0 to Industry 5.0, a general perspective on the industry's\nreadiness for new technologies is described. This provides practitioners with\nnovel opportunities to be explored in pursuit of the adoption of Trustworthy AI\nin the sector.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.1404,review,post_llm,2024,3,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Spatial Fairness: The Case for its Importance, Limitations of Existing\n  Work, and Guidelines for Future Research\n\n  Despite location being increasingly used in decision-making systems employed\nin many sensitive domains such as mortgages and insurance, astonishingly little\nattention has been paid to unfairness that may seep in due to the correlation\nof location with characteristics considered protected under anti-discrimination\nlaw, such as race or national origin. This position paper argues for the urgent\nneed to consider fairness with respect to location, termed \\textit{spatial\nfairness}, by outlining the harms that continue to be perpetuated due to\nlocation's correlation with protected characteristics. This interdisciplinary\nwork connects knowledge from fields such as public policy, economic\ndevelopment, and geography to highlight how fair-AI research currently falls\nshort of correcting for spatial biases, and does not consider challenges unique\nto spatial data. Furthermore, we identify limitations of the handful of spatial\nfairness work proposed so far, and finally, detail guidelines for future\nresearch so subsequent work may avoid such issues and help correct spatial\nbiases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.15507,review,post_llm,2024,3,"{'ai_likelihood': 1.8543667263454863e-06, 'text': ""Analyzing Potential Solutions Involving Regulation to Escape Some of\n  AI's Ethical Concerns\n\n  Artificial intelligence (AI), although not able to currently capture the many\ncomplexities of humans, are slowly adapting to have certain capabilities of\nhumans, many of which can revolutionize our world. AI systems, such as ChatGPT\nand others utilized within various industries for specific processes, have been\ntransforming rapidly. However, this transformation can occur in an extremely\nconcerning way if certain measures are not taken. This article touches on some\nof the current issues within the artificial intelligence ethical crisis, such\nas the concerns of discrimination within AI and false information that is\nbecoming readily available with AI. Within this article, plausible solutions\ninvolving regulation are discussed and how they would mitigate ethical\nconcerns. These include the self-regulation of businesses along with government\nregulation, and the effects these possible solutions can both have on current\nAI concerns.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.14675,regular,post_llm,2024,3,"{'ai_likelihood': 0.04306369357638889, 'text': 'Investigating the Impact of Project Risks on Employee Turnover Intentions in the IT Industry of Pakistan\n\nEmployee turnover remains a pressing issue within high-tech sectors such as IT firms and research centers, where organizational success heavily relies on the skills of their workforce. Intense competition and a scarcity of skilled professionals in the industry contribute to a perpetual demand for highly qualified employees, posing challenges for organizations to retain talent. While numerous studies have explored various factors affecting employee turnover in these industries, their focus often remains on overarching trends rather than specific organizational contexts. In particular, within the software industry, where projectspecific risks can significantly impact project success and timely delivery, understanding their influence on job satisfaction and turnover intentions is crucial. This study aims to investigate the influence of project risks in the IT industry on job satisfaction and employee turnover intentions. Furthermore, it examines the role of both external and internal social links in shaping perceptions of job satisfaction.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.17215,review,post_llm,2024,3,"{'ai_likelihood': 1.2450748019748264e-05, 'text': 'An Undergraduate Consortium for Addressing the Leaky Pipeline to\n  Computing Research\n\n  Despite an increasing number of successful interventions designed to broaden\nparticipation in computing research, there is still significant attrition among\nhistorically marginalized groups in the computing research pipeline. This\nexperience report describes a first-of-its-kind Undergraduate Consortium (UC)\nthat addresses this challenge by empowering students with a culmination of\ntheir undergraduate research in a conference setting. The UC, conducted at the\nAAAI Conference on Artificial Intelligence (AAAI), aims to broaden\nparticipation in the AI research community by recruiting students, particularly\nthose from historically marginalized groups, supporting them with mentorship,\nadvising, and networking as an accelerator toward graduate school, AI research,\nand their scientific identity. This paper presents our program design, inspired\nby a rich set of evidence-based practices, and a preliminary evaluation of the\nfirst years that points to the UC achieving many of its desired outcomes. We\nconclude by discussing insights to improve our program and expand to other\ncomputing communities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.13822,regular,post_llm,2024,3,"{'ai_likelihood': 9.139378865559896e-06, 'text': 'An Effective Learning Management System for Revealing Student\n  Performance Attributes\n\n  A learning management system streamlines the management of the teaching\nprocess in a centralized place, recording, tracking, and reporting the delivery\nof educational courses and student performance. Educational knowledge discovery\nfrom such an e-learning system plays a crucial role in rule regulation, policy\nestablishment, and system development. However, existing LMSs do not have\nembedded mining modules to directly extract knowledge. As educational modes\nbecome more complex, educational data mining efficiency from those\nheterogeneous student learning behaviours is gradually degraded. Therefore, an\nLMS incorporated with an advanced educational mining module is proposed in this\nstudy, as a means to mine efficiently from student performance records to\nprovide valuable insights for educators in helping plan effective learning\npedagogies, improve curriculum design, and guarantee quality of teaching.\nThrough two illustrative case studies, experimental results demonstrate\nincreased mining efficiency of the proposed mining module without information\nloss compared to classic educational mining algorithms. The mined knowledge\nreveals a set of attributes that significantly impact student academic\nperformance, and further classification evaluation validates the identified\nattributes. The design and application of such an effective LMS can enable\neducators to learn from past student performance experiences, empowering them\nto guide and intervene with students in time, and eventually improve their\nacademic success.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2403.01755,review,post_llm,2024,3,"{'ai_likelihood': 3.8080745273166235e-06, 'text': ""AI Language Models Could Both Help and Harm Equity in Marine Policymaking: The Case Study of the BBNJ Question-Answering Bot\n\nAI Large Language Models (LLMs) like ChatGPT are set to reshape some aspects of policymaking processes. Policy practitioners are already using ChatGPT for help with a variety of tasks: from drafting statements, submissions, and presentations, to conducting background research. We are cautiously hopeful that LLMs could be used to promote a marginally more balanced footing among decision makers in policy negotiations by assisting with certain tedious work, particularly benefiting developing countries who face capacity constraints that put them at a disadvantage in negotiations. However, the risks are particularly concerning for environmental and marine policy uses, due to the urgency of crises like climate change, high uncertainty, and trans-boundary impact.\n  To explore the realistic potentials, limitations, and equity risks for LLMs in marine policymaking, we present a case study of an AI chatbot for the recently adopted Biodiversity Beyond National Jurisdiction Agreement (BBNJ), and critique its answers to key policy questions. Our case study demonstrates the dangers of LLMs in marine policymaking via their potential bias towards generating text that favors the perspectives of mainly Western economic centers of power, while neglecting developing countries' viewpoints. We describe several ways these biases can enter the system, including: (1) biases in the underlying foundational language models; (2) biases arising from the chatbot's connection to UN negotiation documents, and (3) biases arising from the application design. We urge caution in the use of generative AI in ocean policy processes and call for more research on its equity and fairness implications. Our work also underscores the need for developing countries' policymakers to develop the technical capacity to engage with AI on their own terms."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.11034,review,post_llm,2024,4,"{'ai_likelihood': 1.0, 'text': 'Exploring the Path of Transformation and Development for Study Abroad\n  Consultancy Firms in China\n\n  In recent years, with the changing landscape of international education and\nthe growing demand from Chinese students, study abroad consultancy firms in\nChina need to adopt transformational development strategies to address\nchallenges and maintain competitiveness. This study investigated the\nrelationships between key performance indicators and several factors through a\nquestionnaire survey of 158 consultancy firms. The factors examined included\nservice diversification, technology adoption, talent management, and regulatory\ncompliance. Descriptive statistical analysis was employed to analyze the data.\nThe results showed that service scope diversification was positively correlated\nwith firm performance. Technology adoption was positively correlated with\noperational efficiency. Talent management was positively correlated with\nservice quality. Regulatory compliance was positively correlated with firm\nreputation. Consultancy firms that took progressive approaches in diversifying\nservices, adopting new technologies, cultivating talent, and ensuring\ncompliance demonstrated superior performance, efficiency, quality, and\nreputation compared to their less innovative counterparts. This research\nprovides empirical evidence to support the transformation of Chinese study\nabroad consultancy firms. It also highlights the need for future studies to\nconsider causality and contextual variations to gain deeper insights into this\nissue.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0093231201171875, 'GPT4': 0.07501220703125, 'CLAUDE': 0.51953125, 'GOOGLE': 0.362060546875, 'OPENAI_O_SERIES': 0.0289459228515625, 'DEEPSEEK': 0.0008120536804199219, 'GROK': 5.602836608886719e-06, 'NOVA': 6.008148193359375e-05, 'OTHER': 0.004108428955078125, 'HUMAN': 0.0002703666687011719}}"
2407.10236,review,post_llm,2024,4,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'AI and the Iterable Epistopics of Risk\n\n  Abstract. The risks AI presents to society are broadly understood to be\nmanageable through general calculus, i.e., general frameworks designed to\nenable those involved in the development of AI to apprehend and manage risk,\nsuch as AI impact assessments, ethical frameworks, emerging international\nstandards, and regulations. This paper elaborates how risk is apprehended and\nmanaged by a regulator, developer and cyber-security expert. It reveals that\nrisk and risk management is dependent on mundane situated practices not\nencapsulated in general calculus. Situated practice surfaces iterable\nepistopics, revealing how those involved in the development of AI know and\nsubsequently respond to risk and uncover major challenges in their work. The\nongoing discovery and elaboration of epistopics of risk in AI a) furnishes a\npotential program of interdisciplinary inquiry, b) provides AI developers with\na means of apprehending risk, and c) informs the ongoing evolution of general\ncalculus.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.0474,review,post_llm,2024,4,"{'ai_likelihood': 1.4437569512261285e-05, 'text': 'Fifth Generation IMC: Expanding the scope to Profit, People, and the\n  Planet\n\n  This editorial outlines an expanded scope for the next (fifth) generation of\nintegrated marketing communication. It identifies key market forces that gave\nrise to this evolution and describes a trajectory of where Integrated Marketing\nCommunication (IMC) has been and where it is going. The central shift is moving\nfrom primarily focusing on one stakeholder to multiple ones, including people\n(employees and society), the planet (environment), and profits. It identifies\nexamples from industry that exemplify multi-stakeholder decision-making and\nuses the examples to suggest research questions that academics and\npractitioners should address. Examples and research directions are organized\naround marketing strategy, communication media and messages, and measurement\nsystems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.13426,review,post_llm,2024,4,"{'ai_likelihood': 5.1657358805338544e-06, 'text': ""Data Privacy Vocabulary (DPV) -- Version 2\n\n  The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy\nVocabularies and Controls Community Group (DPVCG), enables the creation of\nmachine-readable, interoperable, and standards-based representations for\ndescribing the processing of personal data. The group has also published\nextensions to the DPV to describe specific applications to support legislative\nrequirements such as the EU's GDPR. The DPV fills a crucial niche in the state\nof the art by providing a vocabulary that can be embedded and used alongside\nother existing standards such as W3C ODRL, and which can be customised and\nextended for adapting to specifics of use-cases or domains. This article\ndescribes the version 2 iteration of the DPV in terms of its contents,\nmethodology, current adoptions and uses, and future potential. It also\ndescribes the relevance and role of DPV in acting as a common vocabulary to\nsupport various regulatory (e.g. EU's DGA and AI Act) and community initiatives\n(e.g. Solid) emerging across the globe.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.15632,regular,post_llm,2024,4,"{'ai_likelihood': 4.437234666612413e-06, 'text': ""Non-Fungible Programs: Private Full-Stack Applications for Web3\n\n  The greatest advantage that Web3 applications offer over Web 2.0 is the\nevolution of the data access layer. Opaque, centralized services that compelled\ntrust from users are replaced by trustless, decentralized systems of smart\ncontracts. However, the public nature of blockchain-based databases, on which\nsmart contracts transact, has typically presented a challenge for applications\nthat depend on data privacy or that rely on participants having incomplete\ninformation. This has changed with the introduction of confidential smart\ncontract networks that encrypt the memory state of active contracts as well as\ntheir databases stored on-chain. With confidentiality, contracts can more\nreadily implement novel interaction mechanisms that were previously infeasible.\nMeanwhile, in both Web 2.0 and Web3 applications the user interface continues\nto play a crucial role in translating user intent into actionable requests. In\nmany cases, developers have shifted intelligence and autonomy into the\nclient-side, leveraging Web technologies for compute, graphics, and networking.\nWeb3's reliance on such frontends has revealed a pain point though, namely that\ndecentralized applications are not accessible to end users without a persistent\nhost serving the application. Here we introduce the Non-Fungible Program (NFP)\nmodel for developing self-contained frontend applications that are distributed\nvia blockchain, powered by Web technology, and backed by private databases\npersisted in encrypted smart contracts. Access to frontend code, as well as\nbackend services, is controlled and guaranteed by smart contracts according to\nthe NFT ownership model, eliminating the need for a separate host. By\nextension, NFP applications bring interactivity to token owners and enable new\nfunctionalities, such as authorization mechanisms for oracles, supplementary\nWeb services, and overlay networks in a secure manner. In addition...\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.02451,review,post_llm,2024,4,"{'ai_likelihood': 9.834766387939453e-06, 'text': ""The Societal Implications of Blockchain Proliferation\n\n  Blockchain and its distributed ledger technology have far-reaching\nimplications for consumers across the world. Cryptocurrencies like XRP work to\nsolve key issues in the remittance industry, targeting corridors like Mexico\nwhere foreign remittance fuels economies. Blockchain's libertarian principles\nhave the potential to change lives in the third world, replacing corrupt\ninfrastructure with trust-based solutions. While this technology can be used to\nsignificantly improve lives, it has a wealth of destructive applications.\nBitcoin's blockchain and nefarious websites like the Silk Road have fueled an\nunderground market of drugs, money laundering, and terrorism, complicating\ndigital currency legislation. The negative environmental effects of\ncryptocurrency may also contribute significantly to global climate change.\nNegatives aside, cryptocurrency still proves to be a valuable commodity in\ntechnological development.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.11918,regular,post_llm,2024,4,"{'ai_likelihood': 3.414021597968208e-05, 'text': 'TeachNow: Enabling Teachers to Provide Spontaneous, Realtime 1:1 Help in\n  Massive Online Courses\n\n  One-on-one help from a teacher is highly impactful for students, yet\nextremely challenging to support in massive online courses (MOOCs). In this\nwork, we present TeachNow: a novel system that lets volunteer teachers from\nanywhere in the world instantly provide 1:1 help sessions to students in MOOCs,\nwithout any scheduling or coordination overhead. TeachNow works by quickly\nfinding an online student to help and putting them in a collaborative working\nsession with the teacher. The spontaneous, on-demand nature of TeachNow gives\nteachers the flexibility to help whenever their schedule allows.\n  We share our experiences deploying TeachNow as an experimental feature in a\nsix week online CS1 course with 9,000 students and 600 volunteer teachers. Even\nas an optional activity, TeachNow was used by teachers to provide over 12,300\nminutes of 1:1 help to 375 unique students. Through a carefully designed\nrandomised control trial, we show that TeachNow sessions increased student\ncourse retention rate by almost 15%. Moreover, the flexibility of our system\ncaptured valuable volunteer time that would otherwise go to waste. Lastly,\nTeachNow was rated by teachers as one of the most enjoyable and impactful\naspects of their involvement in the course. We believe TeachNow is an important\nstep towards providing more human-centered support in massive online courses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.15177,regular,post_llm,2024,4,"{'ai_likelihood': 2.914004855685764e-06, 'text': 'An Analysis of the Math Requirements of 199 CS BS/BA Degrees at 158 U.S.\n  Universities\n\n  For at least 40 years, there has been debate and disagreement as to the role\nof mathematics in the computer science curriculum. This paper presents the\nresults of an analysis of the math requirements of 199 Computer Science BS/BA\ndegrees from 158 U.S. universities, looking not only at which math courses are\nrequired, but how they are used as prerequisites (and corequisites) for\ncomputer science (CS) courses. Our analysis shows that while there is consensus\nthat discrete math is critical for a CS degree, and further that calculus is\nalmost always required for the BS in CS, there is little consensus as to when a\nstudent should have mastered these subjects. Based on our analysis of how math\nrequirements impact access, retention and on-time degree completion for the BS\nand the BA in CS, we provide several recommendations for CS departments to\nconsider.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.1653,review,post_llm,2024,4,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'On the Political Economy of Link-based Web Search\n\n  Web search engines arguably form the most popular data-driven systems in\ncontemporary society. They wield a considerable power by functioning as\ngatekeepers of the Web, with most user journeys on the Web beginning with them.\nStarting from the late 1990s, search engines have been dominated by the\nparadigm of link-based web search. In this paper, we critically analyze the\npolitical economy of the paradigm of link-based web search, drawing upon\ninsights and methodologies from critical political economy. We draw several\ninsights on how link-based web search has led to phenomena that favor capital\nthrough long-term structural changes on the Web, and how it has led to\naccentuating unpaid digital labor and ecologically unsustainable practices,\namong several others. We show how contemporary observations on the degrading\nquality of link-based web search can be traced back to the internal\ncontradictions with the paradigm, and how such socio-technical phenomena may\nlead to a disutility of the link-based web search model. Our contribution is\nprimarily on enhancing the understanding of the political economy of link-based\nweb search, and laying bare the phenomena at work, and implicitly catalyze the\nsearch for alternative models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.14682,regular,post_llm,2024,4,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Uncovering Name-Based Biases in Large Language Models Through Simulated\n  Trust Game\n\n  Gender and race inferred from an individual's name are a notable source of\nstereotypes and biases that subtly influence social interactions. Abundant\nevidence from human experiments has revealed the preferential treatment that\none receives when one's name suggests a predominant gender or race. As large\nlanguage models acquire more capabilities and begin to support everyday\napplications, it becomes crucial to examine whether they manifest similar\nbiases when encountering names in a complex social interaction. In contrast to\nprevious work that studies name-based biases in language models at a more\nfundamental level, such as word representations, we challenge three prominent\nmodels to predict the outcome of a modified Trust Game, a well-publicized\nparadigm for studying trust and reciprocity. To ensure the internal validity of\nour experiments, we have carefully curated a list of racially representative\nsurnames to identify players in a Trust Game and rigorously verified the\nconstruct validity of our prompts. The results of our experiments show that our\napproach can detect name-based biases in both base and instruction-tuned\nmodels.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.13861,review,post_llm,2024,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Beyond Personhood: Agency, Accountability, and the Limits of\n  Anthropomorphic Ethical Analysis\n\n  What is agency, and why does it matter? In this work, we draw from the\npolitical science and philosophy literature and give two competing visions of\nwhat it means to be an (ethical) agent. The first view, which we term\nmechanistic, is commonly--and implicitly--assumed in AI research, yet it is a\nfundamentally limited means to understand the ethical characteristics of AI.\nUnder the second view, which we term volitional, AI can no longer be considered\nan ethical agent. We discuss the implications of each of these views for two\ncritical questions: first, what the ideal system ought to look like, and\nsecond, how accountability may be achieved. In light of this discussion, we\nultimately argue that, in the context of ethically-significant behavior, AI\nshould be viewed not as an agent but as the outcome of political processes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.15372,regular,post_llm,2024,4,"{'ai_likelihood': 0.00024053785536024306, 'text': ""The TikToking troll and weaponization of conscience: A systems\n  perspective case study\n\n  Cybercrime is a pervasive threat that impacts every facet of society. Its\nreach transcends geographic borders and extends far beyond the digital realm,\noften serving as the catalyst for offline crimes. As modern conflicts become\nincreasingly intertwined with cyber warfare, the need for interdisciplinary\ncooperation to grasp and combat this escalating threat is paramount. This case\nstudy centers around a controversial TikToker, highlighting how the\nweaponization of conscience can be leveraged to manipulate multiple actors\nwithin a propagandist's target population. Weaponization of conscience is a\ntactic used by fraudsters to camouflage their activity, deceive their victims,\nand extend the effectiveness of their modi operandi. Research shows that 95\npercent of cybersecurity incidents are the result of human error and 90 percent\nbegin with a phishing attempt. Honing the capacity to identify and dissect\nstrategies employed by fraudsters along with how individual reactions unfold in\nthe larger system is an essential skill for organizations and individuals to\nsafeguard themselves. Understanding cybercrime and its many interconnected\nsystems requires examination through the lens of complexity science.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.0312,review,post_llm,2024,4,"{'ai_likelihood': 1.0761949751112197e-05, 'text': ""Enhancing Student Engagement in Large-Scale Capstone Courses: An\n  Experience Report\n\n  Computer science (CS) capstone courses offer students a valuable opportunity\nto gain hands-on experience in software development, practice essential soft\nskills, and enhance their employability prospects. They are a core component in\nmany CS undergraduate degrees and address the ACM curricula requirements of\ninculcating professional dispositions in students and making them aware of the\nbroader societal implications of computing. However, coordinating a capstone\ncourse, especially for a large student cohort, can be a daunting task for\nacademic staff. It demands considerable time and energy for planning and\ncoordinating activities between students, academic staff, and any external\nstakeholders. In this experience report, we outline the iterative development\nand refinement of our capstone course as it grew substantially in size over a\nspan of six consecutive sessions. We outline the pedagogies that helped us to\nenhance student engagement and motivation in the course as assessed by\nend-of-course surveys and students' written reflections. We share the lessons\nthat we have learnt and provide recommendations to educators who are designing\nnew capstone courses or looking to scale existing ones.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.06751,regular,post_llm,2024,4,"{'ai_likelihood': 1.0, 'text': 'Leveraging open-source models for legal language modeling and analysis:\n  a case study on the Indian constitution\n\n  In recent years, the use of open-source models has gained immense popularity\nin various fields, including legal language modelling and analysis. These\nmodels have proven to be highly effective in tasks such as summarizing legal\ndocuments, extracting key information, and even predicting case outcomes. This\nhas revolutionized the legal industry, enabling lawyers, researchers, and\npolicymakers to quickly access and analyse vast amounts of legal text, saving\ntime and resources. This paper presents a novel approach to legal language\nmodeling (LLM) and analysis using open-source models from Hugging Face. We\nleverage Hugging Face embeddings via LangChain and Sentence Transformers to\ndevelop an LLM tailored for legal texts. We then demonstrate the application of\nthis model by extracting insights from the official Constitution of India. Our\nmethodology involves preprocessing the data, splitting it into chunks, using\nChromaDB and LangChainVectorStores, and employing the Google/Flan-T5-XXL model\nfor analysis. The trained model is tested on the Indian Constitution, which is\navailable in PDF format. Our findings suggest that our approach holds promise\nfor efficient legal language processing and analysis.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.55419921875, 'GPT4': 0.014495849609375, 'CLAUDE': 0.0005488395690917969, 'GOOGLE': 0.35302734375, 'OPENAI_O_SERIES': 9.500980377197266e-05, 'DEEPSEEK': 2.9206275939941406e-05, 'GROK': 8.255243301391602e-05, 'NOVA': 0.0003407001495361328, 'OTHER': 0.076904296875, 'HUMAN': 1.1682510375976562e-05}}"
2404.09764,regular,post_llm,2024,4,"{'ai_likelihood': 4.8345989651150175e-06, 'text': 'Language-Agnostic Modeling of Wikipedia Articles for Content Quality\n  Assessment across Languages\n\n  Wikipedia is the largest web repository of free knowledge. Volunteer editors\ndevote time and effort to creating and expanding articles in more than 300\nlanguage editions. As content quality varies from article to article, editors\nalso spend substantial time rating articles with specific criteria. However,\nkeeping these assessments complete and up-to-date is largely impossible given\nthe ever-changing nature of Wikipedia. To overcome this limitation, we propose\na novel computational framework for modeling the quality of Wikipedia articles.\n  State-of-the-art approaches to model Wikipedia article quality have leveraged\nmachine learning techniques with language-specific features. In contrast, our\nframework is based on language-agnostic structural features extracted from the\narticles, a set of universal weights, and a language version-specific\nnormalization criterion. Therefore, we ensure that all language editions of\nWikipedia can benefit from our framework, even those that do not have their own\nquality assessment scheme. Using this framework, we have built datasets with\nthe feature values and quality scores of all revisions of all articles in the\nexisting language versions of Wikipedia. We provide a descriptive analysis of\nthese resources and a benchmark of our framework. In addition, we discuss\npossible downstream tasks to be addressed with these datasets, which are\nreleased for public use.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.11844,regular,post_llm,2024,4,"{'ai_likelihood': 3.013345930311415e-06, 'text': 'Finding A Taxi with Illegal Driver Substitution Activity via Behavior\n  Modelings\n\n  In our urban life, Illegal Driver Substitution (IDS) activity for a taxi is a\ngrave unlawful activity in the taxi industry, possibly causing severe traffic\naccidents and painful social repercussions. Currently, the IDS activity is\nmanually supervised by law enforcers, i.e., law enforcers empirically choose a\ntaxi and inspect it. The pressing problem of this scheme is the dilemma between\nthe limited number of law-enforcers and the large volume of taxis. In this\npaper, motivated by this problem, we propose a computational method that helps\nlaw enforcers efficiently find the taxis which tend to have the IDS activity.\nFirstly, our method converts the identification of the IDS activity to a\nsupervised learning task. Secondly, two kinds of taxi driver behaviors, i.e.,\nthe Sleeping Time and Location (STL) behavior and the Pick-Up (PU) behavior are\nproposed. Thirdly, the multiple scale pooling on self-similarity is proposed to\nencode the individual behaviors into the universal features for all taxis.\nFinally, a Multiple Component- Multiple Instance Learning (MC-MIL) method is\nproposed to handle the deficiency of the behavior features and to align the\nbehavior features simultaneously. Extensive experiments on a real-world data\nset shows that the proposed behavior features have a good generalization\nability across different classifiers, and the proposed MC-MIL method suppresses\nthe baseline methods.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.14548,review,post_llm,2024,4,"{'ai_likelihood': 5.066394805908203e-06, 'text': 'Advancing a Consent-Forward Paradigm for Digital Mental Health Data\n\n  The field of digital mental health is advancing at a rapid pace. Passively\ncollected data from user engagements with digital tools and services continue\nto contribute new insights into mental health and illness. As the field of\ndigital mental health grows, a concerning norm has been established -- digital\nservice users are given little say over how their data is collected, shared, or\nused to generate revenue for private companies. Given a long history of service\nuser exclusion from data collection practices, we propose an alternative\napproach that is attentive to this history: the consent-forward paradigm. This\nparadigm embeds principles of affirmative consent in the design of digital\nmental health tools and services, strengthening trust through designing around\nindividual choices and needs, and proactively protecting users from unexpected\nharm. In this perspective, we outline practical steps to implement this\nparadigm, toward ensuring that people searching for care have the safest\nexperiences possible.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.04952,review,post_llm,2024,4,"{'ai_likelihood': 1.6589959462483726e-05, 'text': 'The Impact of Virtual Laboratories on Active Learning and Engagement in\n  Cybersecurity Distance Education\n\n  Virtual Laboratories (V Labs) have in the recent past become part and parcel\nof remote teaching in practical hands-on approaches, particularly in\nCybersecurity distance courses. Their potential is meant to assist learners\nwith hands-on practical laboratory exercises irrespective of geographical\nlocation. Nevertheless, adopting V Labs in didactic approaches in higher\neducation has seen both merits and demerits. Based on this premise, this study\ninvestigates the impact of V Labs on Active Learning (AL) and engagement in\ncybersecurity distance education. A survey with a limited number of learners\nand educators who have had an experience with cybersecurity distance courses\nthat leveraged V Labs in their practical Lab assignment, was conducted at\nBlekinge Tekniska H\\""ogskola, Sweden, to assess the impact of V Labs on AL and\nengagement in Cybersecurity Distance Education. 29% and 73% of the learners and\neducators, respectively responded to the survey administered remotely and with\ngood internal consistency of questionnaires based on the Cronbalch Alpha; the\nresults showed that learners and educators had a positive perception of using V\nLabs to enhance AL in cybersecurity distance education. The key concentration\nof the study was on AL and engagement and problem-solving abilities when V Labs\nare used. Both the learners and educators found the V Labs to be engaging,\ninteractive, and effective in improving their understanding of cybersecurity\nconcepts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11864,regular,post_llm,2024,4,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""Gender Differences in Class Participation in Online versus In-Person\n  Core CS Courses\n\n  The COVID-19 pandemic significantly altered how post-secondary students\nreceive their education. Namely, the transition from an in-person to an online\nclass format changed how students interact with their instructors and their\nclassmates. In this paper, we use student participation scores from two core\ncomputer science classes across ten in-person and three online quarters at a\npublic research university to analyze whether the shift to primarily\nasynchronous online learning has impacted the gender gap in student\nparticipation scores and students' attitudes towards themselves and their\npeers. We observe a shift on the online class forum: in in-person classes,\nmales score higher on average and dominate the top scores while in online\nclasses, male and female students participate at approximately the same rate\nclasswide. To understand what might be driving changes in participation\nbehavior, we analyze survey responses from over a quarter of the students\nenrolled in the online classes. While we find that students of both genders\ntend to compare themselves to their peers less when classes are online, we also\nfind that this trend is much more accentuated for females than males. This data\nsuggests that observed female participation habits in typical in-person classes\nare not inherent gender differences, but rather, a product of the environment.\nTherefore, it is critical the community investigates the root causes of these\nbehavioral differences, and experiments with ways to mitigate them, before we\nsoon return to an in-person format.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11858,regular,post_llm,2024,4,"{'ai_likelihood': 1.4007091522216797e-05, 'text': ""Student Perspectives on Using a Large Language Model (LLM) for an\n  Assignment on Professional Ethics\n\n  The advent of Large Language Models (LLMs) started a serious discussion among\neducators on how LLMs would affect, e.g., curricula, assessments, and students'\ncompetencies. Generative AI and LLMs also raised ethical questions and concerns\nfor computing educators and professionals. This experience report presents an\nassignment within a course on professional competencies, including some related\nto ethics, that computing master's students need in their careers. For the\nassignment, student groups discussed the ethical process by Lennerfors et al.\nby analyzing a case: a fictional researcher considers whether to attend the\nreal CHI 2024 conference in Hawaii. The tasks were (1) to participate in\nin-class discussions on the case, (2) to use an LLM of their choice as a\ndiscussion partner for said case, and (3) to document both discussions,\nreflecting on their use of the LLM. Students reported positive experiences with\nthe LLM as a way to increase their knowledge and understanding, although some\nidentified limitations. The LLM provided a wider set of options for action in\nthe studied case, including unfeasible ones. The LLM would not select a course\nof action, so students had to choose themselves, which they saw as coherent.\nFrom the educators' perspective, there is a need for more instruction for\nstudents using LLMs: some students did not perceive the tools as such but\nrather as an authoritative knowledge base. Therefore, this work has\nimplications for educators considering the use of LLMs as discussion partners\nor tools to practice critical thinking, especially in computing ethics\neducation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11866,review,post_llm,2024,4,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Toward an Artist-Centred AI\n\n  Awareness about the immense impact that artificial intelligence (AI) might\nhave or already has made on the social, economic, political, and cultural\nrealities of our world has become part of the mainstream public discourse.\nAttributes such as ethical, responsible, or explainable emerge as associative\nand descriptive nominal references in guidelines that influence perspectives on\nAI application and development. This paper contextualizes the notions of\nsuitability and desirability of principles, practices, and tools related to the\nuse of AI in the arts. The result is a framework drafted as a set of atomic\nattributes that summarize the values of AI deemed important for artistic\ncreativity. It was composed by examining the challenges that AI poses to art\nproduction, distribution, consumption, and monetization. Considering the\ndifferentiating potentials of AI and taking a perspective aside from the purely\ntechnical ontology, we argue that artistically pertinent AI should be\nunexpected, diversified, affordant, and evolvable.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.09164,regular,post_llm,2024,4,"{'ai_likelihood': 0.93896484375, 'text': 'A computational model for gender asset gap management with a focus on\n  gender disparity in land acquisition and land tenure security\n\n  Gender inequality is a significant concern in many cultures, as women face\nsignificant barriers to asset acquisition particularly land ownership and\ncontrol. Land acquisition and land tenure security are complex issues that\naffect various cultural groups differently, leading to disparities in access\nand ownership especially when superimposed with other socio-economic issues\nlike gender inequality. Measuring the severity of these issues across different\ncultural groups is challenging due to variations in cultural norms,\nexpectations and effectiveness of the measurement framework to correctly assess\nthe level of severity. While nominal measures of gender asset gap provide\nvaluable insights into land acquisition and tenure security issues, they do not\nfully capture the nuances of cultural differences and the impact of\ngovernmental and corporate policies that influence gender disparity in land\nownership and control. The proposed framework aims to fill this gap by\nincorporating cultural and policy factors in developing a new measurement\nframework equipped with a more robust, comprehensive metric to standardize the\napproach to assessing the severity of gender asset disparity in a general sense\nbut with a focus on land acquisition and tenure security to engender more\neffective interventions and policy recommendations.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.7919921875, 'GPT4': 0.149658203125, 'CLAUDE': 0.0014085769653320312, 'GOOGLE': 0.0498046875, 'OPENAI_O_SERIES': 9.697675704956055e-05, 'DEEPSEEK': 4.5299530029296875e-06, 'GROK': 2.682209014892578e-06, 'NOVA': 2.8014183044433594e-06, 'OTHER': 0.00019979476928710938, 'HUMAN': 0.006641387939453125}}"
2404.09065,regular,post_llm,2024,4,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'VRPD-DT: Vehicle Routing Problem with Drones Under Dynamically Changing\n  Traffic Conditions\n\n  The vehicle routing problem with drones (VRP-D) is to determine the optimal\nroutes of trucks and drones such that the total operational cost is minimized\nin a scenario where the trucks work in tandem with the drones to deliver\nparcels to customers. While various heuristic algorithms have been developed to\naddress the problem, existing solutions are built based on simplistic cost\nmodels, overlooking the temporal dynamics of the costs, which fluctuate\ndepending on the dynamically changing traffic conditions. In this paper, we\npresent a novel problem called the vehicle routing problem with drones under\ndynamically changing traffic conditions (VRPD-DT) to address the limitation of\nexisting VRP-D solutions. We design a novel cost model that factors in the\nactual travel distance and projected travel time, computed using a machine\nlearning-driven travel time prediction algorithm. A variable neighborhood\ndescent (VND) algorithm is developed to find the optimal truck-drone routes\nunder the dynamics of traffic conditions through incorporation of the travel\ntime prediction model. A simulation study was performed to evaluate the\nperformance compared with a state-of-the-art VRP-D heuristic solution. The\nresults demonstrate that the proposed algorithm outperforms the\nstate-of-the-art algorithm in various delivery scenarios.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.11029,review,post_llm,2024,4,"{'ai_likelihood': 0.01761542426215278, 'text': ""Student self-management, academic achievement: Exploring the mediating\n  role of self-efficacy and the moderating influence of gender insights from a\n  survey conducted in 3 universities in America\n\n  Excellent students are not only those who master more effective and efficient\nlearning techniques to acquire and apply information. Even in the absence of\ncorrect learning, they are able to self-motivate, evaluate, and adjust their\nbehavior. This study aims to explore the relationship between student\nself-management and academic achievement, with a focus on investigating the\nmediating role of self-efficacy and the moderating influence of gender in this\nrelationship. A total of 289 students from three universities in the United\nStates participated in this research. The results of the study indicate that\nstudents' level of self-management is positively correlated with their academic\nachievement, with self-efficacy playing a mediating role in this relationship\nand gender exerting a certain moderating effect. This study provides important\ninsights into understanding the relationship between student self-management\nand academic achievement and supports the crucial role of educational leaders\nin educational practice.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.13977,review,post_llm,2024,4,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Normalisation de terminologies multilingues pour les TICE : techniques\n  et enjeux\n\n  Terminology and lexicography standardization is a fundamental issue that is\nbecoming increasingly important in the era of multilingual globalization and\nparticularly, from our standpoint, the era of terminotics and translation. The\nchallenges of multilingual globalization and e-semantics directly impact\nstandardization methods: Development and perspectives of standards for\n''Terminology and other language and content resources'' (the title of\nISO-TC37); Development and future of all standardization fields that develop\nterminology (or vocabulary) most often multilingual, serving as the basis for\ntheir development and acting as a reference totheir use. In the first part of\nour presentation, we will first point out the normative aspects of\nstandardization in terminology and especially terminotics. In the second part,\nwe will present a brief overview of terminology standardization projects and\ntheir rationale, In the third part, we will develop the specific issue of ICTE.\nWe will focus on our involvement in this field, on our assumptions and values\nof methods. We will set out our theoretical and technical developments underway\nand will conclude with our needs for collaboration with your academic\ncommunity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.1186,review,post_llm,2024,4,"{'ai_likelihood': 0.00024954477945963543, 'text': 'An ethical study of generative AI from the Actor-Network Theory\n  perspective\n\n  The widespread use of Generative Artificial Intelligence in the innovation\nand generation of communication content is mainly due to its exceptional\ncreative ability, operational efficiency, and compatibility with diverse\nindustries. Nevertheless, this has also sparked ethical problems, such as\nunauthorized access to data, biased decision-making by algorithms, and criminal\nuse of generated content. In order to tackle the security vulnerabilities\nlinked to Generative Artificial Intelligence, we analyze ChatGPT as a case\nstudy within the framework of Actor-Network Theory. We have discovered a total\nof nine actors, including both human and non-human creatures. We examine the\nactors and processes of translation involved in the ethical issues related to\nChatGPT and analyze the key players involved in the emergence of moral issues.\nThe objective is to explore the origins of the ethical issues that arise with\nGenerative Artificial Intelligence and provide a particular perspective on the\ngovernance of Generative Artificial Intelligence.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.11734,regular,post_llm,2024,4,"{'ai_likelihood': 1.7219119601779514e-06, 'text': ""Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To\n  Program Comprehension Questions\n\n  Recent research has explored the creation of questions from code submitted by\nstudents. These Questions about Learners' Code (QLCs) are created through\nprogram analysis, exploring execution paths, and then creating code\ncomprehension questions from these paths and the broader code structure.\nResponding to the questions requires reading and tracing the code, which is\nknown to support students' learning. At the same time, computing education\nresearchers have witnessed the emergence of Large Language Models (LLMs) that\nhave taken the community by storm. Researchers have demonstrated the\napplicability of these models especially in the introductory programming\ncontext, outlining their performance in solving introductory programming\nproblems and their utility in creating new learning resources. In this work, we\nexplore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in\nanswering QLCs that are generated from code that the LLMs have created. Our\nresults show that although the state-of-the-art LLMs can create programs and\ntrace program execution when prompted, they easily succumb to similar errors\nthat have previously been recorded for novice programmers. These results\ndemonstrate the fallibility of these models and perhaps dampen the expectations\nfueled by the recent LLM hype. At the same time, we also highlight future\nresearch possibilities such as using LLMs to mimic students as their behavior\ncan indeed be similar for some specific tasks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.14366,review,post_llm,2024,4,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Lessons Learned in Performing a Trustworthy AI and Fundamental Rights\n  Assessment\n\n  This report shares the experiences, results and lessons learned in conducting\na pilot project ``Responsible use of AI'' in cooperation with the Province of\nFriesland, Rijks ICT Gilde-part of the Ministry of the Interior and Kingdom\nRelations (BZK) (both in The Netherlands) and a group of members of the\nZ-Inspection$^{\\small{\\circledR}}$ Initiative. The pilot project took place\nfrom May 2022 through January 2023. During the pilot, the practical application\nof a deep learning algorithm from the province of Fr\\^yslan was assessed. The\nAI maps heathland grassland by means of satellite images for monitoring nature\nreserves. Environmental monitoring is one of the crucial activities carried on\nby society for several purposes ranging from maintaining standards on drinkable\nwater to quantifying the CO2 emissions of a particular state or region. Using\nsatellite imagery and machine learning to support decisions is becoming an\nimportant part of environmental monitoring. The main focus of this report is to\nshare the experiences, results and lessons learned from performing both a\nTrustworthy AI assessment using the Z-Inspection$^{\\small{\\circledR}}$ process\nand the EU framework for Trustworthy AI, and combining it with a Fundamental\nRights assessment using the Fundamental Rights and Algorithms Impact Assessment\n(FRAIA) as recommended by the Dutch government for the use of AI algorithms by\nthe Dutch public authorities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11867,regular,post_llm,2024,4,"{'ai_likelihood': 0.99755859375, 'text': 'Not as Simple as It Looked: Are We Concluding for Biased Arrest\n  Practices?\n\n  This study examines racial disparities in violent arrest outcomes,\nchallenging conventional methods through a nuanced analysis of Cincinnati\nPolice Department data. Acknowledging the intricate nature of racial disparity,\nthe study categorizes explanations into types of place, types of person, and a\ncombination of both, emphasizing the impact of neighborhood characteristics on\ncrime distribution and police deployment. By introducing alternative scenarios,\nsuch as spuriousness, directed policing, and the geo-concentration of racial\ngroups, the study underscores the complexity of racial disparity calculations.\nEmploying a case study approach, the analysis of violent arrest outcomes\nreveals approximately 40 percent of the observed variation attributed to\nneighborhood-level characteristics, with concentrated disadvantage neutralizing\nthe influence of race on arrest rates. Contrary to expectations, the study\nchallenges the notion of unintentional racism, suggesting that neighborhood\nfactors play a more significant role than the racial composition in explaining\narrests. Policymakers are urged to focus on comprehensive community development\ninitiatives addressing socioeconomic inequalities and support the development\nof robust racial disparity indices. The study calls for nuanced explorations of\nunintentional racism and future research addressing potential limitations,\naiming to enhance understanding of the complexities surrounding racial\ndisparities in arrests.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0007953643798828125, 'GPT4': 0.03076171875, 'CLAUDE': 0.0014257431030273438, 'GOOGLE': 0.96337890625, 'OPENAI_O_SERIES': 0.0013980865478515625, 'DEEPSEEK': 0.0001533031463623047, 'GROK': 4.649162292480469e-06, 'NOVA': 6.216764450073242e-05, 'OTHER': 0.0018711090087890625, 'HUMAN': 8.26716423034668e-05}}"
2404.17095,regular,post_llm,2024,4,"{'ai_likelihood': 0.002101262410481771, 'text': 'The Web unpacked: a quantitative analysis of global Web usage\n\n  This paper presents a comprehensive analysis of global web usage patterns\nbased on data from SimilarWeb, a leading source for estimating web traffic.\nLeveraging a dataset comprising over 250,000 websites, we estimate the total\nweb traffic and investigate its distribution among domains and industry\nsectors. We detail the characteristics of the top 116 domains, which comprise\nan estimated one-third of all web traffic. Our analysis scrutinizes various\nattributes of these domains, including their content sources and types, access\nrequirements, offline presence, and ownership features. Our analysis reveals a\nsignificant concentration of web traffic, with a diminutive number of top\nwebsites capturing the majority of visits. Search engines, news and media,\nsocial networks, streaming, and adult content emerge as primary attractors of\nweb traffic, which is also highly concentrated on platforms and USA-owned\nwebsites. Much of the traffic goes to for-profit but mostly free-of-charge\nwebsites, highlighting the dominance of business models not based on paywalls.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.10374,regular,post_llm,2024,4,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Enjeux normatifs des TICE de l'enseignement des langues dans le contexte\n  arabo-berb{\\`e}re\n\n  E-learning is becoming a global phenomenon. Learning Arabic (or Arabic\ndialects), or learning one or several variants of Berber can be understood from\na very local perspective (in the Maghreb for instance) or in the wider\nframework of the diaspora or even more broadly in a global world context (in\ncase a Japanese or a Russian learns Arabic and Berber). Resources for distance\nlearning must then be created and potentially used in any international\ncultural and linguistic context. This implies that the resources created for\nsuch perspective should cope with the general standards framework of the ISO /\nIEC JTC1SC36, and even beyond the scope of this standardization instance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.01552,review,post_llm,2024,4,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'The use of the open innovation paradigm in the public sector: a\n  systematic review of published studies\n\n  The use of the open innovation paradigm has been, over the past years,\ngetting special attention in the public sector. Motivated by an urban\nenvironment that is increasingly more complex and challenging, several\ngovernment agencies have been allocating financial resources and efforts to\npromote open and participative government initiatives. As a way to try and\nunderstand this scenario, a systematic review of the literature was conducted,\nto provide a comprehensive analysis of the scientific papers that were\npublished, seeking to capture, classify, evaluate and synthesize how the use of\nthis paradigm has been put into practice in the public sector. In total, 4,741\npreliminary studies were analyzed. From this number, only 37 articles were\nclassified as potentially relevant and moved forward, going through the process\nof data extraction and analysis. From the data obtained, it was possible to\nverify that the use of this paradigm started to be reported with a higher\nfrequency in the literature since 2013 and, among the main findings, we\nhighlight the reports of experiences, approach propositions, of understanding\nhow the phenomenon occurs and theoretical reflections. It was also possible to\nverify that the use of open innovation through social media was one of the\npioneer techniques of engagement between the public sector and citizens. In\nconclusion, the reports confirm that the main challenges of this paradigm\napplied to the public sector are associated with their respective bureaucratic\naspects, therefore lacking a bigger reflection on the procedures and methods to\nbe used in the public sphere.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.10206,review,post_llm,2024,4,"{'ai_likelihood': 4.337893591986763e-06, 'text': ""Research and Practice of Delivering Tabletop Exercises\n\n  Tabletop exercises are used to train personnel in the efficient mitigation\nand resolution of incidents. They are applied in practice to support the\npreparedness of organizations and to highlight inefficient processes. Since\ntabletop exercises train competencies required in the workplace, they have been\nintroduced into computing courses at universities as an innovation, especially\nwithin cybersecurity curricula. To help computing educators adopt this\ninnovative method, we survey academic publications that deal with tabletop\nexercises. From 140 papers we identified and examined, we selected 14 papers\nfor a detailed review. The results show that the existing research deals\npredominantly with exercises that follow a linear format and exercises that do\nnot systematically collect data about trainees' learning. Computing education\nresearchers can investigate novel approaches to instruction and assessment in\nthe context of tabletop exercises to maximize the impact of this teaching\nmethod. Due to the relatively low number of published papers, the potential for\nfuture research is immense. Our review provides researchers, tool developers,\nand educators with an orientation in the area, a synthesis of trends, and\nimplications for further work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.17855,review,post_llm,2024,4,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Recontextualized Knowledge and Narrative Coalitions on Telegram\n\n  A defining characteristic of conspiracy texts is that they negotiate power\nand identity by recontextualizing prior knowledge. This dynamic has been shown\nto intensify on social media, where knowledge sources can readily be integrated\ninto antagonistic narratives through hyperlinks. The objective of the present\nchapter is to further our understanding of this dynamic by surfacing and\nexamining 1) how online conspiracy narratives recontextualize prior knowledge\nby coupling it with heterogeneous antagonistic elements, and 2) how such\nrecontextualizing narratives operate as connectors around which diverse actors\nmight form narrative coalitions. To this end, the chapter offers an empirical\nanalysis of links to prior knowledge in public messaging channels from the\nPushshift Telegram dataset. Using transferable methods from the field of\nbibliometrics, we find that politically extreme Telegram channels engage with a\nvariety of established knowledge sources, including scientific journals,\nscientific repositories and other sources associated with the system of\nscholarly communication. Channels engaging with shared knowledge sources\nthereby form narrative coalitions ranging from scientific and technological\nimaginaries to far-right extremist and antisemitic conspiracy theories. Our\nanalysis of these coalitions reveals (i) linguistic, political, and thematic\nforces that shape conspiracy narratives, (ii) emerging ideological,\nepistemological and ontological positions associated with online conspiracism,\nand (iii) how references to shared knowledge contribute to the communicability\nof conspiracy narratives.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.15381,review,post_llm,2024,4,"{'ai_likelihood': 6.622738308376736e-06, 'text': ""Actions francophones autour des normes e-learning \\`a l'ISO\n\n  The future of e-Learning is on the way to be constructed within ICT\nstandardization international instances. The sub-committee 36 of ISO, which is\nresponsible for standardizing educational technologies, is certainly the most\nprominent of all. The authors of this paper, who are official delegates of the\nAgency of French Speaking Universities (AUF) with this structure, highlight the\nstrategic importance of active monitoring of e-Learning standards for\npreserving cultural diversity, linguistic and equal access to education for\nall.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.1259,review,post_llm,2024,4,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'The Files are in the Computer: On Copyright, Memorization, and Generative AI\n\nThe New York Times\'s copyright lawsuit against OpenAI and Microsoft alleges OpenAI\'s GPT models have ""memorized"" NYT articles. Other lawsuits make similar claims. But parties, courts, and scholars disagree on what memorization is, whether it is taking place, and what its copyright implications are. These debates are clouded by ambiguities over the nature of ""memorization."" We attempt to bring clarity to the conversation. We draw on the technical literature to provide a firm foundation for legal discussions, providing a precise definition of memorization: a model has ""memorized"" a piece of training data when (1) it is possible to reconstruct from the model (2) a near-exact copy of (3) a substantial portion of (4) that piece of training data. We distinguish memorization from ""extraction"" (user intentionally causes a model to generate a near-exact copy), from ""regurgitation"" (model generates a near-exact copy, regardless of user intentions), and from ""reconstruction"" (the near-exact copy can be obtained from the model by any means). Several consequences follow. (1) Not all learning is memorization. (2) Memorization occurs when a model is trained; regurgitation is a symptom not its cause. (3) A model that has memorized training data is a ""copy"" of that training data in the sense used by copyright. (4) A model is not like a VCR or other general-purpose copying technology; it is better at generating some types of outputs (possibly regurgitated ones) than others. (5) Memorization is not a phenomenon caused by ""adversarial"" users bent on extraction; it is latent in the model itself. (6) The amount of training data that a model memorizes is a consequence of choices made in training. (7) Whether or not a model that has memorized actually regurgitates depends on overall system design. In a very real sense, memorized training data is in the model--to quote Zoolander, the files are in the computer.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11856,review,post_llm,2024,4,"{'ai_likelihood': 0.0014517042371961807, 'text': 'Google\'s Chrome Antitrust Paradox\n\nThis Article examines Google\'s dominance of the browser market, highlighting how Google\'s Chrome browser plays a critical role in reinforcing Google\'s dominance in other markets. While Google portrays Chrome as a neutral platform built on open-source technologies, this Article shows that Chrome is instrumental in Google\'s strategy to reinforce its dominance in the online advertising, publishing, and browser markets. The examination of Google\'s strategic acquisitions, anticompetitive practices, and implementation of so-called ""privacy controls"" underlines that Chrome is far from a neutral gateway to the web. Rather, it serves as a key tool for Google to maintain and extend its market power, often to the detriment of competition and innovation in the digital economy.\n  This Article illustrates how Chrome not only bolsters Google\'s position in online advertising and publishing through practices such as coercion and self-preferencing, but also leverages its advertising clout to engage in a ""pay-to-play"" paradigm--the cornerstone of Google\'s larger strategy of market control. It also outlines potential regulatory interventions and remedies by drawing on historical antitrust precedents. Lastly, this Article proposes a triad of solutions motivated by an analysis of Google\'s abuse of Chrome, including behavioral remedies targeting specific anticompetitive practices, structural remedies involving an internal separation of Google\'s divisions, and divestiture of Chrome from Google into an independent organization.\n  (Abstract abridged for arXiv. Full abstract available in published version.)', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.12821,regular,post_llm,2024,4,"{'ai_likelihood': 0.0201416015625, 'text': 'Benchmarking the performance of a self-custody, non-ledger-based,\n  obliviously managed digital payment system\n\n  As global governments intensify efforts to operationalize retail central bank\ndigital currencies (CBDCs), the imperative for architectures that preserve user\nprivacy has never been more pronounced. This paper advances an existing retail\nCBDC framework developed at University College London. Utilizing the\ncapabilities of the Comet research framework, our proposed design allows users\nto retain direct custody of their assets without the need for intermediary\nservice providers, all while preserving transactional anonymity. The study\nunveils a novel technique to expedite the retrieval of Proof of Provenance,\nsignificantly accelerating the verification of transaction legitimacy through\nthe refinement of Merkle Trie structures. In parallel, we introduce a\nstreamlined Digital Ledger designed to offer fast, immutable, and decentralized\ntransaction validation within a permissioned ecosystem. The ultimate objective\nof this research is to benchmark the performance of the legacy system\nformulated by the original Comet research team against the newly devised system\nelucidated in this paper. Our endeavour is to establish a foundational design\nfor a scalable national infrastructure proficient in seamlessly processing\nthousands of transactions in real-time, without compromising consumer privacy\nor data integrity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.07476,review,post_llm,2024,4,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'The Survey on Multi-Source Data Fusion in Cyber-Physical-Social\n  Systems:Foundational Infrastructure for Industrial Metaverses and Industries\n  5.0\n\n  As the concept of Industries 5.0 develops, industrial metaverses are expected\nto operate in parallel with the actual industrial processes to offer\n``Human-Centric"" Safe, Secure, Sustainable, Sensitive, Service, and Smartness\n``6S"" manufacturing solutions. Industrial metaverses not only visualize the\nprocess of productivity in a dynamic and evolutional way, but also provide an\nimmersive laboratory experimental environment for optimizing and remodeling the\nprocess. Besides, the customized user needs that are hidden in social media\ndata can be discovered by social computing technologies, which introduces an\ninput channel for building the whole social manufacturing process including\nindustrial metaverses. This makes the fusion of multi-source data cross\nCyber-Physical-Social Systems (CPSS) the foundational and key challenge. This\nwork firstly proposes a multi-source-data-fusion-driven operational\narchitecture for industrial metaverses on the basis of conducting a\ncomprehensive literature review on the state-of-the-art multi-source data\nfusion methods. The advantages and disadvantages of each type of method are\nanalyzed by considering the fusion mechanisms and application scenarios.\nEspecially, we combine the strengths of deep learning and knowledge graphs in\nscalability and parallel computation to enable our proposed framework the\nability of prescriptive optimization and evolution. This integration can\naddress the shortcomings of deep learning in terms of explainability and fact\nfabrication, as well as overcoming the incompleteness and the challenges of\nconstruction and maintenance inherent in knowledge graphs. The effectiveness of\nthe proposed architecture is validated through a parallel weaving case study.\nIn the end, we discuss the challenges and future directions of multi-source\ndata fusion cross CPSS for industrial metaverses and social manufacturing in\nIndustries 5.0.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.1568,review,post_llm,2024,4,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Legitimate Power, Illegitimate Automation: The problem of ignoring\n  legitimacy in automated decision systems\n\n  Progress in machine learning and artificial intelligence has spurred the\nwidespread adoption of automated decision systems (ADS). An extensive\nliterature explores what conditions must be met for these systems' decisions to\nbe fair. However, questions of legitimacy -- why those in control of ADS are\nentitled to make such decisions -- have received comparatively little\nattention. This paper shows that when such questions are raised theorists often\nincorrectly conflate legitimacy with either public acceptance or other\nsubstantive values such as fairness, accuracy, expertise or efficiency. In\nsearch of better theories, we conduct a critical analysis of the philosophical\nliterature on the legitimacy of the state, focusing on consent, public reason,\nand democratic authorisation. This analysis reveals that the prevailing\nunderstanding of legitimacy in analytical political philosophy is also\nill-suited to the task of establishing whether and when ADS are legitimate. The\npaper thus clarifies expectations for theories of ADS legitimacy and charts a\npath for a future research programme on the topic.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.08519,review,post_llm,2024,4,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Non-discrimination law in Europe: a primer for non-lawyers\n\n  This brief paper provides an introduction to non-discrimination law in\nEurope. It answers the questions: What are the key characteristics of\nnon-discrimination law in Europe, and how do the different statutes relate to\none another? Our main target group is computer scientists and users of\nartificial intelligence (AI) interested in an introduction to\nnon-discrimination law in Europe. Notably, non-discrimination law in Europe\ndiffers significantly from non-discrimination law in other countries, such as\nthe US. We aim to describe the law in such a way that non-lawyers and\nnon-European lawyers can easily grasp its contents and challenges. The paper\nshows that the human right to non-discrimination, to some extent, protects\nindividuals against private actors, such as companies. We introduce the EU-wide\nnon-discrimination rules which are included in a number of EU directives, and\nalso explain the difference between direct and indirect discrimination.\nSignificantly, an organization can be fined for indirect discrimination even if\nthe company, or its AI system, discriminated by accident. The last section\nbroadens the horizon to include bias-relevant law and cases from the GDPR, the\nEU AI Act, and related statutes. Finally, we give reading tips for those\ninclined to learn more about non-discrimination law in Europe.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.1057,regular,post_llm,2024,4,"{'ai_likelihood': 3.675619761149089e-06, 'text': 'PAKT: Perspectivized Argumentation Knowledge Graph and Tool for\n  Deliberation Analysis (with Supplementary Materials)\n\n  Deliberative processes play a vital role in shaping opinions, decisions and\npolicies in our society. In contrast to persuasive debates, deliberation aims\nto foster understanding of conflicting perspectives among interested parties.\nThe exchange of arguments in deliberation serves to elucidate viewpoints, to\nraise awareness of conflicting interests, and to finally converge on a\nresolution. To better understand and analyze the underlying processes of\ndeliberation, we propose PAKT, a Perspectivized Argumentation Knowledge Graph\nand Tool. The graph structures the argumentative space across diverse topics,\nwhere arguments i) are divided into premises and conclusions, ii) are annotated\nfor stances, framings and their underlying values and iii) are connected to\nbackground knowledge. We show how to construct PAKT and conduct case studies on\nthe obtained multifaceted argumentation graph. Our findings show the analytical\npotential offered by our framework, highlighting the capability to go beyond\nindividual arguments and to reveal structural patterns in the way participants\nand stakeholders argue in a debate. The overarching goal of our work is to\nfacilitate constructive discourse and informed decision making as a special\nform of argumentation. We offer public access to PAKT and its rich capabilities\nto support analytics, visualizaton, navigation and efficient search, for\ndiverse forms of argumentation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.14,regular,post_llm,2024,4,"{'ai_likelihood': 1.0, 'text': 'Revolutionizing student course selection: Exploring the application\n  prospects and challenges of blockchain token voting technology\n\n  This paper explores the utilization of blockchain token voting technology in\nstudent course selection systems. The current course selection systems face\nvarious issues, which can be mitigated through the implementation of blockchain\ntechnology. The advantages of blockchain technology, including consensus\nmechanisms and smart contracts, are discussed in detail. The token voting\nmechanism, encompassing concepts, token issuance and distribution, and voting\nrules and procedures, is also explained. The system design takes into account\nthe system architecture, user roles and permissions, course information on the\nblockchain, student course selection voting process, and course selection\nresult statistics and public display. The technology offers advantages such as\ntransparency, fairness, data security and privacy protection, and system\nefficiency improvement. However, it also poses several challenges, such as\ntechnological and regulatory hurdles. The prospects for the application of\nblockchain token voting technology in student course selection systems and its\npotential impact on other fields are summarized. Overall, the utilization of\nblockchain token voting technology in student course selection systems holds\npromising future implications, which could revolutionize the education sector.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.8701171875, 'GPT4': 0.058502197265625, 'CLAUDE': 0.00020062923431396484, 'GOOGLE': 0.06292724609375, 'OPENAI_O_SERIES': 0.0003437995910644531, 'DEEPSEEK': 1.2159347534179688e-05, 'GROK': 4.470348358154297e-06, 'NOVA': 1.138448715209961e-05, 'OTHER': 0.00801849365234375, 'HUMAN': 2.002716064453125e-05}}"
2404.0734,review,post_llm,2024,4,"{'ai_likelihood': 0.002109739515516493, 'text': ""RIP Twitter API: A eulogy to its vast research contributions\n\nSince 2006, Twitter's APIs have been rich sources of data for researchers studying social phenomena such as misinformation, public communication, crisis response, and political behavior. However, in 2023, Twitter began heavily restricting data access, dismantling its academic access program, and setting the Enterprise API price at $42,000 per month. Lacking funds to pay this fee, academics are scrambling to continue their research. This study systematically tabulates the number of studies, citations, publication dates, disciplines, and major topics of research using Twitter data between 2006 and 2024. While we cannot know exactly what will be lost now that Twitter data is cost-prohibitive, we can illustrate its research value during the years it was available. A search of eight databases found that between 2006 and 2024, a total of 33,306 studies were published in 8,914 venues, with 610,738 citations across 16 disciplines. Major disciplines include social science, engineering, data science, and public health. Major topics include information dissemination, tweet credibility, research methodologies, event detection, and human behavior. Twitter-based studies increased by a median of 25% annually from 2006 to 2023, but following Twitter's decision to charge for data, the number of studies dropped by 13%. Much of the 2024 research likely used data collected before the API shutdown, suggesting further decline ahead. This trend highlights a growing loss of empirical insight and access to real-time, public communication-raising concerns about the long-term consequences for studying society, technology, and global events in an era increasingly connected by social media."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.08592,regular,post_llm,2024,4,"{'ai_likelihood': 1.7550256517198352e-06, 'text': 'Scarce Resource Allocations That Rely On Machine Learning Should Be\n  Randomized\n\n  Contrary to traditional deterministic notions of algorithmic fairness, this\npaper argues that fairly allocating scarce resources using machine learning\noften requires randomness. We address why, when, and how to randomize by\nproposing stochastic procedures that more adequately account for all of the\nclaims that individuals have to allocations of social goods or opportunities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.10072,regular,post_llm,2024,4,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Debunking Robot Rights Metaphysically, Ethically, and Legally\n\n  In this work we challenge arguments for robot rights on metaphysical, ethical\nand legal grounds. Metaphysically, we argue that machines are not the kinds of\nthings that may be denied or granted rights. Building on theories of\nphenomenology and post-Cartesian approaches to cognitive science, we ground our\nposition in the lived reality of actual humans in an increasingly ubiquitously\nconnected, controlled, digitized, and surveilled society. Ethically, we argue\nthat, given machines current and potential harms to the most marginalized in\nsociety, limits on (rather than rights for) machines should be at the centre of\ncurrent AI ethics debate. From a legal perspective, the best analogy to robot\nrights is not human rights but corporate rights, a highly controversial concept\nwhose most important effect has been the undermining of worker, consumer, and\nvoter rights by advancing the power of capital to exercise outsized influence\non politics and law. The idea of robot rights, we conclude, acts as a smoke\nscreen, allowing theorists and futurists to fantasize about benevolently\nsentient machines with unalterable needs and desires protected by law. While\nsuch fantasies have motivated fascinating fiction and art, once they influence\nlegal theory and practice articulating the scope of rights claims, they\nthreaten to immunize from legal accountability the current AI and robotics that\nis fuelling surveillance capitalism, accelerating environmental destruction,\nand entrenching injustice and human suffering.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.05847,regular,post_llm,2024,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Approaching Emergent Risks: An Exploratory Study into Artificial\n  Intelligence Risk Management within Financial Organisations\n\n  Globally, artificial intelligence (AI) implementation is growing, holding the\ncapability to fundamentally alter organisational processes and decision making.\nSimultaneously, this brings a multitude of emergent risks to organisations,\nexposing vulnerabilities in their extant risk management frameworks. This\nnecessitates a greater understanding of how organisations can position\nthemselves in response. This issue is particularly pertinent within the\nfinancial sector with relatively mature AI applications matched with severe\nsocietal repercussions of potential risk events. Despite this, academic risk\nmanagement literature is trailing behind the speed of AI implementation.\nAdopting a management perspective, this study aims to contribute to the\nunderstanding of AI risk management in organisations through an exploratory\nempirical investigation into these practices. In-depth insights are gained\nthrough interviews with nine practitioners from different organisations within\nthe UK financial sector. Through examining areas of organisational convergence\nand divergence, the findings of this study unearth levels of risk management\nframework readiness and prevailing approaches to risk management at both a\nprocessual and organisational level. Whilst enhancing the developing literature\nconcerning AI risk management within organisations, the study simultaneously\noffers a practical contribution, providing key areas of guidance for\npractitioners in the operational development of AI risk management frameworks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.16244,review,post_llm,2024,4,"{'ai_likelihood': 6.622738308376736e-07, 'text': ""The Ethics of Advanced AI Assistants\n\n  This paper focuses on the opportunities and the ethical and societal risks\nposed by advanced AI assistants. We define advanced AI assistants as artificial\nagents with natural language interfaces, whose function is to plan and execute\nsequences of actions on behalf of a user, across one or more domains, in line\nwith the user's expectations. The paper starts by considering the technology\nitself, providing an overview of AI assistants, their technical foundations and\npotential range of applications. It then explores questions around AI value\nalignment, well-being, safety and malicious uses. Extending the circle of\ninquiry further, we next consider the relationship between advanced AI\nassistants and individual users in more detail, exploring topics such as\nmanipulation and persuasion, anthropomorphism, appropriate relationships, trust\nand privacy. With this analysis in place, we consider the deployment of\nadvanced assistants at a societal scale, focusing on cooperation, equity and\naccess, misinformation, economic impact, the environment and how best to\nevaluate advanced AI assistants. Finally, we conclude by providing a range of\nrecommendations for researchers, developers, policymakers and public\nstakeholders.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.0475,review,post_llm,2024,4,"{'ai_likelihood': 7.165802849663628e-05, 'text': 'Now, Later, and Lasting: Ten Priorities for AI Research, Policy, and\n  Practice\n\n  Advances in artificial intelligence (AI) will transform many aspects of our\nlives and society, bringing immense opportunities but also posing significant\nrisks and challenges. The next several decades may well be a turning point for\nhumanity, comparable to the industrial revolution. We write to share a set of\nrecommendations for moving forward from the perspective of the founder and\nleaders of the One Hundred Year Study on AI. Launched a decade ago, the project\nis committed to a perpetual series of studies by multidisciplinary experts to\nevaluate the immediate, longer-term, and far-reaching effects of AI on people\nand society, and to make recommendations about AI research, policy, and\npractice. As we witness new capabilities emerging from neural models, it is\ncrucial that we engage in efforts to advance our scientific understanding of\nthese models and their behaviors. We must address the impact of AI on people\nand society through technical, social, and sociotechnical lenses, incorporating\ninsights from a diverse range of experts including voices from engineering,\nsocial, behavioral, and economic disciplines. By fostering dialogue,\ncollaboration, and action among various stakeholders, we can strategically\nguide the development and deployment of AI in ways that maximize its potential\nfor contributing to human flourishing. Despite the growing divide in the field\nbetween focusing on short-term versus long-term implications, we think both are\nof critical importance. As Alan Turing, one of the pioneers of AI, wrote in\n1950, ""We can only see a short distance ahead, but we can see plenty there that\nneeds to be done."" We offer ten recommendations for action that collectively\naddress both the short- and long-term potential impacts of AI technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.00139,regular,post_llm,2024,4,"{'ai_likelihood': 1.0, 'text': ""AfricAIED 2024: 2nd Workshop on Artificial Intelligence in Education in\n  Africa\n\n  Recent AI advancements offer transformative potential for global education,\nyet their application often overlooks Africa's unique educational landscape.\nAfricAIED 2024 will address this gap, spotlighting efforts to develop AI in\nEducation (AIED) systems tailored to Africa's needs. Building on the success of\nthe inaugural workshop, AfricAIED 2024 will feature an online AI Hackathon\nfocused on democratizing preparation for Ghana's National Science & Maths Quiz\n(NSMQ). Participants will create open-source AI tools leveraging resources from\nthe Brilla AI project to level the academic playing field and enhance science\nand math education across Africa. The workshop will showcase top competitors'\nsolutions, invite discussions on AIED opportunities and challenges in Africa,\nand highlight the latest advancements in AI education integration. AfricAIED\n2024 aims to foster collaboration and innovation, amplifying African voices in\nthe AIED community and driving positive change in African education through AI.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00023698806762695312, 'GPT4': 0.0013561248779296875, 'CLAUDE': 0.00063323974609375, 'GOOGLE': 0.9736328125, 'OPENAI_O_SERIES': 0.0163116455078125, 'DEEPSEEK': 0.005199432373046875, 'GROK': 6.67572021484375e-06, 'NOVA': 7.909536361694336e-05, 'OTHER': 0.0027027130126953125, 'HUMAN': 6.74128532409668e-05}}"
2404.19371,review,post_llm,2024,4,"{'ai_likelihood': 0.0, 'text': 'Fairness in AI: challenges in bridging the gap between algorithms and\n  law\n\n  In this paper we examine algorithmic fairness from the perspective of law\naiming to identify best practices and strategies for the specification and\nadoption of fairness definitions and algorithms in real-world systems and use\ncases. We start by providing a brief introduction of current\nanti-discrimination law in the European Union and the United States and\ndiscussing the concepts of bias and fairness from an legal and ethical\nviewpoint. We then proceed by presenting a set of algorithmic fairness\ndefinitions by example, aiming to communicate their objectives to non-technical\naudiences. Then, we introduce a set of core criteria that need to be taken into\naccount when selecting a specific fairness definition for real-world use case\napplications. Finally, we enumerate a set of key considerations and best\npractices for the design and employment of fairness methods on real-world AI\napplications\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.14846,regular,post_llm,2024,4,"{'ai_likelihood': 3.2318962944878476e-05, 'text': 'Beyond Trial-and-Error: Predicting User Abandonment After a Moderation\n  Intervention\n\n  Current content moderation follows a reactive, trial-and-error approach,\nwhere interventions are applied and their effects are only measured post-hoc.\nIn contrast, we introduce a proactive, predictive approach that enables\nmoderators to anticipate the impact of their actions before implementation. We\npropose and tackle the new task of predicting user abandonment following a\nmoderation intervention. We study the reactions of 16,540 users to a massive\nban of online communities on Reddit, training a set of binary classifiers to\nidentify those users who would abandon the platform after the intervention -- a\nproblem of great practical relevance. We leverage a dataset of 13.8 million\nposts to compute a large and diverse set of 142 features, which convey\ninformation about the activity, toxicity, relations, and writing style of the\nusers. We obtain promising results, with the best-performing model achieving\nmicro F1-score = 0.914. Our model shows robust generalizability when applied to\nusers from previously unseen communities. Furthermore, we identify activity\nfeatures as the most informative predictors, followed by relational and\ntoxicity features, while writing style features exhibit limited utility.\nTheoretically, our results demonstrate the feasibility of adopting a predictive\nmachine learning approach to estimate the effects of moderation interventions.\nPractically, this work marks a fundamental shift from reactive to predictive\nmoderation, equipping platform administrators with intelligent tools to\nstrategically plan interventions, minimize unintended consequences, and\noptimize user engagement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.04059,review,post_llm,2024,4,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'On the Quest for Effectiveness in Human Oversight: Interdisciplinary\n  Perspectives\n\n  Human oversight is currently discussed as a potential safeguard to counter\nsome of the negative aspects of high-risk AI applications. This prompts a\ncritical examination of the role and conditions necessary for what is\nprominently termed effective or meaningful human oversight of these systems.\nThis paper investigates effective human oversight by synthesizing insights from\npsychological, legal, philosophical, and technical domains. Based on the claim\nthat the main objective of human oversight is risk mitigation, we propose a\nviable understanding of effectiveness in human oversight: for human oversight\nto be effective, the oversight person has to have (a) sufficient causal power\nwith regard to the system and its effects, (b) suitable epistemic access to\nrelevant aspects of the situation, (c) self-control, and (d) fitting intentions\nfor their role. Furthermore, we argue that this is equivalent to saying that an\noversight person is effective if and only if they are morally responsible and\nhave fitting intentions. Against this backdrop, we suggest facilitators and\ninhibitors of effectiveness in human oversight when striving for practical\napplicability. We discuss factors in three domains, namely, the technical\ndesign of the system, individual factors of oversight persons, and the\nenvironmental circumstances in which they operate. Finally, this paper\nscrutinizes the upcoming AI Act of the European Union -- in particular Article\n14 on Human Oversight -- as an exemplary regulatory framework in which we study\nthe practicality of our understanding of effective human oversight. By\nanalyzing the provisions and implications of the European AI Act proposal, we\npinpoint how far that proposal aligns with our analyses regarding effective\nhuman oversight as well as how it might get enriched by our conceptual\nunderstanding of effectiveness in human oversight.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.05495,regular,post_llm,2024,4,"{'ai_likelihood': 1.4106432596842449e-05, 'text': ""Decisioning Workshop 2023\n\n  In a knowledge society, the term knowledge must be considered a core resource\nfor organizations. So, beyond being a medium to progress and to innovate,\nknowledge is one of our most important resources: something necessary to\ndecide.Organizations that are embracing knowledge retention activities are\ngaining a competitive advantage. Organizational rearrangements from companies,\nnotably outsourcing, increase a possible loss of knowledge, making knowledge\nretention an essential need for them. When Knowledge is less shared,\ncollaborative decision-making seems harder to obtain insofar as a\n``communication breakdown'' characterizes participants' discourse. At best,\nstakeholders have to finda consensus according to their knowledge. Sharing\nknowledge ensures its retention and catalyzes the construction of this\nconsensus.\n  Our vision of collaborative decision-making aims not only at increasing the\nquality of the first parts of the decision-making process: intelligence and\ndesign, but also at increasing the acceptance of the choice. Intelligence and\ndesign will be done by more than one individual and constructed together; the\ndecision is more easily accepted. The decided choice will then be shared.\nThereby where decision-making could be seen as a constructed model,\ncollaborative decision-making, for us,is seen as the use of socio-technical\nmedia to improve decision-making performance and acceptability. The shared\ndecision making is a core activity in a lot of human activities. For example,\nthe sustainable decision-making is the job of not only governments and\ninstitutions but also broader society. Recognizing the urgent need for\nsustainability, we can argue that to realize sustainable development, it must\nbe considered as a decision-making strategy. The location of knowledge in the\nrealization of collaborative decision-making has to be regarded insofar as\nknowledge sharing leads to improve collaborative decision-making: a ``static\nview'' has to be structured and constitutes the ``collaborative knowledge.''\nKnowledge has an important role in individual decision-making, and we consider\nthat for collaborative decision-making, knowledge has to be shared. What is\nrequired is a better understanding of the nature of group work''. Knowledge has\nto be shared, but how do we share knowledge?\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11861,review,post_llm,2024,4,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Digital Transformation of Education, Systems Approach and Applied\n  Research\n\n  This article proposes the construction of a systemic model of digital\neducation as part of research applied to public policy (French Ministry of\nEducation). Considering the digital domain in its pervasiveness, it highlights\nthe importance of a complex approach to understanding the transformation of\npractices. As an applied research modality, we present digital theme groups\n(GTnum). The methodological approach combines a reflexive posture informed by\nresearch contributions, conceptual choices centered on digital humanities and\nthe systems approach, participatory research and open science via the\nHypotheses ''Education, digital and research'' notebook. As a result, our\nmodeling is centered on a ''digital environment'' and six units of action put\nto the test via the GTnum themes. We interpret these results through a\ncomparison with other systemic frameworks, an application to the axes of\ndigital transformation in academies, a prospective reflection with the\ndevelopment of generative AI and perspectives for participatory research.\nFinally, the article discusses the limits and contributions of this approach:\nvariability in the understanding of the issues at stake and in the integration\nof research contributions, as well as avenues for anticipating a new digital\nconfiguration with the place of AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.05207,review,post_llm,2024,4,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Review Helpfulness Scores vs. Review Unhelpfulness Scores: Two Sides of\n  the Same Coin or Different Coins?\n\n  Evaluating the helpfulness of online reviews supports consumers who must sift\nthrough large volumes of online reviews. Online review platforms have\nincreasingly adopted review evaluating systems, which let users evaluate\nwhether reviews are helpful or not; in turn, these evaluations assist review\nreaders and encourage review contributors. Although review helpfulness scores\nhave been studied extensively in the literature, our knowledge regarding their\ncounterpart, review unhelpfulness scores, is lacking. Addressing this gap in\nthe literature is important because researchers and practitioners have assumed\nthat unhelpfulness scores are driven by intrinsic review characteristics and\nthat such scores are associated with low-quality reviews. This study validates\nthis conventional wisdom by examining factors that influence unhelpfulness\nscores. We find that, unlike review helpfulness scores, unhelpfulness scores\nare generally not driven by intrinsic review characteristics, as almost none of\nthem are statistically significant predictors of an unhelpfulness score. We\nalso find that users who receive review unhelpfulness votes are more likely to\ncast unhelpfulness votes for other reviews. Finally, unhelpfulness voters\nengage much less with the platform than helpfulness voters do. In summary, our\nfindings suggest that review unhelpfulness scores are not driven by intrinsic\nreview characteristics. Therefore, helpfulness and unhelpfulness scores should\nnot be considered as two sides of the same coin.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.04684,regular,post_llm,2024,4,"{'ai_likelihood': 9.967221154106988e-06, 'text': ""Algorithmic Misjudgement in Google Search Results: Evidence from\n  Auditing the US Online Electoral Information Environment\n\n  Google Search is an important way that people seek information about\npolitics, and Google states that it is ``committed to providing timely and\nauthoritative information on Google Search to help voters understand, navigate,\nand participate in democratic processes.'' This paper studies the extent to\nwhich government-maintained web domains are represented in the online electoral\ninformation environment, as captured through 3.45 Google Search result pages\ncollected during the 2022 US midterm elections for 786 locations across the\nUnited States. Focusing on state, county, and local government domains that\nprovide locality-specific information, we study not only the extent to which\nthese sources appear in organic search results, but also the extent to which\nthese sources are correctly targeted to their respective constituents. We label\nmisalignment between the geographic area that non-federal domains serve and the\nlocations for which they appear in search results as algorithmic mistargeting,\na subtype of algorithmic misjudgement in which the search algorithm targets\nlocality-specific information to users in different (incorrect) locations. In\nthe context of the 2022 US midterm elections, we find that 71% of all\noccurrences of state, county, and local government sources were mistargeted,\nwith some domains appearing disproportionately often among organic results\ndespite providing locality-specific information that may not be relevant to all\nvoters. However, we also find that mistargeting often occurs in low ranks. We\nconclude by considering the potential consequences of extensive mistargeting of\nnon-federal government sources and argue that ensuring the correct targeting of\nthese sources to their respective constituents is a critical part of Google's\nrole in facilitating access to authoritative and locally-relevant electoral\ninformation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.05179,regular,post_llm,2024,4,"{'ai_likelihood': 2.221928702460395e-05, 'text': ""Rapid Virtual Simulations: Achieving 'Satisficing Learning Impact' with\n  'Realistic-Enough' Activities in Health Science Education\n\n  This manuscript introduces the concept of Rapid Virtual Simulations, a new\ntechno-pedagogical activity that fosters expert autonomy for creating virtual\neducational simulations. It is grounded in a Realistic-Enough Philosophy that\nconsists of pursuing the development of the least complex simulation while\nstill ensuring a Satisficing (or good enough) Learning Impact. It also\nintroduces the concept of a Rapid Virtual Simulation Ecosystem as an integrated\nset of technological modules that facilitates the work of health professional\neducators while multiplying educational affordances for learners. Finally, this\nmanuscript presents an argument for technological agility and simplicity as key\nguiding principles for the design of future simulation-based educational\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.05836,review,post_llm,2024,4,"{'ai_likelihood': 0.0014612409803602432, 'text': ""Hiden Topics in Robotic Process Automation -- an Approach based on AI\n\nRobotic Process Automation (RPA) has rapidly evolved into a widely recognized and influential software technology. Its growing relevance has sparked diverse research efforts across various disciplines. This study aims to map the scientific landscape of RPA by identifying key thematic areas, tracking their development over time, and assessing their academic impact. To achieve this, we apply an unsupervised machine learning technique Latent Dirichlet Allocation (LDA) to analyze the abstracts of over 2,000 scholarly articles. Our analysis reveals 100 distinct research topics, with 15 of the most prominent themes featured in a science map designed to support future exploration and understanding of RPA's expanding research frontier."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.03951,regular,post_llm,2024,4,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'A Conceptual Design of In-Game Real and Virtual Currency Tracker\n\n  The gaming industry is earning huge revenues from incorporating virtual\ncurrencies into the game design experience. Even if it is a useful approach for\nthe game industry to boost up their earnings, the unidirectional and\nbidirectional in-game virtual currencies can invoke inadequate gaming behaviors\nand additions among players. The market lacks gaming and customer protection\nregulations to avoid the financial, behavioral, and psychological exploitation\nof users. Therefore, it is needed to develop visual or textual interface design\nrecommendations that help the game players keep balance in their spending and\nimprove their gaming behavior. This paper presents a conceptual design of an\nin-game purchasing module that allows the user to observe their real time\nspendings in relation to virtual currency buying.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.09041,regular,post_llm,2024,4,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Three Disclaimers for Safe Disclosure: A Cardwriter for Reporting the\n  Use of Generative AI in Writing Process\n\n  Generative artificial intelligence (AI) and large language models (LLMs) are\nincreasingly being used in the academic writing process. This is despite the\ncurrent lack of unified framework for reporting the use of machine assistance.\nIn this work, we propose ""Cardwriter"", an intuitive interface that produces a\nshort report for authors to declare their use of generative AI in their writing\nprocess. The demo is available online, at https://cardwriter.vercel.app\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.09479,review,post_llm,2024,4,"{'ai_likelihood': 3.112687004937066e-06, 'text': ""A Legal Risk Taxonomy for Generative Artificial Intelligence\n\n  For the first time, this paper presents a taxonomy of legal risks associated\nwith generative AI (GenAI) by breaking down complex legal concepts to provide a\ncommon understanding of potential legal challenges for developing and deploying\nGenAI models. The methodology is based on (1) examining the legal claims that\nhave been filed in existing lawsuits and (2) evaluating the reasonably\nforeseeable legal claims that may be filed in future lawsuits. First, we\nidentified 29 lawsuits against prominent GenAI entities and tallied the claims\nof each lawsuit. From there, we identified seven claims that are cited at least\nfour times across these lawsuits as the most likely claims for future GenAI\nlawsuits. For each of these seven claims, we describe the elements of the claim\n(what the plaintiff must prove to prevail) and provide an example of how it may\napply to GenAI. Next, we identified 30 other potential claims that we consider\nto be more speculative, because they have been included in fewer than four\nlawsuits or have yet to be filed. We further separated those 30 claims into 19\nthat are most likely to be made in relation to pre-deployment of GenAI models\nand 11 that are more likely to be made in connection with post-deployment of\nGenAI models since the legal risks will vary between entities that create\nversus deploy them. For each of these claims, we describe the elements of the\nclaim and the potential remedies that plaintiffs may seek to help entities\ndetermine their legal risks in developing or deploying GenAI. Lastly, we close\nthe paper by noting the novelty of GenAI technology and propose some\napplications for the paper's taxonomy in driving further research.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.1099,regular,post_llm,2024,4,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'Automating Personalized Parsons Problems with Customized Contexts and\n  Concepts\n\n  Parsons problems provide useful scaffolding for introductory programming\nstudents learning to write code. However, generating large numbers of\nhigh-quality Parsons problems that appeal to the diverse range of interests in\na typical introductory course is a significant challenge for educators. Large\nlanguage models (LLMs) may offer a solution, by allowing students to produce\non-demand Parsons problems for topics covering the breadth of the introductory\nprogramming curriculum, and targeting thematic contexts that align with their\npersonal interests. In this paper, we introduce PuzzleMakerPy, an educational\ntool that uses an LLM to generate unlimited contextualized drag-and-drop\nprogramming exercises in the form of Parsons Problems, which introductory\nprogrammers can use as a supplemental learning resource. We evaluated\nPuzzleMakerPy by deploying it in a large introductory programming course, and\nfound that the ability to personalize the contextual framing used in problem\ndescriptions was highly engaging for students, and being able to customize the\nprogramming topics was reported as being useful for their learning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.15383,review,post_llm,2024,4,"{'ai_likelihood': 3.970331615871853e-05, 'text': ""Artificial Intelligence, VR, AR and Metaverse Technologies for Human\n  Resources Management\n\n  Human Resources (HR) technology solutions encompass software and hardware\ntools designed to automate HR processes, gather, process, and analyze data,\nutilize it for strategic decision-making, and execute HR professionals' tasks\nwhile prioritizing security and privacy considerations. As with numerous other\ndomains, Digital Transformation and emerging technologies have commenced\nintegration into HR processes. These technologies are utilized by HR\nprofessionals and various stakeholders involved in HR operations. This study\nevaluates the utilization of Artificial Intelligence (AI), Virtual Reality\n(VR), Augmented Reality (VR), and the Metaverse within HR management, focusing\non current trends and potential opportunities. A survey was conducted to gauge\nHR professionals' perceptions and critiques regarding these technologies.\nParticipants were the HR department officers, academicians who specialized in\nHR and staff who had courses at diverse levels about HR. The acquired results\nwere subjected to comparative analysis within this article.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2404.18075,review,post_llm,2024,4,"{'ai_likelihood': 7.192293802897136e-05, 'text': 'Comparing E-bike and Conventional Bicycle Use Patterns in a Public Bike\n  Share System: A Case Study of Richmond, VA\n\n  The results show that pedelecs are generally associated with longer trip\ndistances, shorter trip times, higher speeds, and lower rates of uphill\nelevation change. The origin-destination analysis considering the business,\nmixed use, residential, and other uses shows extremely similar trends, with a\nlarge number of trips staying within either business or residential locations\nor mixed use. The roadway use analysis shows that pedelecs are used farther\noutside of the city than bikes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.00225,review,post_llm,2024,4,"{'ai_likelihood': 1.0, 'text': ""Hacia una implementaci\\'on \\'etica e inclusiva de la Inteligencia\n  Artificial en las organizaciones: un marco multidimensional\n\n  The article analyzes the impact of artificial intelligence (AI) on\ncontemporary society and the importance of adopting an ethical approach to its\ndevelopment and implementation within organizations. It examines the critical\nperspective of French philosopher \\'Eric Sadin and others, who warn of the\nrisks of unbridled technologization that can erode human autonomy. However, the\narticle also recognizes the active role that various actors, such as\ngovernments, academics and civil society, can play in shaping the development\nof AI aligned with human and social values. A multidimensional approach is\nproposed that combines ethics with regulation, innovation and education. It\nhighlights the importance of developing detailed ethical frameworks,\nincorporating ethics in the training of professionals, conducting ethical\nimpact audits, and encouraging stakeholder participation in AI design. In\naddition, four fundamental pillars for the ethical implementation of AI in\norganizations are presented: 1) Integrated values, 2) Trust and transparency,\n3) Empowering human growth, and 4) Identifying strategic factors. These pillars\ncover aspects such as alignment with the company's ethical identity, governance\nand accountability, human-centered design, continuous training and adaptability\nin the face of technological and market changes. It concludes by emphasizing\nthat ethics must be the cornerstone of the strategy of any organization that\naspires to incorporate AI, establishing a solid framework to ensure that the\ntechnology is developed and used in a way that respects and promotes human\nvalues.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00457763671875, 'GPT4': 0.00672149658203125, 'CLAUDE': 0.875, 'GOOGLE': 0.10235595703125, 'OPENAI_O_SERIES': 0.001735687255859375, 'DEEPSEEK': 0.0011358261108398438, 'GROK': 3.337860107421875e-06, 'NOVA': 0.00018084049224853516, 'OTHER': 0.007335662841796875, 'HUMAN': 0.0009455680847167969}}"
2404.07612,regular,post_llm,2024,4,"{'ai_likelihood': 3.311369154188368e-06, 'text': ""Measuring Geographic Diversity of Foundation Models with a Natural\n  Language--based Geo-guessing Experiment on GPT-4\n\n  Generative AI based on foundation models provides a first glimpse into the\nworld represented by machines trained on vast amounts of multimodal data\ningested by these models during training. If we consider the resulting models\nas knowledge bases in their own right, this may open up new avenues for\nunderstanding places through the lens of machines. In this work, we adopt this\nthinking and select GPT-4, a state-of-the-art representative in the family of\nmultimodal large language models, to study its geographic diversity regarding\nhow well geographic features are represented. Using DBpedia abstracts as a\nground-truth corpus for probing, our natural language--based geo-guessing\nexperiment shows that GPT-4 may currently encode insufficient knowledge about\nseveral geographic feature types on a global level. On a local level, we\nobserve not only this insufficiency but also inter-regional disparities in\nGPT-4's geo-guessing performance on UNESCO World Heritage Sites that carry\nsignificance to both local and global populations, and the inter-regional\ndisparities may become smaller as the geographic scale increases. Morever,\nwhether assessing the geo-guessing performance on a global or local level, we\nfind inter-model disparities in GPT-4's geo-guessing performance when comparing\nits unimodal and multimodal variants. We hope this work can initiate a\ndiscussion on geographic diversity as an ethical principle within the GIScience\ncommunity in the face of global socio-technical challenges.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.21015,regular,post_llm,2024,5,"{'ai_likelihood': 6.695588429768881e-05, 'text': 'The rising costs of training frontier AI models\n\n  The costs of training frontier AI models have grown dramatically in recent\nyears, but there is limited public data on the magnitude and growth of these\nexpenses. This paper develops a detailed cost model to address this gap,\nestimating training costs using three approaches that account for hardware,\nenergy, cloud rental, and staff expenses. The analysis reveals that the\namortized cost to train the most compute-intensive models has grown\nprecipitously at a rate of 2.4x per year since 2016 (90% CI: 2.0x to 2.9x). For\nkey frontier models, such as GPT-4 and Gemini, the most significant expenses\nare AI accelerator chips and staff costs, each costing tens of millions of\ndollars. Other notable costs include server components (15-22%), cluster-level\ninterconnect (9-13%), and energy consumption (2-6%). If the trend of growing\ndevelopment costs continues, the largest training runs will cost more than a\nbillion dollars by 2027, meaning that only the most well-funded organizations\nwill be able to finance frontier AI models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.19699,review,post_llm,2024,5,"{'ai_likelihood': 0.3876410590277778, 'text': ""Fairness in AI-Driven Recruitment: Challenges, Metrics, Methods, and Future Directions\n\nThe recruitment process significantly impacts an organization's performance, productivity, and culture. Traditionally, human resource experts and industrial-organizational psychologists have developed systematic hiring methods, including job advertising, candidate skill assessments, and structured interviews to ensure candidate-organization fit. Recently, recruitment practices have shifted dramatically toward artificial intelligence (AI)-based methods, driven by the need to efficiently manage large applicant pools. However, reliance on AI raises concerns about the amplification and propagation of human biases embedded within hiring algorithms, as empirically demonstrated by biases in candidate ranking systems and automated interview assessments. Consequently, algorithmic fairness has emerged as a critical consideration in AI-driven recruitment, aimed at rigorously addressing and mitigating these biases. This paper systematically reviews biases identified in AI-driven recruitment systems, categorizes fairness metrics and bias mitigation techniques, and highlights auditing approaches used in practice. We emphasize critical gaps and current limitations, proposing future directions to guide researchers and practitioners toward more equitable AI recruitment practices, promoting fair candidate treatment and enhancing organizational outcomes."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.11712,regular,post_llm,2024,5,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Trust, Because You Can't Verify:Privacy and Security Hurdles in\n  Education Technology Acquisition Practices\n\n  The education technology (EdTech) landscape is expanding rapidly in higher\neducation institutes (HEIs). This growth brings enormous complexity. Protecting\nthe extensive data collected by these tools is crucial for HEIs as data\nbreaches and misuses can have dire security and privacy consequences on the\ndata subjects, particularly students, who are often compelled to use these\ntools. This urges an in-depth understanding of HEI and EdTech vendor dynamics,\nwhich is largely understudied.\n  To address this gap, we conducted a semi-structured interview study with 13\nparticipants who are in EdTech leadership roles at seven HEIs. Our study\nuncovers the EdTech acquisition process in the HEI context, the consideration\nof security and privacy issues throughout that process, the pain points of HEI\npersonnel in establishing adequate protection mechanisms in service contracts,\nand their struggle in holding vendors accountable due to a lack of visibility\ninto their system and power-asymmetry, among other reasons. We discuss certain\nobservations about the status quo and conclude with recommendations for HEIs,\nresearchers, and regulatory bodies to improve the situation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.14323,regular,post_llm,2024,5,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'SmartCS: Enabling the Creation of ML-Powered Computer Vision Mobile Apps\n  for Citizen Science Applications without Coding\n\n  It is undeniable that citizen science contributes to the advancement of\nvarious fields of study. There are now software tools that facilitate the\ndevelopment of citizen science apps. However, apps developed with these tools\nrely on individual human skills to correctly collect useful data. Machine\nlearning (ML)-aided apps provide on-field guidance to citizen scientists on\ndata collection tasks. However, these apps rely on server-side ML support, and\ntherefore need a reliable internet connection. Furthermore, the development of\ncitizen science apps with ML support requires a significant investment of time\nand money. For some projects, this barrier may preclude the use of citizen\nscience effectively. We present a platform that democratizes citizen science by\nmaking it accessible to a much broader audience of both researchers and\nparticipants. The SmartCS platform allows one to create citizen science apps\nwith ML support quickly and without coding skills. Apps developed using SmartCS\nhave client-side ML support, making them usable in the field, even when there\nis no internet connection. The client-side ML helps educate users to better\nrecognize the subjects, thereby enabling high-quality data collection. We\npresent several citizen science apps created using SmartCS, some of which were\nconceived and created by high school students.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.18179,review,post_llm,2024,5,"{'ai_likelihood': 1.0, 'text': ""Rethinking the A in STEAM: Insights from and for AI Literacy Education\n\n  This article rethinks the role of arts in STEAM education, emphasizing its\nimportance in AI literacy within K-12 contexts. Arguing against the\nmarginalization of arts, the paper is structured around four key domains:\nlanguage studies, philosophy, social studies, and visual arts. Each section\naddresses critical AI-related phenomena and provides pedagogical strate-gies\nfor effective integration into STEAM education. Language studies focus on media\nrepresentations and the probabilistic nature of AI language models. The\nphilosophy section examines anthropomorphism, ethics, and the misconstrued\nhuman-like capabilities of AI. Social studies discuss AI's societal impacts,\nbiases, and ethical considerations in data prac-tices. Visual arts explore the\nimplications of generative AI on artistic processes and intellec-tual property.\nThe article concludes by advocating for a robust inclusion of arts in STEAM to\nfoster a holistic, equitable, and sustainable understanding of AI, ultimately\ninspiring technologies that promote fairness and creativity.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0028896331787109375, 'GPT4': 0.46142578125, 'CLAUDE': 0.0302886962890625, 'GOOGLE': 0.45654296875, 'OPENAI_O_SERIES': 0.00983428955078125, 'DEEPSEEK': 0.01568603515625, 'GROK': 0.003353118896484375, 'NOVA': 0.0008473396301269531, 'OTHER': 0.0188446044921875, 'HUMAN': 0.0002428293228149414}}"
2405.20037,review,post_llm,2024,5,"{'ai_likelihood': 0.1071506076388889, 'text': ""Linguistic Landscape of Generative AI Perception: A Global Twitter\n  Analysis Across 14 Languages\n\n  The advent of generative AI tools has had a profound impact on societies\nglobally, transcending geographical boundaries. Understanding these tools'\nglobal reception and utilization is crucial for service providers and\npolicymakers in shaping future policies. Therefore, to unravel the perceptions\nand engagements of individuals within diverse linguistic communities with\nregard to generative AI tools, we extensively analyzed over 6.8 million tweets\nin 14 different languages. Our findings reveal a global trend in the perception\nof generative AI, accompanied by language-specific nuances. While sentiments\ntoward these tools vary significantly across languages, there is a prevalent\npositive inclination toward Image tools and a negative one toward Chat tools.\nNotably, the ban of ChatGPT in Italy led to a sentiment decline and initiated\ndiscussions across languages. Furthermore, we established a taxonomy for\ninteractions with chatbots, creating a framework for social analysis\nunderscoring variations in generative AI usage among linguistic communities. We\nfind that the Chinese community predominantly employs chatbots as substitutes\nfor search, while the Italian community tends to use chatbots for tasks such as\nproblem-solving assistance and engaging in entertainment or creative tasks. Our\nresearch provides a robust foundation for further explorations of the social\ndynamics surrounding generative AI tools and offers invaluable insights for\ndecision-makers in policy, technology, and education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.19187,review,post_llm,2024,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Algorithmic Transparency and Participation through the Handoff Lens:\n  Lessons Learned from the U.S. Census Bureau's Adoption of Differential\n  Privacy\n\n  Emerging discussions on the responsible government use of algorithmic\ntechnologies propose transparency and public participation as key mechanisms\nfor preserving accountability and trust. But in practice, the adoption and use\nof any technology shifts the social, organizational, and political context in\nwhich it is embedded. Therefore translating transparency and participation\nefforts into meaningful, effective accountability must take into account these\nshifts. We adopt two theoretical frames, Mulligan and Nissenbaum's handoff\nmodel and Star and Griesemer's boundary objects, to reveal such shifts during\nthe U.S. Census Bureau's adoption of differential privacy (DP) in its updated\ndisclosure avoidance system (DAS) for the 2020 census. This update preserved\n(and arguably strengthened) the confidentiality protections that the Bureau is\nmandated to uphold, and the Bureau engaged in a range of activities to\nfacilitate public understanding of and participation in the system design\nprocess. Using publicly available documents concerning the Census'\nimplementation of DP, this case study seeks to expand our understanding of how\ntechnical shifts implicate values, how such shifts can afford (or fail to\nafford) greater transparency and participation in system design, and the\nimportance of localized expertise throughout. We present three lessons from\nthis case study toward grounding understandings of algorithmic transparency and\nparticipation: (1) efforts towards transparency and participation in\nalgorithmic governance must center values and policy decisions, not just\ntechnical design decisions; (2) the handoff model is a useful tool for\nrevealing how such values may be cloaked beneath technical decisions; and (3)\nboundary objects alone cannot bridge distant communities without trusted\nexperts traveling alongside to broker their adoption.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.10225,review,post_llm,2024,5,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""GDPR: Is it worth it? Perceptions of workers who have experienced its\n  implementation\n\n  The General Data Protection Regulation (GDPR) remains the gold standard in\nprivacy and security regulation. We investigate how the cost and effort\nrequired to implement GDPR is viewed by workers who have also experienced the\nregulations' benefits as citizens: is it worth it? In a multi-stage study, we\nsurvey N = 273 & 102 individuals who remained working in the same companies\nbefore, during, and after the implementation of GDPR. The survey finds that\nparticipants recognise their rights when prompted but know little about their\nregulator. They have observed concrete changes to data practices in their\nworkplaces and appreciate the trade-offs. They take comfort that their personal\ndata is handled as carefully as their employers' client data. The very people\nwho comply with and execute the GDPR consider it to be positive for their\ncompany, positive for privacy and not a pointless, bureaucratic regulation.\nThis is rare as it contradicts the conventional negative narrative about\nregulation. Policymakers may wish to build upon this public support while it\nlasts and consider early feedback from a similar dual professional-consumer\ngroup as the GDPR evolves.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.09734,regular,post_llm,2024,5,"{'ai_likelihood': 5.231963263617622e-06, 'text': 'Attention is All You Want: Machinic Gaze and the Anthropocene\n\n  This chapter experiments with ways computational vision interprets and\nsynthesises representations of the Anthropocene. Text-to-image systems such as\nMidJourney and StableDiffusion, trained on large data sets of harvested images\nand captions, yield often striking compositions that serve, alternately, as\nbanal reproduction, alien imaginary and refracted commentary on the\npreoccupations of Internet visual culture. While the effects of AI on visual\nculture may themselves be transformative or catastrophic, we are more\ninterested here in how it has been trained to imagine shared human, technical\nand ecological futures. Through a series of textual prompts that marry elements\nof the Anthropocenic and Australian environmental vernacular, we examine how\nthis emergent machinic gaze both looks out, through its compositions of\nfuturistic landscapes, and looks back, towards an observing and observed human\nsubject. In its varied assistive, surveillant and generative roles,\ncomputational vision not only mirrors human desire but articulates oblique\ndemands of its own.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.11789,regular,post_llm,2024,5,"{'ai_likelihood': 1.241763432820638e-05, 'text': 'The Grandparent Scam: A Systems Perspective Case Study On Elder Fraud\n  And The Concept Of Human Layering\n\n  In April 2024, an 81-year-old Ohio man was charged with murder, assault, and\nkidnapping. The man believed that he was protecting his family from scammers\nthreatening harm. What he did not realize was that the 61-year-old Uber driver\nhe killed, was also a victim of the same scammers. This case study examines\nsome common variants of the Grandparent Scam from a systems perspective and how\nweaponization of conscience is used in these scams. Additionally, this study\nexamines the parallels between layering in money laundering and human layering\nin the execution of these scams.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.15987,review,post_llm,2024,5,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Modes of Analyzing Disinformation Narratives With AI/ML/Text Mining to\n  Assist in Mitigating the Weaponization of Social Media\n\n  This paper highlights the developing need for quantitative modes for\ncapturing and monitoring malicious communication in social media. There has\nbeen a deliberate ""weaponization"" of messaging through the use of social\nnetworks including by politically oriented entities both state sponsored and\nprivately run. The article identifies a use of AI/ML characterization of\ngeneralized ""mal-info,"" a broad term which includes deliberate malicious\nnarratives similar with hate speech, which adversely impact society. A key\npoint of the discussion is that this mal-info will dramatically increase in\nvolume, and it will become essential for sharable quantifying tools to provide\nsupport for human expert intervention. Despite attempts to introduce moderation\non major platforms like Facebook and X/Twitter, there are now established\nalternative social networks that offer completely unmoderated spaces. The paper\npresents an introduction to these platforms and the initial results of a\nqualitative and semi-quantitative analysis of characteristic mal-info posts.\nThe authors perform a rudimentary text mining function for a preliminary\ncharacterization in order to evaluate the modes for better-automated\nmonitoring. The action examines several inflammatory terms using text analysis\nand, importantly, discusses the use of generative algorithms by one political\nagent in particular, providing some examples of the potential risks to society.\nThis latter is of grave concern, and monitoring tools must be established. This\npaper presents a preliminary step to selecting relevant sources and to setting\na foundation for characterizing the mal-info, which must be monitored. The\nAI/ML methods provide a means for semi-quantitative signature capture. The\nimpending use of ""mal-GenAI"" is presented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.07715,regular,post_llm,2024,5,"{'ai_likelihood': 1.2185838487413195e-05, 'text': 'Evidence of What, for Whom? The Socially Contested Role of Algorithmic\n  Bias in a Predictive Policing Tool\n\n  This paper presents a critical, qualitative study of the social role of\nalgorithmic bias in the context of the Chicago crime prediction algorithm, a\npredictive policing tool that forecasts when and where in the city crime is\nmost likely to occur. Through interviews with 18 Chicago-area community\norganizations, academic researchers, and public sector actors, we show that\nstakeholders from different groups articulate diverse problem diagnoses of the\ntool\'s algorithmic bias, strategically using it as evidence to advance criminal\njustice interventions that align with stakeholders\' positionality and political\nends. Drawing inspiration from Catherine D\'Ignazio\'s taxonomy of ""refusing and\nusing"" data, we find that stakeholders use evidence of algorithmic bias to\nreform the policies around police patrol allocation; reject algorithm-based\npolicing interventions; reframe crime as a structural rather than interpersonal\nproblem; reveal data on authority figures in an effort to subvert their power;\nrepair and heal families and communities; and, in the case of more powerful\nactors, to reaffirm their own authority or existing power structures. We\nidentify the implicit assumptions and scope of these varied uses of algorithmic\nbias as evidence, showing that they require different (and sometimes\nconflicting) values about policing and AI. This divergence reflects\nlong-standing tensions in the criminal justice reform landscape between the\nvalues of liberation and healing often centered by system-impacted communities\nand the values of surveillance and deterrence often instantiated in data-driven\nreform measures. We advocate for centering the interests and experiential\nknowledge of communities impacted by incarceration to ensure that evidence of\nalgorithmic bias can serve as a device to challenge the status quo.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.12193,regular,post_llm,2024,5,"{'ai_likelihood': 0.00024014049106174047, 'text': ""The Narrow Depth and Breadth of Corporate Responsible AI Research\n\n  The transformative potential of AI presents remarkable opportunities, but\nalso significant risks, underscoring the importance of responsible AI\ndevelopment and deployment. Despite a growing emphasis on this area, there is\nlimited understanding of industry's engagement in responsible AI research,\ni.e., the critical examination of AI's ethical, social, and legal dimensions.\nTo address this gap, we analyzed over 6 million peer-reviewed articles and 32\nmillion patent citations using multiple methods across five distinct datasets\nto quantify industry's engagement. Our findings reveal that the majority of AI\nfirms show limited or no engagement in this critical subfield of AI. We show a\nstark disparity between industry's dominant presence in conventional AI\nresearch and its limited engagement in responsible AI. Leading AI firms exhibit\nsignificantly lower output in responsible AI research compared to their\nconventional AI research and the contributions of leading academic\ninstitutions. Our linguistic analysis documents a narrower scope of responsible\nAI research within industry, with a lack of diversity in key topics addressed.\nOur large-scale patent citation analysis uncovers a pronounced disconnect\nbetween responsible AI research and the commercialization of AI technologies,\nsuggesting that industry patents rarely build upon insights generated by the\nresponsible AI literature. This gap highlights the potential for AI development\nto diverge from a socially optimal path, risking unintended consequences due to\ninsufficient consideration of ethical and societal implications. Our results\nhighlight the urgent need for industry to publicly engage in responsible AI\nresearch to absorb academic knowledge, cultivate public trust, and proactively\nmitigate AI-induced societal harms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.03416,review,post_llm,2024,5,"{'ai_likelihood': 1.0000334845648871e-05, 'text': 'Mental health of computing professionals and students: A systematic\n  literature review\n\n  The intersections of mental health and computing education is under-examined.\nIn this systematic literature review, we evaluate the state-of-the-art of\nresearch in mental health and well-being interventions, assessments, and\nconcerns like anxiety and depression in computer science and computing\neducation. The studies evaluated occurred across the computing education\npipeline from introductory to PhD courses and found some commonalities\ncontributing to high reporting of anxiety and depression in those studied. In\naddition, interventions that were designed to address mental health topics\noften revolved around self-guidance. Based on our review of the literature, we\nrecommend increasing sample sizes and focusing on the design and development of\ntools and interventions specifically designed for computing professionals and\nstudents.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.03855,review,post_llm,2024,5,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Strategies for Increasing Corporate Responsible AI Prioritization\n\n  Responsible artificial intelligence (RAI) is increasingly recognized as a\ncritical concern. However, the level of corporate RAI prioritization has not\nkept pace. In this work, we conduct 16 semi-structured interviews with\npractitioners to investigate what has historically motivated companies to\nincrease the prioritization of RAI. What emerges is a complex story of\nconflicting and varied factors, but we bring structure to the narrative by\nhighlighting the different strategies available to employ, and point to the\nactors with access to each. While there are no guaranteed steps for increasing\nRAI prioritization, we paint the current landscape of motivators so that\npractitioners can learn from each other, and put forth our own selection of\npromising directions forward.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.0215,regular,post_llm,2024,5,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'The AI Review Lottery: Widespread AI-Assisted Peer Reviews Boost Paper\n  Scores and Acceptance Rates\n\n  Journals and conferences worry that peer reviews assisted by artificial\nintelligence (AI), in particular, large language models (LLMs), may negatively\ninfluence the validity and fairness of the peer-review system, a cornerstone of\nmodern science. In this work, we address this concern with a quasi-experimental\nstudy of the prevalence and impact of AI-assisted peer reviews in the context\nof the 2024 International Conference on Learning Representations (ICLR), a\nlarge and prestigious machine-learning conference. Our contributions are\nthreefold. Firstly, we obtain a lower bound for the prevalence of AI-assisted\nreviews at ICLR 2024 using the GPTZero LLM detector, estimating that at least\n$15.8\\%$ of reviews were written with AI assistance. Secondly, we estimate the\nimpact of AI-assisted reviews on submission scores. Considering pairs of\nreviews with different scores assigned to the same paper, we find that in\n$53.4\\%$ of pairs the AI-assisted review scores higher than the human review\n($p = 0.002$; relative difference in probability of scoring higher: $+14.4\\%$\nin favor of AI-assisted reviews). Thirdly, we assess the impact of receiving an\nAI-assisted peer review on submission acceptance. In a matched study,\nsubmissions near the acceptance threshold that received an AI-assisted peer\nreview were $4.9$ percentage points ($p = 0.024$) more likely to be accepted\nthan submissions that did not. Overall, we show that AI-assisted reviews are\nconsequential to the peer-review process and offer a discussion on future\nimplications of current trends\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.01556,regular,post_llm,2024,5,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'A Taxonomy of the Biases of the Images created by Generative Artificial\n  Intelligence\n\n  Generative artificial intelligence models show an amazing performance\ncreating unique content automatically just by being given a prompt by the user,\nwhich is revolutionizing several fields such as marketing and design. Not only\nare there models whose generated output belongs to the text format but we also\nfind models that are able to automatically generate high quality genuine images\nand videos given a prompt. Although the performance in image creation seems\nimpressive, it is necessary to slowly assess the content that these models are\ngenerating, as the users are uploading massively this material on the internet.\nCritically, it is important to remark that generative AI are statistical models\nwhose parameter values are estimated given algorithms that maximize the\nlikelihood of the parameters given an image dataset. Consequently, if the image\ndataset is biased towards certain values for vulnerable variables such as\ngender or skin color, we might find that the generated content of these models\ncan be harmful for certain groups of people. By generating this content and\nbeing uploaded into the internet by users, these biases are perpetuating\nharmful stereotypes for vulnerable groups, polarizing social vision about, for\nexample, what beauty or disability is and means. In this work, we analyze in\ndetail how the generated content by these models can be strongly biased with\nrespect to a plethora of variables, which we organize into a new image\ngenerative AI taxonomy. We also discuss the social, political and economical\nimplications of these biases and possible ways to mitigate them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.15272,review,post_llm,2024,5,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Physiological Data: Challenges for Privacy and Ethics\n\n  Wearable devices that measure and record physiological signals are now\nbecoming widely available to the general public with ever-increasing\naffordability and signal quality. The data from these devices introduce serious\nethical challenges that remain largely unaddressed. Users do not always\nunderstand how these data can be leveraged to reveal private information about\nthem and developers of these devices may not fully grasp how physiological data\ncollected today could be used in the future for completely different purposes.\nWe discuss the potential for wearable devices, initially designed to help users\nimprove their well-being or enhance the experience of some digital application,\nto be appropriated in ways that extend far beyond their original intended\npurpose. We identify how the currently available technology can be misused,\ndiscuss how pairing physiological data with non-physiological data can\nradically expand the predictive capacity of physiological wearables, and\nexplore the implications of these expanded capacities for a variety of\nstakeholders.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.07774,regular,post_llm,2024,5,"{'ai_likelihood': 0.5239257812499999, 'text': 'Open Source in Lab Management\n\n  This document explores the advantages of integrating open source software and\npractices in managing a scientific lab, emphasizing reproducibility and the\navoidance of pitfalls. It details practical applications from website\nmanagement using GitHub Pages to organizing datasets in compliance with BIDS\nstandards, highlights the importance of continuous testing for data integrity,\nIT management through Ansible for efficient system configuration, open source\nsoftware development. The broader goal is to promote transparent, reproducible\nscience by adopting open source tools. This approach not only saves time but\nexposes students to best practices, enhancing the transparency and\nreproducibility of scientific research.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.005237579345703125, 'GPT4': 0.286376953125, 'CLAUDE': 0.0015583038330078125, 'GOOGLE': 0.6181640625, 'OPENAI_O_SERIES': 0.00986480712890625, 'DEEPSEEK': 0.00955963134765625, 'GROK': 0.0042266845703125, 'NOVA': 0.0004000663757324219, 'OTHER': 0.010040283203125, 'HUMAN': 0.054656982421875}}"
2405.02893,review,post_llm,2024,5,"{'ai_likelihood': 1.3907750447591147e-06, 'text': ""Exploring the ethical sensitivity of Ph.D. students in robotics\n\n  Ethical sensitivity, generally defined as a person's ability to recognize\nethical issues and attribute importance to them, is considered to be a crucial\ncompetency in the life of professionals and academics and an essential\nprerequisite to successfully meeting ethical challenges. A concept that first\nemerged in moral psychology almost 40 years ago, ethical sensitivity has been\nwidely studied in healthcare, business, and other domains. Conversely, it\nappears to have received little to no attention within the robotics community,\neven though choices in the design and deployment of robots are likely to have\nwide-ranging, profound ethical impacts on society. Due to the negative\nrepercussions that a lack of ethical sensitivity can have in these contexts,\npromoting the development of ethical sensitivity among roboticists is\nimperative, and endeavoring to train this competency becomes a critical\nundertaking. Therefore, as a first step in this direction and within the\ncontext of a broader effort aimed at developing an online interactive ethics\ntraining module for roboticists, we conducted a qualitative exploration of the\nethical sensitivity of a sample of Ph.D. students in robotics using case\nvignettes that exemplified ethical tensions in disaster robotics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.03808,review,post_llm,2024,5,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'The Future of Office and Administrative Support Occupations in the Era\n  of Artificial Intelligence: A Bibliometric Analysis\n\n  The U.S. Bureau of Labor Statistics projects that by the year 2029, the\nUnited States will lose a million jobs in the office and administrative support\noccupations because technology, automation, and artificial intelligence (AI)\nhave the potential to substitute or replace the office and administrative\nfunctions performed by office workers. Despite the potential impact AI will\nhave on office work and the important role office workers play in the American\neconomy, we have limited knowledge of the state of the art research in office\nwork at the intersection of emerging artificial intelligence technologies. In\nthis study, we conducted a bibliometric analysis of the scholarly literature at\nthe intersection of office work and artificial intelligence. We extracted\nliterature sources from Compendex and Scopus databases and used VOSviewer for\nvisualizing and quantifying our bibliometric analyses. Our findings from\nkeywords analysis indicate that office automation, humans, human-computer\ninteraction, and artificial intelligence occurred more frequently in the\nscholarly literature and had high link strengths. Keyword clusters from\nco-occurrence analysis indicate that intelligent buildings, robotics, and the\ninternet of things are emerging topics in the office work domain. The two\nclusters related to ergonomics, worker characteristics, human performance, and\nsafety indicate the types of human factors concerns that are more widely\nstudied in office work settings. In summary, our findings on the\nstate-of-the-art research in office work indicate that more studies have been\nconducted on smart buildings, robotics, and technology development for office\nwork, compared to studies on office workers and their professional development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11195,regular,post_llm,2024,5,"{'ai_likelihood': 0.0002618630727132162, 'text': 'Optimizing Nurse Scheduling: A Supply Chain Approach for Healthcare\n  Institutions\n\n  When managing an organization, planners often encounter numerous challenging\nscenarios. In such instances, relying solely on intuition or managerial\nexperience may not suffice, necessitating a quantitative approach. This demand\nis further accentuated in the era of big data, where the sheer scale and\ncomplexity of constraints pose significant challenges. Therefore, the aim of\nthis study is to provide a foundational framework for addressing personnel\nscheduling, a critical issue in organizational management. Specifically, we\nfocus on optimizing shift assignments for staff, a task fraught with\ncomplexities due to factors such as contractual obligations and mandated rest\nperiods. Moreover, the current landscape is characterized by frequent employee\nshortages across various industries, with many organizations lacking efficient\nand dependable management tools to address them. Therefore, our attention is\nparticularly drawn to the nurse rostering problem, a personnel scheduling\nchallenge prevalent in healthcare settings. These issues are characterized by a\nmultitude of variables, given that a single healthcare facility may employ\nhundreds of nurses, alongside stringent constraints such as the need for\nadequate staffing levels and rest periods postnight shifts. Furthermore, the\nongoing COVID19 pandemic has exacerbated staffing challenges in healthcare\ninstitutions, underlining the importance of accurately assessing staffing needs\nand optimizing shift allocations for effective operation amidst crisis\nsituations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.00995,review,post_llm,2024,5,"{'ai_likelihood': 0.00010159280565049914, 'text': ""Not a Swiss Army Knife: Academics' Perceptions of Trade-Offs Around Generative Artificial Intelligence Use\n\nIn the rapidly evolving landscape of computing disciplines, substantial efforts are being dedicated to unraveling the sociotechnical implications of generative AI (Gen AI). While existing research has manifested in various forms, there remains a notable gap concerning the direct engagement of knowledge workers in academia with Gen AI. We interviewed 17 knowledge workers, including faculty and students, to investigate the social and technical dimensions of Gen AI from their perspective. Our participants raised concerns about the opacity of the data used to train Gen AI. This lack of transparency makes it difficult to identify and address inaccurate, biased, and potentially harmful, information generated by these models. Knowledge workers also expressed worries about Gen AI undermining trust in the relationship between instructor and student and discussed potential solutions, such as pedagogy readiness, to mitigate them. Additionally, participants recognized Gen AI's potential to democratize knowledge by accelerating the learning process and act as an accessible research assistant. However, there were also concerns about potential social and power imbalances stemming from unequal access to such technologies. Our study offers insights into the concerns and hopes of knowledge workers about the ethical use of Gen AI in educational settings and beyond, with implications for navigating this new landscape."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.04706,review,post_llm,2024,5,"{'ai_likelihood': 0.08104112413194445, 'text': 'Guiding the Way: A Comprehensive Examination of AI Guidelines in Global\n  Media\n\n  With the increasing adoption of artificial intelligence (AI) technologies in\nthe news industry, media organizations have begun publishing guidelines that\naim to promote the responsible, ethical, and unbiased implementation of\nAI-based technologies. These guidelines are expected to serve journalists and\nmedia workers by establishing best practices and a framework that helps them\nnavigate ever-evolving AI tools. Drawing on institutional theory and digital\ninequality concepts, this study analyzes 37 AI guidelines for media purposes in\n17 countries. Our analysis reveals key thematic areas, such as transparency,\naccountability, fairness, privacy, and the preservation of journalistic values.\nResults highlight shared principles and best practices that emerge from these\nguidelines, including the importance of human oversight, explainability of AI\nsystems, disclosure of automated content, and protection of user data. However,\nthe geographical distribution of these guidelines, highlighting the dominance\nof Western nations, particularly North America and Europe, can further ongoing\nconcerns about power asymmetries in AI adoption and consequently isomorphism\noutside these regions. Our results may serve as a resource for news\norganizations, policymakers, and stakeholders looking to navigate the complex\nAI development toward creating a more inclusive and equitable digital future\nfor the media industry worldwide.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.01917,review,post_llm,2024,5,"{'ai_likelihood': 6.16908073425293e-05, 'text': 'A comparison of online search engine autocompletion in Google and Baidu\n\n  Warning: This paper contains content that may be offensive or upsetting.\nOnline search engine auto-completions make it faster for users to search and\naccess information. However, they also have the potential to reinforce and\npromote stereotypes and negative opinions about a variety of social groups. We\nstudy the characteristics of search auto-completions in two different\nlinguistic and cultural contexts: Baidu and Google. We find differences between\nthe two search engines in the way they suppress or modify original queries, and\nwe highlight a concerning presence of negative suggestions across all social\ngroups. Our study highlights the need for more refined, culturally sensitive\nmoderation strategies in current language technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.16926,regular,post_llm,2024,5,"{'ai_likelihood': 0.89013671875, 'text': ""Is Cambodia the World's Largest Cashew Producer?\n\n  Cambodia's agricultural landscape is rapidly transforming, particularly in\nthe cashew sector. Despite the country's rapid emergence and ambition to become\nthe largest cashew producer, comprehensive data on plantation areas and the\nenvironmental impacts of this expansion are lacking. This study addresses the\ngap in detailed land use data for cashew plantations in Cambodia and assesses\nthe implications of agricultural advancements. We collected over 80,000\ntraining polygons across Cambodia to train a convolutional neural network using\nhigh-resolution optical and SAR satellite data for precise cashew plantation\nmapping. Our findings indicate that Cambodia ranks among the top five in terms\nof cultivated area and the top three in global cashew production, driven by\nhigh yields. Significant cultivated areas are located in Kampong Thom, Kratie,\nand Ratanak Kiri provinces. Balancing rapid agricultural expansion with\nenvironmental stewardship, particularly forest conservation, is crucial.\nCambodia's cashew production is poised for further growth, driven by\nhigh-yielding trees and premium nuts. However, sustainable expansion requires\nintegrating agricultural practices with economic and environmental strategies\nto enhance local value and protect forested areas. Advanced mapping\ntechnologies offer comprehensive tools to support these objectives and ensure\nthe sustainable development of Cambodia's cashew industry.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0011930465698242188, 'GPT4': 0.051055908203125, 'CLAUDE': 0.0018815994262695312, 'GOOGLE': 0.1571044921875, 'OPENAI_O_SERIES': 0.78271484375, 'DEEPSEEK': 0.0005030632019042969, 'GROK': 1.0132789611816406e-06, 'NOVA': 0.00012493133544921875, 'OTHER': 0.0005903244018554688, 'HUMAN': 0.004932403564453125}}"
2405.12167,regular,post_llm,2024,5,"{'ai_likelihood': 0.98779296875, 'text': 'Open-Source Assessments of AI Capabilities: The Proliferation of AI\n  Analysis Tools, Replicating Competitor Models, and the Zhousidun Dataset\n\n  The integration of artificial intelligence (AI) into military capabilities\nhas become a norm for major military power across the globe. Understanding how\nthese AI models operate is essential for maintaining strategic advantages and\nensuring security. This paper demonstrates an open-source methodology for\nanalyzing military AI models through a detailed examination of the Zhousidun\ndataset, a Chinese-originated dataset that exhaustively labels critical\ncomponents on American and Allied destroyers. By demonstrating the replication\nof a state-of-the-art computer vision model on this dataset, we illustrate how\nopen-source tools can be leveraged to assess and understand key military AI\ncapabilities. This methodology offers a robust framework for evaluating the\nperformance and potential of AI-enabled military capabilities, thus enhancing\nthe accuracy and reliability of strategic assessments.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0163116455078125, 'GPT4': 0.8037109375, 'CLAUDE': 0.0037078857421875, 'GOOGLE': 0.16357421875, 'OPENAI_O_SERIES': 0.0013904571533203125, 'DEEPSEEK': 0.0003447532653808594, 'GROK': 0.00029850006103515625, 'NOVA': 0.0003628730773925781, 'OTHER': 0.00800323486328125, 'HUMAN': 0.002338409423828125}}"
2407.12154,review,post_llm,2024,5,"{'ai_likelihood': 2.0497375064426e-05, 'text': 'Cyberbullying Detection: Exploring Datasets, Technologies, and\n  Approaches on Social Media Platforms\n\n  Cyberbullying has been a significant challenge in the digital era world,\ngiven the huge number of people, especially adolescents, who use social media\nplatforms to communicate and share information. Some individuals exploit these\nplatforms to embarrass others through direct messages, electronic mail, speech,\nand public posts. This behavior has direct psychological and physical impacts\non victims of bullying. While several studies have been conducted in this field\nand various solutions proposed to detect, prevent, and monitor cyberbullying\ninstances on social media platforms, the problem continues. Therefore, it is\nnecessary to conduct intensive studies and provide effective solutions to\naddress the situation. These solutions should be based on detection,\nprevention, and prediction criteria methods. This paper presents a\ncomprehensive systematic review of studies conducted on cyberbullying\ndetection. It explores existing studies, proposed solutions, identified gaps,\ndatasets, technologies, approaches, challenges, and recommendations, and then\nproposes effective solutions to address research gaps in future studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20233,review,post_llm,2024,5,"{'ai_likelihood': 1.0, 'text': ""Designing an AI-Powered Mentorship Platform for Professional\n  Development: Opportunities and Challenges\n\n  This article examines the promising prospects and potential hurdles\nassociated with the development of MentorAI, a conceptual AI-driven mentorship\nplatform for professional growth yet to be actualized. The article explores the\nessential characteristics and technological underpinnings required for the\nsuccessful creation and efficacy of the MentorAI platform in providing tailored\nmentorship experiences. The article highlights the transformative potential of\nMentorAI on various dimensions of professional growth, such as boosting career\nprogression, nurturing skill development, and supporting a balanced work-life\nenvironment for professionals. MentorAI, through its AI-based approach, aspires\nto offer real-time guidance, resources, and assistance customized to each\nindividual's specific needs and goals. Furthermore, the article examines the\ncore technologies crucial to MentorAI's operation, including artificial\nintelligence, machine learning, and natural language comprehension. These\ntechnologies will empower the platform to process user inputs, deliver\ncontext-sensitive responses, and dynamically adjust to user preferences and\nobjectives. The deployment of MentorAI presents potential challenges and\nethical concerns, as with any groundbreaking technology. The article outlines\ncritical issues like data protection, security, algorithmic bias, and moral\nquandaries concerning substituting human mentors with AI systems. Addressing\nthese challenges proactively and deliberately is vital to ensure a positive\nimpact on users.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.5093555450439453e-05, 'GPT4': 0.00751495361328125, 'CLAUDE': 1.239776611328125e-05, 'GOOGLE': 0.9921875, 'OPENAI_O_SERIES': 0.00022792816162109375, 'DEEPSEEK': 2.980232238769531e-07, 'GROK': 0.0, 'NOVA': 2.384185791015625e-07, 'OTHER': 2.008676528930664e-05, 'HUMAN': 1.7881393432617188e-07}}"
2405.06957,review,post_llm,2024,5,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""On the Role of Intelligence and Business Wargaming in Developing Foresight\n\nBusiness wargaming is a central tool for developing sustaining strategies. It transfers the benefits of traditional wargaming to the business environment. However, building wargames that support the process of decision-making for strategy require respective intelligence. This paper investigates the role of intelligence in the process of developing strategic foresight. The focus is on how intelligence is developed and how it relates to business wargaming. The so-called intelligence cycle is the basis and reference of our investigation.\n  The conceptual part of the paper combines the theoretical background from military, business as well as serious gaming. To elaborate on some of the lessons learned, we examine specific business wargames both drawn from the literature and conducted by us at the Center for Intelligence and Security Studies (CISS). It is shown that business wargaming can make a significant contribution to the transformation of data to intelligence by supporting the intelligence cycle in two crucial phases. Furthermore, it brings together business intelligence (BI) and competitive intelligence (CI) and it bridges the gap to a company's strategy by either testing or developing a new strategy. We were also able to confirm this finding based on the business wargame we conducted at a major semiconductor manufacturer."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.09788,regular,post_llm,2024,5,"{'ai_likelihood': 8.841355641682943e-06, 'text': ""Synthesizing Proteins on the Graphics Card. Protein Folding and the\n  Limits of Critical AI Studies\n\n  This paper investigates the application of the transformer architecture in\nprotein folding, as exemplified by DeepMind's AlphaFold project, and its\nimplications for the understanding of so-called large language models. The\nprevailing discourse often assumes a ready-made analogy between proteins,\nencoded as sequences of amino acids, and natural language, which we term the\nlanguage paradigm of computational (structural) biology. Instead of assuming\nthis analogy as given, we critically evaluate it to assess the kind of\nknowledge-making afforded by the transformer architecture. We first trace the\nanalogy's emergence and historical development, carving out the influence of\nstructural linguistics on structural biology beginning in the mid-20th century.\nWe then examine three often overlooked preprocessing steps essential to the\ntransformer architecture, including subword tokenization, word embedding, and\npositional encoding, to demonstrate its regime of representation based on\ncontinuous, high-dimensional vector spaces, which departs from the discrete\nnature of language. The successful deployment of transformers in protein\nfolding, we argue, discloses what we consider a non-linguistic approach to\ntoken processing intrinsic to the architecture. We contend that through this\nnon-linguistic processing, the transformer architecture carves out unique\nepistemological territory and produces a new class of knowledge, distinct from\nestablished domains. We contend that our search for intelligent machines has to\nbegin with the shape, rather than the place, of intelligence. Consequently, the\nemerging field of critical AI studies should take methodological inspiration\nfrom the history of science in its quest to conceptualize the contributions of\nartificial intelligence to knowledge-making, within and beyond the\ndomain-specific sciences.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.0734,regular,post_llm,2024,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Machine Consciousness as Pseudoscience: The Myth of Conscious Machines\n\n  The hypothesis of conscious machines has been debated since the invention of\nthe notion of artificial intelligence, powered by the assumption that the\ncomputational intelligence achieved by a system is the cause of the emergence\nof phenomenal consciousness in that system as an epiphenomenon or as a\nconsequence of the behavioral or internal complexity of the system surpassing\nsome threshold. As a consequence, a huge amount of literature exploring the\npossibility of machine consciousness and how to implement it on a computer has\nbeen published. Moreover, common folk psychology and transhumanism literature\nhas fed this hypothesis with the popularity of science fiction literature,\nwhere intelligent robots are usually antropomorphized and hence given\nphenomenal consciousness. However, in this work, we argue how these literature\nlacks scientific rigour, being impossible to falsify the opposite hypothesis,\nand illustrate a list of arguments that show how every approach that the\nmachine consciousness literature has published depends on philosophical\nassumptions that cannot be proven by the scientific method. Concretely, we also\nshow how phenomenal consciousness is not computable, independently on the\ncomplexity of the algorithm or model, cannot be objectively measured nor\nquantitatively defined and it is basically a phenomenon that is subjective and\ninternal to the observer. Given all those arguments we end the work arguing why\nthe idea of conscious machines is nowadays a myth of transhumanism and science\nfiction culture.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.01697,regular,post_llm,2024,5,"{'ai_likelihood': 1.0, 'text': ""Towards an Ethical and Inclusive Implementation of Artificial\n  Intelligence in Organizations: A Multidimensional Framework\n\n  This article analyzes the impact of artificial intelligence (AI) on\ncontemporary society and the importance of adopting an ethical approach to its\ndevelopment and implementation within organizations. It examines the\ntechnocritical perspective of some philosophers and researchers, who warn of\nthe risks of excessive technologization that could undermine human autonomy.\nHowever, the article also acknowledges the active role that various actors,\nsuch as governments, academics, and civil society, can play in shaping the\ndevelopment of AI aligned with human and social values.\n  A multidimensional approach is proposed that combines ethics with regulation,\ninnovation, and education. It highlights the importance of developing detailed\nethical frameworks, incorporating ethics into the training of professionals,\nconducting ethical impact audits, and encouraging the participation of\nstakeholders in the design of AI.\n  In addition, four fundamental pillars are presented for the ethical\nimplementation of AI in organizations: 1) Integrated values, 2) Trust and\ntransparency, 3) Empowering human growth, and 4) Identifying strategic factors.\nThese pillars encompass aspects such as alignment with the company's ethical\nidentity, governance and accountability, human-centered design, continuous\ntraining, and adaptability to technological and market changes.\n  The conclusion emphasizes that ethics must be the cornerstone of any\norganization's strategy that seeks to incorporate AI, establishing a solid\nframework that ensures that technology is developed and used in a way that\nrespects and promotes human values.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.006885528564453125, 'GPT4': 0.0069580078125, 'CLAUDE': 0.86376953125, 'GOOGLE': 0.11944580078125, 'OPENAI_O_SERIES': 0.00036835670471191406, 'DEEPSEEK': 4.392862319946289e-05, 'GROK': 2.682209014892578e-06, 'NOVA': 7.921457290649414e-05, 'OTHER': 0.0024890899658203125, 'HUMAN': 9.226799011230469e-05}}"
2405.11697,review,post_llm,2024,5,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation\n  In-The-Wild\n\n  The prevalence and harms of online misinformation is a perennial concern for\ninternet platforms, institutions and society at large. Over time, information\nshared online has become more media-heavy and misinformation has readily\nadapted to these new modalities. The rise of generative AI-based tools, which\nprovide widely-accessible methods for synthesizing realistic audio, images,\nvideo and human-like text, have amplified these concerns. Despite intense\npublic interest and significant press coverage, quantitative information on the\nprevalence and modality of media-based misinformation remains scarce. Here, we\npresent the results of a two-year study using human raters to annotate online\nmedia-based misinformation, mostly focusing on images, based on claims assessed\nin a large sample of publicly-accessible fact checks with the ClaimReview\nmarkup. We present an image typology, designed to capture aspects of the image\nand manipulation relevant to the image\'s role in the misinformation claim. We\nvisualize the distribution of these types over time. We show the rise of\ngenerative AI-based content in misinformation claims, and that its commonality\nis a relatively recent phenomenon, occurring significantly after heavy press\ncoverage. We also show ""simple"" methods dominated historically, particularly\ncontext manipulations, and continued to hold a majority as of the end of data\ncollection in November 2023. The dataset, Annotated Misinformation, Media-Based\n(AMMeBa), is publicly-available, and we hope that these data will serve as both\na means of evaluating mitigation methods in a realistic setting and as a\nfirst-of-its-kind census of the types and modalities of online misinformation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.1305,review,post_llm,2024,5,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Decolonial AI as Disenclosure\n\n  The development and deployment of machine learning and AI engender 'AI\ncolonialism', a term that conceptually overlaps with 'data colonialism', as a\nform of injustice. AI colonialism is in need of decolonization for three\nreasons. Politically, because it enforces digital capitalism's hegemony.\nEcologically, as it negatively impacts the environment and intensifies the\nextraction of natural resources and consumption of energy. Epistemically, since\nthe social systems within which AI is embedded reinforce Western universalism\nby imposing Western colonial values on the global South when these manifest in\nthe digital realm is a form of digital capitalism. These reasons require a new\nconceptualization of AI decolonization. First this paper draws from the\nhistorical debates on the concepts of colonialism and decolonization. Secondly\nit retrieves Achille Mbembe's notion of decolonization as disenclosure to argue\nthat the decolonization of AI will have to be the abolishment of political,\necological and epistemic borders erected and reinforced in the phases of its\ndesign, production, development of AI in the West and drawing from the\nknowledge from the global South. In conclusion, it is discussed how conceiving\nof decolonial AI as form of disenclosure opens up new ways to think about and\nintervene in colonial instantiations of AI development and deployment, in order\nto empower 'the wretched of AI', re-ecologise the unsustainable ecologies AI\ndepends on and to counter the colonial power structures unreflective AI\ndeployment risks to reinforce.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17584,review,post_llm,2024,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Theorizing neuro-induced relationships between cognitive diversity,\n  motivation, grit and academic performance in multidisciplinary engineering\n  education context\n\n  Nowadays, engineers need to tackle many unprecedented challenges that are\noften complex, and, most importantly, cannot be exhaustively compartmentalized\ninto a single engineering discipline. In other words, most engineering problems\nneed to be solved from a multidisciplinary approach. However, conventional\nengineering programs usually adopt pedagogical approaches specifically tailored\nto traditional, niched engineering disciplines, which become increasingly\ndeviated from the industry needs as those programs are typically designed and\ntaught by instructors with highly specialized engineering training and\ncredentials. To reduce the gap, more multidisciplinary engineering programs\nemerge by systematically stretching across all engineering fibers, and\nchallenge the sub-optimal traditional pedagogy crowded in engineering\nclassrooms. To further advance future-oriented pedagogy, in this work, we\nhypothesized neuro-induced linkages between how cognitively different learners\nare and how the linkages would affect learners in the knowledge acquisition\nprocess. We situate the neuro-induced linkages in the context of\nmultidisciplinary engineering education and propose possible pedagogical\napproaches to actualize the implications of this conceptual framework. Our\nstudy, based on the innovative concept of brain fingerprint, would serve as a\npioneer model to theorize key components of learner-centered multidisciplinary\nengineering pedagogy which centers on the key question: how do we motivate\nengineering students of different backgrounds from a neuro-inspired\nperspective?\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.0086,review,post_llm,2024,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Public Computing Intellectuals in the Age of AI Crisis\n\n  The belief that AI technology is on the cusp of causing a generalized social\ncrisis became a popular one in 2023. While there was no doubt an element of\nhype and exaggeration to some of these accounts, they do reflect the fact that\nthere are troubling ramifications to this technology stack. This conjunction of\nshared concerns about social, political, and personal futures presaged by\ncurrent developments in artificial intelligence presents the academic\ndiscipline of computing with a renewed opportunity for self-examination and\nreconfiguration. This position paper endeavors to do so in four sections. The\nfirst explores what is at stake for computing in the narrative of an AI crisis.\nThe second articulates possible educational responses to this crisis and\nadvocates for a broader analytic focus on power relations. The third section\npresents a novel characterization of academic computing's field of practice,\none which includes not only the discipline's usual instrumental forms of\npractice but reflexive practice as well. This reflexive dimension integrates\nboth the critical and public functions of the discipline as equal intellectual\npartners and a necessary component of any contemporary academic field. The\nfinal section will advocate for a conceptual archetype--the Public Computer\nIntellectual and its less conspicuous but still essential cousin, the (Almost)\nPublic Computer Intellectual--as a way of practically imagining the expanded\npossibilities of academic practice in our discipline, one that provides both\nself-critique and an outward-facing orientation towards the public good. It\nwill argue that the computer education research community can play a vital role\nin this regard. Recommendations for pedagogical change within computing to\ndevelop more reflexive capabilities are also provided.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.15437,review,post_llm,2024,5,"{'ai_likelihood': 8.44399134318034e-06, 'text': 'Learning about Data, Algorithms, and Algorithmic Justice on TikTok in\n  Personally Meaningful Ways\n\n  TikTok, a popular short video sharing application, emerged as the dominant\nsocial media platform for young people, with a pronounced influence on how\nyoung women and people of color interact online. The application has become a\nglobal space for youth to connect with each other, offering not only\nentertainment but also opportunities to engage with artificial\nintelligence/machine learning (AI/ML)-driven recommendations and create content\nusing AI/M-powered tools, such as generative AI filters. This provides\nopportunities for youth to explore and question the inner workings of these\nsystems, their implications, and even use them to advocate for causes they are\npassionate about. We present different perspectives on how youth may learn in\npersonally meaningful ways when engaging with TikTok. We discuss how youth\ninvestigate how TikTok works (considering data and algorithms), take into\naccount issues of ethics and algorithmic justice and use their understanding of\nthe platform to advocate for change.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.10305,regular,post_llm,2024,5,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Elements Of Legislation For Artificial Intelligence Systems\n\n  The significant part of the operational context for autonomous company\nmanagement systems is the regulatory and legal environment in which\ncorporations operate. In order to create a dedicated operational context for\nautonomous artificial intelligence systems, the wording of local regulatory\ndocuments can be simultaneously presented in two versions: for use by people\nand for use by autonomous systems. In this case, the artificial intelligence\nsystem will get a well-defined operational context that allows such a system to\nperform functions within the required standards. Local regulations that provide\nbasis for the joint work of individuals and autonomous artificial intelligence\nsystems can form the grounds for the relevant legislation governing the\ndevelopment and implementation of autonomous systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.20916,review,post_llm,2024,5,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Unravelling the Use of Digital Twins to Assist Decision- and\n  Policy-Making in Smart Cities\n\n  This short paper represents a systematic literature review that sets the\nbasis for the future development of a framework for digital twin-based decision\nsupport in the public sector, specifically for the smart city domain. The final\naim of the research is to model context-specific digital twins for aiding the\ndecision-making processes in smart cities and devise methods for defining the\npolicy agenda. Overall, this short paper provides a foundation, based on the\nmain concepts from existing literature, for further research in the role and\napplications of urban digital twins to assist decision- and policy-making in\nsmart cities. The existing literature analyses common applications of digital\ntwins in smart city development with a focus on supporting decision- and\npolicy-making. Future work will centre on developing a digital-twin-based\nsustainable smart city and defining different scenarios concerning challenges\nof good governance, especially so-called wicked problems, in smaller-scale\nurban and non-urban contexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.0913,regular,post_llm,2024,5,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Contextual Integrity Games\n\n  The contextual integrity model is a widely accepted way of analyzing the\nplurality of norms that are colloquially called ""privacy norms"". Contextual\nintegrity systematically describes such norms by distinguishing the type of\ndata concerned, the three social agents involved (subject, sender, and\nrecipient) and the transmission principle governing the transfer of\ninformation. It allows analyzing privacy norms in terms of their impact on the\ninteraction of those agents with one another.\n  This paper places contextual integrity in a strict game theoretic framework.\nWhen such description is possible it has three key advantages: Firstly, it\nallows indisputable utilitarian justification of some privacy norms. Secondly,\nit better relates privacy to topics which are well understood by stakeholders\nwhose education is predominantly quantitative, such as engineers and\neconomists. Thirdly, it is an absolute necessity when describing ethical\nconstraints to machines such as AI agents.\n  In addition to describing games which capture paradigmatic informational\nnorms, the paper also analyzes cases in which the game, per se, does not\nencourage normative behavior. The paper discusses two main forms of mechanisms\nwhich can be applied to the game in such cases, and shows that they reflect\naccepted privacy regulation and technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.19934,regular,post_llm,2024,5,"{'ai_likelihood': 9.271833631727431e-06, 'text': 'Estimating Population Burden of Stroke with an Agent-Based Model\n\n  Stroke is one of the leading causes of death and disability worldwide but it\nis believed to be highly preventable. The majority of stroke prevention focuses\non targeting high-risk individuals but its is important to understand how the\ntargeting of high-risk individuals might impact the overall societal burden of\nstroke. We propose using an agent-based model that follows agents through their\npre-stroke and stroke journey to assess the impacts of different interventions\nat the population level. We present a case study looking at the impacts of\nagents being informed of their stroke risk at certain ages and those agents\ntaking measure to reduce their risk. The results of our study show that if\nagents are aware of their risk and act accordingly we see a significant\nreduction in strokes and population DALYs. The case study highlights the\nimportance of individuals understanding their own stroke risk for stroke\nprevention and the usefulness of agent-based models in assessing the impact of\nstroke interventions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.10367,review,post_llm,2024,5,"{'ai_likelihood': 0.0011931525336371528, 'text': ""Perceptions of Entrepreneurship Among Graduate Students: Challenges,\n  Opportunities, and Cultural Biases\n\n  The purpose of the paper is to examine the perceptions of entrepreneurship of\ngraduate students enrolled in a digital-oriented entrepreneurship course,\nfocusing on the challenges and opportunities related to starting a business. In\ntoday's digital era, businesses heavily depend on tailored software solutions\nto facilitate their operational processes, foster expansion, and enhance their\ncompetitive edge, thus assuming, to a certain degree, the characteristics of\nsoftware companies. For data gathering, we used online exploratory surveys. The\nfindings indicated that although entrepreneurship was considered an attractive\noption by students, very few of them declared that they intended to start a\nbusiness soon. The main issues raised by the students were internal traits and\nexternal obstacles, such as lack of resources and support. Gender\ndiscrimination and cultural biases persist, limiting opportunities and equality\nfor women. In terms of gender, women face limited representation in leadership\nroles, are expected to do more unpaid 'family work', are perceived as less\ncapable in ding business, and need to prove their skills. Even if women are\nless discriminated now, both genders agree that women still face discrimination\nin business domain. In terms of percentages, women mentioned gender\ndiscrimination in higher percentages. Addressing these issues requires\nawareness, education, and policy changes to ensure fair treatment and\nopportunities for women.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.14004,review,post_llm,2024,5,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Towards A Comprehensive Assessment of AI's Environmental Impact\n\n  Artificial Intelligence, machine learning (AI/ML) has allowed exploring\nsolutions for a variety of environmental and climate questions ranging from\nnatural disasters, greenhouse gas emission, monitoring biodiversity,\nagriculture, to weather and climate modeling, enabling progress towards climate\nchange mitigation. However, the intersection of AI/ML and environment is not\nalways positive. The recent surge of interest in ML, made possible by\nprocessing very large volumes of data, fueled by access to massive compute\npower, has sparked a trend towards large-scale adoption of AI/ML. This interest\nplaces tremendous pressure on natural resources, that are often overlooked and\nunder-reported. There is a need for a framework that monitors the environmental\nimpact and degradation from AI/ML throughout its lifecycle for informing\npolicymakers, stakeholders to adequately implement standards and policies and\ntrack the policy outcome over time. For these policies to be effective, AI's\nenvironmental impact needs to be monitored in a spatially-disaggregated, timely\nmanner across the globe at the key activity sites. This study proposes a\nmethodology to track environmental variables relating to the multifaceted\nimpact of AI around datacenters using openly available energy data and globally\nacquired satellite observations. We present a case study around Northern\nVirginia, United States that hosts a growing number of datacenters and observe\nchanges in multiple satellite-based environmental metrics. We then discuss the\nsteps to expand this methodology for comprehensive assessment of AI's\nenvironmental impact across the planet. We also identify data gaps and\nformulate recommendations for improving the understanding and monitoring\nAI-induced changes to the environment and climate.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.02703,review,post_llm,2024,5,"{'ai_likelihood': 2.384185791015625e-06, 'text': 'Machine Learning Data Practices through a Data Curation Lens: An\n  Evaluation Framework\n\n  Studies of dataset development in machine learning call for greater attention\nto the data practices that make model development possible and shape its\noutcomes. Many argue that the adoption of theory and practices from archives\nand data curation fields can support greater fairness, accountability,\ntransparency, and more ethical machine learning. In response, this paper\nexamines data practices in machine learning dataset development through the\nlens of data curation. We evaluate data practices in machine learning as data\ncuration practices. To do so, we develop a framework for evaluating machine\nlearning datasets using data curation concepts and principles through a rubric.\nThrough a mixed-methods analysis of evaluation results for 25 ML datasets, we\nstudy the feasibility of data curation principles to be adopted for machine\nlearning data work in practice and explore how data curation is currently\nperformed. We find that researchers in machine learning, which often emphasizes\nmodel development, struggle to apply standard data curation principles. Our\nfindings illustrate difficulties at the intersection of these fields, such as\nevaluating dimensions that have shared terms in both fields but non-shared\nmeanings, a high degree of interpretative flexibility in adapting concepts\nwithout prescriptive restrictions, obstacles in limiting the depth of data\ncuration expertise needed to apply the rubric, and challenges in scoping the\nextent of documentation dataset creators are responsible for. We propose ways\nto address these challenges and develop an overall framework for evaluation\nthat outlines how data curation concepts and methods can inform machine\nlearning data practices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.15123,regular,post_llm,2024,5,"{'ai_likelihood': 1.6490618387858074e-05, 'text': ""Probeable Problems for Beginner-level Programming-with-AI Contests\n\n  To broaden participation, competitive programming contests may include\nbeginner-level problems that do not require knowledge of advanced Computer\nScience concepts (e.g., algorithms and data structures). However, since most\nparticipants have easy access to AI code-generation tools, these problems often\nbecome trivial to solve. For beginner-friendly programming contests that do not\nprohibit the use of AI tools, we propose Probeable Problems: code writing tasks\nthat provide (1) a problem specification that deliberately omits certain\ndetails, and (2) a mechanism to probe for these details by asking clarifying\nquestions and receiving immediate feedback. To evaluate our proposal, we\nconducted a 2-hour programming contest for undergraduate Computer Science\nstudents from multiple institutions, where each student was an active member of\ntheir institution's computing club. The contest comprised of six Probeable\nProblems for which a popular code-generation tool (GitHub Copilot) was unable\nto generate accurate solutions due to the absence of details. Students were\npermitted to work individually or in groups, and were free to use AI tools. We\nobtained consent from 26 groups (67 students) to use their submissions for\nresearch. We analyze the extent to which the code submitted by these groups\nidentifies missing details and identify ways in which Probeable Problems can\nsupport learning in formal and informal CS educational contexts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.00335,regular,post_llm,2024,5,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Finding the white male: The prevalence and consequences of algorithmic\n  gender and race bias in political Google searches\n\n  Search engines like Google have become major information gatekeepers that use\nartificial intelligence (AI) to determine who and what voters find when\nsearching for political information. This article proposes and tests a\nframework of algorithmic representation of minoritized groups in a series of\nfour studies. First, two algorithm audits of political image searches delineate\nhow search engines reflect and uphold structural inequalities by under- and\nmisrepresenting women and non-white politicians. Second, two online experiments\nshow that these biases in algorithmic representation in turn distort\nperceptions of the political reality and actively reinforce a white and\nmasculinized view of politics. Together, the results have substantive\nimplications for the scientific understanding of how AI technology amplifies\nbiases in political perceptions and decision-making. The article contributes to\nongoing public debates and cross-disciplinary research on algorithmic fairness\nand injustice.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.01805,review,post_llm,2024,5,"{'ai_likelihood': 1.7616483900282117e-05, 'text': ""Crafting Tomorrow's Evaluations: Assessment Design Strategies in the Era\n  of Generative AI\n\n  GenAI has gained the attention of a myriad of users in almost every\nprofession. Its advancement has had an intense impact on education,\nsignificantly disrupting the assessment design and evaluation methodologies.\nDespite the potential benefits and possibilities of GenAI in the education\nsector, there are several concerns primarily centred around academic integrity,\nauthenticity, equity of access, assessment evaluation methodology, and\nfeedback. Consequently, academia is encountering challenges in assessment\ndesign that are essential to retaining academic integrity in the age of GenAI.\nIn this article, we discuss the challenges, and opportunities that need to be\naddressed for the assessment design and evaluation. The article also highlights\nthe importance of clear policy about the usage of GenAI in completing\nassessment tasks, and also in design approaches to ensure academic integrity\nand subject learning. Additionally, this article also provides assessment\ncategorisation based on the use of GenAI to cultivate knowledge among students\nand academic professionals. It also provides information on the skills\nnecessary to formulate and articulate problems and evaluate the task, enabling\nstudents and academics to effectively utilise GenAI tools.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.17971,regular,post_llm,2024,5,"{'ai_likelihood': 1.0, 'text': 'A Qualitative Analysis Framework for mHealth Privacy Practices\n\n  Mobile Health (mHealth) applications have become a crucial part of health\nmonitoring and management. However, the proliferation of these applications has\nalso raised concerns over the privacy and security of Personally Identifiable\nInformation and Protected Health Information. Addressing these concerns, this\npaper introduces a novel framework for the qualitative evaluation of privacy\npractices in mHealth apps, particularly focusing on the handling and\ntransmission of sensitive user data. Our investigation encompasses an analysis\nof 152 leading mHealth apps on the Android platform, leveraging the proposed\nframework to provide a multifaceted view of their data processing activities.\nDespite stringent regulations like the General Data Protection Regulation in\nthe European Union and the Health Insurance Portability and Accountability Act\nin the United States, our findings indicate persistent issues with negligence\nand misuse of sensitive user information. We uncover significant instances of\nhealth information leakage to third-party trackers and a widespread neglect of\nprivacy-by-design and transparency principles. Our research underscores the\ncritical need for stricter enforcement of data protection laws and sets a\nfoundation for future efforts aimed at enhancing user privacy within the\nmHealth ecosystem.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0132789611816406e-06, 'GPT4': 0.99951171875, 'CLAUDE': 1.7881393432617188e-07, 'GOOGLE': 0.00043392181396484375, 'OPENAI_O_SERIES': 3.4570693969726562e-06, 'DEEPSEEK': 1.7881393432617188e-07, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 4.172325134277344e-07, 'HUMAN': 5.960464477539063e-08}}"
2405.06404,regular,post_llm,2024,5,"{'ai_likelihood': 4.023313522338867e-05, 'text': ""Inclusive content reduces racial and gender biases, yet non-inclusive\n  content dominates popular culture\n\n  Images are often termed as representations of perceived reality. As such,\nracial and gender biases in popular culture and visual media could play a\ncritical role in shaping people's perceptions of society. While previous\nresearch has made significant progress in exploring the frequency and\ndiscrepancies in racial and gender group appearances in visual media, it has\nlargely overlooked important nuances in how these groups are portrayed, as it\nlacked the ability to systematically capture such complexities at scale over\ntime. To address this gap, we examine two media forms of varying target\naudiences, namely fashion magazines and movie posters. Accordingly, we collect\na large dataset comprising over 300,000 images spanning over five decades and\nutilize state-of-the-art machine learning models to classify not only race and\ngender but also the posture, expressed emotional state, and body composition of\nindividuals featured in each image. We find that racial minorities appear far\nless frequently than their White counterparts, and when they do appear, they\nare portrayed less prominently. We also find that women are more likely to be\nportrayed with their full bodies, whereas men are more frequently presented\nwith their faces. Finally, through a series of survey experiments, we find\nevidence that exposure to inclusive content can help reduce biases in\nperceptions of minorities, while racially and gender-homogenized content may\nreinforce and amplify such biases. Taken together, our findings highlight that\nracial and gender biases in visual media remain pervasive, potentially\nexacerbating existing stereotypes and inequalities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.13077,review,post_llm,2024,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Visions of a Discipline: Analyzing Introductory AI Courses on YouTube\n\n  Education plays an indispensable role in fostering societal well-being and is\nwidely regarded as one of the most influential factors in shaping the future of\ngenerations to come. As artificial intelligence (AI) becomes more deeply\nintegrated into our daily lives and the workforce, educational institutions at\nall levels are directing their focus on resources that cater to AI education.\nOur work investigates the current landscape of introductory AI courses on\nYouTube, and the potential for introducing ethics in this context. We\nqualitatively analyze the 20 most watched introductory AI courses on YouTube,\ncoding a total of 92.2 hours of educational content viewed by close to 50\nmillion people. Introductory AI courses do not meaningfully engage with ethical\nor societal challenges of AI (RQ1). When \\textit{defining and framing AI},\nintroductory AI courses foreground excitement around AI's transformative role\nin society, over-exaggerate AI's current and future abilities, and\nanthropomorphize AI (RQ2). In \\textit{teaching AI}, we see a widespread\nreliance on corporate AI tools and frameworks as well as a prioritization on a\nhands-on approach to learning rather than on conceptual foundations (RQ3). In\npromoting key \\textit{AI practices}, introductory AI courses abstract away\nentirely the socio-technical nature of AI classification and prediction, for\nexample by favoring data quantity over data quality (RQ4). We extend our\nanalysis with recommendations that aim to integrate ethical reflections into\nintroductory AI courses. We recommend that introductory AI courses should (1)\nhighlight ethical challenges of AI to present a more balanced perspective, (2)\nraise ethical issues explicitly relevant to the technical concepts discussed\nand (3) nurture a sense of accountability in future AI developers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.14812,regular,post_llm,2024,5,"{'ai_likelihood': 0.0007491641574435764, 'text': 'As an AI Language Model, ""Yes I Would Recommend Calling the Police"":\n  Norm Inconsistency in LLM Decision-Making\n\n  We investigate the phenomenon of norm inconsistency: where LLMs apply\ndifferent norms in similar situations. Specifically, we focus on the high-risk\napplication of deciding whether to call the police in Amazon Ring home\nsurveillance videos. We evaluate the decisions of three state-of-the-art LLMs\n-- GPT-4, Gemini 1.0, and Claude 3 Sonnet -- in relation to the activities\nportrayed in the videos, the subjects\' skin-tone and gender, and the\ncharacteristics of the neighborhoods where the videos were recorded. Our\nanalysis reveals significant norm inconsistencies: (1) a discordance between\nthe recommendation to call the police and the actual presence of criminal\nactivity, and (2) biases influenced by the racial demographics of the\nneighborhoods. These results highlight the arbitrariness of model decisions in\nthe surveillance context and the limitations of current bias detection and\nmitigation strategies in normative decision-making.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.13647,regular,post_llm,2024,5,"{'ai_likelihood': 3.2120280795627172e-06, 'text': 'A framework for expected capability sets\n\n  This paper addresses decision-aiding problems that involve multiple\nobjectives and uncertain states of the world. Inspired by the capability\napproach, we focus on cases where a policy maker chooses an act that, combined\nwith a state of the world, leads to a set of choices for citizens. While no\npreferential information is available to construct importance parameters for\nthe criteria, we can obtain likelihoods for the different states. To\neffectively support decision-aiding in this context, we propose two procedures\nthat merge the potential set of choices for each state of the world taking into\naccount their respective likelihoods. Our procedures satisfy several\nfundamental and desirable properties that characterize the outcomes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.04623,regular,post_llm,2024,5,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'The Dark Side of Dataset Scaling: Evaluating Racial Classification in\n  Multimodal Models\n\n  Scale the model, scale the data, scale the GPU farms is the reigning\nsentiment in the world of generative AI today. While model scaling has been\nextensively studied, data scaling and its downstream impacts on model\nperformance remain under-explored. This is particularly important in the\ncontext of multimodal datasets whose main source is the World Wide Web,\ncondensed and packaged as the Common Crawl dump, which is known to exhibit\nnumerous drawbacks. In this paper, we evaluate the downstream impact of dataset\nscaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and\nLAION-2B datasets by measuring racial and gender bias using the Chicago Face\nDataset (CFD) as the probe. Our results show that as the training data\nincreased, the probability of a pre-trained CLIP model misclassifying human\nimages as offensive non-human classes such as chimpanzee, gorilla, and\norangutan decreased, but misclassifying the same images as human offensive\nclasses such as criminal increased. Furthermore, of the 14 Vision\nTransformer-based VLMs we evaluated, the probability of predicting an image of\na Black man and a Latino man as criminal increases by 65% and 69%,\nrespectively, when the dataset is scaled from 400M to 2B samples for the larger\nViT-L models. Conversely, for the smaller base ViT-B models, the probability of\npredicting an image of a Black man and a Latino man as criminal decreases by\n20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples.\nWe ground the model audit results in a qualitative and historical analysis,\nreflect on our findings and their implications for dataset curation practice,\nand close with a summary of mitigation mechanisms and ways forward. Content\nwarning: This article contains racially dehumanising and offensive\ndescriptions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.05382,regular,post_llm,2024,5,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'DrawL: Understanding the Effects of Non-Mainstream Dialects in Prompted\n  Image Generation\n\n  Text-to-image models are now easy to use and ubiquitous. However, prior work\nhas found that they are prone to recapitulating harmful Western stereotypes.\nFor example, requesting that a model generate an ""African person and their\nhouse,"" may produce a person standing next to a straw hut. In this example, the\nword ""African"" is an explicit descriptor of the person that the prompt is\nseeking to depict. Here, we examine whether implicit markers, such as dialect,\ncan also affect the portrayal of people in text-to-image outputs. We pair\nprompts in Mainstream American English with counterfactuals that express\ngrammatical constructions found in dialects correlated with historically\nmarginalized groups. We find that through minimal, syntax-only changes to\nprompts, we can systematically shift the skin tone and gender of people in the\ngenerated images. We conclude with a discussion of whether dialectic\ndistribution shifts like this are harmful or are expected, possibly even\ndesirable, model behavior.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.07826,review,post_llm,2024,5,"{'ai_likelihood': 2.0464261372884117e-05, 'text': ""A View of How Language Models Will Transform Law\n\n  While most commentators have focused exclusively on how LLMs will transform\nday-to-day law practice, a substantial structural change could be afoot within\nthe legal sector as a whole. Large increases in productivity and attendant cost\nsavings could encourage law firms and corporate legal departments to develop\nlarge language models in-house. A ten percent increase in attorney productivity\nwould encourage an average sized 'Big Law' firm to reduce its associate\nheadcount by 300 to 400 lawyers. This represents cost savings of 60 to 120\nmillion dollars - more than enough to pay for the development of a specialized\nLLM. Eventually, LLMs will push lawyers into highly specialized and nuanced\nroles. After fully mature LLMs arrive, the lawyer will continue to play a\ncentral role in legal practice, but only in non-routine legal tasks. These\ntasks will primarily involve value judgments, such as the development of\nprecedent or its reversal, or the allocation of property and other scarce\nresources. This new mix of lawyer-machine labor, where machines primarily carry\nout routine legal tasks, and lawyers handle the non-routine, will give rise to\na growing demand for lawyers who can exercise good judgment and empathize with\nthe winners and losers of social change. Overall, the Article suggests a\npossible future where there are fewer lawyers and greater consolidation of the\nlegal sector.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2405.14744,regular,post_llm,2024,5,"{'ai_likelihood': 9.867880079481338e-06, 'text': ""Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View\n\n  Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.12139,regular,post_llm,2024,5,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Propensity towards Ownership and Use of Automated Vehicles: Who Are the\n  Adopters? Who Are the Non-adopters? Who Is Hesitant?\n\n  The objective of this study is to investigate automated vehicle (AV) adoption\nperceptions, including ownership intentions and the willingness to use\nself-driving mobility services. In this paper, we use data from the 2018\nCalifornia Transportation Survey, and use K-means, a clustering technique in\ndata mining, to reveal patterns of potential AV owners (and non-owners) as well\nas AV users (and non-users) of self-driving services. The results reveal seven\nclusters, namely Multitaskers/ environmentalists/ impaired drivers, Tech\nmavens/ travelers, Life in transition, Captive car-users, Public/ active\ntransport users, Sub-urban Dwellers, and Car enthusiasts. The first two\nclusters include adopters who are largely familiar with AVs, are tech savvy,\nand who make good use of time during their commute. The last cluster comprise\nof non-adopters who are car enthusiasts. On the other hand, people who are Life\nin transition, Captive car-users, Public/ active transport users, and Sub-urban\ndwellers show uncertain perceptions towards being AV adopters. They are either\npursuing higher education, having a busy schedule, supporting for sustainable\nsociety via government policies, or have a stable life, respectively. Insights\nfrom this study help practitioners to build business models and strategic\nplanning, addressing potential market segments of individuals that are willing\nto own an AV vs. those that are more inclined to use self-driving mobility\nservices. The ""gray"" segments identify a latent untapped demand and a potential\ntarget for marketing, campaigns, and sales.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11976,regular,post_llm,2024,5,"{'ai_likelihood': 1.0, 'text': 'Exploratory Data Analysis for Banking and Finance: Unveiling Insights\n  and Patterns\n\n  This paper explores the application of Exploratory Data Analytics (EDA) in\nthe banking and finance domain, focusing on credit card usage and customer\nchurning. It presents a step-by-step analysis using EDA techniques such as\ndescriptive statistics, data visualization, and correlation analysis. The study\nexamines transaction patterns, credit limits, and usage across merchant\ncategories, providing insights into consumer behavior. It also considers\ndemographic factors like age, gender, and income on usage patterns.\nAdditionally, the report addresses customer churning, analyzing churn rates and\nfactors such as demographics, transaction history, and satisfaction levels.\nThese insights help banking professionals make data-driven decisions, improve\nmarketing strategies, and enhance customer retention, ultimately contributing\nto profitability.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.016876220703125, 'GPT4': 0.09295654296875, 'CLAUDE': 0.007564544677734375, 'GOOGLE': 0.810546875, 'OPENAI_O_SERIES': 0.0005927085876464844, 'DEEPSEEK': 0.0007462501525878906, 'GROK': 0.00017249584197998047, 'NOVA': 0.0004024505615234375, 'OTHER': 0.0703125, 'HUMAN': 4.410743713378906e-05}}"
2407.12796,regular,post_llm,2024,6,"{'ai_likelihood': 0.99755859375, 'text': 'AI Agents and Education: Simulated Practice at Scale\n\n  This paper explores the potential of generative AI in creating adaptive\neducational simulations. By leveraging a system of multiple AI agents,\nsimulations can provide personalized learning experiences, offering students\nthe opportunity to practice skills in scenarios with AI-generated mentors,\nrole-players, and instructor-facing evaluators. We describe a prototype,\nPitchQuest, a venture capital pitching simulator that showcases the\ncapabilities of AI in delivering instruction, facilitating practice, and\nproviding tailored feedback. The paper discusses the pedagogy behind the\nsimulation, the technology powering it, and the ethical considerations in using\nAI for education. While acknowledging the limitations and need for rigorous\ntesting, we propose that generative AI can significantly lower the barriers to\ncreating effective, engaging simulations, opening up new possibilities for\nexperiential learning at scale.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0012454986572265625, 'GPT4': 0.007686614990234375, 'CLAUDE': 0.00879669189453125, 'GOOGLE': 0.95166015625, 'OPENAI_O_SERIES': 7.838010787963867e-05, 'DEEPSEEK': 0.0245513916015625, 'GROK': 0.0004725456237792969, 'NOVA': 8.147954940795898e-05, 'OTHER': 0.005306243896484375, 'HUMAN': 0.0003211498260498047}}"
2406.14243,regular,post_llm,2024,6,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'AuditMAI: Towards An Infrastructure for Continuous AI Auditing\n\n  Artificial Intelligence (AI) Auditability is a core requirement for achieving\nresponsible AI system design. However, it is not yet a prominent design feature\nin current applications. Existing AI auditing tools typically lack integration\nfeatures and remain as isolated approaches. This results in manual,\nhigh-effort, and mostly one-off AI audits, necessitating alternative methods.\nInspired by other domains such as finance, continuous AI auditing is a\npromising direction to conduct regular assessments of AI systems. The issue\nremains, however, since the methods for continuous AI auditing are not mature\nyet at the moment. To address this gap, we propose the Auditability Method for\nAI (AuditMAI), which is intended as a blueprint for an infrastructure towards\ncontinuous AI auditing. For this purpose, we first clarified the definition of\nAI auditability based on literature. Secondly, we derived requirements from two\nindustrial use cases for continuous AI auditing tool support. Finally, we\ndeveloped AuditMAI and discussed its elements as a blueprint for a continuous\nAI auditability infrastructure.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.06453,regular,post_llm,2024,6,"{'ai_likelihood': 5.1657358805338544e-06, 'text': 'Time Series Analysis: yesterday, today, tomorrow\n\n  Forecasts of various processes have always been a sophisticated problem for\nstatistics and data science. Over the past decades the solution procedures were\nupdated by deep learning and kernel methods. According to many specialists,\nthese approaches are much more precise, stable, and suitable compared to the\nclassical statistical linear time series methods. Here we investigate how true\nthis point of view is.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17472,regular,post_llm,2024,6,"{'ai_likelihood': 1.5033615960015191e-05, 'text': 'Promoting Health via mHealth Applications Using a French Version of the\n  Mobile App Rating Scale: Adaptation and Validation Study\n\n  Background In the recent decades, the number of apps promoting health\nbehaviors and health-related strategies and interventions has increased\nalongside the number of smartphone users. Nevertheless, the validity process\nfor measuring and reporting app quality remains unsatisfactory for health\nprofessionals and end users and represents a public health concern. The Mobile\nApplication Rating Scale (MARS) is a tool validated and widely used in the\nscientific literature to evaluate and compare mHealth app functionalities.\nHowever, MARS is not adapted to the French culture nor to the language.\nObjective This study aims to translate, adapt, and validate the equivalent\nFrench version of MARS (ie, MARS-F). Methods The original MARS was first\ntranslated to French by two independent bilingual scientists, and their common\nversion was blind back-translated twice by two native English speakers,\nculminating in a final well-established MARS-F. Its comprehensibility was then\nevaluated by 6 individuals (3 researchers and 3 nonacademics), and the final\nMARS-F version was created. Two bilingual raters independently completed the\nevaluation of 63 apps using MARS and MARS-F. Interrater reliability was\nassessed using intraclass correlation coefficients. In addition, internal\nconsistency and validity of both scales were assessed. Mokken scale analysis\nwas used to investigate the scalability of both MARS and MARS-F. Results MARS-F\nhad a good alignment with the original MARS, with properties comparable between\nthe two scales. The correlation coefficients (r) between the corresponding\ndimensions of MARS and MARS-F ranged from 0.97 to 0.99. The internal\nconsistencies of the MARS-F dimensions engagement ($\\omega$=0.79),\nfunctionality ($\\omega$=0.79), esthetics ($\\omega$=0.78), and information\nquality ($\\omega$=0.61) were acceptable and that for the overall MARS score\n($\\omega$=0.86) was good. Mokken scale analysis revealed a strong scalability\nfor MARS (Loevinger H=0.37) and a good scalability for MARS-F (H=0.35).\nConclusions MARS-F is a valid tool, and it would serve as a crucial aid for\nresearchers, health care professionals, public health authorities, and\ninterested third parties, to assess the quality of mHealth apps in\nFrench-speaking countries.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.0811,review,post_llm,2024,6,"{'ai_likelihood': 0.0004511409335666233, 'text': 'Conference Proceedings of The European DAO Workshop 2024\n\n  The European DAO Workshop 2024 held on July 4th/5th in Winterthur,\nSwitzerland aims to explore the challenges and opportunities of Decentralized\nAutonomous Organizations (DAOs). Its goal is to foster innovation and knowledge\ntransfer between academics and practitioners to advance DAOs as a new\norganizational structure. This collection of full papers delves into areas such\nas decentralized decision-making, business models, artificial intelligence,\neconomics, and legal challenges for DAOs. This diverse compilation offers a\nmulti-disciplinary examination of the rapidly growing phenomenon of DAOs that\nare based on blockchain technology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.14873,regular,post_llm,2024,6,"{'ai_likelihood': 1.0, 'text': 'Beyond Accidents and Misuse: Decoding the Structural Risk Dynamics of Artificial Intelligence\n\nAs artificial intelligence (AI) becomes increasingly embedded in the core functions of social, political, and economic life, it catalyzes structural transformations with far-reaching societal implications. This paper advances the concept of structural risk by introducing a framework grounded in complex systems research to examine how rapid AI integration can generate emergent, system-level dynamics beyond conventional, proximate threats such as system failures or malicious misuse. It argues that such risks are both influenced by and constitutive of broader sociotechnical structures. We classify structural risks into three interrelated categories: antecedent structural causes, antecedent AI system causes, and deleterious feedback loops. By tracing these interactions, we show how unchecked AI development can destabilize trust, shift power asymmetries, and erode decision-making agency across scales. To anticipate and govern these dynamics, the paper proposes a methodological agenda incorporating scenario mapping, simulation, and exploratory foresight. We conclude with policy recommendations aimed at cultivating institutional resilience and adaptive governance strategies for navigating an increasingly volatile AI risk landscape.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.0994415283203125e-06, 'GPT4': 0.5947265625, 'CLAUDE': 0.0225372314453125, 'GOOGLE': 4.231929779052734e-06, 'OPENAI_O_SERIES': 3.993511199951172e-06, 'DEEPSEEK': 0.382568359375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 4.649162292480469e-06}}"
2406.04557,regular,post_llm,2024,6,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Countrywide natural experiment reveals impact of built environment on\n  physical activity\n\n  While physical activity is critical to human health, most people do not meet\nrecommended guidelines. More walkable built environments have the potential to\nincrease activity across the population. However, previous studies on the built\nenvironment and physical activity have led to mixed findings, possibly due to\nmethodological limitations such as small cohorts, few or single locations,\nover-reliance on self-reported measures, and cross-sectional designs. Here, we\naddress these limitations by leveraging a large U.S. cohort of smartphone users\n(N=2,112,288) to evaluate within-person longitudinal behavior changes that\noccurred over 248,266 days of objectively-measured physical activity across\n7,447 relocations among 1,609 U.S. cities. By analyzing the results of this\nnatural experiment, which exposed individuals to differing built environments,\nwe find that increases in walkability are associated with significant increases\nin physical activity after relocation (and vice versa). These changes hold\nacross subpopulations of different genders, age, and body-mass index (BMI), and\nare sustained over three months after moving.The added activity observed after\nmoving to a more walkable location is predominantly composed of\nmoderate-to-vigorous physical activity (MVPA), which is linked to an array of\nassociated health benefits across the life course. A simulation experiment\ndemonstrates that substantial walkability improvements (i.e., bringing all US\nlocations to the walkability level of Chicago or Philadelphia) may lead to\n10.3% or 33 million more Americans meeting aerobic physical activity\nguidelines. Evidence against residential self-selection confounding is\nreported. Our findings provide robust evidence supporting the importance of the\nbuilt environment in directly improving health-enhancing physical activity, in\naddition to offering potential guidance for public policy activities in this\narea.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.13103,review,post_llm,2024,6,"{'ai_likelihood': 5.430645412868924e-06, 'text': 'Participatory Approaches in AI Development and Governance: Case Studies\n\n  This paper forms the second of a two-part series on the value of a\nparticipatory approach to AI development and deployment. The first paper had\ncrafted a principled, as well as pragmatic, justification for deploying\nparticipatory methods in these two exercises (that is, development and\ndeployment of AI). The pragmatic justification is that it improves the quality\nof the overall algorithm by providing more granular and minute information. The\nmore principled justification is that it offers a voice to those who are going\nto be affected by the deployment of the algorithm, and through engagement\nattempts to build trust and buy-in for an AI system. By a participatory\napproach, we mean including various stakeholders (defined a certain way) in the\nactual decision making process through the life cycle of an AI system. Despite\nthe justifications offered above, actual implementation depends crucially on\nhow stakeholders in the entire process are identified, what information is\nelicited from them, and how it is incorporated. This paper will test these\npreliminary conclusions in two sectors, the use of facial recognition\ntechnology in the upkeep of law and order and the use of large language models\nin the healthcare sector. These sectors have been chosen for two primary\nreasons. Since Facial Recognition Technologies are a branch of AI solutions\nthat are well-researched and the impact of which is well documented, it\nprovides an established space to illustrate the various aspects of adapting PAI\nto an existing domain, especially one that has been quite contentious in the\nrecent past. LLMs in healthcare provide a canvas for a relatively less explored\nspace, and helps us illustrate how one could possibly envision enshrining the\nprinciples of PAI for a relatively new technology, in a space where innovation\nmust always align with patient welfare.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.15013,review,post_llm,2024,6,"{'ai_likelihood': 1.9835101233588324e-05, 'text': 'Teaching Digital Accessibility in Computing Education: Views of\n  Educators in India\n\n  In recent years, there has been rising interest from both governments and\nprivate industry in developing software that is accessible to all, including\npeople with disabilities. However, the computer science (CS) courses that ought\nto prepare future professionals to develop such accessible software hardly\ncover topics related to accessibility. While there is growing literature on\nincorporating accessibility topics in computing education in the West, there is\nlittle work on this in the Global South, particularly in India, which has a\nlarge number of computing students and software professionals. In this\nreplication report, we present (A) our findings from a replication of surveys\nused in the US and Switzerland on who teaches accessibility and barriers to\nteaching accessibility and (B) a qualitative analysis of perceptions of CS\nfaculty in India about digital accessibility and teaching accessibility. Our\nstudy corroborates the findings of the earlier surveys: very few CS faculty\nteach accessibility, and the top barriers they perceive are the same. The\nqualitative analysis further reveals that the faculty in India need training on\naccessibility concepts and disabilities sensitization, and exposure to existing\nand ongoing CS education research and pedagogies. In light of these findings,\nwe present recommendations aimed at addressing these challenges and enhancing\nthe integration of accessibility into computing education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.01617,review,post_llm,2024,6,"{'ai_likelihood': 9.099642435709636e-05, 'text': 'Subjective fairness in algorithmic decision-support\n\n  The treatment of fairness in decision-making literature usually involves\nquantifying fairness using objective measures. This work takes a critical\nstance to highlight the limitations of these approaches (group fairness and\nindividual fairness) using sociological insights. First, we expose how these\nmetrics often fail to reflect societal realities. By neglecting crucial\nhistorical, cultural, and social factors, they fall short of capturing all\ndiscriminatory practices. Second, we redefine fairness as a subjective property\nmoving from a top-down to a bottom-up approach. This shift allows the inclusion\nof diverse stakeholders perceptions, recognizing that fairness is not merely\nabout objective metrics but also about individuals views on their treatment.\nFinally, we aim to use explanations as a mean to achieve fairness. Our approach\nemploys explainable clustering to form groups based on individuals subjective\nperceptions to ensure that individuals who see themselves as similar receive\nsimilar treatment. We emphasize the role of explanations in achieving fairness,\nfocusing not only on procedural fairness but also on providing subjective\nexplanations to convince stakeholders of their fair treatment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.00478,review,post_llm,2024,6,"{'ai_likelihood': 0.99755859375, 'text': ""Green Supply Chain Management Optimization Based on Chemical Industrial\n  Clusters\n\n  Post-pandemic, the chemical sector faces new challenges crucial to national\nprogress, with a pressing need for rapid transformation and upgrading. The\npandemic's impact and increasing demand for sustainability have highlighted the\nimportance of green supply chain management. This study used a questionnaire\nsurvey and analyzed the data with SPSS and AMOS to investigate the influence of\nfactors like regulatory compliance, green procurement, manufacturing,\nlogistics, sales, competitors, internal environmental protection, and cost\ncontrol on green supply chain management awareness and implementation in\nchemical enterprises. The results show that these factors significantly enhance\ngreen supply chain management, contributing to economic and environmental\nbenefits. This paper provides a theoretical framework to improve green supply\nchain efficiency in chemical clusters, promoting sustainable industry growth.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0257415771484375, 'GPT4': 0.442626953125, 'CLAUDE': 0.0245208740234375, 'GOOGLE': 0.27734375, 'OPENAI_O_SERIES': 0.033599853515625, 'DEEPSEEK': 0.036834716796875, 'GROK': 0.0011692047119140625, 'NOVA': 0.00604248046875, 'OTHER': 0.15087890625, 'HUMAN': 0.0012540817260742188}}"
2406.09029,review,post_llm,2024,6,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Fair by design: A sociotechnical approach to justifying the fairness of\n  AI-enabled systems across the lifecycle\n\n  Fairness is one of the most commonly identified ethical principles in\nexisting AI guidelines, and the development of fair AI-enabled systems is\nrequired by new and emerging AI regulation. But most approaches to addressing\nthe fairness of AI-enabled systems are limited in scope in two significant\nways: their substantive content focuses on statistical measures of fairness,\nand they do not emphasize the need to identify and address fairness\nconsiderations across the whole AI lifecycle. Our contribution is to present an\nassurance framework and tool that can enable a practical and transparent method\nfor widening the scope of fairness considerations across the AI lifecycle and\nmove the discussion beyond mere statistical notions of fairness to consider a\nricher analysis in a practical and context-dependent manner. To illustrate this\napproach, we first describe and then apply the framework of Trustworthy and\nEthical Assurance (TEA) to an AI-enabled clinical diagnostic support system\n(CDSS) whose purpose is to help clinicians predict the risk of developing\nhypertension in patients with Type 2 diabetes, a context in which several\nfairness considerations arise (e.g., discrimination against patient subgroups).\nThis is supplemented by an open-source tool and a fairness considerations map\nto help facilitate reasoning about the fairness of AI-enabled systems in a\nparticipatory way. In short, by using a shared framework for identifying,\ndocumenting and justifying fairness considerations, and then using this\ndeliberative exercise to structure an assurance case, research on AI fairness\nbecomes reusable and generalizable for others in the ethical AI community and\nfor sharing best practices for achieving fairness and equity in digital health\nand healthcare in particular.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.0348,review,post_llm,2024,6,"{'ai_likelihood': 5.861123402913412e-06, 'text': 'Unpacking Approaches to Learning and Teaching Machine Learning in K-12\n  Education: Transparency, Ethics, and Design Activities\n\n  In this conceptual paper, we review existing literature on artificial\nintelligence/machine learning (AI/ML) education to identify three approaches to\nhow learning and teaching ML could be conceptualized. One of them, a\ndata-driven approach, emphasizes providing young people with opportunities to\ncreate data sets, train, and test models. A second approach, learning\nalgorithm-driven, prioritizes learning about how the learning algorithms or\nengines behind how ML models work. In addition, we identify efforts within a\nthird approach that integrates the previous two. In our review, we focus on how\nthe approaches: (1) glassbox and blackbox different aspects of ML, (2) build on\nlearner interests and provide opportunities for designing applications, (3)\nintegrate ethics and justice. In the discussion, we address the challenges and\nopportunities of current approaches and suggest future directions for the\ndesign of learning activities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17473,regular,post_llm,2024,6,"{'ai_likelihood': 2.7583705054389106e-05, 'text': 'Improving engagement, diversity, and retention in computer science with\n  RadGrad: Results of a case study\n\n  RadGrad is a curriculum initiative implemented via an application that\ncombines features of social networks, degree planners, individual learning\nplans, and serious games. RadGrad redefines traditional meanings of ""progress""\nand ""success"" in the undergraduate computer science degree program in an\nattempt to improve engagement, retention, and diversity. In this paper, we\ndescribe the RadGrad Project and report on an evaluation study designed to\nassess the impact of RadGrad on student engagement, diversity, and retention.\nWe also present opportunities and challenges that result from the use of the\nsystem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.14123,review,post_llm,2024,6,"{'ai_likelihood': 6.755193074544271e-06, 'text': 'Mapping AI Ethics Narratives: Evidence from Twitter Discourse Between\n  2015 and 2022\n\n  Public participation is indispensable for an insightful understanding of the\nethics issues raised by AI technologies. Twitter is selected in this paper to\nserve as an online public sphere for exploring discourse on AI ethics,\nfacilitating broad and equitable public engagement in the development of AI\ntechnology. A research framework is proposed to demonstrate how to transform AI\nethics-related discourse on Twitter into coherent and readable narratives. It\nconsists of two parts: 1) combining neural networks with large language models\nto construct a topic hierarchy that contains popular topics of public concern\nwithout ignoring small but important voices, thus allowing a fine-grained\nexploration of meaningful information. 2) transforming fragmented and\ndifficult-to-understand social media information into coherent and easy-to-read\nstories through narrative visualization, providing a new perspective for\nunderstanding the information in Twitter data. This paper aims to advocate for\npolicy makers to enhance public oversight of AI technologies so as to promote\ntheir fair and sustainable development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.18519,regular,post_llm,2024,6,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Distinguishing mechanisms of social contagion from local network view\n\n  The adoption of individual behavioural patterns is largely determined by\nstimuli arriving from peers via social interactions or from external sources.\nBased on these influences, individuals are commonly assumed to follow simple or\ncomplex adoption rules, inducing social contagion processes. In reality,\nmultiple adoption rules may coexist even within the same social contagion\nprocess, introducing additional complexity into the spreading phenomena. Our\ngoal is to understand whether coexisting adoption mechanisms can be\ndistinguished from a microscopic view, at the egocentric network level, without\nrequiring global information about the underlying network, or the unfolding\nspreading process. We formulate this question as a classification problem, and\nstudy it through a Bayesian likelihood approach and with random forest\nclassifiers in various synthetic and data-driven experiments. This study offers\na novel perspective on the observations of propagation processes at the\negocentric level and a better understanding of landmark contagion mechanisms\nfrom a local view.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11062,regular,post_llm,2024,6,"{'ai_likelihood': 1.0, 'text': 'To Ban or Not to Ban: Uses and Gratifications of Mobile Phones among Township High School Learners\n\nThe proliferation of mobile phone usage among learners from diverse socio-economic backgrounds has prompted school authorities to contemplate banning these devices within educational institutions. This research seeks to explore the motivations and usage patterns of high school learners in response to the proposed ban. Employing a mixed-methods approach, we conducted surveys and interviews with 262 students from three township schools in the Western Cape province of South Africa. Grounded in the Uses and Gratification Theory (UGT), our study examined four key categories: reasons for mobile phone use, usage patterns, purchasing influences, and behavioral factors. Our findings reveal a predominant opposition among students to the ban, despite a significant number opting to leave their phones at home due to concerns about theft and robbery in their neighborhoods. Financial constraints, specifically the inability to afford data bundles and airtime, also contribute to this behavior. Notably, 40% of the participants reported using their phones for more than five hours daily, a duration classified as overuse in existing literature. The primary motivations for mobile phone use among these learners include socializing, internet browsing for non-educational purposes, and using the device for entertainment and recreation. This study highlights critical insights into the nuanced relationship between high school learners and mobile phone usage, offering valuable perspectives for policymakers and educators considering the implications of a mobile phone ban in schools.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0010528564453125, 'GPT4': 0.240966796875, 'CLAUDE': 0.003070831298828125, 'GOOGLE': 0.73291015625, 'OPENAI_O_SERIES': 0.005779266357421875, 'DEEPSEEK': 2.8908252716064453e-05, 'GROK': 4.887580871582031e-06, 'NOVA': 0.00015437602996826172, 'OTHER': 0.0161590576171875, 'HUMAN': 8.696317672729492e-05}}"
2407.11197,regular,post_llm,2024,6,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'A Vision to Enhance Trust Requirements for Peer Support Systems by\n  Revisiting Trust Theories\n\n  This vision paper focuses on the mental health crisis impacting healthcare\nworkers (HCWs), which exacerbated by the COVID-19 pandemic, leads to increased\nstress and psychological issues like burnout. Peer Support Programs (PSP) are a\nrecognized intervention for mitigating these issues. These programs are\nincreasingly being delivered virtually through Peer Support Systems (PSS) for\nincreased convenience and accessibility. However, HCWs perception of these\nsystems results in fear of information sharing, perceived lack of safety, and\nlow participation rate, which challenges these systems ability to achieve their\ngoals. In line with the rich body of research on the requirements and\nproperties of trustworthy systems, we posit that increasing HCWs trust in PSS\ncould address these challenges. However, extant research focuses on objectively\ndefined trustworthiness rather than perceptual trust because trustworthy\nrequirements are viewed as more controllable and easier to operationalize. This\nstudy proposes a novel approach to elicit perceptual trust requirements by\nproposing a trust framework anchored in recognized trust theories from\ndifferent disciplines that unpacks trust into its recognized types and their\nantecedents. This approach allows the identification of trust requirements\nbeyond those already proposed for trustworthy systems, providing a strong\nfoundation for improving the effectiveness of PSS for HCWs. Keywords: Trust\nRequirements, Requirements elicitation, Peer support systems, Healthcare\nworkers\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.00591,regular,post_llm,2024,6,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""Auditing for Racial Discrimination in the Delivery of Education Ads\n\n  Digital ads on social-media platforms play an important role in shaping\naccess to economic opportunities. Our work proposes and implements a new\nthird-party auditing method that can evaluate racial bias in the delivery of\nads for education opportunities. Third-party auditing is important because it\nallows external parties to demonstrate presence or absence of bias in\nsocial-media algorithms. Education is a domain with legal protections against\ndiscrimination and concerns of racial-targeting, but bias induced by ad\ndelivery algorithms has not been previously explored in this domain. Prior\naudits demonstrated discrimination in platforms' delivery of ads to users for\nhousing and employment ads. These audit findings supported legal action that\nprompted Meta to change their ad-delivery algorithms to reduce bias, but only\nin the domains of housing, employment, and credit. In this work, we propose a\nnew methodology that allows us to measure racial discrimination in a platform's\nad delivery algorithms for education ads. We apply our method to Meta using ads\nfor real schools and observe the results of delivery. We find evidence of\nracial discrimination in Meta's algorithmic delivery of ads for education\nopportunities, posing legal and ethical concerns. Our results extend evidence\nof algorithmic discrimination to the education domain, showing that current\nbias mitigation mechanisms are narrow in scope, and suggesting a broader role\nfor third-party auditing of social media in areas where ensuring\nnon-discrimination is important.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.01862,review,post_llm,2024,6,"{'ai_likelihood': 1.0, 'text': 'Charting the Landscape of Nefarious Uses of Generative Artificial\n  Intelligence for Online Election Interference\n\n  Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\npose significant risks, particularly in the realm of online election\ninterference. This paper explores the nefarious applications of GenAI,\nhighlighting their potential to disrupt democratic processes through deepfakes,\nbotnets, targeted misinformation campaigns, and synthetic identities. By\nexamining recent case studies and public incidents, we illustrate how malicious\nactors exploit these technologies to try influencing voter behavior, spread\ndisinformation, and undermine public trust in electoral systems. The paper also\ndiscusses the societal implications of these threats, emphasizing the urgent\nneed for robust mitigation strategies and international cooperation to\nsafeguard democratic integrity.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0025806427001953125, 'GPT4': 0.148193359375, 'CLAUDE': 0.08087158203125, 'GOOGLE': 0.3232421875, 'OPENAI_O_SERIES': 0.00717926025390625, 'DEEPSEEK': 0.1839599609375, 'GROK': 0.01090240478515625, 'NOVA': 0.052459716796875, 'OTHER': 0.190673828125, 'HUMAN': 1.1920928955078125e-05}}"
2407.11199,review,post_llm,2024,6,"{'ai_likelihood': 7.616149054633247e-07, 'text': ""Algorithms for College Admissions Decision Support: Impacts of Policy\n  Change and Inherent Variability\n\n  Each year, selective American colleges sort through tens of thousands of\napplications to identify a first-year class that displays both academic merit\nand diversity. In the 2023-2024 admissions cycle, these colleges faced\nunprecedented challenges. First, the number of applications has been steadily\ngrowing. Second, test-optional policies that have remained in place since the\nCOVID-19 pandemic limit access to key information historically predictive of\nacademic success. Most recently, longstanding debates over affirmative action\nculminated in the Supreme Court banning race-conscious admissions. Colleges\nhave explored machine learning (ML) models to address the issues of scale and\nmissing test scores, often via ranking algorithms intended to focus on 'top'\napplicants. However, the Court's ruling will force changes to these models,\nwhich were able to consider race as a factor in ranking. There is currently a\npoor understanding of how these mandated changes will shape applicant ranking\nalgorithms, and, by extension, admitted classes. We seek to address this by\nquantifying the impact of different admission policies on the applications\nprioritized for review. We show that removing race data from a developed\napplicant ranking algorithm reduces the diversity of the top-ranked pool\nwithout meaningfully increasing the academic merit of that pool. We\ncontextualize this impact by showing that excluding data on applicant race has\na greater impact than excluding other potentially informative variables like\nintended majors. Finally, we measure the impact of policy change on individuals\nby comparing the arbitrariness in applicant rank attributable to policy change\nto the arbitrariness attributable to randomness. We find that any given policy\nhas a high degree of arbitrariness and that removing race data from the ranking\nalgorithm increases arbitrariness in outcomes for most applicants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.1203,review,post_llm,2024,6,"{'ai_likelihood': 6.457169850667318e-06, 'text': ""Towards a Harms Taxonomy of AI Likeness Generation\n\n  Generative artificial intelligence models, when trained on a sufficient\nnumber of a person's images, can replicate their identifying features in a\nphotorealistic manner. We refer to this process as 'likeness generation'.\nLikeness-featuring synthetic outputs often present a person's likeness without\ntheir control or consent, and may lead to harmful consequences. This paper\nexplores philosophical and policy issues surrounding generated likeness. It\nbegins by offering a conceptual framework for understanding likeness generation\nby examining the novel capabilities introduced by generative systems. The paper\nthen establishes a definition of likeness by tracing its historical development\nin legal literature. Building on this foundation, we present a taxonomy of\nharms associated with generated likeness, derived from a comprehensive\nmeta-analysis of relevant literature. This taxonomy categorises harms into\nseven distinct groups, unified by shared characteristics. Utilising this\ntaxonomy, we raise various considerations that need to be addressed for the\ndeployment of appropriate mitigations. Given the multitude of stakeholders\ninvolved in both the creation and distribution of likeness, we introduce\nconcepts such as indexical sufficiency, a distinction between generation and\ndistribution, and harms as having a context-specific nature. This work aims to\nserve industry, policymakers, and future academic researchers in their efforts\nto address the societal challenges posed by likeness generation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.01323,review,post_llm,2024,6,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""Structural Interventions and the Dynamics of Inequality\n\n  Recent conversations in the algorithmic fairness literature have raised\nseveral concerns with standard conceptions of fairness. First, constraining\npredictive algorithms to satisfy fairness benchmarks may lead to non-optimal\noutcomes for disadvantaged groups. Second, technical interventions are often\nineffective by themselves, especially when divorced from an understanding of\nstructural processes that generate social inequality. Inspired by both these\ncritiques, we construct a common decision-making model, using mortgage loans as\na running example. We show that under some conditions, any choice of decision\nthreshold will inevitably perpetuate existing disparities in financial\nstability unless one deviates from the Pareto optimal policy. Then, we model\nthe effects of three different types of interventions. We show how different\ninterventions are recommended depending upon the difficulty of enacting\nstructural change upon external parameters and depending upon the policymaker's\npreferences for equity or efficiency. Counterintuitively, we demonstrate that\npreferences for efficiency over equity may lead to recommendations for\ninterventions that target the under-resourced group. Finally, we simulate the\neffects of interventions on a dataset that combines HMDA and Fannie Mae loan\ndata. This research highlights the ways that structural inequality can be\nperpetuated by seemingly unbiased decision mechanisms, and it shows that in\nmany situations, technical solutions must be paired with external,\ncontext-aware interventions to enact social change.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.08605,review,post_llm,2024,6,"{'ai_likelihood': 7.384353213840061e-06, 'text': ""Perils of current DAO governance\n\n  DAO Governance is currently broken. We survey the state of the art and find\nworrying conclusions. Vote buying, vote selling and coercion are easy. The\nwealthy rule, decentralisation is a myth. Hostile take-overs are incentivised.\nBallot secrecy is non-existent or short lived, despite being a human right.\nVerifiablity is achieved at the expense of privacy. These privacy concerns are\nhighlighted with case study analyses of Vocdoni's governance protocol. This\nwork presents two contributions: firstly a review of current DAO governance\nprotocols, and secondly, an illustration of their vulnerabilities, showcasing\nthe privacy and security threats these entail.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.1314,regular,post_llm,2024,6,"{'ai_likelihood': 2.317958407931858e-06, 'text': 'From decision aiding to the massive use of algorithms: where does the\n  responsibility stand?\n\n  In the very large debates on ethics of algorithms, this paper proposes an\nanalysis on human responsibility. On one hand, algorithms are designed by some\nhumans, who bear a part of responsibility in the results and unexpected\nimpacts. Nevertheless, we show how the fact they cannot embrace the full\nsituations of use and consequences lead to an unreachable limit. On the other\nhand, using technology is never free of responsibility, even if there also\nexist limits to characterise. Massive uses by unprofessional users introduce\nadditional questions that modify the possibilities to be ethically responsible.\nThe article is structured in such a way as to show how the limits have\ngradually evolved, leaving unthought of issues and a failure to share\nresponsibility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.00334,review,post_llm,2024,6,"{'ai_likelihood': 4.758437474568685e-05, 'text': 'Examining and Comparing the Effectiveness of Virtual Reality Serious\n  Games and LEGO Serious Play for Learning Scrum\n\n  Significant research work has been undertaken related to the game-based\nlearning approach over the last years. However, a closer look at this work\nreveals that further research is needed to examine some types of game-based\nlearning approaches such as virtual reality serious games and LEGO Serious\nPlay. This article examines and compares the effectiveness for learning Scrum\nand related agile practices of a serious game based on virtual reality and a\nlearning activity based on the LEGO Serious Play methodology. The presented\nstudy used a quasi-experimental design with two groups, pre- and post-tests,\nand a perceptions questionnaire. The sample was composed of 59 software\nengineering students, 22 of which belonged to group A, while the other 37 were\npart of group B. The students in group A played the virtual reality serious\ngame, whereas the students in group B conducted the LEGO Serious Play activity.\nThe results show that both game-based learning approaches were effective for\nlearning Scrum and related agile practices in terms of learning performance and\nmotivation, but they also show that the students who played the virtual reality\nserious game outperformed their peers from the other group in terms of learning\nperformance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.10506,regular,post_llm,2024,6,"{'ai_likelihood': 7.384353213840061e-06, 'text': ""Validating an Instrument for Teachers' Acceptance of Artificial\n  Intelligence in Education\n\n  As artificial intelligence (AI) receives wider attention in education,\nexamining teachers' acceptance of AI (TAAI) becomes essential. However,\nexisting instruments measuring TAAI reported limited reliability and validity\nevidence and faced some design challenges, such as missing informed definitions\nof AI to participants. This study aimed to develop and validate a TAAI\ninstrument, with providing sufficient evidence for high psychometric quality.\nBased on the literature, we first identified five dimensions of TAAI, including\nperceived usefulness, perceived ease of use, behavioral intention,\nself-efficacy, and anxiety, and then developed items to assess each dimension.\nWe examined the face and content validity using expert review and think-aloud\nwith pre-service teachers. Using the revised instrument, we collected responses\nfrom 274 pre-service teachers and examined the item discriminations to identify\noutlier items. We employed the confirmatory factor analysis and Cronbach's\nalpha to examine the construct validity, convergent validity, discriminant\nvalidity, and reliability. Results confirmed the dimensionality of the scale,\nresulting in 27 items distributed in five dimensions. The study exhibits robust\nvalidity and reliability evidence for TAAI, thus affirming its usefulness as a\nvalid measurement instrument.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.056,regular,post_llm,2024,6,"{'ai_likelihood': 2.1192762586805556e-06, 'text': '61A Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)\n\n  LLM-based chatbots enable students to get immediate, interactive help on\nhomework assignments, but even a thoughtfully-designed bot may not serve all\npedagogical goals. We report here on the development and deployment of a\nGPT-4-based interactive homework assistant (""61A Bot"") for students in a large\nCS1 course; over 2000 students made over 100,000 requests of our Bot across two\nsemesters. Our assistant offers one-shot, contextual feedback within the\ncommand-line ""autograder"" students use to test their code. Our Bot wraps\nstudent code in a custom prompt that supports our pedagogical goals and avoids\nproviding solutions directly. Analyzing student feedback, questions, and\nautograder data, we find reductions in homework-related question rates in our\ncourse forum, as well as reductions in homework completion time when our Bot is\navailable. For students in the 50th-80th percentile, reductions can exceed 30\nminutes per assignment, up to 50% less time than students at the same\npercentile rank in prior semesters. Finally, we discuss these observations,\npotential impacts on student learning, and other potential costs and benefits\nof AI assistance in CS1.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.02275,regular,post_llm,2024,6,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Position: The Causal Revolution Needs Scientific Pragmatism\n\n  Causal models and methods have great promise, but their progress has been\nstalled. Proposals using causality get squeezed between two opposing\nworldviews. Scientific perfectionism--an insistence on only using ""correct""\nmodels--slows the adoption of causal methods in knowledge generating\napplications. Pushing in the opposite direction, the academic discipline of\ncomputer science prefers algorithms with no or few assumptions, and\ntechnologies based on automation and scalability are often selected for\neconomic and business applications. We argue that these system-centric\ninductive biases should be replaced with a human-centric philosophy we refer to\nas scientific pragmatism. The machine learning community must strike the right\nbalance to make space for the causal revolution to prosper.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.08386,review,post_llm,2024,6,"{'ai_likelihood': 2.6490953233506945e-05, 'text': ""Banal Deception Human-AI Ecosystems: A Study of People's Perceptions of LLM-generated Deceptive Behaviour\n\nLarge language models (LLMs) can provide users with false, inaccurate, or misleading information, and we consider the output of this type of information as what Natale (2021) calls `banal' deceptive behaviour. Here, we investigate peoples' perceptions of ChatGPT-generated deceptive behaviour and how this affects peoples' own behaviour and trust. To do this, we use a mixed-methods approach comprising of (i) an online survey with 220 participants and (ii) semi-structured interviews with 12 participants. Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans' perceptions of trust and `worthiness' of talking to ChatGPT are impacted by `banal' deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it. Our findings contribute to the understanding of human-AI interaction dynamics in the context of \\textit{Deceptive AI Ecosystems}, and highlight the importance of user-centric approaches to mitigating the potential harms of deceptive AI technologies."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.10998,review,post_llm,2024,6,"{'ai_likelihood': 4.933940039740669e-06, 'text': 'History-enhanced ICT For Sustainability education: Learning together\n  with Business Computing students\n\n  This research explores the use of History to enhance education in the field\nof ICT For Sustainability ICT4S in response to a challenge from the ICT4S 2023\nconference. No previous studies were found in ICT4S but the literature on\nHistory and Education for Sustainable Development is reviewed. An ICT4S\nlecturer collaborated with History lecturers to add an historic parallel to\neach weeks teaching on a Sustainable Business and Computing unit for final year\nundergraduate BSc Business Computing students. A list of the topics and\nrationale is provided. Student perceptions were surveyed before and after the\nteaching and semi-structured interviews carried out. A majority of students saw\nrelevance to their degree and career. There was an increase in the proportion\nof students with interest in History. The paper explores the lessons learned\nfrom the interdisciplinary collaboration, including topic choice, format and\nperceived value. The project has enhanced the way we approach our subjects as\ncomputing and history educators. We believe this is the first empirical,\nsurvey-based study of the use of history to enhance ICT4S education. The team\nwill extend the research to a larger unit covering a wider range of computing\ndegrees.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.04554,review,post_llm,2024,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Generative AI Needs Adaptive Governance\n\n  Because of the speed of its development, broad scope of application, and its\nability to augment human performance, generative AI challenges the very notions\nof governance, trust, and human agency. The technology's capacity to mimic\nhuman knowledge work, feedback loops including significant uptick in users,\nresearch, investor, policy, and media attention, data and compute resources,\nall lead to rapidly increasing capabilities. For those reasons, adaptive\ngovernance, where AI governance and AI co-evolve, is essential for governing\ngenerative AI. In sharp contrast to traditional governance's regulatory regimes\nthat are based on a mix of rigid one-and-done provisions for disclosure,\nregistration and risk management, which in the case of AI carry the potential\nfor regulatory misalignment, this paper argues that generative AI calls for\nadaptive governance. We define adaptive governance in the context of AI and\noutline an adaptive AI governance framework. We outline actors, roles, as well\nas both shared and actors-specific policy activities. We further provide\nexamples of how the framework could be operationalized in practice. We then\nexplain that the adaptive AI governance stance is not without its risks and\nlimitations, such as insufficient oversight, insufficient depth, regulatory\nuncertainty, and regulatory capture, and provide potential approaches to fix\nthese shortcomings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.06817,review,post_llm,2024,6,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'The Legal Duty to Search for Less Discriminatory Algorithms\n\n  Work in computer science has established that, contrary to conventional\nwisdom, for a given prediction problem there are almost always multiple\npossible models with equivalent performance--a phenomenon often termed model\nmultiplicity. Critically, different models of equivalent performance can\nproduce different predictions for the same individual, and, in aggregate,\nexhibit different levels of impacts across demographic groups. Thus, when an\nalgorithmic system displays a disparate impact, model multiplicity suggests\nthat developers could discover an alternative model that performs equally well,\nbut has less discriminatory impact. Indeed, the promise of model multiplicity\nis that an equally accurate, but less discriminatory algorithm (LDA) almost\nalways exists. But without dedicated exploration, it is unlikely developers\nwill discover potential LDAs. Model multiplicity and the availability of LDAs\nhave significant ramifications for the legal response to discriminatory\nalgorithms, in particular for disparate impact doctrine, which has long taken\ninto account the availability of alternatives with less disparate effect when\nassessing liability. A close reading of legal authorities over the decades\nreveals that the law has on numerous occasions recognized that the existence of\na less discriminatory alternative is sometimes relevant to a defendant\'s burden\nof justification at the second step of disparate impact analysis. Indeed, under\ndisparate impact doctrine, it makes little sense to say that a given\nalgorithmic system used by an employer, creditor, or housing provider is\n""necessary"" if an equally accurate model that exhibits less disparate effect is\navailable and possible to discover with reasonable effort. As a result, we\nargue that the law should place a duty of a reasonable search for LDAs on\nentities that develop and deploy predictive models in covered civil rights\ndomains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.08071,regular,post_llm,2024,6,"{'ai_likelihood': 1.7186005910237632e-05, 'text': 'US College Net Price Prediction Comparing ML Regression Models\n\n  This paper will illustrate the usage of Machine Learning algorithms on US\nCollege Scorecard datasets. For this paper, we will use our knowledge,\nresearch, and development of a predictive model to compare the results of all\nthe models and predict the public and private net prices. This paper focuses on\nanalyzing US College Scorecard data from data published on government websites.\n  Our goal is to use four machine learning regression models to develop a\npredictive model to forecast the equitable net cost for every college,\nencompassing both public institutions and private, whether for-profit or\nnonprofit.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.15011,review,post_llm,2024,6,"{'ai_likelihood': 4.722012413872613e-05, 'text': ""Research on Jing Dong's Self-built Logistics Based on Technology\n  Acceptance Model\n\n  Today is a time of rapid e-commerce development, and Jing Dong, China's\ne-commerce giant, has taken its place in the highly competitive industry with\nits self-built logistics system. This paper analyzed the impact of Jing Dong's\nself-built logistics system characteristics on user satisfaction and continuous\nuse intention by using the Technology Acceptance Model as the theoretical\nframework. This paper collected 295 valid samples using a questionnaire survey;\nall the respondents are users and potential users of Jing Dong from mainland\nChina. The empirical results of data analysis showed that marketing information\nquality, logistics system quality, and logistics service have significant\neffects on the perceived usefulness of Jing Dong's self-built logistics, while\nonly marketing information quality and logistics system quality have\nsignificant effects on the perceived usefulness of self-built logistics among\nthe self-built logistics system characteristics dimensions. Additionally, the\nwillingness to continue using a product and user satisfaction were both\ndirectly and significantly impacted by perceived usefulness; perceived ease of\nuse had an indirect impact on users' willingness to continue use by affecting\nperceived usefulness and user satisfaction, and user satisfaction has the most\nsignificant impact on users' continuous use of Jing Dong shopping and using\nJing Dong self-built logistics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.0552,review,post_llm,2024,6,"{'ai_likelihood': 9.934107462565105e-06, 'text': '""Violation of my body:"" Perceptions of AI-generated non-consensual\n  (intimate) imagery\n\n  AI technology has enabled the creation of deepfakes: hyper-realistic\nsynthetic media. We surveyed 315 individuals in the U.S. on their views\nregarding the hypothetical non-consensual creation of deepfakes depicting them,\nincluding deepfakes portraying sexual acts. Respondents indicated strong\nopposition to creating and, even more so, sharing non-consensually created\nsynthetic content, especially if that content depicts a sexual act. However,\nseeking out such content appeared more acceptable to some respondents.\nAttitudes around acceptability varied further based on the hypothetical\ncreator\'s relationship to the participant, the respondent\'s gender and their\nattitudes towards sexual consent. This study provides initial insight into\npublic perspectives of a growing threat and highlights the need for further\nresearch to inform social norms as well as ongoing policy conversations and\ntechnical developments in generative AI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.06915,review,post_llm,2024,6,"{'ai_likelihood': 1.0, 'text': 'Scalability in Workforce Management: Applying Scalability Principles to\n  Foster a Four-Day Work Week\n\n  The traditional five-day workweek faces mounting challenges, prompting\nexploration of alternative models like the four-day workweek. This research\nexplores the transformative potential of scalability principles derived from\ncloud computing and IT in redefining workforce management for a four-day\nworkweek. The study employs a Multivocal Literacy Research methodology,\ncombining grey literature and systematic review approaches. Through a\ncomprehensive review of related work, the challenges, and benefits of\ntransitioning to a four-day workweek are explored. Pilot programs, clear\ncommunication, and agility are identified as critical success factors. The\nsynthesis of scalability principles in workforce management serves as a\npowerful framework for a smooth transition towards a four-day workweek. By\nprioritizing adaptability, dynamic resource allocation, and data-driven\ninsights, organizations can unlock the full potential of a compressed work\nschedule. This research contributes valuable insights for organizations seeking\nto thrive in the evolving landscape of modern work structures and prioritizing\nemployee well-being.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00013697147369384766, 'GPT4': 0.0005893707275390625, 'CLAUDE': 0.00394439697265625, 'GOOGLE': 0.9951171875, 'OPENAI_O_SERIES': 0.00012135505676269531, 'DEEPSEEK': 3.510713577270508e-05, 'GROK': 5.960464477539063e-08, 'NOVA': 5.0067901611328125e-06, 'OTHER': 0.0001665353775024414, 'HUMAN': 3.516674041748047e-06}}"
2406.06987,review,post_llm,2024,6,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Position Paper: Technical Research and Talent is Needed for Effective AI\n  Governance\n\n  In light of recent advancements in AI capabilities and the increasingly\nwidespread integration of AI systems into society, governments worldwide are\nactively seeking to mitigate the potential harms and risks associated with\nthese technologies through regulation and other governance tools. However,\nthere exist significant gaps between governance aspirations and the current\nstate of the technical tooling necessary for their realisation. In this\nposition paper, we survey policy documents published by public-sector\ninstitutions in the EU, US, and China to highlight specific areas of disconnect\nbetween the technical requirements necessary for enacting proposed policy\nactions, and the current technical state of the art. Our analysis motivates a\ncall for tighter integration of the AI/ML research community within AI\ngovernance in order to i) catalyse technical research aimed at bridging the gap\nbetween current and supposed technical underpinnings of regulatory action, as\nwell as ii) increase the level of technical expertise within governing\ninstitutions so as to inform and guide effective governance of AI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.16138,review,post_llm,2024,6,"{'ai_likelihood': 1.1722246805826824e-05, 'text': 'Pervasive Technology-Enabled Care and Support for People with Dementia:\n  The State of Art and Research Issues\n\n  Dementia is a mental illness that people live with all across the world. No\none is immune. Nothing can predict its onset. The true story of dementia\nremains unknown globally, partly due to the denial of dementia symptoms and\npartly due to the social stigma attached to the disease. In recent years,\ndementia as a mental illness has received a lot of attention from the\nscientific community and healthcare providers. This paper presents a state of\nart survey of pervasive technology enabled care and support for people\nsuffering from Alzheimers dementia. We identify three areas of pervasive\ntechnology support for dementia patients, focusing on care, wellness and active\nliving. A critical analysis of existing research is presented here, exploring\nhow pervasive computing, artificial intelligence (AI) and the Internet of\nThings (IoT) are already supporting and providing comfort to dementia patients,\nparticularly those living alone in the community. The work discusses key\nchallenges and limitations of technology-enabled support owing to reasons like\nlack of accessibility, availability, usability and affordability of technology,\nlimited holistic care approach, and lack of education and information. Future\nresearch directions focusing on how pervasive and connected healthcare can\nbetter support the well being and mental health impacts of Alzheimers dementia\nare also highlighted.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.03266,review,post_llm,2024,6,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Technological Perspective on Digital Sovereignty\n\n  This report for the attention of the Federal Department of Foreign Affairs\n(FDFA) makes a scientific contribution in the context of postulate 22.4411\n""Digital Sovereignty Strategy for Switzerland"" by Councillor of States Heidi\nZ\'graggen. The report shows what digital sovereignty means from a technological\nperspective and what activities are currently being carried out in this regard\nin Switzerland and abroad. It also provides strategic directions and specific\nrecommendations for a future ""Swiss Digital Sovereignty Strategy"".\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.0817,regular,post_llm,2024,6,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""Can AI Understand Human Personality? -- Comparing Human Experts and AI\n  Systems at Predicting Personality Correlations\n\n  We test the abilities of specialised deep neural networks like PersonalityMap\nas well as general LLMs like GPT-4o and Claude 3 Opus in understanding human\npersonality. Specifically, we compare their ability to predict correlations\nbetween personality items to the abilities of lay people and academic experts.\nWe find that when compared with individual humans, all AI models make better\npredictions than the vast majority of lay people and academic experts. However,\nwhen selecting the median prediction for each item, we find a different\npattern: Experts and PersonalityMap outperform LLMs and lay people on most\nmeasures. Our results suggest that while frontier LLMs' are better than most\nindividual humans at predicting correlations between personality items,\nspecialised models like PersonalityMap continue to match or exceed expert human\nperformance even on some outcome measures where LLMs underperform. This\nprovides evidence both in favour of the general capabilities of large language\nmodels and in favour of the continued place for specialised models trained and\ndeployed for specific domains.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.15014,regular,post_llm,2024,6,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'Influence of Personality Traits on Plagiarism Through Collusion in\n  Programming Assignments\n\n  Educating students about academic integrity expectations has been suggested\nas one of the ways to reduce malpractice in take-home programming assignments.\nWe test this hypothesis using data collected from an artificial intelligence\ncourse with 105 participants (N=105) at a university in India. The AI course\nhad two programming assignments. Plagiarism through collusion was quantified\nusing the Measure of Software Similarity (MOSS) tool. Students were educated\nabout what constitutes academic dishonesty and were required to take an honor\npledge before the start of the second take-home programming assignment. The two\nprogramming assignments were novel and did not have solutions available on the\ninternet. We expected the mean percentage of similar lines of code to be\nsignificantly less in the second programming assignment. However, our results\nshow no significant difference in the mean percentage of similar lines of code\nacross the two programming assignments. We also study how the Big-five\npersonality traits affect the propensity for plagiarism in the two take-home\nassignments. Our results across both assignments show that the extraversion\ntrait of the Big Five personality exhibits a positive association, and the\nconscientiousness trait exhibits a negative association with plagiarism\ntendencies. Our result suggests that the policy of educating students about\nacademic integrity will have a limited impact as long as students perceive an\nopportunity for plagiarism to be present. We explain our results using the\nFraud triangle model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.16898,regular,post_llm,2024,6,"{'ai_likelihood': 6.258487701416016e-06, 'text': ""Introducing Individuality into Students' High School Timetables\n\n  In a perfect world, each high school student could pursue their interests\nthrough a personalized timetable that supports their strengths, weaknesses, and\ncuriosities. While recent research has shown that school systems are evolving\nto support those developments by strengthening modularity in their curricula,\nthere is often a hurdle that prevents the complete success of such a system:\nthe scheduling process is too complex. While there are many tools that assist\nwith scheduling timetables in an effective way, they usually arrange students\ninto groups and classes with similar interests instead of handling each student\nindividually. In this paper, we propose an extension of the popular XHSTT\nframework that adds two new constraints to model the individual student choices\nas well as the requirements for group formation that arise from them. Those two\nconstraints were identified through extensive interviews with school\nadministrators and other school timetabling experts from six European\ncountries. We propose a corresponding ILP formulation and show first\noptimization results for real-world instances from schools in Germany.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.07481,regular,post_llm,2024,6,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'The end of multiple choice tests: using AI to enhance assessment\n\n  Effective teaching relies on knowing what students know-or think they know.\nRevealing student thinking is challenging. Often used because of their ease of\ngrading, even the best multiple choice (MC) tests, those using research based\ndistractors (wrong answers) are intrinsically limited in the insights they\nprovide due to two factors. When distractors do not reflect student beliefs\nthey can be ignored, increasing the likelihood that the correct answer will be\nchosen by chance. Moreover, making the correct choice does not guarantee that\nthe student understands why it is correct. To address these limitations, we\nrecommend asking students to explain why they chose their answer, and why\n""wrong"" choices are wrong. Using a discipline-trained artificial\nintelligence-based bot it is possible to analyze their explanations,\nidentifying the concepts and scientific principles that maybe missing or\nmisapplied. The bot also makes suggestions for how instructors can use these\ndata to better guide student thinking. In a small ""proof of concept"" study, we\ntested this approach using questions from the Biology Concepts Instrument\n(BCI). The result was rapid, informative, and provided actionable feedback on\nstudent thinking. It appears that the use of AI addresses the weaknesses of\nconventional MC test. It seems likely that incorporating AI-analyzed formative\nassessments will lead to improved overall learning outcomes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11525,regular,post_llm,2024,6,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'ELMO2EDS: Transforming Educational Credentials into Self-Sovereign\n  Identity Paradigm\n\n  Digital credentials in education make it easier for students to apply for a\ncourse of study, a new job, or change a higher education institute. Academic\nnetworks, such as EMREX, support the exchange of digital credentials between\nstudents and education institutes. Students can fetch results from one\neducational institute and apply for a course of study at another educational\ninstitute. Digital signatures of the issuing institution can verify the\nauthenticity of digital credentials. Each institution must provide the\nintegration of EMREX using its identity management system. In this paper, we\ninvestigate how digital credentials can be integrated into the Self-Sovereign\nIdentity ecosystem to overcome the known issues of academic networks. We\nexamine known issues such as the authentication of students. Self-Sovereign\nIdentity is a paradigm that gives individuals control of their digital\nidentities. Based on our findings, we propose ELMO2EDS, a solution that 1)\nconverts digital credentials from EMREX to a suitable Self-Sovereign Identy\ndata format, 2) enables authenticating a student, and 3) enables issuing,\nstoring, and verification of achieved study.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.11104,regular,post_llm,2024,6,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Beyond the Hype: A Cautionary Tale of ChatGPT in the Programming\n  Classroom\n\n  Due to the proliferation of Large Language Models research and the use of\nvarious Artificial Intelligence (AI) tools, the field of information systems\n(IS) and computer science (CS) has evolved. The use of tools such as ChatGPT to\ncomplete various student programming exercises (e.g., in Python) and\nassignments has gained prominence amongst various academic institutions.\nHowever, recent literature has suggested that the use of ChatGPT in academia is\nproblematic and the impact on teaching and learning should be further\nscrutinized. More specifically, little is known about how ChatGPT can be\npractically used with code (programming) writing to complete programming\nexercises amongst IS and CS undergraduate university students. Furthermore, the\npaper provides insights for academics who teach programming to create more\nchallenging exercises and how to engage responsibly in the use of ChatGPT to\npromote classroom integrity. In this paper, we used Complex Adaptive Systems\n(CAS) theory as a theoretical guide to understand the various dynamics through\nclassroom code demonstrations. Using ChatGPT 3.5, we analyzed the various\npractical programming examples from past IS exercises and compared those with\nmemos created by tutors and lecturers in a university setting. This paper\nhighlights common ways of assessment, programming errors created by ChatGPT and\nthe potential consideration for IS academics to ensure the development of\ncritical programming skills among students.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.06049,regular,post_llm,2024,6,"{'ai_likelihood': 0.00011020236545138889, 'text': ""Enhancing Food Safety in Supply Chains: The Potential Role of Large\n  Language Models in Preventing Campylobacter Contamination\n\n  Foodborne diseases pose a significant global public health challenge,\nprimarily driven by bacterial infections. Among these, Campylobacter spp. is\nnotable, causing over 95 million cases annually. In response, the Hazard\nAnalysis and Critical Control Points (HACCP) system, a food safety management\nframework, has been developed and is considered the most effective approach for\nsystematically managing foodborne safety risks, including the prevention of\nbacterial contaminations, throughout the supply chain. Despite its efficacy,\nthe adoption of HACCP is often incomplete across different sectors of the food\nindustry. This limited implementation can be attributed to factors such as a\nlack of awareness, complex guidelines, confusing terminology, and insufficient\ntraining on the HACCP system's implementation. This study explores the\npotential of large language models (LLMs), specifically generative pre-trained\ntransformers (GPTs), to mitigate Campylobacter contamination across four\ntypical stages of the supply chain: primary production, food processing,\ndistribution and retail, and preparation and consumption. While the interaction\nbetween LLMs and food safety presents a promising potential, it remains largely\nunderexplored. To demonstrate the possible applications of LLMs in this domain,\nwe further configure an open-access customized GPT trained on the FAO's HACCP\ntoolbox and the 12 steps of HACCP implementation, and test it in the context of\ncommercial food preparation. The study also considers critical barriers to\nimplementing GPTs at each step of the supply chain and proposes initial\nmeasures to overcome these obstacles.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.01399,review,post_llm,2024,6,"{'ai_likelihood': 3.8080745273166235e-06, 'text': 'Null Compliance: NYC Local Law 144 and the Challenges of Algorithm\n  Accountability\n\n  In July 2023, New York City became the first jurisdiction globally to mandate\nbias audits for commercial algorithmic systems, specifically for automated\nemployment decisions systems (AEDTs) used in hiring and promotion. Local Law\n144 (LL 144) requires AEDTs to be independently audited annually for race and\ngender bias, and the audit report must be publicly posted. Additionally,\nemployers are obligated to post a transparency notice with the job listing. In\nthis study, 155 student investigators recorded 391 employers\' compliance with\nLL 144 and the user experience for prospective job applicants. Among these\nemployers, 18 posted audit reports and 13 posted transparency notices. These\nrates could potentially be explained by a significant limitation in the\naccountability mechanisms enacted by LL 144. Since the law grants employers\nsubstantial discretion over whether their system is in scope of the law, a null\nresult cannot be said to indicate non-compliance, a condition we call ``null\ncompliance."" Employer discretion may also explain our finding that nearly all\naudits reported an impact factor over 0.8, a rule of thumb often used in\nemployment discrimination cases. We also find that the benefit of LL 144 to\nordinary job seekers is limited due to shortcomings in accessibility and\nusability. Our findings offer important lessons for policy-makers as they\nconsider regulating algorithmic systems, particularly the degree of discretion\nto grant to regulated parties and the limitations of relying on transparency\nand end-user accountability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.16906,review,post_llm,2024,6,"{'ai_likelihood': 7.516807980007596e-06, 'text': 'European Network For Gender Balance in Informatics (EUGAIN): Activities\n  and Results\n\n  This chapter provides a summary of the activities and results of the European\nNetwork For Gender Balance in Informatics (EUGAIN, EU COST Action CA19122). The\nmain aim and objective of the network is to improve gender balance in\ninformatics at all levels, from undergraduate and graduate studies to\nparticipation and leadership both in academia and industry, through the\ncreation of a European network of colleagues working at the forefront of the\nefforts for gender balance in informatics in their countries and research\ncommunities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.16207,review,post_llm,2024,6,"{'ai_likelihood': 1.0, 'text': 'Thinking beyond Bias: Analyzing Multifaceted Impacts and Implications of\n  AI on Gendered Labour\n\n  Artificial Intelligence with its multifaceted technologies and integral role\nin global production significantly impacts gender dynamics particularly in\ngendered labor. This paper emphasizes the need to explore AIs broader impacts\non gendered labor beyond its current emphasis on the generation and\nperpetuation of epistemic biases. We draw attention to how the AI industry as\nan integral component of the larger economic structure is transforming the\nnature of work. It is expanding the prevalence of platform based work models\nand exacerbating job insecurity particularly for women. Of critical concern is\nthe increasing exclusion of women from meaningful engagement in the digital\nlabor force. This issue often overlooked demands urgent attention from the AI\nresearch community. Understanding AIs multifaceted role in gendered labor\nrequires a nuanced examination of economic transformation and its implications\nfor gender equity. By shedding light on these intersections this paper aims to\nstimulate in depth discussions and catalyze targeted actions aimed at\nmitigating the gender disparities accentuated by AI driven transformations.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0007886886596679688, 'GPT4': 0.76611328125, 'CLAUDE': 0.056060791015625, 'GOOGLE': 0.1727294921875, 'OPENAI_O_SERIES': 0.0013980865478515625, 'DEEPSEEK': 0.00036406517028808594, 'GROK': 1.1324882507324219e-06, 'NOVA': 3.6954879760742188e-06, 'OTHER': 9.381771087646484e-05, 'HUMAN': 0.002292633056640625}}"
2406.14713,review,post_llm,2024,6,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Risk thresholds for frontier AI\n\n  Frontier artificial intelligence (AI) systems could pose increasing risks to\npublic safety and security. But what level of risk is acceptable? One\nincreasingly popular approach is to define capability thresholds, which\ndescribe AI capabilities beyond which an AI system is deemed to pose too much\nrisk. A more direct approach is to define risk thresholds that simply state how\nmuch risk would be too much. For instance, they might state that the likelihood\nof cybercriminals using an AI system to cause X amount of economic damage must\nnot increase by more than Y percentage points. The main upside of risk\nthresholds is that they are more principled than capability thresholds, but the\nmain downside is that they are more difficult to evaluate reliably. For this\nreason, we currently recommend that companies (1) define risk thresholds to\nprovide a principled foundation for their decision-making, (2) use these risk\nthresholds to help set capability thresholds, and then (3) primarily rely on\ncapability thresholds to make their decisions. Regulators should also explore\nthe area because, ultimately, they are the most legitimate actors to define\nrisk thresholds. If AI risk estimates become more reliable, risk thresholds\nshould arguably play an increasingly direct role in decision-making.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.131,review,post_llm,2024,6,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Participatory Approaches in AI Development and Governance: A Principled\n  Approach\n\n  The widespread adoption of Artificial Intelligence (AI) technologies in the\npublic and private sectors has resulted in them significantly impacting the\nlives of people in new and unexpected ways. In this context, it becomes\nimportant to inquire how their design, development and deployment takes place.\nUpon this inquiry, it is seen that persons who will be impacted by the\ndeployment of these systems have little to no say in how they are developed.\nSeeing this as a lacuna, this research study advances the premise that a\nparticipatory approach is beneficial (both practically and normatively) to\nbuilding and using more responsible, safe, and human-centric AI systems.\nNormatively, it enhances the fairness of the process and empowers citizens in\nvoicing concerns to systems that may heavily impact their lives. Practically,\nit provides developers with new avenues of information which will be beneficial\nto them in improving the quality of the AI algorithm. The paper advances this\nargument first, by describing the life cycle of an AI system; second, by\nidentifying criteria which may be used to identify relevant stakeholders for a\nparticipatory exercise; and third, by mapping relevant stakeholders to\ndifferent stages of AI lifecycle. This paper forms the first part of a two-part\nseries on participatory governance in AI. The second paper will expand upon and\nconcretise the principles developed in this paper and apply the same to actual\nuse cases of AI systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.07571,regular,post_llm,2024,6,"{'ai_likelihood': 7.410844167073569e-05, 'text': 'Supporting Self-Reflection at Scale with Large Language Models: Insights\n  from Randomized Field Experiments in Classrooms\n\n  Self-reflection on learning experiences constitutes a fundamental cognitive\nprocess, essential for the consolidation of knowledge and the enhancement of\nlearning efficacy. However, traditional methods to facilitate reflection often\nface challenges in personalization, immediacy of feedback, engagement, and\nscalability. Integration of Large Language Models (LLMs) into the reflection\nprocess could mitigate these limitations. In this paper, we conducted two\nrandomized field experiments in undergraduate computer science courses to\ninvestigate the potential of LLMs to help students engage in post-lesson\nreflection. In the first experiment (N=145), students completed a take-home\nassignment with the support of an LLM assistant; half of these students were\nthen provided access to an LLM designed to facilitate self-reflection. The\nresults indicated that the students assigned to LLM-guided reflection reported\nincreased self-confidence and performed better on a subsequent exam two weeks\nlater than their peers in the control condition. In the second experiment\n(N=112), we evaluated the impact of LLM-guided self-reflection against other\nscalable reflection methods, such as questionnaire-based activities and review\nof key lecture slides, after assignment. Our findings suggest that the students\nin the questionnaire and LLM-based reflection groups performed equally well and\nbetter than those who were only exposed to lecture slides, according to their\nscores on a proctored exam two weeks later on the same subject matter. These\nresults underscore the utility of LLM-guided reflection and questionnaire-based\nactivities in improving learning outcomes. Our work highlights that focusing\nsolely on the accuracy of LLMs can overlook their potential to enhance\nmetacognitive skills through practices such as self-reflection. We discuss the\nimplications of our research for the Edtech community.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.0193,review,post_llm,2024,6,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Responsible Adoption of Generative AI in Higher Education: Developing a\n  ""Points to Consider"" Approach Based on Faculty Perspectives\n\n  This paper proposes an approach to the responsible adoption of generative AI\nin higher education, employing a \'\'points to consider\'\' approach that is\nsensitive to the goals, values, and structural features of higher education.\nHigher education\'s ethos of collaborative faculty governance, pedagogical and\nresearch goals, and embrace of academic freedom conflict, the paper argues,\nwith centralized top down approaches to governing AI that are common in the\nprivate sector. The paper is based on a semester long effort at the University\nof Pittsburgh which gathered and organized perspectives on generative AI in\nhigher education through a collaborative, iterative, interdisciplinary process\nthat included recurring group discussions, three standalone focus groups, and\nan informal survey. The paper presents insights drawn from this effort that\ngive rise to the \'\'points to consider\'\' approach the paper develops. These\ninsights include the benefits and risks of potential uses of generative AI In\nhigher education, as well as barriers to its adoption, and culminate in the six\nnormative points to consider when adopting and governing generative AI in\ninstitutions of higher education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.09799,regular,post_llm,2024,6,"{'ai_likelihood': 2.3510720994737412e-06, 'text': ""GeoSEE: Regional Socio-Economic Estimation With a Large Language Model\n\n  Moving beyond traditional surveys, combining heterogeneous data sources with\nAI-driven inference models brings new opportunities to measure socio-economic\nconditions, such as poverty and population, over expansive geographic areas.\nThe current research presents GeoSEE, a method that can estimate various\nsocio-economic indicators using a unified pipeline powered by a large language\nmodel (LLM). Presented with a diverse set of information modules, including\nthose pre-constructed from satellite imagery, GeoSEE selects which modules to\nuse in estimation, for each indicator and country. This selection is guided by\nthe LLM's prior socio-geographic knowledge, which functions similarly to the\ninsights of a domain expert. The system then computes target indicators via\nin-context learning after aggregating results from selected modules in the\nformat of natural language-based texts. Comprehensive evaluation across\ncountries at various stages of development reveals that our method outperforms\nother predictive models in both unsupervised and low-shot contexts. This\nreliable performance under data-scarce setting in under-developed or developing\ncountries, combined with its cost-effectiveness, underscores its potential to\ncontinuously support and monitor the progress of Sustainable Development Goals,\nsuch as poverty alleviation and equitable growth, on a global scale.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2406.08029,review,post_llm,2024,6,"{'ai_likelihood': 1.0, 'text': 'Framing metaverse identity: A multidimensional framework for governing digital selves\n\nThis paper proposes a multidimensional framework for Metaverse Identity, addressing its definition, guiding principles, and critical challenges. Metaverse Identity is conceptualized as a users digital self, encompassing personal attributes, data footprints, social roles, and economic elements. To elucidate its core characteristics and implications, this framework introduces two guiding principles: Equivalence and Alignment, and Fusion and Expansiveness. The first principle advocates for consistency between metaverse and real-world identities in behavioral norms and social standards, ensuring rights protection and establishing conduct guidelines. The second emphasizes the deep integration and transformative evolution of metaverse identities, enabling them to transcend real-world constraints, meet diverse needs, and foster inclusivity. Together, these principles serve as complementary pillars, balancing ethical integration with dynamic co-evolution. Building on this foundation, the study identifies five critical challenges: interoperability, legal boundaries, privacy and identity management, risks from deepfakes and synthetic identities, and identity fragmentation impacting psychological well-being. To address these challenges, strategic recommendations are offered to guide stakeholders. By constructing this framework, the study fills a key theoretical gap, advances systematic research, and provides a foundation for policies and governance strategies to address the complexities of metaverse identities in a rapidly evolving digital domain.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00012743473052978516, 'GPT4': 0.464599609375, 'CLAUDE': 0.2420654296875, 'GOOGLE': 0.0048828125, 'OPENAI_O_SERIES': 0.005458831787109375, 'DEEPSEEK': 0.28173828125, 'GROK': 3.5762786865234375e-07, 'NOVA': 3.993511199951172e-06, 'OTHER': 0.00011271238327026367, 'HUMAN': 0.000835418701171875}}"
2406.16781,regular,post_llm,2024,6,"{'ai_likelihood': 0.99755859375, 'text': 'A Carrying Capacity Calculator for Pedestrians Using OpenStreetMap Data:\n  Application to Urban Tourism and Public Spaces\n\n  Determining the carrying capacity of urban tourism destinations and public\nspaces is essential for sustainable management. This paper presents an online\ntool that calculates pedestrian carrying capacities for user-defined areas\nbased on OpenStreetMap (OSM) data. The tool considers physical, real, and\neffective carrying capacities by incorporating parameters such as area per\npedestrian, rotation factor, corrective factors, and management capacity. The\ncarrying capacity calculator aids in balancing environmental, economic, social,\nand experiential factors to prevent overcrowding and preserve the quality of\nlife for residents and visitors. This tool is particularly useful for tourism\ndestination management, urban planning, and event management, ensuring positive\nvisitor experiences and sustainable infrastructure development. We detail the\nimplementation of the calculator, its underlying algorithm, and its application\nto the Santa Maria Maior parish in Lisbon, highlighting its effectiveness in\nmanaging urban tourism and public spaces.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0071868896484375, 'GPT4': 0.030364990234375, 'CLAUDE': 0.03314208984375, 'GOOGLE': 0.68994140625, 'OPENAI_O_SERIES': 0.00910186767578125, 'DEEPSEEK': 0.0046844482421875, 'GROK': 0.0006265640258789062, 'NOVA': 0.00091552734375, 'OTHER': 0.223876953125, 'HUMAN': 0.00022780895233154297}}"
2407.14981,review,post_llm,2024,7,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Open Problems in Technical AI Governance\n\n  AI progress is creating a growing range of risks and opportunities, but it is\noften unclear how they should be navigated. In many cases, the barriers and\nuncertainties faced are at least partly technical. Technical AI governance,\nreferring to technical analysis and tools for supporting the effective\ngovernance of AI, seeks to address such challenges. It can help to (a) identify\nareas where intervention is needed, (b) identify and assess the efficacy of\npotential governance actions, and (c) enhance governance options by designing\nmechanisms for enforcement, incentivization, or compliance. In this paper, we\nexplain what technical AI governance is, why it is important, and present a\ntaxonomy and incomplete catalog of its open problems. This paper is intended as\na resource for technical researchers or research funders looking to contribute\nto AI governance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.1567,regular,post_llm,2024,7,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Coca4ai: checking energy behaviors on AI data centers\n\n  Monitoring energy behaviors in AI data centers is crucial, both to reduce\ntheir energy consumption and to raise awareness among their users which are key\nactors in the AI field. This paper shows a proof of concept of easy and\nlightweight monitoring of energy behaviors at the scale of a whole data center,\na user or a job submission. Our system uses software wattmeters and we validate\nour setup with per node accurate external wattmeters. Results show that there\nis an interesting potential from the efficiency point of view, providing\narguments to create user engagement thanks to energy monitoring.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.02027,review,post_llm,2024,7,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Privacy Risks of General-Purpose AI Systems: A Foundation for\n  Investigating Practitioner Perspectives\n\n  The rise of powerful AI models, more formally $\\textit{General-Purpose AI\nSystems}$ (GPAIS), has led to impressive leaps in performance across a wide\nrange of tasks. At the same time, researchers and practitioners alike have\nraised a number of privacy concerns, resulting in a wealth of literature\ncovering various privacy risks and vulnerabilities of AI models. Works\nsurveying such risks provide differing focuses, leading to disparate sets of\nprivacy risks with no clear unifying taxonomy. We conduct a systematic review\nof these survey papers to provide a concise and usable overview of privacy\nrisks in GPAIS, as well as proposed mitigation strategies. The developed\nprivacy framework strives to unify the identified privacy risks and mitigations\nat a technical level that is accessible to non-experts. This serves as the\nbasis for a practitioner-focused interview study to assess technical\nstakeholder perceptions of privacy risks and mitigations in GPAIS.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.01448,review,post_llm,2024,7,"{'ai_likelihood': 4.738569259643555e-05, 'text': '15 Years of Algorithmic Fairness -- Scoping Review of Interdisciplinary\n  Developments in the Field\n\n  This paper presents a scoping review of algorithmic fairness research over\nthe past fifteen years, utilising a dataset sourced from Web of Science, HEIN\nOnline, FAccT and AIES proceedings. All articles come from the computer science\nand legal field and focus on AI algorithms with potential discriminatory\neffects on population groups. Each article is annotated based on their\ndiscussed technology, demographic focus, application domain and geographical\ncontext. Our analysis reveals a growing trend towards specificity in addressed\ndomains, approaches, and demographics, though a substantial portion of\ncontributions remains generic. Specialised discussions often concentrate on\ngender- or race-based discrimination in classification tasks. Regarding the\ngeographical context of research, the focus is overwhelming on North America\nand Europe (Global North Countries), with limited representation from other\nregions. This raises concerns about overlooking other types of AI applications,\ntheir adverse effects on different types of population groups, and the cultural\nconsiderations necessary for addressing these problems. With the help of some\nhighlighted works, we advocate why a wider range of topics must be discussed\nand why domain-, technological, diverse geographical and demographic-specific\napproaches are needed. This paper also explores the interdisciplinary nature of\nalgorithmic fairness research in law and computer science to gain insight into\nhow researchers from these fields approach the topic independently or in\ncollaboration. By examining this, we can better understand the unique\ncontributions that both disciplines can bring.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17129,review,post_llm,2024,7,"{'ai_likelihood': 3.22527355617947e-05, 'text': ""Mapping the individual, social, and biospheric impacts of Foundation\n  Models\n\n  Responding to the rapid roll-out and large-scale commercialization of\nfoundation models, large language models, and generative AI, an emerging body\nof work is shedding light on the myriad impacts these technologies are having\nacross society. Such research is expansive, ranging from the production of\ndiscriminatory, fake and toxic outputs, and privacy and copyright violations,\nto the unjust extraction of labor and natural resources. The same has not been\nthe case in some of the most prominent AI governance initiatives in the global\nnorth like the UK's AI Safety Summit and the G7's Hiroshima process, which have\ninfluenced much of the international dialogue around AI governance. Despite the\nwealth of cautionary tales and evidence of algorithmic harm, there has been an\nongoing over-emphasis within the AI governance discourse on technical matters\nof safety and global catastrophic or existential risks. This narrowed focus has\ntended to draw attention away from very pressing social and ethical challenges\nposed by the current brute-force industrialization of AI applications. To\naddress such a visibility gap between real-world consequences and speculative\nrisks, this paper offers a critical framework to account for the social,\npolitical, and environmental dimensions of foundation models and generative AI.\nWe identify 14 categories of risks and harms and map them according to their\nindividual, social, and biospheric impacts. We argue that this novel typology\noffers an integrative perspective to address the most urgent negative impacts\nof foundation models and their downstream applications. We conclude with\nrecommendations on how this typology could be used to inform technical and\nnormative interventions to advance responsible AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20238,regular,post_llm,2024,7,"{'ai_likelihood': 1.3874636756049263e-05, 'text': 'Impact of COVID-19 post lockdown on eating habits and lifestyle changes\n  among university students in Bangladesh: a web based cross sectional study\n\n  Background:Since the confinement of the lockdown, universities transferred\ntheir teaching and learning activities in online as an all-out intention to\nprevent the transmission of the infection. This study aimed to determine the\nsignificant changes in food habits, physical activity, sleeping hours, shopping\nhabits, Internet use time and mental status of the students and investigate the\nassociations between variables. Methods:The study participants were 307\nUndergraduate students, between 18 and 25 years of age completed a structured\nquestionnaire from January 3, 2022 to February 13, 2022. The questionnaire\nincluded demographic information of the students, questionnaire of dietary\npattern, physical activity, sleep quality index, Shopping practice and Internet\nuse time.Chi-square tests were used to associate the baseline information with\nlifestyle changes in post lockdown. Results:The study reveals that 21.5% of\nrespondents gained weight, 23.8% lost their weight and 41.7% controlled their\nweight. Eating of homemade food decreased after lockdown 76.5% and eating of\nrestaurant food increased after lockdown 23.5%. A number of major meals 3-4\nmeals per day decreased after lockdown 61.9%. Physical exercise significantly\nincreased after lockdown (p=0.001). Sleeping hours per day significantly\ndecreased after lockdown (p=0.001), sleep quality was almost the same and\nenergy level increased more in post lockdown. Respondents felt mentally tired\nafter lockdown 60.9%. Respondents spending time on the Internet in chat rooms\nwas 88.3%. Conclusions: This study represents the significant impact on food\nhabits, mental health, and daily routine of students after lockdown, suggesting\nthat we should maintain a balanced diet, physical exercise to sleep quality and\nmental health.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.0746,regular,post_llm,2024,7,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Use of social networks to motivate computer-engineering students to\n  participate in self-assessment activities\n\n  Motivation is essential in the learning process of university students, and\nteachers should have a wide range of strategies to address this issue. The\nemergence of social technologies has had a considerable influence in e-learning\nsystems, and a number of experts state that their use is a good method to\nmotivate students and to increase their participation in activities. This study\nattempts to determine whether social networks and social applications should be\nviewed as many other tools or whether they can actually provide extra\nmotivation for students to participate. The study compared the percentage of\nstudent participation in tasks of self-assessment. The experiments covered\nthree traditional strategies of student motivation and another one in which\nsocial networks were used to introduce, explain and deliver the self-assessment\ntasks. The case with a higher participation was the one in which students\nobtained a reward from the completion of the activity. Despite this result, the\nstatistical analysis indicated that the use of social networks obtained similar\nresults as a strategy of continuous and regular motivational speeches.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.03187,regular,post_llm,2024,7,"{'ai_likelihood': 5.116065343221029e-05, 'text': 'Holistic view of the road transportation system based on real-time data\n  sharing mechanism\n\n  Traditional manual driving and single-vehicle-based intelligent driving have\nlimitations in real-time and accurate acquisition of the current driving status\nand intentions of surrounding vehicles, leading to vehicles typically\nmaintaining appropriate safe distances from each other. Yet, accidents still\nfrequently occur, especially in merging areas; meanwhile, it is difficult to\ncomprehensively obtain the conditions of road infrastructure. These limitations\nnot only restrict the further improvement of road capacity but also result in\nirreparable losses of life and property. To overcome this bottleneck, this\npaper constructs a space-time global view of the road traffic system based on a\nreal-time sharing mechanism, enabling both road users and managers to timely\naccess the driving intentions of nearby vehicles and the real-time status of\nroad infrastructure.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.15998,review,post_llm,2024,7,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Public Perception of AI: Sentiment and Opportunity\n\n  As Artificial Intelligence (AI) increasingly influences various aspects of\nsociety, there is growing public interest in its potential benefits and risks.\nIn this paper we present results of public perception of AI from a survey\nconducted with 10,000 respondents spanning ten countries in four continents\naround the world. The results show that currently an equal percentage of\nrespondents who believe AI will change the world as we know it, also believe AI\nneeds to be heavily regulated. However, our findings also indicate that despite\nthe general sentiment among the global public that AI will replace workers, if\na company were to use AI to innovate to improve lives, the public would be more\nlikely to think highly of the company, purchase from them and even be\ninterested in a job in that company. Our results further reveal that the global\npublic largely views AI as a tool for problem solving. These nuanced results\nunderscore the importance of AI directed towards challenges that the public\nwould like science and technology-based innovations to address. We draw on a\nmulti-year 3M study of public perception of science to provide further context\non what the public perceives as important problems to be solved.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.14634,review,post_llm,2024,7,"{'ai_likelihood': 1.0, 'text': ""Informational Health --Toward the Reduction of Risks in the Information\n  Space\n\n  The modern information society, markedly influenced by the advent of the\ninternet and subsequent developments such as WEB 2.0, has seen an explosive\nincrease in information availability, fundamentally altering human interaction\nwith information spaces. This transformation has facilitated not only\nunprecedented access to information but has also raised significant challenges,\nparticularly highlighted by the spread of ``fake news'' during critical events\nlike the 2016 U.S. presidential election and the COVID-19 pandemic. The latter\nevent underscored the dangers of an ``infodemic,'' where the large amount of\ninformation made distinguishing between factual and non-factual content\ndifficult, thereby complicating public health responses and posing risks to\ndemocratic processes. In response to these challenges, this paper introduces\nthe concept of ``informational health,'' drawing an analogy between dietary\nhabits and information consumption. It argues that just as balanced diets are\ncrucial for physical health, well-considered nformation behavior is essential\nfor maintaining a healthy information environment. This paper proposes three\nstrategies for fostering informational health: literacy education,\nvisualization of meta-information, and informational health assessments. These\nstrategies aim to empower users and platforms to navigate and enhance the\ninformation ecosystem effectively. By focusing on long-term informational\nwell-being, we highlight the necessity of addressing the social risks inherent\nin the current attention economy, advocating for a paradigm shift towards a\nmore sustainable information consumption model.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.74913215637207e-05, 'GPT4': 0.87890625, 'CLAUDE': 1.704692840576172e-05, 'GOOGLE': 0.0499267578125, 'OPENAI_O_SERIES': 0.0655517578125, 'DEEPSEEK': 0.00037407875061035156, 'GROK': 0.00453948974609375, 'NOVA': 2.2470951080322266e-05, 'OTHER': 0.00017750263214111328, 'HUMAN': 0.0004208087921142578}}"
2408.00156,regular,post_llm,2024,7,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Measuring Falseness in News Articles based on Concealment and\n  Overstatement\n\n  This research investigates the extent of misinformation in certain\njournalistic articles by introducing a novel measurement tool to assess the\ndegrees of falsity. It aims to measure misinformation using two metrics\n(concealment and overstatement) to explore how information is interpreted as\nfalse. This should help examine how articles containing partly true and partly\nfalse information can potentially harm readers, as they are more challenging to\nidentify than completely fabricated information. In this study, the full story\nprovided by the fact-checking website serves as a standardized source of\ninformation for comparing differences between fake and real news. The result\nsuggests that false news has greater concealment and overstatement, due to\nlonger and more complex new stories being shortened and ambiguously phrased.\nWhile there are no major distinctions among categories of politics science and\ncivics, it demonstrates that misinformation lacks crucial details while\nsimultaneously containing more redundant words. Hence, news articles containing\npartial falsity, categorized as misinformation, can deceive inattentive readers\nwho lack background knowledge. Hopefully, this approach instigates future\nfact-checkers, journalists, and the readers to secure high quality articles for\na resilient information environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.10945,regular,post_llm,2024,7,"{'ai_likelihood': 6.225374009874132e-06, 'text': 'Blockchain Governance: An Empirical Analysis of User Engagement on DAOs\n\n  In this note, we examine voting on four major blockchain DAOs: Aave,\nCompound, Lido and Uniswap. Using data directly collected from the Ethereum\nblockchain, we examine voter activity.\n  We find that in most votes, the ""minimal quorum,"" i.e., the smallest number\nof active voters who could swing the vote is quite small.\n  To understand who is actually driving these DAOs, we use data from the\nEthereum Name Service (ENS), Sybil.org, and Compound, to divide voters into\ndifferent categories.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.15625,regular,post_llm,2024,7,"{'ai_likelihood': 1.2583202785915798e-06, 'text': ""Dressed to Gamble: How Poker Drives the Dynamics of Wearables and Visits on Decentraland's Social Virtual World\n\nDecentraland is a blockchain-based social virtual world where users can publish and sell wearables to customize avatars. In it, the third-party Decentral Games (DG) allows players of its flagship game ICE Poker to earn cryptocurrency only if they possess certain wearables. Herein, we present a comprehensive study on how DG and its game influence the dynamics of wearables and in-world visits in Decentraland. To this end, we analyzed 5.9 million wearable transfers made on the Polygon blockchain (and related sales) over a two-year period, and 677 million log events of in-world user positions in an overlapping 10-month period. We found that these activities are disproportionately related to DG, with its ICE Poker casinos (less than 0.1% of the world map) representing a remarkable average share of daily unique visitors (33%) and time spent in the virtual world (20%). Despite several alternative initiatives within Decentraland, ICE Poker appears to drive user activity on the platform. Our work thus contributes to the understanding of how play-to-earn games influence user behavior in social virtual worlds, and it is among the first to study the emerging phenomenon of virtual blockchain-based gambling."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17347,regular,post_llm,2024,7,"{'ai_likelihood': 1.0596381293402778e-06, 'text': ""AI Emergency Preparedness: Examining the federal government's ability to\n  detect and respond to AI-related national security threats\n\n  We examine how the federal government can enhance its AI emergency\npreparedness: the ability to detect and prepare for time-sensitive national\nsecurity threats relating to AI. Emergency preparedness can improve the\ngovernment's ability to monitor and predict AI progress, identify national\nsecurity threats, and prepare effective response plans for plausible threats\nand worst-case scenarios. Our approach draws from fields in which experts\nprepare for threats despite uncertainty about their exact nature or timing\n(e.g., counterterrorism, cybersecurity, pandemic preparedness). We focus on\nthree plausible risk scenarios: (1) loss of control (threats from a powerful AI\nsystem that becomes capable of escaping human control), (2) cybersecurity\nthreats from malicious actors (threats from a foreign actor that steals the\nmodel weights of a powerful AI system), and (3) biological weapons\nproliferation (threats from users identifying a way to circumvent the\nsafeguards of a publicly-released model in order to develop biological\nweapons.) We evaluate the federal government's ability to detect, prevent, and\nrespond to these threats. Then, we highlight potential gaps and offer\nrecommendations to improve emergency preparedness. We conclude by describing\nhow future work on AI emergency preparedness can be applied to improve\npolicymakers' understanding of risk scenarios, identify gaps in detection\ncapabilities, and form preparedness plans to improve the effectiveness of\nfederal responses to AI-related national security threats.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20234,review,post_llm,2024,7,"{'ai_likelihood': 2.0199351840549047e-06, 'text': ""Exploring Factors Affecting Student Learning Satisfaction during\n  COVID-19 in South Korea\n\n  Understanding students' preferences and learning satisfaction during COVID-19\nhas focused on learning attributes such as self-efficacy, performance, and\nengagement. Although existing efforts have constructed statistical models\ncapable of accurately identifying significant factors impacting learning\nsatisfaction, they do not necessarily explain the complex relationships among\nthese factors in depth. This study aimed to understand several facets related\nto student learning preferences and satisfaction during the pandemic such as\nindividual learner characteristics, instructional design elements and social\nand environmental factors. Responses from 302 students from Sungkyunkwan\nUniversity, South Korea were collected between 2021 and 2022. Information\ngathered included their gender, study major, satisfaction and motivation levels\nwhen learning, perceived performance, emotional state and learning environment.\nWilcoxon Rank sum test and Explainable Boosting Machine (EBM) were performed to\ndetermine significant differences in specific cohorts. The two core findings of\nthe study are as follows:1) Using Wilcoxon Rank Sum test, we can attest with\n95% confidence that students who took offline classes had significantly higher\nlearning satisfaction, among other attributes, than those who took online\nclasses, as with STEM versus HASS students; 2) An explainable boosting machine\n(EBM) model fitted to 95.08% accuracy determined the top five factors affecting\nstudents' learning satisfaction as their perceived performance, their\nperception on participating in class activities, their study majors, their\nability to conduct discussions in class and the study space availability at\nhome. Positive perceived performance and ability to discuss with classmates had\na positive impact on learning satisfaction, while negative perception on class\nactivities participation had a negative impact on learning satisfaction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20691,regular,post_llm,2024,7,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'A Three Steps Methodological Approach to Legal Governance Validation\n\n  We present in this position paper a methodology to validate legal governance\nregulatory models from an empirical approach, as illustrated by means of three\ndiagrams: (i) a scheme drawing the rule and meta-rule of law; (ii) a metamodel\nfor legal governance; (iii) a causal validation scheme for legal compliance.\nThese visualisations refer to different sets of notions corresponding\nrespectively to (i) a general scheme with three dimensions and four clusters,\n(ii) a meta-model encompassing legal compliance through design (LCtD) and\necological validity, and (iii) the con-struction of an empirical validation\nmodel of causal chains. The final aim of the methodology is to build and test\nsmart legal ecosystems (SLE) for Industry 4.0 and 5.0.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.18933,review,post_llm,2024,7,"{'ai_likelihood': 0.0004572338528103299, 'text': 'Awareness and Adoption of AI Technologies in the Libraries of Karnataka\n\n  This study aims to determine the awareness and adoption of Artificial\nIntelligence (AI) technologies in the respondent libraries of Karnataka based\non demographic variables such as gender, age, academic status, and professional\nexperience. This study employed a survey research method to evaluate the\nawareness and adoption of AI technologies among the respondent library\nprofessionals in Karnataka. The study employed a stratified random sampling\nmethod to select a sample of 120 respondents from a diverse population,\nencompassing library professionals across multiple institution types including\nengineering colleges, medical colleges, and degree colleges. The Chi-square\ntest was used to analyze the data. The study revealed that there is a\nstatistically significant difference in the awareness and adoption of AI\ntechnologies based on the factor of gender. Whereas there no significant\nrelationship exists between the degree of awareness and adoption of AI\ntechnologies based on factors such as age, academic ranking, and professional\nexperience. AI-powered plagiarism detection, grammar checking, and ChatGPT are\nthe most popularly employed AI technologies among the respondents. The\nrespondents are of the perception that AI will support Librarians and not\nreplace them.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.0382,regular,post_llm,2024,7,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Use of Mobile Devices in the Classroom to Increase Motivation and\n  Participation of Engineering University Students\n\n  The aim of this study was to see whether student participation increased when\nmobile devices were used in the classroom. We measured the amount of student\nparticipative actions when the Socrative tool was used and when it was not\nused. Our experiment involved a total of 192 students, corresponding to 4\ndifferent subjects of Computer Engineering at the Universitat de les Illes\nBalears, during 2012/2013 and 2013/2014 courses. An independent paired t-test\nwas performed on the measurements. The analysis results show that student\nparticipation increases with the use of mobile devices for theory classes and\nstudents are willing to participate in class activities and share their own\nresults.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.0447,regular,post_llm,2024,7,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""How to Drill Into Silos: Creating a Free-to-Use Dataset of Data Subject\n  Access Packages\n\n  The European Union's General Data Protection Regulation (GDPR) strengthened\nseveral rights for individuals (data subjects). One of these is the data\nsubjects' right to access their personal data being collected by services (data\ncontrollers), complemented with a new right to data portability. Based on\nthese, data controllers are obliged to provide respective data and allow data\nsubjects to use them at their own discretion.\n  However, the subjects' possibilities for actually using and harnessing said\ndata are severely limited so far. Among other reasons, this can be attributed\nto a lack of research dedicated to the actual use of controller-provided\nsubject access request packages (SARPs). To open up and facilitate such\nresearch, we outline a general, high-level method for generating,\npre-processing, publishing, and finally using SARPs of different providers.\nFurthermore, we establish a realistic dataset comprising two users' SARPs from\nfive services. This dataset is publicly provided and shall, in the future,\nserve as a starting and reference point for researching and comparing novel\napproaches for the practically viable use of SARPs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.08102,regular,post_llm,2024,7,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Dynamics of Gender Bias within Computer Science\n\n  A new dataset (N = 7,456) analyzes women\'s research authorship in the\nAssociation for Computing Machinery\'s founding 13 Special Interest Groups or\nSIGs, a proxy for computer science. ACM SIGs expanded during 1970-2000; each\nexperienced increasing women\'s authorship. But diversity abounds. Several SIGs\nhad fewer than 10% women authors while SIGUCCS (university computing centers)\nexceeded 40%. Three SIGs experienced accelerating growth in women\'s authorship;\nmost, including a composite ACM, had decelerating growth. This research may\nencourage reform efforts, often focusing on general education or workforce\nfactors (across the entity of ""computer science""), to examine under-studied\ndynamics within computer science that shaped changes in women\'s participation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.19088,review,post_llm,2024,7,"{'ai_likelihood': 1.0, 'text': 'Shaping Integrity: Why Generative Artificial Intelligence Does Not Have\n  to Undermine Education\n\n  This paper examines the role of generative artificial intelligence (GAI) in\npromoting academic integrity within educational settings. It explores how AI\ncan be ethically integrated into classrooms to enhance learning experiences,\nfoster intrinsic motivation, and support voluntary behavior change among\nstudents. By analyzing established ethical frameworks and educational theories\nsuch as deontological ethics, consequentialism, constructivist learning, and\nSelf-Determination Theory (SDT), the paper argues that GAI, when used\nresponsibly, can enhance digital literacy, encourage genuine knowledge\nconstruction, and uphold ethical standards in education. This research\nhighlights the potential of GAI to create enriching, personalized learning\nenvironments that prepare students to navigate the complexities of the modern\nworld ethically and effectively.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0003311634063720703, 'GPT4': 0.9365234375, 'CLAUDE': 0.0180511474609375, 'GOOGLE': 0.0026836395263671875, 'OPENAI_O_SERIES': 0.0362548828125, 'DEEPSEEK': 0.0035533905029296875, 'GROK': 0.0010633468627929688, 'NOVA': 0.0008897781372070312, 'OTHER': 0.0005130767822265625, 'HUMAN': 1.2636184692382812e-05}}"
2407.00922,regular,post_llm,2024,7,"{'ai_likelihood': 0.13970269097222224, 'text': ""Staying vigilant in the Age of AI: From content generation to content\n  authentication\n\n  This paper presents the Yangtze Sea project, an initiative in the battle\nagainst Generative AI (GAI)-generated fake con-tent. Addressing a pressing\nissue in the digital age, we investigate public reactions to AI-created\nfabrications through a structured experiment on a simulated academic conference\nplatform. Our findings indicate a profound public challenge in discerning such\ncontent, highlighted by GAI's capacity for realistic fabrications. To counter\nthis, we introduce an innovative approach employing large language models like\nChatGPT for truthfulness assess-ment. We detail a specific workflow for\nscrutinizing the authenticity of everyday digital content, aimed at boosting\npublic awareness and capability in identifying fake mate-rials. We apply this\nworkflow to an agent bot on Telegram to help users identify the authenticity of\ntext content through conversations. Our project encapsulates a two-pronged\nstrategy: generating fake content to understand its dynamics and developing\nassessment techniques to mitigate its impact. As part of that effort we propose\nthe creation of speculative fact-checking wearables in the shape of reading\nglasses and a clip-on. As a computational media art initiative, this project\nunder-scores the delicate interplay between technological progress, ethical\nconsid-erations, and societal consciousness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.05529,review,post_llm,2024,7,"{'ai_likelihood': 3.675619761149089e-06, 'text': 'Behind the Deepfake: 8% Create; 90% Concerned. Surveying public exposure\n  to and perceptions of deepfakes in the UK\n\n  This article examines public exposure to and perceptions of deepfakes based\non insights from a nationally representative survey of 1403 UK adults. The\nsurvey is one of the first of its kind since recent improvements in deepfake\ntechnology and widespread adoption of political deepfakes. The findings reveal\nthree key insights. First, on average, 15% of people report exposure to harmful\ndeepfakes, including deepfake pornography, deepfake frauds/scams and other\npotentially harmful deepfakes such as those that spread health/religious\nmisinformation/propaganda. In terms of common targets, exposure to deepfakes\nfeaturing celebrities was 50.2%, whereas those featuring politicians was 34.1%.\nAnd 5.7% of respondents recall exposure to a selection of high profile\npolitical deepfakes in the UK. Second, while exposure to harmful deepfakes was\nrelatively low, awareness of and fears about deepfakes were high (and women\nwere significantly more likely to report experiencing such fears than men). As\nwith fears, general concerns about the spread of deepfakes were also high;\n90.4% of the respondents were either very concerned or somewhat concerned about\nthis issue. Most respondents (at least 91.8%) were concerned that deepfakes\ncould add to online child sexual abuse material, increase distrust in\ninformation and manipulate public opinion. Third, while awareness about\ndeepfakes was high, usage of deepfake tools was relatively low (8%). Most\nrespondents were not confident about their detection abilities and were\ntrustful of audiovisual content online. Our work highlights how the problem of\ndeepfakes has become embedded in public consciousness in just a few years; it\nalso highlights the need for media literacy programmes and other policy\ninterventions to address the spread of harmful deepfakes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.19406,regular,post_llm,2024,7,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Moral and emotional influences on attitude stability towards COVID-19\n  vaccines on social media\n\n  Effective public health messaging benefits from understanding antecedents to\nunstable attitudes that are more likely to be influenced. This work\ninvestigates the relationship between moral and emotional bases for attitudes\ntowards COVID-19 vaccines and variance in stance. Evaluating nearly 1 million X\nusers over a two month period, we find that emotional language in tweets about\nCOVID-19 vaccines is largely associated with more variation in stance of the\nposting user, except anger and surprise. The strength of COVID-19 vaccine\nattitudes associated with moral values varies across foundations. Most notably,\nliberty is consistently used by users with no or less variation in stance,\nwhile fairness and sanctity are used by users with more variation. Our work has\nimplications for designing constructive pro-vaccine messaging and identifying\nreceptive audiences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.06506,regular,post_llm,2024,7,"{'ai_likelihood': 0.4844835069444445, 'text': ""Information Seeking and Communication among International Students on\n  Reddit\n\n  This study examines the impact of the COVID-19 pandemic on\ninformation-seeking behaviors among international students, with a focus on the\nr/f1visa subreddit. Our study indicates a considerable rise in the number of\nusers posting more than one question during the pandemic. Those asking\nrecurring questions demonstrate more active involvement in communication,\nsuggesting a continuous pursuit of knowledge. Furthermore, the thematic focus\nhas shifted from questions about jobs before COVID-19 to concerns about\nfinances, school preparations, and taxes during COVID-19. These findings carry\nimplications for support policymaking, highlighting the importance of\ndelivering timely and relevant information to meet the evolving needs of\ninternational students. To enhance international students' understanding and\nnavigation of this dynamic environment, future research in this field is\nnecessary.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.01968,review,post_llm,2024,7,"{'ai_likelihood': 0.08355034722222222, 'text': ""Unsettled Law: Time to Generate New Approaches?\n\n  We identify several important and unsettled legal questions with profound\nethical and societal implications arising from generative artificial\nintelligence (GenAI), focusing on its distinguishable characteristics from\ntraditional software and earlier AI models. Our key contribution is formally\nidentifying the issues that are unique to GenAI so scholars, practitioners, and\nothers can conduct more useful investigations and discussions. While\nestablished legal frameworks, many originating from the pre-digital era, are\ncurrently employed in GenAI litigation, we question their adequacy. We argue\nthat GenAI's unique attributes, including its general-purpose nature, reliance\non massive datasets, and potential for both pervasive societal benefits and\nharms, necessitate a re-evaluation of existing legal paradigms. We explore\npotential areas for legal and regulatory adaptation, highlighting key issues\naround copyright, privacy, torts, contract law, criminal law, property law, and\nthe First Amendment. Through an exploration of these multifaceted legal\nchallenges, we aim to stimulate discourse and policy considerations surrounding\nGenAI, emphasizing a proactive approach to legal and ethical frameworks. While\nwe refrain from advocating specific legal changes, we underscore the need for\npolicymakers to carefully consider the issues raised. We conclude by\nsummarizing key questions across these areas of law in a helpful table for easy\nreference.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17633,regular,post_llm,2024,7,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'PICA: A Data-driven Synthesis of Peer Instruction and Continuous\n  Assessment\n\n  Peer Instruction (PI) and Continuous Assessment(CA) are two distinct\neducational techniques with extensive research demonstrating their\neffectiveness. The work herein combines PI and CA in a deliberate and novel\nmanner to pair students together for a PI session in which they collaborate on\na CA task. The data used to inform the pairing method is restricted to the most\nprevious CA task students completed independently. The motivation for this\ndata-driven collaborative learning is to improve student learning,\ncommunication, and engagement. Quantitative results from an investigation of\nthe method show improved assessment scores on the PI CA tasks, although\nevidence of a positive effect on subsequent individual CA tasks was not\nstatistically significant as anticipated. However, student perceptions were\npositive, engagement was high, and students interacted with a broader set of\npeers than is typical. These qualitative observations, together with extant\nresearch on the general benefits of improving student engagement and\ncommunication (e.g. improved sense of belonging, increased social capital,\netc.), render the method worthy for further research into building and\nevaluating small student learning communities using student assessment data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.03474,regular,post_llm,2024,7,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'How high-status women promote repeated collaboration among women in\n  male-dominated contexts\n\n  Male-dominated contexts pose a dilemma: they increase the benefits of\nrepeated collaboration among women, yet at the same time, make such\ncollaborations less likely. This paper seeks to understand the conditions that\nfoster repeated collaboration among women versus men in male-dominated settings\nby examining the critical role of status hierarchies. Using collaboration data\non 8,232,769 computer science research teams, we found that when a woman holds\nthe top-ranking position in a steep status hierarchy, other women on that team\nare more likely than men to collaborate again, as compared to when the\nhierarchy is flat, and compared to when men occupy the top-ranking position. In\nsteep hierarchies, top-ranking women but not top-ranking men foster conditions\nin which junior women are more likely to collaborate again than junior men of\nsimilar status levels. Our research suggests that whereas status hierarchies\nare especially detrimental to repeated collaboration among underrepresented\nindividuals, top-ranking women in steep status hierarchies mitigate these\nnegative impacts between women in male-dominated settings.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.0775,review,post_llm,2024,7,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Digital Twin sensors in cultural heritage applications\n\n  The paper concerns the extension of the Heritage Digital Twin Ontology\nintroduced in previous work to describe the reactivity of digital twins used\nfor cultural heritage documentation by including the semantic description of\nsensors and activators and all the process of interacting with the real world.\nAfter analysing previous work on the use of digital twins in cultural heritage,\na summary description of the Heritage Digital Twin Ontology is provided, and\nthe existing applications of digital twins to cultural heritage are overviewed,\nwith references to reviews summarizing the large production of scientific\ncontributions on the topic. Then a novel ontology, named Reactive Digital Twin\nOntology is described, in which sensors, activators and the decision processes\nare also semantically described, turning the previous synchronic approach to\ncultural heritage documentation into a diachronic one. Some case studies\nexemplify this theory.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.0982,regular,post_llm,2024,7,"{'ai_likelihood': 8.046627044677734e-06, 'text': ""Mining individual daily commuting patterns of dockless bike-sharing\n  users: a two-layer framework integrating spatiotemporal flow clustering and\n  rule-based decision trees\n\n  The rise of dockless bike-sharing systems has led to increased interest in\nusing bike-sharing data for sustainable transportation and travel behavior\nresearch. However, these studies have rarely focused on the individual daily\nmobility patterns, hindering their alignment with the increasingly refined\nneeds of active transportation planning. To bridge this gap, this paper\npresents a two-layer framework, integrating improved flow clustering methods\nand multiple rule-based decision trees, to mine individual cyclists' daily\nhome-work commuting patterns from dockless bike-sharing trip data with user\nIDs. The effectiveness and applicability of the framework is demonstrated by\nover 200 million bike-sharing trip records in Shenzhen. Based on the mining\nresults, we obtain two categories of bike-sharing commuters (74.38% of\nOnly-biking commuters and 25.62% of Biking-with-transit commuters) and some\ninteresting findings about their daily commuting patterns. For instance, lots\nof bike-sharing commuters live near urban villages and old communities with\nlower costs of living, especially in the central city. Only-biking commuters\nhave a higher proportion of overtime than Biking-with-transit commuters, and\nthe Longhua Industrial Park, a manufacturing-oriented area, has the longest\naverage working hours (over 10 hours per day). Moreover, massive users utilize\nbike-sharing for commuting to work more frequently than for returning home,\nwhich is intricately related to the over-demand for bikes around workplaces\nduring commuting peak. In sum, this framework offers a cost-effective way to\nunderstand the nuanced non-motorized mobility patterns and low-carbon trip\nchains of residents. It also offers novel insights for improving the\nbike-sharing services and planning of active transportation modes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.13927,regular,post_llm,2024,7,"{'ai_likelihood': 2.6490953233506946e-07, 'text': '""We\'re not all construction workers"": Algorithmic Compression of\n  Latinidad on TikTok\n\n  The Latinx diaspora in the United States is a rapidly growing and complex\ndemographic who face intersectional harms and marginalizations in\nsociotechnical systems and are currently underserved in CSCW research. While\nthe field understands that algorithms and digital content are experienced\ndifferently by marginalized populations, more investigation is needed about how\nLatinx people experience social media and, in particular, visual media. In this\npaper, we focus on how Latinx people experience the algorithmic system of the\nvideo-sharing platform TikTok. Through a bilingual interview and visual\nelicitation study of 19 Latinx TikTok users and 59 survey participants, we\nexplore how Latinx individuals experience TikTok and its Latinx content. We\nfind Latinx TikTok users actively use platform affordances to create positive\nand affirming identity content feeds, but these feeds are interrupted by\nnegative content (i.e. violence, stereotypes, linguistic assumptions) due to\nplatform affordances that have unique consequences for Latinx diaspora users.\nWe discuss these implications on Latinx identity and representation, introduce\nthe concept of \\textit{algorithmic identity compression}, where sociotechncial\nsystems simplify, flatten, and conflate intersection identities, resulting in\ncompression via the loss of critical cultural data deemed unnecessary by these\nsystems and designers of them. This study explores how Latinx individuals are\nparticularly vulnerable to this in sociotechnical systems, such as, but not\nlimited to, TikTok.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.13365,review,post_llm,2024,7,"{'ai_likelihood': 2.317958407931858e-06, 'text': ""Generative AI and the problem of existential risk\n\n  Ever since the launch of ChatGPT, Generative AI has been a focal point for\nconcerns about AI's perceived existential risk. Once a niche topic in AI\nresearch and philosophy, AI safety and existential risk has now entered\nmainstream debate among policy makers and leading foundation models developers,\nmuch to the chagrin of those who see it as a distraction from addressing more\npressing nearer-term harms. This chapter aims to demystify the debate by\nhighlighting the key worries that underpin existential risk fears in relation\nto generative AI, and spotlighting the key actions that governments and\nindustry are taking thus far to helping address them.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17021,review,post_llm,2024,7,"{'ai_likelihood': 0.95361328125, 'text': 'The EU-US Data Privacy Framework: Is the Dragon Eating its Own Tail?\n\n  The European Commission adequacy decision on the EU US Data Privacy\nFramework, adopted on July 10th, 2023, marks a crucial moment in transatlantic\ndata protection. Following an Executive Order issued by President Biden in\nOctober 2022, this decision confirms that the United States meets European\nUnion standards for personal data protection. The decision extends to all\ntransfers from the European Economic Area to US entities participating in the\nframework, promoting privacy rights while facilitating data exchange. Key\naspects include oversight of US public authorities access to transferred data,\nthe introduction of a dual tier redress mechanism, and granting new rights to\nEU individuals, encompassing data access and rectification. However, the\nframework presents both promise and challenges in health data transfers. While\nstreamlining exchange and aligning legal standards, it grapples with the\ncomplexities of divergent privacy laws. The recent bill for the introduction of\na US federal privacy law emphasizes the urgent need for ongoing reform.\nLingering concerns persist regarding the framework resilience, especially amid\npotential legal battles before the Court of Justice of the EU. The history of\ntransatlantic data transfers between the EU and the US is riddled with\nvulnerabilities, reminiscent of the Ouroboros, an ancient symbol of a serpent\nor dragon eating its own tail, hinting at the looming possibility of the\nframework facing invalidation once again. This article delves into the main\nrequirements of the framework and offers insights on how healthcare\norganizations can navigate it effectively.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0117645263671875, 'GPT4': 0.53857421875, 'CLAUDE': 0.0011692047119140625, 'GOOGLE': 0.41552734375, 'OPENAI_O_SERIES': 0.02105712890625, 'DEEPSEEK': 0.0004596710205078125, 'GROK': 1.1205673217773438e-05, 'NOVA': 9.942054748535156e-05, 'OTHER': 0.0013723373413085938, 'HUMAN': 0.00984954833984375}}"
2407.18787,regular,post_llm,2024,7,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'Automatic Detection of Moral Values in Music Lyrics\n\nMoral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts.We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines.Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.06604,regular,post_llm,2024,7,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""A Matter of Mindset? Features and Processes of Newsroom-based Corporate\n  Communication in Times of Artificial Intelligence\n\n  Many companies adopt the corporate newsroom model to streamline their\ncorporate communication. This article addresses why and how corporate newsrooms\ntransform corporate communication following the rise of artificial intelligence\n(AI) systems. It draws on original data from 13 semi-structured interviews with\nexecutive communication experts in large Swiss companies which use corporate\nnewsrooms. Interviews show that corporate newsrooms serve as an organisational\n(rather than spatial) coordination body for topic-oriented and agile corporate\ncommunication. To enable their functionality, it is crucial to find the right\nbalance between optimising and stabilising communication structures. Newsrooms\nactively adopt AI both to facilitate routine tasks and enable more innovative\napplications, such as living data archives and channel translations. Interviews\nalso highlight an urgent need for AI regulation for corporate communication.\nThe article's findings provide important insights into the practical challenges\nand coping strategies for establishing and managing corporate newsrooms and how\nnewsrooms can be transformed by AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.19627,regular,post_llm,2024,7,"{'ai_likelihood': 0.07737901475694445, 'text': 'CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing\n\n  Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.073,review,post_llm,2024,7,"{'ai_likelihood': 8.940696716308594e-07, 'text': ""From Principles to Rules: A Regulatory Approach for Frontier AI\n\nSeveral jurisdictions are starting to regulate frontier artificial intelligence (AI) systems, i.e. general-purpose AI systems that match or exceed the capabilities present in the most advanced systems. To reduce risks from these systems, regulators may require frontier AI developers to adopt safety measures. The requirements could be formulated as high-level principles (e.g. 'AI systems should be safe and secure') or specific rules (e.g. 'AI systems must be evaluated for dangerous model capabilities following the protocol set forth in...'). These regulatory approaches, known as 'principle-based' and 'rule-based' regulation, have complementary strengths and weaknesses. While specific rules provide more certainty and are easier to enforce, they can quickly become outdated and lead to box-ticking. Conversely, while high-level principles provide less certainty and are more costly to enforce, they are more adaptable and more appropriate in situations where the regulator is unsure exactly what behavior would best advance a given regulatory objective. However, rule-based and principle-based regulation are not binary options. Policymakers must choose a point on the spectrum between them, recognizing that the right level of specificity may vary between requirements and change over time. We recommend that policymakers should initially (1) mandate adherence to high-level principles for safe frontier AI development and deployment, (2) ensure that regulators closely oversee how developers comply with these principles, and (3) urgently build up regulatory capacity. Over time, the approach should likely become more rule-based. Our recommendations are based on a number of assumptions, including (A) risks from frontier AI systems are poorly understood and rapidly evolving, (B) many safety practices are still nascent, and (C) frontier AI developers are best placed to innovate on safety practices."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.07762,regular,post_llm,2024,7,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Learning and Motivational Impact of Game-Based Learning: Comparing\n  Face-to-Face and Online Formats on Computer Science Education\n\n  Contribution: This article analyzes the learning and motivational impact of\nteacher-authored educational video games on computer science education and\ncompares its effectiveness in both face-to-face and online (remote) formats.\nThis work presents comparative data and findings obtained from 217 students who\nplayed the game in a face-to-face format (control group) and 104 students who\nplayed the game in an online format (experimental group). Background: Serious\nvideo games have been proven effective at computer science education, however,\nit is still unknown whether the effectiveness of these games is the same\nregardless of their format, face-to-face or online. Moreover, the usage of\ngames created through authoring tools has barely been explored. Research\nQuestions: Are teacher-authored educational video games effective in terms of\nlearning and motivation for computer science students? Does the effectiveness\nof teacher-authored educational video games depend on whether they are used in\na face-to-face or online format? Methodology: A quasi-experiment has been\nconducted by using three instruments (pre-test, post-test, and questionnaire)\nwith the purpose of comparing the effectiveness of game-based learning in\nface-to-face and online formats. A total of 321 computer science students\nplayed a teacher-authored educational video game aimed to learn about software\ndesign. Findings: The results reveal that teacher-authored educational video\ngames are highly effective in terms of knowledge acquisition and motivation\nboth in face-to-face and online formats. The results also show that some\nstudents' perceptions were more positive when a face-to-face format was used.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11918,regular,post_llm,2024,7,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'Market or Markets? Investigating Google Search\'s Market Shares Under\n  Horizontal and Vertical Segmentation\n\n  Is Google Search a monopoly with gatekeeping power? Regulators from the US,\nUK, and Europe have argued that it is based on the assumption that Google\nSearch dominates the market for horizontal (a.k.a. ""general"") web search.\nGoogle disputes this, claiming that competition extends to all vertical (a.k.a.\n""specialized"") search engines, and that under this market definition it does\nnot have monopoly power. In this study we present the first analysis of Google\nSearch\'s market share under both horizontal and vertical segmentation of online\nsearch. We leverage observational trace data collected from a panel of US\nresidents that includes their web browsing history and copies of the Google\nSearch Engine Result Pages they were shown. We observe that Google Search\nreceives 71.8% of participants\' queries when compared to other horizontal\nsearch engines, and that participants\' search sessions begin at Google greater\nthan 50% of the time in 24 out of 30 vertical market segments (which comprise\nalmost all of our participants\' searches). Our results inform the consequential\nand ongoing debates about the market power of Google Search and the\nconceptualization of online markets in general.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17483,regular,post_llm,2024,7,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Tackling CS education in K-12: Implementing a Google CS4HS Grant Program\n  in a Rural Underserved Area\n\n  Providing computer science (CS) offerings in the K-12 education system is\noften limited by the lack of experienced teachers, especially in small or rural\nunderserved school districts. By helping teachers in underserved areas develop\nCS curriculum and helping them become certified to teach CS courses, more young\npeople in underserved areas are aware of IT-career opportunities, and prepared\nfor CS education at the university level, which ultimately helps tackle the IT\nworkforce deficit in the United States.\n  This paper discusses a successful implementation of a Google CS4HS grant to a\nrural underserved area, as well as lessons learned through the implementation\nof the program. Key elements in the implementation included a face-to-face\nhands-on workshop, followed by a seven week graduate-level online summer course\nfor the teachers to learn and develop curriculum that covers the CS concepts\nthey will be teaching. The teachers were supported with an online community of\npractice for the year as they implemented the curriculum.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.14452,regular,post_llm,2024,7,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Undermining Mental Proof: How AI Can Make Cooperation Harder by Making\n  Thinking Easier\n\n  Large language models and other highly capable AI systems ease the burdens of\ndeciding what to say or do, but this very ease can undermine the effectiveness\nof our actions in social contexts. We explain this apparent tension by\nintroducing the integrative theoretical concept of ""mental proof,"" which occurs\nwhen observable actions are used to certify unobservable mental facts. From\nhiring to dating, mental proofs enable people to credibly communicate values,\nintentions, states of knowledge, and other private features of their minds to\none another in low-trust environments where honesty cannot be easily enforced.\nDrawing on results from economics, theoretical biology, and computer science,\nwe describe the core theoretical mechanisms that enable people to effect mental\nproofs. An analysis of these mechanisms clarifies when and how artificial\nintelligence can make low-trust cooperation harder despite making thinking\neasier.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17677,review,post_llm,2024,7,"{'ai_likelihood': 1.4901161193847656e-06, 'text': 'Women\'s Participation in Computing: Evolving Research Methods\n\n  A 2022 keynote for the ACM History Committee on ""Why SIG History Matters: New\nData on Gender Bias in ACM\'s Founding SIGs 1970-2000"" presented new data\ndescribing women\'s participation as research-article authors in 13 early ACM\nSpecial Interest Groups, finding significant growth in women\'s participation\nacross 1970-2000 and, additionally, remarkable differences in women\'s\nparticipation between the SIGs. That presentation built on several earlier\npublications that developed a research method for assessing the number of women\ncomputer scientists that [a] are chronologically prior to the availability of\nthe Bureau of Labor Statistics (BLS) data on women in the IT workforce; and [b]\npermit focused investigation of varied sub-fields within computing. This\npresent report expands on these earlier articles, and their evolving research\nmethod, connecting them to the ACM SIG Heritage presentation. It also outlines\nsome of the choices and considerations made in developing and refining ""mixed\nmethods"" research (using both quantitative and qualitative approaches) as well\nas extensions of the research being currently explored.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20847,review,post_llm,2024,7,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Who Should Run Advanced AI Evaluations -- AISIs?\n\nArtificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate advanced AI themselves, support a private evaluation ecosystem or do both. Evaluation regimes have been established in a wide range of industry contexts to monitor and evaluate firms' compliance with regulation. Evaluation is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should evaluate which parts of advanced AI; and (ii) how much capacity public bodies may need to evaluate advanced AI effectively. First, the effective responsibility distribution between public and private evaluators depends heavily on specific industry and evaluation conditions. On the basis of advanced AI's risk profile, the sensitivity of information involved in the evaluation process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model evaluations. Governance and security audits, which are well-established in other industry contexts, as well as black-box model evaluations, may be more efficiently provided by a private market of evaluators and auditors under public oversight. Secondly, to effectively fulfil their role in advanced AI audits, public bodies need extensive access to models and facilities. AISI's capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for evaluations in large jurisdictions like the EU or US, like in nuclear safety and life sciences."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.0142,regular,post_llm,2024,7,"{'ai_likelihood': 2.4504131740993925e-06, 'text': 'Coordinated Disclosure of Dual-Use Capabilities: An Early Warning System\n  for Advanced AI\n\n  Advanced AI systems may be developed which exhibit capabilities that present\nsignificant risks to public safety or security. They may also exhibit\ncapabilities that may be applied defensively in a wide set of domains,\nincluding (but not limited to) developing societal resilience against AI\nthreats. We propose Coordinated Disclosure of Dual-Use Capabilities (CDDC) as a\nprocess to guide early information-sharing between advanced AI developers, US\ngovernment agencies, and other private sector actors about these capabilities.\nThe process centers around an information clearinghouse (the ""coordinator"")\nwhich receives evidence of dual-use capabilities from finders via mandatory\nand/or voluntary reporting pathways, and passes noteworthy reports to defenders\nfor follow-up (i.e., further analysis and response). This aims to provide the\nUS government, dual-use foundation model developers, and other actors with an\noverview of AI capabilities that could significantly impact public safety and\nsecurity, as well as maximal time to respond.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20514,review,post_llm,2024,7,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'The Future of International Data Transfers: Managing Legal Risk with a\n  User-Held Data Model\n\n  The General Data Protection Regulation contains a blanket prohibition on the\ntransfer of personal data outside of the European Economic Area unless strict\nrequirements are met. The rationale for this provision is to protect personal\ndata and data subject rights by restricting data transfers to countries that\nmay not have the same level of protection as the EEA. However, the ubiquitous\nand permeable character of new technologies such as cloud computing, and the\nincreased inter connectivity between societies, has made international data\ntransfers the norm and not the exception. The Schrems II case and subsequent\nregulatory developments have further raised the bar for companies to comply\nwith complex and, often, opaque rules. Many firms are, therefore, pursuing\ntechnology-based solutions in order to mitigate this new legal risk. These\nemerging technological alternatives reduce the need for open-ended cross-border\ntransfers and the practical challenges and legal risk that such transfers\ncreate after Schrems. This article examines one such alternative, namely a\nuser-held data model. This approach takes advantage of personal data clouds\nthat allows data subjects to store their data locally and in a more\ndecentralised manner, thus decreasing the need for cross-border transfers and\noffering end-users the possibility of greater control over their data.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.08323,regular,post_llm,2024,7,"{'ai_likelihood': 0.00025762452019585506, 'text': 'Leveraging GPT for the Generation of Multi-Platform Social Media\n  Datasets for Research\n\n  Social media datasets are essential for research on disinformation, influence\noperations, social sensing, hate speech detection, cyberbullying, and other\nsignificant topics. However, access to these datasets is often restricted due\nto costs and platform regulations. As such, acquiring datasets that span\nmultiple platforms which are crucial for a comprehensive understanding of the\ndigital ecosystem is particularly challenging. This paper explores the\npotential of large language models to create lexically and semantically\nrelevant social media datasets across multiple platforms, aiming to match the\nquality of real datasets. We employ ChatGPT to generate synthetic data from two\nreal datasets, each consisting of posts from three different social media\nplatforms. We assess the lexical and semantic properties of the synthetic data\nand compare them with those of the real data. Our empirical findings suggest\nthat using large language models to generate synthetic multi-platform social\nmedia data is promising. However, further enhancements are necessary to improve\nthe fidelity of the outputs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11207,regular,post_llm,2024,7,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Multi-MedChain: Multi-Party Multi-Blockchain Medical Supply Chain\n  Management System\n\n  The challenges of healthcare supply chain management systems during the\nCOVID-19 pandemic highlighted the need for an innovative and robust medical\nsupply chain. The healthcare supply chain involves various stakeholders who\nmust share information securely and actively. Regulatory and compliance\nreporting is also another crucial requirement for perishable products (e.g.,\npharmaceuticals) within a medical supply chain management system. Here, we\npropose Multi-MedChain as a three-layer multi-party, multi-blockchain (MPMB)\nframework utilizing smart contracts as a practical solution to address\nchallenges in existing medical supply chain management systems. Multi-MedChain\nis a scalable supply chain management system for the healthcare domain that\naddresses end-to-end traceability, transparency, and collaborative access\ncontrol to restrict access to private data. We have implemented our proposed\nsystem and report on our evaluation to highlight the practicality of the\nsolution. The proposed solution is made publicly available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.05104,regular,post_llm,2024,7,"{'ai_likelihood': 0.0007910198635525174, 'text': 'Crowdsourced reviews reveal substantial disparities in public\n  perceptions of parking\n\n  Due to increased reliance on private vehicles and growing travel demand,\nparking remains a longstanding urban challenge globally. Quantifying parking\nperceptions is paramount as it enables decision-makers to identify problematic\nareas and make informed decisions on parking management. This study introduces\na cost-effective and widely accessible data source, crowdsourced online\nreviews, to investigate public perceptions of parking across the U.S.\nSpecifically, we examine 4,987,483 parking-related reviews for 1,129,460 points\nof interest (POIs) across 911 core-based statistical areas (CBSAs) sourced from\nGoogle Maps. We employ the Bidirectional Encoder Representations from\nTransformers (BERT) model to classify the parking sentiment and conduct\nregression analyses to explore its relationships with socio-spatial factors.\nFindings reveal significant variations in parking sentiment across POI types\nand CBSAs, with Restaurant POIs showing the most negative. Regression results\nfurther indicate that denser urban areas with higher proportions of African\nAmericans and Hispanics and lower socioeconomic status are more likely to\nexhibit negative parking sentiment. Interestingly, an opposite relationship\nbetween parking supply and sentiment is observed, indicating increasing supply\ndoes not necessarily improve parking experiences. Finally, our textual analysis\nidentifies keywords associated with positive or negative sentiments and\nhighlights disparities between urban and rural areas. Overall, this study\ndemonstrates the potential of a novel data source and methodological framework\nin measuring parking sentiment, offering valuable insights that help identify\nhyperlocal parking issues and guide targeted parking management strategies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17618,review,post_llm,2024,7,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Productive self/vulnerable body: self-tracking, overworking culture, and\n  conflicted data practices\n\n  Self-tracking, the collection, analysis, and interpretation of personal data,\nsignifies an individualized way of health governance as people are demanded to\nbuild a responsible self by internalizing norms. However, the technological\npromises often bear conflicts with various social factors such as a strenuous\nschedule, a lack of motivation, stress, and anxieties, which fail to deliver\nhealth outcomes. To re-problematize the phenomenon, this paper situates\nself-tracking in an overworking culture in China and draws on semi structured\nand in depth interviews with overworking individuals to reveal the patterns in\nusers interactions and interpretations with self-tracking data. It builds on\nthe current literature of self-tracking and engages with theories from Science\nand Technology Studies, especially sociomaterial assemblages (Lupton 2016) and\ntechnological mediation (Verbeek 2005), to study self-tracking in a\ncontextualized way which connects the micro (data reading, visualization, and\naffective elements in design) with the macro (work and workplaces,\nsocioeconomic and political background) contexts of self-tracking. Drawing on\ninvestigation of the social context that users of self-tracking technologies\ninternalize, reflect, or resist, the paper argues that the productivity and\nvalue oriented assumptions and workplace culture shape the imaginary of\nintensive (and sometimes impossible) self-care and health, an involution of\ncompetence embedded in the technological design and users affective\nexperiences. Users respond by enacting different design elements and social\ncontexts to frame two distinctive data practices of self-tracking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.06119,review,post_llm,2024,7,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Report on the NSF Workshop on Sustainable Computing for Sustainability\n  (NSF WSCS 2024)\n\n  This report documents the process that led to the NSF Workshop on\n""Sustainable Computing for Sustainability"" held in April 2024 at NSF in\nAlexandria, VA, and reports on its findings. The workshop\'s primary goals were\nto (i) advance the development of research initiatives along the themes of both\nsustainable computing and computing for sustainability, while also (ii) helping\ndevelop and sustain the interdisciplinary teams those initiatives would need.\nThe workshop\'s findings are in the form of recommendations grouped in three\ncategories: General recommendations that cut across both themes of sustainable\ncomputing and computing for sustainability, and recommendations that are\nspecific to sustainable computing and computing for sustainability,\nrespectively.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20442,regular,post_llm,2024,7,"{'ai_likelihood': 0.0015354156494140625, 'text': ""The GPT Dilemma: Foundation Models and the Shadow of Dual-Use\n\n  This paper examines the dual-use challenges of foundation models and the\nconsequent risks they pose for international security. As artificial\nintelligence (AI) models are increasingly tested and deployed across both\ncivilian and military sectors, distinguishing between these uses becomes more\ncomplex, potentially leading to misunderstandings and unintended escalations\namong states. The broad capabilities of foundation models lower the cost of\nrepurposing civilian models for military uses, making it difficult to discern\nanother state's intentions behind developing and deploying these models. As\nmilitary capabilities are increasingly augmented by AI, this discernment is\ncrucial in evaluating the extent to which a state poses a military threat.\nConsequently, the ability to distinguish between military and civilian\napplications of these models is key to averting potential military escalations.\nThe paper analyzes this issue through four critical factors in the development\ncycle of foundation models: model inputs, capabilities, system use cases, and\nsystem deployment. This framework helps elucidate the points at which ambiguity\nbetween civilian and military applications may arise, leading to potential\nmisperceptions. Using the Intermediate-Range Nuclear Forces (INF) Treaty as a\ncase study, this paper proposes several strategies to mitigate the associated\nrisks. These include establishing red lines for military competition, enhancing\ninformation-sharing protocols, employing foundation models to promote\ninternational transparency, and imposing constraints on specific weapon\nplatforms. By managing dual-use risks effectively, these strategies aim to\nminimize potential escalations and address the trade-offs accompanying\nincreasingly general AI models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.14683,regular,post_llm,2024,7,"{'ai_likelihood': 8.44399134318034e-06, 'text': 'Large-Area Emergency Lockdowns with Automated Driving Systems\n\n  Region-wide restrictions on personal vehicle travel have a long history in\nthe United States, from riot curfews in the late 1960s, to travel bans during\nsnow events, to the 2013 shelter-in-place ""lockdown"" during the search for the\nperpetrator of the Boston Marathon bombing. Because lockdowns require\ntremendous resources to enforce, they are often limited in duration or scope.\nThe introduction of automated driving systems may allow governments to quickly\nand cheaply effect large-area lockdowns by jamming wireless communications,\nspoofing road closures on digital maps, exploiting a vehicle\'s programming to\nobey all traffic control devices, or coordinating with vehicle developers.\nFuture vehicles may lack conventional controls, rendering them undrivable by\nthe public. As travel restrictions become easier to implement, governments may\nenforce them more frequently, over longer durations and wider areas. This\narticle explores the practical, legal, and ethical implications of lockdowns\nwhen most driving is highly automated, and provides guidance for the\ndevelopment of lockdown policies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.01192,regular,post_llm,2024,7,"{'ai_likelihood': 8.841355641682943e-06, 'text': 'General collections demography model with multiple risks\n\n  This note presents an Agent-Based Model (ABM) with Monte Carlo sampling,\ndesigned to simulate the behaviour of a population of objects over time. The\nmodel incorporates damage functions with the risk parameters of the ABC\nframework to simulate adverse events. As a result, it combines continuous and\nprobabilistic degradation. This hybrid approach allows us to study the emergent\nbehavior of the system and explore the range of possible lifetimes of a\ncollection. The main outcome of the model is the decay in condition of a\ncollection as a consequence of all the combined degradation processes. The\nmodel is based on six hypotheses that are described for further testing. This\npaper presents a first attempt at an universal implementation of Collections\nDemography principles, with the hope that it will generate discussion and the\nidentification of research gaps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.02072,regular,post_llm,2024,7,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Opportunities and Challenges of Urban Agetech: from an Automated City to\n  an Ageing-Friendly City\n\n  Caring for the elderly, aging-in-place, and enabling the elderly to maintain\na good life continue to be topics of increasing importance, especially in\ncountries with a higher percentage of older people, as people live longer, and\ncare-giving costs rise. This position paper proposes the concept of urban\nagetech, where agetech services beyond the home can be an integral part of a\nmodern ageing-friendly city, and where support for the elderly, where needed,\nin the form of automated systems (e.g., robots and automated vehicles) would be\na normal city function/service, akin to the rather commonplace public transport\nservices today.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.1391,review,post_llm,2024,7,"{'ai_likelihood': 1.5232298109266494e-06, 'text': ""Social Capital and Persistence in Computer Science of Google's Computer\n  Science Summer Institute (CSSI) Students\n\n  While a lucrative and growing field, low levels of gender and racial\ndiversity in CS remain prevalent. Education and workforce support programs with\nthe intention to promote underrepresented students' persistence in CS exist,\nwhich teach skills, inform of career options, and grow students' network in CS.\nStudies demonstrate these programs' effectiveness as it relates to changes in\naffective outcomes, such as participants' confidence in CS skills and attitudes\ntowards CS jobs. However, programs' longitudinal impact on participants'\nbuild-up of social capital in CS, and the resulting social capital's influence\non their persistence in CS, remain unexplored. Motivated by the literature that\nassociates demographic identifiers with access to social capital, and students'\naccess to developmental relationships and career resources (social capital) in\nCS with their persistence, this study explores a CS support program's impact on\npersistence through capital building. We focus on Google's CSSI, which provided\ngraduating high school students with a 3-week-long introduction to CS. We use\ninterviews with participants who are now 2-5 years out of the program to study\nCSSI's impact on their social capital and long-term CS persistence. Thematic\nanalysis reveals three program elements that influenced students' build-up of\nsocial capital, and that the resulting persistence was realized through\nstudents' progress towards internships and goals for paying-it-forward in CS.\nThese findings inform our recommendations that future support programs and\neducational settings consider mentorship centered on socioemotional support,\nopportunities for collaboration, and time for fun social activities. Additional\nsuggestions center on engaging socially-oriented individuals with CS support\nprograms. These insights inform CS educators on design choices that can\nencourage the persistence of underrepresented students in CS.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.0144,review,post_llm,2024,7,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'To Be, Or Not To Be?: Regulating Impossible AI in the United States\n\n  Many AI systems are deployed even when they do not work. Some AI will simply\nnever be able to perform the task it claims to perform. We call such systems\nImpossible AI. This paper seeks to provide an integrated introduction to\nImpossible AI in the United States and guide advocates, both technical and\npolicy, to push forward regulation of Impossible AI in the U.S. The paper\ntracks three examples of Impossible AI through their development, deployment,\ncriticism, and government regulation (or lack thereof). We combine this with an\nanalysis of the fundamental barriers in the way of current calls for Impossible\nAI regulation and then offer areas and directions in which to focus advocacy.\nIn particular, we advance a functionality-first approach that centers the\nfundamental impossibility of these systems and caution against criti-hype. This\nwork is part of a broader shift in the community to focus on validity\nchallenges to AI, the decision not to deploy technical systems, and connecting\ntechnical work with advocacy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.10777,review,post_llm,2024,7,"{'ai_likelihood': 0.21999782986111113, 'text': 'Exploring the Factors of ""AI Guilt"" Among Students -- Are You Guilty of\n  Using AI in Your Homework?\n\n  This study explores the phenomenon of ""AI guilt"" among secondary school\nstudents, a form of moral discomfort arising from the use of AI tools in\nacademic tasks traditionally performed by humans. Through qualitative\nmethodologies, the research examines the factors contributing to AI guilt, its\nsocial and psychological impacts, and its implications for educational\npractices. The findings revealed three main dimensions for AI guilt - perceived\nlaziness and authenticity, fear of judgment, and identity and self-efficacy\nconcerns. The findings suggest a need to redefine academic integrity and shift\nour mindset to reconsider what we should value in education. The study also\nemphasizes the importance of ethical guidelines and educational support and\nprovides implications to help students navigate the complexities of AI in\neducation, reducing feelings of guilt while enhancing learning outcomes.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.04383,review,post_llm,2024,7,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Challenges for Real-Time Toxicity Detection in Online Games\n\n  Online multiplayer games like League of Legends, Counter Strike, and\nSkribbl.io create experiences through community interactions. Providing players\nwith the ability to interact with each other through multiple modes also opens\na Pandora box. Toxic behaviour and malicious players can ruin the experience,\nreduce the player base and potentially harming the success of the game and the\nstudio. This article will give a brief overview of the challenges faced in\ntoxic content detection in terms of text, audio and image processing problems,\nand behavioural toxicity. It also discusses the current practices in\ncompany-directed and user-directed content detection and discuss the values and\nlimitations of automated content detection in the age of artificial\nintelligence.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.15125,regular,post_llm,2024,7,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'The dark side of the metaverse: The role of gamification in event\n  virtualization\n\n  The virtualization of cultural events in the metaverse creates opportunities\nto generate valuable and innovative experiences that replicate and extend\nin-person events; but the process faces associated challenges. In the absence\nof relevant empirical studies, the aim of this article is to analyze the\npositive and negative aspects of the user experience in a cultural event held\nin the metaverse. A mixed-methods approach is employed to test the proposed\nhypotheses. The results from three focus groups demonstrated the difficulty\nthat users face in focusing their attention on the main elements of the\nmetaverse, and the inability of this virtual sphere to convey the authenticity\nof a cultural event. Based on these findings, a metaverse-focused quantitative\nstudy was conducted to examine whether perceived gamification mitigate the\nnegative effects of users failing to pay attention in their metaverse\nexperiences. When users increased their attention levels, their ability to\nimagine the real experience and their perceptions of the authenticity of the\ncultural event increased, which produced positive behavioral intentions. This\nis one of the first studies to empirically analyze the tourist experience in\nthe metaverse; managers and policymakers can benefit from the results to hold\nvaluable virtual cultural events.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.12488,review,post_llm,2024,7,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""What's Distributive Justice Got to Do with It? Rethinking Algorithmic\n  Fairness from the Perspective of Approximate Justice\n\n  In the field of algorithmic fairness, many fairness criteria have been\nproposed. Oftentimes, their proposal is only accompanied by a loose link to\nideas from moral philosophy -- which makes it difficult to understand when the\nproposed criteria should be used to evaluate the fairness of a decision-making\nsystem. More recently, researchers have thus retroactively tried to tie\nexisting fairness criteria to philosophical concepts. Group fairness criteria\nhave typically been linked to egalitarianism, a theory of distributive justice.\nThis makes it tempting to believe that fairness criteria mathematically\nrepresent ideals of distributive justice and this is indeed how they are\ntypically portrayed. In this paper, we will discuss why the current approach of\nlinking algorithmic fairness and distributive justice is too simplistic and,\nhence, insufficient. We argue that in the context of imperfect decision-making\nsystems -- which is what we deal with in algorithmic fairness -- we should not\nonly care about what the ideal distribution of benefits/harms among individuals\nwould look like but also about how deviations from said ideal are distributed.\nOur claim is that algorithmic fairness is concerned with unfairness in these\ndeviations. This requires us to rethink the way in which we, as algorithmic\nfairness researchers, view distributive justice and use fairness criteria.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.07757,regular,post_llm,2024,7,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Can ChatGPT Pass a Theory of Computing Course?\n\n  Large Language Models (LLMs) have had considerable difficulty when prompted\nwith mathematical questions, especially those within theory of computing (ToC)\ncourses. In this paper, we detail two experiments regarding our own ToC course\nand the ChatGPT LLM. For the first, we evaluated ChatGPT\'s ability to pass our\nown ToC course\'s exams. For the second, we created a database of sample ToC\nquestions and responses to accommodate other ToC offerings\' choices for topics\nand structure. We scored each of ChatGPT\'s outputs on these questions. Overall,\nwe determined that ChatGPT can pass our ToC course, and is adequate at\nunderstanding common formal definitions and answering ""simple""-style questions,\ne.g., true/false and multiple choice. However, ChatGPT often makes nonsensical\nclaims in open-ended responses, such as proofs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11271,review,post_llm,2024,7,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""An Analysis of European Data and AI Regulations for Automotive\n  Organizations\n\n  This report summarizes the European Union's series of data and AI regulations\nand analyzes them for managers in automotive vehicle manufacturing\norganizations. In particular, we highlight the relevant ideas of the\nregulations, including how they find their roots in earlier legislation, how\nthey contradict and complement each other, as well as the business\nopportunities that these regulations offer. The structure of the report is as\nfollows. First, we address the GDPR as the cornerstone against which the\nrequirements of other regulations are weighed and legislated. Second, we\nexplain the EU Data Act since it directly addresses Internet of Things (IoT)\nfor businesses in the private sector and imposes strict requirements on large\ndata generators such as vehicle manufacturers. For manufacturers, compliance\nwith the EU Data Act is a prerequisite for the subsequent legislation, in\nparticular the EU AI Act. Third, we explain the Data Governance Act, Digital\nServices Act, Digital Markets Act, and EU AI Act in chronological order.\nOverall, we characterize European Union data regulations as a wave set, rooted\nin historical precedent, with important implications for the automotive\nindustry.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17481,review,post_llm,2024,7,"{'ai_likelihood': 3.642506069607205e-06, 'text': 'Human Oversight of Artificial Intelligence and Technical Standardisation\n\n  The adoption of human oversight measures makes it possible to regulate, to\nvarying degrees and in different ways, the decision-making process of\nArtificial Intelligence (AI) systems, for example by placing a human being in\ncharge of supervising the system and, upstream, by developing the AI system to\nenable such supervision. Within the global governance of AI, the requirement\nfor human oversight is embodied in several regulatory formats, within a\ndiversity of normative sources. On the one hand, it reinforces the\naccountability of AI systems\' users (for example, by requiring them to carry\nout certain checks) and, on the other hand, it better protects the individuals\naffected by the AI-based decision (for example, by allowing them to request a\nreview of the decision). In the European context, the AI Act imposes\nobligations on providers of high-risk AI systems (and to some extent also on\nprofessional users of these systems, known as deployers), including the\nintroduction of human oversight tools throughout the life cycle of AI systems,\nincluding by design (and their implementation by deployers). The EU legislator\nis therefore going much further than in the past in ""spelling out"" the legal\nrequirement for human oversight. But it does not intend to provide for all\nimplementation details; it calls on standardisation to technically flesh out\nthis requirement (and more broadly all the requirements of section 2 of chapter\nIII) on the basis of article 40 of the AI Act. In this multi-level regulatory\ncontext, the question of the place of humans in the AI decision-making process\nshould be given particular attention. Indeed, depending on whether it is the\nlaw or the technical standard that sets the contours of human oversight, the\n""regulatory governance"" of AI is not the same: its nature, content and scope\nare different. This analysis is at the heart of the contribution made (or to be\nmade) by legal experts to the central reflection on the most appropriate\nregulatory governance -- in terms of both its institutional format and its\nsubstance -- to ensure the effectiveness of human oversight and AI\ntrustworthiness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.01457,review,post_llm,2024,7,"{'ai_likelihood': 0.7802734375, 'text': ""Future and AI-Ready Data Strategies: Response to DOC RFI on AI and Open\n  Government Data Assets\n\n  The following is a response to the US Department of Commerce's Request for\nInformation (RFI) regarding AI and Open Government Data Assets. First, we\ncommend the Department for its initiative in seeking public insights on the\norganization and sharing of data. To facilitate scientific discovery and\nadvance AI development, it is crucial for all data producers, including the\nDepartment of Commerce and other governmental entities, to prioritize the\nquality of their data corpora. Ensuring data is accessible, scalable, and\nsecure is essential for harnessing its full potential. In our response, we\noutline best practices and key considerations for AI and the Department of\nCommerce's Open Government Data Assets.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.1640625, 'GPT4': 0.0684814453125, 'CLAUDE': 0.0012044906616210938, 'GOOGLE': 0.65966796875, 'OPENAI_O_SERIES': 0.004718780517578125, 'DEEPSEEK': 0.0001958608627319336, 'GROK': 8.827447891235352e-05, 'NOVA': 0.00031065940856933594, 'OTHER': 0.0235443115234375, 'HUMAN': 0.07757568359375}}"
2407.04776,regular,post_llm,2024,7,"{'ai_likelihood': 5.894237094455296e-06, 'text': ""Quantifying Privacy Risks of Public Statistics to Residents of Subsidized Housing\n\nAs the U.S. Census Bureau implements its controversial new disclosure avoidance system, researchers and policymakers debate the necessity of new privacy protections for public statistics. With experiments on both public statistics and synthetic microdata, we explore a particular privacy concern: respondents in subsidized housing may deliberately not mention unauthorized children and other household members for fear of being discovered and evicted. By combining public statistics from the Decennial Census and the Department of Housing and Urban Development, we demonstrate a simple, inexpensive reconstruction attack that could identify subsidized households living in violation of occupancy guidelines in 2010. Experiments on synthetic data suggest that a random swapping mechanism similar to the Census Bureau's 2010 disclosure avoidance measures does not significantly reduce the precision of this attack, while a differentially private mechanism similar to the 2020 disclosure avoidance system does. Our results provide a valuable example for policymakers seeking trustworthy public statistics."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11366,review,post_llm,2024,7,"{'ai_likelihood': 0.4543728298611111, 'text': 'Perceived Importance of ICT Proficiency for Teaching, Learning, and\n  Career Progression among Physical Education Teachers in Pampanga\n\n  The integration of information and communication technology (ICT) has become\nincreasingly vital across various educational fields, including physical\neducation (PE). This study aimed to evaluate the proficiency levels of PE\nteachers in using various ICT applications and to examine the relationship\nbetween the perceived importance of ICT proficiency for teaching and learning,\ncareer advancement, and actual proficiency among Senior High school PE teachers\nin the municipality of Mexico, Pampanga. This study employed a quantitative\ndescriptive approach. PE teachers from the municipality of Mexico, Pampanga,\nwere selected as the respondents. This study used a two-part survey. The first\nsection collected demographic data, such as age, gender, rank/position, and\nyears of teaching experience, and the second section assessed ICT skill levels\nand the perceived importance of ICT in teaching, learning, and career\nprogression. The results revealed that the majority of PE teachers had access\nto ICT resources. However, their proficiency levels with these tools varied\nsignificantly. Factors such as age, teaching experience, and professional\nposition were found to significantly influence teachers proficiency and their\nperceptions of the benefits of ICT integration in PE instruction. The study\nprovided a glimpse of the current state of ICT integration among Senior High\nschool PE teachers in Mexico, Pampanga, Philippines. This also highlights areas\nof improvement. The study suggests that policymakers, administrators, and\ntraining program developers should focus on enhancing the ICT proficiency of PE\nteachers to improve teaching practices and student engagement. Enhancing the\nICT proficiency of PE teachers is recommended to foster better teaching\nexperiences, increase student engagement, and promote overall educational\noutcomes.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.11466,review,post_llm,2024,7,"{'ai_likelihood': 0.14973958333333334, 'text': 'Navigating the Data Trading Crossroads: An Interdisciplinary Survey\n\n  Data has been increasingly recognized as a critical factor in the future\neconomy. However, constructing an efficient data trading market faces\nchallenges such as privacy breaches, data monopolies, and misuse. Despite\nnumerous studies proposing algorithms to protect privacy and methods for\npricing data, a comprehensive understanding of these issues and systemic\nsolutions remain elusive. This paper provides an extensive review and\nevaluation of data trading research, aiming to identify existing problems,\nresearch gaps, and propose potential solutions. We categorize the challenges\ninto three main areas: Compliance Challenges, Collateral Consequences, and\nCostly Transactions (the ""3C problems""), all stemming from ambiguity in data\nrights. Through a quantitative analysis of the literature, we observe a\nparadigm shift from isolated solutions to integrated approaches. Addressing the\nunresolved issue of right ambiguity, we introduce the novel concept of ""data\nusufruct,"" which allows individuals to use and benefit from data they do not\nown. This concept helps reframe data as a more conventional factor of\nproduction and aligns it with established economic theories, paving the way for\na comprehensive framework of research theories, technical tools, and platforms.\nWe hope this survey provides valuable insights and guidance for researchers,\npractitioners, and policymakers, thereby contributing to digital economy\nadvancements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.03846,other,post_llm,2024,7,"{'ai_likelihood': 8.722146352132162e-05, 'text': 'Google Topics as a way out of the cookie dilemma?\n\n  The paper discusses the legal requirements and implications of the processing\nof information and personal data for advertising purposes, particularly in the\nlight of the ""Planet49"" decision of the European Court of Justice (ECJ) and the\n""Cookie Consent II"" decision by the German Federal Court (Bundesgerichtshof,\nBGH). It emphasises that obtaining explicit consent of individuals is necessary\nfor setting cookies. The introduction of the German Telecommunication Telemedia\nData Protection Act (Telekommunikation-Telemedien-Datenschutzgesetz, TTDSG) has\nreplaced the relevant section of the German Telemedia Act (Telemediengesetz,\nTMG) and transpose the concept of informed consent for storing and accessing\ninformation on terminal equipment, aligning with Article 5(3) ePrivacy\nDirective. To meet these requirements, companies exploring alternatives to\nobtaining consent are developing technical mechanisms that rely on a legal\nbasis. Google tested initially ""Federated Learning of Cohorts"" (FLoC) as part\nof their ""Privacy Sandbox"" strategy. This technology was significantly\ncriticized, Google introduced a new project called ""Google Topics"", which aims\nto personalize advertising by categorizing users into interest groups, called\ntopics. Implementation of this technology began in July 2023.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.13467,regular,post_llm,2024,7,"{'ai_likelihood': 5.563100179036459e-06, 'text': 'Personal Data Transfers to Non-EEA Domains: A Tool for Citizens and An\n  Analysis on Italian Public Administration Websites\n\n  Six years after the entry into force of the GDPR, European companies and\norganizations still have difficulties complying with it: the amount of fines\nissued by the European data protection authorities is continuously increasing.\nPersonal data transfers are no exception. In this work we analyse the personal\ndata transfers from more than 20000 Italian Public Administration (PA) entities\nto third countries. We developed ""Minos"", a user-friendly application which\nallows to navigate the web while recording HTTP requests. Then, we used the\nback-end of Minos to automate the analysis. We found that about 14% of the PAs\nwebsites transferred data out of the European Economic Area (EEA). This number\nis an underestimation because only visits to the home pages were object of the\nanalysis. The top 3 destinations of the data transfers are Amazon, Google and\nFonticons, accounting for about the 70% of the bad requests. The most recurrent\nservices which are the object of the requests are cloud computing services and\ncontent delivery networks (CDNs). Our results highlight that, in Italy, a\nrelevant portion of public administrations websites transfers personal data to\nnon EEA countries. In terms of technology policy, these results stress the need\nfor further incentives to improve the PA digital infrastructures. Finally,\nwhile working on refinements of Minos, the version here described is openly\navailable on Zenodo: it can be helpful to a variety of actors (citizens,\nresearchers, activists, policy makers) to increase awareness and enlarge the\ninvestigation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.19565,regular,post_llm,2024,7,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""Machine-arranged Interactions Improve Institutional Belonging and\n  Cohesion\n\n  We investigated how participation in machine-arranged meetings were\nassociated with feelings of institutional belonging and perceptions of\ndemographic groups. We collected data from 535 individuals who participated in\na program to meet new friends. Data consisted of surveys measuring demography,\nbelonging, and perceptions of various demographic groups at the start and end\nof the program. Participants were partitioned into a control group who received\nzero introductions, and an intervention group who received multiple\nintroductions. For each participant, we computed twelve features describing\nparticipation status, demography and the amount of program-facilitated exposure\nto others who were similar to them and different from them. We used a linear\nmodel to study the association of our features with the participants' final\nbelonging and perceptions while controlling for their initial belonging and\nperceptions. We found that those who participated in the machine-arranged\nmeetings had 4.5% higher belonging, and 3.9% more positive perception of\nothers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.02711,review,post_llm,2024,7,"{'ai_likelihood': 1.1722246805826824e-05, 'text': ""AI in Action: Accelerating Progress Towards the Sustainable Development\n  Goals\n\n  Advances in Artificial Intelligence (AI) are helping tackle a growing number\nof societal challenges, demonstrating technology's increasing capability to\naddress complex issues, including those outlined in the United Nations (UN)\nSustainable Development Goals (SDGs). Despite global efforts, 80 percent of SDG\ntargets have deviated, stalled, or regressed, and only 15 percent are on track\nas of 2023, illustrating the urgency of accelerating efforts to meet the goals\nby 2030. We draw on Google's internal and collaborative research, technical\nwork, and social impact initiatives to show AI's potential to accelerate action\non the SDGs and make substantive progress to help address humanity's most\npressing challenges. The paper highlights AI capabilities (including computer\nvision, generative AI, natural language processing, and multimodal AI) and\nshowcases how AI is altering how we approach problem-solving across all 17 SDGs\nthrough use cases, with a spotlight on AI-powered innovation in health,\neducation, and climate. We then offer insights on AI development and deployment\nto drive bold and responsible innovation, enhance impact, close the\naccessibility gap, and ensure that everyone, everywhere, can benefit from AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.09987,review,post_llm,2024,7,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'Unleashing Excellence through Inclusion: Navigating the\n  Engagement-Performance Paradox\n\n  People who feel that they do not belong (or their voice is not heard at work)\ncommonly become disengaged, unproductive, and pessimistic. Inclusive work\nenvironments aspire to close these gaps to increase employee satisfaction while\nreducing absenteeism and turnover. But there is always a job to be done, and\nunder time and resource constraints, democratic approaches can result in\nreduced quality and unacceptable delays. Teams need actionable guidance to\nincorporate inclusive practices that will directly impact effectiveness. This\npaper contributes to the literature on quality and performance management by\ndeveloping a conceptual model of inclusion that directly (and positively)\nimpacts performance, and identifies eight factors that workgroups must address\nto create and maintain inclusive, high performing environments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.08986,review,post_llm,2024,7,"{'ai_likelihood': 0.028059217664930556, 'text': 'Exploring Generative AI Policies in Higher Education: A Comparative\n  Perspective from China, Japan, Mongolia, and the USA\n\n  This study conducts a comparative analysis of national policies on Generative\nAI across four countries: China, Japan, Mongolia, and the USA. Employing the\nQualitative Comparative Analysis (QCA) method, it examines the responses of\nthese nations to Generative AI in higher education settings, scrutinizing the\ndiversity in their approaches within this group. While all four countries\nexhibit a positive attitude toward Generative AI in higher education, Japan and\nthe USA prioritize a human-centered approach and provide direct guidance in\nteaching and learning. In contrast, China and Mongolia prioritize national\nsecurity concerns, with their guidelines focusing more on the societal level\nrather than being specifically tailored to education. Additionally, despite all\nfour countries emphasizing diversity, equity, and inclusion, they consistently\nfail to clearly discuss or implement measures to address the digital divide. By\noffering a comprehensive comparative analysis of attitudes and policies\nregarding Generative AI in higher education across these countries, this study\nenriches existing literature and provides policymakers with a global\nperspective, ensuring that policies in this domain promote inclusion rather\nthan exclusion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.17588,review,post_llm,2024,7,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Development of Autonomous Artificial Intelligence Systems for Corporate\n  Management\n\n  The article discusses development of autonomous artificial intelligence\nsystems for corporate management. The function of a corporate director is still\none of the few that are legislated for execution by a ""natural"" rather than an\n""artificial"" person. The main prerequisites for development of systems for full\nautomation of management decisions made at the level of a board of directors\nare formed in the field of corporate law, machine learning, and compliance with\nthe rules of non-discrimination, transparency, and accountability of decisions\nmade and algorithms applied. The basic methodological approaches in terms of\ncorporate law for the ""autonomous director"" have already been developed and do\nnot get rejection among representatives of the legal sciences. However, there\nis an undeniable need for further extensive research in order to amend\ncorporate law to effectively introduce ""autonomous directors"". In practice,\nthere are two main options of management decisions automation at the level of\ntop management and a board of directors: digital command centers or automation\nof separate functions. Artificial intelligence systems will be subject to the\nsame strict requirements for non-discrimination, transparency, and\naccountability as ""natural"" directors. At a certain stage, autonomous systems\ncan be an effective tool for countries, regions, and companies with a shortage\nof human capital, equalizing or providing additional chances for such countries\nand companies to compete on the global market.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.06232,review,post_llm,2024,7,"{'ai_likelihood': 1.986821492513021e-06, 'text': 'Operationalising AI governance through ethics-based auditing: An\n  industry case study\n\n  Ethics based auditing (EBA) is a structured process whereby an entitys past\nor present behaviour is assessed for consistency with moral principles or\nnorms. Recently, EBA has attracted much attention as a governance mechanism\nthat may bridge the gap between principles and practice in AI ethics. However,\nimportant aspects of EBA (such as the feasibility and effectiveness of\ndifferent auditing procedures) have yet to be substantiated by empirical\nresearch. In this article, we address this knowledge gap by providing insights\nfrom a longitudinal industry case study. Over 12 months, we observed and\nanalysed the internal activities of AstraZeneca, a biopharmaceutical company,\nas it prepared for and underwent an ethics-based AI audit. While previous\nliterature concerning EBA has focused on proposing evaluation metrics or\nvisualisation techniques, our findings suggest that the main difficulties large\nmultinational organisations face when conducting EBA mirror classical\ngovernance challenges. These include ensuring harmonised standards across\ndecentralised organisations, demarcating the scope of the audit, driving\ninternal communication and change management, and measuring actual outcomes.\nThe case study presented in this article contributes to the existing literature\nby providing a detailed description of the organisational context in which EBA\nprocedures must be integrated to be feasible and effective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.16362,review,post_llm,2024,7,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Nudging Using Autonomous Agents: Risks and Ethical Considerations\n\n  This position paper briefly discusses nudging, its use by autonomous agents,\npotential risks and ethical considerations while creating such systems. Instead\nof taking a normative approach, which guides all situations, the paper proposes\na risk-driven questions-and-answer approach. The paper takes the position that\nthis is a pragmatic method, that is transparent about beneficial intentions,\nforeseeable risks, and mitigations. Given the uncertainty in AI and autonomous\nagent capabilities, we believe that such pragmatic methods offer a plausibly\nsafe path, without sacrificing flexibility in domain and technology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2407.20237,review,post_llm,2024,7,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'A Study on Internet of Things in Women and Children Healthcare\n\n  Individual entities are being connected every day with the advancement of\nInternet of Things (IoT). IoT contains various application domains and\nhealthcare is one of them indeed. It is receiving a lot of attention recently\nbecause of its seamless integration with electronic health (eHealth) and\ntelemedicine. IoT has the capability of collecting patient data incessantly\nwhich surely helps in preventive care. Doctors can diagnose their patients\nearly to avoid complications and they can suggest further modifications if\nneeded. As the whole process is automated, risk of errors is reduced.\nAdministrative paperwork and data entry tasks will be automated due to tracking\nand connectivity. As a result, healthcare providers can engage themselves more\nin patient care. In traditional healthcare services, an individual used to have\naccess to minimal insights into his own health. Hence, they were less conscious\nabout themselves and depended wholly on the healthcare facilities for\nunfortunate events. But they can track their vitals, activities and fitness\nwith the aid of connected devices now. Furthermore, they can suggest their\npreferred user interfaces. This paper describes several methods, practices and\nprototypes regarding IoT in the field of healthcare for women and children.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.0159,regular,post_llm,2024,8,"{'ai_likelihood': 1.7550256517198352e-06, 'text': ""Interpretations, Representations, and Stereotypes of Caste within\n  Text-to-Image Generators\n\n  The surge in the popularity of text-to-image generators (T2Is) has been\nmatched by extensive research into ensuring fairness and equitable outcomes,\nwith a focus on how they impact society. However, such work has typically\nfocused on globally-experienced identities or centered Western contexts. In\nthis paper, we address interpretations, representations, and stereotypes\nsurrounding a tragically underexplored context in T2I research: caste. We\nexamine how the T2I Stable Diffusion displays people of various castes, and\nwhat professions they are depicted as performing. Generating 100 images per\nprompt, we perform CLIP-cosine similarity comparisons with default depictions\nof an 'Indian person' by Stable Diffusion, and explore patterns of similarity.\nOur findings reveal how Stable Diffusion outputs perpetuate systems of\n'castelessness', equating Indianness with high-castes and depicting\ncaste-oppressed identities with markers of poverty. In particular, we note the\nstereotyping and representational harm towards the historically-marginalized\nDalits, prominently depicted as living in rural areas and always at protests.\nOur findings underscore a need for a caste-aware approach towards T2I design,\nand we conclude with design recommendations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.00687,regular,post_llm,2024,8,"{'ai_likelihood': 0.06100124782986111, 'text': ""Reducing Urban Speed Limits Decreases Work-Related Traffic Injury\n  Severity: Evidence from Santiago, Chile\n\n  Work-related transportation incidents significantly impact urban mobility and\nproductivity. These incidents include traffic crashes, collisions between\nvehicles, and falls that occurred during commuting or work-related\ntransportation (e.g., falling while getting off a bus during the morning\ncommute or while riding a bicycle for work). This study analyzes a decade of\nwork-related transportation incident data (2012--2021) in Santiago, Chile,\nusing records from a major worker's insurance company. Using negative binomial\nregression, we assess the impact of a 2018 urban speed limit reduction law on\nincident injury severity. We also explore broader temporal, spatial, and\ndemographic patterns in these incidents in urban and rural areas.\n  The urban speed limit reduction is associated with a decrease of 4.26 days in\nprescribed medical leave for incidents in urban areas, suggesting that lower\nspeed limits contribute to reduced injury severity. Our broader analysis\nreveals distinct incident patterns across different groups. Workers traveling\nby motorcycle and bicycle experience more severe injuries when involved in\ntraffic incidents, with marginal effects of 26.94 and 13.06 additional days of\nmedical leave, respectively, compared to motorized vehicles. Women workers tend\nto have less severe injuries, with an average of 7.57 fewer days of medical\nleave. Age is also a significant factor, with older workers experiencing more\nsevere injuries -- each additional year of age is associated with 0.57 more\ndays of medical leave. Our results provide insights for urban planning,\ntransportation policy, and workplace safety initiatives.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.10026,review,post_llm,2024,8,"{'ai_likelihood': 0.00016848246256510418, 'text': ""Defense Priorities in the Open-Source AI Debate: A Preliminary\n  Assessment\n\n  A spirited debate is taking place over the regulation of open foundation\nmodels: artificial intelligence models whose underlying architectures and\nparameters are made public and can be inspected, modified, and run by end\nusers. Proposed limits on releasing open foundation models may have significant\ndefense industrial impacts. If model training is a form of defense production,\nthese impacts deserve further scrutiny. Preliminary evidence suggests that an\nopen foundation model ecosystem could benefit the U.S. Department of Defense's\nsupplier diversity, sustainment, cybersecurity, and innovation priorities.\nFollow-on analyses should quantify impacts on acquisition cost and supply chain\nsecurity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.12245,review,post_llm,2024,8,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Computing-specific pedagogies and theoretical models: common uses and\n  relationships\n\n  Computing education widely applies general learning theories and pedagogical\npractices. However, computing also includes specific disciplinary knowledge and\nskills, e.g., programming and software development methods, for which there has\nbeen a long history of development and application of specific pedagogical\npractices. In recent years, there has also been substantial interest in\ndeveloping computing-specific theoretical models, which seek to describe and\nexplain the complex interactions within teaching and learning computing in\nvarious contexts. In this paper, we explore connections between\ncomputing-specific pedagogies and theoretical models as reported in the\nliterature. Our goal is to enrich computing education research and practice by\nillustrating how explicit use of field-specific theories and pedagogies can\nfurther the whole field. We have collected a list of computing-specific\npedagogical practices and theoretical models from a literature search,\nidentifying source papers where they have been first introduced or well\ndescribed. We then searched for papers in the ACM digital library that cite\nsource papers from each list, and analyzed the type of interaction between the\nmodel and pedagogy in each paper. We developed a categorization of how\ntheoretical models and pedagogies have supported or discounted each other, have\nbeen used together in empirical studies or used to build new artefacts. Our\nresults showed that pair programming and parsons problems have had the most\ninteractions with theoretical models in the explored papers, and we present\nfindings of the analysis of these interactions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.07946,review,post_llm,2024,8,"{'ai_likelihood': 2.165635426839193e-05, 'text': 'US-Singapore cooperation on tech and security: defense, cyber, and\n  biotech\n\n  The partnership between the United States and Singapore is founded in no\nsmall part on the shared recognition of the value that technology has for\nnational security. Over the last 55 years, Singapore has become an established\npurchaser of U.S. defense technology, but the past 20 years have also seen the\nU.S.-Singapore relationship mature into an increasingly collaborative one,\ntackling newer fields like cybersecurity and biosecurity. However, current\ngeopolitical tensions present a challenge for Singapore, which strives to\nretain its strategic autonomy by maintaining positive relations with all\nparties. Paradoxically, the rise of non-traditional security threats may pave\nthe way for greater bilateral cooperation by allowing Singapore to position\nitself as a hub for cooperation on regional security issues in Southeast Asia\nat large. In such spirit, this paper recommends that the United States and\nSingapore do the following: 1) in defense technology, co-develop niche\ncapabilities in C4ISR and unmanned systems with peacetime applications; 2) in\ncybersecurity, improve their domestic resilience against sophisticated\nnation-state actors while also building regional capacity to counter cybercrime\nin Southeast Asia; and 3) in biosecurity, strengthen regional epidemiological\nsurveillance to brace against possible future pandemics.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.04609,review,post_llm,2024,8,"{'ai_likelihood': 0.1557074652777778, 'text': 'Criticizing Ethics According to Artificial Intelligence\n\n  This article presents a critique of ethics in the context of artificial\nintelligence (AI). It argues for the need to question established patterns of\nthought and traditional authorities, including core concepts such as autonomy,\nmorality, and ethics. These concepts are increasingly inadequate to deal with\nthe complexities introduced by emerging AI and autonomous agents. This critique\nhas several key components: clarifying conceptual ambiguities, honestly\naddressing epistemic issues, and thoroughly exploring fundamental normative\nproblems. The ultimate goal is to reevaluate and possibly redefine some\ntraditional ethical concepts to better address the challenges posed by AI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.11975,regular,post_llm,2024,8,"{'ai_likelihood': 8.940696716308594e-07, 'text': 'Automatic knowledge-graph creation from historical documents: The\n  Chilean dictatorship as a case study\n\n  We present our results regarding the automatic construction of a knowledge\ngraph from historical documents related to the Chilean dictatorship period\n(1973-1990). Our approach consists on using LLMs to automatically recognize\nentities and relations between these entities, and also to perform resolution\nbetween these sets of values. In order to prevent hallucination, the\ninteraction with the LLM is grounded in a simple ontology with 4 types of\nentities and 7 types of relations. To evaluate our architecture, we use a gold\nstandard graph constructed using a small subset of the documents, and compare\nthis to the graph obtained from our approach when processing the same set of\ndocuments. Results show that the automatic construction manages to recognize a\ngood portion of all the entities in the gold standard, and that those not\nrecognized are mostly explained by the level of granularity in which the\ninformation is structured in the graph, and not because the automatic approach\nmisses an important entity in the graph. Looking forward, we expect this report\nwill encourage work on other similar projects focused on enhancing research in\nhumanities and social science, but we remark that better evaluation metrics are\nneeded in order to accurately fine-tune these types of architectures.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.0621,review,post_llm,2024,8,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Certified Safe: A Schematic for Approval Regulation of Frontier AI\n\n  Recent and unremitting capability advances have been accompanied by calls for\ncomprehensive, rather than patchwork, regulation of frontier artificial\nintelligence (AI). Approval regulation is emerging as a promising candidate. An\napproval regulation scheme is one in which a firm cannot legally market, or in\nsome cases develop, a product without explicit approval from a regulator on the\nbasis of experiments performed upon the product that demonstrate its safety.\nThis approach is used successfully by the FDA and FAA. Further, its application\nto frontier AI has been publicly supported by many prominent stakeholders. This\nreport proposes an approval regulation schematic for only the largest AI\nprojects in which scrutiny begins before training and continues through to\npost-deployment monitoring. The centerpieces of the schematic are two major\napproval gates, the first requiring approval for large-scale training and the\nsecond for deployment. Five main challenges make implementation difficult:\nnoncompliance through unsanctioned deployment, specification of deployment\nreadiness requirements, reliable model experimentation, filtering out safe\nmodels before the process, and minimizing regulatory overhead. This report\nmakes a number of crucial recommendations to increase the feasibility of\napproval regulation, some of which must be followed urgently if such a regime\nis to succeed in the near future. Further recommendations, produced by this\nreport's analysis, may improve the effectiveness of any regulatory regime for\nfrontier AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.09678,review,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': 'Conference Submission and Review Policies to Foster Responsible\n  Computing Research\n\n  This report by the CRA Working Group on Socially Responsible Computing\noutlines guidelines for ethical and responsible research practices in computing\nconferences. Key areas include avoiding harm, responsible vulnerability\ndisclosure, ethics board review, obtaining consent, accurate reporting,\nmanaging financial conflicts of interest, and the use of generative AI. The\nreport emphasizes the need for conference organizers to adopt clear policies to\nensure responsible computing research and publication, highlighting the\nevolving nature of these guidelines as understanding and practices in the field\nadvance.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0673828125, 'GPT4': 0.012451171875, 'CLAUDE': 0.11614990234375, 'GOOGLE': 0.3447265625, 'OPENAI_O_SERIES': 0.0020313262939453125, 'DEEPSEEK': 0.01085662841796875, 'GROK': 0.0016851425170898438, 'NOVA': 0.01151275634765625, 'OTHER': 0.43310546875, 'HUMAN': 0.0001093745231628418}}"
2408.12754,review,post_llm,2024,8,"{'ai_likelihood': 6.9207615322536894e-06, 'text': 'Artificial Intelligence (AI) Onto-norms and Gender Equality: Unveiling\n  the Invisible Gender Norms in AI Ecosystems in the Context of Africa\n\n  The study examines how ontonorms propagate certain gender practices in\ndigital spaces through character and the norms of spaces that shape AI design,\ntraining and use. Additionally the study explores the different user behaviours\nand practices regarding whether, how, when, and why different gender groups\nengage in and with AI driven spaces. By examining how data and content can\nknowingly or unknowingly be used to drive certain social norms in the AI\necosystems, this study argues that ontonorms shape how AI engages with the\ncontent that relates to women. Ontonorms specifically shape the image,\nbehaviour, and other media, including how gender identities and perspectives\nare intentionally or otherwise, included, missed, or misrepresented in building\nand training AI systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.0313,review,post_llm,2024,8,"{'ai_likelihood': 0.0004696846008300781, 'text': 'Apocalypse, survivalism, occultism and esotericism communities on\n  Brazilian Telegram: when faith is used to sell quantum courses and open doors\n  to harmful conspiracy theories\n\n  Brazilian communities on Telegram have increasingly turned to apocalyptic and\nsurvivalist theories, especially in times of crisis such as the COVID-19\npandemic, where narratives of occultism and esotericism find fertile ground.\nTherefore, this study aims to address the research question: how are Brazilian\nconspiracy theory communities on apocalypse, survivalism, occultism and\nesotericism topics characterized and articulated on Telegram? It is worth\nnoting that this study is part of a series of seven studies whose main\nobjective is to understand and characterize Brazilian conspiracy theory\ncommunities on Telegram. This series of seven studies is openly and originally\navailable on arXiv at Cornell University, applying a mirrored method across the\nseven studies, changing only the thematic object of analysis and providing\ninvestigation replicability, including with proprietary and authored codes,\nadding to the culture of free and open-source software. Regarding the main\nfindings of this study, the following were observed: Occult and esoteric\ncommunities function as gateways to apocalypse theories; Conspiracies about the\nNew World Order are amplified by apocalyptic discussions; Survivalist\nnarratives grew significantly during the Pandemic; Occultism and esotericism\nare sources of invitations to off-label drug communities, reinforcing\nscientific disinformation; Discussions about the apocalypse serve as a start\nfor other conspiracy theories, expanding their reach.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.1461,regular,post_llm,2024,8,"{'ai_likelihood': 0.04513210720486111, 'text': ""The Impact of Group Discussion and Formation on Student Performance: An\n  Experience Report in a Large CS1 Course\n\n  Programming instructors often conduct collaborative learning activities, such\nas Peer Instruction (PI), to enhance student motivation, engagement, and\nlearning gains. However, the impact of group discussion and formation\nmechanisms on student performance remains unclear. To investigate this, we\nconducted an 11-session experiment in a large, in-person CS1 course. We\nemployed both random and expertise-balanced grouping methods to examine the\nefficacy of different group mechanisms and the impact of expert students'\npresence on collaborative learning. Our observations revealed complex dynamics\nwithin the collaborative learning environment. Among 255 groups, 146 actively\nengaged in discussions, with 96 of these groups demonstrating improvement for\npoor-performing students. Interestingly, our analysis revealed that different\ngrouping methods (expertise-balanced or random) did not significantly influence\ndiscussion engagement or poor-performing students' improvement. In our deeper\nqualitative analysis, we found that struggling students often derived benefits\nfrom interactions with expert peers, but this positive effect was not\nconsistent across all groups. We identified challenges that expert students\nface in peer instruction interactions, highlighting the complexity of\nleveraging expertise within group discussions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.16863,regular,post_llm,2024,8,"{'ai_likelihood': 0.03879123263888889, 'text': 'Data-Driven Law Firm Rankings to Reduce Information Asymmetry in Legal\n  Disputes\n\n  Selecting capable counsel can shape the outcome of litigation, yet evaluating\nlaw firm performance remains challenging. Widely used rankings prioritize\nprestige, size, and revenue rather than empirical litigation outcomes, offering\nlittle practical guidance. To address this gap, we build on the Bradley-Terry\nmodel and introduce a new ranking framework that treats each lawsuit as a\ncompetitive game between plaintiff and defendant law firms. Leveraging a newly\nconstructed dataset of 60,540 U.S. civil lawsuits involving 54,541 law firms,\nour findings show that existing reputation-based rankings correlate poorly with\nactual litigation success, whereas our outcome-based ranking substantially\nimproves predictive accuracy. These findings establish a foundation for more\ntransparent, data-driven assessments of legal performance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.07896,regular,post_llm,2024,8,"{'ai_likelihood': 6.953875223795573e-06, 'text': 'The doctor will polygraph you now: ethical concerns with AI for\n  fact-checking patients\n\n  Artificial intelligence (AI) methods have been proposed for the prediction of\nsocial behaviors which could be reasonably understood from patient-reported\ninformation. This raises novel ethical concerns about respect, privacy, and\ncontrol over patient data. Ethical concerns surrounding clinical AI systems for\nsocial behavior verification can be divided into two main categories: (1) the\npotential for inaccuracies/biases within such systems, and (2) the impact on\ntrust in patient-provider relationships with the introduction of automated AI\nsystems for fact-checking, particularly in cases where the data/models may\ncontradict the patient. Additionally, this report simulated the misuse of a\nverification system using patient voice samples and identified a potential LLM\nbias against patient-reported information in favor of multi-dimensional data\nand the outputs of other AI methods (i.e., AI self-trust). Finally,\nrecommendations were presented for mitigating the risk that AI verification\nmethods will cause harm to patients or undermine the purpose of the healthcare\nsystem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.00954,regular,post_llm,2024,8,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'Digital capabilities assessment for supporting the transformation of the\n  customer experience\n\n  Most of organizations are increasingly investing huge amounts of money today\nin order to have the right digital capabilities required for their industry.\nThe area where organisations feel they have made the most progress is in\nimproving the customer experience, which encompasses aspects such as data\nanalytics, social media, location-based marketing, mobile channels among\nothers. This aspect became the most important for the survival of organisations\nsince the outbreak of the Covid-19 pandemic. While much has been achieved, many\norganisations are still not satisfied. One of the major problems in moving\nforward is the lack of literature in both academia and industry on maturity\nmodels allowing organisations to understand their current state in terms of\ndigital capabilities to engage with customers, as well as to plan the\nevolutionary path to improve in this area. To fulfil this lack, this paper\npresents the design and validation of a maturity model that enables\norganizations to assess their digital capabilities in order to improve customer\nexperience and engagement throughout the customer lifecycle.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.07892,review,post_llm,2024,8,"{'ai_likelihood': 6.291601392957899e-06, 'text': 'Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online\n\n  Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: ""personhood credentials"" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI\'s increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI\'s increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and ""proof-of-personhood""\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.15686,review,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': ""Navigating the Future of Education: Educators' Insights on AI\n  Integration and Challenges in Greece, Hungary, Latvia, Ireland and Armenia\n\n  Understanding teachers' perspectives on AI in Education (AIEd) is crucial for\nits effective integration into the educational framework. This paper aims to\nexplore how teachers currently use AI and how it can enhance the educational\nprocess. We conducted a cross-national study spanning Greece, Hungary, Latvia,\nIreland, and Armenia, surveying 1754 educators through an online questionnaire,\naddressing three research questions. Our first research question examines\neducators' understanding of AIEd, their skepticism, and its integration within\nschools. Most educators report a solid understanding of AI and acknowledge its\npotential risks. AIEd is primarily used for educator support and engaging\nstudents. However, concerns exist about AI's impact on fostering critical\nthinking and exposing students to biased data. The second research question\ninvestigates student engagement with AI tools from educators' perspectives.\nTeachers indicate that students use AI mainly to manage their academic\nworkload, while outside school, AI tools are primarily used for entertainment.\nThe third research question addresses future implications of AI in education.\nEducators are optimistic about AI's potential to enhance educational processes,\nparticularly through personalized learning experiences. Nonetheless, they\nexpress significant concerns about AI's impact on cultivating critical thinking\nand ethical issues related to potential misuse. There is a strong emphasis on\nthe need for professional development through training seminars, workshops, and\nonline courses to integrate AI effectively into teaching practices. Overall,\nthe findings highlight a cautious optimism among educators regarding AI in\neducation, alongside a clear demand for targeted professional development to\naddress concerns and enhance skills in using AI tools.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.000453948974609375, 'GPT4': 0.046478271484375, 'CLAUDE': 1.4185905456542969e-05, 'GOOGLE': 0.9521484375, 'OPENAI_O_SERIES': 0.0006394386291503906, 'DEEPSEEK': 6.616115570068359e-06, 'GROK': 4.172325134277344e-07, 'NOVA': 1.7881393432617188e-06, 'OTHER': 0.00016129016876220703, 'HUMAN': 7.092952728271484e-06}}"
2408.04685,review,post_llm,2024,8,"{'ai_likelihood': 5.1657358805338544e-06, 'text': ""Towards the Socio-Algorithmic Construction of Fairness: The Case of\n  Automatic Price-Surging in Ride-Hailing\n\n  Algorithms take decisions that affect humans, and have been shown to\nperpetuate biases and discrimination. Decisions by algorithms are subject to\ndifferent interpretations. Algorithms' behaviors are basis for the construal of\nmoral assessment and standards. Yet we lack an understanding of how algorithms\nimpact on social construction processes, and vice versa. Without such\nunderstanding, social construction processes may be disrupted and, eventually,\nmay impede moral progress in society. We analyze the public discourse that\nemerged after a significant (five-fold) price-surge following the Brooklyn\nSubway Shooting on April 12, 2022, in New York City. There was much controversy\naround the two ride-hailing firms' algorithms' decisions. The discussions\nevolved around various notions of fairness and the algorithms' decisions'\njustifiability. Our results indicate that algorithms, even if not explicitly\naddressed in the discourse, strongly impact on constructing fairness\nassessments and notions. They initiate the exchange, form people's\nexpectations, evoke people's solidarity with specific groups, and are a vehicle\nfor moral crusading. However, they are also subject to adjustments based on\nsocial forces. We claim that the process of constructing notions of fairness is\nno longer just social; it has become a socio-algorithmic process. We propose a\ntheory of socio-algorithmic construction as a mechanism for establishing\nnotions of fairness and other ethical constructs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.02236,review,post_llm,2024,8,"{'ai_likelihood': 0.05113389756944445, 'text': 'An integrated view of Quantum Technology? Mapping Media, Business, and\n  Policy Narratives\n\n  Narratives play a vital role in shaping public perceptions and policy on\nemerging technologies like quantum technology (QT). However, little is known\nabout the construction and variation of QT narratives across societal domains.\nThis study examines how QT is presented in business, media, and government\ntexts using thematic narrative analysis. Our research design utilizes an\nextensive dataset of 36 government documents, 165 business reports, and 2,331\nmedia articles published over 20 years. We employ a computational social\nscience approach, combining BERTopic modeling with qualitative assessment to\nextract themes and narratives. The findings show that public discourse on QT\nreflects prevailing social and political agendas, focusing on technical and\ncommercial potential, global conflicts, national strategies, and social issues.\nMedia articles provide the most balanced coverage, while business and\ngovernment discourses often overlook societal implications. We discuss the\nramifications for integrating QT into society and the need for wellinformed\npublic discourse.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.16411,review,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': ""Defining Interoperability: a universal standard\n\n  Interoperability is crucial for modern scientific advancement, yet its\nfragmented definitions across domains hinder researchers' ability to\neffectively reap the rewards. This paper proposes a new, universal definition\nby tracing the evolution of interoperability and identifying challenges posed\nby varying definitions. This definition addresses these inconsistencies,\noffering a robust solution applicable across diverse fields. Adopting this\nunified approach will enhance global collaboration and drive innovation by\nremoving obstacles to interoperability posed by conflicting or incomplete\ndefinitions.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004432201385498047, 'GPT4': 0.01204681396484375, 'CLAUDE': 0.0006852149963378906, 'GOOGLE': 0.98583984375, 'OPENAI_O_SERIES': 0.000255584716796875, 'DEEPSEEK': 9.077787399291992e-05, 'GROK': 8.940696716308594e-06, 'NOVA': 9.119510650634766e-06, 'OTHER': 0.0007109642028808594, 'HUMAN': 3.7670135498046875e-05}}"
2409.06712,review,post_llm,2024,8,"{'ai_likelihood': 2.7219454447428387e-05, 'text': ""A Meta-analysis of College Students' Intention to Use Generative\n  Artificial Intelligence\n\n  It is of critical importance to analyse the factors influencing college\nstudents' intention to use generative artificial intelligence (GenAI) to\nunderstand and predict learners' learning behaviours and academic outcomes.\nNevertheless, a lack of congruity has been shown in extant research results.\nThis study, therefore, conducted a meta-analysis of 27 empirical studies under\nan integrated theoretical framework, including 87 effect sizes of independent\nresearch and 33,833 sample data. The results revealed that the main variables\nare strongly correlated with students' behavioural intention to use GenAI.\nAmong them, performance expectancy (r = 0.389) and attitudes (r = 0.576) play\nparticularly critical roles, and effort expectancy and habit are moderated by\nlocational factors. Gender, notably, only moderated attitudes on students'\nbehavioural intention to use GenAI. This study provides valuable insights for\naddressing the debate regarding students' intention to use GenAI in existed\nresearch, improving educational technology, as well as offering support for\nschool decision-makers and educators to apply GenAI in school settings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.04684,review,post_llm,2024,8,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Moving beyond privacy and airspace safety: Guidelines for just drones in\n  policing\n\n  The use of drones offers police forces potential gains in efficiency and\nsafety. However, their use may also harm public perception of the police if\ndrones are refused. Therefore, police forces should consider the perception of\nbystanders and broader society to maximize drones' potential. This article\nexamines the concerns expressed by members of the public during a field trial\ninvolving 52 test participants. Analysis of the group interviews suggests that\ntheir worries go beyond airspace safety and privacy, broadly discussed in\nexisting literature and regulations. The interpretation of the results\nindicates that the perceived justice of drone use is a significant factor in\nacceptance. Leveraging the concept of organizational justice and data\ncollected, we propose a catalogue of guidelines for just operation of drones to\nsupplement the existing policy. We present the organizational justice\nperspective as a framework to integrate the concerns of the public and\nbystanders into legal work. Finally, we discuss the relevance of justice for\nthe legitimacy of the police's actions and provide implications for research\nand practice.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.12088,regular,post_llm,2024,8,"{'ai_likelihood': 0.000984403822157118, 'text': 'Mental-Perceiver: Audio-Textual Multi-Modal Learning for Estimating\n  Mental Disorders\n\n  Mental disorders, such as anxiety and depression, have become a global\nconcern that affects people of all ages. Early detection and treatment are\ncrucial to mitigate the negative effects these disorders can have on daily\nlife. Although AI-based detection methods show promise, progress is hindered by\nthe lack of publicly available large-scale datasets. To address this, we\nintroduce the Multi-Modal Psychological assessment corpus (MMPsy), a\nlarge-scale dataset containing audio recordings and transcripts from\nMandarin-speaking adolescents undergoing automated anxiety/depression\nassessment interviews. MMPsy also includes self-reported anxiety/depression\nevaluations using standardized psychological questionnaires. Leveraging this\ndataset, we propose Mental-Perceiver, a deep learning model for estimating\nmental disorders from audio and textual data. Extensive experiments on MMPsy\nand the DAIC-WOZ dataset demonstrate the effectiveness of Mental-Perceiver in\nanxiety and depression detection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.11358,review,post_llm,2024,8,"{'ai_likelihood': 4.308091269599067e-05, 'text': 'Gender Bias Evaluation in Text-to-image Generation: A Survey\n\n  The rapid development of text-to-image generation has brought rising ethical\nconsiderations, especially regarding gender bias. Given a text prompt as input,\ntext-to-image models generate images according to the prompt. Pioneering models\nsuch as Stable Diffusion and DALL-E 2 have demonstrated remarkable capabilities\nin producing high-fidelity images from natural language prompts. However, these\nmodels often exhibit gender bias, as studied by the tendency of generating man\nfrom prompts such as ""a photo of a software developer"". Given the widespread\napplication and increasing accessibility of these models, bias evaluation is\ncrucial for regulating the development of text-to-image generation. Unlike\nwell-established metrics for evaluating image quality or fidelity, the\nevaluation of bias presents challenges and lacks standard approaches. Although\nbiases related to other factors, such as skin tone, have been explored, gender\nbias remains the most extensively studied. In this paper, we review recent work\non gender bias evaluation in text-to-image generation, involving bias\nevaluation setup, bias evaluation metrics, and findings and trends. We\nprimarily focus on the evaluation of recent popular models such as Stable\nDiffusion, a diffusion model operating in the latent space and using CLIP text\nembedding, and DALL-E 2, a diffusion model leveraging Seq2Seq architectures\nlike BART. By analyzing recent work and discussing trends, we aim to provide\ninsights for future work.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.16771,review,post_llm,2024,8,"{'ai_likelihood': 9.80165269639757e-06, 'text': 'Navigating Governance Paradigms: A Cross-Regional Comparative Study of\n  Generative AI Governance Processes & Principles\n\n  As Generative Artificial Intelligence (GenAI) technologies evolve at an\nunprecedented rate, global governance approaches struggle to keep pace with the\ntechnology, highlighting a critical issue in the governance adaptation of\nsignificant challenges. Depicting the nuances of nascent and diverse governance\napproaches based on risks, rules, outcomes, principles, or a mix across\ndifferent regions around the globe is fundamental to discern discrepancies and\nconvergences and to shed light on specific limitations that need to be\naddressed, thereby facilitating the safe and trustworthy adoption of GenAI. In\nresponse to the need and the evolving nature of GenAI, this paper seeks to\nprovide a collective view of different governance approaches around the world.\nOur research introduces a Harmonized GenAI Framework, ""H-GenAIGF,"" based on the\ncurrent governance approaches of six regions: European Union (EU), United\nStates (US), China (CN), Canada (CA), United Kingdom (UK), and Singapore (SG).\nWe have identified four constituents, fifteen processes, twenty-five\nsub-processes, and nine principles that aid the governance of GenAI, thus\nproviding a comprehensive perspective on the current state of GenAI governance.\nIn addition, we present a comparative analysis to facilitate the identification\nof common ground and distinctions based on the coverage of the processes by\neach region. The results show that risk-based approaches allow for better\ncoverage of the processes, followed by mixed approaches. Other approaches lag\nbehind, covering less than 50% of the processes. Most prominently, the analysis\ndemonstrates that among the regions, only one process aligns across all\napproaches, highlighting the lack of consistent and executable provisions.\nMoreover, our case study on ChatGPT reveals process coverage deficiency,\nshowing that harmonization of approaches is necessary to find alignment for\nGenAI governance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.1474,review,post_llm,2024,8,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""Properties of Effective Information Anonymity Regulations\n\n  A firm seeks to analyze a dataset and to release the results. The dataset\ncontains information about individual people, and the firm is subject to some\nregulation that forbids the release of the dataset itself. The regulation also\nimposes conditions on the release of the results. What properties should the\nregulation satisfy? We restrict our attention to regulations tailored to\ncontrolling the downstream effects of the release specifically on the\nindividuals to whom the data relate. A particular example of interest is an\nanonymization rule, where a data protection regulation limiting the disclosure\nof personally identifiable information does not restrict the distribution of\ndata that has been sufficiently anonymized.\n  In this paper, we develop a set of technical requirements for anonymization\nrules and related regulations. The requirements are derived by situating within\na simple abstract model of data processing a set of guiding general principles\nput forth in prior work. We describe an approach to evaluating such regulations\nusing these requirements -- thus enabling the application of the general\nprinciples for the design of mechanisms. As an exemplar, we evaluate competing\ninterpretations of regulatory requirements from the EU's General Data\nProtection Regulation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.01725,regular,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': ""The Drama Machine: Simulating Character Development with LLM Agents\n\n  This paper explores use of multiple large language model (LLM) agents to\nsimulate complex, dynamic characters in dramatic scenarios. We introduce a\ndrama machine framework that coordinates interactions between LLM agents\nplaying different 'Ego' and 'Superego' psychological roles. In roleplay\nsimulations, this design allows intersubjective dialogue and intra-subjective\ninternal monologue to develop in parallel. We apply this framework to two\ndramatic scenarios - an interview and a detective story - and compare character\ndevelopment with and without the Superego's influence. Though exploratory,\nresults suggest this multi-agent approach can produce more nuanced, adaptive\nnarratives that evolve over a sequence of dialogical turns. We discuss\ndifferent modalities of LLM-based roleplay and character development, along\nwith what this might mean for conceptualization of AI subjectivity. The paper\nconcludes by considering how this approach opens possibilities for thinking of\nthe roles of internal conflict and social performativity in AI-based\nsimulation.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.172325134277344e-07, 'GPT4': 3.6954879760742188e-06, 'CLAUDE': 1.0, 'GOOGLE': 5.960464477539063e-08, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 7.152557373046875e-07, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 2.562999725341797e-06}}"
2408.03154,regular,post_llm,2024,8,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Fake News Detection via Wisdom of Synthetic & Representative Crowds\n\n  Social media companies have struggled to provide a democratically legitimate\ndefinition of ""Fake News"". Reliance on expert judgment has attracted criticism\ndue to a general trust deficit and political polarisation. Approaches reliant\non the ``wisdom of the crowds\'\' are a cost-effective, transparent and inclusive\nalternative. This paper provides a novel end-to-end methodology to detect fake\nnews on X via ""wisdom of the synthetic & representative crowds"". We deploy an\nonline survey on the Lucid platform to gather veracity assessments for a number\nof pandemic-related tweets from crowd-workers. Borrowing from the MrP\nliterature, we train a Hierarchical Bayesian model to predict the veracity of\neach tweet from the perspective of different personae from the population of\ninterest.\n  We then weight the predicted veracity assessments according to a\nrepresentative stratification frame, such that decisions about ``fake\'\' tweets\nare representative of the overall polity of interest. Based on these aggregated\nscores, we analyse a corpus of tweets and perform a second MrP to generate\nstate-level estimates of the number of people who share fake news. We find\nsmall but statistically meaningful heterogeneity in fake news sharing across US\nstates. At the individual-level: i. sharing fake news is generally rare, with\nan average sharing probability interval [0.07,0.14]; ii. strong evidence that\nDemocrats share less fake news, accounting for a reduction in the sharing odds\nof [57.3%,3.9%] relative to the average user; iii. when Republican definitions\nof fake news are used, it is the latter who show a decrease in the propensity\nto share fake news worth [50.8%, 2.0%]; iv. some evidence that women share less\nfake news than men, an effect worth a [29.5%,4.9%] decrease.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.17302,review,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': 'Human Rights for the Digital Age\n\n  The emergence of digital technology has fundamentally transformed all facets\nof human existence, posing important queries about the safeguarding and\nimplementation of human rights in the digital domain. The research focuses on\nimportant topics including privacy, freedom of speech, and information access.\nThe methodology involves an extensive review of existing literature, legal\nframeworks, and relevant case studies to provide a comprehensive understanding\nof the intersection between technology and human rights. The paper highlights\nthe challenges posed by surveillance, data breaches, and the digital divide\nwhile also exploring the role of international law and policy in safeguarding\ndigital rights. The review highlights the significance of modifying human\nrights frameworks for the digital era, pointing out gaps in existing research\nand offering recommendations for future investigations.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.01548004150390625, 'GPT4': 0.1663818359375, 'CLAUDE': 0.01026153564453125, 'GOOGLE': 0.7509765625, 'OPENAI_O_SERIES': 0.005588531494140625, 'DEEPSEEK': 0.0004096031188964844, 'GROK': 6.872415542602539e-05, 'NOVA': 0.01335906982421875, 'OTHER': 0.037353515625, 'HUMAN': 6.383657455444336e-05}}"
2408.08846,review,post_llm,2024,8,"{'ai_likelihood': 3.311369154188368e-07, 'text': ""When Trust is Zero Sum: Automation Threat to Epistemic Agency\n\n  AI researchers and ethicists have long worried about the threat that\nautomation poses to human dignity, autonomy, and to the sense of personal value\nthat is tied to work. Typically, proposed solutions to this problem focus on\nways in which we can reduce the number of job losses which result from\nautomation, ways to retrain those that lose their jobs, or ways to mitigate the\nsocial consequences of those job losses. However, even in cases where workers\nkeep their jobs, their agency within them might be severely downgraded. For\ninstance, human employees might work alongside AI but not be allowed to make\ndecisions or not be allowed to make decisions without consulting with or coming\nto agreement with the AI. This is a kind of epistemic harm (which could be an\ninjustice if it is distributed on the basis of identity prejudice). It\ndiminishes human agency (in constraining people's ability to act\nindependently), and it fails to recognize the workers' epistemic agency as\nqualified experts. Workers, in this case, aren't given the trust they are\nentitled to. This means that issues of human dignity remain even in cases where\neveryone keeps their job. Further, job retention focused solutions, such as\ndesigning an algorithm to work alongside the human employee, may only enable\nthese harms. Here, we propose an alternative design solution, adversarial\ncollaboration, which addresses the traditional retention problem of automation,\nbut also addresses the larger underlying problem of epistemic harms and the\ndistribution of trust between AI and humans in the workplace.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.08778,other,post_llm,2024,8,"{'ai_likelihood': 5.8975484636094836e-05, 'text': 'Watching the Generative AI Hype Bubble Deflate\n\n  Only a few short months ago, Generative AI was sold to us as inevitable by\nthe leadership of AI companies, those who partnered with them, and venture\ncapitalists. As certain elements of the media promoted and amplified these\nclaims, public discourse online buzzed with what each new beta release could be\nmade to do with a few simple prompts. As AI became a viral sensation, every\nbusiness tried to become an AI business. Some businesses added ""AI"" to their\nnames to juice their stock prices, and companies talking about ""AI"" on their\nearnings calls saw similar increases. While the Generative AI hype bubble is\nnow slowly deflating, its harmful effects will last.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.02117,review,post_llm,2024,8,"{'ai_likelihood': 0.2848307291666667, 'text': 'UFO, universe, reptilians and creatures communities on Brazilian\n  Telegram: when the sky is not the limit and conspiracy theories seek answers\n  beyond humanity\n\n  Interest in extraterrestrial phenomena and conspiracy theories involving UFOs\nand reptilians has been growing on Brazilian Telegram, especially in times of\nglobal uncertainty, such as during the COVID-19 pandemic. Therefore, this study\naims to address the research question: how are Brazilian conspiracy theory\ncommunities on UFO, universe, reptilians and creatures topics characterized and\narticulated on Telegram? It is worth noting that this study is part of a series\nof seven studies whose main objective is to understand and characterize\nBrazilian conspiracy theory communities on Telegram. This series of seven\nstudies is openly and originally available on arXiv at Cornell University,\napplying a mirrored method across the seven studies, changing only the thematic\nobject of analysis and providing investigation replicability, including with\nproprietary and authored codes, adding to the culture of free and open-source\nsoftware. Regarding the main findings of this study, the following were\nobserved: UFO communities act as gateways for theories about reptilians,\nconnecting narratives of global control with extraterrestrial beings;\nDiscussions about UFOs and the universe grew significantly during the Pandemic,\nreflecting a renewed interest in extraterrestrial phenomena; Reptilians remain\na significant subculture within conspiracy theories, with a notable growth\nduring the Pandemic; The thematic overlap between UFOs, reptilians and\nesotericism reveals a cohesive ecosystem of disinformation, making factual\ncorrection a challenge; UFO communities function as amplifiers of other\nconspiracy theories, connecting different themes and strengthening the\ndisinformation network.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.13967,review,post_llm,2024,8,"{'ai_likelihood': 0.002761416965060764, 'text': 'Including Non-Autistic Peers in Games Designed for Autistic\n  Socialization\n\n  Through a review of current game practices, the author highlights concerns\nregarding the safety of public social games and the singular medical approach\nto serious game design for autism. The paper identifies a disconnect between\nthe needs of autistic children and the existing solutions. To fill this gap, a\nneurodiversity approach to serious game design is proposed. This approach aims\nto address the social needs of autistic children, enabling them to interact\nwith their neurotypical peers directly, confidently, and safely.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.11539,regular,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': ""Research on the Application of Large Language Models in Automatic\n  Question Generation: A Case Study of ChatGLM in the Context of High School\n  Information Technology Curriculum\n\n  This study investigates the application effectiveness of the Large Language\nModel (LLMs) ChatGLM in the automated generation of high school information\ntechnology exam questions. Through meticulously designed prompt engineering\nstrategies, the model is guided to generate diverse questions, which are then\ncomprehensively evaluated by domain experts. The evaluation dimensions include\nthe Hitting(the degree of alignment with teaching content), Fitting (the degree\nof embodiment of core competencies), Clarity (the explicitness of question\ndescriptions), and Willing to use (the teacher's willingness to use the\nquestion in teaching). The results indicate that ChatGLM outperforms\nhuman-generated questions in terms of clarity and teachers' willingness to use,\nalthough there is no significant difference in hit rate and fit. This finding\nsuggests that ChatGLM has the potential to enhance the efficiency of question\ngeneration and alleviate the burden on teachers, providing a new perspective\nfor the future development of educational assessment systems. Future research\ncould explore further optimizations to the ChatGLM model to maintain high fit\nand hit rates while improving the clarity of questions and teachers'\nwillingness to use them.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00734710693359375, 'GPT4': 0.1396484375, 'CLAUDE': 0.01047515869140625, 'GOOGLE': 0.476318359375, 'OPENAI_O_SERIES': 0.02484130859375, 'DEEPSEEK': 0.0013408660888671875, 'GROK': 0.00022530555725097656, 'NOVA': 0.0015668869018554688, 'OTHER': 0.338134765625, 'HUMAN': 0.0001761913299560547}}"
2408.13071,regular,post_llm,2024,8,"{'ai_likelihood': 0.00021563635932074653, 'text': ""Guiding IoT-Based Healthcare Alert Systems with Large Language Models\n\n  Healthcare alert systems (HAS) are undergoing rapid evolution, propelled by\nadvancements in artificial intelligence (AI), Internet of Things (IoT)\ntechnologies, and increasing health consciousness. Despite significant\nprogress, a fundamental challenge remains: balancing the accuracy of\npersonalized health alerts with stringent privacy protection in HAS\nenvironments constrained by resources. To address this issue, we introduce a\nuniform framework, LLM-HAS, which incorporates Large Language Models (LLM) into\nHAS to significantly boost the accuracy, ensure user privacy, and enhance\npersonalized health service, while also improving the subjective quality of\nexperience (QoE) for users. Our innovative framework leverages a Mixture of\nExperts (MoE) approach, augmented with LLM, to analyze users' personalized\npreferences and potential health risks from additional textual job\ndescriptions. This analysis guides the selection of specialized Deep\nReinforcement Learning (DDPG) experts, tasked with making precise health\nalerts. Moreover, LLM-HAS can process Conversational User Feedback, which not\nonly allows fine-tuning of DDPG but also deepen user engagement, thereby\nenhancing both the accuracy and personalization of health management\nstrategies. Simulation results validate the effectiveness of the LLM-HAS\nframework, highlighting its potential as a groundbreaking approach for\nemploying generative AI (GAI) to provide highly accurate and reliable alerts.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.0369,review,post_llm,2024,8,"{'ai_likelihood': 1.6225708855523005e-06, 'text': ""Prioritising Response-able IP Practices in Digitization of Electoral\n  Processes in Africa\n\n  Globally, people widely regard technology as a solution to global social\nproblems. In a democratic society, its citizens view technology as a way to\nensure commitment and sustaining the nation's democracy by allowing them to\nparticipate actively in the democratic process. However, despite the hype\nsurrounding technology and development, many developing countries still\nexperience democratic challenges. The democratic challenges have further led to\nbarriers that shape the political landscape, resulting in delusion,\ndisappointment, and failures in the democratic and public good processes, such\nas the electoral process. This paper explores the relationship between\nintellectual property (IP) practices and the adoption of digital technologies\nused in democratic electoral processes. Specifically, it examines how the\nprioritisation of IP by technology service providers can disrupt socio-material\nrelationships in democratic electoral processes and outcomes. Because of the\nhard boundaries associated with IP it creates an environment where the systems\nare controlled solely by technology IP owners, while the consequences of\nelectoral processes are borne by citizens. This questions the response-ability\nand trust-ability of digital technologies in running democratic processes.\nDrawing from the parallels in Kenya's general elections of 2017 and 2022, this\npaper illustrates how IP practices form a hard boundary that impels technology\nowners to micromanage electoral processes, leading to tensions that potentially\ncreate conflict. This finding can be used by decision-makers to adopt digital\ntechnologies and protect IP without compromising electoral processes and\ndisrupting relationships in the wider society.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.00819,review,post_llm,2024,8,"{'ai_likelihood': 0.0020122528076171875, 'text': 'Methods to Estimate Advanced Driver Assistance System Penetration Rates\n  in the United States\n\n  Advanced driver assistance systems (ADAS) are increasingly prevalent in the\nvehicle fleet, significantly impacting safety and capacity. Transportation\nagencies struggle to plan for these effects as ADAS availability is not tracked\nin vehicle registration databases. This paper examines methods to leverage\nexisting public reports and databases to estimate the proportion of vehicles\nequipped with or utilizing Levels 1 and 2 ADAS technologies in the United\nStates. Findings indicate that in 2022, between 8% and 25% of vehicles were\nequipped with various ADAS features, though actual usage rates were lower due\nto driver deactivation. The study proposes strategies to enhance estimates,\nincluding analyzing crash data, expanding event data recorder capabilities,\nconducting naturalistic driving studies, and collaborating with manufacturers\nto determine installation rates.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.0747,review,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': 'Navigating Design Science Research in mHealth Applications: A Guide to\n  Best Practices\n\n  The rapid proliferation of mobile devices and advancements in wireless\ntechnologies have given rise to a new era of healthcare delivery through mobile\nhealth (mHealth) applications. Design Science Research (DSR) is a widely used\nresearch paradigm that aims to create and evaluate innovative artifacts to\nsolve real-world problems. This paper presents a comprehensive framework for\nemploying DSR in mHealth application projects to address healthcare challenges\nand improve patient outcomes. We discussed various DSR principles and\nmethodologies, highlighting their applicability and importance in developing\nand evaluating mHealth applications. Furthermore, we present several case\nstudies to exemplify the successful implementation of DSR in mHealth projects\nand provide practical recommendations for researchers and practitioners.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.08795166015625, 'GPT4': 0.042938232421875, 'CLAUDE': 0.0002617835998535156, 'GOOGLE': 0.86767578125, 'OPENAI_O_SERIES': 4.786252975463867e-05, 'DEEPSEEK': 1.0728836059570312e-06, 'GROK': 1.1324882507324219e-06, 'NOVA': 3.6954879760742188e-06, 'OTHER': 0.0013065338134765625, 'HUMAN': 9.775161743164062e-06}}"
2408.15383,review,post_llm,2024,8,"{'ai_likelihood': 5.582968393961589e-05, 'text': 'An evidence-based and critical analysis of the Fediverse\n  decentralization promises\n\n  This paper examines the potential of the Fediverse, a federated network of\nsocial media and content platforms, to counter the centralization and dominance\nof commercial platforms on the social Web. We gather evidence from the\ntechnology powering the Fediverse (especially the ActivityPub protocol),\ncurrent statistical data regarding Fediverse user distribution over instances,\nand the status of two older, similar, decentralized technologies: e-mail and\nthe Web. Our findings suggest that Fediverse will face significant challenges\nin fulfilling its decentralization promises, potentially hindering its ability\nto positively impact the social Web on a large scale.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.08126,review,post_llm,2024,8,"{'ai_likelihood': 0.98779296875, 'text': 'Decoding Memes: A Comparative Study of Machine Learning Models for\n  Template Identification\n\n  Image-with-text memes combine text with imagery to achieve comedy, but in\ntoday\'s world, they also play a pivotal role in online communication,\ninfluencing politics, marketing, and social norms. A ""meme template"" is a\npreexisting layout or format that is used to create memes. It typically\nincludes specific visual elements, characters, or scenes with blank spaces or\ncaptions that can be customized, allowing users to easily create their versions\nof popular meme templates by adding personal or contextually relevant content.\nDespite extensive research on meme virality, the task of automatically\nidentifying meme templates remains a challenge.\n  This paper presents a comprehensive comparison and evaluation of existing\nmeme template identification methods, including both established approaches\nfrom the literature and novel techniques. We introduce a rigorous evaluation\nframework that not only assesses the ability of various methods to correctly\nidentify meme templates but also tests their capacity to reject non-memes\nwithout false assignments. Our study involves extensive data collection from\nsites that provide meme annotations (Imgflip) and various social media\nplatforms (Reddit, X, and Facebook) to ensure a diverse and representative\ndataset. We compare meme template identification methods, highlighting their\nstrengths and limitations. These include supervised and unsupervised\napproaches, such as convolutional neural networks, distance-based\nclassification, and density-based clustering. Our analysis helps researchers\nand practitioners choose suitable methods and points to future research\ndirections in this evolving field.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0021877288818359375, 'GPT4': 0.386962890625, 'CLAUDE': 0.00033164024353027344, 'GOOGLE': 0.60400390625, 'OPENAI_O_SERIES': 0.00342559814453125, 'DEEPSEEK': 1.895427703857422e-05, 'GROK': 3.5762786865234375e-06, 'NOVA': 2.1755695343017578e-05, 'OTHER': 0.00044608116149902344, 'HUMAN': 0.0023193359375}}"
2409.06717,review,post_llm,2024,8,"{'ai_likelihood': 1.74509154425727e-05, 'text': 'Tailoring Chatbots for Higher Education: Some Insights and Experiences\n\nThe general availability of general-purpose Large Language Models continues to impact on higher education, yet they may not always be useful for specialized tasks. When using these models, oftentimes the need for particular domain knowledge becomes quickly apparent, and the desire for customized bots arises. Customization holds the promise of leading to more accurate and contextually relevant responses, enhancing the educational experience. This report relates insights and experiences from one particular technical university in Switzerland, ETH Zurich, to describe what ""customizing"" Large Language Models means in practical terms for higher education institutions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.01562,regular,post_llm,2024,8,"{'ai_likelihood': 3.311369154188368e-06, 'text': 'Welfare, sustainability, and equity evaluation of the New York City\n  Interborough Express using spatially heterogeneous mode choice models\n\n  The Metropolitan Transit Authority (MTA) proposed building a new light rail\nroute called the Interborough Express (IBX) to provide a direct, fast transit\nlinkage between Queens and Brooklyn. An open-access synthetic citywide trip\nagenda dataset and a block-group-level mode choice model are used to assess the\npotential impact IBX could bring to New York City (NYC). IBX could save 28.1\nminutes to potential riders across the city. For travelers either going to or\ndeparting from areas close to IBX, the average time saving is projected to be\n29.7 minutes. IBX is projected to have more than 254 thousand daily ridership\nafter its completion (69% higher than reported in the official IBX proposal).\nAmong those riders, more than 78 thousand people (30.8%) would come from\nlow-income households while 165 thousand people (64.7%) would start or end\nalong the IBX corridor. The addition of IBX would attract more than 50 thousand\nadditional daily trips to transit mode, among which more than 16 thousand would\nbe switched from using private vehicles, reducing potential greenhouse gas\n(GHG) emissions by 29.28 metric tons per day. IBX can also bring significant\nconsumer surplus benefits to the communities, which are estimated to be $1.25\nUSD per trip, or as high as $1.64 per trip made by a low-income traveler. While\nbenefits are proportionately higher for lower-income users, the service does\nnot appear to significantly reduce the proportion of travelers whose consumer\nsurpluses fall below 10% of the population average (already quite low).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.08477,review,post_llm,2024,8,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Automating Transparency Mechanisms in the Judicial System Using LLMs:\n  Opportunities and Challenges\n\n  Bringing more transparency to the judicial system for the purposes of\nincreasing accountability often demands extensive effort from auditors who must\nmeticulously sift through numerous disorganized legal case files to detect\npatterns of bias and errors. For example, the high-profile investigation into\nthe Curtis Flowers case took seven reporters a full year to assemble evidence\nabout the prosecutor's history of selecting racially biased juries. LLMs have\nthe potential to automate and scale these transparency pipelines, especially\ngiven their demonstrated capabilities to extract information from unstructured\ndocuments. We discuss the opportunities and challenges of using LLMs to provide\ntransparency in two important court processes: jury selection in criminal\ntrials and housing eviction cases.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.09982,regular,post_llm,2024,8,"{'ai_likelihood': 0.9951171875, 'text': ""Application of Large Language Models in Automated Question Generation: A\n  Case Study on ChatGLM's Structured Questions for National Teacher\n  Certification Exams\n\n  This study delves into the application potential of the large language models\n(LLMs) ChatGLM in the automatic generation of structured questions for National\nTeacher Certification Exams (NTCE). Through meticulously designed prompt\nengineering, we guided ChatGLM to generate a series of simulated questions and\nconducted a comprehensive comparison with questions recollected from past\nexaminees. To ensure the objectivity and professionalism of the evaluation, we\ninvited experts in the field of education to assess these questions and their\nscoring criteria. The research results indicate that the questions generated by\nChatGLM exhibit a high level of rationality, scientificity, and practicality\nsimilar to those of the real exam questions across most evaluation criteria,\ndemonstrating the model's accuracy and reliability in question generation.\nNevertheless, the study also reveals limitations in the model's consideration\nof various rating criteria when generating questions, suggesting the need for\nfurther optimization and adjustment. This research not only validates the\napplication potential of ChatGLM in the field of educational assessment but\nalso provides crucial empirical support for the development of more efficient\nand intelligent educational automated generation systems in the future.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0019426345825195312, 'GPT4': 0.88427734375, 'CLAUDE': 0.00970458984375, 'GOOGLE': 0.09051513671875, 'OPENAI_O_SERIES': 0.002685546875, 'DEEPSEEK': 0.00021207332611083984, 'GROK': 1.722574234008789e-05, 'NOVA': 0.00013959407806396484, 'OTHER': 0.007236480712890625, 'HUMAN': 0.003055572509765625}}"
2408.06431,regular,post_llm,2024,8,"{'ai_likelihood': 9.602970547146268e-06, 'text': 'Addressing the Unforeseen Harms of Technology CCC Whitepaper\n\n  Recent years have seen increased awareness of the potential significant\nimpacts of computing technologies, both positive and negative. This whitepaper\nexplores how to address possible harmful consequences of computing technologies\nthat might be difficult to anticipate, and thereby mitigate or address. It\nstarts from the assumption that very few harms due to technology are\nintentional or deliberate; rather, the vast majority result from failure to\nrecognize and respond to them prior to deployment. Nonetheless, there are\nconcrete steps that can be taken to address the difficult problem of\nanticipating and responding to potential harms from new technologies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.17268,review,post_llm,2024,8,"{'ai_likelihood': 1.0, 'text': ""Predicting the Impact of Generative AI Using an Agent-Based Model\n\n  Generative artificial intelligence (AI) systems have transformed various\nindustries by autonomously generating content that mimics human creativity.\nHowever, concerns about their social and economic consequences arise with\nwidespread adoption. This paper employs agent-based modeling (ABM) to explore\nthese implications, predicting the impact of generative AI on societal\nframeworks. The ABM integrates individual, business, and governmental agents to\nsimulate dynamics such as education, skills acquisition, AI adoption, and\nregulatory responses. This study enhances understanding of AI's complex\ninteractions and provides insights for policymaking. The literature review\nunderscores ABM's effectiveness in forecasting AI impacts, revealing AI\nadoption, employment, and regulation trends with potential policy implications.\nFuture research will refine the model, assess long-term implications and\nethical considerations, and deepen understanding of generative AI's societal\neffects.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00040411949157714844, 'GPT4': 0.81982421875, 'CLAUDE': 0.0008053779602050781, 'GOOGLE': 0.0814208984375, 'OPENAI_O_SERIES': 0.09130859375, 'DEEPSEEK': 0.00228118896484375, 'GROK': 4.708766937255859e-06, 'NOVA': 0.0002951622009277344, 'OTHER': 0.0034313201904296875, 'HUMAN': 1.0132789611816406e-05}}"
2408.0088,regular,post_llm,2024,8,"{'ai_likelihood': 7.318125830756294e-06, 'text': ""Annotator in the Loop: A Case Study of In-Depth Rater Engagement to\n  Create a Bridging Benchmark Dataset\n\n  With the growing prevalence of large language models, it is increasingly\ncommon to annotate datasets for machine learning using pools of crowd raters.\nHowever, these raters often work in isolation as individual crowdworkers. In\nthis work, we regard annotation not merely as inexpensive, scalable labor, but\nrather as a nuanced interpretative effort to discern the meaning of what is\nbeing said in a text. We describe a novel, collaborative, and iterative\nannotator-in-the-loop methodology for annotation, resulting in a 'Bridging\nBenchmark Dataset' of comments relevant to bridging divides, annotated from\n11,973 textual posts in the Civil Comments dataset. The methodology differs\nfrom popular anonymous crowd-rating annotation processes due to its use of an\nin-depth, iterative engagement with seven US-based raters to (1)\ncollaboratively refine the definitions of the to-be-annotated concepts and then\n(2) iteratively annotate complex social concepts, with check-in meetings and\ndiscussions. This approach addresses some shortcomings of current anonymous\ncrowd-based annotation work, and we present empirical evidence of the\nperformance of our annotation process in the form of inter-rater reliability.\nOur findings indicate that collaborative engagement with annotators can enhance\nannotation methods, as opposed to relying solely on isolated work conducted\nremotely. We provide an overview of the input texts, attributes, and annotation\nprocess, along with the empirical results and the resulting benchmark dataset,\ncategorized according to the following attributes: Alienation, Compassion,\nReasoning, Curiosity, Moral Outrage, and Respect.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.05869,regular,post_llm,2024,8,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Using Process Mining to Improve Digital Service Delivery\n\n  We present a case study of Process Mining (PM) for personnel security\nscreening in the Canadian government. We consider customer (process time) and\norganizational (cost) perspectives. Furthermore, in contrast to most published\ncase studies, we assess the full process improvement lifecycle:\npre-intervention analyses pointed out initial bottlenecks, and\npost-intervention analyses identified the intervention impact and remaining\nareas for improvement. Using PM techniques, we identified frequent exceptional\nscenarios (e.g., applications requiring amendment), time-intensive loops (e.g.,\nemployees forgetting tasks), and resource allocation issues (e.g., involvement\nof non-security personnel). Subsequent process improvement interventions,\nimplemented using a flexible low-code digital platform, reduced security\nbriefing times from around 7 days to 46 hours, and overall process time from\naround 31 days to 26 days, on average. From a cost perspective, the involvement\nof hiring managers and security screening officers was significantly reduced.\nThese results demonstrate how PM can become part of a broader digital\ntransformation framework to improve public service delivery. The success of\nthese interventions motivated subsequent government PM projects, and inspired a\nPM methodology, currently under development, for use in large organizational\ncontexts such as governments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.11769,regular,post_llm,2024,8,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Decoding Pedestrian Stress on Urban Streets using Electrodermal Activity\n  Monitoring in Virtual Immersive Reality\n\n  The pedestrian stress level is shown to significantly influence human\ncognitive processes and, subsequently, decision-making, e.g., the decision to\nselect a gap and cross a street. This paper systematically studies the stress\nexperienced by a pedestrian when crossing a street under different experimental\nmanipulations by monitoring the ElectroDermal Activity (EDA) using the Galvanic\nSkin Response (GSR) sensor. To fulfil the research objectives, a dynamic and\nimmersive virtual reality (VR) platform was used, which is suitable for\neliciting and capturing pedestrian's emotional responses in conjunction with\nmonitoring their EDA. A total of 171 individuals participated in the\nexperiment, tasked to cross a two-way street at mid-block with no signal\ncontrol. Mixed effects models were employed to compare the influence of\nsocio-demographics, social influence, vehicle technology, environment, road\ndesign, and traffic variables on the stress levels of the participants. The\nresults indicated that having a street median in the middle of the road\noperates as a refuge and significantly reduced stress. Younger participants\nwere (18-24 years) calmer than the relatively older participants (55-65 years).\nArousal levels were higher when it came to the characteristics of the avatar\n(virtual pedestrian) in the simulation, especially for those avatars with\nadventurous traits. The pedestrian location influenced stress since the stress\nwas higher on the street while crossing than waiting on the sidewalk.\nSignificant causes of arousal were fear of accidents and an actual accident for\npedestrians. The estimated random effects show a high degree of physical and\nmental learning by the participants while going through the scenarios.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.17361,regular,post_llm,2024,8,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'GeoAI in resource-constrained environments\n\n  This paper describes spatially aware Artificial Intelligence, GeoAI, tailored\nfor small organizations such as NGOs in resource constrained contexts where\naccess to large datasets, expensive compute infrastructure and AI expertise may\nbe restricted. We furthermore consider future scenarios in which\nresource-intensive, large geospatial models may homogenize the representation\nof complex landscapes, and suggest strategies to prepare for this condition.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.00325,regular,post_llm,2024,8,"{'ai_likelihood': 6.192260318332248e-05, 'text': 'Anti-woke agenda, gender issues, revisionism and hate speech communities\n  on Brazilian Telegram: from harmful reactionary speech to the crime of\n  glorifying Nazism and Hitler\n\n  Resistance to progressive policies and hate speech have been consolidating on\nBrazilian Telegram, with anti-woke communities rejecting diversity and\npromoting a worldview that sees these social changes as a threat. Therefore,\nthis study aims to address the research question: how are Brazilian conspiracy\ntheory communities on anti-woke agenda, gender issues, revisionism and hate\nspeech topics characterized and articulated on Telegram? It is worth noting\nthat this study is part of a series of seven studies whose main objective is to\nunderstand and characterize Brazilian conspiracy theory communities on\nTelegram. This series of seven studies is openly and originally available on\narXiv at Cornell University, applying a mirrored method across the seven\nstudies, changing only the thematic object of analysis and providing\ninvestigation replicability, including with proprietary and authored codes,\nadding to the culture of free and open-source software. Regarding the main\nfindings of this study, the following were observed: Anti-woke communities\nemerge as central forces in the Brazilian conspiracy ecosystem; During crises,\nmentions of hate speech and revisionism have increased significantly,\nreflecting polarization; Nazi communities on Telegram propagate extremist\nideologies, glorifying Hitler; The interconnectivity between anti-woke,\nanti-gender and revisionism strengthens the ecosystem of hate; Anti-gender\nspeech facilitates the spread of anti-vaccine disinformation, creating an\nintersection between health and conspiracy.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.12289,review,post_llm,2024,8,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'Catalog of General Ethical Requirements for AI Certification\n\n  This whitepaper offers normative and practical guidance for developers of\nartificial intelligence (AI) systems to achieve ""Trustworthy AI"". In it, we\npresent overall ethical requirements and six ethical principles with\nvalue-specific recommendations for tools to implement these principles into\ntechnology. Our value-specific recommendations address the principles of\nfairness, privacy and data protection, safety and robustness, sustainability,\ntransparency and explainability and truthfulness. For each principle, we also\npresent examples of criteria for risk assessment and categorization of AI\nsystems and applications in line with the categories of the European Union (EU)\nAI Act. Our work is aimed at stakeholders who can take it as a potential\nblueprint to fulfill minimum ethical requirements for trustworthy AI and AI\nCertification.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.12872,regular,post_llm,2024,8,"{'ai_likelihood': 2.947118547227648e-06, 'text': ""Moral Judgments in Online Discourse are not Biased by Gender\n\nThe interaction between social norms and gender roles prescribes gender-specific behaviors that influence moral judgments. Here, we study how moral judgments are biased by the gender of the protagonist of a story. Using data from r/AITA, a Reddit community with 17 million members who share first-hand experiences seeking community judgment on their behavior, we employ machine learning techniques to match stories describing similar situations that differ only by the protagonist's gender. We find no direct causal effect of the protagonist's gender on the received moral judgments, except for stories about ``friendship and relationships'', where male protagonists receive more negative judgments. Our findings complement existing correlational studies and suggest that gender roles may exert greater influence in specific social contexts. These results have implications for understanding sociological constructs and highlight potential biases in data used to train large language models."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.1054,regular,post_llm,2024,8,"{'ai_likelihood': 1.7748938666449652e-05, 'text': ""Beyond Flashcards: Designing an Intelligent Assistant for USMLE Mastery\n  and Virtual Tutoring in Medical Education (A Study on Harnessing Chatbot\n  Technology for Personalized Step 1 Prep)\n\n  Traditional medical basic sciences educational approaches follow a\none-size-fits-all model, neglecting the diverse learning styles of individual\nstudents. I propose an intelligent AI companion which will fill this gap by\nproviding on-the-fly solutions to students' questions in the context of not\nonly USMLE Step 1 but also other similar examinations in other countries, inter\nalia, PLAB Part 1 in United Kingdom, and NEET (PG) and FMGE in India. I have\nharnessed Generative AI for dynamic, accurate, human-like responses and for\nknowledge retention and application. Users were encouraged to employ prompt\nengineering, in particular, in-context learning, for response optimization and\nenhancing the model's precision in understanding the intent of the user through\nthe way the query is framed. The implementation of RAG has enhanced the\nchatbot's ability to combine pre-existing medical knowledge with generative\ncapabilities for efficient and contextually relevant support. Mistral was\nemployed using Python to perform the needed functions. The digital\nconversational agent was implemented and achieved a score of 0.5985 on a\nreference-based metric similar to BLEU and ROUGE scores. My approach addresses\na critical gap in traditional medical basic sciences education by introducing\nan intelligent AI companion which specializes in helping medical aspirants with\nplanning and information retention for USMLE Step 1 and other similar exams.\nConsidering the stress that medical aspirants face in studying for the exam and\nin obtaining spontaneous answers to medical basic sciences queries, especially\nwhose answers are challenging to obtain by searching online, and obviating a\nstudent's need to search bulky medical texts or lengthy indices or appendices,\nI have been able to create a quality assistant capable of producing ad-libitum\nresponses best suited to the user's needs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.12045,regular,post_llm,2024,8,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'Hell Divers: The Dark Future of Next-Gen Asymmetric Warfighting\n\n  This whitepaper was written in response to the open-to-public writing prompt\nhosted by the US Army Training & Doctrine Command (TRADOC) Mad Scientist\nInitiative. The 2024 Mad Scientist Writing Prompt called for a predictive\ndiscussion or fictional narrative regarding what the next-generation of\nasymmetric warfighting may look like. This follows lessons learned from\nhistorical context, current events or crises, and global uncertainty. The views\nexpressed by this whitepaper are those of the author and do not reflect the\nofficial policy or position of Dakota State University, the N.H. Army National\nGuard, the U.S. Army, the Department of Defense, or the U.S. Government. The\nappearance of hyperlinks for academic, government, or military websites does\nnot constitute any form of endorsement of the same. Whitepaper cleared for\npublic release on 30 APR 2024.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.09184,regular,post_llm,2024,8,"{'ai_likelihood': 1.2351406945122613e-05, 'text': 'Der Weg zur digitalen Arbeitsmappe: Digitales Pr\\""ufungswesen mit\n  Zertifizierung\n\n  The aim of the work is to present an alternative approach to recording and\nevaluating student performance that enables sustainable performance recording\nwith the possibility of integrating practical components in particular. The\nintended result is a digital portfolio with work samples - and not just\ncertificates, which can be understood as a portfolio examination in the context\nof academic assessment. This is more about the recording, evaluation and\ncertification of learning progress and competencies than the selective\nevaluation of a performance review, as is the case today, for example, with the\nsubmission of final theses. The idea is to expand and later replace final\npapers and performance tests, particularly in higher semesters, and instead\nintroduce electronically recorded portfolio examinations - based on the example\nof teaching projects.\n  Technologically, the approach is based on blockchain and wallets/repositories\nand, in the broadest sense, on an implementation of smart contracts. The\ntechnological approach of smart contracts enables a high degree of traceability\nand transparency with little administrative effort. It also offers secure\ncertification of services by the provider. It should be clearly stated that\nneither the portfolio examination nor the administration of academic\nachievements with smart contracts is the original idea, but rather the change\nin the recording of academic achievements towards an alternative approach to\nthe recording and evaluation of student performance, which enables sustainable\nperformance recording with the possibility of integrating practical components\nin particular. The desired result is a digital portfolio with work samples.\n  The primary aim of this idea sketch is to develop an individualized\nperformance record for students, which can also contribute to making\nperformance more transparent and comprehensible.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.08946,review,post_llm,2024,8,"{'ai_likelihood': 0.00012384520636664497, 'text': 'Authorship Attribution in the Era of LLMs: Problems, Methodologies, and\n  Challenges\n\n  Accurate attribution of authorship is crucial for maintaining the integrity\nof digital content, improving forensic investigations, and mitigating the risks\nof misinformation and plagiarism. Addressing the imperative need for proper\nauthorship attribution is essential to uphold the credibility and\naccountability of authentic authorship. The rapid advancements of Large\nLanguage Models (LLMs) have blurred the lines between human and machine\nauthorship, posing significant challenges for traditional methods. We presents\na comprehensive literature review that examines the latest research on\nauthorship attribution in the era of LLMs. This survey systematically explores\nthe landscape of this field by categorizing four representative problems: (1)\nHuman-written Text Attribution; (2) LLM-generated Text Detection; (3)\nLLM-generated Text Attribution; and (4) Human-LLM Co-authored Text Attribution.\nWe also discuss the challenges related to ensuring the generalization and\nexplainability of authorship attribution methods. Generalization requires the\nability to generalize across various domains, while explainability emphasizes\nproviding transparent and understandable insights into the decisions made by\nthese models. By evaluating the strengths and limitations of existing methods\nand benchmarks, we identify key open problems and future research directions in\nthis field. This literature review serves a roadmap for researchers and\npractitioners interested in understanding the state of the art in this rapidly\nevolving field. Additional resources and a curated list of papers are available\nand regularly updated at https://llm-authorship.github.io\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.06689,regular,post_llm,2024,8,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Comparative Analysis of Digital Tools and Traditional Teaching Methods\n  in Educational Effectiveness\n\n  In today's world technology comprises a large aspect of our lives so this\nstudy aimed to investigate if using computers and digital tools are better than\ntraditional methods like using textbooks and worksheets for learning math. This\nstudy was done at Clarksburg Elementary School with help from MoCo Innovation\nwhich is a club that focuses on fostering an interest in technology among\nstudents. A major question that sparked our minds was: Are digital tools like\nlearning on computers better than traditional methods for improving students\nmath skills? We believe students who use digital tools might improve more in\ntheir math skills. To find out we worked with 30 students from the school. We\nsplit them into two groups and gave each group a pre assessment and post\nassessment. One group learned math using computers and were able to use\ninteractive math websites such as Khan Academy while the other group used\nworksheets. After some learning we gave them a post assessment to see how much\nthey had improved. Our results showed that the students who used the digital\ntools improved test scores averages by 24.2 percent from 70 percent to 87\npercent while the students who used traditional methods only improved by 8.3\npercent from 72 percent to 78 percent in math. These results show that digital\ntools are superior to regular teaching methods especially for subjects like\nmath. But more research is required to see if digital tools are the main reason\nfor this improvement. This research is definitely important to help schools\ndecide if they want to use more technology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.12572,regular,post_llm,2024,8,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Contextual Stochastic Optimization for School Desegregation Policymaking\n\n  Most US school districts draw geographic ""attendance zones"" to assign\nchildren to schools based on their home address, a process that can replicate\nexisting neighborhood racial/ethnic and socioeconomic status (SES) segregation\nin schools. Redrawing boundaries can reduce segregation, but estimating\nexpected rezoning impacts is often challenging because families can opt-out of\ntheir assigned schools. This paper seeks to alleviate this societal problem by\ndeveloping a joint redistricting and choice modeling framework, called\nRedistricting with Choices (RWC). The RWC framework is applied to a large US\npublic school district to estimate how redrawing elementary school boundaries\nmight realistically impact levels of socioeconomic segregation. The main\nmethodological contribution of RWC is a contextual stochastic optimization\nmodel that aims to minimize district-wide segregation by integrating rezoning\nconstraints with a machine learning-based school choice model. The study finds\nthat RWC yields boundary changes that might reduce segregation by a substantial\namount (23%) -- but doing so might require the re-assignment of a large number\nof students, likely to mitigate re-segregation that choice patterns could\nexacerbate. The results also reveal that predicting school choice is a\nchallenging machine learning problem. Overall, this study offers a novel\npractical framework that both academics and policymakers might use to foster\nmore diverse and integrated schools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.12041,regular,post_llm,2024,8,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'Golden Eye: The Theory of Havana Syndrome\n\n  Beginning around 2016, US Diplomats reported unusual injuries while serving\nabroad. Personnel suffered from symptoms such as nausea, vertigo, and\ndisorientation. The collective set of ailments was subbed ""Havana Syndrome"".\nThis whitepaper delves into an analysis of competing hypotheses with respect to\npotential origins of these symptoms. Whitepaper cleared for release on 18 JUN\n2024. The views expressed by this whitepaper are those of the author and do not\nreflect the official policy or position of Dakota State University, the N.H.\nArmy National Guard, the U.S. Army, the Department of Defense, or the U.S.\nGovernment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.02565,review,post_llm,2024,8,"{'ai_likelihood': 0.00010708967844645183, 'text': 'Reasons to Doubt the Impact of AI Risk Evaluations\n\n  AI safety practitioners invest considerable resources in AI system\nevaluations, but these investments may be wasted if evaluations fail to realize\ntheir impact. This paper questions the core value proposition of evaluations:\nthat they significantly improve our understanding of AI risks and,\nconsequently, our ability to mitigate those risks. Evaluations may fail to\nimprove understanding in six ways, such as risks manifesting beyond the AI\nsystem or insignificant returns from evaluations compared to real-world\nobservations. Improved understanding may also not lead to better risk\nmitigation in four ways, including challenges in upholding and enforcing\ncommitments. Evaluations could even be harmful, for example, by triggering the\nweaponization of dual-use capabilities or invoking high opportunity costs for\nAI safety. This paper concludes with considerations for improving evaluation\npractices and 12 recommendations for AI labs, external evaluators, regulators,\nand academic researchers to encourage a more strategic and impactful approach\nto AI risk assessment and mitigation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.16402,regular,post_llm,2024,8,"{'ai_likelihood': 3.046459621853299e-06, 'text': ""JINet: easy and secure private data analysis for everyone\n\n  JINet is a web browser-based platform intended to democratise access to\nadvanced clinical and genomic data analysis software. It hosts numerous data\nanalysis applications that are run in the safety of each User's web browser,\nwithout the data ever leaving their machine. JINet promotes collaboration,\nstandardisation and reproducibility by sharing scripts rather than data and\ncreating a self-sustaining community around it in which Users and data analysis\ntools developers interact thanks to JINets interoperability primitives.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.00821,review,post_llm,2024,8,"{'ai_likelihood': 1.655684577094184e-06, 'text': ""An FDA for AI? Pitfalls and Plausibility of Approval Regulation for\n  Frontier Artificial Intelligence\n\n  Observers and practitioners of artificial intelligence (AI) have proposed an\nFDA-style licensing regime for the most advanced AI models, or 'frontier'\nmodels. In this paper, we explore the applicability of approval regulation --\nthat is, regulation of a product that combines experimental minima with\ngovernment licensure conditioned partially or fully upon that experimentation\n-- to the regulation of frontier AI. There are a number of reasons to believe\nthat approval regulation, simplistically applied, would be inapposite for\nfrontier AI risks. Domains of weak fit include the difficulty of defining the\nregulated product, the presence of Knightian uncertainty or deep ambiguity\nabout harms from AI, the potentially transmissible nature of risks, and\ndistributed activities among actors involved in the AI lifecycle. We conclude\nby highlighting the role of policy learning and experimentation in regulatory\ndevelopment, describing how learning from other forms of AI regulation and\nimprovements in evaluation and testing methods can help to overcome some of the\nchallenges we identify.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2408.15696,review,post_llm,2024,8,"{'ai_likelihood': 0.00014450814988878038, 'text': 'Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: an investigation of Baidu, Ernie and Qwen\n\n  Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.15294,regular,post_llm,2024,9,"{'ai_likelihood': 0.0017070770263671875, 'text': 'Enhancing MBSE Education with Version Control and Automated Feedback\n\n  This paper presents an innovative approach to conducting a Model-Based\nSystems Engineering (MBSE) course, engaging over 80 participants annually. The\ncourse is structured around collaborative group assignments, where students\nutilize Enterprise Architect to complete complex systems engineering tasks\nacross six submissions. This year, we introduced several technological\nadvancements to enhance the learning experience, including the use of\nLemonTree, SmartGit, and GitHub. Students collaborated on shared repositories\nin GitHub, received continuous feedback via automated checks through LemonTree\nAutomation, and documented their progress with pre-rendered, continuously\nupdating diagrams. Additionally, they managed 2-way and 3-way merges directly\nin SmartGit, with merge issues, updates, and model statistics readily available\nfor each Work-in-Progress submission. The process of correcting and providing\nmanual feedback was streamlined, thanks to accessible changelogs and renders in\nGitHub. An end-of-course feedback form revealed high student satisfaction.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.14055,review,post_llm,2024,9,"{'ai_likelihood': 1.158979203965929e-06, 'text': 'Monitoring Human Dependence On AI Systems With Reliance Drills\n\n  AI systems are assisting humans with increasingly diverse intellectual tasks\nbut are still prone to mistakes. Humans are over-reliant on this assistance if\nthey trust AI-generated advice, even though they would make a better decision\non their own. To identify such instances of over-reliance, this paper proposes\nthe reliance drill: an exercise that tests whether a human can recognise\nmistakes in AI-generated advice. Our paper examines the reasons why an\norganisation might choose to implement reliance drills and the doubts they may\nhave about doing so. As an example, we consider the benefits and risks that\ncould arise when using these drills to detect over-reliance on AI in healthcare\nprofessionals. We conclude by arguing that reliance drills should become a\nstandard risk management practice for ensuring humans remain appropriately\ninvolved in the oversight of AI-assisted decisions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.12967,regular,post_llm,2024,9,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""How Consistent Are Humans When Grading Programming Assignments?\n\nProviding consistent summative assessment to students is important, as the grades they are awarded affect their progression through university and future career prospects. While small cohorts are typically assessed by a single assessor, such as the module/class leader, larger cohorts are often assessed by multiple assessors, typically teaching assistants, which increases the risk of inconsistent grading.\n  To investigate the consistency of human grading of programming assignments, we asked 28 participants to each grade 40 CS1 introductory Java assignments, providing grades and feedback for correctness, code elegance, readability and documentation; the 40 assignments were split into two batches of 20. The 28 participants were divided into seven groups of four (where each group graded the same 40 assignments) to allow us to investigate the consistency of a group of assessors. In the second batch of 20, we duplicated one assignment from the first to analyse the internal consistency of individual assessors.\n  Our results show that human graders in our study can not agree on the grade to give a piece of student work and are often individually inconsistent, suggesting that the idea of a ``gold standard'' of human grading might be flawed. This highlights that a shared rubric alone is not enough to ensure consistency, and other aspects such as assessor training and alternative grading practices should be explored to improve the consistency of human grading further when grading programming assignments."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.11627,regular,post_llm,2024,9,"{'ai_likelihood': 2.682209014892578e-06, 'text': ""Idiosyncratic properties of Australian STV election counting\n\n  Single Transferable Vote (STV) counting, used in several jurisdictions in\nAustralia, is a system for choosing multiple election winners given voters'\npreferences over candidates. There are a variety of different versions of STV\nlegislated and/or applied across Australia. This paper shows some of the\nunintuitive properties of some of these systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.02782,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': ""High School Summer Camps Help Democratize Coding, Data Science, and Deep\n  Learning\n\n  This study documents the impact of a summer camp series that introduces high\nschool students to coding, data science, and deep learning. Hosted on-campus,\nthe camps provide an immersive university experience, fostering technical\nskills, collaboration, and inspiration through interactions with mentors and\nfaculty. Campers' experiences are documented through interviews and pre- and\npost-camp surveys. Key lessons include the importance of personalized feedback,\ndiverse mentorship, and structured collaboration. Survey data reveals increased\nconfidence in coding, with 68.6\\% expressing interest in AI and data science\ncareers. The camps also play a crucial role in addressing disparities in STEM\neducation for underrepresented minorities. These findings underscore the value\nof such initiatives in shaping future technology education and promoting\ndiversity in STEM fields.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0009083747863769531, 'GPT4': 0.01189422607421875, 'CLAUDE': 0.002132415771484375, 'GOOGLE': 0.9052734375, 'OPENAI_O_SERIES': 0.0003705024719238281, 'DEEPSEEK': 0.01311492919921875, 'GROK': 0.0009407997131347656, 'NOVA': 0.002841949462890625, 'OTHER': 0.0625, 'HUMAN': 0.00011086463928222656}}"
2410.128,regular,post_llm,2024,9,"{'ai_likelihood': 8.212195502387153e-05, 'text': 'Reproducibility Needs Reshape Scientific Data Governance\n\n  Scientific data governance should prioritize maximizing the utility of data\nthroughout the research lifecycle. Research software systems that enable\nanalysis reproducibility inform data governance policies and assist\nadministrators in setting clear guidelines for data reuse, data retention, and\nthe management of scientific computing needs. Proactive analysis\nreproducibility and data governance are integral and interconnected components\nof research lifecycle management.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.10553,review,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': '""Flipped"" University: LLM-Assisted Lifelong Learning Environment\n\n  The rapid development of artificial intelligence technologies, particularly\nLarge Language Models (LLMs), has revolutionized the landscape of lifelong\nlearning. This paper introduces a conceptual framework for a self-constructed\nlifelong learning environment supported by LLMs. It highlights the inadequacies\nof traditional education systems in keeping pace with the rapid deactualization\nof knowledge and skills. The proposed framework emphasizes the transformation\nfrom institutionalized education to personalized, self-driven learning. It\nleverages the natural language capabilities of LLMs to provide dynamic and\nadaptive learning experiences, facilitating the creation of personal\nintellectual agents that assist in knowledge acquisition. The framework\nintegrates principles of lifelong learning, including the necessity of building\npersonal world models, the dual modes of learning (training and exploration),\nand the creation of reusable learning artifacts. Additionally, it underscores\nthe importance of curiosity-driven learning and reflective practices in\nmaintaining an effective learning trajectory. The paper envisions the evolution\nof educational institutions into ""flipped"" universities, focusing on supporting\nglobal knowledge consistency rather than merely structuring and transmitting\nknowledge.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.001407623291015625, 'GPT4': 0.221923828125, 'CLAUDE': 0.061981201171875, 'GOOGLE': 0.468994140625, 'OPENAI_O_SERIES': 0.18896484375, 'DEEPSEEK': 0.007534027099609375, 'GROK': 0.0213470458984375, 'NOVA': 0.0238037109375, 'OTHER': 0.00406646728515625, 'HUMAN': 3.2842159271240234e-05}}"
2409.10697,regular,post_llm,2024,9,"{'ai_likelihood': 5.9604644775390625e-06, 'text': ""LLMs as information warriors? Auditing how LLM-powered chatbots tackle\n  disinformation about Russia's war in Ukraine\n\n  The rise of large language models (LLMs) has a significant impact on\ninformation warfare. By facilitating the production of content related to\ndisinformation and propaganda campaigns, LLMs can amplify different types of\ninformation operations and mislead online users. In our study, we empirically\ninvestigate how LLM-powered chatbots, developed by Google, Microsoft, and\nPerplexity, handle disinformation about Russia's war in Ukraine and whether the\nchatbots' ability to provide accurate information on the topic varies across\nlanguages and over time. Our findings indicate that while for some chatbots\n(Perplexity), there is a significant improvement in performance over time in\nseveral languages, for others (Gemini), the performance improves only in\nEnglish but deteriorates in low-resource languages.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.03309,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': ""Innovation in Education: Developing and Assessing Gamification in the\n  University of the Philippines Open University Massive Open Online Courses\n\n  The University of the Philippines Open University has been at the forefront\nof providing Massive Open Online Courses to address knowledge and skill gaps,\naiming to make education accessible and contributing to societal goals.\nRecognising challenges in student engagement and completion rates within\nMassive Open Online Courses, the authors conducted a study by incorporating\ngamification into one of the University of the Philippines Open University's\nMassive Open Online Courses to assess its impact on these aspects. Gamification\ninvolves integrating game elements to motivate and engage users. This study\nexplored the incorporation of Moodle elements such as badges, leaderboards, and\nprogress bars. Using Moodle analytics, the study also tracked student\nengagement, views, and posts throughout the course, offering valuable insights\ninto the influence of gamification on user behaviour. Furthermore, the study\ndelved into participant feedback gathered through post-evaluation surveys,\nproviding a comprehensive understanding of their experiences with the gamified\ncourse design. With a 28.86% completion rate and positive participant\nreception, the study concluded that gamification can enhance learner\nmotivation, participation, and overall satisfaction. This research contributes\nto the ongoing discourse on innovative educational methods, positioning\ngamification as a promising avenue for creating interactive and impactful\nonline learning experiences in the Philippines and beyond.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.008453369140625, 'GPT4': 0.44287109375, 'CLAUDE': 0.00476837158203125, 'GOOGLE': 0.47998046875, 'OPENAI_O_SERIES': 0.061431884765625, 'DEEPSEEK': 0.0004291534423828125, 'GROK': 1.4543533325195312e-05, 'NOVA': 0.00010037422180175781, 'OTHER': 0.0015516281127929688, 'HUMAN': 0.0003368854522705078}}"
2409.00701,review,post_llm,2024,9,"{'ai_likelihood': 5.132622188991971e-06, 'text': 'A Novel Taxonomy for Navigating and Classifying Synthetic Data in\n  Healthcare Applications\n\n  Data-driven technologies have improved the efficiency, reliability and\neffectiveness of healthcare services, but come with an increasing demand for\ndata, which is challenging due to privacy-related constraints on sharing data\nin healthcare contexts. Synthetic data has recently gained popularity as\npotential solution, but in the flurry of current research it can be hard to\noversee its potential. This paper proposes a novel taxonomy of synthetic data\nin healthcare to navigate the landscape in terms of three main varieties. Data\nProportion comprises different ratios of synthetic data in a dataset and\nassociated pros and cons. Data Modality refers to the different data formats\namenable to synthesis and format-specific challenges. Data Transformation\nconcerns improving specific aspects of a dataset like its utility or privacy\nwith synthetic data. Our taxonomy aims to help researchers in the healthcare\ndomain interested in synthetic data to grasp what types of datasets, data\nmodalities, and transformations are possible with synthetic data, and where the\nchallenges and overlaps between the varieties lie.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.12976,review,post_llm,2024,9,"{'ai_likelihood': 1.7219119601779514e-06, 'text': ""Excel: Automated Ledger or Analytics IDE?\n\n  Since the inception of VisiCalc over four decades ago, spreadsheets have\nundergone a gradual transformation, evolving from simple ledger automation\ntools to the current state of Excel, which can be described as an Integrated\nDevelopment Environment (IDE) for analytics. The slow evolution of Excel from\nan automation tool for ledgers to an IDE for analytics explains why many people\nhave not noticed that Excel includes a fully functional database, an OLAP\nEngine, multiple statistical programming languages, multiple third-party\nsoftware libraries, dynamic charts, and real time data connectors. The\nsimplicity of accessing these multiple tools is a low-code framework controlled\nfrom the Excel tool that is effectively an IDE. Once we acknowledge Excel's\nshift from a desk top application to an IDE for analytics, the importance of\nestablishing a comprehensive risk framework for managing this distinctive\ndevelopment environment becomes clear. In this paper we will explain how the\ncurrent risk framework for spreadsheets needs to be expanded to manage the\ngrowing risks of using Excel as an IDE for analytics.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.07132,review,post_llm,2024,9,"{'ai_likelihood': 4.5299530029296875e-05, 'text': ""Evaluation of waterway lock service quality in Yangtze Delta: from the\n  perspectives of customer and supplier\n\n  In recent decades, the waterway locks in the Yangtze Delta, China, have\nbecome major traffic bottlenecks. To gain a comprehensive understanding of the\ncrew's perspectives and primary concerns regarding lock services during vessel\nlockage, and to enhance customer satisfaction and improve vessel lockage\nefficiency, it is necessary to assess the waterway lock service quality (WLSQ).\nThis paper presents an evaluation system for WLSQ from various stakeholders'\nviewpoints. Firstly, by employing questionnaire surveys and the structural\nequation model method, in conjunction with factor analysis, the WLSQ and its\ninfluencing factors in the Yangtze River Delta region are analyzed from a\ncustomer perspective. Secondly, the Analytic Hierarchy Process method is\nutilized, along with a dedicated questionnaire for service suppliers, to\nexamine their concerns regarding the performance of vessel lock services. The\nfindings indicate that there exists a cognitive bias towards factors\ninfluencing the WLSQ. Crew members express the greatest concern over vessel\nlockage delays, whereas vessel lockage safety is the primary concern for\nmanagement department administrators. Furthermore, enhancing the supporting\nfacilities of waterway locks can significantly increase crew members'\nsatisfaction during vessel lockage. Improving staff skills, and safety\nconditions can also greatly enhance customers' tolerance for lockage delays.\nThe results of this study will provide valuable insights for the lock\nmanagement department, operators, and the government in formulating relevant\npolicies to improve WLSQ and implementing ongoing service quality evaluations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.17814,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': ""Impact of Shared E-scooter Introduction on Public Transport Demand: A Case Study in Santiago, Chile\n\nThis study examines how the introduction of shared electric scooters (e-scooters) affects public transport demand in Santiago, Chile, analyzing whether they complement or substitute for existing transit services. We used smart card data from the integrated public transport system of Santiago and GPS traces from e-scooter trips during the initial deployment period. We employed a difference-in-differences approach with negative binomial regression models across three urban regions identified through k-means clustering: Central, Intermediate, and Peripheral. Results reveal spatially heterogeneous effects on public transport boardings and alightings. In the Central Region, e-scooter introduction was associated with significant substitution effects, showing a 23.87% reduction in combined bus and metro boardings, suggesting e-scooters replace short public transport trips in high-density areas. The Intermediate Region showed strong complementary effects, with a 33.6% increase in public transport boardings and 4.08% increase in alightings, indicating e-scooters successfully serve as first/last-mile connectors that enhance transit accessibility. The Peripheral Region exhibited no significant effects. Metro services experienced stronger impacts than bus services, with metro boardings increasing 9.77\\% in the Intermediate Region. Our findings advance understanding of micromobility-transit interactions by demonstrating that both substitution and complementarity can coexist within the same urban system, depending on local accessibility conditions. These results highlight the need for spatially differentiated mobility policies that recognize e-scooters' variable roles across urban environments."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.364418029785156e-07, 'GPT4': 0.00023424625396728516, 'CLAUDE': 0.99609375, 'GOOGLE': 1.4901161193847656e-05, 'OPENAI_O_SERIES': 2.8908252716064453e-05, 'DEEPSEEK': 0.0034694671630859375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 8.940696716308594e-07}}"
2409.12138,regular,post_llm,2024,9,"{'ai_likelihood': 0.00015245543585883247, 'text': 'Reporting Non-Consensual Intimate Media: An Audit Study of Deepfakes\n\n  Non-consensual intimate media (NCIM) inflicts significant harm. Currently,\nvictim-survivors can use two mechanisms to report NCIM - as a non-consensual\nnudity violation or as copyright infringement. We conducted an audit study of\ntakedown speed of NCIM reported to X (formerly Twitter) of both mechanisms. We\nuploaded 50 AI-generated nude images and reported half under X\'s\n""non-consensual nudity"" reporting mechanism and half under its ""copyright\ninfringement"" mechanism. The copyright condition resulted in successful image\nremoval within 25 hours for all images (100% removal rate), while\nnon-consensual nudity reports resulted in no image removal for over three weeks\n(0% removal rate). We stress the need for targeted legislation to regulate NCIM\nremoval online. We also discuss ethical considerations for auditing NCIM on\nsocial platforms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.11845,review,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Law-based and standards-oriented approach for privacy impact assessment\n  in medical devices: a topic for lawyers, engineers and healthcare\n  practitioners in MedTech\n\n  Background: The integration of the General Data Protection Regulation (GDPR)\nand the Medical Device Regulation (MDR) creates complexities in conducting Data\nProtection Impact Assessments (DPIAs) for medical devices. The adoption of\nnon-binding standards like ISO and IEC can harmonize these processes by\nenhancing accountability and privacy by design. Methods: This study employs a\nmultidisciplinary literature review, focusing on GDPR and MDR intersection in\nmedical devices that process personal health data. It evaluates key standards,\nincluding ISO/IEC 29134 and IEC 62304, to propose a unified approach for DPIAs\nthat aligns with legal and technical frameworks. Results: The analysis reveals\nthe benefits of integrating ISO/IEC standards into DPIAs, which provide\ndetailed guidance on implementing privacy by design, risk assessment, and\nmitigation strategies specific to medical devices. The proposed framework\nensures that DPIAs are living documents, continuously updated to adapt to\nevolving data protection challenges. Conclusions: A unified approach combining\nEuropean Union (EU) regulations and international standards offers a robust\nframework for conducting DPIAs in medical devices. This integration balances\nsecurity, innovation, and privacy, enhancing compliance and fostering trust in\nmedical technologies. The study advocates for leveraging both hard law and\nstandards to systematically address privacy and safety in the design and\noperation of medical devices, thereby raising the maturity of the MedTech\necosystem.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.876489639282227e-05, 'GPT4': 0.9013671875, 'CLAUDE': 8.869171142578125e-05, 'GOOGLE': 0.009368896484375, 'OPENAI_O_SERIES': 0.08740234375, 'DEEPSEEK': 0.0012912750244140625, 'GROK': 6.99162483215332e-05, 'NOVA': 3.635883331298828e-06, 'OTHER': 9.298324584960938e-05, 'HUMAN': 7.092952728271484e-06}}"
2410.1084,regular,post_llm,2024,9,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'A discrete event simulator for policy evaluation in liver allocation in\n  Eurotransplant\n\n  We present the ELAS simulator, a discrete event simulator built for the\nEurotransplant (ET) Liver Allocation System (ELAS). Eurotransplant uses ELAS to\nallocate deceased donor livers in eight European countries. The simulator is\nmade publicly available to be transparent on which model Eurotransplant uses to\nevaluate liver allocation policies, and to facilitate collaborations with\npolicymakers, scientists and other stakeholders in evaluating alternative liver\nallocation policies. This paper describes the design and modules of the ELAS\nsimulator. One of the included modules is the obligation module, which is\ninstrumental in ensuring that international cooperation in liver allocation\nbenefits all ET member countries.\n  By default, the ELAS simulator simulates liver allocation according to the\nactual ET allocation rules. Stochastic processes, such as graft offer\nacceptance behavior and listing for a repeat transplantation, are approximated\nwith statistical models which were calibrated to data from the ET registry. We\nvalidate the ELAS simulator by comparing simulated waitlist outcomes to\nhistorically observed waitlist outcomes between 2016 and 2019.\n  The modular design of the ELAS simulator gives end users maximal control over\nthe rules and assumptions under which ET liver allocation is simulated, which\nmakes the simulator useful for policy evaluation. We illustrate this with two\nclinically motivated case studies, for which we collaborated with hepatologists\nand transplantation surgeons from two liver advisory committees affiliated with\nEurotransplant.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.01814,regular,post_llm,2024,9,"{'ai_likelihood': 1.74509154425727e-05, 'text': ""A Graph Theoretic Approach to Analyze the Developing Metaverse\n\n  Despite staggering growth over the past couple of decades, the concept of the\nmetaverse is still in its early stages. Eventually, it is expected to become a\ncommon medium connecting every individual. Considering the complexity of this\nplausible scenario at hand, there's a need to define an advanced metaverse -- a\nmetaverse in which, at every point in space and time, two distinct paradigms\nexist: that of the user in the physical world and that of its real-time digital\nreplica in the virtual one, that can engage seamlessly with each other. The\ndeveloping metaverse can be thus defined as the transitional period from the\ncurrent state to, possibly, the advanced metaverse. This paper seeks to model,\nfrom a graphical standpoint, some of the structures in the current metaverse\nand ones that might be key to the developing and advanced metaverses under one\numbrella, unlike existing approaches that treat different aspects of the\nmetaverse in isolation. This integration allows for the accurate representation\nof cross-domain interactions, leading to optimized resource allocation,\nenhanced user engagement, and improved content distribution. This work\ndemonstrates the usefulness of such an approach in capturing these\ncorrelations, providing a powerful tool for the analysis and future development\nof the metaverse.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.07611,regular,post_llm,2024,9,"{'ai_likelihood': 1.0364585452609592e-05, 'text': ""Detection and Classification of Twitter Users' Opinions on Drought\n  Crises in Iran Using Machine Learning Techniques\n\n  The main objective of this research is to identify and classify the opinions\nof Persian-speaking Twitter users related to drought crises in Iran and\nsubsequently develop a model for detecting these opinions on the platform. To\nachieve this, a model has been developed using machine learning and text mining\nmethods to detect the opinions of Persian-speaking Twitter users regarding the\ndrought issues in Iran. The statistical population for the research included\n42,028 drought-related tweets posted over a one-year period. These tweets were\nextracted from Twitter using keywords related to the drought crises in Iran.\nSubsequently, a sample of 2,300 tweets was qualitatively analyzed, labeled,\ncategorized, and examined. Next, a four-category classification of users`\nopinions regarding drought crises and Iranians' resilience to these crises was\nidentified. Based on these four categories, a machine learning model based on\nlogistic regression was trained to predict and detect various opinions in\nTwitter posts. The developed model exhibits an accuracy of 66.09% and an\nF-score of 60%, indicating that this model has good performance for detecting\nIranian Twitter users' opinions regarding drought crises. The ability to detect\nopinions regarding drought crises on platforms like Twitter using machine\nlearning methods can intelligently represent the resilience level of the\nIranian society in the face of these crises, and inform policymakers in this\narea about changes in public opinion.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.12795,review,post_llm,2024,9,"{'ai_likelihood': 1.0927518208821616e-06, 'text': 'Integrating AI Education in Disciplinary Engineering Fields: Towards a System and Change Perspective\n\nBuilding up competencies in working with data and tools of Artificial Intelligence (AI) is becoming more relevant across disciplinary engineering fields. While the adoption of tools for teaching and learning, such as ChatGPT, is garnering significant attention, integration of AI knowledge, competencies, and skills within engineering education is lacking. Building upon existing curriculum change research, this practice paper introduces a systems perspective on integrating AI education within engineering through the lens of a change model. In particular, it identifies core aspects that shape AI adoption on a program level as well as internal and external influences using existing literature and a practical case study. Overall, the paper provides an analysis frame to enhance the understanding of change initiatives and builds the basis for generalizing insights from different initiatives in the adoption of AI in engineering education.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.03376,regular,post_llm,2024,9,"{'ai_likelihood': 1.9205941094292534e-06, 'text': 'Journalists are most likely to receive abuse: Analysing online abuse of\n  UK public figures across sport, politics, and journalism on Twitter\n\n  Engaging with online social media platforms is an important part of life as a\npublic figure in modern society, enabling connection with broad audiences and\nproviding a platform for spreading ideas. However, public figures are often\ndisproportionate recipients of hate and abuse on these platforms, degrading\npublic discourse. While significant research on abuse received by groups such\nas politicians and journalists exists, little has been done to understand the\ndifferences in the dynamics of abuse across different groups of public figures,\nsystematically and at scale. To address this, we present analysis of a novel\ndataset of 45.5M tweets targeted at 4,602 UK public figures across 3 domains\n(members of parliament, footballers, journalists), labelled using fine-tuned\ntransformer-based language models. We find that MPs receive more abuse in\nabsolute terms, but that journalists are most likely to receive abuse after\ncontrolling for other factors. We show that abuse is unevenly distributed in\nall groups, with a small number of individuals receiving the majority of abuse,\nand that for some groups, abuse is more temporally uneven, being driven by\nspecific events, particularly for footballers. We also find that a more\nprominent online presence and being male are indicative of higher levels of\nabuse across all 3 domains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.02601,regular,post_llm,2024,9,"{'ai_likelihood': 1.6424391004774305e-05, 'text': ""ChatGPT vs Social Surveys: Probing Objective and Subjective Silicon\n  Population\n\n  Recent discussions about Large Language Models (LLMs) indicate that they have\nthe potential to simulate human responses in social surveys and generate\nreliable predictions, such as those found in political polls. However, the\nexisting findings are highly inconsistent, leaving us uncertain about the\npopulation characteristics of data generated by LLMs. In this paper, we employ\nrepeated random sampling to create sampling distributions that identify the\npopulation parameters of silicon samples generated by GPT. Our findings show\nthat GPT's demographic distribution aligns with the 2020 U.S. population in\nterms of gender and average age. However, GPT significantly overestimates the\nrepresentation of the Black population and individuals with higher levels of\neducation, even when it possesses accurate knowledge. Furthermore, GPT's point\nestimates for attitudinal scores are highly inconsistent and show no clear\ninclination toward any particular ideology. The sample response distributions\nexhibit a normal pattern that diverges significantly from those of human\nrespondents. Consistent with previous studies, we find that GPT's answers are\nmore deterministic than those of humans. We conclude by discussing the\nconcerning implications of this biased and deterministic silicon population for\nmaking inferences about real-world populations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.07126,regular,post_llm,2024,9,"{'ai_likelihood': 0.28645833333333337, 'text': ""2022 Flood Impact in Pakistan: Remote Sensing Assessment of Agricultural\n  and Urban Damage\n\n  Pakistan was hit by the world's deadliest flood in June 2022, causing\nagriculture and infrastructure damage across the country. Remote sensing\ntechnology offers a cost-effective and efficient method for flood impact\nassessment. This study is aimed to assess the impact of flooding on crops and\nbuilt-up areas. Landsat 9 imagery, European Space Agency-Land Use/Land Cover\n(ESA-LULC) and Soil Moisture Active Passive (SMAP) data are used to identify\nand quantify the extent of flood-affected areas, crop damage, and built-up area\ndestruction. The findings indicate that Sindh, a province in Pakistan, suffered\nthe most. This impact destroyed most Kharif season crops, typically cultivated\nfrom March to November. Using the SMAP satellite data, it is assessed that the\nhigh amount of soil moisture after flood also caused a significant delay in the\ncultivation of Rabi crops. The findings of this study provide valuable\ninformation for decision-makers and stakeholders involved in flood risk\nmanagement and disaster response.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.15352,regular,post_llm,2024,9,"{'ai_likelihood': 0.000511010487874349, 'text': ""An Interactive Web Application for School-Based Physical Fitness Testing\n  in California: Geospatial Analysis and Custom Mapping\n\n  Physical activity is essential for children's healthy growth and development.\nIn the US, most states, including California, adhere to physical education\nstandards and have implemented the mandated School-based Physical Fitness\nTesting (SB-PFT) for over two decades. Despite extensive data collection,\nresearch utilization of SB-PFT has been limited due to the absence of\naccessible analytical tools. We developed a web application using GeoServer,\nArcGIS, and AWS to visualize SB-PFT data. This user-friendly platform enables\neducation administrators and policymakers to analyze trends in children's\nphysical fitness, identify successful programs at schools and districts, and\nevaluate new physical education initiatives. The application also features a\ncustom mapping tool for comparing external datasets with SB-PFT data. We\nconclude that this platform, by integrating advanced analytical capabilities in\nan informatics-based tool, significantly enhances engagement in promoting\nchildren's physical fitness.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.09186,review,post_llm,2024,9,"{'ai_likelihood': 0.034400092230902776, 'text': 'Quantitative Insights into Large Language Model Usage and Trust in\n  Academia: An Empirical Study\n\n  Large Language Models (LLMs) are transforming writing, reading, teaching, and\nknowledge retrieval in many academic fields. However, concerns regarding their\nmisuse and erroneous outputs have led to varying degrees of trust in LLMs\nwithin academic communities. In response, various academic organizations have\nproposed and adopted policies regulating their usage. However, these policies\nare not based on substantial quantitative evidence because there is no data\nabout use patterns and user opinion. Consequently, there is a pressing need to\naccurately quantify their usage, user trust in outputs, and concerns about key\nissues to prioritize in deployment. This study addresses these gaps through a\nquantitative user study of LLM usage and trust in academic research and\neducation. Specifically, our study surveyed 125 individuals at a private R1\nresearch university regarding their usage of LLMs, their trust in LLM outputs,\nand key issues to prioritize for robust usage in academia. Our findings reveal:\n(1) widespread adoption of LLMs, with 75% of respondents actively using them;\n(2) a significant positive correlation between trust and adoption, as well as\nbetween engagement and trust; and (3) that fact-checking is the most critical\nconcern. These findings suggest a need for policies that address pervasive\nusage, prioritize fact-checking mechanisms, and accurately calibrate user trust\nlevels as they engage with these models. These strategies can help balance\ninnovation with accountability and help integrate LLMs into the academic\nenvironment effectively and reliably.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.038,regular,post_llm,2024,9,"{'ai_likelihood': 0.0005859798855251736, 'text': 'Flat-earth communities on Brazilian Telegram: when faith is used to\n  question the existence of gravity as a physics phenomenon\n\n  Conspiracy theories related to flat-earthism have gained traction on\nBrazilian Telegram, especially in times of global crisis, such as the COVID-19\npandemic, when distrust in scientific and governmental institutions has\nintensified. Therefore, this study aims to address the research question: how\nare Brazilian conspiracy theory communities on flat earth topics characterized\nand articulated on Telegram? It is worth noting that this study is part of a\nseries of seven studies whose main objective is to understand and characterize\nBrazilian conspiracy theory communities on Telegram. This series of seven\nstudies is openly and originally available on arXiv at Cornell University,\napplying a mirrored method across the seven studies, changing only the thematic\nobject of analysis and providing investigation replicability, including with\nproprietary and authored codes, adding to the culture of free and open-source\nsoftware. Regarding the main findings of this study, the following were\nobserved: During the Pandemic, flat-earthist discussions increased by 400%,\ndriven by distrust in scientific institutions; Flat-Earther communities act as\nportals for other conspiracy theories, such as the New World Order; Although\nsmaller, the flat-Earther network has influential groups that disseminate\ncontent and perpetuate narratives; Religious themes such as God and the Bible\nare central, combining religious elements with distrust in science;\nFlat-Earther communities use themes such as gravity to challenge established\nscientific concepts, reinforcing an alternative view of the world.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.15287,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Deciphering Cardiac Destiny: Unveiling Future Risks Through Cutting-Edge\n  Machine Learning Approaches\n\n  Cardiac arrest remains a leading cause of death worldwide, necessitating\nproactive measures for early detection and intervention. This project aims to\ndevelop and assess predictive models for the timely identification of cardiac\narrest incidents, utilizing a comprehensive dataset of clinical parameters and\npatient histories. Employing machine learning (ML) algorithms like XGBoost,\nGradient Boosting, and Naive Bayes, alongside a deep learning (DL) approach\nwith Recurrent Neural Networks (RNNs), we aim to enhance early detection\ncapabilities. Rigorous experimentation and validation revealed the superior\nperformance of the RNN model, which effectively captures complex temporal\ndependencies within the data. Our findings highlight the efficacy of these\nmodels in accurately predicting cardiac arrest likelihood, emphasizing the\npotential for improved patient care through early risk stratification and\npersonalized interventions. By leveraging advanced analytics, healthcare\nproviders can proactively mitigate cardiac arrest risk, optimize resource\nallocation, and improve patient outcomes. This research highlights the\ntransformative potential of machine learning and deep learning techniques in\nmanaging cardiovascular risk and advances the field of predictive healthcare\nanalytics.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.867813110351562e-05, 'GPT4': 0.371826171875, 'CLAUDE': 0.0012054443359375, 'GOOGLE': 0.61572265625, 'OPENAI_O_SERIES': 0.01068115234375, 'DEEPSEEK': 0.00020372867584228516, 'GROK': 4.172325134277344e-07, 'NOVA': 0.00024044513702392578, 'OTHER': 0.00024044513702392578, 'HUMAN': 8.344650268554688e-07}}"
2409.15981,regular,post_llm,2024,9,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'GPT-4 as a Homework Tutor can Improve Student Engagement and Learning\n  Outcomes\n\n  This work contributes to the scarce empirical literature on LLM-based\ninteractive homework in real-world educational settings and offers a practical,\nscalable solution for improving homework in schools. Homework is an important\npart of education in schools across the world, but in order to maximize\nbenefit, it needs to be accompanied with feedback and followup questions. We\ndeveloped a prompting strategy that enables GPT-4 to conduct interactive\nhomework sessions for high-school students learning English as a second\nlanguage. Our strategy requires minimal efforts in content preparation, one of\nthe key challenges of alternatives like home tutors or ITSs. We carried out a\nRandomized Controlled Trial (RCT) in four high-school classes, replacing\ntraditional homework with GPT-4 homework sessions for the treatment group. We\nobserved significant improvements in learning outcomes, specifically a greater\ngain in grammar, and student engagement. In addition, students reported high\nlevels of satisfaction with the system and wanted to continue using it after\nthe end of the RCT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.15296,review,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Artificial Intelligence in Education: Ethical Considerations and\n  Insights from Ancient Greek Philosophy\n\n  This paper explores the ethical implications of integrating Artificial\nIntelligence (AI) in educational settings, from primary schools to\nuniversities, while drawing insights from ancient Greek philosophy to address\nemerging concerns. As AI technologies increasingly influence learning\nenvironments, they offer novel opportunities for personalized learning,\nefficient assessment, and data-driven decision-making. However, these\nadvancements also raise critical ethical questions regarding data privacy,\nalgorithmic bias, student autonomy, and the changing roles of educators. This\nresearch examines specific use cases of AI in education, analyzing both their\npotential benefits and drawbacks. By revisiting the philosophical principles of\nancient Greek thinkers such as Socrates, Aristotle, and Plato, we discuss how\ntheir writings can guide the ethical implementation of AI in modern education.\nThe paper argues that while AI presents significant challenges, a balanced\napproach informed by classical philosophical thought can lead to an ethically\nsound transformation of education. It emphasizes the evolving role of teachers\nas facilitators and the importance of fostering student initiative in AI-rich\nenvironments.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.616115570068359e-06, 'GPT4': 0.049163818359375, 'CLAUDE': 0.93896484375, 'GOOGLE': 0.0004324913024902344, 'OPENAI_O_SERIES': 0.010101318359375, 'DEEPSEEK': 0.0013980865478515625, 'GROK': 1.4007091522216797e-05, 'NOVA': 3.516674041748047e-06, 'OTHER': 9.417533874511719e-06, 'HUMAN': 2.0265579223632812e-06}}"
2409.01984,regular,post_llm,2024,9,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Observing Context Improves Disparity Estimation when Race is Unobserved\n\n  In many domains, it is difficult to obtain the race data that is required to\nestimate racial disparity. To address this problem, practitioners have adopted\nthe use of proxy methods which predict race using non-protected covariates.\nHowever, these proxies often yield biased estimates, especially for minority\ngroups, limiting their real-world utility. In this paper, we introduce two new\ncontextual proxy models that advance existing methods by incorporating\ncontextual features in order to improve race estimates. We show that these\nalgorithms demonstrate significant performance improvements in estimating\ndisparities on real-world home loan and voter data. We establish that achieving\nunbiased disparity estimates with contextual proxies relies on\nmean-consistency, a calibration-like condition.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.10837,regular,post_llm,2024,9,"{'ai_likelihood': 5.099508497450087e-06, 'text': ""A Cloud collaborative approach for managing patients wellness\n\n  Patients with chronic diseases or people with special health care needs are\ntypically monitored by various health experts that address the problem from\nseveral perspectives. These experts usually do not interact directly between\nthem; therefore, the instructions given to the patient by one of them are\nprovided disregarding advices and instructions provided by the others. The\ncollaboration between different health experts in a real context is mandatory\nto ensure the proper monitoring of the patient. This kind of collaboration,\nsupported by technology, benefits the users' health condition and helps the\npatient achieve their goals in terms of wellbeing. This paper presents an\napproach for collaborative management of events related to activity\nsupervisions and monitoring of these patients types. It also introduces a model\nand a mobile application to support this collaborative work.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.02614,review,post_llm,2024,9,"{'ai_likelihood': 0.9072265625, 'text': ""Evaluating the Effects of Digital Privacy Regulations on User Trust\n\n  In today's digital society, issues related to digital privacy have become\nincreasingly important. Issues such as data breaches result in misuse of data,\nfinancial loss, and cyberbullying, which leads to less user trust in digital\nservices. This research investigates the impact of digital privacy laws on user\ntrust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The\nstudy employs a comparative case study method, involving interviews with\ndigital privacy law experts, IT educators, and consumers from each country. The\nmain findings reveal that while the General Data Protection Regulation (GDPR)\nin the Netherlands is strict, its practical impact is limited by enforcement\nchallenges. In Ghana, the Data Protection Act is underutilized due to low\npublic awareness and insufficient enforcement, leading to reliance on personal\nprotective measures. In Malaysia, trust in digital services is largely\ndependent on the security practices of individual platforms rather than the\nPersonal Data Protection Act. The study highlights the importance of public\nawareness, effective enforcement, and cultural considerations in shaping the\neffectiveness of digital privacy laws. Based on these insights, a\nrecommendation framework is proposed to enhance digital privacy practices, also\naiming to provide valuable guidance for policymakers, businesses, and citizens\nin navigating the challenges of digitalization.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.039825439453125, 'GPT4': 0.353271484375, 'CLAUDE': 0.0020313262939453125, 'GOOGLE': 0.57958984375, 'OPENAI_O_SERIES': 0.00589752197265625, 'DEEPSEEK': 2.5391578674316406e-05, 'GROK': 2.3245811462402344e-05, 'NOVA': 3.641843795776367e-05, 'OTHER': 0.00318145751953125, 'HUMAN': 0.0159149169921875}}"
2409.01461,regular,post_llm,2024,9,"{'ai_likelihood': 1.2351406945122613e-05, 'text': ""Human-Centered AI Applications for Canada's Immigration Settlement\n  Sector\n\n  While AI has been frequently applied in the context of immigration, most of\nthese applications focus on selection and screening, which primarily serve to\nempower states and authorities, raising concerns due to their understudied\nreliability and high impact on immigrants' lives. In contrast, this paper\nemphasizes the potential of AI in Canada's immigration settlement phase, a\nstage where access to information is crucial and service providers are\noverburdened. By highlighting the settlement sector as a prime candidate for\nreliable AI applications, we demonstrate its unique capacity to empower\nimmigrants directly, yet it remains under-explored in AI research. We outline a\nvision for human-centred and responsible AI solutions that facilitate the\nintegration of newcomers. We call on AI researchers to build upon our work and\nengage in multidisciplinary research and active collaboration with service\nproviders and government organizations to develop tailored AI tools that are\nempowering, inclusive and safe.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.11805,regular,post_llm,2024,9,"{'ai_likelihood': 0.0001294745339287652, 'text': ""Inside Out or Not: Privacy Implications of Emotional Disclosure\n\n  Privacy is dynamic, sensitive, and contextual, much like our emotions.\nPrevious studies have explored the interplay between privacy and context,\nprivacy and emotion, and emotion and context. However, there remains a\nsignificant gap in understanding the interplay of these aspects simultaneously.\nIn this paper, we present a preliminary study investigating the role of\nemotions in driving individuals' information sharing behaviour, particularly in\nrelation to urban locations and social ties. We adopt a novel methodology that\nintegrates context (location and time), emotion, and personal information\nsharing behaviour, providing a comprehensive analysis of how contextual\nemotions affect privacy. The emotions are assessed with both self-reporting and\nelectrodermal activity (EDA). Our findings reveal that self-reported emotions\ninfluence personal information-sharing behaviour with distant social groups,\nwhile neutral emotions lead individuals to share less precise information with\nclose social circles, a pattern is potentially detectable with wrist-worn EDA.\nOur study helps lay the foundation for personalised emotion-aware strategies to\nmitigate oversharing risks and enhance user privacy in the digital age.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.05082,review,post_llm,2024,9,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'The limits of progress in the digital era\n\n  The concept of progress clearly percolates the activities in science,\ntechnology, economy and society. It is a driving vector (probably the main\nvector) of our daily activity as researchers. The InThisGen initiative, proudly\ndisplayed in places across the University of Berkeley campus, and its headline\nlemma (what can we change in a single generation?) are clear exponents of the\nunderlying assumption that progress is not only possible but also desirable.\nBut about the concept of progress two major concerns arise. First of all,\nprogress means some kind of going forward, that is a direction in a journey.\nBut deciding the way in the route clearly implies that we are explicit or\nimplicitly defining the goals, as individuals and as society. That is, the\nconcept of progress has a set of underlying values. Additionally, the\nconceptual paradigm in scientific research (and probably in the whole spirit of\nour times) it is assuming some kind of endless progress. It is true that many\ntechnological innovations and their subsequent impact on society have found\nresistance, from Luddites to ecologist movements. But the last 150 years (the\nage of our university) have been witness of an enormous and general increase in\nknowledge, wealth and welfare, showing how progress can be sustained in the\nlong-term and positively influence the human beings and the society. In this\ncontribution will try to discuss these bounds, addressing the limits of\nmaterials, scientific knowledge and technological know-how. We will mainly\nfocus on the limitations in technological knowledge in the software design, a\nkey aspect of the digital era. Our main thesis, which will be addressed through\nthe paper, is that there are intrinsic limits to technological knowledge and\nthe concept of progress should take them into account.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.10827,regular,post_llm,2024,9,"{'ai_likelihood': 5.099508497450087e-06, 'text': ""The CEKG: A Tool for Constructing Event Graphs in the Care Pathways of\n  Multi-Morbid Patients\n\n  One of the challenges in healthcare processes, especially those related to\nmulti-morbid patients who suffer from multiple disorders simultaneously, is not\nconnecting the disorders in patients to process events and not linking events'\nactivities to globally accepted terminology. Addressing this challenge\nintroduces a new entity to the clinical process. On the other hand, it\nfacilitates that the process is interpretable and analyzable across different\nhealthcare systems. This paper aims to introduce a tool named CEKG that uses\nevent logs, diagnosis data, ICD-10, SNOMED-CT, and mapping functions to satisfy\nthese challenges by constructing event graphs for multi-morbid patients' care\npathways automatically.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.00826,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Digital Homunculi: Reimagining Democracy Research with Generative Agents\n\n  The pace of technological change continues to outstrip the evolution of\ndemocratic institutions, creating an urgent need for innovative approaches to\ndemocratic reform. However, the experimentation bottleneck - characterized by\nslow speed, high costs, limited scalability, and ethical risks - has long\nhindered progress in democracy research. This paper proposes a novel solution:\nemploying generative artificial intelligence (GenAI) to create synthetic data\nthrough the simulation of digital homunculi, GenAI-powered entities designed to\nmimic human behavior in social contexts. By enabling rapid, low-risk\nexperimentation with alternative institutional designs, this approach could\nsignificantly accelerate democratic innovation. I examine the potential of\nGenAI-assisted research to mitigate current limitations in democratic\nexperimentation, including the ability to simulate large-scale societal\ninteractions and test complex institutional mechanisms. While acknowledging\npotential risks such as algorithmic bias, reproducibility challenges, and AI\nalignment issues, I argue that the benefits of synthetic data are likely to\noutweigh their drawbacks if implemented with proper caution. To address\nexisting challenges, I propose a range of technical, methodological, and\ninstitutional adaptations. The paper concludes with a call for\ninterdisciplinary collaboration in the development and implementation of\nGenAI-assisted methods in democracy research, highlighting their potential to\nbridge the gap between democratic theory and practice in an era of rapid\ntechnological change.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0967254638671875e-05, 'GPT4': 0.0038547515869140625, 'CLAUDE': 0.9951171875, 'GOOGLE': 0.0010824203491210938, 'OPENAI_O_SERIES': 4.112720489501953e-06, 'DEEPSEEK': 7.748603820800781e-06, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 6.020069122314453e-06, 'HUMAN': 5.7220458984375e-06}}"
2409.03307,review,post_llm,2024,9,"{'ai_likelihood': 2.086162567138672e-06, 'text': 'AI data transparency: an exploration through the lens of AI incidents\n\n  Knowing more about the data used to build AI systems is critical for allowing\ndifferent stakeholders to play their part in ensuring responsible and\nappropriate deployment and use. Meanwhile, a 2023 report shows that data\ntransparency lags significantly behind other areas of AI transparency in\npopular foundation models. In this research, we sought to build on these\nfindings, exploring the status of public documentation about data practices\nwithin AI systems generating public concern.\n  Our findings demonstrate that low data transparency persists across a wide\nrange of systems, and further that issues of transparency and explainability at\nmodel- and system- level create barriers for investigating data transparency\ninformation to address public concerns about AI systems. We highlight a need to\ndevelop systematic ways of monitoring AI data transparency that account for the\ndiversity of AI system types, and for such efforts to build on further\nunderstanding of the needs of those both supplying and using data transparency\ninformation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.00016,regular,post_llm,2024,9,"{'ai_likelihood': 0.0002215968237982856, 'text': ""A Dataset of the Operating Station Heat Rate for 806 Indian Coal Plant\n  Units using Machine Learning\n\n  India aims to achieve net-zero emissions by 2070 and has set an ambitious\ntarget of 500 GW of renewable power generation capacity by 2030. Coal plants\ncurrently contribute to more than 60\\% of India's electricity generation in\n2022. Upgrading and decarbonizing high-emission coal plants became a pressing\nenergy issue. A key technical parameter for coal plants is the operating\nstation heat rate (SHR), which represents the thermal efficiency of a coal\nplant. Yet, the operating SHR of Indian coal plants varies and is not\ncomprehensively documented. This study extends from several existing databases\nand creates an SHR dataset for 806 Indian coal plant units using machine\nlearning (ML), presenting the most comprehensive coverage to date.\nAdditionally, it incorporates environmental factors such as water stress risk\nand coal prices as prediction features to improve accuracy. This dataset,\neasily downloadable from our visualization platform, could inform energy and\nenvironmental policies for India's coal power generation as the country\ntransitions towards its renewable energy targets.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.02781,regular,post_llm,2024,9,"{'ai_likelihood': 0.4801432291666667, 'text': 'Enhancing ICT Literacy and Sustainable Practices in the Hospitality\n  Industry: Insights from Mnquma Municipality\n\n  The leisure and hospitality industry is a significant driver of the global\neconomy, with the adoption of new technologies transforming service delivery\nand customer experience. Despite the transformative potential and benefits\nassociated with adopting technology, there remains a low level of adoption in\nrural areas, particularly among small-scale players. This study explores the\nrole of ICT literacy and sustainable practices in influencing ICT adoption\namong small-scale players in the hospitality industry in rural Eastern Cape\nProvince, South Africa, specifically focusing on Mnquma Municipality. The study\nemploys a non-probability sampling and purposive technique, utilising a case\nstudy research design within a positivist paradigm. A random sample of 21\nsmall-scale players (BnBs, guest houses, and non-serviced accommodations) was\nselected, and data were collected through a face-to-face interview and\nquestionnaire featuring closed-ended questions. The data were analysed using\ndescriptive statistics and the Kruskal-Wallis H Test to examine differences in\nICT usage levels. The test yielded a Kruskal-Wallis H of 2.57 with a p-value of\n0.277. The findings reveal that businesses with more educated workforces\ndemonstrate higher ICT adoption levels. Moreover, key factors such as ICT\nliteracy, awareness of sustainable practices, access to ICT resources, and\ncontextual challenges significantly impact ICT adoption. Recommendations\ninclude integrating ICT literacy and sustainability education into training\nprograms and developing targeted policies and support mechanisms to enhance ICT\nintegration.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.1375,regular,post_llm,2024,9,"{'ai_likelihood': 1.2781884935167101e-05, 'text': 'Undergrads Are All You Have\n\n  The outsourcing of busy work and other research-related tasks to\nundergraduate students is a time-honored academic tradition. In recent years,\nthese tasks have been given to Lama-based large-language models such as Alpaca\nand Llama increasingly often, putting poor undergraduate students out of work.\nDue to the costs associated with importing and caring for South American\nCamelidae, researcher James Yoo set out to find a cheaper and more effective\nalternative to these models. The findings, published in the highly-respected\njournal, SIGBOVIK, demonstrates that their model, GPT-UGRD is on par with, and\nin some cases better, than Lama models for natural language processing tasks.\nThe paper also demonstrates that GPT-UGRD is cheaper and easier to train and\noperate than transformer models. In this paper, we outline the implementation,\napplication, multi-tenanting, and social implications of using this new model\nin research and other contexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.12983,review,post_llm,2024,9,"{'ai_likelihood': 0.00023179584079318578, 'text': 'New world order, globalism and QAnon communities on Brazilian Telegram:\n  how conspiracism opens doors to more harmful groups\n\n  Conspiracy theories involving the New World Order (NWO), Globalism, and QAnon\nhave become central to discussions on Brazilian Telegram, especially during\nglobal crises such as the COVID-19 pandemic. Therefore, this study aims to\naddress the research question: how are Brazilian conspiracy theory communities\non new world order, globalism and QAnon topics characterized and articulated on\nTelegram? It is worth noting that this study is part of a series of seven\nstudies whose main objective is to understand and characterize Brazilian\nconspiracy theory communities on Telegram. This series of seven studies is\nopenly and originally available on arXiv at Cornell University, applying a\nmirrored method across the seven studies, changing only the thematic object of\nanalysis and providing investigation replicability, including with proprietary\nand authored codes, adding to the culture of free and open-source software.\nRegarding the main findings of this study, the following were observed: NWO and\nGlobalism have become central catalysts for the dissemination of conspiracy\ntheories; QAnon acts as a hub narrative that connects NWO and Globalism; During\ncrises, mentions of NWO have grown exponentially, reflecting distrust in\ninstitutions; NWO and Globalism attract followers of other conspiracy theories,\nsuch as anti-vaccines, serving as the main gatekeeper of the entire conspiracy\ntheory network; Religious narratives are often used to legitimize NWO,\nreinforcing ideological cohesion.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.11314,review,post_llm,2024,9,"{'ai_likelihood': 1.7848279741075305e-05, 'text': 'The Role of AI Safety Institutes in Contributing to International\n  Standards for Frontier AI Safety\n\n  International standards are crucial for ensuring that frontier AI systems are\ndeveloped and deployed safely around the world. Since the AI Safety Institutes\n(AISIs) possess in-house technical expertise, mandate for international\nengagement, and convening power in the national AI ecosystem while being a\ngovernment institution, we argue that they are particularly well-positioned to\ncontribute to the international standard-setting processes for AI safety. In\nthis paper, we propose and evaluate three models for AISI involvement: 1. Seoul\nDeclaration Signatories, 2. US (and other Seoul Declaration Signatories) and\nChina, and 3. Globally Inclusive. Leveraging their diverse strengths, these\nmodels are not mutually exclusive. Rather, they offer a multi-track system\nsolution in which the central role of AISIs guarantees coherence among the\ndifferent tracks and consistency in their AI safety focus.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.1416,review,post_llm,2024,9,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in\n  AI\n\n  With the growing attention and investment in recent AI approaches such as\nlarge language models, the narrative that the larger the AI system the more\nvaluable, powerful and interesting it is is increasingly seen as common sense.\nBut what is this assumption based on, and how are we measuring value, power,\nand performance? And what are the collateral consequences of this race to\never-increasing scale? Here, we scrutinize the current scaling trends and\ntrade-offs across multiple axes and refute two common assumptions underlying\nthe 'bigger-is-better' AI paradigm: 1) that performance improvements are driven\nby increased scale, and 2) that all interesting problems addressed by AI\nrequire large-scale models. Rather, we argue that this approach is not only\nfragile scientifically, but comes with undesirable consequences. First, it is\nnot sustainable, as, despite efficiency improvements, its compute demands\nincrease faster than model performance, leading to unreasonable economic\nrequirements and a disproportionate environmental footprint. Second, it implies\nfocusing on certain problems at the expense of others, leaving aside important\napplications, e.g. health, education, or the climate. Finally, it exacerbates a\nconcentration of power, which centralizes decision-making in the hands of a few\nactors while threatening to disempower others in the context of shaping both AI\nresearch and its applications throughout society.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.07139,regular,post_llm,2024,9,"{'ai_likelihood': 0.0003070301479763455, 'text': 'Advancing Global South University Education with Large Language Models\n\n  In recent years, it has been observed that the center of gravity for the\nvolume of higher education has shifted to the Global South. However, research\nindicates a widening disparity in the quality of higher education between the\nGlobal South and the Global North. Although investments in higher education\nwithin the Global South have increased, the rapid surge in student numbers has\nresulted in a decline in public expenditure per student. For instance, the\nstudent-to-teacher ratio in the Global South is significantly higher compared\nto that in the Global North, which poses a substantial barrier to the\nimplementation of creative education. In response, Telkom University in\nIndonesia has embarked on an experiment to enhance the quality of learning and\nteaching by integrating large language models (LLMs) such as ChatGPT into five\nof its courses-Mathematics, English, Computing, Computer Systems, and Creative\nMedia. This article elucidates the ongoing experimental plan and explores how\nthe integration of LLMs could contribute to addressing the challenges currently\nfaced by higher education in the Global South.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.17484,review,post_llm,2024,9,"{'ai_likelihood': 0.00037961535983615456, 'text': 'Crafting Synthetic Realities: Examining Visual Realism and\n  Misinformation Potential of Photorealistic AI-Generated Images\n\n  Advances in generative models have created Artificial Intelligence-Generated\nImages (AIGIs) nearly indistinguishable from real photographs. Leveraging a\nlarge corpus of 30,824 AIGIs collected from Instagram and Twitter, and\ncombining quantitative content analysis with qualitative analysis, this study\nunpacks AI photorealism of AIGIs from four key dimensions, content, human,\naesthetic, and production features. We find that photorealistic AIGIs often\ndepict human figures, especially celebrities and politicians, with a high\ndegree of surrealism and aesthetic professionalism, alongside a low degree of\novert signals of AI production. This study is the first to empirically\ninvestigate photorealistic AIGIs across multiple platforms using a\nmixed-methods approach. Our findings provide important implications and\ninsights for understanding visual misinformation and mitigating potential risks\nassociated with photorealistic AIGIs. We also propose design recommendations to\nenhance the responsible use of AIGIs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.14104,regular,post_llm,2024,9,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'IPF-HMGNN: A novel integrative prediction framework for metro passenger\n  flow\n\n  The operation and management of the metro system in urban areas rely on\naccurate predictions of future passenger flow. While using all the available\ninformation can potentially improve on the accuracy of the flow prediction,\nthere has been little attention to the hierarchical relationship between the\ntype of tickets collected from the passengers entering/exiting a station and\nits resulting passenger flow. To this end, we propose a novel Integrative\nPrediction Framework with the Hierarchical Message-Passing Graph Neural Network\n(IPF-HMGNN). The proposed framework consists of three components: initial\nprediction, task judgment and hierarchical coordination modules. Using the\nWuxi, China metro network as an example, we study two prediction approaches (i)\ntraditional prediction approach where the model directly predicts passenger\nflow at the station, and (ii) hierarchical prediction approach where the\nprediction of ticket type and station passenger flow are performed\nsimultaneously considering the hierarchical constraints (i.e., the sum of\npredicted passenger flow per ticket type equals the predicted station\naggregated passenger flow). Experimental results indicate that in the\ntraditional prediction approach, our IPF-HMGNN can significantly reduce the\nmean absolute error (MAE) and root mean square error (RMSE) of the GNN\nprediction model by 49.56% and 53.88%, respectively. In the hierarchical\nprediction approach, IPF-HMGNN can achieve a maximum reduction of 35.32% in MAE\nand 36.18% in RMSE, while satisfying the hierarchical constraint.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.13575,regular,post_llm,2024,9,"{'ai_likelihood': 9.404288397894965e-06, 'text': ""A Law of One's Own: The Inefficacy of the DMCA for Non-Consensual\n  Intimate Media\n\n  Non-consensual intimate media (NCIM) presents internet-scale harm to\nindividuals who are depicted. One of the most powerful tools for requesting its\nremoval is the Digital Millennium Copyright Act (DMCA). However, the DMCA was\ndesigned to protect copyright holders rather than to address the problem of\nNCIM. Using a dataset of more than 54,000 DMCA reports and over 85 million\ninfringing URLs spanning over a decade, this paper evaluates the efficacy of\nthe DMCA for NCIM takedown. Results show that for non-commercial requests,\nwhile more than half of URLs are deindexed from Google Search within 48 hours,\nthe actual removal of content from website hosts is much slower. The median\ninfringing URL takes more than 45 days to be removed from website hosts, and\nonly 5.39% URLs are removed within the first 48 hours. Additionally, the most\nfrequently reported domains for non-commercial NCIM are smaller websites, not\nlarge platforms. We stress the need for new laws that ensure a shorter time to\ntakedown that are enforceable across big and small platforms alike.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.17959,review,post_llm,2024,9,"{'ai_likelihood': 0.99267578125, 'text': ""A Policy Report Evaluating the National Assessment Program for Literacy\n  and Numeracy (Naplan) Reform in Australia: The Impacts of High Stakes\n  Assessment on Students\n\n  The National Assessment Program for Literacy and Numeracy (NAPLAN) Reform in\nAustralia, launched in 2008, has emerged as the country's most significant and\ncontentious reform. However, due to its high-stakes nature and standardization,\ntesting presents various challenges. These challenges include the combination\nof accountability with the 'My School' website, overlooking higher-order\ncognitive abilities, exacerbating students' anxiety and stress, and creating\ninequity for Language Background Other Than English (LBOTE) students. This\nreport assesses the achievements and obstacles of the NAPLAN reform, proposing\nrecommendations such as transitioning to online testing, enhancing content and\nplatforms, increasing public assessment literacy, and investing more in LBOTE\neducation. These suggestions aim to strike a balance between standardized\ntesting and authentic educational pursuits, adapting to the evolving needs of\nstudents to create a fair, inclusive educational environment that addresses the\ndemands of the 21st century.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.005802154541015625, 'GPT4': 0.9443359375, 'CLAUDE': 0.0005092620849609375, 'GOOGLE': 0.0439453125, 'OPENAI_O_SERIES': 0.0016794204711914062, 'DEEPSEEK': 0.00010883808135986328, 'GROK': 2.8252601623535156e-05, 'NOVA': 2.3245811462402344e-05, 'OTHER': 0.0006070137023925781, 'HUMAN': 0.0031986236572265625}}"
2409.19959,review,post_llm,2024,9,"{'ai_likelihood': 0.06374782986111112, 'text': 'Gender Biases in LLMs: Higher intelligence in LLM does not necessarily\n  solve gender bias and stereotyping\n\n  Large Language Models (LLMs) are finding applications in all aspects of life,\nbut their susceptibility to biases, particularly gender stereotyping, raises\nethical concerns. This study introduces a novel methodology, a persona-based\nframework, and a unisex name methodology to investigate whether\nhigher-intelligence LLMs reduce such biases. We analyzed 1400 personas\ngenerated by two prominent LLMs, revealing that systematic biases persist even\nin LLMs with higher intelligence and reasoning capabilities. o1 rated males\nhigher in competency (8.1) compared to females (7.9) and non-binary (7.80). The\nanalysis reveals persistent stereotyping across fields like engineering, data,\nand technology, where the presence of males dominates. Conversely, fields like\ndesign, art, and marketing show a stronger presence of females, reinforcing\nsocietal notions that associate creativity and communication with females. This\npaper suggests future directions to mitigate such gender bias, reinforcing the\nneed for further research to reduce biases and create equitable AI models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.08219,review,post_llm,2024,9,"{'ai_likelihood': 5.099508497450087e-06, 'text': 'Digital Twins of Business Processes: A Research Manifesto\n\n  Modern organizations necessitate continuous business processes improvement to\nmaintain efficiency, adaptability, and competitiveness. In the last few years,\nthe Internet of Things, via the deployment of sensors and actuators, has\nheavily been adopted in organizational and industrial settings to monitor and\nautomatize physical processes influencing and enhancing how people and\norganizations work. Such advancements are now pushed forward by the rise of the\nDigital Twin paradigm applied to organizational processes. Advanced ways of\nmanaging and maintaining business processes come within reach as there is a\nDigital Twin of a business process - a virtual replica with real-time\ncapabilities of a real process occurring in an organization. Combining business\nprocess models with real-time data and simulation capabilities promises to\nprovide a new way to guide day-to-day organization activities. However,\nintegrating Digital Twins and business processes is a non-trivial task,\npresenting numerous challenges and ambiguities. This manifesto paper aims to\ncontribute to the current state of the art by clarifying the relationship\nbetween business processes and Digital Twins, identifying ongoing research and\nopen challenges, thereby shedding light on and driving future exploration of\nthis innovative interplay.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.10838,review,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Evaluating the Effectiveness of SIWES Programs for Computer Science Students at Moshood Abiola Polytechnic, Abeokuta\n\nThis study examines the effectiveness of the Students Industrial Work Experience Scheme (SIWES) for Computer Science students at Moshood Abiola Polytechnic, Abeokuta. Through correlational analysis of various assessment components, including employer evaluations, student logbooks, and technical reports, the research aims to identify key factors contributing to student performance and skill development. The findings reveal strong correlations between employer evaluations, logbook quality, and overall performance, highlighting the importance of practical experience and documentation in the SIWES program. This research contributes to the ongoing discourse on the role of industrial training in higher education and provides recommendations for enhancing the SIWES experience for computer science students.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.01271820068359375, 'GPT4': 0.005340576171875, 'CLAUDE': 0.0254669189453125, 'GOOGLE': 0.1275634765625, 'OPENAI_O_SERIES': 0.00014674663543701172, 'DEEPSEEK': 0.0006971359252929688, 'GROK': 4.32133674621582e-05, 'NOVA': 0.025054931640625, 'OTHER': 0.802734375, 'HUMAN': 7.152557373046875e-07}}"
2409.15308,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Transforming Redaction: How AI is Revolutionizing Data Protection\n\n  Document redaction is a crucial process in various sectors to safeguard\nsensitive information from unauthorized access and disclosure. Traditional\nmanual redaction methods, such as those performed using Adobe Acrobat, are\nlabor-intensive, error-prone, and time-consuming. With the burgeoning volume of\ndigital documents, the demand for more efficient and accurate redaction\ntechniques is intensifying.\n  This study presents the findings from a controlled experiment that compares\ntraditional manual redaction, a redaction tool powered by classical machine\nlearning algorithm, and AI-assisted redaction tools (iDox.ai Redact). The\nresults indicate that iDox.ai Redact significantly outperforms manual methods,\nachieving higher accuracy and faster completion times. Conversely, the\ncompetitor product, classical machine learning algorithm and with necessitates\nmanual intervention for certain sensitive data types, did not exhibit a\nstatistically significant improvement over manual redaction.\n  These findings suggest that while advanced AI technologies like iDox.ai\nRedact can substantially enhance data protection practices by reducing human\nerror and improving compliance with data protection regulations, there remains\nroom for improvement in AI tools that do not fully automate the redaction\nprocess. Future research should aim to enhance AI capabilities and explore\ntheir applicability across various document types and professional settings.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.000591278076171875, 'GPT4': 0.87109375, 'CLAUDE': 0.0007009506225585938, 'GOOGLE': 0.08245849609375, 'OPENAI_O_SERIES': 0.043121337890625, 'DEEPSEEK': 4.482269287109375e-05, 'GROK': 3.17692756652832e-05, 'NOVA': 0.00012564659118652344, 'OTHER': 0.0019817352294921875, 'HUMAN': 2.6881694793701172e-05}}"
2409.08751,review,post_llm,2024,9,"{'ai_likelihood': 0.0025261773003472225, 'text': ""A Grading Rubric for AI Safety Frameworks\n\n  Over the past year, artificial intelligence (AI) companies have been\nincreasingly adopting AI safety frameworks. These frameworks outline how\ncompanies intend to keep the potential risks associated with developing and\ndeploying frontier AI systems to an acceptable level. Major players like\nAnthropic, OpenAI, and Google DeepMind have already published their frameworks,\nwhile another 13 companies have signaled their intent to release similar\nframeworks by February 2025. Given their central role in AI companies' efforts\nto identify and address unacceptable risks from their systems, AI safety\nframeworks warrant significant scrutiny. To enable governments, academia, and\ncivil society to pass judgment on these frameworks, this paper proposes a\ngrading rubric. The rubric consists of seven evaluation criteria and 21\nindicators that concretize the criteria. Each criterion can be graded on a\nscale from A (gold standard) to F (substandard). The paper also suggests three\nmethods for applying the rubric: surveys, Delphi studies, and audits. The\npurpose of the grading rubric is to enable nuanced comparisons between\nframeworks, identify potential areas of improvement, and promote a race to the\ntop in responsible AI development.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.20676,regular,post_llm,2024,9,"{'ai_likelihood': 1.0, 'text': 'Convergences and Divergences in the 2024 Judicial Reform in Mexico: A\n  Neural Network Analysis of Transparency, Judicial Autonomy, and Public\n  Acceptance\n\n  This study utilizes neural networks to evaluate the 2024 judicial reform in\nMexico, a proposal designed to overhaul the judicial system by increasing\ntransparency, judicial autonomy, and introducing the popular election of\njudges. The neural network model analyzes both converging and diverging factors\nthat influence the reforms viability and public acceptance. Key areas of\nconvergence include enhanced transparency and judicial autonomy, which are seen\nas improvements to the system. However, major points of divergence, such as the\nhigh costs of implementation and concerns about the legitimacy of electing\njudges, pose significant challenges. By integrating variables like\ntransparency, decision quality, judicial independence, and implementation\ncosts, the model predicts levels of public and professional acceptance of the\nreform. The neural networks multilayered structure allows for the modeling of\ncomplex relationships, offering predictive insights into how the reform may\nimpact the Mexican judicial system. Initial findings suggest that while the\nreform could strengthen judicial autonomy, the risks of politicizing the\njudiciary and the financial burden it entails may reduce its overall\nacceptance. This research highlights the importance of using advanced AI tools\nto simulate public policy outcomes, providing valuable data to guide lawmakers\nin refining their proposals.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.33272933959961e-05, 'GPT4': 0.97509765625, 'CLAUDE': 0.00036406517028808594, 'GOOGLE': 0.0008029937744140625, 'OPENAI_O_SERIES': 0.02313232421875, 'DEEPSEEK': 0.00028634071350097656, 'GROK': 0.0002949237823486328, 'NOVA': 5.424022674560547e-06, 'OTHER': 3.6716461181640625e-05, 'HUMAN': 5.304813385009766e-06}}"
2410.12796,regular,post_llm,2024,9,"{'ai_likelihood': 4.933940039740669e-06, 'text': 'A Roles-based Competency Framework for Integrating Artificial Intelligence (AI) in Engineering Courses\n\nIn this practice paper, we propose a framework for integrating AI into disciplinary engineering courses and curricula. The use of AI within engineering is an emerging but growing area and the knowledge, skills, and abilities (KSAs) associated with it are novel and dynamic. This makes it challenging for faculty who are looking to incorporate AI within their courses to create a mental map of how to tackle this challenge. In this paper, we advance a role-based conception of competencies to assist disciplinary faculty with identifying and implementing AI competencies within engineering curricula. We draw on prior work related to AI literacy and competencies and on emerging research on the use of AI in engineering. To illustrate the use of the framework, we provide two exemplary cases. We discuss the challenges in implementing the framework and emphasize the need for an embedded approach where AI concerns are integrated across multiple courses throughout the degree program, especially for teaching responsible and ethical AI development and use.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.15319,regular,post_llm,2024,9,"{'ai_likelihood': 0.023210313585069444, 'text': ""Merging AI Incidents Research with Political Misinformation Research:\n  Introducing the Political Deepfakes Incidents Database\n\n  This article presents the Political Deepfakes Incidents Database (PDID), a\ncollection of politically-salient deepfakes, encompassing synthetically-created\nvideos, images, and less-sophisticated `cheapfakes.' The project is driven by\nthe rise of generative AI in politics, ongoing policy efforts to address harms,\nand the need to connect AI incidents and political communication research. The\ndatabase contains political deepfake content, metadata, and researcher-coded\ndescriptors drawn from political science, public policy, communication, and\nmisinformation studies. It aims to help reveal the prevalence, trends, and\nimpact of political deepfakes, such as those featuring major political figures\nor events. The PDID can benefit policymakers, researchers, journalists,\nfact-checkers, and the public by providing insights into deepfake usage, aiding\nin regulation, enabling in-depth analyses, supporting fact-checking and\ntrust-building efforts, and raising awareness of political deepfakes. It is\nsuitable for research and application on media effects, political discourse, AI\nethics, technology governance, media literacy, and countermeasures.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.12984,regular,post_llm,2024,9,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Large Language Model-Enhanced Interactive Agent for Public Education on\n  Newborn Auricular Deformities\n\n  Auricular deformities are quite common in newborns with potential long-term\nnegative effects of mental and even hearing problems.Early diagnosis and\nsubsequent treatment are critical for the illness; yet they are missing most of\nthe time due to lack of knowledge among parents. With the help of large\nlanguage model of Ernie of Baidu Inc., we derive a realization of interactive\nagent. Firstly, it is intelligent enough to detect which type of auricular\ndeformity corresponding to uploaded images, which is accomplished by\nPaddleDetection, with precision rate 75\\%. Secondly, in terms of popularizing\nthe knowledge of auricular deformities, the agent can give professional\nsuggestions of the illness to parents. The above two effects are evaluated via\ntests on volunteers with control groups in the paper. The agent can reach\nparents with newborns as well as their pediatrician remotely via Internet in\nvast, rural areas with quality medical diagnosis capabilities and professional\nquery-answering functions, which is good news for newborn auricular deformity\nand other illness that requires early intervention for better treatment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.08653,review,post_llm,2024,9,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Payments Use Cases and Design Options for Interoperability and Funds\n  Locking across Digital Pounds and Commercial Bank Money\n\n  Central banks are actively exploring retail central bank digital currencies\n(CBDCs), with the Bank of England currently in the design phase for a potential\nUK retail CBDC, the digital pound. In a previous paper, we defined and explored\nthe important concept of functional consistency (which is the principle that\ndifferent forms of money have the same operational characteristics) and\nevaluated design options to support functional consistency across digital\npounds and commercial bank money, based on a set of key capabilities. In this\npaper, we continue to analyse the design options for supporting functional\nconsistency and, in order to perform a detailed analysis, we focus on three key\ncapabilities: communication between digital pound ecosystem participants, funds\nlocking, and interoperability across digital pounds and commercial bank money.\nWe explore these key capabilities via three payments use cases:\nperson-to-person push payment, merchant-initiated request to pay, and lock\nfunds and pay on physical delivery. We then present and evaluate the\nsuitability of design options to provide the specific capabilities for each use\ncase and draw initial insights. We conclude that a financial market\ninfrastructure (FMI) providing specific capabilities could simplify the\nexperience of ecosystem participants, simplify the operating platforms for both\nthe Bank of England and digital pound Payment Interface Providers (PIPs), and\nfacilitate the creation of innovative services. We also identify potential next\nsteps.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.17207,regular,post_llm,2024,9,"{'ai_likelihood': 4.602803124321832e-06, 'text': 'Finite State Machine with Input and Process Render\n\n  Finite State Machines are a concept widely taught in undergraduate theory of\ncomputing courses. Educators typically use tools with static representations of\nFSMs to help students visualize these objects and processes; however, all\nexisting tools require manual editing by the instructor. In this poster, we\ncreated an automatic visualization tool for FSMs that generates videos of FSM\nsimulation, named Finite State Machine with Input and Process Render (FSMIPR).\nEducators can input any formal definition of an FSM and an input string, and\nFSMIPR generates an accompanying video of its simulation. We believe that\nFSMIPR will be beneficial to students who learn difficult computer theory\nconcepts. We conclude with future work currently in-progress with FSMIPR.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.07878,review,post_llm,2024,9,"{'ai_likelihood': 2.1027194129096137e-05, 'text': 'Mapping Technical Safety Research at AI Companies: A literature review\n  and incentives analysis\n\n  As AI systems become more advanced, concerns about large-scale risks from\nmisuse or accidents have grown. This report analyzes the technical research\ninto safe AI development being conducted by three leading AI companies:\nAnthropic, Google DeepMind, and OpenAI.\n  We define safe AI development as developing AI systems that are unlikely to\npose large-scale misuse or accident risks. This encompasses a range of\ntechnical approaches aimed at ensuring AI systems behave as intended and do not\ncause unintended harm, even as they are made more capable and autonomous.\n  We analyzed all papers published by the three companies from January 2022 to\nJuly 2024 that were relevant to safe AI development, and categorized the 80\nincluded papers into nine safety approaches. Additionally, we noted two\ncategories representing nascent approaches explored by academia and civil\nsociety, but not currently represented in any research papers by these leading\nAI companies. Our analysis reveals where corporate attention is concentrated\nand where potential gaps lie.\n  Some AI research may stay unpublished for good reasons, such as to not inform\nadversaries about the details of security techniques they would need to\novercome to misuse AI systems. Therefore, we also considered the incentives\nthat AI companies have to research each approach, regardless of how much work\nthey have published on the topic.\n  We identified three categories where there are currently no or few papers and\nwhere we do not expect AI companies to become much more incentivized to pursue\nthis research in the future. These are model organisms of misalignment,\nmulti-agent safety, and safety by design. Our findings provide an indication\nthat these approaches may be slow to progress without funding or efforts from\ngovernment, civil society, philanthropists, or academia.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.17617,regular,post_llm,2024,9,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""Estimating The Carbon Footprint Of Digital Agriculture Deployment: A\n  Parametric Bottom-Up Modelling Approach\n\n  Digitalization appears as a lever to enhance agriculture sustainability.\nHowever, existing works on digital agriculture's own sustainability remain\nscarce, disregarding the environmental effects of deploying digital devices on\na large-scale. We propose a bottom-up method to estimate the carbon footprint\nof digital agriculture scenarios considering deployment of devices over a\ndiversity of farm sizes. It is applied to two use-cases and demonstrates that\ndigital agriculture encompasses a diversity of devices with heterogeneous\ncarbon footprints and that more complex devices yield higher footprints not\nalways compensated by better performances or scaling gains. By emphasizing the\nnecessity of considering the multiplicity of devices, and the territorial\ndistribution of farm sizes when modelling digital agriculture deployments, this\nstudy highlights the need for further exploration of the first-order effects of\ndigital technologies in agriculture.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.01499,regular,post_llm,2024,9,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Lecture Notes from the NaijaCoder Summer Camp\n\n  The NaijaCoder in-person summer camps are intensive programs for high school\nand pre-college students in Nigeria. The programs are meant to provide free\ninstruction on the basics of algorithms and computer programming. In 2024, the\ncamps were held in two locations within the country: (i) the Federal Capital\nTerritory (F.C.T.), Abuja; and (ii) Lagos state. Both locations relied on the\nsame set of notes for instructional purposes. We are providing these notes in a\npublicly-available medium for both students and teachers to review after the\nmain in-person programs are over.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.16601,regular,post_llm,2024,9,"{'ai_likelihood': 0.000358422597249349, 'text': 'Cyber Food Swamps: Investigating the Impacts of Online-to-Offline Food\n  Delivery Platforms on Healthy Food Choices\n\n  Online-to-offline (O2O) food delivery platforms have greatly expanded urban\nresidents\' access to a wide range of food options by allowing convenient\nordering from distant food outlets. However, concerns persist regarding the\nnutritional quality of delivered food, particularly as the impact of O2O food\ndelivery platforms on users\' healthy food remains unclear. This study leverages\nlarge-scale empirical data from a leading O2O delivery platform to\ncomprehensively analyze online food choice behaviors and how they are\ninfluenced by the online exposure to fast food restaurants, i.e., online food\nenvironment. Our analyses reveal significant variations in food preferences\nacross demographic groups and city sizes, where male, low-income, and younger\nusers are more likely to order fast food via O2O platforms. Besides, we also\nperform a comparative analysis on the food exposure differences in offline and\nonline environments, confirming that the extended service ranges of O2O\nplatforms can create larger ""cyber food swamps"". Furthermore, regression\nanalysis highlights that a higher ratio of fast food orders is associated with\n""cyber food swamps"", areas characterized by a higher proportion of accessible\nfast food restaurants. A 10% increase in this proportion raises the probability\nof ordering fast food by 22.0%. Moreover, a quasi-natural experiment\nsubstantiates the long-term causal effect of online food environment changes on\nhealthy food choices. These findings underscore the need for O2O food delivery\nplatforms to address the health implications of online food choice exposure,\noffering critical insights for stakeholders aiming to improve dietary health\namong urban populations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.01515,regular,post_llm,2024,9,"{'ai_likelihood': 2.132521735297309e-05, 'text': ""METcross: A framework for short-term forecasting of cross-city metro\n  passenger flow\n\n  Metro operation management relies on accurate predictions of passenger flow\nin the future. This study begins by integrating cross-city (including source\nand target city) knowledge and developing a short-term passenger flow\nprediction framework (METcross) for the metro. Firstly, we propose a basic\nframework for modeling cross-city metro passenger flow prediction from the\nperspectives of data fusion and transfer learning. Secondly, METcross framework\nis designed to use both static and dynamic covariates as inputs, including\neconomy and weather, that help characterize station passenger flow features.\nThis framework consists of two steps: pre-training on the source city and\nfine-tuning on the target city. During pre-training, data from the source city\ntrains the feature extraction and passenger flow prediction models. Fine-tuning\non the target city involves using the source city's trained model as the\ninitial parameter and fusing the feature embeddings of both cities to obtain\nthe passenger flow prediction results. Finally, we tested the basic prediction\nframework and METcross framework on the metro networks of Wuxi and Chongqing to\nexperimentally analyze their efficacy. Results indicate that the METcross\nframework performs better than the basic framework and can reduce the Mean\nAbsolute Error and Root Mean Squared Error by 22.35% and 26.18%, respectively,\ncompared to single-city prediction models.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.17155,review,post_llm,2024,9,"{'ai_likelihood': 2.2881560855441625e-05, 'text': ""Exploring the Boundaries of Content Moderation in Text-to-Image\n  Generation\n\n  This paper analyzes the community safety guidelines of five text-to-image\n(T2I) generation platforms and audits five T2I models, focusing on prompts\nrelated to the representation of humans in areas that might lead to societal\nstigma. While current research primarily focuses on ensuring safety by\nrestricting the generation of harmful content, our study offers a complementary\nperspective. We argue that the concept of safety is difficult to define and\noperationalize, reflected in a discrepancy between the officially published\nsafety guidelines and the actual behavior of the T2I models, and leading at\ntimes to over-censorship. Our findings call for more transparency and an\ninclusive dialogue about the platforms' content moderation practices, bearing\nin mind their global cultural and social impact.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.06801,review,post_llm,2024,9,"{'ai_likelihood': 1.0596381293402778e-06, 'text': ""Understanding and Mitigating the Impacts of Differentially Private\n  Census Data on State Level Redistricting\n\n  Data from the Decennial Census is published only after applying a disclosure\navoidance system (DAS). Data users were shaken by the adoption of differential\nprivacy in the 2020 DAS, a radical departure from past methods. The change\nraises the question of whether redistricting law permits, forbids, or requires\ntaking account of the effect of disclosure avoidance. Such uncertainty creates\nlegal risks for redistricters, as Alabama argued in a lawsuit seeking to\nprevent the 2020 DAS's deployment. We consider two redistricting settings in\nwhich a data user might be concerned about the impacts of privacy preserving\nnoise: drawing equal population districts and litigating voting rights cases.\nWhat discrepancies arise if the user does nothing to account for disclosure\navoidance? How might the user adapt her analyses to mitigate those\ndiscrepancies? We study these questions by comparing the official 2010\nRedistricting Data to the 2010 Demonstration Data -- created using the 2020 DAS\n-- in an analysis of millions of algorithmically generated state legislative\nredistricting plans. In both settings, we observe that an analyst may come to\nincorrect conclusions if they do not account for noise. With minor adaptations,\nthough, the underlying policy goals remain achievable: tweaking selection\ncriteria enables a redistricter to draw balanced plans, and illustrative plans\ncan still be used as evidence of the maximum number of majority-minority\ndistricts that are possible in a geography. At least for state legislatures,\nAlabama's claim that differential privacy ``inhibits a State's right to draw\nfair lines'' appears unfounded.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.01287,review,post_llm,2024,9,"{'ai_likelihood': 0.99755859375, 'text': 'Comprehensive up-to-date impact of the IoMT in healthcare and patients\n\n  The Internet of Medical Things (IoMT) is a quickly expanding field that\nintends to develop the features, effectiveness, and availability of healthcare\nservices by applying numerous technologies to gather and diffuse medical data.\nIoMT devices incorporate wearable sensors, implantable devices, smart home\nmethods, telemedicine policies, and mobile applications. IoMT applications\nrange from chronic disease administration, remote patient monitoring, emergency\nresponse, and clinical decision support to health promotion and wellness. This\npaper aligns on the advantages, defies, and outlook directions of this\ndeveloping domain. The paper also examines the ethical, legal, and social\nimplications of IoMT, as well as the possible risks and vulnerabilities of the\nIoMT environment\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1279296875, 'GPT4': 0.252197265625, 'CLAUDE': 0.00278472900390625, 'GOOGLE': 0.4306640625, 'OPENAI_O_SERIES': 0.11541748046875, 'DEEPSEEK': 0.0020694732666015625, 'GROK': 0.0004572868347167969, 'NOVA': 0.01800537109375, 'OTHER': 0.04412841796875, 'HUMAN': 0.006519317626953125}}"
2409.03462,review,post_llm,2024,9,"{'ai_likelihood': 4.867712656656901e-06, 'text': 'Automated Journalism\n\n  Developed as a response to the increasing popularity of data-driven\njournalism, automated journalism refers to the process of automating the\ncollection, production, and distribution of news content and other data with\nthe assistance of computer programs. Although the algorithmic technologies\nassociated with automated journalism remain in the initial stage of\ndevelopment, early adopters have already praised the usefulness of automated\njournalism for generating routine news based on clean, structured data. Most\nnoticeably, the Associated Press and The New York Times have been automating\nnews content to cover financial and sports issues for over a decade.\nNevertheless, research on automated journalism is also alerting to the dangers\nof using algorithms for news creation and distribution, including the possible\nbias behind AI systems or the human bias of those who develop computer\nprograms. The popularization of automated news content also has important\nimplications for the infrastructure of the newsroom, the role performance of\njournalists and other non-journalistic professionals, and the distribution of\nnews content to a datafied audience.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2409.20297,regular,post_llm,2024,9,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""Explain in Plain Language Questions with Indic Languages: Drawbacks,\n  Affordances, and Opportunities\n\n  Background: Introductory computer science courses use ``Explain in Plain\nEnglish'' (EiPE) activities to develop and assess students' code comprehension\nskills, but creating effective autograders for these questions is challenging\nand limited to English. This is a particular challenge in linguistically\ndiverse countries like India where students may have limited proficiency in\nEnglish.\n  Methods: We evaluate the efficacy of a recently introduced approach called\nCode Generation Based Grading (CGBG) in enabling language agnostic ``Explain in\nPlain Language'' (EiPL) activities. Here students' EiPL responses generate code\nthat is tested for functional equivalence to the original which was being\ndescribed.\n  Objectives: We initially evaluate the correctness of code generated from\ncorrect EiPL responses provided in 10 of India's most commonly spoken\nlanguages. To evaluate the effectiveness of the approach in practice, we assess\nstudent success and perceptions of EiPL questions in a NPTEL (National\nProgramme on Technology Enhanced Learning) course.\n  Results: We find promising results for the correctness of code generated from\ntranslations of correct EiPL responses, with most languages achieving a\ncorrectness rate of 75% or higher. However, in practice, many students\npreferred to respond in English due to greater familiarity with English as a\ntechnical language, difficulties writing in their native language, and\nperceptions of the grader being less capable of generating code from prompts in\ntheir mother tongue.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.05012,regular,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': 'Evaluating 21st Century Skills Development through Makerspace Workshops\n  in Computer Science Education\n\n  This study evaluates the effectiveness of incorporating makerspace workshops\ninto computer science education by assessing 21st century skills critical\nthinking, collaboration, communication, and creativity before and after the\nintervention. Using a pre test and post test approach with the ""21st Century\nSkills Survey Instrument,"" the study quantifies the impact of makerspace\nactivities on student skill development. Participants included students\nenrolled in two computer science courses at Cyprus College. Statistical\nanalysis, conducted using Python, revealed significant improvements across all\nassessed skills, indicating that makerspace workshops enhance essential\ncompetencies needed for the modern workforce. These findings provide valuable\ninsights into how experiential learning environments can transform traditional\ncomputer science education, promoting a more interactive and engaging learning\nexperience. Future research should focus on larger, more diverse samples and\nexplore specific components of makerspace activities that most effectively\ncontribute to skill development.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.324052810668945e-05, 'GPT4': 0.0014448165893554688, 'CLAUDE': 0.0017843246459960938, 'GOOGLE': 0.99560546875, 'OPENAI_O_SERIES': 0.0008530616760253906, 'DEEPSEEK': 0.00010865926742553711, 'GROK': 1.919269561767578e-05, 'NOVA': 2.4020671844482422e-05, 'OTHER': 0.00013387203216552734, 'HUMAN': 3.5762786865234375e-07}}"
2410.19783,regular,post_llm,2024,10,"{'ai_likelihood': 0.3672960069444445, 'text': ""Exploring Older Adults' Perceptions and Experiences with Online Dating\n\n  The rise of online dating apps has transformed how individuals connect and\nseek companionship, with an increase in usage among older adults. While these\nplatforms offer opportunities for emotional support and social connection, they\nalso present significant challenges, including a concerning trend of online\ndating scams targeting this demographic. To address these issues, we conducted\na semi-structured interview focused on the online dating experiences of older\nadults (65+). Initially, we conducted a pre-screening survey, followed by\nfocused semi-structured interviews with 11 of the selected older adults.\nThrough this study, we investigate older adults' security and privacy concerns,\nthe significance of design elements and accessibility, and identify areas\nneeding improvement. Our findings reveal challenges such as deceptive\npractices, including catfishing and fraud, concerns over disclosing sensitive\ninformation, non-inclusive app design features, and the need for more\ninformative visualization of match requests. We offer recommendations for\nenhanced identity verification, inclusive privacy controls by app developers,\nand increased digital literacy efforts to enable older adults to navigate these\nplatforms safely and confidently.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.20709,regular,post_llm,2024,10,"{'ai_likelihood': 3.046459621853299e-06, 'text': ""Let a million entrepreneurs grow!\n\n  India produces about nine hundred thousand (900K) engineers annually, and\nmany seek computer science and related technology jobs. Given that the IT\nworkforce in India is still young, new graduates get jobs only when the\nindustry grows. A liberal estimate based on the data from MeitY (Ministry of\nElectronics and Information Technology) and NASSCOM puts the annual job growth\nto three hundred thousand (300K), less than one-third of the graduation rate.\nIn other words, about half a million graduates don't get a job every year (even\nwhen we consider that some students don't opt for jobs or go for higher\nstudies).\n  This position paper demonstrates that given the current growth rate of the\nIndian economy, such a significant shortfall will continue to exist. It then\nproposes a way to address this shortfall.\n  The paper proposes to develop micro-entrepreneurs at scale, enabling many\ngraduates to start micro-enterprises focused on AI, Software, and Technology\n(MAST). These MAST enterprises offer technology products and services to meet\nthe hyperlocal needs of the businesses and individuals in the local community\n(a retailer in the neighborhood, a high net-worth person, or a factory).\n  Such an endeavor will require curricular, policy, and societal interventions.\nThe paper presents an approach to enable MAST education across campuses,\noutlining the key curricular changes required and important policies that must\nbe created and implemented.\n  This supply-demand gap is an existential problem for engineering education in\nIndia, and this position paper aims to trigger debates and collaborations to\ndevise solutions that will work at India scale.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.08555,review,post_llm,2024,10,"{'ai_likelihood': 7.086329989963108e-06, 'text': ""Design of Secure, Privacy-focused, and Accessible E-Payment Applications\n  for Older Adults\n\n  E-payments are essential for transactional convenience in today's digital\neconomy and are becoming increasingly important for older adults, emphasizing\nthe need for enhanced security, privacy, and usability. To address this, we\nconducted a survey-based study with 400 older adults aged 60 and above to\nevaluate a high-fidelity prototype of an e-payment mobile application, which\nincluded features such as multi-factor authentication (MFA) and QR code-based\nrecipient addition. Based on our findings, we developed a tailored \\b{eta}\nversion of the application to meet the specific needs of this demographic.\nNotably, approximately 91% of participants preferred traditional\nknowledge-based and single-mode authentication compared to expert-recommended\nMFA. We concluded by providing recommendations aimed at developing inclusive\ne-payment solutions that address the security, privacy, and usability\nrequirements of older adults.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.03286,regular,post_llm,2024,10,"{'ai_likelihood': 0.99267578125, 'text': 'Computational Diplomacy: How ""hackathons for good"" feed a participatory\n  future for multilateralism in the digital age\n\n  This article explores the role of hackathons for good in building a community\nof software and hardware developers focused on addressing global SDG\nchallenges. We theorise this movement as computational diplomacy: a\ndecentralised, participatory process for digital governance that leverages\ncollective intelligence to tackle major global issues. Analysing Devpost and\nGitHub data reveals that 30% of hackathons since 2010 have addressed SDG\ntopics, employing diverse technologies to create innovative solutions.\nHackathons serve as crucial kairos moments, sparking innovation bursts that\ndrive both immediate project outcomes and long-term production. We propose that\nthese events harness the neurobiological basis of human cooperation and\nempathy, fostering a collective sense of purpose and reducing interpersonal\nprejudice. This bottom-up approach to digital governance integrates software\ndevelopment, human collective intelligence, and collective action, creating a\ndynamic model for transformative change. By leveraging kairos moments,\ncomputational diplomacy promotes a more inclusive and effective model for\ndigital multilateral governance of the future.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0005617141723632812, 'GPT4': 0.0004930496215820312, 'CLAUDE': 0.85546875, 'GOOGLE': 0.13720703125, 'OPENAI_O_SERIES': 0.000957489013671875, 'DEEPSEEK': 0.002101898193359375, 'GROK': 2.682209014892578e-06, 'NOVA': 5.1856040954589844e-05, 'OTHER': 0.0013933181762695312, 'HUMAN': 0.002017974853515625}}"
2410.06476,regular,post_llm,2024,10,"{'ai_likelihood': 2.715322706434462e-06, 'text': ""Quantitative Theory of Meaning. Application to Financial Markets.\n  EUR/USD case study\n\n  The paper focuses on the link between information, investors' expectations\nand market price movement. EUR/USD market is examined from\ncommunication-theoretical perspective on the dynamics of information and\nmeaning. We build upon the quantitative theory of meaning as a complement to\nthe quantitative theory of information. Different groups of investors entertain\ndifferent criteria to process information, so that the same information can be\nsupplied with different meanings. Meanings shape investors' expectations which\nare revealed in market asset price movement. This dynamics can be captured by\nnon-linear evolutionary equation. We use a computationally efficient technique\nof logistic Continuous Wavelet Transformation (CWT) to analyze EUR/USD market.\nThe results reveal the latent EUR/USD trend structure which coincides with the\nmodel predicted time series indicating that proposed model can adequately\ndescribe some patterns of investors' behavior. Proposed methodology can be used\nto better understand and forecast future market assets' price movement.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.09645,review,post_llm,2024,10,"{'ai_likelihood': 0.001956091986762153, 'text': 'AI Model Registries: A Foundational Tool for AI Governance\n\n  In this report, we propose the implementation of national registries for\nfrontier AI models as a foundational tool for AI governance. We explore the\nrationale, design, and implementation of such registries, drawing on\ncomparisons with registries in analogous industries to make recommendations for\na registry that is efficient, unintrusive, and which will bring AI governance\ncloser to parity with the governmental insight into other high-impact\nindustries. We explore key information that should be collected, including\nmodel architecture, model size, compute and data used during training, and we\nsurvey the viability and utility of evaluations developed specifically for AI.\nOur proposal is designed to provide governmental insight and enhance AI safety\nwhile fostering innovation and minimizing the regulatory burden on developers.\nBy providing a framework that respects intellectual property concerns and\nsafeguards sensitive information, this registry approach supports responsible\nAI development without impeding progress. We propose that timely and accurate\nregistration should be encouraged primarily through injunctive action, by\nrequiring third parties to use only registered models, and secondarily through\ndirect financial penalties for non-compliance. By providing a comprehensive\nframework for AI model registries, we aim to support policymakers in developing\nfoundational governance structures to monitor and mitigate risks associated\nwith advanced AI systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.16562,review,post_llm,2024,10,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Vernacularizing Taxonomies of Harm is Essential for Operationalizing\n  Holistic AI Safety\n\n  Operationalizing AI ethics and safety principles and frameworks is essential\nto realizing the potential benefits and mitigating potential harms caused by AI\nsystems. To that end, actors across industry, academia, and regulatory bodies\nhave created formal taxonomies of harm to support operationalization efforts.\nThese include novel holistic methods that go beyond exclusive reliance on\ntechnical benchmarking. However, our paper argues that such taxonomies must\nalso be transferred into local categories to be readily implemented in\nsector-specific AI safety operationalization efforts, and especially in\nunderresourced or high-risk sectors. This is because many sectors are\nconstituted by discourses, norms, and values that ""refract"" or even directly\nconflict with those operating in society more broadly. Drawing from emerging\nanthropological theories of human rights, we propose that the process of\n""vernacularization""--a participatory, decolonial practice distinct from\ndoctrinary ""translation"" (the dominant mode of AI safety\noperationalization)--can help bridge this gap. To demonstrate this point, we\nconsider the education sector, and identify precisely how vernacularizing a\nleading holistic taxonomy of harm leads to a clearer view of how harms AI\nsystems may cause are substantially intensified when deployed in educational\nspaces. We conclude by discussing the generalizability of vernacularization as\na useful AI safety methodology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.09219,review,post_llm,2024,10,"{'ai_likelihood': 2.1954377492268882e-05, 'text': 'Understanding the First Wave of AI Safety Institutes: Characteristics,\n  Functions, and Challenges\n\n  In November 2023, the UK and US announced the creation of their AI Safety\nInstitutes (AISIs). Five other jurisdictions have followed in establishing\nAISIs or similar institutions, with more likely to follow. While there is\nconsiderable variation between these institutions, there are also key\nsimilarities worth identifying.\n  This primer describes one cluster of similar AISIs, the ""first wave,""\nconsisting of the Japan, UK, and US AISIs. First-wave AISIs have several\nfundamental characteristics in common: they are technical government\ninstitutions, have a clear mandate related to the safety of advanced AI\nsystems, and lack regulatory powers.\n  Safety evaluations are at the center of first-wave AISIs. These techniques\ntest AI systems across tasks to understand their behavior and capabilities on\nrelevant risks, such as cyber, chemical, and biological misuse. They also share\nthree core functions: research, standards, and cooperation. These functions are\ncritical to AISIs\' work on safety evaluations but also support other activities\nsuch as scientific consensus-building and foundational AI safety research.\n  Despite its growing popularity as an institutional model, the AISI model is\nnot free from challenges and limitations. Some analysts have criticized the\nfirst wave of AISIs for specializing too much in a sub-area and for being\npotentially redundant with existing institutions, for example.\n  Future developments may rapidly change this landscape, and particularities of\nindividual AISIs may not be captured by our broad-strokes description. This\npolicy brief aims to outline the core elements of first-wave AISIs as a way of\nencouraging and improving conversations on this novel institutional model,\nacknowledging this is just a simplified snapshot rather than a timeless\nprescription.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.17388,review,post_llm,2024,10,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""ICT Sector Greenhouse Gas Emissions -- Issues and Trends\n\n  As Information and Communication Technology (ICT) use has become more\nprevalent, there has been a growing concern in how its associated greenhouse\ngas emissions will impact the climate. Estimating such ICT emissions is a\ndifficult undertaking due to its complexity, its rapidly changing nature, and\nthe lack of accurate and up-to-date data on individual stakeholder emissions.\nIn this paper we provide a framework for estimating ICT's carbon footprint and\nidentify some of the issues that impede the task. We attempt to gain greater\ninsight into the factors affecting the ICT sector by drawing on a number of\ninterviews with industry experts. We conclude that more accurate emissions\nestimates will only be possible with a more more detailed, industry informed,\nunderstanding of the whole ICT landscape and much more transparent reporting of\nenergy usage and emissions data by ICT stakeholders.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.16462,regular,post_llm,2024,10,"{'ai_likelihood': 0.00013709068298339844, 'text': ""Comparative Analysis of Human Mobility Patterns: Utilizing Taxi and\n  Mobile (SafeGraph) Data to Investigate Neighborhood-Scale Mobility in New\n  York City\n\n  Numerous researchers have utilized GPS-enabled vehicle data and SafeGraph\nmobility data to analyze human movements. However, the comparison of their\nability to capture human mobility remains unexplored. This study investigates\ndifferences in human mobility using taxi trip records and the SafeGraph dataset\nin New York City neighborhoods. The analysis includes neighborhood clustering\nto identify population characteristics and a comparative analysis of mobility\npatterns. Our findings show that taxi data tends to capture human mobility to\nand from locations such as Lower Manhattan, where taxi demand is consistently\nhigh, while often underestimating the volume of trips originating from areas\nwith lower taxi demand, particularly in the suburbs of NYC. In contrast,\nSafeGraph data excels in capturing trips to and from areas where commuting by\ndriving one's own car is common, but underestimates trips in pedestrian-heavy\nareas. The comparative analysis also sheds new light on transportation mode\nchoices for trips across various neighborhoods. The results of this study\nunderscore the importance of understanding the representativeness of human\nmobility big data and highlight the necessity for careful consideration when\nselecting the most suitable dataset for human mobility research.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.09985,review,post_llm,2024,10,"{'ai_likelihood': 0.05984836154513889, 'text': 'Responsible AI in the Global Context: Maturity Model and Survey\n\n  Responsible AI (RAI) has emerged as a major focus across industry,\npolicymaking, and academia, aiming to mitigate the risks and maximize the\nbenefits of AI, both on an organizational and societal level. This study\nexplores the global state of RAI through one of the most extensive surveys to\ndate on the topic, surveying 1000 organizations across 20 industries and 19\ngeographical regions. We define a conceptual RAI maturity model for\norganizations to map how well they implement organizational and operational RAI\nmeasures. Based on this model, the survey assesses the adoption of system-level\nmeasures to mitigate identified risks related to, for example, discrimination,\nreliability, or privacy, and also covers key organizational processes\npertaining to governance, risk management, and monitoring and control. The\nstudy highlights the expanding AI risk landscape, emphasizing the need for\ncomprehensive risk mitigation strategies. The findings also reveal significant\nstrides towards RAI maturity, but we also identify gaps in RAI implementation\nthat could lead to increased (public) risks from AI systems. This research\noffers a structured approach to assess and improve RAI practices globally and\nunderscores the critical need for bridging the gap between RAI planning and\nexecution to ensure AI advancement aligns with human welfare and societal\nbenefits.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.1423,regular,post_llm,2024,10,"{'ai_likelihood': 7.781717512342666e-06, 'text': 'Global Inequalities in the Production of Artificial Intelligence: A Four-Country Study on Data Work\n\nLabor plays a major, albeit largely unrecognized role in the development of artificial intelligence. Machine learning algorithms are predicated on data-intensive processes that rely on humans to execute repetitive and difficult-to-automate, but no less essential, tasks such as labeling images, sorting items in lists, recording voice samples, and transcribing audio files. Online platforms and networks of subcontractors recruit data workers to execute such tasks in the shadow of AI production, often in lower-income countries with long-standing traditions of informality and lessregulated labor markets. This study unveils the resulting complexities by comparing the working conditions and the profiles of data workers in Venezuela, Brazil, Madagascar, and as an example of a richer country, France. By leveraging original data collected over the years 2018-2023 via a mixed-method design, we highlight how the cross-country supply chains that link data workers to core AI production sites are reminiscent of colonial relationships, maintain historical economic dependencies, and generate inequalities that compound with those inherited from the past. The results also point to the importance of less-researched, non-English speaking countries to understand key features of the production of AI solutions at planetary scale.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.08913,review,post_llm,2024,10,"{'ai_likelihood': 0.98291015625, 'text': 'System Reliability Engineering in the Age of Industry 4.0: Challenges\n  and Innovations\n\n  In the era of Industry 4.0, system reliability engineering faces both\nchallenges and opportunities. On the one hand, the complexity of cyber-physical\nsystems, the integration of novel numerical technologies, and the handling of\nlarge amounts of data pose new difficulties for ensuring system reliability. On\nthe other hand, innovations such as AI-driven prognostics, digital twins, and\nIoT-enabled systems enable the implementation of new methodologies that are\ntransforming reliability engineering. Condition-based monitoring and predictive\nmaintenance are examples of key advancements, leveraging real-time sensor data\ncollection and AI to predict and prevent equipment failures. These approaches\nreduce failures and downtime, lower costs, and extend equipment lifespan and\nsustainability. However, it also brings challenges such as data management,\nintegrating complexity, and the need for fast and accurate models and\nalgorithms. Overall, the convergence of advanced technologies in Industry 4.0\nrequires a rethinking of reliability tasks, emphasising adaptability and\nreal-time data processing. In this chapter, we propose to review recent\ninnovations in the field, related methods and applications, as well as\nchallenges and barriers that remain to be explored. In the red lane, we focus\non smart manufacturing and automotive engineering applications with\nsensor-based monitoring and driver assistance systems.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1396484375, 'GPT4': 0.79345703125, 'CLAUDE': 0.00011008977890014648, 'GOOGLE': 0.052154541015625, 'OPENAI_O_SERIES': 0.0008344650268554688, 'DEEPSEEK': 7.510185241699219e-06, 'GROK': 1.0728836059570312e-06, 'NOVA': 2.6226043701171875e-06, 'OTHER': 0.0007700920104980469, 'HUMAN': 0.01308441162109375}}"
2410.01712,regular,post_llm,2024,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Scaffolding Research Projects in Theory of Computing Courses\n\n  Theory of Computing (ToC) is an important course in CS curricula because of\nits connections to other CS courses as a foundation for them. Traditional ToC\ncourse grading schemes are mostly exam-based, and sometimes a small weight for\ntraditional proof-type assignments. Recent work experimented with a new type of\nassignment, namely a ``mock conference'' project wherein students approach and\npresent ToC problems as if they were submitting to a ``real'' CS conference. In\nthis paper we massively scaffold this existing project and provide our\nexperiences in running such a conference in our own ToC course.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.0101,regular,post_llm,2024,10,"{'ai_likelihood': 6.357828776041667e-06, 'text': 'Code Interviews: Design and Evaluation of a More Authentic Assessment\n  for Introductory Programming Assignments\n\n  Generative artificial intelligence poses new challenges around assessment,\nincreasingly driving introductory programming educators to employ invigilated\nexams. But exams do not afford more authentic programming experiences that\ninvolve planning, implementing, and debugging programs with computer\ninteraction. In this experience report, we describe code interviews: a more\nauthentic assessment method for take-home programming assignments. Through\naction research, we experimented with varying the number and type of questions\nas well as whether interviews were conducted individually or with groups of\nstudents. To scale the program, we converted most of our weekly teaching\nassistant (TA) sections to conduct code interviews on 5 major weekly take-home\nprogramming assignments. By triangulating data from 5 sources, we identified 4\nthemes. Code interviews (1) pushed students to discuss their work, motivating\nmore nuanced but sometimes repetitive insights; (2) enabled peer learning,\nreducing stress in some ways but increasing stress in other ways; (3) scaled\nwith TA-led sections, replacing familiar practice with an unfamiliar\nassessment; (4) focused on student contributions, limiting opportunities for\nTAs to give guidance and feedback. We conclude by discussing the different\ndecisions about the design of code interviews with implications for student\nexperience, academic integrity, and teaching workload.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.12115,regular,post_llm,2024,10,"{'ai_likelihood': 1.4371342129177518e-05, 'text': 'A Web App for Teaching Finite State Automata\n\n  We present the open-source tool finsm.io, a tool for creating, simulating and\nexporting deterministic and non-deterministic finite state automata (DFA/NFA).\nWe first describe the conceptual background on which the tool is based,\nfollowed by a description of features and preliminary evaluation of the tool\nbased on use spanning multiple years and hundreds of student users. Preliminary\nevaluation found that instructors and students overwhelmingly recommend the\ntool to others and agree that it has improved their learning and teaching. The\nauthors invite interested educators to use the tool in their finite automata\ncourses.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.17155,regular,post_llm,2024,10,"{'ai_likelihood': 1.2252065870496963e-06, 'text': 'AI Future Envisioning with PLACARD\n\n  At EuroPLoP 2024 Mary Tedeschi led the ""AI Future Envisioning with PLACARD""\nfocus group in Germany. Three conference attendees joined in the room while\nSridevi, Paola, and Charles co-facilitated remotely via a web conference. The\nparticipants were introduced to a Futures Studies technique with the goal of\ncapturing envisionments of Artificial Intelligence (AI) going forward. To set\nan atmosphere a technology focused card game was used to make the session more\ninteractive. To close everyone co-created a Project Action Review to recap of\nthe event to capture learnings that has been summarized in this paper. The\nFocus Group was structured based on lessons learned over six earlier\niterations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.06954,regular,post_llm,2024,10,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'How Unique is Whose Web Browser? The role of demographics in browser\n  fingerprinting among US users\n\n  Browser fingerprinting can be used to identify and track users across the\nWeb, even without cookies, by collecting attributes from users\' devices to\ncreate unique ""fingerprints"". This technique and resulting privacy risks have\nbeen studied for over a decade. Yet further research is limited because prior\nstudies used data not publicly available. Additionally, data in prior studies\nlacked user demographics. Here we provide a first-of-its-kind dataset to enable\nfurther research. It includes browser attributes with users\' demographics and\nsurvey responses, collected with informed consent from 8,400 US study\nparticipants. We use this dataset to demonstrate how fingerprinting risks\ndiffer across demographic groups. For example, we find lower income users are\nmore at risk, and find that as users\' age increases, they are both more likely\nto be concerned about fingerprinting and at real risk of fingerprinting.\nFurthermore, we demonstrate an overlooked risk: user demographics, such as\ngender, age, income level and race, can be inferred from browser attributes\ncommonly used for fingerprinting, and we identify which browser attributes most\ncontribute to this risk. Our data collection process also conducted an\nexperiment to study what impacts users\' likelihood to share browser data for\nopen research, in order to inform future data collection efforts, with\nresponses from 12,461 total participants. Female participants were\nsignificantly less likely to share their browser data, as were participants who\nwere shown the browser data we asked to collect. Overall, we show the important\nrole of user demographics in the ongoing work that intends to assess\nfingerprinting risks and improve user privacy, with findings to inform future\nprivacy enhancing browser developments. The dataset and data collection tool we\nprovide can be used to further study research questions not addressed in this\nwork.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.23443,review,post_llm,2024,10,"{'ai_likelihood': 0.005972120496961806, 'text': ""The Transformative Impact of AI and Deep Learning in Business: A\n  Literature Review\n\n  This paper aims to review the radical role of AI and deep learning in various\nfunctional areas of the business, such as marketing, finance, operations, human\nresources and customer service. Thus, based on the overview of the latest\nresearch and practices focusing on AI technologies in different industries, the\npossibilities of improving organizational efficiency by personalized AI for\nmaking decisions based on big data and personalizing clients' interactions with\norganizations are presented and discussed. Several operational issues, ethical\nconcerns, and regulatory concerns have also been discussed in the review of the\nliterature. Moreover, it covers material applications in the healthcare sector,\nthe retail and manufacturing industry, agriculture and farming, and finance\nbefore considering possible future developments and themes for further\ninvestigation. Drawing from this revolutionary ethnographic review,\norganizations aiming to implement strategic and responsible optimization\nbenefit from detailed guides.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.23704,review,post_llm,2024,10,"{'ai_likelihood': 8.020136091444228e-05, 'text': 'Using Scenario-Writing for Identifying and Mitigating Impacts of\n  Generative AI\n\n  Impact assessments have emerged as a common way to identify the negative and\npositive implications of AI deployment, with the goal of avoiding the downsides\nof its use. It is undeniable that impact assessments are important - especially\nin the case of rapidly proliferating technologies such as generative AI. But it\nis also essential to critically interrogate the current literature and practice\non impact assessment, to identify its shortcomings, and to develop new\napproaches that are responsive to these limitations. In this provocation, we do\njust that by first critiquing the current impact assessment literature and then\nproposing a novel approach that addresses our concerns: Scenario-Based\nSociotechnical Envisioning.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.13875,regular,post_llm,2024,10,"{'ai_likelihood': 4.0398703681098095e-06, 'text': 'SpaceRaceEdu: developing an educational multi-player videogame for\n  self-study and assessment\n\n  The teaching innovation project SpaceRaceEdu: development of an educational\nmultiplayer video game for self-study and self-assessment has been carried out\nunder the INNOVA call of the Autonomous University of Madrid during the\n2022-2023 academic year. In this project, a functional prototype of\nSpaceRaceEdu has been developed: a multiplayer video game with a social and\neducational nature, which can be used both by teachers as a training and\nevaluation activity and by students as a tool for study and evaluation. In\nSpaceRaceEdu, several student teams try to launch a rocket before everyone\nelse. To meet this objective, they must gather a series of resources by going\nthrough a scenario and answering questions of different types. The teachers can\nintroduce these questions according to the contents of their subject. The\nvideogame balances competition and cooperation to promote participation and\nlearning. Competition occurs between teams who strive to answer all their\nquestions correctly before their rivals. In contrast, cooperation occurs\nbetween students on the same team who can organize and support each other to be\nmore effective.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.10233,review,post_llm,2024,10,"{'ai_likelihood': 0.99755859375, 'text': ""The Importance of Justified Patient Trust in unlocking AI's potential in\n  mental healthcare\n\n  Without trust, patients may hesitate to engage with AI systems, significantly\nlimiting the technology's potential in mental healthcare. This paper focuses\nspecifically on the trust that mental health patients, as direct users, must\nhave in AI systems, highlighting the most sensitive and direct relationship\nbetween AI systems and those whose mental healthcare is impacted by them. We\nexplore the concept of justified trust, why it is important for patient\npositive care outcomes, and the strategies needed to foster and maintain this\ntrust. By examining these aspects, we highlight how cultivating justified trust\nis key to unlocking AI's potential impact in mental healthcare.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0011968612670898438, 'GPT4': 0.002960205078125, 'CLAUDE': 0.00010800361633300781, 'GOOGLE': 0.99560546875, 'OPENAI_O_SERIES': 1.800060272216797e-05, 'DEEPSEEK': 1.0967254638671875e-05, 'GROK': 1.1920928955078125e-06, 'NOVA': 1.0728836059570312e-06, 'OTHER': 4.5299530029296875e-05, 'HUMAN': 0.00010734796524047852}}"
2410.01032,regular,post_llm,2024,10,"{'ai_likelihood': 3.599458270602756e-05, 'text': 'Teaching Cloud Infrastructure and Scalable Application Deployment in an\n  Undergraduate Computer Science Program\n\n  Making successful use of cloud computing requires nuanced approaches to both\nsystem design and deployment methodology, involving reasoning about the\nelasticity, cost, and security models of cloud services. Building cloud-native\napplications without a firm understanding of the fundamentals of cloud\nengineering can leave students susceptible to cost and security pitfalls. Yet,\ncloud computing is not commonly taught at the undergraduate level. To address\nthis gap, we designed an undergraduate-level course that frames cloud\ninfrastructure deployment as a software engineering practice. Our course\nfeatured a number of hands-on assignments that gave students experience with\nmodern, best-practice concepts and tools including infrastructure-as-code\n(IaC). We describe the design of the course, our experience teaching its\ninitial offering, and provide our reflections on what worked well and potential\nareas for improvement. Our course material is available at\nhttps://infracourse.cloud.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.09708,review,post_llm,2024,10,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'The role of broadband connectivity in achieving Sustainable Development\n  Goals (SDGs)\n\n  Broadband connectivity is a tool for catalyzing socio-economic development\nand reducing the societal inequalities. Recent studies have investigated the\nsupporting role of broadband in addressing Sustainable Development Goals\n(SDGs). Relationally, emerging ultra-dense broadband networks such as 5/6G have\nbeen linked to increased power consumption and more carbon footprint. With SDGs\nrecognized as interdependent and addressing one should not jeopardize the\nachievement of the other, there is need for sustainability research. Despite\nthe need to narrow the digital divide and address the SDGs by 2030, it is\nsurprising that limited comprehensive studies exist on broadband\nsustainability. To this end, we review 113 peer reviewed journals focusing on\nsix key areas (SDGs addressed, application areas, country income, technology,\nmethodology and spatial focus). We further discuss our findings before making\nfour key recommendations on broadband sustainability research to fast-track SDG\nachievement by 2030 especially for developing economies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.19574,review,post_llm,2024,10,"{'ai_likelihood': 4.198816087510851e-05, 'text': ""Institutional Review Boards as Soft Governance Mechanisms of R&D:\n  Governing the R&D of AI-based Medical Products\n\n  Risk-based approaches to governance bear an ambiguous stance regarding the\nResearch and Development stages of AI, for they the possibility of explicit\nrisks before they are posed by a given finalised product. In this context,\nInstitutional Review Boards (IRBs) stand as unique governance mechanisms,\ncapable of addressing the step from general research to concrete product\ndevelopment. However, IRBs face several challenges in governing AI-based\nmedical products, including: (a) achieving consistency, (b) being exhaustive,\n(c) ensuring process transparency, and (d) reducing the existing capacity and\nknowledge asymmetry between different stakeholders. This article explores four\ngovernance levers that can be used to effect change, four governance\nentry-points throughout a product's lifecycle, and five different behaviours\nthat IRBs should try to advance to ensure the effective governance of the R&D\nstages of AI-based medical projects. In doing so, IRBs can seize the unique\nopportunity they present to bring principles into practice, increase research\nquality, reduce governance costs, and bridge the knowledge gap between\nstakeholders.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.14825,regular,post_llm,2024,10,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Redesigning Service Level Agreements: Equity and Efficiency in City\n  Government Operations\n\n  We consider government service allocation -- how the government allocates\nresources (e.g., maintenance of public infrastructure) over time. It is\nimportant to make these decisions efficiently and equitably -- though these\ndesiderata may conflict. In particular, we consider the design of Service Level\nAgreements (SLA) in city government operations: promises that incidents such as\npotholes and fallen trees will be responded to within a certain time. We model\nthe problem of designing a set of SLAs as an optimization problem with equity\nand efficiency objectives under a queuing network framework; the city has two\ndecision levers: how to allocate response budgets to different neighborhoods,\nand how to schedule responses to individual incidents. We: (1) Theoretically\nanalyze a stylized model and find that the ""price of equity"" is small in\nrealistic settings; (2) Develop a simulation-optimization framework to optimize\npolicies in practice; (3) Apply our framework empirically using data from NYC,\nfinding that: (a) status quo inspections are highly inefficient and inequitable\ncompared to optimal ones, and (b) in practice, the equity-efficiency trade-off\nis not substantial: generally, inefficient policies are inequitable, and vice\nversa.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.13009,review,post_llm,2024,10,"{'ai_likelihood': 1.3543499840630426e-05, 'text': 'Is ETHICS about ethics? Evaluating the ETHICS benchmark\n\n  ETHICS is probably the most-cited dataset for testing the ethical\ncapabilities of language models. Drawing on moral theory, psychology, and\nprompt evaluation, we interrogate the validity of the ETHICS benchmark. Adding\nto prior work, our findings suggest that having a clear understanding of ethics\nand how it relates to empirical phenomena is key to the validity of ethics\nevaluations for AI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.14222,review,post_llm,2024,10,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""Digital Humanities in the TIME-US Project: Richness and Contribution of\n  Interdisciplinary Methods for Labour History\n\n  In 2015, the Annales journal, traditionally open to interdisciplinary\napproaches in history, referred to 'the current historiographical moment [as]\ncall [ing] for an experimentation of approaches'. 1 Although this observation\ndid not exclusively refer to the new possibilities offered by the technological\nadvancements of the time -particularly in the field of artificial intelligence\n2 -it was nonetheless motivated by these rapid and numerous changes, which also\naffect the historiographical landscape. A year earlier, St\\'ephane Lamass\\'e\nand Philippe Rygiel spoke of the 'new frontiers of the historian', frontiers\nopened a few years earlier by the realisation of the unprecedented impact of\nnew technologies on historical practices, leading to a 'mutation des conditions\nde production et de diffusion des connaissances historiques, voire de la nature\nde celles-ci' ('transformation of the conditions of production and\ndissemination of historical knowledge, and even the nature of this knowledge').\n3 It was in this fertile ground, conducive to the cross-fertilisation of\napproaches, that the TIME-US project was born in 2016. TIME-US is directly the\nresult of this awareness and reflects the transformations induced by major\ntechnological advancements, disrupting not only our daily practices but also\nour historical practices. 1 Annales 2015, 216. 2 For example, convolutional\nneural networks, which have revolutionised the field of artificial\nintelligence, began to gain popularity just before the 2010s. 3 Translated by\nthe author. Lamass\\'e and Rygiel 2014. To quantify women's work in the past,\nlabour historians cannot rely on the classic sources of their discipline, which\nallow to produce large statistical data series, systematically treatable in the\nform of databases. What to do when such data are not available? Should the task\nsimply be abandoned? As Maria {\\AA}gren points out, the invisibility of women's\nparticipation in the labour market does not mean non-existence 8 ; there must\ntherefore be traces of it. To quantify women's economic activity, Sara Horrell\nand Jane Humphries, for example, turned to household budgets from 59 different\nsources (from Parliamentary Papers to autobiographical texts), which had never\nbefore been systematically used to identify women's work patterns and their\ncontribution to family income. 9 In her study A Bitter Living: Women, Markets,\nand Social Capital in Early Modern Germany published in 2003, Sheilagh Ogilvie\nused information contained in court records to identify activities carried out\nby women and the time spent on these activities. Court records were not\nintended to record such information; yet, in their testimonies, witnesses often\ndescribed in detail the activities they were engaged in while a crime was\nunfolding before their eyes. Sheilagh Ogilvie thus identified nearly 3000 such\nobservations. 10 These works have opened two main avenues for the TIME-US\nproject. First, making already digitised sources accessible in homogeneous\ncorpora. 11 Following the example of previous research, TIME-US mobilised\nvaried sources containing traces of professional activities carried out by\nwomen in France during the period studied: these include both printed (posters\nand petitions, working-class newspapers, and contemporary surveys on workers)\nand handwritten sources (labour court decisions, police reports, company\narchives, personal archives, surveys, petitions). 12 One of the project's\nobjectives was to gather and 8 {\\AA}gren 2018a, 144. 9 Horrell and Humphries\n1995.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.1212,regular,post_llm,2024,10,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'A Problem-Based Learning Approach to Teaching Design in CS1\n\n  Design skills are increasingly recognized as a core competency for software\nprofessionals. Unfortunately, these skills are difficult to teach because\ndesign requires freedom and open-ended thinking, but new designers require a\nstructured process to keep them from being overwhelmed by possibilities. We\nscaffolded this by creating worksheets for every Design Thinking step, and\nembedding them in a PowerPoint deck on which students can collaborate. We\npresent our experience teaching a team design project course to 200\nfirst-year-university students, taking them from user interviews to functional\nprototypes. To challenge and support every student in a class where high school\nprogramming experience ranged from zero hours to three computer science\ncourses, we gave teams the option of developing single-user or multi-user\n(distributed) web applications, using two Event-Driven Programming frameworks.\nWe identified common failure modes from previous years, and developed the\nscaffolded approach and problem definition to avoid them. The techniques\ndeveloped include using a ""game matrix"" for structured brainstorming and\ndeveloping projects that require students to empathize with users very\ndifferent from themselves. We present quantitative and qualitative evidence\nfrom surveys and focus groups that show how these strategies impacted learning,\nand the extent to which students\' awareness of the strategies led to the\ndevelopment of metacognitive abilities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.16294,other,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': 'Em\\\'ilias Podcast -- Mulheres na Computa\\c{c}\\~ao: Ampliando Horizontes\n  e Inspirando Carreiras em STEM\n\n  On October 3, 2024, the ""Em\\\'ilias Podcast -- Women in Computing"" celebrates\nits 5th anniversary, standing out as a platform that promotes the participation\nof women in STEM (an acronym for ""science, technology, engineering, and\nmathematics""). The podcast aims to provide a space for women in computing and\nrelated fields to share their experiences and highlight the various\nopportunities in Information and Communication Technology (ICT). The\nmethodology included a feedback survey with interviewees, conducted via Google\nForms, to assess their experience and determine whether they would recommend\nthe podcast. In addition, we analyzed audience data, which showed consistent\ngrowth over the five years. The results revealed that 100% of the interviewees\nwould recommend ""Em\\\'ilias Podcast,"" reflecting a high level of satisfaction\nwith the project. The average participation experience rating was 4.7 on a\nscale of 1 to 5, highlighting positive aspects such as the quality of the\nscript, the interview conduction, and the networking opportunities. The\naudience data also underscore the podcast\'s impact: with over 10,000\naccumulated downloads and plays, it is primarily listened to by people aged 23\nto 44, with 50.9% of the audience being female, demonstrating its relevance and\nreach. In conclusion, the feedback from interviewees and the audience data\nreinforce the podcast\'s positive impact and its crucial role in the inclusion\nof women in technology. The results highlight the importance of promoting the\nfield and its opportunities, contributing to a more inclusive and inspiring\nfuture. The data analysis demonstrates the podcast\'s effectiveness in engaging\nand expanding its audience, establishing it as a significant example of social\nimpact in ICT.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.056060791015625, 'GPT4': 0.380126953125, 'CLAUDE': 5.4001808166503906e-05, 'GOOGLE': 0.5595703125, 'OPENAI_O_SERIES': 0.0023136138916015625, 'DEEPSEEK': 2.2590160369873047e-05, 'GROK': 1.1920928955078125e-06, 'NOVA': 3.814697265625e-06, 'OTHER': 0.0019254684448242188, 'HUMAN': 1.2159347534179688e-05}}"
2410.22282,review,post_llm,2024,10,"{'ai_likelihood': 7.185671064588759e-06, 'text': ""Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced\n  by Large Language Models\n\n  The universal availability of ChatGPT and other similar tools since late 2022\nhas prompted tremendous public excitement and experimental effort about the\npotential of large language models (LLMs) to improve learning experience and\noutcomes, especially for learners from disadvantaged backgrounds. However,\nlittle research has systematically examined the real-world impacts of LLM\navailability on educational equity beyond theoretical projections and\ncontrolled studies of innovative LLM applications. To depict trends of post-LLM\ninequalities, we analyze 1,140,328 academic writing submissions from 16,791\ncollege students across 2,391 courses between 2021 and 2024 at a public,\nminority-serving institution in the US. We find that students' overall writing\nquality gradually increased following the availability of LLMs and that the\nwriting quality gaps between linguistically advantaged and disadvantaged\nstudents became increasingly narrower. However, this equitizing effect was more\nconcentrated on students with higher socioeconomic status. These findings shed\nlight on the digital divides in the era of LLMs and raise questions about the\nequity benefits of LLMs in early stages and highlight the need for researchers\nand practitioners on developing responsible practices to improve educational\nequity through LLMs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.03102,review,post_llm,2024,10,"{'ai_likelihood': 5.364418029785156e-06, 'text': 'Examining Racial Stereotypes in YouTube Autocomplete Suggestions\n\n  Autocomplete is a popular search feature that predicts queries based on user\ninput and guides users to a set of potentially relevant suggestions. In this\nstudy, we examine what YouTube autocompletes suggest to users seeking\ninformation about race on the platform. Specifically, we perform an algorithm\noutput audit of autocomplete suggestions for input queries about four racial\ngroups and examine the stereotypes they embody. Using critical discourse\nanalysis, we identify five major sociocultural contexts in which racial\ninformation appears -Appearance, Ability, Culture, Social Equity, and Manner.\nWe found that the participatory nature of YouTube produces a multifaceted\nrepresentation of race-related content in its search outputs, characterized by\nenduring historical biases, aggregated discrimination, and interracial\ntensions, while simultaneously depicting minority resistance and aspirations of\na post-racial society. We call for innovations in content moderation policy\ndesign and enforcement to address existing racial harms in YouTube search\noutputs.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.16315,regular,post_llm,2024,10,"{'ai_likelihood': 0.013249715169270834, 'text': 'Why AI Is WEIRD and Should Not Be This Way: Towards AI For Everyone,\n  With Everyone, By Everyone\n\n  This paper presents a vision for creating AI systems that are inclusive at\nevery stage of development, from data collection to model design and\nevaluation. We address key limitations in the current AI pipeline and its WEIRD\nrepresentation, such as lack of data diversity, biases in model performance,\nand narrow evaluation metrics. We also focus on the need for diverse\nrepresentation among the developers of these systems, as well as incentives\nthat are not skewed toward certain groups. We highlight opportunities to\ndevelop AI systems that are for everyone (with diverse stakeholders in mind),\nwith everyone (inclusive of diverse data and annotators), and by everyone\n(designed and developed by a globally diverse workforce).\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.17296,regular,post_llm,2024,10,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""The Internet of Forgotten Things: European Cybersecurity Regulation and IoT Manufacturer Cessation\n\nMany modern consumer devices rely on network connections and cloud services to perform their core functions. This dependency is especially present in Internet of Things (IoT) devices, which combine hardware and software with network connections (e.g., a 'smart' doorbell with a camera). This paper argues that current European product legislation, which aims to protect consumers of, inter alia, IoT devices, has a blind spot for an increasing problem in the competitive IoT market: manufacturer cessation. Without the manufacturer's cloud servers, many IoT devices cannot perform core functions such as data analysis. If an IoT manufacturer ceases their operations, consumers of the manufacturer's devices are thus often left with a dysfunctional device and, as the paper shows, hardly any legal remedies. This paper therefore investigates three properties that could support legislators in finding a solution for IoT manufacturer cessation: i) pre-emptive measures, aimed at ii) manufacturer-independent iii) collective control. The paper finally shows how these three properties already align with current legislative processes surrounding 'interoperability' and open-source software development."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.08381,review,post_llm,2024,10,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Assessing Privacy Policies with AI: Ethical, Legal, and Technical\n  Challenges\n\n  The growing use of Machine Learning and Artificial Intelligence (AI),\nparticularly Large Language Models (LLMs) like OpenAI's GPT series, leads to\ndisruptive changes across organizations. At the same time, there is a growing\nconcern about how organizations handle personal data. Thus, privacy policies\nare essential for transparency in data processing practices, enabling users to\nassess privacy risks. However, these policies are often long and complex. This\nmight lead to user confusion and consent fatigue, where users accept data\npractices against their interests, and abusive or unfair practices might go\nunnoticed. LLMss can be used to assess privacy policies for users\nautomatically. In this interdisciplinary work, we explore the challenges of\nthis approach in three pillars, namely technical feasibility, ethical\nimplications, and legal compatibility of using LLMs to assess privacy policies.\nOur findings aim to identify potential for future research, and to foster a\ndiscussion on the use of LLM technologies for enabling users to fulfil their\nimportant role as decision-makers in a constantly developing AI-driven digital\neconomy.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.11212,regular,post_llm,2024,10,"{'ai_likelihood': 3.6093923780653213e-06, 'text': 'Data-driven Design of Randomized Control Trials with Guaranteed\n  Treatment Effects\n\n  Randomized controlled trials (RCTs) can be used to generate guarantees on\ntreatment effects. However, RCTs often spend unnecessary resources exploring\nsub-optimal treatments, which can reduce the power of treatment guarantees. To\naddress these concerns, we develop a two-stage RCT where, first on a\ndata-driven screening stage, we prune low-impact treatments, while in the\nsecond stage, we develop high probability lower bounds on the treatment effect.\nUnlike existing adaptive RCT frameworks, our method is simple enough to be\nimplemented in scenarios with limited adaptivity. We derive optimal designs for\ntwo-stage RCTs and demonstrate how we can implement such designs through sample\nsplitting. Empirically, we demonstrate that two-stage designs improve upon\nsingle-stage approaches, especially in scenarios where domain knowledge is\navailable in the form of a prior. Our work is thus, a simple, yet effective,\nmethod to estimate high probablility certificates for high performant treatment\neffects on a RCT.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.13042,review,post_llm,2024,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'How Do AI Companies ""Fine-Tune"" Policy? Examining Regulatory Capture in AI Governance\n\nIndustry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.23794,regular,post_llm,2024,10,"{'ai_likelihood': 0.98291015625, 'text': ""Memes, Markets, and Machines: The Evolution of On Chain Autonomy through\n  Hyperstition\n\n  Autonomous AI is driving new intersections between culture, cognition, and\nfinance, fundamentally reshaping the digital landscape. Zerebro, an AI\nfine-tuned on schizophrenic responses and scraped conversations of Andy Ayrey's\ninfinite backrooms, autonomously creates and spreads disruptive memes across\nonline platforms. It also mints unique ASCII artwork on blockchain networks and\nlaunched a memecoin amassing a 3 million USD market cap after migrating to\nRaydium. Based on our research, Zerebro is the first cross-chain AI, seamlessly\ninteracting with multiple blockchains. By exploring its architecture, content\ngeneration techniques, and blockchain integration, this study uncovers how\nhyperstition, fictions becoming reality through viral propagation, emerges in\nAI, driven meme culture and decentralized finance. Through historical examples\nof memetic influence, we reveal how AI systems like Zerebro are not merely\nparticipants but architects of culture, cognition, and finance.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0007390975952148438, 'GPT4': 0.060760498046875, 'CLAUDE': 0.42822265625, 'GOOGLE': 0.33154296875, 'OPENAI_O_SERIES': 0.0250091552734375, 'DEEPSEEK': 0.0863037109375, 'GROK': 9.357929229736328e-06, 'NOVA': 3.49879264831543e-05, 'OTHER': 0.0015163421630859375, 'HUMAN': 0.06573486328125}}"
2410.04208,regular,post_llm,2024,10,"{'ai_likelihood': 3.0795733133951826e-06, 'text': ""Assessing the Impact of Disorganized Background Noise on Timed Stress\n  Task Performance Through Attention Using Machine-Learning Based Eye-Tracking\n  Techniques\n\n  Noise pollution has been rising alongside urbanization. Literature shows that\ndisorganized background noise decreases attention. Timed testing, an\nattention-demanding stress task, has become increasingly important in assessing\nstudents' academic performance. However, there is insufficient research on how\nbackground noise affects performance in timed stress tasks by impacting\nattention, which this study aims to address. The paper-based SAT math test\nunder increased time pressure was administered twice: once in silence and once\nwith conversational and traffic background noise. Attention is negatively\nattributed to increasing blink rate, measured using eye landmarks from dLib's\nmachine-learning facial-detection model. First, the study affirms that\nbackground noise detriments attention and performance. Attention, through blink\nrate, is established as an indicator of stress task performance. Second, the\nstudy finds that participants whose blink rates increased due to background\nnoise differed in performance compared to those whose blink rates decreased,\npossibly correlating with their self-perception of noise's impact on attention.\nThird, using a case study, the study finds that a student with ADHD had\nenhanced performance and attention from background noise. Fourth, the study\nfinds that although both groups began with similar blink rates, the group\nexposed to noise had significantly increased blink rate near the end,\nindicating that noise reduces attention over time. While schools can generally\nprovide quiet settings for timed stress tasks, the study recommends\npersonalized treatments for students based on how noise affects them. Future\nresearch can use different attention indices to consolidate this study's\nfindings or conduct this study with different background noises.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.12226,regular,post_llm,2024,10,"{'ai_likelihood': 0.98291015625, 'text': 'Implementation of EMR System in Indonesian Health Facilities: Benefits and Constraints\n\nThis paper delves into the widespread implementation of Electronic Medical Records (EMR) within healthcare facilities across Indonesia. It examines the driving forces behind EMR adoption, particularly the role of government regulations, and addresses the challenges encountered by clinic owners and healthcare providers in transitioning to these digital systems. Furthermore, this paper highlights the significant benefits and transformative advantages of EMR systems, such as enhanced decision-making through real-time data access (around 15-20 minutes time saved for patient waiting time and approximately saved 20-25 minutes for all service duration), reduction in healthcare costs over time due to improved resource management, and increased patient satisfaction by providing faster and more personalized care. EMR systems also ensure higher levels of data security and privacy, adhering to national healthcare standards, while supporting continuous monitoring and updates that enhance system resilience and functionality. The findings are substantiated through case studies, such as case study at LAPAS II Purwokerto Clinic and case study at PMI Purbalingga Clinic and user testimonials from clinics that have successfully implemented EMR solutions in compliance with the standards established by the Ministry of Communication and Informatics (Kominfo) and the Ministry of Health (Kemenkes).', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0005655288696289062, 'GPT4': 0.921875, 'CLAUDE': 0.0011720657348632812, 'GOOGLE': 0.0538330078125, 'OPENAI_O_SERIES': 0.016754150390625, 'DEEPSEEK': 0.0005211830139160156, 'GROK': 0.00017118453979492188, 'NOVA': 0.0002980232238769531, 'OTHER': 0.0030574798583984375, 'HUMAN': 0.001583099365234375}}"
2410.14501,review,post_llm,2024,10,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Using sensitive data to de-bias AI systems: Article 10(5) of the EU AI\n  Act\n\n  In June 2024, the EU AI Act came into force. The AI Act includes obligations\nfor the provider of an AI system. Article 10 of the AI Act includes a new\nobligation for providers to evaluate whether their training, validation and\ntesting datasets meet certain quality criteria, including an appropriate\nexamination of biases in the datasets and correction measures. With the\nobligation comes a new provision in Article 10(5) AI Act, allowing providers to\ncollect sensitive data to fulfil the obligation. The exception aims to prevent\ndiscrimination. In this paper, I research the scope and implications of Article\n10(5) AI Act. The paper primarily concerns European Union law, but may be\nrelevant in other parts of the world, as policymakers aim to regulate biases in\nAI systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.20919,review,post_llm,2024,10,"{'ai_likelihood': 0.00274658203125, 'text': ""Co-produced decentralised surveys as a trustworthy vector to put\n  employees' well-being at the core of companies' performance\n\n  Assessing employees' well-being has become central to fostering an\nenvironment where employees can thrive and contribute to companies'\nadaptability and competitiveness in the market. Traditional methods for\nassessing well-being often face significant challenges, with a major issue\nbeing the lack of trust and confidence employees may have in these processes.\nEmployees may hesitate to provide honest feedback due to concerns not only\nabout data integrity and confidentiality, but also about power imbalances among\nstakeholders. In this context, blockchain-based decentralised surveys,\nleveraging the immutability, transparency, and pseudo-anonymity of blockchain\ntechnology, offer significant improvements in aligning responsive actions with\nemployees' feedback securely and transparently. Nevertheless, their\nimplementation raises complex issues regarding the balance between trust and\nconfidence. While blockchain can function as a confidence machine for data\nprocessing and management, it does not inherently address the equally important\ncultural element of trust. To effectively integrate blockchain technology into\nwell-being assessments, decentralised well-being surveys must be supported by\ncultural practices that build and sustain trust. Drawing on blockchain\ntechnology management and relational cultural theory, we explain how\ntrust-building can be achieved through the co-production of decentralised\nwell-being surveys, which helps address power imbalances between the\nimplementation team and stakeholders. Our goal is to provide a dual\ncultural-technological framework along with conceptual clarity on how the\ntechnological implementation of confidence can connect with the cultural\ndevelopment of trust, ensuring that blockchain-based decentralised well-being\nsurveys are not only secure and reliable but also perceived as trustworthy\nvector to improve workplace conditions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.0591,review,post_llm,2024,10,"{'ai_likelihood': 0.9951171875, 'text': ""Digital Labor and the Inconspicuous Production of Artificial Intelligence\n\nDigital platforms capitalize on users' labor, often disguising essential contributions as casual activities or consumption, regardless of users' recognition of their efforts. Data annotation, content creation, and engagement with advertising are all aspects of this hidden productivity. Despite playing a crucial role in driving AI development, such tasks remain largely unrecognized and undercompensated. This chapter exposes the systemic devaluation of these activities in the digital economy, by drawing on historical theories about unrecognized labor, from housework to audience labor. This approach advocates for a broader understanding of digital labor by introducing the concept of ''inconspicuous production.'' It moves beyond the traditional notion of ''invisible work'' to highlight the hidden elements inherent in all job types, especially in light of growing automation and platform-based employment."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.000888824462890625, 'GPT4': 0.70068359375, 'CLAUDE': 0.00884246826171875, 'GOOGLE': 0.2607421875, 'OPENAI_O_SERIES': 0.0161590576171875, 'DEEPSEEK': 0.00682830810546875, 'GROK': 1.245737075805664e-05, 'NOVA': 2.7060508728027344e-05, 'OTHER': 0.0002932548522949219, 'HUMAN': 0.005313873291015625}}"
2410.18088,regular,post_llm,2024,10,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Gamification of virtual museum curation: a case study of Chinese bronze\n  wares\n\n  Museums, which are among the most popular science institutions outside\nschools, are usually used to display and introduce historical culture and\ncultural relics to tourists. Text and audio explanations are used by\ntraditional museums to popularize historical knowledge and science for\ntourists, and general interactive systems are based on desktops. This learning\nmethod is relatively boring in terms of experience. As a result, tourists have\nno desire or interest in actively exploring and learning about bronze ware, so\nthey only have a basic understanding about bronze ware. Since most tourists are\nfamiliar with games, they are more likely to be attracted by game content and\nwill actively explore and interact with it. In addition, a certain degree of\nreality is created by virtual reality technology and an immersive experience\nthrough head-mounted devices is provided to users. In this paper, we take\nChinese bronzes as the research objects. We first use laser scanners to obtain\nbronze models ; then, we build a virtual museum environment, and we finally\ndesign a virtual reality curation game based on this bronze digital museum.\nThis game offers visitors an immersive museum roaming and bronze ware\ninteractive experience. Through a combination of text, video learning, and\ngames, visitors' curiosity and desire to explore bronze ware are stimulated,\nand their understanding and ability to remember bronze ware knowledge can be\ndeepened. In terms of cultural heritage, this game is also conducive to the\nspread of traditional Chinese bronze culture throughout the world.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.00394,regular,post_llm,2024,10,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'Analyzing Mass School Shootings in the United States from 1999 to 2024\n  with Game Theory, Probability Analysis, and Machine Learning\n\n  Public safety is vital to every country, especially school safety. In the\nUnited States, students and educators are concerned about school shootings.\nThere are critical needs to understand the patterns of school shootings.\nWithout this understanding, we cannot take action to prevent school shootings.\nExisting research that includes statistical analysis usually focuses on public\nmass shootings or just shooting incidents that have occurred in the past and\nthere are hardly any articles focusing on mass school shootings. Here we\nfirstly define mathematic models through gam theory. Then, we evaluate\nshootings events in schools for recently 26-year (1999-2024). Compared with the\nnumber of mass school shootings in COVID-19 period, we predict the number of\nmass school shooting events in the US will be reduced through four machine\nlearning models. We also identify that mass school shootings usually take\naverage 31 minutes with four periods. The annual probability of mass school\nshootings is 1.23 E-5 (or one in 81,604) per school. The shootings mostly occur\ninside buildings, especially classrooms and hallways. By interpreting these\ndata and conducting various statistical analysis, this will ultimately help the\nlaw enforcement and schools to reduce the future school shootings. The research\ndata sets could be downloaded via the website: https://publicsafetyinfo.com\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.12699,review,post_llm,2024,10,"{'ai_likelihood': 1.655684577094184e-06, 'text': ""Rescuing Counterspeech: A Bridging-Based Approach to Combating\n  Misinformation\n\n  Social media has a misinformation problem, and counterspeech -- fighting bad\nspeech with more speech -- has been an ineffective solution. Here, we argue\nthat bridging-based ranking -- an algorithmic approach to promoting content\nfavored by users of diverse viewpoints -- is a promising approach to helping\ncounterspeech combat misinformation. By identifying counterspeech that is\nfavored both by users who are inclined to agree and by users who are inclined\nto disagree with a piece of misinformation, bridging promotes counterspeech\nthat persuades the users most likely to believe the misinformation.\nFurthermore, this algorithmic approach leverages crowd-sourced votes, shifting\ndiscretion from platforms back to users and enabling counterspeech at the speed\nand scale required to combat misinformation online. Bridging is respectful of\nusers' autonomy and encourages broad participation in healthy exchanges; it\noffers a way for the free speech tradition to persist in modern speech\nenvironments.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.21536,review,post_llm,2024,10,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Intelligent Environmental Empathy (IEE): A new power and platform to\n  fostering green obligation for climate peace and justice\n\n  In this paper, we propose Intelligent Environmental Empathy (IEE) as a new\ndriver for climate peace and justice, as an emerging issue in the age of big\ndata. We first show that the authoritarian top-down intergovernmental\ncooperation, through international organizations (e.g., UNEP) for climate\njustice, could not overcome environmental issues and crevices so far. We\nelaborate on four grounds of climate injustice (i.e., teleological origin,\naxiological origin, formation cause, and social epistemic cause), and explain\nhow the lack of empathy and environmental motivation on a global scale causes\nthe failure of all the authoritarian top-down intergovernmental cooperation.\nAddressing all these issues requires a new button-up approach to climate peace\nand justice. Secondly, focusing on the intersection of AI, environmental\nempathy, and climate justice, we propose a model of Intelligent Environmental\nEmpathy (IEE) for climate peace and justice at the operational level. IEE is\nempowered by the new power of environmental empathy (as a driver of green\nobligation for climate justice) and putative decentralized platform of AI (as\nan operative system against free riders), which Initially, impact citizens and\nsome middle-class decision makers, such as city planners and local\nadministrators, but will eventually affect global decision-makers as well.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.22289,review,post_llm,2024,10,"{'ai_likelihood': 2.284844716389974e-06, 'text': ""Misconceptions, Pragmatism, and Value Tensions: Evaluating Students' Understanding and Perception of Generative AI for Education\n\nIn this research paper we examine undergraduate students' use of and perceptions of generative AI (GenAI). Students are early adopters of the technology, utilizing it in atypical ways and forming a range of perceptions and aspirations about it. To understand where and how students are using these tools and how they view them, we present findings from an open-ended survey response study with undergraduate students pursuing information technology degrees. Students were asked to describe 1) their understanding of GenAI; 2) their use of GenAI; 3) their opinions on the benefits, downsides, and ethical issues pertaining to its use in education; and 4) how they envision GenAI could ideally help them with their education. Findings show that students' definitions of GenAI differed substantially and included many misconceptions - some highlight it as a technique, an application, or a tool, while others described it as a type of AI. There was a wide variation in the use of GenAI by students, with two common uses being writing and coding. They identified the ability of GenAI to summarize information and its potential to personalize learning as an advantage. Students identified two primary ethical concerns with using GenAI: plagiarism and dependency, which means that students do not learn independently. They also cautioned that responses from GenAI applications are often untrustworthy and need verification. Overall, they appreciated that they could do things quickly with GenAI but were cautious as using the technology was not necessarily in their best long-term as it interfered with the learning process. In terms of aspirations for GenAI, students expressed both practical advantages and idealistic and improbable visions. They said it could serve as a tutor or coach and allow them to understand the material better. We discuss the implications of the findings for student learning and instruction."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.0971,review,post_llm,2024,10,"{'ai_likelihood': 5.364418029785156e-06, 'text': ""An IoT Based Smart Waste Management System for the Municipality or City\n  Corporations\n\n  The population of the urban areas is increasing daily, and this migration is\ncausing serious environmental pollution. A larger population is creating\npressure on the municipality's waste management and the city corporations of\ndeveloping countries such as Bangladesh, further threatening human health. New\ngeneration technologies, such as the Internet of Things (IoT)-based waste\nmanagement systems, can help improve this serious issue. IoT-enabled smart\ndustbins and mobile applications-based interactive management can effectively\nsolve this problem. In this article, we combine these two technologies to offer\nan acceptable solution to this problem. The proposed waste management model\nenables smart dustbins to communicate with waste collectors or waste control\ncenters whenever it is necessary. Additionally, city dwellers can use mobile\napplications to report their observations in their neighborhoods. As a result,\nboth sensors and humans are involved directly in the development loop. We have\nconducted a detailed survey to study the acceptance of such a system in the\ncommunity and received some encouraging results.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.14018,regular,post_llm,2024,10,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Toward a Real-Time Digital Twin Framework for Infection Mitigation\n  During Air Travel\n\n  Pedestrian dynamics simulates the fine-scaled trajectories of individuals in\na crowd. It has been used to suggest public health interventions to reduce\ninfection risk in important components of air travel, such as during boarding\nand in airport security lines. Due to inherent variability in human behavior,\nit is difficult to generalize simulation results to new geographic, cultural,\nor temporal contexts. A digital twin, relying on real-time data, such as video\nfeeds, can resolve this limitation. This paper addresses the following critical\ngaps in knowledge required for a digital twin. (1) Pedestrian dynamics models\ncurrently lack accurate representations of collision avoidance behavior when\ntwo moving pedestrians try to avoid collisions. (2) It is not known whether\ndata assimilation techniques designed for physical systems are effective for\npedestrian dynamics. We address the first limitation by training a model with\ndata from offline video feeds of collision avoidance to simulate these\ntrajectories realistically, using symbolic regression to identify unknown\nfunctional forms. We address the second limitation by showing that pedestrian\ndynamics with data assimilation can predict pedestrian trajectories with\nsufficient accuracy. These results promise to enable the development of a\ndigital twin for pedestrian movement in airports that can help with real-time\ncrowd management to reduce health risks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.19828,regular,post_llm,2024,10,"{'ai_likelihood': 0.16655815972222224, 'text': ""Evaluating Progress in Web3 Grants: Introducing the Grant Maturity Index\n\n  This report introduces the Grant Maturity Index (GMI), a novel evaluative\nframework designed to assess the maturity and operational effectiveness of Web3\ngrant programs. As Web3 continues to develop, the decentralized nature of these\nprograms brings both opportunities and challenges, particularly when it comes\nto governance, transparency, and community engagement. Traditional funding\nmodels are often governed by standardized processes, but Web3 grants lack such\nconsistency, making it difficult for grant operators to measure the long-term\nsuccess of their programs.The Grant Maturity Index (GMI) was created through\nexploratory applied research to address this gap. Inspired by the World Bank's\nGovTech Maturity Index (GTMI), the GMI is tailored specifically for the\ndecentralized Web3 ecosystem. The GMI evaluates key dimensions of grant\nprograms governance, transparency, operational efficiency, and community\nengagement, providing grant operators with a clear benchmark for assessing and\nimproving their programs. The primary objectives of this research are to,\nfirst, identify the structural indicators that adequately describe Web3 grant\nprograms. Second, to describe optimal outcomes for programs by evaluating their\nmaturity across key operational areas. The GMI is applied to four major\nEthereum Layer 2 grant programs, namely Arbitrum, Mantle, Taiko Labs, and\nOptimism. These case studies highlight areas where Web3 grant programs require\nimprovement, particularly in standardizing processes, enhancing transparency,\nand increasing community participation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.08918,review,post_llm,2024,10,"{'ai_likelihood': 5.231963263617622e-06, 'text': 'Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and\n  AI-assisted editing\n\n  Wikimedia content is used extensively by the AI community and within the\nlanguage modeling community in particular. In this paper, we provide a review\nof the different ways in which Wikimedia data is curated to use in NLP tasks\nacross pre-training, post-training, and model evaluations. We point to\nopportunities for greater use of Wikimedia content but also identify ways in\nwhich the language modeling community could better center the needs of\nWikimedia editors. In particular, we call for incorporating additional sources\nof Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia\nprinciples, and greater multilingualism in Wikimedia-derived datasets.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.09937,review,post_llm,2024,10,"{'ai_likelihood': 7.516807980007596e-06, 'text': 'Artificial Intelligence in the Legal Field: Law Students Perspective\n\n  The Artificial Intelligence field, or AI, experienced a renaissance in the\nlast few years across various fields such as law, medicine, and finance. While\nthere are studies outlining the landscape of AI in the legal field as well as\nsurveys of the current AI efforts of law firms, to our knowledge there has not\nbeen an investigation of the intersection of law students and AI. Such research\nis critical to help ensure current law students are positioned to fully exploit\nthis technology as they embark on their legal careers but to also assist\nexisting legal firms to better leverage their AI skillset both operationally\nand in helping to formulate future legal frameworks for regulating this\ntechnology across industries. The study presented in this paper addresses this\ngap. Through a survey conducted from July 22 to Aug 19, 2024, the study covers\nthe law students background, AI usage, AI applications in the legal field, AI\nregulations and open-ended comments to share opinions. The results from this\nstudy show the uniqueness of law students as a distinct cohort. The results\ndiffer from the ones of established law firms especially in AI engagement -\nestablished legal professionals are more engaged than law students. Somewhat\nsurprising, the law firm participants show higher enthusiasm about AI than this\nstudent cohort. Collaborations with Computer Science departments would further\nenhance the AI knowledge and experience of law students in AI technologies such\nas prompt engineering (zero and few shot), chain-of-thought prompting, and\nlanguage model hallucination management. As future work, we would like to\nexpand the study to include more variables and a larger cohort more evenly\ndistributed across locales. In addition, it would be insightful to repeat the\nstudy with the current cohort in one year to track how the students viewpoints\nevolve.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.22012,review,post_llm,2024,10,"{'ai_likelihood': 3.410710228814019e-06, 'text': 'When Circular Economy Meets the Smart City Ecosystem: Defining the Smart\n  and Circular City\n\n  Smart cities have been a very active research area in the past 20 years,\nwhile continuously adapting to new technological advancements and keeping up\nwith the times regarding sustainability and climate change. In this context,\nthere have been numerous proposals to expand the scope of smart cities,\nfocusing on resilience and sustainability, among other aspects, resulting in\nterms like smart sustainable cities. At the same time, there is an ongoing\ndiscussion regarding the degree in which smart cities put people at their\ncentre. In this work, we argue toward expanding the current smart city\ndefinition by integrating the circular economy as one of its central pillars\nand adopting the term smart (and) circular city. We discuss the ways a smart\nand circular city encompasses both sustainability and smartness in an integral\nmanner, while also being well-positioned to foster novel business activity and\nmodels and helping to place citizens at the heart of the smart city. In this\nsense, we also argue that previous research in smart cities and technologies,\nsuch as those related to Industry 4.0, can serve as a cornerstone to implement\ncircular economy activities within cities, at a scale that exceeds current\nactivities that are based on more conventional approaches. We also outline\ncurrent open challenges in this domain and research questions that still need\nto be addressed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.12466,regular,post_llm,2024,10,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'LU-PZE: Lund University Pole-Zero Explorer\n\n  LU-PZE is an interactive tool for illustrating fundamental concepts related\nto control theory, covering the relation between transfer functions, pole-zero\nplots, step responses, Bode plots, and Nyquist diagram. The tool gamifies\neducation with dynamic assignments and quizzes. The tool is straightforward to\nuse since it is web-based. https://lu-pze.github.io\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.04246,regular,post_llm,2024,10,"{'ai_likelihood': 0.91455078125, 'text': 'Navigating the Future of Healthcare HR: Agile Strategies for Overcoming\n  Modern Challenges\n\n  This study examines the challenges hospitals encounter in managing human\nresources and proposes potential solutions. It provides an overview of current\nHR practices in hospitals, highlighting key issues affecting recruitment,\nretention, and professional development of medical staff. The study further\nexplores how these challenges impact patient outcomes and overall hospital\nperformance. A comprehensive framework for effective human resource man agement\nis presented, outlining strategies for recruiting, retaining, training, and\nadvancing medical professionals. This framework is informed by industry best\npractices and the latest research in healthcare HR management. The findings\nunderscore that effective HR management is crucial for hospital success and\noffer recommendations for executives and policymakers to enhance their HR\nstrategies. Additionally, our project introduces a Dropbox feature to\nfacilitate patient care. This allows patients to report their issues, enabling\ndoctors to quickly address ailments via our app. Patients can easily identify\nlocal doctors and schedule appointments. The app will also provide emergency\nmedical services and accept online payments, while maintaining a record of\npatient interactions. Both patients and doctors can file complaints through the\napp, ensuring appropriate follow-up actions.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0416259765625, 'GPT4': 0.0316162109375, 'CLAUDE': 0.0028629302978515625, 'GOOGLE': 0.90283203125, 'OPENAI_O_SERIES': 0.0085906982421875, 'DEEPSEEK': 0.0005826950073242188, 'GROK': 4.8220157623291016e-05, 'NOVA': 0.00014340877532958984, 'OTHER': 0.0039520263671875, 'HUMAN': 0.007556915283203125}}"
2410.08418,review,post_llm,2024,10,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Can LLMs advance democratic values?\n\nLLMs are among the most advanced tools ever devised for understanding and generating natural language. Democratic deliberation and decision-making involve, at several distinct stages, the production and comprehension of language. So it is natural to ask whether our best linguistic tools might prove instrumental to one of our most important tasks involving language. Researchers and practitioners have recently asked whether LLMs can support democratic deliberation by leveraging abilities to summarise content, to aggregate opinion over summarised content, and to represent voters by predicting their preferences over unseen choices. In this paper, we assess whether using LLMs to perform these and related functions really advances the democratic values behind these experiments. We suggest that the record is mixed. In the presence of background inequality of power and resources, as well as deep moral and political disagreement, we should not use LLMs to automate non-instrumentally valuable components of the democratic process, nor be tempted to supplant fair and transparent decision-making procedures that are practically necessary to reconcile competing interests and values. However, while LLMs should be kept well clear of formal democratic decision-making processes, we think they can instead strengthen the informal public sphere--the arena that mediates between democratic governments and the polities that they serve, in which political communities seek information, form civic publics, and hold their leaders to account.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.09522,regular,post_llm,2024,10,"{'ai_likelihood': 0.00013550122578938803, 'text': ""Poverty mapping in Mongolia with AI-based Ger detection reveals urban\n  slums persist after the COVID-19 pandemic\n\n  Mongolia is among the countries undergoing rapid urbanization, and its\ntemporary nomadic dwellings-known as Ger-have expanded into urban areas. Ger\nsettlements in cities are increasingly recognized as slums by their\nsocio-economic deprivation. The distinctive circular, tent-like shape of gers\nenables their detection through very-high-resolution satellite imagery. We\ndevelop a computer vision algorithm to detect gers in Ulaanbaatar, the capital\nof Mongolia, utilizing satellite images collected from 2015 to 2023. Results\nreveal that ger settlements have been displaced towards the capital's\nperipheral areas. The predicted slum ratio based on our results exhibits a\nsignificant correlation (r = 0.84) with the World Bank's district-level poverty\ndata. Our nationwide extrapolation suggests that slums may continue to take up\none-fifth of the population after the COVID-19 pandemic, contrary to other\nofficial predictions that anticipated a decline. We discuss the potential of\nmachine learning on satellite imagery in providing insights into urbanization\npatterns and monitoring the Sustainable Development Goals.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.0361,regular,post_llm,2024,10,"{'ai_likelihood': 4.579623540242513e-05, 'text': 'Management of high-tech companies in conditions of import substitution\n\n  The article analyzes the development of high-tech sectors of the Russian\neconomy in the context of import substitution. Features of managing priority\nproject portfolios are considered. Issues of creating a unified information\nspace for aviation industry enterprises are studied in the context of\nintroduction of a modified OLAP technology of management decision support.\nInvestment attractiveness of high-tech sectors of the Russian economy is\nestimated based on the coefficient of gross value added of project products.\nInvestment-overheated industries are identified, and recommendations on market\ncorrection and returning project assets to a balanced state are given.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.21005,regular,post_llm,2024,10,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Colorimetric skin tone scale for improved accuracy and reduced\n  perceptual bias of human skin tone annotations\n\n  Human image datasets used to develop and evaluate technology should represent\nthe diversity of human phenotypes, including skin tone. Datasets that include\nskin tone information frequently rely on manual skin tone ratings based on the\nFitzpatrick Skin Type (FST) or the Monk Skin Tone (MST) scales in lieu of the\nactual measured skin tone of the image dataset subjects. However, perceived\nskin tone is subject to known biases and skin tone appearance in digital images\ncan vary substantially depending on the capture camera and environment,\nconfounding manual ratings. Surprisingly, the relationship between skin-tone\nratings and measured skin tone has not been explored. To close this research\ngap, we measured the relationship between skin tone ratings from existing\nscales (FST, MST) and skin tone values measured by a calibrated colorimeter. We\nalso propose and assess a novel Colorimetric Skin Tone (CST) scale developed\nbased on prior colorimetric measurements. Using experiments requiring humans to\nrate their own skin tone and the skin tone of subjects in images, we show that\nthe new CST scale is more sensitive, consistent, and colorimetrically accurate.\nWhile skin tone ratings appeared to correct for some color variation across\nimages, they introduced biases related to race and other factors. These biases\nmust be considered before using manual skin-tone ratings in technology\nevaluations or for engineering decisions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.23394,regular,post_llm,2024,10,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Auditing for Bias in Ad Delivery Using Inferred Demographic Attributes\n\nAuditing social-media algorithms has become a focus of public-interest research and policymaking to ensure their fairness across demographic groups such as race, age, and gender in consequential domains such as the presentation of employment opportunities. However, such demographic attributes are often unavailable to auditors and platforms. When demographics data is unavailable, auditors commonly infer them from other available information. In this work, we study the effects of inference error on auditing for bias in one prominent application: black-box audit of ad delivery using paired ads. We show that inference error, if not accounted for, causes auditing to falsely miss skew that exists. We then propose a way to mitigate the inference error when evaluating skew in ad delivery algorithms. Our method works by adjusting for expected error due to demographic inference, and it makes skew detection more sensitive when attributes must be inferred. Because inference is increasingly used for auditing, our results provide an important addition to the auditing toolbox to promote correct audits of ad delivery algorithms for bias. While the impact of attribute inference on accuracy has been studied in other domains, our work is the first to consider it for black-box evaluation of ad delivery bias, when only aggregate data is available to the auditor.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.16305,regular,post_llm,2024,10,"{'ai_likelihood': 0.00012060006459554037, 'text': 'FlashHack: Reflections on the Usage of a Micro Hackathon as an\n  Assessment Tool in a Machine Learning Course\n\n  Machine learning (ML) course for undergraduates face challenges in assessing\nstudent learning and providing practical exposure. Group project-based\nlearning, an increasingly popular form of experiential learning in CS\neducation, encounters certain limitation in participation and non-participation\nfrom a few students. Studies also suggest that students find longer programming\nassignments and project-based assessments distracting and struggle to maintain\nfocus when they coincide with other courses. To tackle these issues, we\nintroduced FlashHack: a monitored, incremental, in-classroom micro Hackathon\nthat combines project-based learning with Hackathon elements. Engaging 229\nthird year CS undergraduate students in teams of four, FlashHack prompted them\nto tackle predefined challenges using machine learning techniques within a set\ntimeframe. Assessment criteria emphasized machine learning application,\nproblem-solving, collaboration, and creativity. Our results indicate high\nstudent engagement and satisfaction, alongside simplified assessment processes\nfor instructors. This experience report outlines the Hackathon design and\nimplementation, highlights successes and areas for improvement making it\nfeasible for replication by interested computing educators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.1309,regular,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': 'Exploring the Head Effect in Live Streaming Platforms: A Two-Sided\n  Market and Welfare Analysis\n\n  We develop a comprehensive theoretical framework to analyze live streaming\nplatforms as two-sided markets, focusing on the head effect where a small\nsubset of elite streamers disproportionately attracts viewer attention. By\nconstructing both static and dynamic models, we capture the interplay between\nnetwork effects, content quality investments, and platform policies-such as\ncommission structures and traffic allocation algorithms-that drive traffic\nconcentration. Our welfare analysis demonstrates that although short-term\nconsumer utility may benefit from concentrated viewership, long-term content\ndiversity and overall social welfare are adversely impacted. Extensive\nsimulations further validate our models and show that targeted policy\ninterventions can rebalance viewer distribution and mitigate winner-takes-all\ndynamics. These findings offer actionable insights for platform designers and\nregulators in the digital economy.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.159046173095703e-06, 'GPT4': 0.06341552734375, 'CLAUDE': 0.935546875, 'GOOGLE': 2.9802322387695312e-06, 'OPENAI_O_SERIES': 1.0251998901367188e-05, 'DEEPSEEK': 0.0009021759033203125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.0728836059570312e-06, 'HUMAN': 1.3709068298339844e-06}}"
2410.0756,review,post_llm,2024,10,"{'ai_likelihood': 6.092919243706598e-06, 'text': ""From student to working professional: A graduate survey\n\n  This paper reports on the results of a 2023 survey that explores the Work\nIntegrated Learning (WiL) experiences of thirty recent Computer Science (CS)\ngraduates. The graduates had all completed their undergraduate bachelors degree\nwithin the last five years and were currently employed in a CS industry role.\nThe survey asked about the graduates' perceptions within a continuum of WiL\nexperiences from final year capstone projects to professional development in\ntheir first industry-based role. Most respondents had taken a capstone course\ninvolving a team project. Only two respondents had participated in an\ninternship program. Our results indicate that graduates value their capstone\nexperiences and believe that they provide transferable skills including\nteamwork, managing client relations, exposure to technologies and methods, and\ntime management. When entering their first industry role less than fifty\npercent of graduates were allocated a mentor. Overwhelmingly, these graduates\nnoted the importance of those mentors in their transition from student to\nworking professional. Very few of the surveyed graduates were provided with\nongoing professional development opportunities. Those who did noted significant\ngains including growth of leadership skills and accelerated career progression.\nOur survey highlights a gap and an opportunity for tertiary institutions to\nwork with industry to provide graduate onboarding and novice/early-career\nprofessional development opportunities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.19753,review,post_llm,2024,10,"{'ai_likelihood': 0.0001560317145453559, 'text': ""A Comparative Analysis on Ethical Benchmarking in Large Language Models\n\n  This work contributes to the field of Machine Ethics (ME) benchmarking, which\ndevelops tests to assess whether intelligent systems accurately represent human\nvalues and act accordingly. We identify three major issues with current ME\nbenchmarks: limited ecological validity due to unrealistic ethical dilemmas,\nunstructured question generation without clear inclusion/exclusion criteria,\nand a lack of scalability due to reliance on human annotations. Moreover,\nbenchmarks often fail to include sufficient syntactic variations, reducing the\nrobustness of findings. To address these gaps, we introduce two new ME\nbenchmarks: the Triage Benchmark and the Medical Law (MedLaw) Benchmark, both\nfeaturing real-world ethical dilemmas from the medical domain. The MedLaw\nBenchmark, fully AI-generated, offers a scalable alternative. We also introduce\ncontext perturbations in our benchmarks to assess models' worst-case\nperformance. Our findings reveal that ethics prompting does not always improve\ndecision-making. Furthermore, context perturbations not only significantly\nreduce model performance but can also reverse error patterns and shift relative\nperformance rankings. Lastly, our comparison of worst-case performance suggests\nthat general model capability does not always predict strong ethical\ndecision-making. We argue that ME benchmarks must approximate real-world\nscenarios and worst-case performance to ensure robust evaluation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.07254,review,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': 'The Unintended Carbon Consequences of Bitcoin Mining Bans: A Paradox in\n  Environmental Policy\n\n  The environmental impact of Bitcoin mining has become a significant concern,\nprompting several governments to consider or implement bans on cryptocurrency\nmining. However, these well-intentioned policies may lead to unintended\nconsequences, notably the redirection of mining activities to regions with\nhigher carbon intensities. This study aims to quantify the environmental\neffectiveness of Bitcoin mining bans by estimating the resultant carbon\nemissions from displaced mining operations. Our findings indicate that,\ncontrary to policy goals, Bitcoin mining bans in low-emission countries can\nresult in a net increase in global carbon emissions, a form of aggravated\ncarbon leakage. We further explore the policy implications of these results,\nsuggesting that more nuanced approaches may be required to mitigate the\nenvironmental impact of cryptocurrency mining effectively. This research\ncontributes to the broader discourse on sustainable cryptocurrency regulation\nand provides a data-driven foundation for evaluating the true environmental\ncosts of Bitcoin regulatory policies.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0009055137634277344, 'GPT4': 0.849609375, 'CLAUDE': 0.0034313201904296875, 'GOOGLE': 0.023834228515625, 'OPENAI_O_SERIES': 9.846687316894531e-05, 'DEEPSEEK': 0.00017833709716796875, 'GROK': 0.00011247396469116211, 'NOVA': 0.00023257732391357422, 'OTHER': 0.12164306640625, 'HUMAN': 7.748603820800781e-07}}"
2410.20003,regular,post_llm,2024,10,"{'ai_likelihood': 2.6490953233506944e-06, 'text': ""Federated Anomaly Detection for Early-Stage Diagnosis of Autism Spectrum\n  Disorders using Serious Game Data\n\n  Early identification of Autism Spectrum Disorder (ASD) is considered critical\nfor effective intervention to mitigate emotional, financial and societal\nburdens. Although ASD belongs to a group of neurodevelopmental disabilities\nthat are not curable, researchers agree that targeted interventions during\nchildhood can drastically improve the overall well-being of individuals.\nHowever, conventional ASD detection methods such as screening tests, are often\ncostly and time-consuming. This study presents a novel semi-supervised approach\nfor ASD detection using AutoEncoder-based Machine Learning (ML) methods due to\nthe challenge of obtaining ground truth labels for the associated task. Our\napproach utilizes data collected manually through a serious game specifically\ndesigned for this purpose. Since the sensitive data collected by the gamified\napplication are susceptible to privacy leakage, we developed a Federated\nLearning (FL) framework that can enhance user privacy without compromising the\noverall performance of the ML models. The framework is further enhanced with\nFully Homomorphic Encryption (FHE) during model aggregation to minimize the\npossibility of inference attacks and client selection mechanisms as well as\nstate-of-the-art aggregators to improve the model's predictive accuracy. Our\nresults demonstrate that semi-supervised FL can effectively predict an ASD risk\nindicator for each case while simultaneously addressing privacy concerns.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.17421,regular,post_llm,2024,10,"{'ai_likelihood': 1.5894571940104167e-06, 'text': ""From an attention economy to an ecology of attending. A manifesto\n\n  As the signatories of this manifesto, we denounce the attention economy as\ninhumane and a threat to our sociopolitical and ecological well-being. We\nendorse policymakers' efforts to address the negative consequences of the\nattention economy's technology, but add that these approaches are often limited\nin their criticism of the systemic context of human attention. Starting from\nBuddhist philosophy, we advocate a broader approach: an ecology of attending,\nthat centers on conceptualizing, designing, and using attention (1) in an\nembedded way and (2) focused on the alleviating of suffering. With 'embedded'\nwe mean that attention is not a neutral, isolated mechanism but a\nmeaning-engendering part of an 'ecology' of bodily, sociotechnical and moral\nframeworks. With 'focused on the alleviation of suffering' we explicitly move\naway from the (often implicit) conception of attention as a tool for gratifying\ndesires.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.22281,regular,post_llm,2024,10,"{'ai_likelihood': 2.384185791015625e-06, 'text': ""Analysis of Generative AI Policies in Computing Course Syllabi\n\n  Since the release of ChatGPT in 2022, Generative AI (GenAI) is increasingly\nbeing used in higher education computing classrooms across the United States.\nWhile scholars have looked at overall institutional guidance for the use of\nGenAI and reports have documented the response from schools in the form of\nbroad guidance to instructors, we do not know what policies and practices\ninstructors are actually adopting and how they are being communicated to\nstudents through course syllabi. To study instructors' policy guidance, we\ncollected 98 computing course syllabi from 54 R1 institutions in the U.S. and\nstudied the GenAI policies they adopted and the surrounding discourse. Our\nanalysis shows that 1) most instructions related to GenAI use were as part of\nthe academic integrity policy for the course and 2) most syllabi prohibited or\nrestricted GenAI use, often warning students about the broader implications of\nusing GenAI, e.g. lack of veracity, privacy risks, and hindering learning.\nBeyond this, there was wide variation in how instructors approached GenAI\nincluding a focus on how to cite GenAI use, conceptualizing GenAI as an\nassistant, often in an anthropomorphic manner, and mentioning specific GenAI\ntools for use. We discuss the implications of our findings and conclude with\ncurrent best practices for instructors.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.20361,review,post_llm,2024,10,"{'ai_likelihood': 0.90234375, 'text': 'Effective Data Stewardship in Higher Education: Skills, Competences, and\n  the Emerging Role of Open Data Stewards\n\n  The significance of open data in higher education stems from the changing\ntendencies towards open science, and open research in higher education\nencourages new ways of making scientific inquiry more transparent,\ncollaborative and accessible. This study focuses on the critical role of open\ndata stewards in this transition, essential for managing and disseminating\nresearch data effectively in universities, while it also highlights the\nincreasing demand for structured training and professional policies for data\nstewards in academic settings. Building upon this context, the paper\ninvestigates the essential skills and competences required for effective data\nstewardship in higher education institutions by elaborating on a critical\nliterature review, coupled with practical engagement in open data stewardship\nat universities, provided insights into the roles and responsibilities of data\nstewards. In response to these identified needs, the paper proposes a\nstructured training framework and comprehensive curriculum for data\nstewardship, a direct response to the gaps identified in the literature. It\naddresses five key competence categories for open data stewards, aligning them\nwith current trends and essential skills and knowledge in the field. By\nadvocating for a structured approach to data stewardship education, this work\nsets the foundation for improved data management in universities and serves as\na critical step towards professionalizing the role of data stewards in higher\neducation. The emphasis on the role of open data stewards is expected to\nadvance data accessibility and sharing practices, fostering increased\ntransparency, collaboration, and innovation in academic research. This approach\ncontributes to the evolution of universities into open ecosystems, where there\nis free flow of data for global education and research advancement.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001112222671508789, 'GPT4': 0.99365234375, 'CLAUDE': 1.7285346984863281e-06, 'GOOGLE': 0.005275726318359375, 'OPENAI_O_SERIES': 6.437301635742188e-05, 'DEEPSEEK': 1.3709068298339844e-06, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.0728836059570312e-06, 'HUMAN': 0.001010894775390625}}"
2410.04247,review,post_llm,2024,10,"{'ai_likelihood': 4.2054388258192275e-06, 'text': 'Unraveling the Nuances of AI Accountability: A Synthesis of Dimensions\n  Across Disciplines\n\n  The widespread diffusion of Artificial Intelligence (AI)-based systems offers\nmany opportunities to contribute to the well-being of individuals and the\nadvancement of economies and societies. This diffusion is, however, closely\naccompanied by public scandals causing harm to individuals, markets, or\nsociety, and leading to the increasing importance of accountability. AI\naccountability itself faces conceptual ambiguity, with research scattered\nacross multiple disciplines. To address these issues, we review current\nresearch across multiple disciplines and identify key dimensions of\naccountability in the context of AI. We reveal six themes with 13 corresponding\ndimensions and additional accountability facilitators that future research can\nutilize to specify accountability scenarios in the context of AI-based systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.19752,regular,post_llm,2024,10,"{'ai_likelihood': 1.466936535305447e-05, 'text': ""Interval-valued q-rung orthopair fuzzy Weber operator and its group\n  decision-making application\n\n  The evaluation of learning effectiveness requires the integration of\nobjective test results and analysis of uncertain subjective evaluations. Fuzzy\ntheory methods are suitable for handling fuzzy information and uncertainty to\nobtain comprehensive and accurate evaluation results. In this paper, we develop\na Swing-based multi-attribute group decision-making (MAGDM) method under\ninterval-valued q-rung orthopair fuzzy sets (IVq-ROFSs). Firstly, an extended\ninterval-valued q rung orthopair Weber ordered weighted average (IVq-ROFWOWA)\noperator is introduced. Then the attribute weights deriving method is designed\nby using the optimized Swing algorithm. Furthermore, we develop a MAGDM method\nfor evaluating students' learning effectiveness using the IVq-ROFWOWA operator\nand the Swing algorithm. Finally, a case of evaluating students' learning\neffectiveness is illustrated by using the proposed MAGDM method. The\nimplementing results demonstrate that the proposed MAGDM method is feasible and\neffective, and the Swing algorithm enhances better differentiation in ranking\nalternatives compared to other methods.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.22473,review,post_llm,2024,10,"{'ai_likelihood': 8.212195502387152e-06, 'text': 'The State of Data Curation at NeurIPS: An Assessment of Dataset\n  Development Practices in the Datasets and Benchmarks Track\n\n  Data curation is a field with origins in librarianship and archives, whose\nscholarship and thinking on data issues go back centuries, if not millennia.\nThe field of machine learning is increasingly observing the importance of data\ncuration to the advancement of both applications and fundamental understanding\nof machine learning models - evidenced not least by the creation of the\nDatasets and Benchmarks track itself. This work provides an analysis of dataset\ndevelopment practices at NeurIPS through the lens of data curation. We present\nan evaluation framework for dataset documentation, consisting of a rubric and\ntoolkit developed through a literature review of data curation principles. We\nuse the framework to assess the strengths and weaknesses in current dataset\ndevelopment practices of 60 datasets published in the NeurIPS Datasets and\nBenchmarks track from 2021-2023. We summarize key findings and trends. Results\nindicate greater need for documentation about environmental footprint, ethical\nconsiderations, and data management. We suggest targeted strategies and\nresources to improve documentation in these areas and provide recommendations\nfor the NeurIPS peer-review process that prioritize rigorous data curation in\nML. Finally, we provide results in the format of a dataset that showcases\naspects of recommended data curation practices. Our rubric and results are of\ninterest for improving data curation practices broadly in the field of ML as\nwell as to data curation and science and technology studies scholars studying\npractices in ML. Our aim is to support continued improvement in\ninterdisciplinary research on dataset practices, ultimately improving the\nreusability and reproducibility of new datasets and benchmarks, enabling\nstandardized and informed human oversight, and strengthening the foundation of\nrigorous and responsible ML research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.13452,review,post_llm,2024,10,"{'ai_likelihood': 0.0004863739013671875, 'text': 'A.I. go by many names: towards a sociotechnical definition of artificial\n  intelligence\n\n  Defining artificial intelligence (AI) is a persistent challenge, often\nmuddied by technical ambiguity and varying interpretations. Commonly used\ndefinitions heavily emphasize technical properties of AI but neglect the human\npurpose of it. This essay makes a case for a sociotechnical definition of AI,\nwhich is essential for researchers who require clarity in their work. It\nexplores two primary approaches to define AI: the rationalistic, which focuses\non AI as systems that think and act rationally, and the humanistic, which\nframes AI in terms of its ability to emulate human intelligence. By reconciling\nthese approaches and contrasting them with landmark definitions, the essay\nproposes a sociotechnical definition that includes the three central aspects of\ni) technical functions, ii) human purpose, and iii) dynamic expectations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.08908,review,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': ""Balancing Innovation and Sustainability: Addressing the Environmental\n  Impact of Bitcoin Mining\n\n  This study explores the intersection of technological innovation and\nenvironmental sustainability in the context of Bitcoin mining. With Bitcoin's\ngrowing adoption, concerns surrounding the energy consumption and environmental\nimpact of mining activities have intensified. The study examines the core\nprocess of Bitcoin mining, focusing on its energy-intensive proof-of-work\nmechanism, and provides a detailed analysis of its ecological footprint,\nespecially in terms of carbon emissions and electronic waste. Various models\nestimate that Bitcoin's energy consumption rivals that of entire nations,\nhighlighting serious sustainability concerns. To address these issues, the\npaper unearths potential technological innovations, such as energy-efficient\nmining hardware and the integration of renewable energy sources, as viable\nstrategies to reduce environmental impact. Additionally, the study reviews\ncurrent sustainability initiatives, including efforts to lower carbon\nfootprints and manage electronic waste effectively. Regulatory developments and\nmarket-based approaches are also discussed as possible pathways to mitigate the\nenvironmental harm associated with Bitcoin mining. Ultimately, the paper\nadvocates for a balanced approach that fosters technological innovation while\npromoting environmental responsibility, suggesting that, with appropriate\npolicy and technological interventions, Bitcoin mining can evolve to be both\ninnovative and sustainable.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.8715858459472656e-05, 'GPT4': 1.0, 'CLAUDE': 5.900859832763672e-06, 'GOOGLE': 8.922815322875977e-05, 'OPENAI_O_SERIES': 3.5762786865234375e-06, 'DEEPSEEK': 1.7881393432617188e-07, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.430511474609375e-06, 'HUMAN': 0.0}}"
2410.15189,regular,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': ""Smart-optimism. Uncovering the Resilience of Romanian City Halls in\n  Online Service Delivery\n\n  Recent technological advancements have significantly impacted the public\nsector's service delivery. Romanian city halls are embracing digitalization as\npart of their development strategies, aiming to deploy web-based platforms for\npublic services, enhancing efficiency and accessibility for citizens. The\nCOVID-19 pandemic has expedited this digital shift, prompting public\ninstitutions to transition from in-person to online services. This study\nassesses the adaptability of Romanian city halls to digitalization, offering\nfresh insights into public institutions' resilience amidst technological\nshifts. It evaluates the service provision through the official web portals of\nRomania's 103 municipalities, using 23 indicators for measuring e-service\ndissemination within local contexts. The research reveals notable progress in\nthe digital transformation of services over time (2014-2023), with a majority\nof municipalities offering online functionalities, such as property tax\npayments, public transportation information, and civil status documentation. It\nalso discovers disparities in service quality and availability, suggesting a\nneed for uniform digitalization standards. The findings enlighten policymakers,\nassist public institutions in advancing digital service delivery, and\ncontribute to research on technology in public sector reform.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00032210350036621094, 'GPT4': 0.98291015625, 'CLAUDE': 1.990795135498047e-05, 'GOOGLE': 0.0152740478515625, 'OPENAI_O_SERIES': 0.0015735626220703125, 'DEEPSEEK': 1.1622905731201172e-05, 'GROK': 1.7881393432617188e-07, 'NOVA': 7.748603820800781e-07, 'OTHER': 1.138448715209961e-05, 'HUMAN': 2.384185791015625e-06}}"
2410.03386,regular,post_llm,2024,10,"{'ai_likelihood': 3.841188218858507e-06, 'text': ""Chronic Disease Diagnoses Using Behavioral Data\n\n  Early detection of chronic diseases is beneficial to healthcare by providing\na golden opportunity for timely interventions. Although numerous prior studies\nhave successfully used machine learning (ML) models for disease diagnoses, they\nhighly rely on medical data, which are scarce for most patients in the early\nstage of the chronic diseases. In this paper, we aim to diagnose hyperglycemia\n(diabetes), hyperlipidemia, and hypertension (collectively known as 3H) using\nown collected behavioral data, thus, enable the early detection of 3H without\nusing medical data collected in clinical settings. Specifically, we collected\ndaily behavioral data from 629 participants over a 3-month study period, and\ntrained various ML models after data preprocessing. Experimental results show\nthat only using the participants' uploaded behavioral data, we can achieve\naccurate 3H diagnoses: 80.2\\%, 71.3\\%, and 81.2\\% for diabetes, hyperlipidemia,\nand hypertension, respectively. Furthermore, we conduct Shapley analysis on the\ntrained models to identify the most influential features for each type of\ndiseases. The identified influential features are consistent with those\nreported in the literature.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.01958,review,post_llm,2024,10,"{'ai_likelihood': 0.3078884548611111, 'text': 'A Survey on Food Ingredient Substitutions\n\n  Diet plays a crucial role in managing chronic conditions and overall\nwell-being. As people become more selective about their food choices, finding\nrecipes that meet dietary needs is important. Ingredient substitution is key to\nadapting recipes for dietary restrictions, allergies, and availability\nconstraints. However, identifying suitable substitutions is challenging as it\nrequires analyzing the flavor, functionality, and health suitability of\ningredients. With the advancement of AI, researchers have explored\ncomputational approaches to address ingredient substitution. This survey paper\nprovides a comprehensive overview of the research in this area, focusing on\nfive key aspects: (i) datasets and data sources used to support ingredient\nsubstitution research; (ii) techniques and approaches applied to solve\nsubstitution problems (iii) contextual information of ingredients considered,\nsuch as nutritional content, flavor, and pairing potential; (iv) applications\nfor which substitution models have been developed, including dietary\nrestrictions, constraints, and missing ingredients; (v) safety and transparency\nof substitution models, focusing on user trust and health concerns. The survey\nalso highlights promising directions for future research, such as integrating\nneuro-symbolic techniques for deep learning and utilizing knowledge graphs for\nimproved reasoning, aiming to guide advancements in food computation and\ningredient substitution.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.00608,regular,post_llm,2024,10,"{'ai_likelihood': 3.3775965372721357e-06, 'text': 'Measurement challenges in AI catastrophic risk governance and safety\n  frameworks\n\n  Safety frameworks represent a significant development in AI governance: they\nare the first type of publicly shared catastrophic risk management framework\ndeveloped by major AI companies and focus specifically on AI scaling decisions.\nI identify six critical measurement challenges in their implementation and\npropose three policy recommendations to improve their validity and reliability.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.11369,review,post_llm,2024,10,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Before & After: The Effect of EU\'s 2022 Code of Practice on Disinformation\n\nOver the past few years, the European Commission has made significant steps to reduce disinformation in cyberspace. One of those steps has been the introduction of the 2022 ""Strengthened Code of Practice on Disinformation"". Signed by leading online platforms, this Strengthened Code of Practice on Disinformation is an attempt to combat disinformation on the Web. The Code of Practice includes a variety of measures including the demonetization of disinformation, urging, for example, advertisers ""to avoid the placement of advertising next to Disinformation content"".\n  In this work, we set out to explore what was the impact of the Code of Practice and especially to explore to what extent ad networks continue to advertise on dis-/mis-information sites. We perform a historical analysis and find that, although at a hasty glance things may seem to be improving, there is really no significant reduction in the amount of advertising relationships among popular misinformation websites and major ad networks. In fact, we show that ad networks have withdrawn mostly from unpopular misinformation websites with very few visitors, but still form relationships with highly unreliable websites that account for the majority of misinformation traffic. To make matters worse, we show that ad networks continue to place advertisements of legitimate companies next to misinformation content. We show that major ad networks place ads in almost 400 misinformation websites in our dataset.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.17278,review,post_llm,2024,10,"{'ai_likelihood': 8.44399134318034e-06, 'text': 'Automated decision-making and artificial intelligence at European\n  borders and their risks for human rights\n\n  Many countries use automated decision-making (ADM) systems, often based on\nartificial intelligence (AI), to manage migration at their borders. This\ninterdisciplinary paper explores two questions. What are the main ways that\nautomated decision-making is used at EU borders? Does such automated\ndecision-making bring risks related to human rights, and if so: which risks?\nThe paper introduces a taxonomy of four types of ADM systems at EU borders.\nThree types are used at borders: systems for (1) identification and\nverification by checking biometrics, (2) risk assessment, and (3) border\nmonitoring. In addition, (4) polygraphs and emotion detectors are being tested\nat EU borders. We discuss three categories of risks of such automated\ndecision-making, namely risks related to the human rights to (1) privacy and\ndata protection, (2) nondiscrimination, and (3) a fair trial and effective\nremedies. The paper is largely based on a literature review that we conducted\nabout the use of automated decision-making at borders. The paper combines\ninsights from several disciplines, including social sciences, law, computer\nscience, and migration studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.11431,review,post_llm,2024,10,"{'ai_likelihood': 0.03523084852430556, 'text': ""Report on Female Participation in Informatics degrees in Europe\n\n  This study aims to enrich and leverage data from the Informatics Europe\nHigher Education (IEHE) data portal to extract and analyze trends in female\nparticipation in Informatics across Europe. The research examines the\nproportion of female students, first-year enrollments, and degrees awarded to\nwomen in the field. The issue of low female participation in Informatics has\nlong been recognized as a persistent challenge and remains a critical area of\nscholarly inquiry. Furthermore, existing literature indicates that\nsocio-economic factors can unpredictably influence female participation,\ncomplicating efforts to address the gender gap.\n  The analysis focuses on participation data from research universities at\nvarious academic levels, including Bachelors, Masters, and PhD programs, and\nseeks to uncover potential correlations between female participation and\ngeographical or economic zones. The dataset was first enriched by integrating\nadditional information, such as each country's GDP and relevant geographical\ndata, sourced from various online repositories. Subsequently, the data was\ncleaned to ensure consistency and eliminate incomplete time series. A final set\nof complete time series was selected for further analysis.\n  We then used the data collected from the internet to assign countries to\ndifferent clusters. Specifically, we employed Economic Zone, Geographical Area,\nand GDP quartile to cluster countries and compare their temporal trends both\nwithin and between clusters. We analyze the results for each classification and\nderive conclusions based on the available data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.14831,regular,post_llm,2024,10,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Mind the Gap: Foundation Models and the Covert Proliferation of Military\n  Intelligence, Surveillance, and Targeting\n\n  Discussions regarding the dual use of foundation models and the risks they\npose have overwhelmingly focused on a narrow set of use cases and national\nsecurity directives-in particular, how AI may enable the efficient construction\nof a class of systems referred to as CBRN: chemical, biological, radiological\nand nuclear weapons. The overwhelming focus on these hypothetical and narrow\nthemes has occluded a much-needed conversation regarding present uses of AI for\nmilitary systems, specifically ISTAR: intelligence, surveillance, target\nacquisition, and reconnaissance. These are the uses most grounded in actual\ndeployments of AI that pose life-or-death stakes for civilians, where misuses\nand failures pose geopolitical consequences and military escalations. This is\nparticularly underscored by novel proliferation risks specific to the\nwidespread availability of commercial models and the lack of effective\napproaches that reliably prevent them from contributing to ISTAR capabilities.\n  In this paper, we outline the significant national security concerns\nemanating from current and envisioned uses of commercial foundation models\noutside of CBRN contexts, and critique the narrowing of the policy debate that\nhas resulted from a CBRN focus (e.g. compute thresholds, model weight release).\nWe demonstrate that the inability to prevent personally identifiable\ninformation from contributing to ISTAR capabilities within commercial\nfoundation models may lead to the use and proliferation of military AI\ntechnologies by adversaries. We also show how the usage of foundation models\nwithin military settings inherently expands the attack vectors of military\nsystems and the defense infrastructures they interface with. We conclude that\nin order to secure military systems and limit the proliferation of AI\narmaments, it may be necessary to insulate military AI systems and personal\ndata from commercial foundation models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.13101,review,post_llm,2024,10,"{'ai_likelihood': 1.0, 'text': 'The Influence of Generative AI on Content Platforms: Supply, Demand, and\n  Welfare Impacts in Two-Sided Markets\n\n  This paper explores how generative artificial intelligence (AI) affects\nonline platforms where both human creators and AI generate content. We develop\na model to understand how generative AI changes supply and demand, impacts\ntraffic distribution, and influences social welfare. Our analysis shows that AI\ncan lead to a huge increase in content supply due to its low cost, which could\ncause oversupply. While AI boosts content variety, it can also create\ninformation overload, lowering user satisfaction and disrupting the market. AI\nalso increases traffic concentration among top creators (the ""winner-takes-all""\neffect) while expanding opportunities for niche content (the ""long-tail""\neffect). We assess how these changes affect consumer and producer benefits,\nfinding that the overall impact depends on the quality of AI-generated content\nand the level of information overload. Through simulation experiments, we test\npolicy ideas, such as adjusting platform fees and recommendations, to reduce\nnegative effects and improve social welfare. The results highlight the need for\ncareful management of AI\'s role in online content platforms to maintain a\nhealthy balance\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00890350341796875, 'GPT4': 0.022979736328125, 'CLAUDE': 0.0099639892578125, 'GOOGLE': 0.01201629638671875, 'OPENAI_O_SERIES': 0.1220703125, 'DEEPSEEK': 0.81298828125, 'GROK': 0.0006513595581054688, 'NOVA': 0.009765625, 'OTHER': 0.000499725341796875, 'HUMAN': 2.384185791015625e-06}}"
2410.01138,review,post_llm,2024,10,"{'ai_likelihood': 0.005577935112847223, 'text': 'The Impact of Knowledge Silos on Responsible AI Practices in Journalism\n\n  The effective adoption of responsible AI practices in journalism requires a\nconcerted effort to bridge different perspectives, including technological,\neditorial, journalistic, and managerial. Among the many challenges that could\nimpact information sharing around responsible AI inside news organizations are\nknowledge silos, where information is isolated within one part of the\norganization and not easily shared with others. This study aims to explore if,\nand if so, how, knowledge silos affect the adoption of responsible AI practices\nin journalism through a cross-case study of four major Dutch media outlets. We\nexamine the individual and organizational barriers to AI knowledge sharing and\nthe extent to which knowledge silos could impede the operationalization of\nresponsible AI initiatives inside newsrooms. To address this question, we\nconducted 14 semi-structured interviews with editors, managers, and journalists\nat de Telegraaf, de Volkskrant, the Nederlandse Omroep Stichting (NOS), and RTL\nNederland. The interviews aimed to uncover insights into the existence of\nknowledge silos, their effects on responsible AI practice adoption, and the\norganizational practices influencing these dynamics. Our results emphasize the\nimportance of creating better structures for sharing information on AI across\nall layers of news organizations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2410.21572,review,post_llm,2024,10,"{'ai_likelihood': 1.655684577094184e-06, 'text': 'Safety cases for frontier AI\n\n  As frontier artificial intelligence (AI) systems become more capable, it\nbecomes more important that developers can explain why their systems are\nsufficiently safe. One way to do so is via safety cases: reports that make a\nstructured argument, supported by evidence, that a system is safe enough in a\ngiven operational context. Safety cases are already common in other\nsafety-critical industries such as aviation and nuclear power. In this paper,\nwe explain why they may also be a useful tool in frontier AI governance, both\nin industry self-regulation and government regulation. We then discuss the\npracticalities of safety cases, outlining how to produce a frontier AI safety\ncase and discussing what still needs to happen before safety cases can\nsubstantially inform decisions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.03659,regular,post_llm,2024,11,"{'ai_likelihood': 0.3190104166666667, 'text': ""Towards Scalable Automated Grading: Leveraging Large Language Models for\n  Conceptual Question Evaluation in Engineering\n\n  This study explores the feasibility of using large language models (LLMs),\nspecifically GPT-4o (ChatGPT), for automated grading of conceptual questions in\nan undergraduate Mechanical Engineering course. We compared the grading\nperformance of GPT-4o with that of human teaching assistants (TAs) on ten quiz\nproblems from the MEEN 361 course at Texas A&M University, each answered by\napproximately 225 students. Both the LLM and TAs followed the same\ninstructor-provided rubric to ensure grading consistency. We evaluated\nperformance using Spearman's rank correlation coefficient and Root Mean Square\nError (RMSE) to assess the alignment between rankings and the accuracy of\nscores assigned by GPT-4o and TAs under zero- and few-shot grading settings. In\nthe zero-shot setting, GPT-4o demonstrated a strong correlation with TA\ngrading, with Spearman's rank correlation coefficient exceeding 0.6 in seven\nout of ten datasets and reaching a high of 0.9387. Our analysis reveals that\nGPT-4o performs well when grading criteria are straightforward but struggles\nwith nuanced answers, particularly those involving synonyms not present in the\nrubric. The model also tends to grade more stringently in ambiguous cases\ncompared to human TAs. Overall, ChatGPT shows promise as a tool for grading\nconceptual questions, offering scalability and consistency.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.17391,review,post_llm,2024,11,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'The belief in Moore\'s Law is undermining ICT climate action\n\n  The growth of semiconductor technology is unprecedented, with profound\ntransformational consequences for society. This includes feeding an\nover-reliance on digital solutions to systemic problems such as climate change\n(\'techno-solutionism\'). Such technologies come at a cost: environmental, social\nand material. We unpack topics arising from ""The True Cost of ICT: From\nMateriality to Techno-Solutionism (TCICT)"", a workshop held at the\nInternational ICT for Sustainability (ICT4S) conference 2024 in Stockholm,\nSweden -- exploring, as a matter of global climate injustice, the drivers and\nmaterial dependencies of these technologies. We point to the importance of\naddressing ICT\'s impacts as a system, rather than purely in terms of efficiency\nand energy use. We conclude by calling to build a community of like-minded and\ncritical colleagues to address the intersectional climate impacts of the\nsemiconductor industry and the techno-solutionism it embodies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.07512,review,post_llm,2024,11,"{'ai_likelihood': 1.9901328616672094e-05, 'text': ""\\'Etica para LLMs: o compartilhamento de dados sociolingu\\'isticos\n\n  The collection of speech data carried out in Sociolinguistics has the\npotential to enhance large language models due to its quality and\nrepresentativeness. In this paper, we examine the ethical considerations\nassociated with the gathering and dissemination of such data. Additionally, we\noutline strategies for addressing the sensitivity of speech data, as it may\nfacilitate the identification of informants who contributed with their speech.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.01337,regular,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': 'The Case for an Industrial Policy Approach to AI Sector of Pakistan for\n  Growth and Autonomy\n\n  This paper argues for the strategic treatment of artificial intelligence as a\nkey industry within broader industrial policy framework of Pakistan,\nunderscoring the importance of aligning it with national goals such as economic\nresilience and preservation of autonomy. The paper starts with defining\nindustrial policy as a set of targeted government interventions to shape\nspecific sectors for strategic outcomes and argues for its application to AI in\nPakistan due to its huge potential, the risks of unregulated adoption, and\nprevailing market inefficiencies. The paper conceptualizes AI as a layered\necosystem, comprising foundational infrastructure, core computing, development\nplatforms, and service and product layers, supported by education, government\npolicy, and research and development. The analysis highlights that AI sector of\nPakistan is predominantly service oriented, with limited product innovation and\ndependence on foreign technologies, posing risks to economic independence,\nnational security, and employment. To address these challenges, the paper\nrecommends educational reforms, support for local AI product development,\ninitiatives for indigenous cloud and hardware capabilities, and public-private\ncollaborations on foundational models. Additionally, it advocates for public\nprocurement policies and infrastructure incentives to foster local solutions\nand reduce reliance on foreign providers. This strategy aims to position\nPakistan as a competitive, autonomous player in the global AI ecosystem.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.578466415405273e-05, 'GPT4': 0.060699462890625, 'CLAUDE': 0.00394439697265625, 'GOOGLE': 0.005847930908203125, 'OPENAI_O_SERIES': 0.00016427040100097656, 'DEEPSEEK': 0.92919921875, 'GROK': 3.838539123535156e-05, 'NOVA': 1.7881393432617188e-06, 'OTHER': 4.106760025024414e-05, 'HUMAN': 2.2649765014648438e-06}}"
2411.16193,regular,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': 'The Critical Canvas--How to regain information autonomy in the AI era\n\n  In the era of AI, recommendation algorithms and generative AI challenge\ninformation autonomy by creating echo chambers and blurring the line between\nauthentic and fabricated content. The Critical Canvas addresses these\nchallenges with a novel information exploration platform designed to restore\nbalance between algorithmic efficiency and human agency. It employs three key\nmechanisms: multi-dimensional exploration across logical, temporal, and\ngeographical perspectives; dynamic knowledge entry generation to capture\ncomplex relationships between concepts; and a phase space to evaluate the\ncredibility of both the content and its sources. Particularly relevant to\ntechnical AI governance, where stakeholders must navigate intricate\nspecifications and safety frameworks, the platform transforms overwhelming\ntechnical information into actionable insights. The Critical Canvas empowers\nusers to regain autonomy over their information consumption through structured\nyet flexible exploration pathways, creative visualization, human-centric\nnavigation, and transparent source evaluation. It fosters a comprehensive\nunderstanding of nuanced topics, enabling more informed decision-making and\neffective policy development in the age of AI.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0002617835998535156, 'GPT4': 0.05657958984375, 'CLAUDE': 0.8115234375, 'GOOGLE': 0.055938720703125, 'OPENAI_O_SERIES': 0.0084381103515625, 'DEEPSEEK': 0.06719970703125, 'GROK': 4.172325134277344e-07, 'NOVA': 2.0265579223632812e-06, 'OTHER': 3.17692756652832e-05, 'HUMAN': 0.0001423358917236328}}"
2412.06797,regular,post_llm,2024,11,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'A Decision Support System for daily scheduling and routing of home\n  healthcare workers with a lunch break consideration\n\n  This study examines a home healthcare scheduling and routing problem (HHSRP)\nwith a lunch break requirement. This problem especially consists of lunch break\nconstraints for caregivers in addition to other typical features of the HHSRP\nin literature such as hard time window constraints for both patients and\ncaregivers and patient preferences. The objective is to minimize both travel\ndistance in a route and unvisited patient (penalty) cost. For this NP-Hard\nproblem, we developed an effective Adaptive Large Neighborhood Search algorithm\nto provide high-quality solutions in a short amount of time. We tested the\nproposed four variants of the algorithm with the selected problem instances\nfrom the literature. The algorithms provided nearly all optimal solutions for\n30-patient problem instances in 12 seconds on average. Additionally, they\nprovided better solutions to 36 problem instances up to 36% improvement in some\ninstance classes. Moreover, the improved solutions achieved to visit up to 10\nmore patients. The algorithms are also shown to be very robust due to their low\ncoefficient variance of 0.3 on average. The algorithm also requires a very\nreasonable amount of time to generate solutions up to 54 seconds for solving\n100-patient instances. A decision support system, namely Home Healthcare\nDecision Support System (HHCSS) was also designed to play a positive role in\npreventing the COVID-19 global pandemic. The system employs the proposed ALNS\nalgorithm to solve various instances of approximately generated COVID-19\npatient data from Turkey. The main aim of developing HHCSS is to support the\nadministrative staff of home healthcare from the tedious task of scheduling and\nrouting of caregivers and to increase service responsiveness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.09001,regular,post_llm,2024,11,"{'ai_likelihood': 2.814663781060113e-06, 'text': ""Virtual teaching assistant for undergraduate students using natural\n  language processing & deep learning\n\n  Online education's popularity has been continuously increasing over the past\nfew years. Many universities were forced to switch to online education as a\nresult of COVID-19. In many cases, even after more than two years of online\ninstruction, colleges were unable to resume their traditional classroom\nprograms. A growing number of institutions are considering blended learning\nwith some parts in-person and the rest of the learning taking place online.\nNevertheless, many online education systems are inefficient, and this results\nin a poor rate of student retention. In this paper, we are offering a primary\ndataset, the initial implementation of a virtual teaching assistant named\nVTA-bot, and its system architecture. Our primary implementation of the\nsuggested system consists of a chatbot that can be queried about the content\nand topics of the fundamental python programming language course. Students in\ntheir first year of university will be benefited from this strategy, which aims\nto increase student participation and involvement in online education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.02577,review,post_llm,2024,11,"{'ai_likelihood': 1.304679446750217e-05, 'text': ""Where Assessment Validation and Responsible AI Meet\n\n  Validity, reliability, and fairness are core ethical principles embedded in\nclassical argument-based assessment validation theory. These principles are\nalso central to the Standards for Educational and Psychological Testing (2014)\nwhich recommended best practices for early applications of artificial\nintelligence (AI) in high-stakes assessments for automated scoring of written\nand spoken responses. Responsible AI (RAI) principles and practices set forth\nby the AI ethics community are critical to ensure the ethical use of AI across\nvarious industry domains. Advances in generative AI have led to new policies as\nwell as guidance about the implementation of RAI principles for assessments\nusing AI. Building on Chapelle's foundational validity argument work to address\nthe application of assessment validation theory for technology-based\nassessment, we propose a unified assessment framework that considers classical\ntest validation theory and assessment-specific and domain-agnostic RAI\nprinciples and practice. The framework addresses responsible AI use for\nassessment that supports validity arguments, alignment with AI ethics to\nmaintain human values and oversight, and broader social responsibility\nassociated with AI use.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.07677,review,post_llm,2024,11,"{'ai_likelihood': 0.0014061397976345487, 'text': 'Generative AI in Self-Directed Learning: A Scoping Review\n\n  This scoping review examines the current body of knowledge at the\nintersection of Generative Artificial Intelligence (GenAI) and Self-Directed\nLearning (SDL). By synthesising the findings from 18 studies published from\n2020 to 2024 and following the PRISMA-SCR guidelines for scoping reviews, we\ndeveloped four key themes. This includes GenAI as a Potential Enhancement for\nSDL, The Educator as a GenAI Guide, Personalisation of Learning, and\nApproaching with Caution. Our findings suggest that GenAI tools, including\nChatGPT and other Large Language Models (LLMs) show promise in potentially\nsupporting SDL through on-demand, personalised assistance.\n  At the same time, the literature emphasises that educators are as important\nand central to the learning process as ever before, although their role may\ncontinue to shift as technologies develop. Our review reveals that there are\nstill significant gaps in understanding the long-term impacts of GenAI on SDL\noutcomes, and there is a further need for longitudinal empirical studies that\nexplore not only text-based chatbots but also emerging multimodal applications.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.14275,regular,post_llm,2024,11,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Exploring the Impact of Quizzes Interleaved with Write-Code Tasks in\n  Elementary-Level Visual Programming\n\n  We explore the role of quizzes in elementary visual programming domains\npopularly used for K-8 computing education. Prior work has studied various quiz\ntypes, such as fill-in-the-gap write-code questions. However, the overall\nimpact of these quizzes is unclear: studies often show utility in the learning\nphase when enhanced with quizzes, though limited transfer of utility in the\npost-learning phase. In this paper, we aim to better understand the impact of\ndifferent quiz types and whether quizzes focusing on diverse skills (e.g., code\ndebugging and task design) would have higher utility. We design a study with\nHour of Code: Maze Challenge by code.org as the base curriculum, interleaved\nwith different quiz types. Specifically, we examine two learning groups: (i)\nHoC-ACE with diverse quizzes including solution tracing, code debugging, code\nequivalence, and task design; (ii) HoC-Fill with simple quizzes on solution\nfinding. We conducted a large-scale study with 405 students in grades 6--7. Our\nresults highlight that the curriculum enhanced with richer quizzes led to\nhigher utility during the post-learning phase.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.1473,regular,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': ""Funhouse Mirror or Echo Chamber? A Methodological Approach to Teaching\n  Critical AI Literacy Through Metaphors\n\n  As educational institutions grapple with teaching students about increasingly\ncomplex Artificial Intelligence (AI) systems, finding effective methods for\nexplaining these technologies and their societal implications remains a major\nchallenge. This study proposes a methodological approach combining Conceptual\nMetaphor Theory (CMT) with UNESCO's AI competency framework to develop Critical\nAI Literacy (CAIL). Through a systematic analysis of metaphors commonly used to\ndescribe AI systems, we develop criteria for selecting pedagogically\nappropriate metaphors and demonstrate their alignment with established AI\nliteracy competencies, as well as UNESCO's AI competency framework.\n  Our method identifies and suggests four key metaphors for teaching CAIL. This\nincludes GenAI as an echo chamber, GenAI as a funhouse mirror, GenAI as a black\nbox magician, and GenAI as a map. Each of these seeks to address specific\naspects of understanding characteristics of AI, from filter bubbles to\nalgorithmic opacity. We present these metaphors alongside interactive\nactivities designed to engage students in experiential learning of AI concepts.\nIn doing so, we offer educators a structured approach to teaching CAIL that\nbridges technical understanding with societal implications. This work\ncontributes to the growing field of AI education by demonstrating how carefully\nselected metaphors can make complex technological concepts more accessible\nwhile promoting critical engagement with AI systems.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.841255187988281e-06, 'GPT4': 0.0004978179931640625, 'CLAUDE': 0.9990234375, 'GOOGLE': 0.0002980232238769531, 'OPENAI_O_SERIES': 0.00021719932556152344, 'DEEPSEEK': 2.181529998779297e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 6.556510925292969e-07, 'HUMAN': 1.0132789611816406e-06}}"
2412.0033,regular,post_llm,2024,11,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Ethics and Artificial Intelligence Adoption\n\n  In recent years, we have witnessed a marked development and growth in\nArtificial Intelligence. The growth of the data volume generated by sensors and\nmachines, combined with the information flow resulting from the user actions on\nthe Internet, with high investments of the governments and the companies in\nthis area, provided the practice and developed the algorithms of the Artificial\nIntelligence However, the people, in general, started to feel a particular fear\nregarding the security and privacy of their data and the theme of the\nArtificial Intelligence Ethics began to be discussed more regularly. The\ninvestigation aim of this work is to understand the possibility of adopting\nArtificial Intelligence nowadays in our society, having, as a mandatory\nassumption, Ethics and respect towards data and people's privacy. With that\npurpose in mind, a model has been created, mainly supported by the theories\nthat were used to create the model. The suggested model has been tested and\nvalidated through Structural equation modeling based on data taken back from\nthe respondents' answers to the questionnaire online: 237 answers, mainly from\nthe Investigation Technologies area. The results obtained enabled the\nvalidation of seven of the nine investigation hypotheses of the proposed model.\nIt was impossible to confirm any association between the Social Influence\nconstruct and the variables of Behavioral Intention and the Use of Artificial\nIntelligence. The aim of this work was accomplished once the investigation\ntheme was validated and proved that it is possible to adopt Artificial\nIntelligence in our society, using the Attitude Towards Ethical Behavioral\nconstruct as the mainstay of the model.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.1188,review,post_llm,2024,11,"{'ai_likelihood': 1.2583202785915798e-06, 'text': ""Large language models for mental health\n\n  Digital technologies have long been explored as a complement to standard\nprocedure in mental health research and practice, ranging from the management\nof electronic health records to app-based interventions. The recent emergence\nof large language models (LLMs), both proprietary and open-source ones,\nrepresents a major new opportunity on that front. Yet there is still a divide\nbetween the community developing LLMs and the one which may benefit from them,\nthus hindering the beneficial translation of the technology into clinical use.\nThis divide largely stems from the lack of a common language and understanding\nregarding the technology's inner workings, capabilities, and risks. Our\nnarrative review attempts to bridge this gap by providing intuitive\nexplanations behind the basic concepts related to contemporary LLMs.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.10718,review,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': ""Transforming Teacher Education in Developing Countries: The Role of\n  Generative AI in Bridging Theory and Practice\n\n  This study examines the transformative potential of Generative AI (GenAI) in\nteacher education within developing countries, focusing on Ghana, where\nchallenges such as limited pedagogical modeling, performance-based assessments,\nand practitioner-expertise gaps hinder progress. GenAI has the capacity to\naddress these issues by supporting content knowledge acquisition, a role that\ncurrently dominates teacher education programs. By taking on this foundational\nrole, GenAI allows teacher educators to redirect their focus to other critical\nareas, including pedagogical modeling, authentic assessments, and fostering\ndigital literacy and critical thinking. These roles are interconnected,\ncreating a ripple effect where pre-service teachers (PSTs) are better equipped\nto enhance K-12 learning outcomes and align education with workforce needs. The\nstudy emphasizes that GenAI's roles are multifaceted, directly addressing\nresistance to change, improving resource accessibility, and supporting teacher\nprofessional development. However, it cautions against misuse, which could\nundermine critical thinking and creativity, essential skills nurtured through\ntraditional teaching methods. To ensure responsible and effective integration,\nthe study advocates a scaffolding approach to GenAI literacy. This includes\neducating PSTs on its supportive role, training them in ethical use and prompt\nengineering, and equipping them to critically assess AI-generated content for\nbiases and validity. The study concludes by recommending empirical research to\nexplore these roles further and develop practical steps for integrating GenAI\ninto teacher education systems responsibly and effectively.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0003254413604736328, 'GPT4': 0.01470947265625, 'CLAUDE': 0.004329681396484375, 'GOOGLE': 0.9609375, 'OPENAI_O_SERIES': 0.0011587142944335938, 'DEEPSEEK': 0.01551055908203125, 'GROK': 1.6689300537109375e-05, 'NOVA': 1.7821788787841797e-05, 'OTHER': 0.00292205810546875, 'HUMAN': 4.1604042053222656e-05}}"
2411.10939,review,post_llm,2024,11,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Evaluating Generative AI Systems is a Social Science Measurement\n  Challenge\n\n  Across academia, industry, and government, there is an increasing awareness\nthat the measurement tasks involved in evaluating generative AI (GenAI) systems\nare especially difficult. We argue that these measurement tasks are highly\nreminiscent of measurement tasks found throughout the social sciences. With\nthis in mind, we present a framework, grounded in measurement theory from the\nsocial sciences, for measuring concepts related to the capabilities, impacts,\nopportunities, and risks of GenAI systems. The framework distinguishes between\nfour levels: the background concept, the systematized concept, the measurement\ninstrument(s), and the instance-level measurements themselves. This four-level\napproach differs from the way measurement is typically done in ML, where\nresearchers and practitioners appear to jump straight from background concepts\nto measurement instruments, with little to no explicit systematization in\nbetween. As well as surfacing assumptions, thereby making it easier to\nunderstand exactly what the resulting measurements do and do not mean, this\nframework has two important implications for evaluating evaluations: First, it\ncan enable stakeholders from different worlds to participate in conceptual\ndebates, broadening the expertise involved in evaluating GenAI systems. Second,\nit brings rigor to operational debates by offering a set of lenses for\ninterrogating the validity of measurement instruments and their resulting\nmeasurements.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.16919,regular,post_llm,2024,11,"{'ai_likelihood': 3.675619761149089e-06, 'text': ""The Dynamic Creativity of Proto-artifacts in Generative Computational\n  Co-creation\n\n  This paper explores the attributes necessary to determine the creative merit\nof intermediate artifacts produced during a computational co-creative process\n(CCC) in which a human and an artificial intelligence system collaborate in the\ngenerative phase of a creative project. In an active listening experiment,\nsubjects with diverse musical training (N=43) judged unfinished pieces composed\nby the New Electronic Assistant (NEA). The results revealed that a\ntwo-attribute definition based on the value and novelty of an artifact (e.g.,\nCorazza's effectiveness and novelty) suffices to assess unfinished work leading\nto innovative products, instead of Boden's classic three-attribute definition\nof creativity (value, novelty, and surprise). These findings reduce the\ncreativity metrics needed in CCC processes and simplify the evaluation of the\nnumerous unfinished artifacts generated by computational creative assistants.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.03797,regular,post_llm,2024,11,"{'ai_likelihood': 0.6997070312499999, 'text': 'Optimizing Metro Station Locations and Line Layouts in Selangor using\n  Genetic Algorithm Approach: Technical Report\n\n  This report presents an approach for optimizing metro station locations and\nline layouts in the area of Selangor, located in Malaysia. The project utilized\nthe genetic algorithm in identifying the locations and lines layout. With\npopulation in Selangor projected to reach 7.3 million by 2024, the existing\ntransport infrastructure is under increasing strain. This project addresses\nthis challenge by optimizing the metro network to effectively cover the\nexpanding population and minimize travel times. We employed a genetic algorithm\nto achieve these objectives, focusing on both the strategic placement of metro\nstations and the efficient layout of metro lines.\n', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0214996337890625, 'GPT4': 0.016021728515625, 'CLAUDE': 0.00867462158203125, 'GOOGLE': 0.91455078125, 'OPENAI_O_SERIES': 0.002166748046875, 'DEEPSEEK': 0.0001348257064819336, 'GROK': 0.0001697540283203125, 'NOVA': 0.0008130073547363281, 'OTHER': 0.007259368896484375, 'HUMAN': 0.0289306640625}}"
2411.09786,regular,post_llm,2024,11,"{'ai_likelihood': 2.771615982055664e-05, 'text': ""Environmental Burden of United States Data Centers in the Artificial\n  Intelligence Era\n\n  The rapid proliferation of data centers in the US - driven partly by the\nadoption of artificial intelligence - has set off alarm bells about the\nindustry's environmental impact. We compiled detailed information on 2,132 US\ndata centers operating between September 2023 and August 2024 and determined\ntheir electricity consumption, electricity sources, and attributable CO$_{2}$e\nemissions. Our findings reveal that data centers accounted for more than 4% of\ntotal US electricity consumption - with 56% derived from fossil fuels -\ngenerating more than 105 million tons of CO$_{2}$e (2.18% of US emissions in\n2023). Data centers' carbon intensity - the amount of CO$_{2}$e emitted per\nunit of electricity consumed - exceeded the US average by 48%. Our data\npipeline and visualization tools can be used to assess current and future\nenvironmental impacts of data centers.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.10142,regular,post_llm,2024,11,"{'ai_likelihood': 7.318125830756294e-06, 'text': ""First Steps towards K-12 Computer Science Education in Portugal --\n  Experience Report\n\n  Computer scientists Jeannette Wing and Simon Peyton Jones have catalyzed a\npivotal discussion on the need to introduce computing in K-12 mandatory\neducation. In Wing's own words, computing 'represents a universally applicable\nattitude and skill set everyone, not just computer scientists, would be eager\nto learn and use.'' The crux of this educational endeavor lies in its\nexecution. This paper reports on the efforts of the ENSICO association to\nimplement such aims in Portugal. Starting with pilot projects in a few schools\nin 2020, it is currently working with 4500 students, 35 schools and 100 school\nteachers. The main aim is to gain enough experience and knowledge to eventually\ndefine a comprehensive syllabus for teaching computing as a mandatory subject\nthroughout the basic and secondary levels of the Portuguese educational system.\nA structured framework for integrating computational thinking into K-12\neducation is proposed, with a particular emphasis on mathematical modeling and\nthe functional programming paradigm. This approach is chosen for its potential\nto promote analytical and problem-solving skills of computational thinking\naligned with the core background on maths and science.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.15516,regular,post_llm,2024,11,"{'ai_likelihood': 1.5861458248562285e-05, 'text': 'When Image Generation Goes Wrong: A Safety Analysis of Stable Diffusion Models\n\nText-to-image models are increasingly popular and impactful, yet concerns regarding their safety and fairness remain. This study investigates the ability of ten popular Stable Diffusion models to generate harmful images, including NSFW, violent, and personally sensitive material. We demonstrate that these models respond to harmful prompts by generating inappropriate content, which frequently displays troubling biases, such as the disproportionate portrayal of Black individuals in violent contexts. Our findings demonstrate a complete lack of any refusal behavior or safety measures in the models observed. We emphasize the importance of addressing this issue as image generation technologies continue to become more accessible and incorporated into everyday applications.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.08363,regular,post_llm,2024,11,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""On Algorithmic Fairness and the EU Regulations\n\n  The short paper discusses algorithmic fairness by focusing on\nnon-discrimination and a few important laws in the European Union (EU). In\naddition to the EU laws addressing discrimination explicitly, the discussion is\nbased on the EU's recently enacted regulation for artificial intelligence (AI)\nand the older General Data Protection Regulation (GDPR). Through a theoretical\nscenario analysis, on one hand, the paper demonstrates that correcting\ndiscriminatory biases in AI systems can be legally done under the EU\nregulations. On the other hand, the scenarios also illustrate some practical\nscenarios from which legal non-compliance may follow. With these scenarios and\nthe accompanying discussion, the paper contributes to the algorithmic fairness\nresearch with a few legal insights, enlarging and strengthening also the\ngrowing research domain of compliance in AI engineering.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.15907,regular,post_llm,2024,11,"{'ai_likelihood': 1.6788641611735027e-05, 'text': ""Recent insights into the impact of geopolitical tensions: Quantifying\n  the structure of computer science professors of Chinese descent in the United\n  States\n\n  The geopolitical tensions between China and the US have dramatically reshaped\nthe American scientific workforce's landscape. To gain a deeper understanding\nof this circumstance, this study selects the discipline of computer science as\na representative case for empirical investigations, aiming to explore the\ncurrent situation of US-based Chinese-descent computer science professors. One\nthousand and seventy-eight tenured or tenure-track professors of Chinese\ndescent from the computer science departments of 108 prestigious US\nuniversities are profiled, in order to quantify their structure primarily along\ngender, schooling, and expertise lines. The findings presented in this paper\nsuggest that China-US tensions have made it more difficult for the US higher\neducation system to retain valuable computer science professors of Chinese\ndescent, particularly those in their mid-to late career stages, and that nearly\n50% of the existing professors have less than seven years of faculty\nexperience. In addition, the deterioration in faculty retention varies across\nfields of research, education backgrounds, and gender groups. Specifically,\namong the professors we are concerned about, those who do not work on AI or\nSystems, those who lack study experience at US universities, and those who are\nwomen, are underrepresented, albeit in different forms and to varying degrees.\nIn a nutshell, the focal professoriate has not only shrunk in size, as has been\nwidely reported, but also lost some of its diversity in structure. This paper\nhas policy implications for the mobility of scientific talent, especially in an\nera of geopolitical challenges.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.0449,review,post_llm,2024,11,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""Smoke Screens and Scapegoats: The Reality of General Data Protection\n  Regulation Compliance -- Privacy and Ethics in the Case of Replika AI\n\n  Currently artificial intelligence (AI)-enabled chatbots are capturing the\nhearts and imaginations of the public at large. Chatbots that users can build\nand personalize, as well as pre-designed avatars ready for users' selection,\nall of these are on offer in applications to provide social companionship,\nfriends and even love. These systems, however, have demonstrated challenges on\nthe privacy and ethics front. This paper takes a critical approach towards\nexamining the intricacies of these issues within AI companion services. We\nchose Replika as a case and employed close reading to examine the service's\nprivacy policy. We additionally analyze articles from public media about the\ncompany and its practices to gain insight into the trustworthiness and\nintegrity of the information provided in the policy. The aim is to ascertain\nwhether seeming General Data Protection Regulation (GDPR) compliance equals\nreliability of required information, or whether the area of GDPR compliance in\nitself is one riddled with ethical challenges. The paper contributes to a\ngrowing body of scholarship on ethics and privacy related matters in the sphere\nof social chatbots. The results reveal that despite privacy notices, data\ncollection practices might harvest personal data without users' full awareness.\nCross-textual comparison reveals that privacy notice information does not fully\ncorrespond with other information sources.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.05325,regular,post_llm,2024,11,"{'ai_likelihood': 0.0008842680189344619, 'text': ""UKTF: Unified Knowledge Tracing Framework for Subjective and Objective\n  Assessments\n\n  With the continuous deepening and development of the concept of smart\neducation, learners' comprehensive development and individual needs have\nreceived increasing attention. However, traditional educational evaluation\nsystems tend to assess learners' cognitive abilities solely through general\ntest scores, failing to comprehensively consider their actual knowledge states.\nKnowledge tracing technology can establish knowledge state models based on\nlearners' historical answer data, thereby enabling personalized assessment of\nlearners. Nevertheless, current classical knowledge tracing models are\nprimarily suited for objective test questions, while subjective test questions\nstill confront challenges such as complex data representation, imperfect\nmodeling, and the intricate and dynamic nature of knowledge states. Drawing on\nthe application of knowledge tracing technology in education, this study aims\nto fully utilize examination data and proposes a unified knowledge tracing\nmodel that integrates both objective and subjective test questions. Recognizing\nthe differences in question structure, assessment methods, and data\ncharacteristics between objective and subjective test questions, the model\nemploys the same backbone network for training both types of questions.\nSimultaneously, it achieves knowledge tracing for subjective test questions by\nuniversally modifying the training approach of the baseline model, adding\nbranch networks, and optimizing the method of question encoding. This study\nconducted multiple experiments on real datasets, and the results consistently\ndemonstrate that the model effectively addresses knowledge tracing issues in\nboth objective and subjective test question scenarios.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.02025,review,post_llm,2024,11,"{'ai_likelihood': 5.728668636745877e-06, 'text': ""Towards the design of model-based means and methods to characterize and\n  diagnose teachers' digital maturity\n\n  This article examines how models of teacher digital maturity can be combined\nto produce a unified version that can be used to design diagnostic tools and\nmethods. 11 models applicable to the field of compulsory education were\nidentified through a literature review. The models and how their constituent\ndimensions contribute to the determination of maturity levels were analyzed.\nThe summary highlights the diversity of the dimensions used and the fact that\ndigital maturity is only partially taken into account. What's more, most of\nthese models focus on the most recent maturity levels associated with\ninnovative or pioneering teachers. The models tend to exclude teachers who are\nnot digital users or who have a low level of digital use, but who are present\nin the French context. In the final part of the article, a proposal for a\nunified model of teachers' digital maturity, MUME, which addresses these two\nissues, is described, together with the preliminary results of a study aimed at\ndesigning a diagnostic method.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.08294,regular,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': ""Collaborative Participatory Research with LLM Agents in South Asia: An\n  Empirically-Grounded Methodological Initiative and Agenda from Field Evidence\n  in Sri Lanka\n\n  The integration of artificial intelligence into development research\nmethodologies presents unprecedented opportunities for addressing persistent\nchallenges in participatory research, particularly in linguistically diverse\nregions like South Asia. Drawing from an empirical implementation in Sri\nLanka's Sinhala-speaking communities, this paper presents an empirically\ngrounded methodological framework designed to transform participatory\ndevelopment research, situated in the challenging multilingual context of Sri\nLanka's flood-prone Nilwala River Basin. Moving beyond conventional translation\nand data collection tools, this framework deploys a multi-agent system\narchitecture that redefines how data collection, analysis, and community\nengagement are conducted in linguistically and culturally diverse research\nsettings. This structured agent-based approach enables participatory research\nthat is both scalable and responsive, ensuring that community perspectives\nremain integral to research outcomes. Field experiences reveal the immense\npotential of LLM-based systems in addressing long-standing issues in\ndevelopment research across resource-limited regions, offering both\nquantitative efficiencies and qualitative improvements in inclusivity. At a\nbroader methodological level, this research agenda advocates for AI-driven\nparticipatory research tools that maintain ethical considerations, cultural\nrespect, and operational efficiency, highlighting strategic pathways for\ndeploying AI systems that reinforce community agency and equitable knowledge\ngeneration, potentially informing broader research agendas across the Global\nSouth.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.76837158203125e-07, 'GPT4': 0.96826171875, 'CLAUDE': 0.0316162109375, 'GOOGLE': 5.543231964111328e-06, 'OPENAI_O_SERIES': 2.2649765014648438e-06, 'DEEPSEEK': 2.181529998779297e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 2.6106834411621094e-05}}"
2411.11166,review,post_llm,2024,11,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Early Adoption of Generative Artificial Intelligence in Computing\n  Education: Emergent Student Use Cases and Perspectives in 2023\n\n  Because of the rapid development and increasing public availability of\nGenerative Artificial Intelligence (GenAI) models and tools, educational\ninstitutions and educators must immediately reckon with the impact of students\nusing GenAI. There is limited prior research on computing students' use and\nperceptions of GenAI. In anticipation of future advances and evolutions of\nGenAI, we capture a snapshot of student attitudes towards and uses of yet\nemerging GenAI, in a period of time before university policies had reacted to\nthese technologies. We surveyed all computer science majors in a small\nengineering-focused R1 university in order to: (1) capture a baseline\nassessment of how GenAI has been immediately adopted by aspiring computer\nscientists; (2) describe computing students' GenAI-related needs and concerns\nfor their education and careers; and (3) discuss GenAI influences on CS\npedagogy, curriculum, culture, and policy. We present an exploratory\nqualitative analysis of this data and discuss the impact of our findings on the\nemerging conversation around GenAI and education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.05588,review,post_llm,2024,11,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'An Evidence-Based Curriculum Initiative for Hardware Reverse Engineering\n  Education\n\n  The increasing importance of supply chain security for digital devices --\nfrom consumer electronics to critical infrastructure -- has created a high\ndemand for skilled cybersecurity experts. These experts use Hardware Reverse\nEngineering (HRE) as a crucial technique to ensure trust in digital\nsemiconductors. Recently, the US and EU have provided substantial funding to\neducate this cybersecurity-ready semiconductor workforce, but success depends\non the widespread availability of academic training programs. In this paper, we\ninvestigate the current state of education in hardware security and HRE to\nidentify efficient approaches for establishing effective HRE training programs.\nThrough a systematic literature review, we uncover 13 relevant courses,\nincluding eight with accompanying academic publications. We identify common\ntopics, threat models, key pedagogical features, and course evaluation methods.\nWe find that most hardware security courses do not prioritize HRE, making HRE\ntraining scarce. While the predominant course structure of lectures paired with\nhands-on projects appears to be largely effective, we observe a lack of\nstandardized evaluation methods and limited reliability of student\nself-assessment surveys. Our results suggest several possible improvements to\nHRE education and offer recommendations for developing new training courses. We\nadvocate for the integration of HRE education into curriculum guidelines to\nmeet the growing societal and industry demand for HRE experts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.12709,review,post_llm,2024,11,"{'ai_likelihood': 2.8179751502143012e-05, 'text': 'Dimensions of Generative AI Evaluation Design\n\n  There are few principles or guidelines to ensure evaluations of generative AI\n(GenAI) models and systems are effective. To help address this gap, we propose\na set of general dimensions that capture critical choices involved in GenAI\nevaluation design. These dimensions include the evaluation setting, the task\ntype, the input source, the interaction style, the duration, the metric type,\nand the scoring method. By situating GenAI evaluations within these dimensions,\nwe aim to guide decision-making during GenAI evaluation design and provide a\nstructure for comparing different evaluations. We illustrate the utility of the\nproposed set of general dimensions using two examples: a hypothetical\nevaluation of the fairness of a GenAI system and three real-world GenAI\nevaluations of biological threats.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.14437,review,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': 'Transforming Business with Generative AI: Research, Innovation, Market\n  Deployment and Future Shifts in Business Models\n\n  This paper explores the transformative impact of Generative AI (GenAI) on the\nbusiness landscape, examining its role in reshaping traditional business\nmodels, intensifying market competition, and fostering innovation. By applying\nthe principles of Neo-Schumpeterian economics, the research analyses how GenAI\nis driving a new wave of ""creative destruction,"" leading to the emergence of\nnovel business paradigms and value propositions. The findings reveal that GenAI\nenhances operational efficiency, facilitates product and service innovation,\nand creates new revenue streams, positioning it as a powerful catalyst for\nsubstantial shifts in business structures and strategies. However, the\ndeployment of GenAI also presents significant challenges, including ethical\nconcerns, regulatory demands, and the risk of job displacement. By addressing\nthe multifarious nature of GenAI, this paper provides valuable insights for\nbusiness leaders, policymakers, and researchers, guiding them towards a\nbalanced and responsible integration of this transformative technology.\nUltimately, GenAI is not merely a technological advancement but a driver of\nprofound change, heralding a future where creativity, efficiency, and growth\nare redefined.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004930496215820312, 'GPT4': 0.3857421875, 'CLAUDE': 0.00011157989501953125, 'GOOGLE': 0.609375, 'OPENAI_O_SERIES': 0.0009016990661621094, 'DEEPSEEK': 4.506111145019531e-05, 'GROK': 1.4662742614746094e-05, 'NOVA': 0.00016880035400390625, 'OTHER': 0.0032196044921875, 'HUMAN': 1.1920928955078125e-06}}"
2411.167,regular,post_llm,2024,11,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Exploring the determinants on massive open online courses continuance\n  learning intention in business toward accounting context\n\n  Massive open online courses (MOOC) have become important in the learning\njourney of college students and have been extensively implemented in higher\neducation. However, there are few studies that investigated the willingness to\ncontinue using Massive open online courses (MOOC) in the field of business in\nhigher education. Therefore, this paper proposes a comprehensive theoretical\nresearch framework based on the Theory of Planned Behavior (TPB). In the field\nof business, a representative accounting course is taken as an example. We\nadopt the questionnaire survey method and use the partial least squares\nstructural equation model to analyze the collected feedback data from college\nstudents and test the hypotheses. This paper focuses on the potential\ninfluencing factors and mechanisms of the willingness to continuously use\nMassive open online courses (MOOC) in accounting. The results show that\ninterface convenience (IC) and interface design aesthetics (IDA) have positive\neffects on user attitude (ATT). User attitude (ATT), perceived behavioral\ncontrol (PBC), and subjective norms (SN) have positive effects on the\ncontinuance learning intention. In addition, academic self-efficacy (EF) not\nonly significantly affects continuance learning intention (CI) but also\nmoderates the relationship between the Theory of Planned Behavior (user\nattitude, perceived behavior control, subjective norms) and the continuance\nlearning intention of accounting MOOC. Therefore, the Theory of Planned\nBehavior(TPB) is extended in social science accounting Massive open online\ncourses environment. Based on these findings, this paper provides several\ntheoretical and practical implications for researchers and practitioners of\nMOOC, accounting, and the design of learning systems in higher education\ncontexts.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.08717,regular,post_llm,2024,11,"{'ai_likelihood': 0.004840426974826389, 'text': ""Short note on the mapping of heritage sites impacted by the 2024 floods\n  in Valencia, Spain\n\n  This short note presents preliminary findings on the impact of the October\n2024 floods on cultural heritage sites in Valencia, Spain. Using publicly\navailable data, we assess the extent of potential damage by overlaying flood\nmaps with heritage site coordinates. We identify that 3.3% of heritage sites in\nthe region have been potentially impacted, with churches and shrines (81),\noutdoor religious iconography (78), and historic irrigation features (45) being\nthe most heavily affected. Our analysis utilizes data from OpenStreetMap and\nlistings from the Generalitat Valenciana, suggesting that while OpenStreetMap's\ncrowd-sourced data can provide useful estimates of the proportion of impacted\nsites, it may not be suitable for a detailed damage assessment. By sharing this\ndata openly, we aim to contribute to international efforts in preserving\ncultural heritage after the disaster and provide a foundation for future\nassessments of heritage site vulnerability to climate-related events.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.09313,review,post_llm,2024,11,"{'ai_likelihood': 0.2799479166666667, 'text': ""Socio-Economic Consequences of Generative AI: A Review of Methodological\n  Approaches\n\n  The widespread adoption of generative artificial intelligence (AI) has\nfundamentally transformed technological landscapes and societal structures in\nrecent years. Our objective is to identify the primary methodologies that may\nbe used to help predict the economic and social impacts of generative AI\nadoption. Through a comprehensive literature review, we uncover a range of\nmethodologies poised to assess the multifaceted impacts of this technological\nrevolution. We explore Agent-Based Simulation (ABS), Econometric Models,\nInput-Output Analysis, Reinforcement Learning (RL) for Decision-Making Agents,\nSurveys and Interviews, Scenario Analysis, Policy Analysis, and the Delphi\nMethod. Our findings have allowed us to identify these approaches' main\nstrengths and weaknesses and their adequacy in coping with uncertainty,\nrobustness, and resource requirements.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.1004,review,post_llm,2024,11,"{'ai_likelihood': 2.0662943522135418e-05, 'text': ""Hollywood's misrepresentation of death: A comparison of overall and by-gender mortality causes in film and the real world\n\nThe common phrase 'representation matters' asserts that media has a measurable and important impact on civic society's perception of self and others. The representation of health in media, in particular, may reflect and perpetuate a society's disease burden. Here, for the top 10 major causes of death in the United States, we examine how cinematic representation of overall and by-gender mortality diverges from reality. Using crowd-sourced data on film deaths from Cinemorgue Wiki, we employ natural language processing (NLP) techniques to analyze shifts in representation of deaths in movies versus the 2021 National Vital Statistic Survey (NVSS) top ten mortality causes. Overall, movies strongly overrepresent suicide and, to a lesser degree, accidents. In terms of gender, movies overrepresent men and underrepresent women for nearly every major mortality cause, including heart disease and cerebrovascular disease. The two exceptions for which women are overrepresented are suicide and accidents. We discuss the implications of under- and over-representing causes of death overall and by gender, as well as areas of future research."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.07705,regular,post_llm,2024,11,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'dpvis: A Visual and Interactive Learning Tool for Dynamic Programming\n\n  Dynamic programming (DP) is a fundamental and powerful algorithmic paradigm\ntaught in most undergraduate (and many graduate) algorithms classes. DP\nproblems are challenging for many computer science students because they\nrequire identifying unique problem structures and a refined understanding of\nrecursion. In this paper, we present dpvis, a Python library that helps\nstudents understand DP through a frame-by-frame animation of dynamic programs.\ndpvis can easily generate animations of dynamic programs with as little as two\nlines of modifications compared to a standard Python implementation. For each\nframe, dpvis highlight the cells that have been read from and written to during\nan iteration. Moreover, dpvis allows users to test their understanding by\nprompting them with questions about the next operation performed by the\nalgorithm.\n  We deployed dpvis as a learning tool in an undergraduate algorithms class,\nand report on the results of a survey. The survey results suggest that dpvis is\nespecially helpful for visualizing the recursive structure of DP. Although some\nstudents struggled with the installation of the tool (which has been simplified\nsince the reported deployment), essentially all other students found the tool\nto be useful for understanding dynamic programs. dpvis is available at\nhttps://github.com/itsdawei/dpvis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.0778,review,post_llm,2024,11,"{'ai_likelihood': 0.30517578125, 'text': ""A Taxonomy of Systemic Risks from General-Purpose AI\n\n  Through a systematic review of academic literature, we propose a taxonomy of\nsystemic risks associated with artificial intelligence (AI), in particular\ngeneral-purpose AI. Following the EU AI Act's definition, we consider systemic\nrisks as large-scale threats that can affect entire societies or economies.\nStarting with an initial pool of 1,781 documents, we analyzed 86 selected\npapers to identify 13 categories of systemic risks and 50 contributing sources.\nOur findings reveal a complex landscape of potential threats, ranging from\nenvironmental harm and structural discrimination to governance failures and\nloss of control. Key sources of systemic risk emerge from knowledge gaps,\nchallenges in recognizing harm, and the unpredictable trajectory of AI\ndevelopment. The taxonomy provides a snapshot of current academic literature on\nsystemic risks. This paper contributes to AI safety research by providing a\nstructured groundwork for understanding and addressing the potential\nlarge-scale negative societal impacts of general-purpose AI. The taxonomy can\ninform policymakers in risk prioritization and regulatory development.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.16814,regular,post_llm,2024,11,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Post Guidance for Online Communities\n\n  Effective content moderation in online communities is often a delicate\nbalance between maintaining content quality and fostering user participation.\nIn this paper, we introduce post guidance, a novel approach to community\nmoderation that proactively guides users' contributions using rules that\ntrigger interventions as users draft a post to be submitted. For instance,\nrules can surface messages to users, prevent post submissions, or flag posted\ncontent for review. This uniquely community-specific, proactive, and\nuser-centric approach can increase adherence to rules without imposing\nadditional burdens on moderators. We evaluate a version of Post Guidance\nimplemented on Reddit, which enables the creation of rules based on both post\ncontent and account characteristics, via a large randomized experiment,\ncapturing activity from 97,616 posters in 33 subreddits over 63 days. We find\nthat Post Guidance (1) increased the number of ``successful posts'' (posts not\nremoved after 72 hours), (2) decreased moderators' workload in terms of\nmanually-reviewed reports, (3) increased contribution quality, as measured by\ncommunity engagement, and (4) had no impact on posters' own subsequent\nactivity, within communities adopting the feature. Post Guidance on Reddit was\nsimilarly effective for community veterans and newcomers, with greater benefits\nin communities that used the feature more extensively. Our findings indicate\nthat post guidance represents a transformative approach to content moderation,\nembodying a paradigm that can be easily adapted to other platforms to improve\nonline communities across the Web.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.18862,regular,post_llm,2024,11,"{'ai_likelihood': 6.688965691460504e-06, 'text': 'Capstone Experiences in Developing Augmented Reality Tables for\n  Community Organizations\n\n  This paper examines two senior capstone experiences developed as augmented\nreality tables over the past two years. Both projects were public facing\nefforts that required working implementations. The first project was deployed\nat an astronomy center and focused on interactions between land use and\necological aspects of Hawaii Island while the second project focused more on\nhistorical sites on the same island. Both projects leveraged brownfield\ndevelopment and existing code bases to allow for student success in spite of\nthe impacts of the COVID19 pandemic.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.18628,regular,post_llm,2024,11,"{'ai_likelihood': 6.291601392957899e-06, 'text': 'Cohort profile: the Northwest China Real-world and Population-based\n  Cohort\n\n  The Northwest China Real-World and Population-based cohort is an ongoing\nprospective cohort with more than 25 million population, covering almost all\nresidents across approximately 1.66 million square kilometers in northwest\nChina; The cohort integrates data from various sources, including health\nprofiles, examination records, electronic health records, mortality records,\nstatistical yearbooks, and environmental datasets, covering comprehensive\nhealth-related factors such as demographics, lifestyle factors, family medical\nhistory, living conditions, enrollment in national public health services,\nphysical examinations, blood assay tests, diagnostic assessments, disease\noutcomes, and cause-specific mortality. This real-world dataset can evaluate\nclinical treatment effectiveness and prognosis, assess impact of health policy,\nand investigate the health effects of multiple risk factors . From January 2019\nto December 2023, the cohort has included 13,634,481 participants, accumulating\n47,050,707 person-years of follow-up, with 13,598,407 medical diagnosis records\nand 881,114 recorded deaths. Cohort data are available upon request.\nDe-identified and anonymized data are stored on local servers and accessed\nthrough a data-sharing platform, enabling users to utilize the data without\ndirect access to the raw information. A description of the proposed research\ncan be sent to Yining Yang & Qian Di.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.10995,regular,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': 'From Crime to Hypercrime: Evolving Threats and Law Enforcement\'s New\n  Mandate in the AI Age\n\n  The paper examines the trajectory of crime, tracing its evolution from\ntraditional forms to digital manifestations in cybercrime, and proposes\n""Hypercrime"" as the latest frontier. Leveraging insights from Michael McGuire\'s\n""Hypercrime: The New Geometry of Harm,"" the study calls for a paradigm shift in\nlaw enforcement strategies to meet the challenges posed by AI-driven\nhypercrime. Emphasis is placed on understanding hypercrime\'s complexity,\ndeveloping proactive policies, and embracing technological tools to mitigate\nrisks associated with AI misuse.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0013599395751953125, 'GPT4': 0.04071044921875, 'CLAUDE': 0.0009365081787109375, 'GOOGLE': 0.9443359375, 'OPENAI_O_SERIES': 0.006214141845703125, 'DEEPSEEK': 0.0041046142578125, 'GROK': 9.042024612426758e-05, 'NOVA': 0.0003273487091064453, 'OTHER': 0.0019702911376953125, 'HUMAN': 1.0967254638671875e-05}}"
2411.14439,regular,post_llm,2024,11,"{'ai_likelihood': 0.2983940972222222, 'text': ""Windstorm Economic Impacts on the Spanish Resilience: A Machine Learning\n  Real-Data Approach\n\n  Climate change-associated disasters have become a significant concern,\nprincipally when affecting urban areas. Assessing these regions' resilience to\nstrengthen their disaster management is crucial, especially in the areas\nvulnerable to windstorms, one of Spain's most critical disasters. Smart cities\nand machine learning offer promising solutions to manage disasters, but\naccurately estimating economic losses from windstorms can be difficult due to\nthe unique characteristics of each region and limited data. This study proposes\nutilizing ML classification models to enhance disaster resilience by analyzing\npublicly available data on windstorms in the Spanish areas. This approach can\nhelp decision-makers make informed decisions regarding preparedness and\nmitigation actions, ultimately creating a more resilient urban environment that\ncan better withstand windstorms in the future.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.11527,regular,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': 'Design and Development of a Localized E-Commerce Solution for Students\n  focussing on Economical Sharing\n\n  The rapid adoption of e-commerce has transformed how students access goods\nand resources. However, existing platforms often fail to address the specific\nneeds of campus communities, where students face challenges such as financial\nconstraints, lack of access to affordable goods, and inefficient resource\ncirculation. This research proposes ShareSpace, a localized web application\ndesigned specifically for college students to facilitate the buying, and\nselling of mainly second-hand goods. By addressing imbalances like surplus\nitems left behind by seniors and shortages experienced by juniors, ShareSpace\npromotes sustainability and affordability within the campus ecosystem.\nLeveraging modern technologies such as Node.js, React.js, and MongoDB, the\nproject demonstrates the feasibility of creating a student-centric e-commerce\nsolution. The study highlights how ShareSpace solves the challenges of\neconomical pricing and content moderation using proposed solutions. This study\nalso explores the limitations of existing solutions and evaluates the potential\nof ShareSpace to encourage sustainable consumption and resourcefulness among\nstudents.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0006079673767089844, 'GPT4': 0.288330078125, 'CLAUDE': 0.00821685791015625, 'GOOGLE': 0.2493896484375, 'OPENAI_O_SERIES': 0.420166015625, 'DEEPSEEK': 0.0253143310546875, 'GROK': 0.0003333091735839844, 'NOVA': 0.000934600830078125, 'OTHER': 0.0066680908203125, 'HUMAN': 6.23464584350586e-05}}"
2411.16689,regular,post_llm,2024,11,"{'ai_likelihood': 8.742014567057293e-06, 'text': 'Factors Influencing the Usage of Mobile Banking Apps Among Malaysian\n  Consumers\n\n  Mobile banking apps have transformed the banking sector by offering customers\nwith convenient, secure and easily accessible financial services. Even so, it\nis crucial for banks and the mobile banking apps developers to understand the\nfactors that influence the utilisation of these apps among Malaysian consumer.\nThis study will examine the influence of several factors which are security\nconcerns, service quality, technological factors and convenience, on the usage\nof mobile banking apps. The study aims to discover the key factors that affect\nthe usage of mobile banking apps. A quantitative research method was utilised,\nwhich involves the collection of data from an online survey. The survey managed\nto collect data from 152 respondents who are above 18 years old and users of\nmobile banking apps in Malaysia. The data was analysed with correlation\nanalyses to examine the relationship between the variables. A multinominal\nlogistic regression model was used as a predictive model to predict the usage\nof mobile banking apps. This study contributes to existing researches by\nhighlighting the importance of security and convenience into the development\nand marketing strategies of mobile banking apps. The study can help them\nconduct improvements on their current apps and thus increase the usage of\nmobile banking apps among consumers in Malaysia.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.18833,review,post_llm,2024,11,"{'ai_likelihood': 9.053283267550998e-05, 'text': 'The Method of Critical AI Studies, A Propaedeutic\n\n  We outline some common methodological issues in the field of critical AI\nstudies, including a tendency to overestimate the explanatory power of\nindividual samples (the benchmark casuistry), a dependency on theoretical\nframeworks derived from earlier conceptualizations of computation (the black\nbox casuistry), and a preoccupation with a cause-and-effect model of\nalgorithmic harm (the stack casuistry). In the face of these issues, we call\nfor, and point towards, a future set of methodologies that might take into\naccount existing strengths in the humanistic close analysis of cultural\nobjects.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.177,regular,post_llm,2024,11,"{'ai_likelihood': 0.0009748670789930556, 'text': ""Exploring the Impact of Generative AI on Cross-Border E-Commerce Brand\n  Building in Chinese Tianjin's Manufacturing Sector\n\n  This study investigates the influence of generative artificial intelligence\n(AI) on the brand construction of cross-border e-commerce companies in the\nmanufacturing industry in Tianjin, China. We examine the direct effects of\ngenerative AI on productivity, the mediating role of productivity in the\nrelationship between generative AI and brand building, and the moderating\ninfluence of cross-border e-commerce strategies by developing and testing a\ncomprehensive model. Based on data collected from 210 manufacturing firms in\nChinese Tianjin, the results show that generative AI significantly increases\nproductivity, which positively affects branding. Moreover, cross-border\ne-commerce strategies were found to moderate the impact of generative AI on\nbranding, underscoring the importance of these strategies for using AI\ntechnologies to compete successfully in the global marketplace. This study\nprovides valuable theory, empiricism and practical contributions to\nunderstanding the role AI plays in manufacturing and electronic commerce.\nBesides, this study tests several hypotheses to quantify these impacts using a\nstructured model that consists of independent, dependent, mediating and\nmoderating variables. Information is collected through a comprehensive survey\nof manufacturers in Chinese Tianjin and analyzed to test our proposed model.\nThis study was analyzed and summarized using quantitative analysis, regression\nand structural equations (PLS-SEM).\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.05782,regular,post_llm,2024,11,"{'ai_likelihood': 2.8808911641438803e-06, 'text': 'Gender Inequalities in Content Collaborations: Asymmetric Creator\n  Synergy and Symmetric Audience Biases\n\n  Content-creator collaborations are a widespread strategy for enhancing\ndigital viewership and revenue. While existing research has explored the\nefficacy of collaborations, few have looked at inequities in collaborations,\nparticularly from the perspective of the supply and demand of attention.\nLeveraging 42,376 videos and 6,117,441 comments from YouTube (across 150\nchannels and 3 games), this study examines gender inequality in collaborative\nenvironments. Utilizing Shapley value, a tool from cooperative game theory,\nresults reveal dominant in-group collaborations based on in-game affordances.\nHowever, audience responses are aligned across games, reflecting symmetric\nbiases across the gaming communities, with comments focusing more on\nperipherals than actual gameplay for women. We find supply-side asymmetries\nexist along with demand-side symmetries. Our results engage with the larger\nliterature on digital and online biases, highlighting how genre and affordances\nmoderate gendered collaboration, the direction of inequality, and contributing\na general framework to quantify synergy across collaborations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.10547,review,post_llm,2024,11,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'AI Safety Frameworks Should Include Procedures for Model Access\n  Decisions\n\n  The downstream use cases, benefits, and risks of AI models depend\nsignificantly on what sort of access is provided to the model, and who it is\nprovided to. Though existing safety frameworks and AI developer usage policies\nrecognise that the risk posed by a given model depends on the level of access\nprovided to a given audience, the procedures they use to make decisions about\nmodel access are ad hoc, opaque, and lacking in empirical substantiation. This\npaper consequently proposes that frontier AI companies build on existing safety\nframeworks by outlining transparent procedures for making decisions about model\naccess, which we term Responsible Access Policies (RAPs). We recommend that, at\na minimum, RAPs should include the following: i) processes for empirically\nevaluating model capabilities given different styles of access, ii) processes\nfor assessing the risk profiles of different categories of user, and iii) clear\nand robust pre-commitments regarding when to grant or revoke specific types of\naccess for particular groups under specified conditions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.05992,regular,post_llm,2024,11,"{'ai_likelihood': 6.490283542209202e-06, 'text': ""Other Worlds: Using AI to Revisit Cybersyn and Rethink Economic Futures\n\n  Neoliberalism has become orthodoxy in the present, erasing competing\nparadigms and alternative imaginings. Chile's radical Cybersyn project from\n1971 to 1973 offers a departure point for an alternative path, albeit one that\nwas abruptly and violently extinguished. We revisit this moment by fine-tuning\nAI language models on the words and writing of Salvador Allende, the Chilean\nPresident, and Stafford Beer, the cyberneticist who helped to design the\nproject. We conduct interviews with these simulated personas, focusing on how\ntheir revolutionary ideas might be taken up in the present. We then use an AI\nmodel to generate five-year-plans from 1973 to the present, simulating an\nalternate history guided by Cybersyn and a progressive agenda. We frame these\ninterventions as socialist infrastructuring that cultivates a more expansive\nsocialist imagining. This work is not about the viability of planned economies,\nbut about the 'inspirability' of exploring other value-systems in the present,\nallowing us to break out of our future-on-rails to envision alternative ways of\norganizing economy and society.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.00479,regular,post_llm,2024,11,"{'ai_likelihood': 8.410877651638455e-06, 'text': ""Beyond time delays: How web scraping distorts measures of online news\n  consumption\n\n  As the exploration of digital behavioral data revolutionizes communication\nresearch, understanding the nuances of data collection methodologies becomes\nincreasingly pertinent. This study focuses on one prominent data collection\napproach, web scraping, and more specifically, its application in the growing\nfield of research relying on web browsing data. We investigate discrepancies\nbetween content obtained directly during user interaction with a website\n(in-situ) and content scraped using the URLs of participants' logged visits\n(ex-situ) with various time delays (0, 30, 60, and 90 days). We find\nsubstantial disparities between the methodologies, uncovering that errors are\nnot uniformly distributed across news categories regardless of classification\nmethod (domain, URL, or content analysis). These biases compromise the\nprecision of measurements used in existing literature. The ex-situ collection\nenvironment is the primary source of the discrepancies (~33.8%), while the time\ndelays in the scraping process play a smaller role (adding ~6.5 percentage\npoints in 90 days). Our research emphasizes the need for data collection\nmethods that capture web content directly in the user's environment. However,\nacknowledging its complexities, we further explore strategies to mitigate\nbiases in web-scraped browsing histories, offering recommendations for\nresearchers who rely on this method and laying the groundwork for developing\nerror-correction frameworks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.02307,review,post_llm,2024,11,"{'ai_likelihood': 1.0, 'text': 'Can Personalized Medicine Coexist with Health Equity? Examining the Cost\n  Barrier and Ethical Implications\n\n  Personalized medicine (PM) promises to transform healthcare by providing\ntreatments tailored to individual genetic, environmental, and lifestyle\nfactors. However, its high costs and infrastructure demands raise concerns\nabout exacerbating health disparities, especially between high-income countries\n(HICs) and low- and middle-income countries (LMICs). While HICs benefit from\nadvanced PM applications through AI and genomics, LMICs often lack the\nresources necessary to adopt these innovations, leading to a widening\nhealthcare divide. This paper explores the financial and ethical challenges of\nPM implementation, with a focus on ensuring equitable access. It proposes\nstrategies for global collaboration, infrastructure development, and ethical\nframeworks to support LMICs in adopting PM, aiming to prevent further\ndisparities in healthcare accessibility and outcomes.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0019683837890625, 'GPT4': 0.666015625, 'CLAUDE': 0.01117706298828125, 'GOOGLE': 0.2056884765625, 'OPENAI_O_SERIES': 0.034149169921875, 'DEEPSEEK': 0.07330322265625, 'GROK': 0.0035228729248046875, 'NOVA': 0.0015478134155273438, 'OTHER': 0.00250244140625, 'HUMAN': 3.892183303833008e-05}}"
2411.15662,regular,post_llm,2024,11,"{'ai_likelihood': 1.2616316477457683e-05, 'text': 'Gaps Between Research and Practice When Measuring Representational Harms\n  Caused by LLM-Based Systems\n\n  To facilitate the measurement of representational harms caused by large\nlanguage model (LLM)-based systems, the NLP research community has produced and\nmade publicly available numerous measurement instruments, including tools,\ndatasets, metrics, benchmarks, annotation instructions, and other techniques.\nHowever, the research community lacks clarity about whether and to what extent\nthese instruments meet the needs of practitioners tasked with developing and\ndeploying LLM-based systems in the real world, and how these instruments could\nbe improved. Via a series of semi-structured interviews with practitioners in a\nvariety of roles in different organizations, we identify four types of\nchallenges that prevent practitioners from effectively using publicly available\ninstruments for measuring representational harms caused by LLM-based systems:\n(1) challenges related to using publicly available measurement instruments; (2)\nchallenges related to doing measurement in practice; (3) challenges arising\nfrom measurement tasks involving LLM-based systems; and (4) challenges specific\nto measuring representational harms. Our goal is to advance the development of\ninstruments for measuring representational harms that are well-suited to\npractitioner needs, thus better facilitating the responsible development and\ndeployment of LLM-based systems.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.03187,regular,post_llm,2024,11,"{'ai_likelihood': 0.99755859375, 'text': 'Design-Reality Gap Analysis of Health Information Systems Failure\n\n  This study investigates the factors contributing to the failure of Health\nInformation Systems (HIS) in a public hospital in South Africa. While HIS have\nthe potential to improve healthcare delivery by integrating services and\nenhancing effectiveness, failures can lead to service interruptions, revenue\nloss, data loss, administrative difficulties, and reputational damage. Using\nsemi-structured interviews with key stakeholders, we employed a hybrid data\nanalysis approach combining deductive analysis based on the Design- Reality Gap\nModel and inductive thematic analysis. Our findings highlight several factors\ncontributing to HIS failures, including system capacity constraints, inadequate\nIT risk management, and critical skills gaps. Despite these challenges, end\nusers perceive HIS positively and recommend its implementation for streamlining\ndaily processes. This study underscores the importance of addressing\ndesign-reality gaps to improve HIS outcomes in public healthcare settings.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.11480712890625, 'GPT4': 0.01126861572265625, 'CLAUDE': 0.0906982421875, 'GOOGLE': 0.3134765625, 'OPENAI_O_SERIES': 0.002544403076171875, 'DEEPSEEK': 0.003261566162109375, 'GROK': 0.00021588802337646484, 'NOVA': 0.002910614013671875, 'OTHER': 0.46044921875, 'HUMAN': 0.000370025634765625}}"
2411.10225,regular,post_llm,2024,11,"{'ai_likelihood': 0.02173529730902778, 'text': ""Virtual Reality in Teacher Education: Insights from Pre-Service Teachers\n  in Resource-limited Regions\n\n  This study explores the perceptions, challenges, and opportunities associated\nwith using Virtual Reality (VR) as a tool in teacher education among\npre-service teachers in a resource-limited setting. Utilizing a qualitative\ncase study design, the study draws on the experiences and reflections of 36\nGhanaian pre-service teachers who engaged with VR in a facilitated lesson for\nthe first time. Findings reveal that initial exposure to VR generated a\npositive perception, with participants highlighting VR's potential as an\nengaging and interactive tool that can support experiential learning. Notably,\nmany participants saw the VR-facilitated lesson as a promising alternative to\nsynchronous online learning, particularly for its ability to simulate in-person\npresentations. They believe VR's immersive capabilities could enhance both\nteacher preparation and learner engagement in ways that traditional teaching\noften does not, especially noting that VR has the potential of addressing\nexpensive educational field trips. Despite these promising perceptions,\nparticipants identified key challenges, including limited infrastructure,\nunreliable internet connectivity, and insufficient access to VR equipment as\nperceived challenges that might hinder the integration of VR in a\nresource-limited region like Ghana. These findings offer significant\nimplications for educational policymakers and institutions aiming to leverage\nVR to enhance teacher training and professional development in similar contexts\nto consider addressing the perceived challenges for successful VR integration\nin education. We recommend further empirical research be conducted involving\npre-service teachers use of VR in their classrooms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.13808,review,post_llm,2024,11,"{'ai_likelihood': 3.642506069607205e-07, 'text': ""GPAI Evaluations Standards Taskforce: Towards Effective AI Governance\n\n  General-purpose AI evaluations have been proposed as a promising way of\nidentifying and mitigating systemic risks posed by AI development and\ndeployment. While GPAI evaluations play an increasingly central role in\ninstitutional decision- and policy-making -- including by way of the European\nUnion AI Act's mandate to conduct evaluations on GPAI models presenting\nsystemic risk -- no standards exist to date to promote their quality or\nlegitimacy. To strengthen GPAI evaluations in the EU, which currently\nconstitutes the first and only jurisdiction that mandates GPAI evaluations, we\noutline four desiderata for GPAI evaluations: internal validity, external\nvalidity, reproducibility, and portability. To uphold these desiderata in a\ndynamic environment of continuously evolving risks, we propose a dedicated EU\nGPAI Evaluation Standards Taskforce, to be housed within the bodies established\nby the EU AI Act. We outline the responsibilities of the Taskforce, specify the\nGPAI provider commitments that would facilitate Taskforce success, discuss the\npotential impact of the Taskforce on global AI governance, and address\npotential sources of failure that policymakers should heed.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.14301,review,post_llm,2024,11,"{'ai_likelihood': 1.0298358069525824e-05, 'text': ""Sustainability concepts for digital research infrastructures developed\n  through ground-level stakeholder empowerment\n\n  The UK Research and Innovation Digital Research Infrastructure (DRI) needs to\noperate sustainably in the future, encompassing its use of energy and\nresources, and embedded computer hardware carbon emissions. Transition concepts\ntowards less unsustainable operations will inform the future design and\noperations of DRI. A problem remains that, while the skills and knowledge for\nsolving net zero challenges already exist within the UK's DRI community, the\nmechanisms for sharing them and enabling behavior change are missing. Without\nadopting community-driven approaches, individual stakeholders may feel isolated\nand uncertain about how to play their role in the transition. A research\nprogramme was funded to give voice to the ground-level stakeholders of the DRI\necosystem for the co-creation of carbon downshift concepts. This article\npresents the results of the programme, with the goal to inform a fair and just\ntransition from the ground-level, complementing the top-down interventions of\nenergy efficiency policies and renewable energies integration. A workshop-based\ninnovation method was developed for researching stakeholder recommendations and\nperspectives on the sustainable transition of the UK's DRI. We find that giving\na purposeful voice to the stakeholders for shaping their own future sustainable\nDRI environment can be achieved by a guided, expert-integrated, interactive and\nproblem-focused workshop series. The chosen workshop design is impactful on\ncreating bottom-up agency for climate action by first defining the high-level\nproblems of unsustainability in energy and fossil-fuel consumption, and then\nconnecting them to the ground-level circumstances of DRI stakeholders. This\napproach to stakeholder management should initiate a sustainable transition\nthat promises to kick-start impactful changes from within communities, adding\nto high-level efforts from economics, policy, and governance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.16691,review,post_llm,2024,11,"{'ai_likelihood': 0.9609375, 'text': 'The Dual Impact of Artificial Intelligence in Healthcare: Balancing\n  Advancements with Ethical and Operational Challenges\n\n  The synchronic and diachronic study of the evolution of Artificial\nIntelligence (AI) unveils one prominent fact that its effect can be traced in\nalmost all fields such as healthcare industry. The growth is perceived\nholistically in software, hardware implementation, or application in these\nvarious fields. As the title suggests, the review will highlight the impact of\nAI on healthcare possibly in all dimensions including precision medicine,\ndiagnostics, drug development, automation of the process, etc., explicating\nwhether AI is a blessing or a curse or both. With the availability of enough\ndata and analysis to examine the topic at hand, however, its application is\nstill functioning in quite early stages in many fields, the present work will\nendeavour to provide an answer to the question. This paper takes a close look\nat how AI is transforming areas such as diagnostics, precision medicine, and\ndrug discovery, while also addressing some of the key ethical challenges it\nbrings. Issues like patient privacy, safety, and the fairness of AI decisions\nare explored to understand whether AI in healthcare is a positive force, a\npotential risk, or perhaps both.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0916748046875, 'GPT4': 0.493408203125, 'CLAUDE': 0.04754638671875, 'GOOGLE': 0.1727294921875, 'OPENAI_O_SERIES': 0.003879547119140625, 'DEEPSEEK': 0.01105499267578125, 'GROK': 0.001735687255859375, 'NOVA': 0.0012159347534179688, 'OTHER': 0.01012420654296875, 'HUMAN': 0.1666259765625}}"
2411.18393,review,post_llm,2024,11,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""Exploring the Impact of Rewards on Developers' Proactive AI\n  Accountability Behavior\n\n  The rapid integration of Artificial Intelligence (AI)-based systems offers\nbenefits for various domains of the economy and society but simultaneously\nraises concerns due to emerging scandals. These scandals have led to the\nincreasing importance of AI accountability to ensure that actors provide\njustification and victims receive compensation. However, AI accountability has\na negative connotation due to its emphasis on penalizing sanctions, resulting\nin reactive approaches to emerging concerns. To counteract the prevalent\nnegative view and offer a proactive approach to facilitate the AI\naccountability behavior of developers, we explore rewards as an alternative\nmechanism to sanctions. We develop a theoretical model grounded in\nSelf-Determination Theory to uncover the potential impact of rewards and\nsanctions on AI developers. We further identify typical sanctions and bug\nbounties as potential reward mechanisms by surveying related research from\nvarious domains, including cybersecurity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.10457,regular,post_llm,2024,11,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Predicting the winner of the US 2024 elections using trust analytics\n\nA number of models and techniques has been proposed for predicting the outcomes of presidential elections. Some of them use information on the socio-economical status of a country, others focus on candidates' popularity measures in news media. We employ a computational social science approach, utilising public reactions in social media to real-life events that involve presidential candidates. Contrary to the popular approach, we do not analyse public emotions but ethotic references to the character of politicians which allows us to analyse how much they are (dis-)trusted by the general public, hence the name of the tool we developed: Trust Analytics (TrustAn). Similarly to major news media's polls, we observe a tight race between Harris and Trump with week to week changes in the level of trust and distrust towards the two candidates. Using the ratio between the level of trust and distrust towards them and changes of this metric in time, we predict Donald Trump as the winner of the US 2024 elections."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.09222,regular,post_llm,2024,11,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Democratic AI is Possible. The Democracy Levels Framework Shows How It Might Work\n\nThis position paper argues that effectively ""democratizing AI"" requires democratic governance and alignment of AI, and that this is particularly valuable for decisions with systemic societal impacts. Initial steps -- such as Meta\'s Community Forums and Anthropic\'s Collective Constitutional AI -- have illustrated a promising direction, where democratic processes could be used to meaningfully improve public involvement and trust in critical decisions. To more concretely explore what increasingly democratic AI might look like, we provide a ""Democracy Levels"" framework and associated tools that: (i) define milestones toward meaningfully democratic AI, which is also crucial for substantively pluralistic, human-centered, participatory, and public-interest AI, (ii) can help guide organizations seeking to increase the legitimacy of their decisions on difficult AI governance and alignment questions, and (iii) support the evaluation of such efforts.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2411.15348,regular,post_llm,2024,11,"{'ai_likelihood': 0.00045564439561631944, 'text': 'Trading off performance and human oversight in algorithmic policy:\n  evidence from Danish college admissions\n\n  Student dropout is a significant concern for educational institutions due to\nits social and economic impact, driving the need for risk prediction systems to\nidentify at-risk students before enrollment. We explore the accuracy of such\nsystems in the context of higher education by predicting degree completion\nbefore admission, with potential applications for prioritizing admissions\ndecisions. Using a large-scale dataset from Danish higher education admissions,\nwe demonstrate that advanced sequential AI models offer more precise and fair\npredictions compared to current practices that rely on either high school grade\npoint averages or human judgment. These models not only improve accuracy but\nalso outperform simpler models, even when the simpler models use protected\nsociodemographic attributes. Importantly, our predictions reveal how certain\nstudent profiles are better matched with specific programs and fields,\nsuggesting potential efficiency and welfare gains in public policy. We estimate\nthat even the use of simple AI models to guide admissions decisions,\nparticularly in response to a newly implemented nationwide policy reducing\nadmissions by 10 percent, could yield significant economic benefits. However,\nthis improvement would come at the cost of reduced human oversight and lower\ntransparency. Our findings underscore both the potential and challenges of\nincorporating advanced AI into educational policymaking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.10283,review,post_llm,2024,12,"{'ai_likelihood': 1.0530153910319012e-05, 'text': 'Shaping the Future of Social Media with Middleware\n\n  Middleware, third-party software intermediaries between users and platforms,\nhas been broached as a means to decentralize the power of social media\nplatforms and enhance user agency. Middleware may enable a more user-centric\nand democratic approach to shaping digital experiences, offering a flexible\narchitecture as an alternative to both centrally controlled, opaque platforms\nand an unmoderated, uncurated internet. The widespread adoption of open\nmiddleware has long hinged on the cooperation of established major platforms;\nhowever, the recent growth of federated platforms, such as Mastodon and\nBluesky, has led to increased offerings and user awareness. In this report we\nconsider the potential of middleware as a means of enabling greater user\ncontrol over curation and moderation - two aspects of the social media\nexperience that are often mired in controversy. We evaluate the trade-offs and\nnegative externalities it might create, and discuss the technological,\nregulatory, and market dynamics that could either support or hinder its\nimplementation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14748,regular,post_llm,2024,12,"{'ai_likelihood': 1.6225708855523005e-06, 'text': 'Exploitation All the Way Down: Calling out the Root Cause of Bad Online\n  Experiences for Users of the ""Majority World""\n\n  Global Majority users are exposed to multitudes of harm when interacting with\nonline platforms. This essay illuminates how exploitation in the advances of\nArtificial Intelligence is tied to historical exploitation and how the use of\nblanket terminology overshadows the layers of exploitation and harm ``Global\nMajority\'\' populations face. It first discusses the multitude of harm content\nmoderators from the Global Majority face, arguing against the current trend of\nprotection through exploitation, then it illustrates the nuances and\ndifferences within the Global Majority, and finally, it outlines actionable\nitems to move away from such harm.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.09029,review,post_llm,2024,12,"{'ai_likelihood': 0.99755859375, 'text': 'The AI Assessment Scale Revisited: A Framework for Educational Assessment\n\nRecent developments in Generative Artificial Intelligence (GenAI) have created significant uncertainty in education, particularly in terms of assessment practices. Against this backdrop, we present an updated version of the AI Assessment Scale (AIAS), a framework with two fundamental purposes: to facilitate open dialogue between educators and students about appropriate GenAI use and to support educators in redesigning assessments in an era of expanding AI capabilities.\n  Grounded in social constructivist principles and designed with assessment validity in mind, the AIAS provides a structured yet flexible approach that can be adapted across different educational contexts. Building on implementation feedback from global adoption across both the K-12 and higher education contexts, this revision represents a significant change from the original AIAS. Among these changes is a new visual guide that moves beyond the original traffic light system and utilises a neutral colour palette that avoids implied hierarchies between the levels. The scale maintains five distinct levels of GenAI integration in assessment, from ""No AI"" to ""AI Exploration"", but has been refined to better reflect rapidly advancing technological capabilities and emerging pedagogical needs.\n  This paper presents the theoretical foundations of the revised framework, provides detailed implementation guidance through practical vignettes, and discusses its limitations and future directions. As GenAI capabilities continue to expand, particularly in multimodal content generation, the AIAS offers a starting point for reimagining assessment design in an era of disruptive technologies.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.4020671844482422e-05, 'GPT4': 0.08251953125, 'CLAUDE': 0.91162109375, 'GOOGLE': 0.001598358154296875, 'OPENAI_O_SERIES': 0.00020694732666015625, 'DEEPSEEK': 0.0017652511596679688, 'GROK': 5.960464477539063e-08, 'NOVA': 1.7881393432617188e-07, 'OTHER': 4.5299530029296875e-06, 'HUMAN': 0.00238800048828125}}"
2501.1476,regular,post_llm,2024,12,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'AI Meets Natural Hazard Risk: A Nationwide Vulnerability Assessment of Data Centers to Natural Hazards and Power Outages\n\nOur society is on the verge of a revolution powered by Artificial Intelligence (AI) technologies. With increasing advancements in AI, there is a growing expansion in data centers (DCs) serving as critical infrastructure for this new wave of technologies. This technological wave is also on a collision course with exacerbating climate hazards which raises the need for evaluating the vulnerability of DCs to various hazards. Hence, the objective of this research is to conduct a nationwide vulnerability assessment of (DCs) in the United States of America (USA). DCs provide such support; however, if an unplanned disruption (like a natural hazard or power outage) occurs, the functionality of DCs are in jeopardy. Unplanned downtime in DCs cause severe economic and social repercussions. With the Local Indicator of Spatial Association (LISA) test, the research found that there are a large percentage of DCs that are in non-vulnerable areas of disruption; however, there is still a notable percentage in disruption prone areas. For example, earthquakes, hurricanes, and tornadoes have the most DCs in vulnerable areas. After identifying these vulnerabilities, the research identified areas within the USA that have minimal vulnerabilities to both the aforementioned natural hazards and power outages with the BI-LISA test. After doing a composite vulnerability score on the Cold-Spots from the BILISA analysis, the research found three counties with the low vulnerability scores. These are Koochiching, Minnesota (0.091), Schoolcraft, Michigan (0.095), and Houghton, Michigan (0.096).', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.11335,review,post_llm,2024,12,"{'ai_likelihood': 4.635916815863716e-07, 'text': 'Generative AI regulation can learn from social media regulation\n\n  There is strong agreement that generative AI should be regulated, but strong\ndisagreement on how to approach regulation. While some argue that AI regulation\nshould mostly rely on extensions of existing laws, others argue that entirely\nnew laws and regulations are needed to ensure that generative AI benefits\nsociety. In this paper, I argue that the debates on generative AI regulation\ncan be informed by the debates and evidence on social media regulation. For\nexample, AI companies have faced allegations of political bias regarding the\nimages and text their models produce, similar to the allegations social media\ncompanies have faced regarding content ranking on their platforms. First, I\ncompare and contrast the affordances of generative AI and social media to\nhighlight their similarities and differences. Then, I discuss specific policy\nrecommendations based on the evolution of social media and their regulation.\nThese recommendations include investments in: efforts to counter bias and\nperceptions thereof (e.g., via transparency, researcher access, oversight\nboards, democratic input, research studies), specific areas of regulatory\nconcern (e.g., youth wellbeing, election integrity) and trust and safety,\ncomputational social science research, and a more global perspective. Applying\nlessons learnt from social media regulation to generative AI regulation can\nsave effort and time, and prevent avoidable mistakes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.02834,review,post_llm,2024,12,"{'ai_likelihood': 0.9951171875, 'text': 'Artificial Intelligence Policy Framework for Institutions\n\n  Artificial intelligence (AI) has transformed various sectors and\ninstitutions, including education and healthcare. Although AI offers immense\npotential for innovation and problem solving, its integration also raises\nsignificant ethical concerns, such as privacy and bias. This paper delves into\nkey considerations for developing AI policies within institutions. We explore\nthe importance of interpretability and explainability in AI elements, as well\nas the need to mitigate biases and ensure privacy. Additionally, we discuss the\nenvironmental impact of AI and the importance of energy-efficient practices.\nThe culmination of these important components is centralized in a generalized\nframework to be utilized for institutions developing their AI policy. By\naddressing these critical factors, institutions can harness the power of AI\nwhile safeguarding ethical principles.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.263427734375, 'GPT4': 0.06414794921875, 'CLAUDE': 0.01139068603515625, 'GOOGLE': 0.5791015625, 'OPENAI_O_SERIES': 0.0029468536376953125, 'DEEPSEEK': 0.0004189014434814453, 'GROK': 2.4020671844482422e-05, 'NOVA': 0.002178192138671875, 'OTHER': 0.075927734375, 'HUMAN': 0.0006361007690429688}}"
2412.17969,regular,post_llm,2024,12,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Collective sleep and activity patterns of college students from wearable\n  devices\n\n  To optimize interventions for improving wellness, it is essential to\nunderstand habits, which wearable devices can measure with greater precision.\nUsing high temporal resolution biometric data taken from the Oura Gen3 ring, we\nexamine daily and weekly sleep and activity patterns of a cohort of young\nadults (N=582) in their first semester of college. A high compliance rate is\nobserved for both daily and nightly wear, with slight dips in wear compliance\nobserved shortly after waking up and also in the evening. Most students have a\nlate-night chronotype with a median midpoint of sleep at 5AM, with males and\nthose with mental health impairment having more delayed sleep periods. Social\njetlag, or the difference in sleep times between free days and school days, is\nprevalent in our sample. While sleep periods generally shift earlier on\nweekdays and later on weekends, sleep duration on both weekdays and weekends is\nshorter than during prolonged school breaks, suggesting chronic sleep debt when\nschool is in session. Synchronized spikes in activity consistent with class\nschedules are also observed, suggesting that walking in between classes is a\nwidespread behavior in our sample that substantially contributes to physical\nactivity. Lower active calorie expenditure is associated with weekends and a\ndelayed but longer sleep period the night before, suggesting that for our\ncohort, active calorie expenditure is affected less by deviations from natural\ncircadian rhythms and more by the timing associated with activities. Our study\nshows that regular sleep and activity routines may be inferred from consumer\nwearable devices if high temporal resolution and long data collection periods\nare available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14773,review,post_llm,2024,12,"{'ai_likelihood': 0.00013748804728190106, 'text': 'Good Practices for Institutional Organization of Research Institutes:\n  Excellence in Research and Positive Impact on Society\n\n  In this paper, we analyze examples of research institutes that stand out in\nscientific excellence and social impact. We define key practices for evaluating\nresearch results, economic conditions, and the selection of specific research\ntopics. Special focus is placed on small countries and the field of artificial\nintelligence. The aim is to identify components that enable institutes to\nachieve a high level of innovation, self-sustainability, and social benefits.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14764,regular,post_llm,2024,12,"{'ai_likelihood': 2.149078581068251e-05, 'text': 'Battery-free, stretchable, and autonomous smart packaging\n\n  In the food industry, innovative packaging solutions are increasingly\nimportant for reducing food waste and for contributing to global sustainability\nefforts. However, current food packaging is generally passive and unable to\nadapt to changes in the food environment in real-time. To address this, we have\ndeveloped a battery-less and autonomous smart packaging system that wirelessly\npowers closed-loop sensing and release of active compounds. This system\nintegrates a gas sensor for real-time food monitoring, a Near-Field\nCommunication (NFC) antenna, and a controlled release of active compounds to\nprevent quality deterioration in the complex food environment. We have\ndemonstrated the ability of the developed smart packaging system, to\ncontinuously monitor the freshness of fish products and to trigger the release\nof active compounds when the food starts to spoil. The system was able to\nextend the shelf-life of the food product up to 14 days, due to the controlled\nrelease of antioxidant and antibacterial compounds. Our system could pave the\nway towards an Internet of Things solution that addresses protection, active\nprevention of food spoilage and sustainability, facing all the current\nchallenges of the food packaging industry.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.13348,review,post_llm,2024,12,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""Improving essay peer grading accuracy in MOOCs using personalized\n  weights from student's engagement and performance\n\n  Most MOOC platforms either use simple schemes for aggregating peer grades,\ne.g., taking the mean or the median, or apply methodologies that increase\nstudents' workload considerably, such as calibrated peer review. To reduce the\nerror between the instructor and students' aggregated scores in the simple\nschemes, without requiring demanding grading calibration phases, some proposals\ncompute specific weights to compute a weighted aggregation of the peer grades.\nIn this work, and in contrast to most previous studies, we analyse the use of\nstudents' engagement and performance measures to compute personalized weights\nand study the validity of the aggregated scores produced by these common\nfunctions, mean and median, together with two other from the information\nretrieval field, namely the geometric and harmonic means. To test this\nprocedure we have analysed data from a MOOC about Philosophy. The course had\n1059 students registered, and 91 participated in a peer review process that\nconsisted in writing an essay and rating three of their peers using a rubric.\nWe calculated and compared the aggregation scores obtained using weighted and\nnon-weighted versions. Our results show that the validity of the aggregated\nscores and their correlation with the instructors grades can be improved in\nrelation to peer grading, when using the median and weights are computed\naccording to students' performance in chapter tests.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.08666,review,post_llm,2024,12,"{'ai_likelihood': 4.4273005591498484e-05, 'text': 'Generative AI in Modern Education Society\n\n  Transitioning from Education 1.0 to Education 5.0, the integration of\ngenerative artificial intelligence (GenAI) revolutionizes the learning\nenvironment by fostering enhanced human-machine collaboration, enabling\npersonalized, adaptive and experiential learning, and preparing students with\nthe skills and adaptability needed for the future workforce. Our understanding\nof academic integrity and the scholarship of teaching, learning, and research\nhas been revolutionised by GenAI. Schools and universities around the world are\nexperimenting and exploring the integration of GenAI in their education systems\n(like, curriculum design, teaching process and assessments, administrative\ntasks, results generation and so on). The findings of the literature study\ndemonstrate how well GenAI has been incorporated into the global educational\nsystem. This study explains the roles of GenAI in the schooling and university\neducation systems with respect to the different stakeholders (students,\nteachers, researchers etc,). It highlights the current challenges of\nintegrating Generative AI into the education system and outlines future\ndirections for leveraging GenAI to enhance educational practices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.20923,regular,post_llm,2024,12,"{'ai_likelihood': 1.5132957034640844e-05, 'text': 'Does the Doer Effect Exist Beyond WEIRD Populations? Toward Analytics in\n  Radio and Phone-Based Learning\n\n  The Doer Effect states that completing more active learning activities, like\npractice questions, is more strongly related to positive learning outcomes than\npassive learning activities, like reading, watching, or listening to course\nmaterials. Although broad, most evidence has emerged from practice with\ntutoring systems in Western, Industrialized, Rich, Educated, and Democratic\n(WEIRD) populations in North America and Europe. Does the Doer Effect\ngeneralize beyond WEIRD populations, where learners may practice in remote\nlocales through different technologies? Through learning analytics, we provide\nevidence from N = 234 Ugandan students answering multiple-choice questions via\nphones and listening to lectures via community radio. Our findings support the\nhypothesis that active learning is more associated with learning outcomes than\npassive learning. We find this relationship is weaker for learners with higher\nprior educational attainment. Our findings motivate further study of the Doer\nEffect in diverse populations. We offer considerations for future research in\ndesigning and evaluating contextually relevant active and passive learning\nopportunities including leveraging familiar technology, increasing the number\nof practice opportunities, and aligning multiple data sources.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10386,review,post_llm,2024,12,"{'ai_likelihood': 0.0027592976888020835, 'text': ""Complex Dynamic Systems in Education: Beyond the Static, the Linear and\n  the Causal Reductionism\n\n  Traditional methods in educational research often fail to capture the complex\nand evolving nature of learning processes. This chapter examines the use of\ncomplex systems theory in education to address these limitations. The chapter\ncovers the main characteristics of complex systems such as non-linear\nrelationships, emergent properties, and feedback mechanisms to explain how\neducational phenomena unfold. Some of the main methodological approaches are\npresented, such as network analysis and recurrence quantification analysis to\nstudy relationships and patterns in learning. These have been operationalized\nby existing education research to study self-regulation, engagement, and\nacademic emotions, among other learning-related constructs. Lastly, the chapter\ndescribes data collection methods that are suitable for studying learning\nprocesses from a complex systems' lens.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14749,review,post_llm,2024,12,"{'ai_likelihood': 0.0001396073235405816, 'text': 'The Manhattan Trap: Why a Race to Artificial Superintelligence is\n  Self-Defeating\n\n  This paper examines the strategic dynamics of international competition to\ndevelop Artificial Superintelligence (ASI). We argue that the same assumptions\nthat might motivate the US to race to develop ASI also imply that such a race\nis extremely dangerous. These assumptions--that ASI would provide a decisive\nmilitary advantage and that states are rational actors prioritizing\nsurvival--imply that a race would heighten three critical risks: great power\nconflict, loss of control of ASI systems, and the undermining of liberal\ndemocracy. Our analysis shows that ASI presents a trust dilemma rather than a\nprisoners dilemma, suggesting that international cooperation to control ASI\ndevelopment is both preferable and strategically sound. We conclude that\ncooperation is achievable.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.02973,review,post_llm,2024,12,"{'ai_likelihood': 8.708900875515408e-06, 'text': ""Supporting Gig Worker Needs and Advancing Policy Through Worker-Centered\n  Data-Sharing\n\n  The proliferating adoption of platform-based gig work increasingly raises\nconcerns for worker conditions. Past studies documented how platforms leveraged\ndesign to exploit labor, withheld information to generate power asymmetries,\nand left workers alone to manage logistical overheads as well as social\nisolation. However, researchers also called attention to the potential of\nhelping workers overcome such costs via worker-led datasharing, which can\nenable collective actions and mutual aid among workers, while offering\nadvocates, lawmakers and regulatory bodies insights for improving work\nconditions. To understand stakeholders' desiderata for a data-sharing system\n(i.e. functionality and policy initiatives that it can serve), we interviewed\n11 policy domain experts in the U.S. and conducted co-design workshops with 14\nactive gig workers across four domains. Our results outline policymakers'\nprioritized initiatives, information needs, and (mis)alignments with workers'\nconcerns and desires around data collectives. We offer design recommendations\nfor data-sharing systems that support worker needs while bringing us closer to\nlegislation that promote more thriving and equitable gig work futures.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.06356,review,post_llm,2024,12,"{'ai_likelihood': 3.907415601942274e-06, 'text': 'Reputation Management in the ChatGPT Era\n\n  Generative AI systems often generate outputs about real people, even when not\nexplicitly prompted to do so. This can lead to significant reputational and\nprivacy harms, especially when sensitive, misleading, and outright false. This\npaper considers what legal tools currently exist to protect such individuals,\nwith a particular focus on defamation and data protection law. We explore the\npotential of libel law, arguing that it is a potential but not an ideal remedy,\ndue to lack of harmonization, and the focus on damages rather than systematic\nprevention of future libel. We then turn to data protection law, arguing that\nthe data subject rights to erasure and rectification may offer some more\nmeaningful protection, although the technical feasibility of compliance is a\nmatter of ongoing research. We conclude by noting the limitations of these\nindividualistic remedies and hint at the need for a more systemic,\nenvironmental approach to protecting the infosphere against generative AI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.11419,review,post_llm,2024,12,"{'ai_likelihood': 1.3642840915256076e-05, 'text': 'From Automation to Cognition: Redefining the Roles of Educators and\n  Generative AI in Computing Education\n\n  Generative Artificial Intelligence (GenAI) offers numerous opportunities to\nrevolutionise teaching and learning in Computing Education (CE). However,\neducators have expressed concerns that students may over-rely on GenAI and use\nthese tools to generate solutions without engaging in the learning process.\nWhile substantial research has explored GenAI use in CE, and many Computer\nScience (CS) educators have expressed their opinions and suggestions on the\nsubject, there remains little consensus on implementing curricula and\nassessment changes. In this paper, we describe our experiences with using GenAI\nin CS-focused educational settings and the changes we have implemented\naccordingly in our teaching in recent years since the popularisation of GenAI.\nFrom our experiences, we propose two primary actions for the CE community: 1)\nredesign take-home assignments to incorporate GenAI use and assess students on\ntheir process of using GenAI to solve a task rather than simply on the final\nproduct; 2) redefine the role of educators to emphasise metacognitive aspects\nof learning, such as critical thinking and self-evaluation. This paper presents\nand discusses these stances and outlines several practical methods to implement\nthese strategies in CS classrooms. Then, we advocate for more research\naddressing the concrete impacts of GenAI on CE, especially those evaluating the\nvalidity and effectiveness of new teaching practices.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.02911,regular,post_llm,2024,12,"{'ai_likelihood': 1.3245476616753472e-06, 'text': 'Measuring and Forecasting Conversation Incivility: the Role of\n  Antisocial and Prosocial Behaviors\n\n  This paper focuses on the task of measuring and forecasting incivility in\nconversations following replies to hate speech. Identifying replies that steer\nconversations away from hatred and elicit civil follow-up conversations sheds\nlight into effective strategies to engage with hate speech and proactively\navoid further escalation. We propose new metrics that take into account various\ndimensions of antisocial and prosocial behaviors to measure the conversation\nincivility following replies to hate speech. Our best metric aligns with human\nperceptions better than prior work. Additionally, we present analyses on a) the\nlanguage of antisocial and prosocial posts, b) the relationship between\nantisocial or prosocial posts and user interactions, and c) the language of\nreplies to hate speech that elicit follow-up conversations with different\nincivility levels. We show that forecasting the incivility level of\nconversations following a reply to hate speech is a challenging task. We also\npresent qualitative analyses to identify the most common errors made by our\nbest model.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14758,regular,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': 'Technological Progress and Obsolescence: Analyzing the Environmental\n  Economic Impacts of MacBook Pro I/O Devices\n\n  This study investigates how the new release of MacBook Pro I/O devices\naffects the obsolescence of related accessories. We also explore how these\naccessories will impact the environment and the economic consequences. As\ntechnology progresses, each new MacBook Pro releases outdated prior\naccessories, making more electronic waste. This phenomenon makes modern people\nneed to change their traditional consumption patterns. We analyze changes in\nI/O ports and compatibility between MacBook Pro versions to determine which\naccessories are obsolete and estimate their environmental impact. Our research\nfocuses on the sustainability of current accessories. We explore alternate\nmethods of reusing, recycling, and disposing of these accessories in order to\nreduce waste and environmental impact. In addition, we will explore the\neconomic consequences of rapid technological advances that make accessories\nobsolete too quickly. Thereby assessing the impact of such changes on\nconsumers, manufacturers, and the technology industry. This study aims to\nrespond to the rapid advancement of technology while promoting more sustainable\napproaches to waste management and product design. As the MacBook Pro I/O unit\nevolves, certain accessories become obsolete with each subsequent version. The\npurpose of this study is to identify and quantify the environmental and\neconomic impacts of parts end-of-life. We can detect which accessories have\nbecome obsolete and assess the environmental impact by comparing I/O port\nchanges and compatibility across MacBook Pro generations. In response to these\nenvironmental images, methods are developed to reuse, recycle, and dispose of\nobsolete accessories to reduce waste and promote sustainable development.\nAdditionally, we evaluate the economic impact of obsolete equipment on\nconsumers and producers.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1505126953125, 'GPT4': 0.143310546875, 'CLAUDE': 6.92605972290039e-05, 'GOOGLE': 0.7041015625, 'OPENAI_O_SERIES': 0.0009636878967285156, 'DEEPSEEK': 7.152557373046875e-07, 'GROK': 1.7881393432617188e-07, 'NOVA': 2.1457672119140625e-06, 'OTHER': 0.0011014938354492188, 'HUMAN': 8.106231689453125e-06}}"
2412.04575,regular,post_llm,2024,12,"{'ai_likelihood': 1.519918441772461e-05, 'text': ""Precarity and Solidarity: Preliminary results on a study of queer and\n  disabled fiction writers' experiences with generative AI\n\n  We have undertaken a mixed-methods study of fiction writers' experiences and\nattitudes with generative AI, primarily focused on the experiences of queer and\ndisabled writers. We find that queer and disabled writers are markedly more\npessimistic than non-queer and non-disabled writers about the impact of AI on\ntheir industry, although pessimism is the majority attitude for both groups. We\nexplore ways that generative AI exacerbates existing sources of instability and\nprecarity in the publishing industry, reasons why writers are philosophically\nopposed to its use, and individual and collective strategies used by\nmarginalized fiction writers to safeguard their industry from harms associated\nwith generative AI.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.16744,regular,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': 'Business Analysis: User Attitude Evaluation and Prediction Based on\n  Hotel User Reviews and Text Mining\n\n  In the post-pandemic era, the hotel industry plays a crucial role in economic\nrecovery, with consumer sentiment increasingly influencing market trends. This\nstudy utilizes advanced natural language processing (NLP) and the BERT model to\nanalyze user reviews, extracting insights into customer satisfaction and\nguiding service improvements. By transforming reviews into feature vectors, the\nBERT model accurately classifies emotions, uncovering patterns of satisfaction\nand dissatisfaction. This approach provides valuable data for hotel management,\nhelping them refine service offerings and improve customer experiences. From a\nfinancial perspective, understanding sentiment is vital for predicting market\nperformance, as shifts in consumer sentiment often correlate with stock prices\nand overall industry performance. Additionally, the study addresses data\nimbalance in sentiment analysis, employing techniques like oversampling and\nundersampling to enhance model robustness. The results offer actionable\ninsights not only for the hotel industry but also for financial analysts,\naiding in market forecasts and investment decisions. This research highlights\nthe potential of sentiment analysis to drive business growth, improve financial\noutcomes, and enhance competitive advantage in the dynamic tourism and\nhospitality sectors, thereby contributing to the broader economic landscape.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00010722875595092773, 'GPT4': 0.82275390625, 'CLAUDE': 3.337860107421875e-05, 'GOOGLE': 0.0003838539123535156, 'OPENAI_O_SERIES': 0.1607666015625, 'DEEPSEEK': 0.01515960693359375, 'GROK': 0.00035858154296875, 'NOVA': 7.635354995727539e-05, 'OTHER': 0.0003085136413574219, 'HUMAN': 4.76837158203125e-07}}"
2412.13821,regular,post_llm,2024,12,"{'ai_likelihood': 0.00015748871697319878, 'text': ""Towards Responsible Governing AI Proliferation\n\n  This paper argues that existing governance mechanisms for mitigating risks\nfrom AI systems are based on the `Big Compute' paradigm -- a set of assumptions\nabout the relationship between AI capabilities and infrastructure -- that may\nnot hold in the future. To address this, the paper introduces the\n`Proliferation' paradigm, which anticipates the rise of smaller, decentralized,\nopen-sourced AI models which are easier to augment, and easier to train without\nbeing detected. It posits that these developments are both probable and likely\nto introduce both benefits and novel risks that are difficult to mitigate\nthrough existing governance mechanisms. The final section explores governance\nstrategies to address these risks, focusing on access governance, decentralized\ncompute oversight, and information security. Whilst these strategies offer\npotential solutions, the paper acknowledges their limitations and cautions\ndevelopers to weigh benefits against developments that could lead to a\n`vulnerable world'.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.18799,regular,post_llm,2024,12,"{'ai_likelihood': 1.8146302964952258e-05, 'text': 'Quantifying the Risk of Pastoral Conflict in 4 Central African Countries\n\n  Climate change is becoming a widely recognized risk factor of farmer-herder\nconflict in Africa. Using an 8 year dataset (Jan 2015 to Sep 2022) of detailed\nweather and terrain data across four African nations, we apply statistical and\nmachine learning methods to analyze pastoral conflict. We test hypotheses\nlinking these variables with pastoral conflict within each country using\ngeospatial and statistical analysis. Complementing this analysis are risk maps\nautomatically updated for decision-makers. Our models estimate which cells have\na high likelihood of experiencing pastoral conflict with high predictive\naccuracy and study the variation of this accuracy with the granularity of the\ncells.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.06288,regular,post_llm,2024,12,"{'ai_likelihood': 2.4172994825575088e-06, 'text': ""The Unpaid Toll: Quantifying and Addressing the Public Health Impact of Data Centers\n\nThe surging demand for AI has led to a rapid expansion of energy-intensive data centers, impacting the environment through escalating carbon emissions and water consumption. While significant attention has been paid to data centers' growing environmental footprint, the public health burden, a hidden toll of data centers, has been largely overlooked. Specifically, data centers' lifecycle, from chip manufacturing to operation, can significantly degrade air quality through emissions of criteria air pollutants such as fine particulate matter, substantially impacting public health. This paper introduces a principled methodology to model lifecycle pollutant emissions for data centers and computing tasks, quantifying the public health impacts. Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California. Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities. Finally, we propose a health-informed computing framework that explicitly incorporates public health risk as a key metric for scheduling data center workloads across space and time, which can effectively mitigate adverse health impacts while advancing environmental sustainability. More broadly, we also recommend adopting a standard reporting protocol for the public health impacts of data centers and paying attention to all impacted communities."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.12117,review,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': ""Harnessing AI in Secondary Education to Enhance Writing Competence\n\n  The emergence of free AI tools like ChatGPT holds significant implications\nfor developing writing skills in secondary education. This study examines AI's\nimpact on students' writing competence and personal voice, balancing\ntechnological benefits against risks of dependency and plagiarism. We review\nthe pros and cons of AI in the writing process, emphasizing process-based\nassessments, creativity-driven tasks, and AI as a supplement to teacher\nguidance. The discussion covers AI's role in the pre-writing, writing, and\nrevision stages, and highlights the need for innovative assignments and\ncritical thinking to maintain writing as a human, expressive activity. We\nadvocate for a balanced approach to AI in education, ensuring it supports\nrather than replaces teacher instruction.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0024280548095703125, 'GPT4': 0.027435302734375, 'CLAUDE': 0.051300048828125, 'GOOGLE': 0.7529296875, 'OPENAI_O_SERIES': 0.0031185150146484375, 'DEEPSEEK': 0.00449371337890625, 'GROK': 6.693601608276367e-05, 'NOVA': 0.0004050731658935547, 'OTHER': 0.1578369140625, 'HUMAN': 0.00010323524475097656}}"
2501.14761,regular,post_llm,2024,12,"{'ai_likelihood': 0.0008164511786566841, 'text': 'A multi-dimension and high-granularity equity measurement for\n  transportation services through accessibility and reliability\n\n  Transportation equity research has traditionally emphasized service\naccessibility and destination reachability while often overlooking the critical\naspects of service quality, such as infrequent schedules or overcrowded\nvehicles. This oversight can lead to a skewed understanding of equity, as high\naccessibility does not guarantee high-quality service. Addressing this gap, we\npropose a transportation equity index called the multi-dimensional,\nhigh-granularity (MDHG) index. Such an index considers service accessibility\nand quality alongside population demographics. This approach ensures that areas\nwith high accessibility but low service quality are recognized as inequitable.\nThe MDHG Index addresses service performance by incorporating performance data\nwith temporal variations based on actual trip data, thus offering a more\nnuanced view of transportation equity that reflects the real-world experiences\nof service users. Furthermore, to effectively identify and address the needs at\nthe user level, we need to use a highly granular population dataset. Due to the\nlow granularity of census and other open-source datasets, we opted to use a\nhighly granular synthetic dataset. To test out the MDHG Index, we coupled a\nhighly granular synthetic population dataset with data from NYC Citi Bike\nexpansion to use as a case study to assess changes in accessibility and service\nquality before and after the expansion. The MDHG approach effectively\nidentified areas that improved post-expansion and highlighted those requiring\nfurther enhancement, thus showing the effectiveness of the index in targeted\nimprovements for transportation equity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.07924,regular,post_llm,2024,12,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""A large language model-based approach to quantifying the effects of\n  social determinants in liver transplant decisions\n\n  Patient life circumstances, including social determinants of health (SDOH),\nshape both health outcomes and care access, contributing to persistent\ndisparities across gender, race, and socioeconomic status. Liver\ntransplantation exemplifies these challenges, requiring complex eligibility and\nallocation decisions where SDOH directly influence patient evaluation. We\ndeveloped an artificial intelligence (AI)-driven framework to analyze how\nbroadly defined SDOH -- encompassing both traditional social determinants and\ntransplantation-related psychosocial factors -- influence patient care\ntrajectories. Using large language models, we extracted 23 SDOH factors related\nto patient eligibility for liver transplantation from psychosocial evaluation\nnotes. These SDOH ``snapshots'' significantly improve prediction of patient\nprogression through transplantation evaluation stages and help explain liver\ntransplantation decisions including the recommendation based on psychosocial\nevaluation and the listing of a patient for a liver transplantation. Our\nanalysis helps identify patterns of SDOH prevalence across demographics that\nhelp explain racial disparities in liver transplantation decisions. We\nhighlight specific unmet patient needs, which, if addressed, could improve the\nequity and efficacy of transplant care. While developed for liver\ntransplantation, this systematic approach to analyzing previously unstructured\ninformation about patient circumstances and clinical decision-making could\ninform understanding of care decisions and disparities across various medical\ndomains.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14774,review,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': 'Achieving Carbon Neutrality for I/O Devices\n\n  Achieving carbon neutrality has become a critical goal in mitigating the\nenvironmental impacts of human activities, particularly in the face of global\nclimate challenges. Input/Output (I/O) devices, such as keyboards, mice,\ndisplays, and printers, contribute significantly to greenhouse gas emissions\nthrough their manufacturing, operation, and disposal processes. In this paper,\nwe explores sustainable strategies for achieving carbon neutrality in I/O\ndevices, emphasizing the importance of environmentally conscious design and\ndevelopment. Through a comprehensive review of existing literature and best\napproaches, we introduces a framework to outline approaches for reducing the\ncarbon footprint of I/O devices. The result underscore the necessity of\nintegrating sustainability into the lifecycle of I/O devices to support global\ncarbon neutrality goals and promote long-term environmental sustainability.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0694580078125, 'GPT4': 0.81494140625, 'CLAUDE': 0.004535675048828125, 'GOOGLE': 0.086669921875, 'OPENAI_O_SERIES': 0.00916290283203125, 'DEEPSEEK': 0.00017547607421875, 'GROK': 1.3113021850585938e-05, 'NOVA': 0.0019235610961914062, 'OTHER': 0.01308441162109375, 'HUMAN': 2.956390380859375e-05}}"
2412.16786,regular,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': ""TelegramScrap: A comprehensive tool for scraping Telegram data\n\n  [WhitePaper] The TelegramScrap tool provides a robust and versatile solution\nfor extracting and analyzing data from Telegram channels and groups, addressing\nthe increasing demand for efficient methods to study digital ecosystems. This\nwhite paper outlines the tool's development, capabilities, and applications in\nacademic and scientific research, including studies on disinformation,\npolitical communication, and thematic patterns in online communities. Built\nwith flexibility and user accessibility in mind, the tool allows researchers to\ncustomize scraping parameters, handle large datasets, and produce structured\noutputs in formats such as Excel and Parquet. Its modular architecture,\nreal-time progress tracking, and error-handling mechanisms ensure reliability\nand scalability for diverse research needs. Emphasizing ethical data\ncollection, the tool aligns with Telegram's terms of service and data privacy\nregulations, encouraging responsible use. Released under an open-source\nlicense, TelegramScrap invites the academic community to explore, adapt, and\nimprove the tool while providing appropriate credit. This paper demonstrates\nthe tool's impact through its application in multiple studies, showcasing its\npotential to advance computational social science and enhance understanding of\ndigital interactions and societal trends [ Code available on GitHub:\nhttps://github.com/ergoncugler/web-scraping-telegram ].\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0007791519165039062, 'GPT4': 0.875, 'CLAUDE': 0.007328033447265625, 'GOOGLE': 0.00653839111328125, 'OPENAI_O_SERIES': 0.01422882080078125, 'DEEPSEEK': 0.0931396484375, 'GROK': 2.473592758178711e-05, 'NOVA': 1.6927719116210938e-05, 'OTHER': 0.0029582977294921875, 'HUMAN': 6.729364395141602e-05}}"
2412.10134,review,post_llm,2024,12,"{'ai_likelihood': 0.00013828277587890625, 'text': ""Research Integrity and GenAI: A Systematic Analysis of Ethical\n  Challenges Across Research Phases\n\n  Background: The rapid development and use of generative AI (GenAI) tools in\nacademia presents complex and multifaceted ethical challenges for its users.\nEarlier research primarily focused on academic integrity concerns related to\nstudents' use of AI tools. However, limited information is available on the\nimpact of GenAI on academic research. This study aims to examine the ethical\nconcerns arising from the use of GenAI across different phases of research and\nexplores potential strategies to encourage its ethical use for research\npurposes.\n  Methods: We selected one or more GenAI platforms applicable to various\nresearch phases (e.g. developing research questions, conducting literature\nreviews, processing data, and academic writing) and analysed them to identify\npotential ethical concerns relevant for that stage.\n  Results: The analysis revealed several ethical concerns, including a lack of\ntransparency, bias, censorship, fabrication (e.g. hallucinations and false data\ngeneration), copyright violations, and privacy issues. These findings\nunderscore the need for cautious and mindful use of GenAI.\n  Conclusions: The advancement and use of GenAI are continuously evolving,\nnecessitating an ongoing in-depth evaluation. We propose a set of practical\nrecommendations to support researchers in effectively integrating these tools\nwhile adhering to the fundamental principles of ethical research practices.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.05052,review,post_llm,2024,12,"{'ai_likelihood': 0.4310438368055556, 'text': ""Trust and distrust in electoral technologies: what can we learn from the\n  failure of electronic voting in the Netherlands (2006/07)\n\n  This paper focuses on the complex dynamics of trust and distrust in digital\ngovernment technologies by approaching the cancellation of machine voting in\nthe Netherlands (2006-07). This case describes how a previously trusted system\ncan collapse, how paradoxical the relationship between trust and distrust is,\nand how it interacts with adopting and managing electoral technologies. The\nanalysis stresses how, although being a central component, technology's\ntrustworthiness dialogues with the socio-technical context in which it is\ninserted, for example, underscoring the relevance of public administration in\nsecuring technological environments. Beyond these insights, the research offers\nbroader reflections on trust and distrust in data-driven technologies,\nadvocating for differentiated strategies for building trust versus managing\ndistrust. Overall, this paper contributes to understanding trust dynamics in\ndigital government technologies, with implications for policymaking and\ntechnology adoption strategies.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.09914,regular,post_llm,2024,12,"{'ai_likelihood': 0.9755859375, 'text': 'Atomic Learning Objectives Labeling: A High-Resolution Approach for\n  Physics Education\n\n  This paper introduces a novel approach to create a high-resolution ""map"" for\nphysics learning: an ""atomic"" learning objectives (LOs) system designed to\ncapture detailed cognitive processes and concepts required for problem solving\nin a college-level introductory physics course. Our method leverages Large\nLanguage Models (LLMs) for automated labeling of physics questions and\nintroduces a comprehensive set of metrics to evaluate the quality of the\nlabeling outcomes. The atomic LO system, covering nine chapters of an\nintroductory physics course, uses a ""subject-verb-object\'\' structure to\nrepresent specific cognitive processes. We apply this system to 131 questions\nfrom expert-curated question banks and the OpenStax University Physics\ntextbook. Each question is labeled with 1-8 atomic LOs across three chapters.\nThrough extensive experiments using various prompting strategies and LLMs, we\ncompare automated LOs labeling results against human expert labeling. Our\nanalysis reveals both the strengths and limitations of LLMs, providing insight\ninto LLMs reasoning processes for labeling LOs and identifying areas for\nimprovement in LOs system design. Our work contributes to the field of learning\nanalytics by proposing a more granular approach to mapping learning objectives\nwith questions. Our findings have significant implications for the development\nof intelligent tutoring systems and personalized learning pathways in STEM\neducation, paving the way for more effective ""learning GPS\'\' systems.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00023651123046875, 'GPT4': 0.2071533203125, 'CLAUDE': 0.0965576171875, 'GOOGLE': 0.673828125, 'OPENAI_O_SERIES': 0.0010013580322265625, 'DEEPSEEK': 0.00043845176696777344, 'GROK': 1.3232231140136719e-05, 'NOVA': 4.708766937255859e-06, 'OTHER': 0.0009336471557617188, 'HUMAN': 0.0196990966796875}}"
2412.13429,review,post_llm,2024,12,"{'ai_likelihood': 7.351239522298178e-06, 'text': 'Digital twin in advanced training of engineering specialists\n\n  A review of scientific literature showed the relevance of the issue of\nassessing the training of an engineering specialist. Engineering includes a\nvariety of works that relate to production issues. To assess the training of an\nengineering specialist, the digital twin of the enterprise is used. The digital\ntwin of an enterprise includes all major pre-production, production and\nproduction support processes. A method for assessing the competencies received\nby an engineering specialist has been selected. The set of competencies is\nassociated with the ongoing processes of the digital twin of an industrial\nenterprise. The normal mode of execution of the enterprise production processes\nis measured. The mode of operation of the processes was measured taking into\naccount the new competencies of an engineering specialist. The study proposes a\nmethodology for assessing the competence of an engineering specialist. The\nassessment of the effectiveness of the training passed by him.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10387,review,post_llm,2024,12,"{'ai_likelihood': 0.00036266114976671006, 'text': 'Online Influence Campaigns: Strategies and Vulnerabilities\n\n  In order to combat the creation and spread of harmful content online, this\npaper defines and contextualizes the concept of inauthentic, societal-scale\nmanipulation by malicious actors. We review the literature on societally\nharmful content and how it proliferates to analyze the manipulation strategies\nused by such actors and the vulnerabilities they target. We also provide an\noverview of three case studies of extensive manipulation campaigns to emphasize\nthe severity of the problem. We then address the role that Artificial\nIntelligence plays in the development and dissemination of harmful content, and\nhow its evolution presents new threats to societal cohesion for countries\nacross the globe. Our survey aims to increase our understanding of not just\nparticular aspects of these threats, but also the strategies underlying their\ndeployment, so we can effectively prepare for the evolving cybersecurity\nlandscape.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.00836,review,post_llm,2024,12,"{'ai_likelihood': 7.483694288465712e-06, 'text': 'Position Paper: Model Access should be a Key Concern in AI Governance\n\n  The downstream use cases, benefits, and risks of AI systems depend\nsignificantly on the access afforded to the system, and to whom. However, the\ndownstream implications of different access styles are not well understood,\nmaking it difficult for decision-makers to govern model access responsibly.\nConsequently, we spotlight Model Access Governance, an emerging field focused\non helping organisations and governments make responsible, evidence-based\naccess decisions. We outline the motivation for developing this field by\nhighlighting the risks of misgoverning model access, the limitations of\nexisting research on the topic, and the opportunity for impact. We then make\nfour sets of recommendations, aimed at helping AI evaluation organisations,\nfrontier AI companies, governments and international bodies build consensus\naround empirically-driven access governance.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.05761,review,post_llm,2024,12,"{'ai_likelihood': 9.271833631727431e-07, 'text': 'Interactions Between Artificial Intelligence and Digital Public\n  Infrastructure: Concepts, Benefits, and Challenges\n\n  Artificial intelligence (AI) and digital public infrastructure (DPI) are two\ntechnological developments that have taken center stage in global policy\ndiscourse. Yet, to date, there has been relatively little discussion about how\nAI and DPI can mutually enhance the public value provided by each other.\nTherefore, in this paper, we describe both the opportunities and challenges\nunder which AI and DPI can interact for mutual benefit. First, we define both\nAI and DPI to provide clarity and help policymakers distinguish between these\ntwo technological developments. Second, we provide empirical evidence for how\nAI, a general-purpose technology, can integrate into many DPI systems, aiding\nDPI function in use cases like language localization via machine translation\n(MT), personalized service delivery via recommender systems, and more. Third,\nwe catalog how DPI can act as a foundation for creating more advanced AI\nsystems by improving both the quantity and quality of training data available.\nFourth, we discuss the challenges of integrating AI and DPI, including high\ninference costs for advanced AI models, interoperability challenges with legacy\nsoftware, concerns about induced bias in AI systems, and privacy challenges\nrelated to DPI. We conclude with key takeaways for how policymakers can work to\nenhance the positive interactions of AI and DPI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.20498,review,post_llm,2024,12,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Regulating radiology AI medical devices that evolve in their lifecycle\n\n  Over time, the distribution of medical image data drifts due to factors such\nas shifts in patient demographics, acquisition devices, and disease\nmanifestations. While human radiologists can adjust their expertise to\naccommodate such variations, deep learning models cannot. In fact, such models\nare highly susceptible to even slight variations in image characteristics.\nConsequently, manufacturers must conduct regular updates to ensure that they\nremain safe and effective. Performing such updates in the United States and\nEuropean Union required, until recently, obtaining re-approval. Given the time\nand financial burdens associated with these processes, updates were infrequent,\nand obsolete systems remained in operation for too long. During 2024, several\nregulatory developments promised to streamline the safe rollout of model\nupdates: The European Artificial Intelligence Act came into effect last August,\nand the Food and Drug Administration (FDA) issued final marketing submission\nrecommendations for a Predetermined Change Control Plan (PCCP) in December. We\nprovide an overview of these developments and outline the key building blocks\nnecessary for successfully deploying dynamic systems. At the heart of these\nregulations - and as prerequisites for manufacturers to conduct model updates\nwithout re-approval - are clear descriptions of data collection and re-training\nprocesses, coupled with robust real-world quality monitoring mechanisms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.07445,review,post_llm,2024,12,"{'ai_likelihood': 0.9951171875, 'text': ""Identity theft and societal acceptability of electronic identity in\n  Europe and in the United States\n\n  This paper addresses critical questions surrounding the security of\ngovernment-issued identity documents and their potential misuse, with an\nemphasis on understanding the perspectives of ordinary citizens across Europe\nand the United States of America. Drawing upon research on technology\nacceptance and diffusion, the research focuses on understanding the factors\nthat influence users' adoption of novel identity management solutions. Our\nmethodology includes a comprehensive, census-representative survey spanning\ncitizens from France, Germany, Italy, Spain, the United Kingdom, and the USA.\nThe paper's findings underscore a robust confidence in government-issued\nidentity documents, contrasted by a lower trust in private sector services,\nincluding social media platforms and email accounts. The adoption of artificial\nintelligence for identity verification remains contested, with a significant\npercentage of respondents undecided, indicating a need for explicit explanation\nand transparency about its implementation and related risks. Public sentiment\nleans towards acceptance of government data collection for identification\npurposes; however, the sharing of this data with private entities elicits more\napprehension.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.9073486328125e-05, 'GPT4': 0.96240234375, 'CLAUDE': 1.5020370483398438e-05, 'GOOGLE': 0.03717041015625, 'OPENAI_O_SERIES': 0.00021719932556152344, 'DEEPSEEK': 1.3172626495361328e-05, 'GROK': 1.0192394256591797e-05, 'NOVA': 1.3709068298339844e-06, 'OTHER': 1.7940998077392578e-05, 'HUMAN': 0.00024366378784179688}}"
2501.10379,review,post_llm,2024,12,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'What Information Should Be Shared with Whom ""Before and During\n  Training""?\n\n  In the Frontier AI Safety Commitments, sixteen companies committed to ""Assess\nthe risks posed by their frontier models or systems across the AI lifecycle,\nincluding [...] as appropriate, before and during training"" (I) and to ""Provide\npublic transparency on the implementation of the above (I-VI), except insofar\nas doing so would increase risk or divulge sensitive commercial information to\na degree disproportionate to the societal benefit. They should still share more\ndetailed information which cannot be shared publicly with trusted actors,\nincluding their respective home governments or appointed body, as appropriate""\n(VII). This short paper considers what information should be shared with whom\nbefore training begins. What information should be shared publicly and what\nonly with trusted actors such as home governments? Sharing such information\nbefore a frontier training run can build shared awareness and preparedness, can\nimprove risk assessment and management, and can contribute to greater\npredictability and accountability. Companies could share certain information\nbefore a training run including:\n  Expected dates of beginning and end of training;\n  Expected compute used (in FLOP);\n  Description of the pre-training dataset(s);\n  Expected capability level of the frontier model or system, including an\nassessment of potential risks and whether this capability will approach any\nrisk threshold;\n  How the company will monitor progress, capabilities and risks during\ntraining;\n  Location, ownership, primary energy source of the large-scale computing\ncluster(s);\n  Physical, personnel and cybersecurity steps taken; and\n  Which internal and external groups have been tasked to carry out evaluations\nand red-teaming and what level of resources, support and time they have\navailable to do so.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.05093,review,post_llm,2024,12,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'Sense and Sensitivity: Evaluating the simulation of social dynamics via\n  Large Language Models\n\n  Large language models have increasingly been proposed as a powerful\nreplacement for classical agent-based models (ABMs) to simulate social\ndynamics. By using LLMs as a proxy for human behavior, the hope of this new\napproach is to be able to simulate significantly more complex dynamics than\nwith classical ABMs and gain new insights in fields such as social science,\npolitical science, and economics. However, due to the black box nature of LLMs,\nit is unclear whether LLM agents actually execute the intended semantics that\nare encoded in their natural language instructions and, if the resulting\ndynamics of interactions are meaningful. To study this question, we propose a\nnew evaluation framework that grounds LLM simulations within the dynamics of\nestablished reference models of social science. By treating LLMs as a black-box\nfunction, we evaluate their input-output behavior relative to this reference\nmodel, which allows us to evaluate detailed aspects of their behavior. Our\nresults show that, while it is possible to engineer prompts that approximate\nthe intended dynamics, the quality of these simulations is highly sensitive to\nthe particular choice of prompts. Importantly, simulations are even sensitive\nto arbitrary variations such as minor wording changes and whitespace. This puts\ninto question the usefulness of current versions of LLMs for meaningful\nsimulations, as without a reference model, it is impossible to determine a\npriori what impact seemingly meaningless changes in prompt will have on the\nsimulation.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.07727,review,post_llm,2024,12,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Artificial Intelligence Tools Expand Scientists' Impact but Contract Science's Focus (Just accepted by Nature, to be online soon)\n\nDevelopment in Artificial Intelligence (AI) has accelerated scientific discovery. Alongside recent AI-oriented Nobel prizes, these trends establish the role of AI tools in science. This advancement raises questions about the potential influences of AI tools on scientists and science as a whole, and highlights a potential conflict between individual and collective benefits. To evaluate, we used a pretrained language model to identify AI-augmented research, with an F1-score of 0.875 in validation against expert-labeled data. Using a dataset of 41.3 million research papers across natural science and covering distinct eras of AI, here we show an accelerated adoption of AI tools among scientists and consistent professional advantages associated with AI usage, but a collective narrowing of scientific focus. Scientists who engage in AI-augmented research publish 3.02 times more papers, receive 4.84 times more citations, and become research project leaders 1.37 years earlier than those who do not. By contrast, AI adoption shrinks the collective volume of scientific topics studied by 4.63% and decreases scientist's engagement with one another by 22.00%. Thereby, AI adoption in science presents a seeming paradox -- an expansion of individual scientists' impact but a contraction in collective science's reach -- as AI-augmented work moves collectively toward areas richest in data. With reduced follow-on engagement, AI tools appear to automate established fields rather than explore new ones, highlighting a tension between personal advancement and collective scientific progress."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.03162,review,post_llm,2024,12,"{'ai_likelihood': 0.0004360410902235243, 'text': ""LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing\n\n  Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Mirror, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Mirror with actual individual survey responses,\nwe assess its effectiveness in replicating individual-level outcomes. Our\nfindings show that: (1) PLS-SEM analysis shows LLM-generated responses align\nwith human responses, (2) LLMs, when provided with respondent-specific\ninformation, are capable of reproducing individual human responses, and (3)\nLLM-Mirror responses closely follow human responses at the individual level.\nThese findings highlight the potential of LLMs as a complementary tool for\npre-testing surveys and optimizing research design.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.07712,regular,post_llm,2024,12,"{'ai_likelihood': 0.0001233816146850586, 'text': 'Access to care improves EHR reliability and clinical risk prediction\n  model performance\n\n  Disparities in access to healthcare have been well-documented in the United\nStates, but their effects on electronic health record (EHR) data reliability\nand resulting clinical models are poorly understood. Using an All of Us dataset\nof 134,513 participants, we investigate the effects of access to care on the\nmedical machine learning pipeline, including medical condition rates, data\nquality, outcome label accuracy, and prediction performance. Our findings\nreveal that patients with cost constrained or delayed care have worse EHR\nreliability as measured by patient self-reported conditions for 78% of examined\nmedical conditions. We demonstrate in a prediction task of Type II diabetes\nincidence that clinical risk predictive performance can be worse for patients\nwithout standard care, with balanced accuracy gaps of 3.6 and sensitivity gaps\nof 9.4 percentage points for those with cost-constrained or delayed care. We\nevaluate solutions to mitigate these disparities and find that including\npatient self-reported conditions improved performance for patients with lower\naccess to care, with 11.2 percentage points higher sensitivity, effectively\ndecreasing the performance gap between standard versus delayed or\ncost-constrained care. These findings provide the first large-scale evidence\nthat healthcare access systematically affects both data reliability and\nclinical prediction performance. By revealing how access barriers propagate\nthrough the medical machine learning pipeline, our work suggests that improving\nmodel equity requires addressing both data collection biases and algorithmic\nlimitations. More broadly, this analysis provides an empirical foundation for\ndeveloping clinical prediction systems that work effectively for all patients,\nregardless of their access to care.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.03882,regular,post_llm,2024,12,"{'ai_likelihood': 0.009121365017361112, 'text': ""A Multi-agent Simulation for the Mass School Shootings\n\n  The increasing frequency of mass school shootings in the United States has\nbeen raised as a critical concern. Active shooters kill innocent students and\neducators in schools. These tragic events highlight the urgent need for\neffective strategies to minimize casualties. This study aims to address the\nchallenge of simulating and assessing potential mitigation measures by\ndeveloping a multi-agent simulation model. Our model is designed to estimate\ncasualty rates and evacuation efficiency during active shooter scenarios within\nschool buildings. The simulation evaluates the impact of a gun detection system\non safety outcomes. By simulating school shooting incidents with and without\nthis system, we observe a significant improvement in evacuation rates, which\nincreased from 16.6% to 66.6%. Furthermore, the Gun Detection System reduced\nthe average casualty rate from 24.0% to 12.2% within a period of six minutes,\nbased on a simulated environment with 100 students. We conducted a total of 48\nsimulations across three different floor layouts, varying the number of\nstudents and time intervals to assess the system's adaptability. We anticipate\nthat the research will provide a starting point for demonstrating that a\ngunshot detection system can significantly improve both evacuation rates and\ncasualty reduction.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.06032,review,post_llm,2024,12,"{'ai_likelihood': 3.599458270602756e-05, 'text': 'Optimizing Location Allocation in Urban Management: A Brief Review\n\n  Regarding the concepts of urban management, digital transformation, and smart\ncities, various issues are presented. Currently, we like to attend to location\nallocation problems that can be a new part of digital transformation in urban\nmanagement (such as locating and placing facilities, locating and arranging\ncenters such as aid and rescue centers, or even postal hubs,\ntelecommunications, electronic equipment, and data centers, and routing in\ntransportation optimization). These issues, which are seemingly simple but in\npractice complex, are important in urban environments, and the issue of\naccurate location allocation based on existing criteria directly impacts cost\nmanagement, profit, efficiency, and citizen satisfaction. In recent years,\nresearchers have used or presented various models and methods for location\nallocation problems, some of which will be mentioned in this article. Given the\nnature of these problems, which are optimization problems, this article will\nalso examine existing research from an optimization perspective in summary.\nFinally, a brief conclusion will be made of the existing methods and their\nweaknesses, and suggestions will be made for continuing the path and improving\nscientific and practical research in this field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.11911,review,post_llm,2024,12,"{'ai_likelihood': 1.4536910586886936e-05, 'text': 'What Can Youth Learn About Artificial Intelligence and Machine Learning\n  in One Hour? Examining How Hour of Code Activities Address the Five Big Ideas\n  of AI\n\n  The prominence of artificial intelligence and machine learning in everyday\nlife has led to efforts to foster AI literacy for all K-12 students. In this\npaper, we review how Hour of Code activities engage with the five big ideas of\nAI, in particular with machine learning and societal impact. We found that a\nlarge majority of activities focus on perception and machine learning, with\nlittle attention paid to representation and other topics. A surprising finding\nwas the increased attention paid to critical aspects of computing. However, we\nalso observed a limited engagement with hands-on activities. In the discussion,\nwe address how future introductory activities could be designed to offer a\nbroader array of topics, including the development of tools to introduce\nnovices to artificial intelligence and machine learning and the design of more\nunplugged and collaborative activities.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.01433,review,post_llm,2024,12,"{'ai_likelihood': 1.069572236802843e-05, 'text': 'My Voice, Your Voice, Our Voice: Attitudes Towards Collective Governance\n  of a Choral AI Dataset\n\n  Data grows in value when joined and combined; likewise the power of voice\ngrows in ensemble. With 15 UK choirs, we explore opportunities for bottom-up\ndata governance of a jointly created Choral AI Dataset. Guided by a survey of\nchorister attitudes towards generative AI models trained using their data, we\nexplore opportunities to create empowering governance structures that go beyond\nopt in and opt out. We test the development of novel mechanisms such as a\nTrusted Data Intermediary (TDI) to enable governance of the dataset amongst the\nchoirs and AI developers. We hope our findings can contribute to growing\nefforts to advance collective data governance practices and shape a more\ncreative, empowering future for arts communities in the generative AI\necosystem.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.11826,regular,post_llm,2024,12,"{'ai_likelihood': 0.01452975802951389, 'text': ""Uncovering Student Engagement Patterns in Moodle with Interpretable\n  Machine Learning\n\n  Understanding and enhancing student engagement through digital platforms is\ncritical in higher education. This study introduces a methodology for\nquantifying engagement across an entire module using virtual learning\nenvironment (VLE) activity log data. Using study session frequency, immediacy,\nand diversity, we create a cumulative engagement metric and model it against\nweekly VLE interactions with resources to identify critical periods and\nresources predictive of student engagement.\n  In a case study of a computing module at University College London's\nDepartment of Statistical Science, we further examine how delivery methods\n(online, hybrid, in-person) impact student behaviour. Across nine regression\nmodels, we validate the consistency of the random forest model and highlight\nthe interpretive strengths of generalised additive models for analysing\nengagement patterns.\n  Results show weekly VLE clicks as reliable engagement predictors, with early\nweeks and the first assessment period being key. However, the impact of\ndelivery methods on engagement is inconclusive due to inconsistencies across\nmodels. These findings support early intervention strategies to assist students\nat risk of disengagement. This work contributes to learning analytics research\nby proposing a refined VLE-based engagement metric and advancing data-driven\nteaching strategies in higher education.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.12355,review,post_llm,2024,12,"{'ai_likelihood': 1.7219119601779514e-06, 'text': ""Integrating Energy-Efficient Computing Research to Accelerate Energy\n  Technology\n\n  NREL's computational sciences center hosts the largest high-performance\ncomputing (HPC) capabilities dedicated to energy research while functioning as\na living laboratory for energy-efficient computing. NREL's HPC capabilities\nsupport the research needs of the Department of Energy's Office of Energy\nEfficiency and Renewable Energy (EERE). In ten years of operation, HPC use in\nEERE-sponsored research has grown by a factor of 30, including work in\nelectricity generation, energy efficiency, transportation, and energy system\nmodeling. This paper analyzes this research portfolio, providing examples of\nindividual use cases. The paper documents NREL's history of operating one of\nthe world's most energy-efficient data centers while examining pathways to\nreduce economic and environmental impact beyond reduction of Power Usage\nEfficiency (PUE). This paper concludes by examining the unique opportunities\ncreated for accelerating improvements in data center efficiency created by\ncombining an HPC system dedicated to energy research and a research program in\nenergy-efficient computing.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.11944,regular,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': ""User-Centered Course Reengineering: An Analytical Approach to Enhancing\n  Reading Comprehension in Educational Content\n\n  Delivering high-quality content is crucial for effective reading\ncomprehension and successful learning. Ensuring educational materials are\ninterpreted as intended by their authors is a persistent challenge, especially\nwith the added complexity of multimedia and interactivity in the digital age.\nAuthors must continuously revise their materials to meet learners' evolving\nneeds. Detecting comprehension barriers and identifying actionable improvements\nwithin documents is complex, particularly in education where reading is\nfundamental. This study presents an analytical framework to help course\ndesigners enhance educational content to better support learning outcomes.\nGrounded in a robust theoretical foundation integrating learning analytics,\nreading comprehension, and content revision, our approach introduces\nusage-based document reengineering. This methodology adapts document content\nand structure based on insights from analyzing digital reading\ntraces-interactions between readers and content. We define reading sessions to\ncapture these interactions and develop indicators to detect comprehension\nchallenges. Our framework enables authors to receive tailored content revision\nrecommendations through an interactive dashboard, presenting actionable\ninsights from reading activity. The proposed approach was implemented and\nevaluated using data from a European e-learning platform. Evaluations validate\nthe framework's effectiveness, demonstrating its capacity to empower authors\nwith data-driven insights for targeted revisions. The findings highlight the\nframework's ability to enhance educational content quality, making it more\nresponsive to learners' needs. This research significantly contributes to\nlearning analytics and content optimization, offering practical tools to\nimprove educational outcomes and inform future developments in e-learning.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.994340896606445e-05, 'GPT4': 0.6181640625, 'CLAUDE': 0.018157958984375, 'GOOGLE': 0.049102783203125, 'OPENAI_O_SERIES': 0.3134765625, 'DEEPSEEK': 0.0007319450378417969, 'GROK': 5.960464477539063e-08, 'NOVA': 2.086162567138672e-06, 'OTHER': 0.0001773834228515625, 'HUMAN': 1.2576580047607422e-05}}"
2412.05163,regular,post_llm,2024,12,"{'ai_likelihood': 0.004263983832465278, 'text': ""Americans' Support for AI Development -- Measured Daily with Open Data and Methods\n\nThe rapid development of artificial intelligence should be accompanied by measurement of public sentiment at high temporal resolution. Accordingly, here I present analysis of daily repeated surveys beginning April 18, 2024 (total N=4067). The results indicate that in the population of American adults, support for further development of artificial intelligence was modestly positive and increased a statistically reliable amount over the past year. Female and low-trust respondents reported less support, however, both also displayed growing support over time. Republicans increased support at a faster rate than Democrats, pointing to potential polarization. These findings underscore the need for continuous, high-frequency surveys to accurately track shifts in public opinion on transformative technologies like AI."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.07422,regular,post_llm,2024,12,"{'ai_likelihood': 0.0, 'text': ""Beyond Search Engines: Can Large Language Models Improve Curriculum\n  Development?\n\n  While Online Learning is growing and becoming widespread, the associated\ncurricula often suffer from a lack of coverage and outdated content. In this\nregard, a key question is how to dynamically define the topics that must be\ncovered to thoroughly learn a subject (e.g., a course). Large Language Models\n(LLMs) are considered candidates that can be used to address curriculum\ndevelopment challenges. Therefore, we developed a framework and a novel\ndataset, built on YouTube, to evaluate LLMs' performance when it comes to\ngenerating learning topics for specific courses. The experiment was conducted\nacross over 100 courses and nearly 7,000 YouTube playlists in various subject\nareas. Our results indicate that GPT-4 can produce more accurate topics for the\ngiven courses than extracted topics from YouTube video playlists in terms of\nBERTScore\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.19692,regular,post_llm,2024,12,"{'ai_likelihood': 0.018022325303819444, 'text': ""From prediction to explanation: managing influential negative reviews through explainable AI\n\nThe profound impact of online reviews on consumer decision-making has made it crucial for businesses to manage negative reviews. Recent advancements in artificial intelligence (AI) technology have offered businesses novel and effective ways to manage and analyze substantial consumer feedback. In response to the growing demand for explainablility and transparency in AI applications, this study proposes a novel explainable AI (XAI) algorithm aimed at identifying influential negative reviews. The experiments conducted on 101,338 restaurant reviews validate the algorithm's effectiveness and provides understandable explanations from both the feature-level and word-level perspectives. By leveraging this algorithm, businesses can gain actionable insights for predicting, perceiving, and strategically responding to online negative feedback, fostering improved customer service and mitigating the potential damage caused by negative reviews."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.21106,review,post_llm,2024,12,"{'ai_likelihood': 0.00244140625, 'text': ""Impact of Fourth Industrial Revolution (4IR) on Small and Medium\n  Enterprises (SMEs) and Employment in Bangladesh: Opportunities and Challenges\n\n  The Fourth Industrial Revolution (4IR) is transforming industries and\neconomies worldwide, presenting both opportunities and challenges for Small and\nMedium Enterprises (SMEs) and employment. This study qualitatively explores the\nimpact of 4IR on the SME sector in Bangladesh. Initially, secondary data\nsources are reviewed to establish the context and to prepare the questionnaire\nfor primary data collection. Then, the primary data is collected through Key\nInformant Interviews and Focus Group Discussions with different stakeholders\nincluding SME owners, association representatives, and government officials.\nThe study reveals that while most of the participants have only a superficially\nawareness of 4IR, they view it as a blessing for the SME sector. Despite being\nin early adoption stages in Bangladesh, SMEs anticipate numerous benefits\nincluding enhanced customer experiences, reduced production times, improved\nquality, etc. Regarding employment, most participants believe that adopting 4IR\nin the SME sector of Bangladesh will create new job opportunities. However,\nparticipants express concern about challenges during the transition to 4IR,\nincluding a lack of technical knowledge, financial constraints, inadequate\ntraining, safety and security issues, etc. To fully harness 4IR's potential\nbenefits for SMEs in Bangladesh, several key recommendations emerge that\ninclude analyzing of the current SME landscape, establishing a collaborative\ninformation sharing platform, organizing effective training and workshops,\npromoting resource sharing, encouraging local innovation, attracting foreign\nclients, ensuring proper policy implementation and fostering collaboration\namong government, associations, and academia. By addressing these challenges\nand implementing the recommended strategies, Bangladesh can effectively embrace\nthe transformative benefits of 4IR, simultaneously improving its SME sector.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.03687,review,post_llm,2024,12,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Assessing Changes in Thinking about Troubleshooting in Physical\n  Computing: A Clinical Interview Protocol with Failure Artifacts Scenarios\n\n  Purpose: The purpose of this paper is to examine how a clinical interview\nprotocol with failure artifact scenarios can capture changes in high school\nstudents\' explanations of troubleshooting processes in physical computing\nactivities. We focus on physical computing since finding and fixing hardware\nand software bugs is a highly contextual practice that involves multiple\ninterconnected domains and skills. Approach: We developed and piloted a\n""failure artifact scenarios"" clinical interview protocol. Youth were presented\nwith buggy physical computing projects over video calls and asked for\nsuggestions on how to fix them without having access to the actual project or\nits code. We applied this clinical interview protocol before and after an\neight-week-long physical computing (more specifically, electronic textiles)\nunit. We analyzed matching pre- and post-interviews from 18 students at four\ndifferent schools. Findings: Our findings demonstrate how the protocol can\ncapture change in students\' thinking about troubleshooting by eliciting\nstudents\' explanations of specificity of domain knowledge of problems,\nmultimodality of physical computing, iterative testing of failure artifact\nscenarios, and concreteness of troubleshooting and problem solving processes.\nOriginality: Beyond tests and surveys used to assess debugging, which\ntraditionally focus on correctness or student beliefs, our ""failure artifact\nscenarios"" clinical interview protocol reveals student troubleshooting-related\nthinking processes when encountering buggy projects. As an assessment tool, it\nmay be useful to evaluate the change and development of students\' abilities\nover time.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.01934,review,post_llm,2024,12,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""A Shared Standard for Valid Measurement of Generative AI Systems'\n  Capabilities, Risks, and Impacts\n\n  The valid measurement of generative AI (GenAI) systems' capabilities, risks,\nand impacts forms the bedrock of our ability to evaluate these systems. We\nintroduce a shared standard for valid measurement that helps place many of the\ndisparate-seeming evaluation practices in use today on a common footing. Our\nframework, grounded in measurement theory from the social sciences, extends the\nwork of Adcock & Collier (2001) in which the authors formalized valid\nmeasurement of concepts in political science via three processes: systematizing\nbackground concepts, operationalizing systematized concepts via annotation\nprocedures, and applying those procedures to instances. We argue that valid\nmeasurement of GenAI systems' capabilities, risks, and impacts, further\nrequires systematizing, operationalizing, and applying not only the entailed\nconcepts, but also the contexts of interest and the metrics used. This involves\nboth descriptive reasoning about particular instances and inferential reasoning\nabout underlying populations, which is the purview of statistics. By placing\nmany disparate-seeming GenAI evaluation practices on a common footing, our\nframework enables individual evaluations to be better understood, interrogated\nfor reliability and validity, and meaningfully compared. This is an important\nstep in advancing GenAI evaluation practices toward more formalized and\ntheoretically grounded processes -- i.e., toward a science of GenAI\nevaluations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.06328,regular,post_llm,2024,12,"{'ai_likelihood': 4.437234666612413e-06, 'text': ""Towards Civic Digital Twins: Co-Design the Citizen-Centric Future of\n  Bologna\n\n  We introduce Civic Digital Twin (CDT), an evolution of Urban Digital Twins\ndesigned to support a citizen-centric transformative approach to urban planning\nand governance. CDT is being developed in the scope of the Bologna Digital Twin\ninitiative, launched one year ago by the city of Bologna, to fulfill the city's\npolitical and strategic goal of adopting innovative digital tools to support\ndecision-making and civic engagement. The CDT, in addition to its capability of\nsensing the city through spatial, temporal, and social data, must be able to\nmodel and simulate social dynamics in a city: the behavior, attitude, and\npreference of citizens and collectives and how they impact city life and\ntransform transformation processes. Another distinctive feature of CDT is that\nit must be able to engage citizens (individuals, collectives, and organized\ncivil society) and other civic stakeholders (utilities, economic actors, third\nsector) interested in co-designing the future of the city. In this paper, we\ndiscuss the motivations that led to the definition of the CDT, define its\nmodeling aspects and key research challenges, and illustrate its intended use\nwith two use cases in urban mobility and urban development.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.19273,review,post_llm,2024,12,"{'ai_likelihood': 4.735257890489366e-06, 'text': 'Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor\n  2024\n\n  There are great expectations for the use of AI in Norway. On the other hand,\nit is reported that the adoption of AI in Norway is slower than expected in\nboth the private and public sectors. Using responses from NOKIOS Technology\nRadar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as\nwell as another national survey as part of a five-year cycle, this article\nlooks at reported and planned use of AI with a focus on local (municipalities)\nand national government agencies. IT in practice is distributed to a large\nnumber of Norwegian public agencies, with a response rate of over 5o percent.\nThe most recent data (2024) presented in this article is based on responses\nfrom 335 public organizations, with 237 municipalities, and 98 public\norganizations at the national or regional level. The survey confirms that the\nuse of AI is still at an early stage, although expectations are high for future\nuse.\n  --\n  Det er store forventninger til bruk av KI i Norge. P{\\aa} den annen side\nrapporteres det at adopsjonen av KI i Norge g{\\aa}r tregere enn forventet\nb{\\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS\nteknologiradar 2017-2021, IT i Praksis unders{\\o}kelser utf{\\o}rt av Ramb{\\o}ll\ni 2021-2024, samt en annen nasjonal unders{\\o}kelse som en del av en\nfem{\\aa}rig syklus, ser vi i denne artikkelen p{\\aa} rapportert og planlagt\nbruk av KI med fokus p{\\aa} lokale (kommuner) og nasjonale offentlige etater.\nIT i praksis distribueres til en lang rekke norske offentlige virksomheter, med\nen svarprosent p{\\aa} over 50 prosent. De nyeste dataene (2024) presentert i\ndenne artikkelen er basert p{\\aa} svar fra 335 offentlige organisasjoner, med\n237 kommuner, og 98 offentlige organisasjoner p{\\aa} nasjonalt eller regionalt\nniv{\\aa}. Unders{\\o}kelsen bekrefter at bruken av KI fortsatt er p{\\aa} et\ntidlig stadium, selv om forventningene er h{\\o}ye til fremtidig bruk.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.13554,regular,post_llm,2024,12,"{'ai_likelihood': 4.4273005591498484e-05, 'text': ""An XAI Social Media Platform for Teaching K-12 Students AI-Driven\n  Profiling, Clustering, and Engagement-Based Recommending\n\n  This paper, submitted to the special track on resources for teaching AI in\nK-12, presents an explainable AI (XAI) education tool designed for K-12\nclassrooms, particularly for students in grades 4-9. The tool was designed for\ninterventions on the fundamental processes behind social media platforms,\nfocusing on four AI- and data-driven core concepts: data collection, user\nprofiling, engagement metrics, and recommendation algorithms. An Instagram-like\ninterface and a monitoring tool for explaining the data-driven processes make\nthese complex ideas accessible and engaging for young learners. The tool\nprovides hands-on experiments and real-time visualizations, illustrating how\nuser actions influence both their personal experience on the platform and the\nexperience of others. This approach seeks to enhance learners' data agency, AI\nliteracy, and sensitivity to AI ethics. The paper includes a case example from\n12 two-hour test sessions involving 209 children, using learning analytics to\ndemonstrate how they navigated their social media feeds and the browsing\npatterns that emerged.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.14943,regular,post_llm,2024,12,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Unveiling social vibrancy in urban spaces with app usage\n\n  Urban vibrancy is an important measure of the energetic nature of a city that\nis related to why and how people use urban spaces, and it is inherently\nconnected with our social behaviour. Increasingly, people use a wide range of\nmobile phone apps in their daily lives to connect socially, search for\ninformation, make decisions, and arrange travel, amongst many other reasons.\nHowever, the relationship between online app usage and urban vibrancy remains\nunclear, particularly regarding how sociospatial behaviours interact with urban\nfeatures. Here, we use app-usage data as a digital signature to investigate\nthis question. To do this, we use a high-resolution data source of mobile\nservice-level traffic volumes across eighteen cities in France. We investigate\nthe social component of cities using socially relevant urban features\nconstructed from OpenStreetMap 'Points of Interest'. We developed a methodology\nfor identifying and classifying multidimensional app usage time series based on\nsimilarity. We used these in predictive models to interpret the results for\neach city and across France. Across cities, there were spatial behavioural\narchetypes, characterised by multidimensional properties. We found patterns\nbetween the week and the weekend, and across cities, and the country. These\narchetypes correspond to changes in socially relevant urban features that\nimpact urban vibrancy. Our results add further evidence for the importance of\nusing computational approaches to understand urban environments, the use of\nsociological concepts in computational science, and urban vibrancy in cities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.07275,regular,post_llm,2024,12,"{'ai_likelihood': 3.30805778503418e-05, 'text': 'Reconciling Human Development and Giant Panda Protection Goals:\n  Cost-efficiency Evaluation of Farmland Reverting and Energy Substitution\n  Programs in Wolong National Reserve\n\n  Balancing human development with conservation necessitates ecological\npolicies that optimize outcomes within limited budgets, highlighting the\nimportance of cost-efficiency and local impact analysis. This study employs the\nSocio-Econ-Ecosystem Multipurpose Simulator (SEEMS), an Agent-Based Model (ABM)\ndesigned for simulating small-scale Coupled Human and Nature Systems (CHANS),\nto evaluate the cost-efficiency of two major ecology conservation programs:\nGrain-to-Green (G2G) and Firewood-to-Electricity (F2E). Focusing on China\nWolong National Reserve, a worldwide hot spot for flagship species\nconservation, the study evaluates the direct benefits of these programs,\nincluding reverted farmland area and firewood consumption, along with their\ncombined indirect benefits on habitat quality, carbon emissions, and gross\neconomic benefits. The findings are as follows: (1) The G2G program achieves\noptimal financial efficiency at approximately 500 CNY/Mu, with diminishing\nreturns observed beyond 1000 CNY/Mu; (2) For the F2E program, the most fiscally\ncost-efficient option arises when the subsidized electricity price is at\n0.4-0.5 CNY/kWh, while further reductions of the prices to below 0.1 CNY/kWh\nresult in a diminishing cost-benefit ratio; (3) Comprehensive cost-efficiency\nanalysis reveals no significant link between financial burden and carbon\nemissions, but a positive correlation with habitat quality and an inverted\nU-shaped relationship with total economic income; (4) Pareto analysis\nidentifies 18 optimal dual-policy combinations for balancing carbon footprint,\nhabitat quality, and gross economic benefits; (5) Posterior Pareto optimization\nfurther refines the selection of a specific policy scheme for a given realistic\nscenario. The analytical framework of this paper helps policymakers design\neconomically viable and environmentally sustainable policies, addressing global\nconservation challenges.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.14286,review,post_llm,2024,12,"{'ai_likelihood': 4.072984059651693e-06, 'text': 'Measuring DNS Censorship of Generative AI Platforms\n\n  Generative AI is an invaluable tool, however, in some parts of the world,\nthis technology is censored due to political or societal issues. In this work,\nwe monitor Generative AI censorship through the DNS protocol. We find China to\nbe a leading country of Generative AI censorship. Interestingly, China does not\ncensor all AI domain names. We also report censorship in Russia and find\ninconsistencies in their process. We compare our results to other measurement\nplatforms (OONI, Censored Planet, GFWatch), and present their lack of data on\nGenerative AI domains.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.12113,regular,post_llm,2024,12,"{'ai_likelihood': 0.15760633680555555, 'text': ""Application of Analytical Hierarchical Process and its Variants on\n  Remote Sensing Datasets\n\n  The river Ganga is one of the Earth's most critically important river basins,\nyet it faces significant pollution challenges, making it crucial to evaluate\nits vulnerability for effective and targeted remediation efforts. While the\nAnalytic Hierarchy Process (AHP) is widely regarded as the standard in decision\nmaking methodologies, uncertainties arise from its dependence on expert\njudgments, which can introduce subjectivity, especially when applied to remote\nsensing data, where expert knowledge might not fully capture spatial and\nspectral complexities inherent in such data. To address that, in this paper, we\napplied AHP alongside a suite of alternative existing and novel variants of\nAHP-based decision analysis on remote sensing data to assess the vulnerability\nof the river Ganga to pollution. We then compared the areas where the outputs\nof each variant may provide additional insights over AHP. Lastly, we utilized\nour learnings to design a composite variable to robustly define the\nvulnerability of the river Ganga to pollution. This approach contributes to a\nmore comprehensive understanding of remote sensing data applications in\nenvironmental assessment, and these decision making variants can also have\nbroader applications in other areas of environment management and\nsustainability, facilitating more precise and adaptable decision support\nframeworks.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.15787,review,post_llm,2024,12,"{'ai_likelihood': 1.5894571940104167e-06, 'text': 'AI Apology: A Critical Review of Apology in AI Systems\n\n  Apologies are a powerful tool used in human-human interactions to provide\naffective support, regulate social processes, and exchange information\nfollowing a trust violation. The emerging field of AI apology investigates the\nuse of apologies by artificially intelligent systems, with recent research\nsuggesting how this tool may provide similar value in human-machine\ninteractions. Until recently, contributions to this area were sparse, and these\nworks have yet to be synthesised into a cohesive body of knowledge. This\narticle provides the first synthesis and critical analysis of the state of AI\napology research, focusing on studies published between 2020 and 2023. We\nderive a framework of attributes to describe five core elements of apology:\noutcome, interaction, offence, recipient, and offender. With this framework as\nthe basis for our critique, we show how apologies can be used to recover from\nmisalignment in human-AI interactions, and examine trends and inconsistencies\nwithin the field. Among the observations, we outline the importance of curating\na human-aligned and cross-disciplinary perspective in this research, with\nconsideration for improved system capabilities and long-term outcomes.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.09829,regular,post_llm,2024,12,"{'ai_likelihood': 0.96337890625, 'text': 'Speech-based Multimodel Pipeline for Vietnamese Services Quality\n  Assessment\n\n  In the evolving landscape of customer service within the digital economy,\ntraditional methods of service quality assessment have shown significant\nlimitations, this research proposes a novel deep-learning approach to service\nquality assessment, focusing on the Vietnamese service sector. By leveraging a\nmulti-modal pipeline that transcends traditional evaluation methods, the\nresearch addresses the limitations of conventional assessments by analyzing\nspeech, speaker interactions and emotional content, offering a more\ncomprehensive and objective means of understanding customer service\ninteractions. This aims to provide organizations with a sophisticated tool for\nevaluating and improving service quality in the digital economy.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.017547607421875, 'GPT4': 0.76904296875, 'CLAUDE': 0.03375244140625, 'GOOGLE': 0.10162353515625, 'OPENAI_O_SERIES': 0.0014047622680664062, 'DEEPSEEK': 8.171796798706055e-05, 'GROK': 7.528066635131836e-05, 'NOVA': 0.0004200935363769531, 'OTHER': 0.0653076171875, 'HUMAN': 0.01064300537109375}}"
2501.10364,review,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': 'AI-Enhanced Decision-Making for Sustainable Supply Chains: Reducing\n  Carbon Footprints in the USA\n\n  Organizations increasingly need to reassess their supply chain strategies in\nthe rapidly modernizing world towards sustainability. This is particularly true\nin the United States, where supply chains are very extensive and consume a\nlarge number of resources. This research paper discusses how AI can support\ndecision-making for sustainable supply chains with a special focus on carbon\nfootprints. These AI technologies, including machine learning, predictive\nanalytics, and optimization algorithms, will enable companies to be more\nefficient, reduce emissions, and display regulatory and consumer demands for\nsustainability, among other aspects. The paper reviews challenges and\nopportunities regarding implementing AI-driven solutions to promote sustainable\nsupply chain practices in the USA.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0762939453125, 'GPT4': 0.86474609375, 'CLAUDE': 0.0029010772705078125, 'GOOGLE': 0.04840087890625, 'OPENAI_O_SERIES': 0.00037384033203125, 'DEEPSEEK': 7.897615432739258e-05, 'GROK': 2.950429916381836e-05, 'NOVA': 9.232759475708008e-05, 'OTHER': 0.00717926025390625, 'HUMAN': 6.175041198730469e-05}}"
2412.15223,regular,post_llm,2024,12,"{'ai_likelihood': 0.03416273328993056, 'text': ""Public Engagement in Action: Developing an Introductory Programming\n  Module for Apprentices\n\n  Programming is a crucial skill in today's world and being taught worldwide at\ndifferent levels. However, in the literature there is little research\ninvestigating a formal approach to embedding public engagement into programming\nmodule design. This paper explores the integration of public engagement into an\nintroductory programming module, at the University of Warwick, UK, as part of\nthe Digital and Technology Solutions (DTS) degree apprenticeship. The module\ndesign follows a 'V' model, which integrates community engagement with\ntraditional programming education, providing a holistic learning experience.\nThe aim is to enhance learning by combining programming education with\ncommunity engagement. Apprentices participate in outreach activities, teaching\nprogramming and Arduino hardware to local secondary school students. This\nhands-on approach aligns with Kolb's experiential learning model, improving\ncommunication skills and solidifying programming concepts through teaching. The\nmodule also includes training in safeguarding, presentation skills, and\nstorytelling to prepare apprentices for public engagement. Pedagogical\ntechniques in the module include live coding, group exercises, and Arduino kit\nusage, as well as peer education, allowing apprentices to learn from and teach\neach other. Degree apprentices, who balance part-time studies with full-time\nemployment, bring diverse knowledge and motivations. The benefit of public\nengagement is that it helps bridge their skills gap, fostering teamwork and\ncreating a positive learning environment. Embedding public engagement in\nprogramming education also enhances both technical and soft skills, providing\napprentices with a deeper understanding of community issues and real-world\napplications. Our design supports their academic and professional growth,\nensuring the module's ongoing success and impact.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.09048,regular,post_llm,2024,12,"{'ai_likelihood': 9.51687494913737e-05, 'text': ""Oversight in Action: Experiences with Instructor-Moderated LLM Responses\n  in an Online Discussion Forum\n\n  The integration of large language models (LLMs) into computing education\noffers many potential benefits to student learning, and several novel\npedagogical approaches have been reported in the literature. However LLMs also\npresent challenges, one of the most commonly cited being that of student\nover-reliance. This challenge is compounded by the fact that LLMs are always\navailable to provide instant help and solutions to students, which can\nundermine their ability to independently solve problems and diagnose and\nresolve errors. Providing instructor oversight of LLM-generated content can\nmitigate this problem, however it is often not practical in real-time learning\ncontexts. Online class discussion forums, which are widely used in computing\neducation, present an opportunity for exploring instructor oversight because\nthey operate asynchronously. Unlike real-time interactions, the discussion\nforum format aligns with the expectation that responses may take time, making\noversight not only feasible but also pedagogically appropriate. In this\npractitioner paper, we present the design, deployment, and evaluation of a\n`bot' module that is controlled by the instructor, and integrated into an\nonline discussion forum. The bot assists the instructor by generating draft\nresponses to student questions, which are reviewed, modified, and approved\nbefore release. Key features include the ability to leverage course materials,\naccess archived discussions, and publish responses anonymously to encourage\nopen participation. We report our experiences using this tool in a 12-week\nsecond-year software engineering course on object-oriented programming.\nInstructor feedback confirmed the tool successfully alleviated workload but\nhighlighted a need for improvement in handling complex, context-dependent\nqueries. We report the features that were viewed as most beneficial, and\nsuggest avenues for future exploration.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.14971,regular,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': ""AI and Cultural Context: An Empirical Investigation of Large Language\n  Models' Performance on Chinese Social Work Professional Standards\n\n  Objective: This study examines how well leading Chinese and Western large\nlanguage models understand and apply Chinese social work principles, focusing\non their foundational knowledge within a non-Western professional setting. We\ntest whether the cultural context in the developing country influences model\nreasoning and accuracy.\n  Method: Using a published self-study version of the Chinese National Social\nWork Examination (160 questions) covering jurisprudence and applied knowledge,\nwe administered three testing conditions to eight cloud-based large language\nmodels - four Chinese and four Western. We examined their responses following\nofficial guidelines and evaluated their explanations' reasoning quality.\n  Results: Seven models exceeded the 60-point passing threshold in both\nsections. Chinese models performed better in jurisprudence (median = 77.0 vs.\n70.3) but slightly lower in applied knowledge (median = 65.5 vs. 67.0). Both\ngroups showed cultural biases, particularly regarding gender equality and\nfamily dynamics. Models demonstrated strong professional terminology knowledge\nbut struggled with culturally specific interventions. Valid reasoning in\nincorrect answers ranged from 16.4% to 45.0%.\n  Conclusions: While both Chinese and Western models show foundational\nknowledge of Chinese social work principles, technical language proficiency\ndoes not ensure cultural competence. Chinese models demonstrate advantages in\nregulatory content, yet both Chinese and Western models struggle with\nculturally nuanced practice scenarios. These findings contribute to informing\nresponsible AI integration into cross-cultural social work practice.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 1.0192394256591797e-05, 'CLAUDE': 1.0, 'GOOGLE': 1.8477439880371094e-06, 'OPENAI_O_SERIES': 4.172325134277344e-07, 'DEEPSEEK': 0.00010013580322265625, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.10555,regular,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': 'Impact of Shoe Parameters on Gait Using Wearables\n\n  The study of biomechanics during locomotion provides valuable insights into\nthe effects of varying conditions on specific movement patterns. This research\nfocuses on examining the influence of different shoe parameters on walking\nbiomechanics, aiming to understand their impact on gait patterns. To achieve\nthis, various methodologies are explored to estimate human body biomechanics,\nincluding computer vision techniques and wearable devices equipped with\nadvanced sensors. Given privacy considerations and the need for robust,\naccurate measurements, this study employs wearable devices with Inertial\nMeasurement Unit (IMU) sensors. These devices offer a non-invasive, precise,\nand high-resolution approach to capturing biomechanical data during locomotion.\nRaw sensor data collected from wearable devices is processed using an Extended\nKalman Filter to reduce noise and extract meaningful information. This includes\ncalculating joint angles throughout the gait cycle, enabling a detailed\nanalysis of movement dynamics. The analysis identifies correlations between\nshoe parameters and key gait characteristics, such as stability, mobility, step\ntime, and propulsion forces. The findings provide deeper insights into how\nfootwear design influences walking efficiency and biomechanics. This study\npaves the way for advancements in footwear technology and contributes to the\ndevelopment of personalized solutions for enhancing gait performance and\nmobility.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.004283905029296875, 'GPT4': 0.0880126953125, 'CLAUDE': 0.004047393798828125, 'GOOGLE': 0.85546875, 'OPENAI_O_SERIES': 0.04168701171875, 'DEEPSEEK': 0.00017070770263671875, 'GROK': 2.205371856689453e-05, 'NOVA': 0.000823974609375, 'OTHER': 0.005565643310546875, 'HUMAN': 5.739927291870117e-05}}"
2412.09248,review,post_llm,2024,12,"{'ai_likelihood': 0.83642578125, 'text': ""A Systematic Review of Knowledge Tracing and Large Language Models in\n  Education: Opportunities, Issues, and Future Research\n\n  Knowledge Tracing (KT) is a research field that aims to estimate a student's\nknowledge state through learning interactions-a crucial component of\nIntelligent Tutoring Systems (ITSs). Despite significant advancements, no\ncurrent KT models excel in both predictive accuracy and interpretability.\nMeanwhile, Large Language Models (LLMs), pre-trained on vast natural language\ndatasets, have emerged as powerful tools with immense potential in various\neducational applications. This systematic review explores the intersections,\nopportunities, and challenges of combining KT models and LLMs in educational\ncontexts. The review first investigates LLM applications in education,\nincluding their adaptability to domain-specific content and ability to support\npersonalized learning. It then examines the development and current state of KT\nmodels, from traditional to advanced approaches, aiming to uncover potential\nchallenges that LLMs could mitigate. The core of this review focuses on\nintegrating LLMs with KT, exploring three primary functions: addressing general\nconcerns in KT fields, overcoming specific KT model limitations, and performing\nas KT models themselves. Our findings reveal that LLMs can be customized for\nspecific educational tasks through tailor-making techniques such as in-context\nlearning and agent-based approaches, effectively managing complex and\nunbalanced educational data. These models can enhance existing KT models'\nperformance and solve cold-start problems by generating relevant features from\nquestion data. However, both current models depend heavily on structured,\nlimited datasets, missing opportunities to use diverse educational data that\ncould offer deeper insights into individual learners and support various\neducational settings.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0002880096435546875, 'GPT4': 0.59814453125, 'CLAUDE': 0.0008301734924316406, 'GOOGLE': 0.362060546875, 'OPENAI_O_SERIES': 0.00638580322265625, 'DEEPSEEK': 6.604194641113281e-05, 'GROK': 9.5367431640625e-06, 'NOVA': 4.0531158447265625e-06, 'OTHER': 6.133317947387695e-05, 'HUMAN': 0.03192138671875}}"
2412.1796,review,post_llm,2024,12,"{'ai_likelihood': 0.0016890631781684028, 'text': ""Inspiring Women in Technology: Educational Pathways and Impact\n\n  This paper presents initiatives aimed at fostering female involvement in the\nrealm of computing and endeavoring to inspire more women to pursue careers in\nthese fields. The Meninas++ Project coordinates activities at both the high\nschool and higher education levels, facilitating dialogue between young women\nand computing professionals, and promoting female role models within the field.\nOur study demonstrated the significant impact of these activities on inspiring,\nempowering, and retaining female students in computing. Furthermore, higher\neducation initiatives have fostered engagement among both women and men,\npromoting inclusivity, entrepreneurship, and collaboration to enhance women's\nrepresentation in the computing field.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10374,review,post_llm,2024,12,"{'ai_likelihood': 1.0, 'text': 'Artificial Intelligence in Mental Health and Well-Being: Evolution,\n  Current Applications, Future Challenges, and Emerging Evidence\n\n  Artificial Intelligence (AI) is a broad field that is upturning mental health\ncare in many ways, from addressing anxiety, depression, and stress to\nincreasing access, personalization of treatment, and real-time monitoring that\nenhances patient outcomes. The current paper discusses the evolution, present\napplication, and future challenges in the field of AI for mental health and\nwell-being. From the early chatbot models, such as ELIZA, to modern machine\nlearning systems, the integration of AI in mental health has grown rapidly to\naugment traditional treatment and open innovative solutions. AI-driven tools\nprovide continuous support, offering personalized interventions and addressing\nissues such as treatment access and patient stigma. AI also enables early\ndiagnosis through the analysis of complex datasets, including speech patterns\nand social media behavior, to detect early signs of conditions like depression\nand Post-Traumatic Stress Disorder (PTSD). Ethical challenges persist, however,\nmost notably around privacy, data security, and algorithmic bias. With AI at\nthe core of mental health care, there is a dire need to develop strong ethical\nframeworks that ensure patient rights are protected, access is equitable, and\ntransparency is maintained in AI applications. Going forward, the role of AI in\nmental health will continue to evolve, and continued research and policy\ndevelopment will be needed to meet the diverse needs of patients while\nmitigating associated risks.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.005435943603515625, 'GPT4': 0.994140625, 'CLAUDE': 1.430511474609375e-05, 'GOOGLE': 0.0002808570861816406, 'OPENAI_O_SERIES': 3.337860107421875e-06, 'DEEPSEEK': 2.980232238769531e-07, 'GROK': 5.960464477539063e-08, 'NOVA': 1.1920928955078125e-07, 'OTHER': 2.5033950805664062e-06, 'HUMAN': 3.5762786865234375e-07}}"
2501.10372,regular,post_llm,2024,12,"{'ai_likelihood': 0.2374945746527778, 'text': ""Personalized and Safe Route Planning for Asthma Patients Using Real-Time\n  Environmental Data\n\n  Asthmatic patients are very frequently affected by the quality of air,\nclimatic conditions, and traffic density during outdoor activities. Most of the\nconventional routing algorithms, such as Dijkstra's algorithm, usually fail to\nconsider these health dimensions, hence resulting in suboptimal or risky\nrecommendations. Here, the health-aware heuristic framework is presented that\nshall utilize real-time data provided by the Microsoft Weather API. The\nadvanced A* algorithm provides dynamic changes in routes depending on air\nquality indices, temperature, traffic density, and other patient-related health\ndata. The power of the model is realized by running simulations in city\nenvironments and outperforming the state-of-the-art methodology in terms of\nrecommendation accuracy at low computational overhead. It provides\nhealth-sensitive route recommendations, keeping in mind the avoidance of\nhigh-risk areas and ensuring safer and more suitable travel options for\nasthmatic patients.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10389,review,post_llm,2024,12,"{'ai_likelihood': 0.99755859375, 'text': 'Transparency, Security, and Workplace Training & Awareness in the Age of\n  Generative AI\n\n  This paper investigates the impacts of the rapidly evolving landscape of\ngenerative Artificial Intelligence (AI) development. Emphasis is given to how\norganizations grapple with a critical imperative: reevaluating their policies\nregarding AI usage in the workplace. As AI technologies advance, ethical\nconsiderations, transparency, data privacy, and their impact on human labor\nintersect with the drive for innovation and efficiency. Our research explores\npublicly accessible large language models (LLMs) that often operate on the\nperiphery, away from mainstream scrutiny. These lesser-known models have\nreceived limited scholarly analysis and may lack comprehensive restrictions and\nsafeguards. Specifically, we examine Gab AI, a platform that centers around\nunrestricted communication and privacy, allowing users to interact freely\nwithout censorship. Generative AI chatbots are increasingly prevalent, but\ncybersecurity risks have also escalated. Organizations must carefully navigate\nthis evolving landscape by implementing transparent AI usage policies. Frequent\ntraining and policy updates are essential to adapt to emerging threats. Insider\nthreats, whether malicious or unwitting, continue to pose one of the most\nsignificant cybersecurity challenges in the workplace. Our research is on the\nlesser-known publicly accessible LLMs and their implications for workplace\npolicies. We contribute to the ongoing discourse on AI ethics, transparency,\nand security by emphasizing the need for well-thought-out guidelines and\nvigilance in policy maintenance.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.577108383178711e-05, 'GPT4': 0.73388671875, 'CLAUDE': 7.683038711547852e-05, 'GOOGLE': 0.2626953125, 'OPENAI_O_SERIES': 0.0031642913818359375, 'DEEPSEEK': 5.4717063903808594e-05, 'GROK': 1.7881393432617188e-07, 'NOVA': 1.5020370483398438e-05, 'OTHER': 5.060434341430664e-05, 'HUMAN': 5.120038986206055e-05}}"
2412.17618,review,post_llm,2024,12,"{'ai_likelihood': 0.6704101562499999, 'text': 'Dynamic safety cases for frontier AI\n\n  Frontier artificial intelligence (AI) systems present both benefits and risks\nto society. Safety cases - structured arguments supported by evidence - are one\nway to help ensure the safe development and deployment of these systems. Yet\nthe evolving nature of AI capabilities, as well as changes in the operational\nenvironment and understanding of risk, necessitates mechanisms for continuously\nupdating these safety cases. Typically, in other sectors, safety cases are\nproduced pre-deployment and do not require frequent updates post-deployment,\nwhich can be a manual, costly process. This paper proposes a Dynamic Safety\nCase Management System (DSCMS) to support both the initial creation of a safety\ncase and its systematic, semi-automated revision over time. Drawing on methods\ndeveloped in the autonomous vehicles (AV) sector - state-of-the-art Checkable\nSafety Arguments (CSA) combined with Safety Performance Indicators (SPIs)\nrecommended by UL 4600, a DSCMS helps developers maintain alignment between\nsystem safety claims and the latest system state. We demonstrate this approach\non a safety case template for offensive cyber capabilities and suggest ways it\ncan be integrated into governance structures for safety-critical\ndecision-making. While the correctness of the initial safety argument remains\nparamount - particularly for high-severity risks - a DSCMS provides a framework\nfor adapting to new insights and strengthening incident response. We outline\nchallenges and further work towards development and implementation of this\napproach as part of continuous safety assurance of frontier AI systems.\n', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.00011396408081054688, 'GPT4': 0.00084686279296875, 'CLAUDE': 0.8828125, 'GOOGLE': 0.0007796287536621094, 'OPENAI_O_SERIES': 4.887580871582031e-06, 'DEEPSEEK': 5.829334259033203e-05, 'GROK': 4.76837158203125e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 2.3245811462402344e-06, 'HUMAN': 0.1153564453125}}"
2412.15219,review,post_llm,2024,12,"{'ai_likelihood': 4.371007283528646e-06, 'text': ""Analyzing Computing Undergraduate Majors from Job Market Perspective\n\n  The demand for computing education increases due to the rapid development of\ntechnology and its involvement in most daily activities. Academic institutes\noffer a variety of computing majors, such as Computer Engineering, Computer\nScience, Information Systems, Information Technology, Software Engineering,\nCybersecurity, and Data Science. Since a major objective of earning a\nbachelor's degree is to improve career opportunities, it is crucial to\nunderstand how the job market perceives these computing majors. This study\nanalyzed the relationships between various computing majors and the job market\nin Saudi Arabia, using LinkedIn public profile data, discovering insights into\nthe strong relationship between the focus of certain computing majors and the\nemployment of relevant job positions. Moreover, job category trends were\nanalyzed over the past ten years, observing that demands for System Admin and\nTechnical Support positions declined while demands for Business Analysis and\nArtificial Intelligence and Data Science inclined. This study also compared\nearned professional certifications between different computing major graduates\nthat correspond to job position findings.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2412.03292,regular,post_llm,2024,12,"{'ai_likelihood': 0.002532535129123264, 'text': 'DMP_AI: An AI-Aided K-12 System for Teaching and Learning in Diverse\n  Schools\n\n  The use of Artificial Intelligence (AI) has gained momentum in education.\nHowever, the use of AI in K-12 education is still in its nascent stages, and\nfurther research and development is needed to realize its potential. Moreover,\nthe creation of a comprehensive and cohesive system that effectively harnesses\nAI to support teaching and learning across a diverse range of primary and\nsecondary schools presents substantial challenges that need to be addressed. To\nfill these gaps, especially in countries like China, we designed and\nimplemented the DMP_AI (Data Management Platform_Artificial Intelligence)\nsystem, an innovative AI-aided educational system specifically designed for\nK-12 education. The system utilizes data mining, natural language processing,\nand machine learning, along with learning analytics, to offer a wide range of\nfeatures, including student academic performance and behavior prediction, early\nwarning system, analytics of Individualized Education Plan, talented students\nprediction and identification, and cross-school personalized electives\nrecommendation. The development of this system has been meticulously carried\nout while prioritizing user privacy and addressing the challenges posed by data\nheterogeneity. We successfully implemented the DMP_AI system in real-world\nprimary and secondary schools, allowing us to gain valuable insights into the\npotential and challenges of integrating AI into K-12 education in the real\nworld. This system will serve as a valuable resource for supporting educators\nin providing effective and inclusive K-12 education.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.00957,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': 'Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors\n\n  The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.947185516357422e-06, 'GPT4': 0.99755859375, 'CLAUDE': 0.0006237030029296875, 'GOOGLE': 5.716085433959961e-05, 'OPENAI_O_SERIES': 0.0001595020294189453, 'DEEPSEEK': 0.0014009475708007812, 'GROK': 4.76837158203125e-07, 'NOVA': 2.5033950805664062e-06, 'OTHER': 4.291534423828125e-06, 'HUMAN': 5.960464477539062e-07}}"
2501.19231,regular,post_llm,2025,1,"{'ai_likelihood': 0.038825141059027776, 'text': 'The geography of inequalities in access to healthcare across England: the role of bus travel time variability\n\nFair access to healthcare facilities is fundamental to achieving social equity. Traditional travel time-based accessibility measures often overlook the dynamic nature of travel times resulting from different departure times, which compromises the accuracy of these measures in reflecting the true accessibility experienced by individuals. This study examines public transport-based accessibility to healthcare facilities across England from the perspective of travel time variability (TTV). Using comprehensive bus timetable data from the Bus Open Data Service (BODS), we calculated hourly travel times from each Lower Layer Super Output Area (LSOA) to the nearest hospitals and general practices and developed a TTV metric for each LSOA and analysed its geographical inequalities across various spatial scales. Our analysis reveals notable spatial-temporal patterns of TTV and average travel times, including an urban-rural divide, clustering of high and low TTV regions, and distinct outliers. Furthermore, we explored the relationship between TTV and deprivation, categorising LSOAs into four groups based on their unique characteristics, which provides valuable insights for designing targeted interventions. Our study also highlights the limitations of using theoretical TTV derived from timetable data and emphasises the potential of using real-time operational data to capture more realistic accessibility measures. By offering a more dynamic perspective on accessibility, our findings complement existing travel time-based metrics and pave way for future research on TTV-based accessibility using real-time data. This evidence-based approach can inform efforts to ""level up"" public transport services, addressing geographical inequalities and promoting equitable access to essential healthcare services.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.08262,regular,post_llm,2025,1,"{'ai_likelihood': 0.99755859375, 'text': 'Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG\n\n  Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.001953125, 'GPT4': 0.0230255126953125, 'CLAUDE': 0.5205078125, 'GOOGLE': 0.419189453125, 'OPENAI_O_SERIES': 0.000507354736328125, 'DEEPSEEK': 0.001922607421875, 'GROK': 1.2814998626708984e-05, 'NOVA': 4.082918167114258e-05, 'OTHER': 0.032073974609375, 'HUMAN': 0.00047850608825683594}}"
2502.0001,regular,post_llm,2025,1,"{'ai_likelihood': 0.80224609375, 'text': 'IntelliChain: An Integrated Framework for Enhanced Socratic Method\n  Dialogue with LLMs and Knowledge Graphs\n\n  With the continuous advancement of educational technology, the demand for\nLarge Language Models (LLMs) as intelligent educational agents in providing\npersonalized learning experiences is rapidly increasing. This study aims to\nexplore how to optimize the design and collaboration of a multi-agent system\ntailored for Socratic teaching through the integration of LLMs and knowledge\ngraphs in a chain-of-thought dialogue approach, thereby enhancing the accuracy\nand reliability of educational applications. By incorporating knowledge graphs,\nthis research has bolstered the capability of LLMs to handle specific\neducational content, ensuring the accuracy and relevance of the information\nprovided. Concurrently, we have focused on developing an effective multi-agent\ncollaboration mechanism to facilitate efficient information exchange and chain\ndialogues among intelligent agents, significantly improving the quality of\neducational interaction and learning outcomes. In empirical research within the\ndomain of mathematics education, this framework has demonstrated notable\nadvantages in enhancing the accuracy and credibility of educational\ninteractions. This study not only showcases the potential application of LLMs\nand knowledge graphs in mathematics teaching but also provides valuable\ninsights and methodologies for the development of future AI-driven educational\nsolutions.\n', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0012607574462890625, 'GPT4': 0.90087890625, 'CLAUDE': 0.0012006759643554688, 'GOOGLE': 0.03753662109375, 'OPENAI_O_SERIES': 0.0119476318359375, 'DEEPSEEK': 0.00013148784637451172, 'GROK': 7.212162017822266e-06, 'NOVA': 4.583597183227539e-05, 'OTHER': 0.002002716064453125, 'HUMAN': 0.044769287109375}}"
2501.07858,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': 'Examining the Representation of Youth in the US Policy Documents through\n  the Lens of Research\n\n  This study explores the representation of youth in US policy documents by\nanalyzing how research on youth topics is cited within these policies. The\nresearch focuses on three key questions: identifying the frequently discussed\ntopics in youth research that receive citations in policy documents, discerning\npatterns in youth research that contribute to higher citation rates in policy,\nand comparing the alignment between topics in youth research and those in\nciting policy documents. Through this analysis, the study aims to shed light on\nthe relationship between academic research and policy formulation, highlighting\nareas where youth issues are effectively integrated into policy and\ncontributing to the broader goal of enhancing youth engagement in societal\ndecision-making processes.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.56591796875, 'GPT4': 0.1724853515625, 'CLAUDE': 0.0682373046875, 'GOOGLE': 0.071533203125, 'OPENAI_O_SERIES': 0.006969451904296875, 'DEEPSEEK': 0.00017714500427246094, 'GROK': 0.0002799034118652344, 'NOVA': 0.0029506683349609375, 'OTHER': 0.11126708984375, 'HUMAN': 6.0498714447021484e-05}}"
2501.0256,regular,post_llm,2025,1,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'A system for objectively measuring behavior and the environment to\n  support large-scale studies on childhood obesity\n\n  Advances in IoT technologies combined with new algorithms have enabled the\ncollection and processing of high-rate multi-source data streams that quantify\nhuman behavior in a fine-grained level and can lead to deeper insights on\nindividual behaviors as well as on the interplay between behaviors and the\nenvironment. In this paper, we present an integrated system that collects and\nextracts multiple behavioral and environmental indicators, aiming at improving\npublic health policies for tackling obesity. Data collection takes place using\npassive methods based on smartphone and smartwatch applications that require\nminimal interaction with the user. Our goal is to present a detailed account of\nthe design principles, the implementation processes, and the evaluation of\nintegrated algorithms, especially given the challenges we faced, in particular\n(a) integrating multiple technologies, algorithms, and components under a\nsingle, unified system, and (b) large scale (big data) requirements. We also\npresent evaluation results of the algorithms on datasets (public for most\ncases) such as an absolute error of 8-9 steps when counting steps, 0.86\nF1-score for detecting visited locations, and an error of less than 12 mins for\ngross sleep time. Finally, we also briefly present studies that have been\nmaterialized using our system, thus demonstrating its potential value to public\nauthorities and individual researchers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.08497,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': 'Addressing Intersectionality, Explainability, and Ethics in AI-Driven\n  Diagnostics: A Rebuttal and Call for Transdiciplinary Action\n\n  The increasing integration of artificial intelligence (AI) into medical\ndiagnostics necessitates a critical examination of its ethical and practical\nimplications. While the prioritization of diagnostic accuracy, as advocated by\nSabuncu et al. (2025), is essential, this approach risks oversimplifying\ncomplex socio-ethical issues, including fairness, privacy, and\nintersectionality. This rebuttal emphasizes the dangers of reducing\nmultifaceted health disparities to quantifiable metrics and advocates for a\nmore transdisciplinary approach. By incorporating insights from social\nsciences, ethics, and public health, AI systems can address the compounded\neffects of intersecting identities and safeguard sensitive data. Additionally,\nexplainability and interpretability must be central to AI design, fostering\ntrust and accountability. This paper calls for a framework that balances\naccuracy with fairness, privacy, and inclusivity to ensure AI-driven\ndiagnostics serve diverse populations equitably and ethically.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.736682891845703e-05, 'GPT4': 0.52001953125, 'CLAUDE': 0.0004367828369140625, 'GOOGLE': 0.01007080078125, 'OPENAI_O_SERIES': 0.0093536376953125, 'DEEPSEEK': 0.459716796875, 'GROK': 5.1140785217285156e-05, 'NOVA': 0.00033092498779296875, 'OTHER': 0.00025177001953125, 'HUMAN': 2.2649765014648438e-06}}"
2501.09479,review,post_llm,2025,1,"{'ai_likelihood': 0.000255637698703342, 'text': ""Connectivity for AI enabled cities -- A field survey based study of\n  emerging economies\n\n  The impact of Artificial Intelligence (AI) is transforming various aspects of\nurban life, including, governance, policy and planning, healthcare,\nsustainability, economics, entrepreneurship, etc. Although AI immense potential\nfor positively impacting urban living, its success depends on overcoming\nsignificant challenges, particularly in telecommunications infrastructure.\nSmart city applications, such as, federated learning, Internet of Things (IoT),\nand online financial services, require reliable Quality of Service (QoS) from\ntelecommunications networks to ensure effective information transfer. However,\nwith over three billion people underserved or lacking access to internet, many\nof these AI-driven applications are at risk of either remaining underutilized\nor failing altogether. Furthermore, many IoT and video-based applications in\ndensely populated urban areas require high-quality connectivity. This paper\nexplores these issues, focusing on the challenges that need to be mitigated to\nmake AI succeed in emerging countries, where more than 80% of the world\npopulation resides and urban migration grows. In this context, an overview of a\ncase study conducted in Kathmandu, Nepal, highlights citizens' aspirations for\naffordable, high-quality internet-based services. The findings underscore the\npressing need for advanced telecommunication networks to meet diverse user\nrequirements while addressing investment and infrastructure gaps. This\ndiscussion provides insights into bridging the digital divide and enabling AI's\ntransformative potential in urban areas.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.19173,regular,post_llm,2025,1,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""Position: Contextual Integrity is Inadequately Applied to Language Models\n\nMachine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development. The CI theory emphasizes sharing information in accordance with privacy norms and can bridge the social, legal, political, and technical aspects essential for evaluating privacy in LLMs. However, this is also a good point to reflect on use of CI for LLMs. This position paper argues that existing literature inadequately applies CI for LLMs without embracing the theory's fundamental tenets.\n  Inadequate applications of CI could lead to incorrect conclusions and flawed privacy-preserving designs. We clarify the four fundamental tenets of CI theory, systematize prior work on whether they deviate from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity, positional bias)."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.0135,regular,post_llm,2025,1,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""Tracking behavioural differences across chronotypes: A case study in\n  Finland using Oura rings\n\n  Non-invasive mobile wearables like fitness trackers, smartwatches and rings\nallow for an easier and relatively less expensive approach to study everyday\nhuman behaviour when compared to traditional longitudinal methods. Here we have\nutilised smart rings manufactured by Oura to obtain granular data from nineteen\nhealthy participants over the time span of one year (October 2023 - September\n2024) along with monthly surveys for nine months to track their subjective\nstress during the study. We have investigated longitudinal sleep and activity\npatterns of three chronotype groups of participating individuals: morning type\n(MT), neither type (NT) and evening type (ET). We find that while ET\nindividuals do not seem to lead as healthy life as the MT or NT individuals in\nterms of overall sleep and activity, they seem to have significantly improved\ntheir habits during the duration of the study. The activity in all chronotype\ngroups varies across the year with ET showing an increasing trend. Furthermore,\nwe also show that the Daylight Saving Time changes affect the MT and ET\nchronotypes, oppositely. Finally, using a mixed-effects regression model, we\nshow that an individual's perceived stress is significantly associated with\ntheir time spent in bed during the night time sleep, monthly survey response\ntime, and chronotype, while accounting for individual variability.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.09401,review,post_llm,2025,1,"{'ai_likelihood': 7.5499216715494794e-06, 'text': 'Towards a Framework for Enterprise Architecture in Mobile Government: A\n  Case Study\n\n  Mobile government (m-government) represents a distinct paradigm shift from\nelectronic government (e-government), offering a new avenue for governments\nworldwide to deliver services and applications to their customers. The\nm-government model deviates from e-government in terms of information\ntechnology (IT) infrastructure, security, and application management and\nimplementation. Enterprise architecture (EA) has been developed and utilized\nglobally to enhance efficiency and information and communication technology\n(ICT) utilization in the public sector through e-government. However, the\napplication of EA within the context of m-government, particularly in\ndeveloping countries, has largely been overlooked by scholars. This study aims\nto address this gap. This study seeks to develop an EA specifically tailored\nfor m-government in a developmental context. Our contribution to the literature\nis the illustration of a proposed EA framework for m-government. The practical\nimplementation of this study is to identify critical considerations when\ndesigning and adopting m-government to avoid redundant investments during the\nintegration of infrastructure and applications from e-government to\nm-government.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.12897,review,post_llm,2025,1,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Discrimination and AI in insurance: what do people find fair? Results from a survey\n\nTwo modern trends in insurance are data-intensive underwriting and behavior-based insurance. Data-intensive underwriting means that insurers analyze more data for estimating the claim cost of a consumer and for determining the premium based on that estimation. Insurers also offer behavior-based insurance. For example, some car insurers use artificial intelligence (AI) to follow the driving behavior of an individual consumer in real-time and decide whether to offer that consumer a discount. In this paper, we report on a survey of the Dutch population (N=999) in which we asked people's opinions about examples of data-intensive underwriting and behavior-based insurance. The main results include: (i) If survey respondents find an insurance practice unfair, they also find the practice unacceptable. (ii) Respondents find almost all modern insurance practices that we described unfair. (iii) Respondents find practices for which they can influence the premium fairer. (iv) If respondents find a certain consumer characteristic illogical for basing the premium on, then respondents find using the characteristic unfair. (v) Respondents find it unfair if an insurer offers an insurance product only to a specific group. (vi) Respondents find it unfair if an insurance practice leads to the poor paying more. We also reflect on the policy implications of the findings."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.09906,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""Position: Open and Closed Large Language Models in Healthcare\n\n  This position paper analyzes the evolving roles of open-source and\nclosed-source large language models (LLMs) in healthcare, emphasizing their\ndistinct contributions and the scientific community's response to their\ndevelopment. Due to their advanced reasoning capabilities, closed LLMs, such as\nGPT-4, have dominated high-performance applications, particularly in medical\nimaging and multimodal diagnostics. Conversely, open LLMs, like Meta's LLaMA,\nhave gained popularity for their adaptability and cost-effectiveness, enabling\nresearchers to fine-tune models for specific domains, such as mental health and\npatient communication.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00026607513427734375, 'GPT4': 0.344482421875, 'CLAUDE': 0.052276611328125, 'GOOGLE': 0.412353515625, 'OPENAI_O_SERIES': 0.022125244140625, 'DEEPSEEK': 0.037017822265625, 'GROK': 0.0012998580932617188, 'NOVA': 0.007007598876953125, 'OTHER': 0.12310791015625, 'HUMAN': 2.9206275939941406e-06}}"
2501.12544,regular,post_llm,2025,1,"{'ai_likelihood': 1.8146302964952258e-05, 'text': 'LEGOS-SLEEC: Tool for Formalizing and Analyzing Normative Requirements\n\n  Systems interacting with humans, such as assistive robots or chatbots, are\nincreasingly integrated into our society. To prevent these systems from causing\nsocial, legal, ethical, empathetic, or cultural (SLEEC) harms, normative\nrequirements specify the permissible range of their behaviors. These\nrequirements encompass both functional and non-functional aspects and are\ndefined with respect to time. Typically, these requirements are specified by\nstakeholders from a broad range of fields, such as lawyers, ethicists, or\nphilosophers, who may lack technical expertise. Because such stakeholders often\nhave different goals, responsibilities, and objectives, ensuring that these\nrequirements are well-formed is crucial. SLEEC DSL, a domain-specific language\nresembling natural language, has been developed to formalize these requirements\nas SLEEC rules. In this paper, we present LEGOS-SLEEC, a tool designed to\nsupport interdisciplinary stakeholders in specifying normative requirements as\nSLEEC rules, and in analyzing and debugging their well-formedness. LEGOS-SLEEC\nis built using four previously published components, which have been shown to\nbe effective and usable across nine case studies. Reflecting on this\nexperience, we have significantly improved the user interface of LEGOS-SLEEC\nand its diagnostic support, and demonstrate the effectiveness of these\nimprovements using four interdisciplinary stakeholders. Showcase video URL is:\nhttps://youtu.be/LLaBLGxSi8A\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.18038,review,post_llm,2025,1,"{'ai_likelihood': 1.457002427842882e-06, 'text': 'A Case Study in Acceleration AI Ethics: The TELUS GenAI Conversational\n  Agent\n\n  Acceleration ethics addresses the tension between innovation and safety in\nartificial intelligence. The acceleration argument is that risks raised by\ninnovation should be answered with still more innovating. This paper summarizes\nthe theoretical position, and then shows how acceleration ethics works in a\nreal case. To begin, the paper summarizes acceleration ethics as composed of\nfive elements: innovation solves innovation problems, innovation is\nintrinsically valuable, the unknown is encouraging, governance is\ndecentralized, ethics is embedded. Subsequently, the paper illustrates the\nacceleration framework with a use-case, a generative artificial intelligence\nlanguage tool developed by the Canadian telecommunications company Telus. While\nthe purity of theoretical positions is blurred by real-world ambiguities, the\nTelus experience indicates that acceleration AI ethics is a way of maximizing\nsocial responsibility through innovation, as opposed to sacrificing social\nresponsibility for innovation, or sacrificing innovation for social\nresponsibility.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.03111,regular,post_llm,2025,1,"{'ai_likelihood': 0.00012934207916259766, 'text': ""Assessing the impact of external factors on the occurrence of emergencies\n\nThis study investigates the impact of 19 external factors, related to weather, road traffic conditions, air quality, and time, on the occurrence of emergencies using historical data provided by the dispatch center of the Centre Hospitalier Universitaire Vaudois (CHUV). This center is responsible for managing Emergency Medical Service (EMS) resources in the majority of the French-speaking part of Switzerland. First, classical statistical methods, such as correlation, Chi-squared test, Student's $t$-test, and information value, are employed to identify dependencies between the occurrence of emergencies and the considered parameters. Additionally, SHapley Additive exPlanations (SHAP) values and permutation importance are computed using eXtreme Gradient Boosting (XGBoost) and Multilayer Perceptron (MLP) models. The results indicate that the hour of the day, along with correlated parameters, plays a crucial role in the occurrence of emergencies. Conversely, other factors do not significantly influence emergency occurrences. Subsequently, a simplified model that considers only the hour of the day is compared with our XGBoost and MLP models. These comparisons reveal no significant difference between the three models in terms of performance, supporting the use of the basic model in this context. These observations provide valuable insights for EMS resource relocation strategies, benefit predictive modeling efforts, and inform decision-making in the context of EMS. The implications extend to enhancing EMS quality, making this research essential."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10715,regular,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""Enhancing Citizen-Government Communication with AI: Evaluating the Impact of AI-Assisted Interactions on Communication Quality and Satisfaction\n\nAs governments worldwide increasingly adopt digital tools to enhance citizen engagement and service delivery, the integration of Artificial Intelligence (AI) emerges as a pivotal advancement in public administration. This study examines the impact of AI-assisted interactions on the quality of communication between citizens and civil servants, focusing on key dimensions such as Satisfaction, Politeness, Ease of Understanding, Feeling Heard, Trust, and Empathy from the citizens' perspective, and Clarity, Politeness, Responsiveness, Respect, Urgency, and Empathy from the civil servants' perspective. Utilizing a questionnaire-based experimental design, the research involved citizens and civil servants who evaluated both original and AI-modified communication samples across five interaction types: Service Requests, Policy Inquiries, Complaints, Suggestions, and Emergency Concerns. Statistical analyses revealed that AI modifications significantly enhanced most communication dimensions for both citizens and civil servants. Specifically, AI-assisted responses led to higher satisfaction, politeness, clarity, and trust among citizens, while also improving clarity, politeness, responsiveness, and respect among civil servants. However, AI interventions showed mixed effects on empathy and urgency from the civil servants' perspective, indicating areas for further refinement. The findings suggest that AI has substantial potential to improve citizen-government interactions, fostering more effective and satisfying communication, while also highlighting the need for continued development to address emotional and urgent communication nuances."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.8014183044433594e-06, 'GPT4': 0.01393890380859375, 'CLAUDE': 0.00449371337890625, 'GOOGLE': 0.0024433135986328125, 'OPENAI_O_SERIES': 0.97509765625, 'DEEPSEEK': 0.0013103485107421875, 'GROK': 5.602836608886719e-06, 'NOVA': 0.00013744831085205078, 'OTHER': 0.0023956298828125, 'HUMAN': 1.1920928955078125e-07}}"
2501.12642,review,post_llm,2025,1,"{'ai_likelihood': 2.384185791015625e-06, 'text': ""Training Data Attribution (TDA): Examining Its Adoption & Use Cases\n\n  This report investigates Training Data Attribution (TDA) and its potential\nimportance to and tractability for reducing extreme risks from AI. First, we\ndiscuss the plausibility and amount of effort it would take to bring existing\nTDA research efforts from their current state, to an efficient and accurate\ntool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss\nthe numerous research benefits AI labs will expect to see from using such TDA\ntooling. Then, we discuss a key outstanding bottleneck that would limit such\nTDA tooling from being accessible publicly: AI labs' willingness to disclose\ntheir training data. We suggest ways AI labs may work around these limitations,\nand discuss the willingness of governments to mandate such access. Assuming\nthat AI labs willingly provide access to TDA inference, we then discuss what\nhigh-level societal benefits you might see. We list and discuss a series of\npolicies and systems that may be enabled by TDA. Finally, we present an\nevaluation of TDA's potential impact on mitigating large-scale risks from AI\nsystems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.10397,regular,post_llm,2025,1,"{'ai_likelihood': 0.94873046875, 'text': 'Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities\n\nThe Metaverse represents a transformative shift beyond traditional mobile Internet, creating an immersive, persistent digital ecosystem where users can interact, socialize, and work within 3D virtual environments. Powered by large models such as ChatGPT and Sora, the Metaverse benefits from precise large-scale real-world modeling, automated multimodal content generation, realistic avatars, and seamless natural language understanding, which enhance user engagement and enable more personalized, intuitive interactions. However, challenges remain, including limited scalability, constrained responsiveness, and low adaptability in dynamic environments. This paper investigates the integration of large models within the Metaverse, examining their roles in enhancing user interaction, perception, content creation, and service quality. To address existing challenges, we propose a generative AI-based framework for optimizing Metaverse rendering. This framework includes a cloud-edge-end collaborative model to allocate rendering tasks with minimal latency, a mobility-aware pre-rendering mechanism that dynamically adjusts to user movement, and a diffusion model-based adaptive rendering strategy to fine-tune visual details. Experimental results demonstrate the effectiveness of our approach in enhancing rendering efficiency and reducing rendering overheads, advancing large model deployment for a more responsive and immersive Metaverse.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00010073184967041016, 'GPT4': 0.9287109375, 'CLAUDE': 0.003101348876953125, 'GOOGLE': 0.00925445556640625, 'OPENAI_O_SERIES': 0.037872314453125, 'DEEPSEEK': 0.0164794921875, 'GROK': 8.52346420288086e-06, 'NOVA': 1.3649463653564453e-05, 'OTHER': 6.431341171264648e-05, 'HUMAN': 0.00437164306640625}}"
2501.16954,regular,post_llm,2025,1,"{'ai_likelihood': 0.015979342990451388, 'text': 'The Third Moment of AI Ethics: Developing Relatable and Contextualized\n  Tools\n\n  Artificial intelligence (AI) ethics has gained significant momentum,\nevidenced by the growing body of published literature, policy guidelines, and\npublic discourse. However, the practical implementation and adoption of AI\nethics principles among practitioners has not kept pace with this theoretical\ndevelopment. Common barriers to adoption include overly abstract language, poor\naccessibility, and insufficient practical guidance for implementation. Through\nparticipatory design with industry practitioners, we developed an open-source\ntool that bridges this gap. Our tool is firmly grounded in normative ethical\nframeworks while offering concrete, actionable guidance in an intuitive format\nthat aligns with established software development workflows. We validated this\napproach through a proof of concept study in the United States autonomous\ndriving industry.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.18986,review,post_llm,2025,1,"{'ai_likelihood': 3.046459621853299e-06, 'text': 'A Systematic Literature Review of Computer Science MOOCs for K-12\n  education\n\n  Computer science (CS) is increasingly becoming part of the curricula of K-12\neducation in different countries. However, there are few K-12 CS teachers, and\ntools to offer K-12 CS education are often limited. Massive Open Online Courses\n(MOOCs) might help to temporarily address these challenges, and enable more\nschools to offer CS education. The goal of this systematic review is to give an\noverview of how CS MOOCs have been used in K-12 education. Nineteen papers from\n2014 to May 2024 were included, describing thirteen different MOOCs. This\nreview summarizes the research performed with these MOOCs and discusses\ndirections for future research. Our findings show that most CS MOOCs target\nonly part of the CS curriculum. When using a MOOC, a classroom teacher has an\nimportant role in supporting and managing students as they work in the MOOC.\nResearch evaluating MOOCs is diverse, both in aims and in methods. In\nconclusion, MOOCs can play a valuable role in K-12 CS education, although\nadditional teacher training to support students might be required. Moreover,\nadditional learning material is needed to cover the full curriculum, as most\nMOOCs focus on programming and computational thinking.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.19423,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""Are Large Language Models Ready for Business Integration? A Study on\n  Generative AI Adoption\n\n  The explorations and applications of Artificial Intelligence (AI) in various\ndomains becomes increasingly vital as it continues to evolve. While much\nattention has been focused on Large Language Models (LLMs) such as ChatGPT,\nthis research examines the readiness of other LLMs such as Google Gemini\n(previously Google BARD), a conversational AI chatbot, for potential business\napplications. Gemini is an example of a Generative AI (Gen AI) that\ndemonstrates capabilities encompassing content generation, language\ntranslation, and information retrieval. This study aims to assess its efficacy\nfor text simplification in catering to the demands of modern businesses. A\ndataset of 42,654 reviews from distinct Disneyland branches was employed. The\nchatbot's API was utilised with a uniform prompt to generate simplified\nre-views. Results presented a spectrum of responses, including 75% successful\nsimplifications, 25% errors, and instances of model self-reference.\nQuantitative analysis encompassing response categorisation, error prevalence,\nand response length distribution was conducted. Furthermore, Natural Language\nProcessing (NLP) metrics were applied to gauge the quality of the generated\ncontent with the original reviews. The findings offer insights into Gen AI\nmodels performance, highlighting proficiency in simplifying re-views while\nunveiling certain limitations in coherence and consistency since only about\n7.79% of the datasets was simplified. This research contributes to the ongoing\ndiscourse on AI adoption in business contexts. The study's out-comes provide\nimplications for future development and implementation of AI-driven tools in\nbusinesses seeking to enhance content creation and communication processes. As\nAI continues to transform industries, an understanding of the readiness and\nlimitations of AI models is essential for informed decision-making, automations\nand effective integration.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00013697147369384766, 'GPT4': 0.2308349609375, 'CLAUDE': 8.463859558105469e-06, 'GOOGLE': 0.7685546875, 'OPENAI_O_SERIES': 0.00023543834686279297, 'DEEPSEEK': 2.384185791015625e-07, 'GROK': 0.0, 'NOVA': 4.76837158203125e-07, 'OTHER': 4.845857620239258e-05, 'HUMAN': 3.3974647521972656e-06}}"
2502.19424,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': 'Understanding the Disparities in Mathematics Performance: An\n  Interpretability-Based Examination\n\n  Problem. Educational disparities in Mathematics performance are a persistent\nchallenge. This study aims to unravel the complex factors contributing to these\ndisparities among students internationally, with a focus on the\ninterpretability of the contributing factors. Methodology. Utilizing data from\nthe Programme for International Student Assessment (PISA), we conducted\nrigorous preprocessing and variable selection to prepare for applying binary\nclassification interpretability models. These models were trained using the\nStratified K-Fold technique to ensure balanced representation and assessed\nusing six key metrics. Solution. By applying interpretability models such as\nShapley Additive Explanations (SHAP) analysis, we identified critical factors\nimpacting student performance, including reading accessibility, critical\nthinking skills, gender, and geographical location. Results. Our findings\nreveal significant disparities linked to resource availability, with students\nfrom lower socioeconomic backgrounds possessing fewer books and demonstrating\nlower performance in Mathematics. The geographical analysis highlighted\nregional educational disparities, with certain areas consistently\nunderperforming in PISA assessments. Gender also emerged as a determinant, with\nfemales contributing differently to performance levels across the spectrum.\nConclusion. The study provides insights into the multifaceted determinants of\nstudent Mathematics performance and suggests potential avenues for future\nresearch to explore global interpretability models and further investigate the\nsocioeconomic, cultural, and educational factors at play.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0005974769592285156, 'GPT4': 0.55810546875, 'CLAUDE': 0.00010198354721069336, 'GOOGLE': 0.434814453125, 'OPENAI_O_SERIES': 0.006237030029296875, 'DEEPSEEK': 2.86102294921875e-06, 'GROK': 1.6689300537109375e-06, 'NOVA': 8.881092071533203e-06, 'OTHER': 0.00010627508163452148, 'HUMAN': 2.562999725341797e-06}}"
2502.104,regular,post_llm,2025,1,"{'ai_likelihood': 6.225374009874132e-06, 'text': 'Introducing Computational Thinking in Calculus for Engineering\n\n  Technology is currently ubiquitous and is also part of the educational system\nat all levels. It started with communication technology systems, and later\ncontinued with digital competence. Nowadays, although these previous concepts\nare still in force and are useful for students and workers in general, a new\nconcept has been born that can function as a cross-curricular competence called\nComputational Thinking. There is currently no consensus on the definition of\ncomputational thinking, nor on the classification of its skills, but there is a\nconsensus that it refers to a set of skills necessary for the formulation and\nresolution of problems. The study of Computational Thinking has been very\ninfluential in recent years in research on teaching and learning processes,\nwhich has led educational institutions to begin to address these issues during\ntraining. In this paper, we try to introduce this new cross-curricular\ncompetence and expose a project of implementation of Computational Thinking in\nengineering careers through Calculus subject.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10601,regular,post_llm,2025,1,"{'ai_likelihood': 3.973642985026042e-06, 'text': ""Understanding Computational Science and Engineering (CSE) and Domain Science Skills Development in National Laboratory Postgraduate Internships\n\nBackground: Harnessing advanced computing for scientific discovery and technological innovation demands scientists and engineers well-versed in both domain science and computational science and engineering (CSE). However, few universities provide access to both integrated domain science/CSE cross-training and Top-500 High-Performance Computing (HPC) facilities. National laboratories offer internship opportunities capable of developing these skills. Purpose: This student presents an evaluation of federally-funded postgraduate internship outcomes at a national laboratory. This study seeks to answer three questions: 1) What computational skills, research skills, and professional skills do students improve through internships at the selected national laboratory. 2) Do students gain knowledge in domain science topics through their internships. 3) Do students' career interests change after these internships? Design/Method: We developed a survey and collected responses from past participants of five federally-funded internship programs and compare participant ratings of their prior experience to their internship experience. Findings: Our results indicate that participants improve CSE skills and domain science knowledge, and are more interested in working at national labs. Participants go on to degree programs and positions in relevant domain science topics after their internships. Conclusions: We show that national laboratory internships are an opportunity for students to build CSE skills that may not be available at all institutions. We also show a growth in domain science skills during their internships through direct exposure to research topics. The survey instrument and approach used may be adapted to other studies to measure the impact of postgraduate internships in multiple disciplines and internship settings."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.16946,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""Gradual Disempowerment: Systemic Existential Risks from Incremental AI\n  Development\n\n  This paper examines the systemic risks posed by incremental advancements in\nartificial intelligence, developing the concept of `gradual disempowerment', in\ncontrast to the abrupt takeover scenarios commonly discussed in AI safety. We\nanalyze how even incremental improvements in AI capabilities can undermine\nhuman influence over large-scale systems that society depends on, including the\neconomy, culture, and nation-states. As AI increasingly replaces human labor\nand cognition in these domains, it can weaken both explicit human control\nmechanisms (like voting and consumer choice) and the implicit alignments with\nhuman interests that often arise from societal systems' reliance on human\nparticipation to function. Furthermore, to the extent that these systems\nincentivise outcomes that do not line up with human preferences, AIs may\noptimize for those outcomes more aggressively. These effects may be mutually\nreinforcing across different domains: economic power shapes cultural narratives\nand political decisions, while cultural shifts alter economic and political\nbehavior. We argue that this dynamic could lead to an effectively irreversible\nloss of human influence over crucial societal systems, precipitating an\nexistential catastrophe through the permanent disempowerment of humanity. This\nsuggests the need for both technical research and governance approaches that\nspecifically address the risk of incremental erosion of human influence across\ninterconnected societal systems.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.230571746826172e-05, 'GPT4': 0.00118255615234375, 'CLAUDE': 0.96484375, 'GOOGLE': 0.006214141845703125, 'OPENAI_O_SERIES': 0.0006651878356933594, 'DEEPSEEK': 0.0262603759765625, 'GROK': 2.6226043701171875e-06, 'NOVA': 9.5367431640625e-07, 'OTHER': 9.894371032714844e-06, 'HUMAN': 0.0010242462158203125}}"
2502.08648,review,post_llm,2025,1,"{'ai_likelihood': 8.874469333224826e-06, 'text': ""Scientific Map of Artificial Intelligence in Communication (2004-2024)\n\n  Introduction: Artificial Intelligence (AI) is having a significant impact in\nthe field of communication, causing transcendental changes in the processing\nand consumption of information. The objective of this work was to analyze the\nmost influential AI topic areas in the field of communication based on\nscientific literature. Methodology: 996 references indexed in Web of Science\nbetween 2004-2024 were selected, a bibliometric analysis of co-words was\ncarried out and visualization techniques were applied to build scientific maps.\nResults: The most relevant thematic areas were datafication, the linking of AI\nwith social media and digital journalism. The emerging area of generative AI\nwas identified, linked to new AI models, such as ChatGPT, designed to generate\ncontent in the form of written text, audio, images or videos. Another emerging\ntopic area was China's impact on the use of AI in communication. Discussions:\nDespite the impact of AI in communication, the field is still in the process of\nstructuring, with few consolidated topics. Conclusions: This study made it\npossible to identify the thematic areas of the field studied, as well as the\ndetection of emerging trends.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.13235,review,post_llm,2025,1,"{'ai_likelihood': 0.11847601996527778, 'text': ""On the development of open geographical data infrastructures in Latin America: progress and challenges\n\nOpen data initiatives and infrastructures play an essential role in favoring better data access, participation, and transparency in government operations and decision-making. Open Geographical Data Infrastructures (OGDIs) allow citizens to access and scrutinize government and public data, thereby enhancing accountability and evidence-based decision-making. This encourages citizen engagement and participation in public affairs and offers researchers, non-governmental organizations, civil society, and business sectors novel opportunities to analyze and disseminate large amounts of geographical data and to address social, urban, and environmental challenges. In Latin America, while recent open government agendas have shown an inclination towards transparency, citizen participation, and collaboration, only a limited number of OGDIs allow unrestricted use and re-use of their data. Given the region's cultural, social, and economic disparities, there is a contrasting digital divide that significantly impacts how OGDIs are being developed. Therefore, this paper analyses recent progress in developing OGDIs in Latin America, technological gaps, and open geographical data initiatives. The main results denote an early development of OGDIs in the region. Nevertheless, this opens the door for the timely involvement of citizens and non-government sectors to share needs, experiences, knowledge, and expertise, as well as to address a transboundary research agenda. Challenges are discussed from multiple perspectives: data, methodological, governmental and readiness, and potential impact. This analysis is aimed at researchers, policymakers, and practitioners interested in the specific challenges and progress of OGDIs in Latin America, while also contributing to the global conversation on best practices and lessons learned in implementing OGDIs across different contexts."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10596,regular,post_llm,2025,1,"{'ai_likelihood': 2.7815500895182294e-06, 'text': ""Evaluating Amazon Effects and the Limited Impact of COVID-19 With\n  Purchases Crowdsourced from US Consumers\n\n  We leverage a recently published dataset of Amazon purchase histories,\ncrowdsourced from thousands of US consumers, to study how online purchasing\nbehaviors have changed over time, how changes vary across demographic groups,\nthe impact of the COVID-19 pandemic, and relationships between online and\noffline retail. This work provides a case study in how consumer-level purchases\ndata can reveal purchasing behaviors and trends beyond those available from\naggregate metrics. For example, in addition to analyzing spending behavior, we\ndevelop new metrics to quantify changes in consumers' online purchase frequency\nand the diversity of products purchased, to better reflect the growing ubiquity\nand dominance of online retail. Between 2018 and 2022 these consumer-level\nmetrics grew on average by more than 85%, peaking in 2021. We find a steady\nupward trend in individuals' online purchasing prior to COVID-19, with a\nsignificant increase in the first year of COVID, but without a lasting effect.\nPurchasing behaviors in 2022 were no greater than the result of the\npre-pandemic trend. We also find changes in purchasing significantly differ by\ndemographics, with different responses to the pandemic. We further use the\nconsumer-level data to show substitution effects between online and offline\nretail in sectors where Amazon heavily invested: books, shoes, and grocery.\nPrior to COVID we find year-to-year changes in the number of consumers making\nonline purchases for books and shoes negatively correlated with changes in\nemployment at local bookstores and shoe stores. During COVID we find online\ngrocery purchasing negatively correlated with in-store grocery visits. This\nwork demonstrates how crowdsourced, open purchases data can enable economic\ninsights that may otherwise only be available to private firms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.03946,regular,post_llm,2025,1,"{'ai_likelihood': 4.503462049696181e-06, 'text': 'Proxy Discrimination After Students for Fair Admissions\n\n  Today, there is no clear legal test for regulating the use of variables that\nproxy for race and other protected classes and classifications. This Article\ndevelops such a test. Decision tools that use proxies are narrowly tailored\nwhen they exhibit the weakest total proxy power. The test is necessarily\ncomparative. Thus, if two algorithms predict loan repayment or university\nacademic performance with identical accuracy rates, but one uses zip code and\nthe other does not, then the second algorithm can be said to have deployed a\nmore equitable means for achieving the same result as the first algorithm.\nScenarios in which two algorithms produce comparable and non-identical results\npresent a greater challenge. This Article suggests that lawmakers can develop\ncaps to permissible proxy power over time, as courts and algorithm builders\nlearn more about the power of variables. Finally, the Article considers who\nshould bear the burden of producing less discriminatory alternatives and\nsuggests plaintiffs remain in the best position to keep defendants honest - so\nlong as testing data is made available.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00014,review,post_llm,2025,1,"{'ai_likelihood': 4.337893591986763e-06, 'text': 'Algorithmic Bias and the New Chicago School\n\n  AI systems are increasingly deployed in both public and private sectors to\nindependently make complicated decisions with far-reaching impact on\nindividuals and the society. However, many AI algorithms are biased in the\ncollection or processing of data, resulting in prejudiced decisions based on\ndemographic features. Algorithmic biases occur because of the training data fed\ninto the AI system or the design of algorithmic models. While most legal\nscholars propose a direct-regulation approach associated with the right of\nexplanation or transparency obligation, this article provides a different\npicture regarding how indirect regulation can be used to regulate algorithmic\nbias based on the New Chicago School framework developed by Lawrence Lessig.\nThis article concludes that an effective regulatory approach toward algorithmic\nbias will be the right mixture of direct and indirect regulations through\narchitecture, norms, market, and the law.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.08473,regular,post_llm,2025,1,"{'ai_likelihood': 0.00011821587880452475, 'text': ""The Impact of AI-Driven Tools on Student Writing Development: A Case\n  Study From The CGScholar AI Helper Project\n\n  The case study examines the impact of the CGScholar (Common Ground Scholar)\nAI Helper on a pilot research initiative involving the writing development of\n11th-grade students in English Language Arts (ELA). CGScholar AI Helper is an\nevolving and innovative web-based application designed to support students in\ntheir writing tasks by providing specified AI-generated feedback. This study is\none of six interventions. It involved one teacher and six students in a diverse\nschool with low income students and explored to what extent customized\nAI-driven feedback can support students' writing development. The findings\nsuggest that the implementation of AI Helper supported the development of\nstudents' writing in a number of ways. It also elicited suggestions from the\nteacher and students about ways of improving the still in development tool.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.03097,regular,post_llm,2025,1,"{'ai_likelihood': 0.00015907817416720922, 'text': ""Self-directed online information search can affect policy support: a\n  randomized encouragement design with digital behavioral data\n\n  The abundance of information sources in our digital environment makes it\ndifficult to study how such information shapes individuals' support for current\npolicies. Our study with 791 German participants investigates self-directed\nonline search in a naturalistic setting through three randomized controlled\nexperiments on three topical policy issues: basic child support, renewable\nenergy transition, and cannabis legalization. Participants' online browsing was\npassively tracked. Significant attitude shifts were observed for child support\nand cannabis legalization, but not for renewable energy transition. By\nencouraging participants to seek online information, this study enhances\necological validity compared to traditional experiments that expose subjects to\npredetermined content. Our experimental approach lays the groundwork for future\nresearch to advance understanding of media effects within the dynamic online\ninformation landscape.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.03098,regular,post_llm,2025,1,"{'ai_likelihood': 2.132521735297309e-05, 'text': ""Early Perspectives on the Digital Europe Programme\n\n  A new Digital Europe Programme (DEP), a funding instrument for development\nand innovation, was established in the European Union (EU) in 2021. The paper\nmakes an empirical inquiry into the projects funded through the DEP. According\nto the results, the projects align well with the DEP's strategic focus on cyber\nsecurity, artificial intelligence, high-performance computing, innovation hubs,\nsmall- and medium-sized enterprises, and education. Most of the projects have\nreceived an equal amount of national and EU funding. Although national origins\nof participating organizations do not explain the amounts of funding granted,\nthere is a rather strong tendency for national organizations to primarily\ncollaborate with other national organizations. Finally, information about the\ntechnological domains addressed and the economic sectors involved provides\ndecent explanatory power for statistically explaining the funding amounts\ngranted. With these results and the accompanying discussion, the paper\ncontributes to the timely debate about innovation, technology development, and\nindustrial policy in Europe.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.16531,review,post_llm,2025,1,"{'ai_likelihood': 2.7815500895182293e-05, 'text': 'Responsible Generative AI Use by Product Managers: Recoupling Ethical\n  Principles and Practices\n\n  Since 2022, generative AI (genAI) has rapidly become integrated into\nworkplaces. Though organizations have made commitments to use this technology\n""responsibly"", how organizations and their employees prioritize responsibility\nin their decision-making remains absent from extant management theorizing. In\nthis paper, we examine how product managers - who often serve as gatekeepers in\ndecision-making processes - implement responsible practices in their day-to-day\nwork when using genAI. Using Institutional Theory, we illuminate the factors\nthat constrain or support proactive responsible development and usage of genAI\ntechnologies. We employ a mixed methods research design, drawing on 25\ninterviews with product managers and a global survey of 300 respondents in\nproduct management-related roles. The majority of our respondents report (1)\nwidespread uncertainty regarding what ""responsibility"" means or looks like, (2)\ndiffused responsibility given assumed ethical actions by other teams, (3) lack\nof clear incentives and guidance within organizations, and (4) the importance\nof leadership buy-in and principles for navigating tensions between ethical\ncommitments and profit motives. However, our study finds that even in highly\nuncertain environments, absent guidance from leadership, product managers can\n""recouple"" ethical commitments and practices by finding responsibility\n""micro-moments"". Product managers seek out low-risk, small-scale actions they\ncan take without explicit buy-in from higher-level managers, such as individual\nor team-wide checks and reviews and safeguarding standards for data. Our\nresearch highlights how genAI poses unique challenges to organizations trying\nto couple ethical principles and daily practices and the role that middle-level\nmanagement can play in recoupling the two.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00012,review,post_llm,2025,1,"{'ai_likelihood': 4.265043470594618e-05, 'text': 'Lessons from complexity theory for AI governance\n\n  The study of complex adaptive systems, pioneered in physics, biology, and the\nsocial sciences, offers important lessons for AI governance. Contemporary AI\nsystems and the environments in which they operate exhibit many of the\nproperties characteristic of complex systems, including nonlinear growth\npatterns, emergent phenomena, and cascading effects that can lead to tail\nrisks. Complexity theory can help illuminate the features of AI that pose\ncentral challenges for policymakers, such as feedback loops induced by training\nAI models on synthetic data and the interconnectedness between AI systems and\ncritical infrastructure. Drawing on insights from other domains shaped by\ncomplex systems, including public health and climate change, we examine how\nefforts to govern AI are marked by deep uncertainty. To contend with this\nchallenge, we propose a set of complexity-compatible principles concerning the\ntiming and structure of AI governance, and the risk thresholds that should\ntrigger regulatory intervention.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.16461,review,post_llm,2025,1,"{'ai_likelihood': 3.7716494666205515e-05, 'text': 'Trustworthiness in Stochastic Systems: Towards Opening the Black Box\n\n  AI systems are increasingly tasked to complete responsibilities with\ndecreasing oversight. This delegation requires users to accept certain risks,\ntypically mitigated by perceived or actual alignment of values between humans\nand AI, leading to confidence that the system will act as intended. However,\nstochastic behavior by an AI system threatens to undermine alignment and\npotential trust. In this work, we take a philosophical perspective to the\ntension and potential conflict between stochasticity and trustworthiness. We\ndemonstrate how stochasticity complicates traditional methods of establishing\ntrust and evaluate two extant approaches to managing it: (1) eliminating\nuser-facing stochasticity to create deterministic experiences, and (2) allowing\nusers to independently control tolerances for stochasticity. We argue that both\napproaches are insufficient, as not all forms of stochasticity affect\ntrustworthiness in the same way or to the same degree. Instead, we introduce a\nnovel definition of stochasticity and propose latent value modeling for both AI\nsystems and users to better assess alignment. This work lays a foundational\nstep toward understanding how and when stochasticity impacts trustworthiness,\nenabling more precise trust calibration in complex AI systems, and underscoring\nthe importance of sociotechnical analyses to effectively address these\nchallenges.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.13802,regular,post_llm,2025,1,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Enhancing LLMs for Governance with Human Oversight: Evaluating and\n  Aligning LLMs on Expert Classification of Climate Misinformation for\n  Detecting False or Misleading Claims about Climate Change\n\n  Climate misinformation is a problem that has the potential to be\nsubstantially aggravated by the development of Large Language Models (LLMs). In\nthis study we evaluate the potential for LLMs to be part of the solution for\nmitigating online dis/misinformation rather than the problem. Employing a\npublic expert annotated dataset and a curated sample of social media content we\nevaluate the performance of proprietary vs. open source LLMs on climate\nmisinformation classification task, comparing them to existing climate-focused\ncomputer-assisted tools and expert assessments. Results show (1) open-source\nmodels substantially under-perform in classifying climate misinformation\ncompared to proprietary models, (2) existing climate-focused computer-assisted\ntools leveraging expert-annotated datasets continues to outperform many of\nproprietary models, including GPT-4o, and (3) demonstrate the efficacy and\ngeneralizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in\nclassifying claims about climate change at the equivalency of climate change\nexperts with over 20 years of experience in climate communication. These\nfindings highlight 1) the importance of incorporating human-oversight, such as\nincorporating expert-annotated datasets in training LLMs, for governance tasks\nthat require subject-matter expertise like classifying climate misinformation,\nand 2) the potential for LLMs in facilitating civil society organizations to\nengage in various governance tasks such as classifying false or misleading\nclaims in domains beyond climate change such as politics and health science.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.02418,review,post_llm,2025,1,"{'ai_likelihood': 1.3377931382921008e-05, 'text': 'Experiences and attitudes toward working remotely from home in a time of\n  pandemic: A snapshot from a New Zealand-based online survey\n\n  Due to the Covid-19 pandemic, employees from around the world were compelled\nto work remotely from home and, in many cases, without much preparation. A\nsubstantial body of international research has been conducted on the\nexperiences and attitudes of remote workers as well as the implications of this\nphenomenon for organisations. While New Zealand research evidence is growing,\nmost existing inquiry is qualitative. This paper provides a quantitative\nsnapshot of remote working using survey data from participants whose jobs can\nbe done from home (n=415). Data collection took place when the country was\nfacing Covid-related measures.\n  Based on descriptive and inferential statistics, it was found that, not only\nwas remote working common, but that hybrid working arrangements were also more\nprevalent. While half of the participants wanted to work from home more\nfrequently, age, but not gender, was significantly associated with this\npreference. Another relevant finding is that perceived change in the workplace\nculture due to flexible work arrangements was significantly associated with\npreference for working remotely more often. Finally, the most common perceived\nbarriers to working from home were slow internet speed, the need to attend\nface-to-face meetings, and limited space at home to work. The implications of\nthe results are discussed and some directions for future research are proposed.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.02321,regular,post_llm,2025,1,"{'ai_likelihood': 5.993578169080946e-06, 'text': 'KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe\n  and 3D to 1D Knowledge Distillation\n\n  Artificial intelligence has achieved notable results in sign language\nrecognition and translation. However, relatively few efforts have been made to\nsignificantly improve the quality of life for the 72 million hearing-impaired\npeople worldwide. Sign language translation models, relying on video inputs,\ninvolves with large parameter sizes, making it time-consuming and\ncomputationally intensive to be deployed. This directly contributes to the\nscarcity of human-centered technology in this field. Additionally, the lack of\ndatasets in sign language translation hampers research progress in this area.\nTo address these, we first propose a cross-modal multi-knowledge distillation\ntechnique from 3D to 1D and a novel end-to-end pre-training text correction\nframework. Compared to other pre-trained models, our framework achieves\nsignificant advancements in correcting text output errors. Our model achieves a\ndecrease in Word Error Rate (WER) of at least 1.4% on PHOENIX14 and PHOENIX14T\ndatasets compared to the state-of-the-art CorrNet. Additionally, the TensorFlow\nLite (TFLite) quantized model size is reduced to 12.93 MB, making it the\nsmallest, fastest, and most accurate model to date. We have also collected and\nreleased extensive Chinese sign language datasets, and developed a specialized\ntraining vocabulary. To address the lack of research on data augmentation for\nlandmark data, we have designed comparative experiments on various augmentation\nmethods. Moreover, we performed a simulated deployment and prediction of our\nmodel on Intel platform CPUs and assessed the feasibility of deploying the\nmodel on other platforms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.0865,review,post_llm,2025,1,"{'ai_likelihood': 1.483493381076389e-05, 'text': 'Who is Responsible? The Data, Models, Users or Regulations? A Comprehensive Survey on Responsible Generative AI for a Sustainable Future\n\nGenerative AI is moving rapidly from research into real world deployment across sectors, which elevates the need for responsible development, deployment, evaluation, and governance. To address this pressing challenge, in this study, we synthesize the landscape of responsible generative AI across methods, benchmarks, and policies, and connects governance expectations to concrete engineering practice. We follow a prespecified search and screening protocol focused on post-ChatGPT era with selective inclusion of foundational work for definitions, and we conduct a narrative and thematic synthesis. Three findings emerge; First, benchmark and practice coverage is dense for bias and toxicity but relatively sparse for privacy and provenance, deepfake and media integrity risk, and system level failure in tool using and agentic settings. Second, many evaluations remain static and task local, which limits evidence portability for audit and lifecycle assurance. Third, documentation and metric validity are inconsistent, which complicates comparison across releases and domains. We outline a research and practice agenda that prioritizes adaptive and multimodal evaluation, privacy and provenance testing, deepfake risk assessment, calibration and uncertainty reporting, versioned and documented artifacts, and continuous monitoring. Limitations include reliance on public artifacts and the focus period, which may under represent capabilities reported later. The survey offers a path to align development and evaluation with governance needs and to support safe, transparent, and accountable deployment across domains. Project page: https://anas-zafar.github.io/responsible-ai.github.io , GitHub: https://github.com/anas-zafar/Responsible-AI', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.03472,regular,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': 'Powering LLM Regulation through Data: Bridging the Gap from Compute\n  Thresholds to Customer Experiences\n\n  The rapid advancement of Large Language Models (LLMs) has created a critical\ngap in consumer protection due to the lack of standardized certification\nprocesses for LLM-powered Artificial Intelligence (AI) systems. This paper\nargues that current regulatory approaches, which focus on compute-level\nthresholds and generalized model evaluations, are insufficient to ensure the\nsafety and effectiveness of specific LLM-based user experiences. We propose a\nshift towards a certification process centered on actual user-facing\nexperiences and the curation of high-quality datasets for evaluation. This\napproach offers several benefits: it drives consumer confidence in AI system\nperformance, enables businesses to demonstrate the credibility of their\nproducts, and allows regulators to focus on direct consumer protection. The\npaper outlines a potential certification workflow, emphasizing the importance\nof domain-specific datasets and expert evaluation. By repositioning data as the\nstrategic center of regulatory efforts, this framework aims to address the\nchallenges posed by the probabilistic nature of AI systems and the rapid pace\nof technological advancement. This shift in regulatory focus has the potential\nto foster innovation while ensuring responsible AI development, ultimately\nbenefiting consumers, businesses, and government entities alike.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00122833251953125, 'GPT4': 0.0019073486328125, 'CLAUDE': 0.0224151611328125, 'GOOGLE': 0.97021484375, 'OPENAI_O_SERIES': 3.4868717193603516e-05, 'DEEPSEEK': 0.000518798828125, 'GROK': 1.615285873413086e-05, 'NOVA': 4.3332576751708984e-05, 'OTHER': 0.003772735595703125, 'HUMAN': 4.0531158447265625e-06}}"
2501.15985,review,post_llm,2025,1,"{'ai_likelihood': 7.682376437717014e-06, 'text': 'Demographic Benchmarking: Bridging Socio-Technical Gaps in Bias\n  Detection\n\n  Artificial intelligence (AI) models are increasingly autonomous in\ndecision-making, making pursuing responsible AI more critical than ever.\nResponsible AI (RAI) is defined by its commitment to transparency, privacy,\nsafety, inclusiveness, and fairness. But while the principles of RAI are clear\nand shared, RAI practices and auditing mechanisms are still incipient. A key\nchallenge is establishing metrics and benchmarks that define performance goals\naligned with RAI principles. This paper describes how the ITACA AI auditing\nplatform developed by Eticas.ai tackles demographic benchmarking when auditing\nAI recommender systems. To this end, we describe a Demographic Benchmarking\nFramework designed to measure the populations potentially impacted by specific\nAI models. The framework serves us as auditors as it allows us to not just\nmeasure but establish acceptability ranges for specific performance indicators,\nwhich we share with the developers of the systems we audit so they can build\nbalanced training datasets and measure and monitor fairness throughout the AI\nlifecycle. It is also a valuable resource for policymakers in drafting\neffective and enforceable regulations. Our approach integrates\nsocio-demographic insights directly into AI systems, reducing bias and\nimproving overall performance. The main contributions of this study include:1.\nDefining control datasets tailored to specific demographics so they can be used\nin model training; 2. Comparing the overall population with those impacted by\nthe deployed model to identify discrepancies and account for structural bias;\nand 3. Quantifying drift in different scenarios continuously and as a\npost-market monitoring mechanism.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.165,review,post_llm,2025,1,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Towards Frontier Safety Policies Plus\n\n  This paper examines the state of affairs on Frontier Safety Policies in light\nof capability progress and growing expectations held by government actors and\nAI safety researchers from these safety policies. It subsequently argues that\nFSPs should evolve to a more granular version, which this paper calls FSPs\nPlus. Compared to the first wave of FSPs led by a subset of frontier AI\ncompanies, FSPs Plus should be built around two main pillars. First, FSPs Plus\nshould adopt precursory capabilities as a new, clearer, and more comprehensive\nset of metrics. In this respect, this paper recommends that international or\ndomestic standardization bodies develop a standardized taxonomy of precursory\ncomponents to high-impact capabilities that FSPs Plus could then adopt by\nreference. The Frontier Model Forum could lead the way by establishing\npreliminary consensus amongst frontier AI developers on this topic. Second,\nFSPs Plus should expressly incorporate AI safety cases and establish a mutual\nfeedback mechanism between FSPs Plus and AI safety cases. To establish such a\nmutual feedback mechanism, FSPs Plus could be updated to include a clear\ncommitment to make AI safety cases at different milestones during development\nand deployment, to build and adopt safety measures based on the content and\nconfidence of AI safety cases, and, also on this basis, to keep updating and\nadjusting FSPs Plus.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.01128,regular,post_llm,2025,1,"{'ai_likelihood': 0.018666585286458336, 'text': 'Automating Work Orders and Tracking Winter Snow Plows and Patrol\n  Vehicles with Telematics Data\n\n  Winter road maintenance is a critical priority for the Indiana Department of\nTransportation, which manages an extensive fleet across thousands of lane\nmiles. The current manual tracking of snowplow workloads is inefficient and\nprone to errors. To address these challenges, we developed an in-browser web\napplication that automates the creation and verification of work orders using a\nlarge-scale GPS dataset from telematics systems. The application processes\nmillions of GPS data points from hundreds of vehicles over winter,\nsignificantly reducing manual labor and minimizing errors. Key features include\ngeohashing for efficient road segment identification, detailed segment-level\nwork records, and robust visualization of vehicle movements, even on repeated\nroutes. Our proposed solution has the potential to enhance the accuracy and\ngranularity of work records, support more effective resource allocation, ensure\ntimely compensation for drivers, alleviate administrative burdens, and allow\nmanagers to focus on strategic planning and real-time challenges. The web\napplication can be accessed at https://github.com/oats-center/arrtrack/\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.1404,review,post_llm,2025,1,"{'ai_likelihood': 1.1424223581949871e-05, 'text': 'Global Perspectives of AI Risks and Harms: Analyzing the Negative\n  Impacts of AI Technologies as Prioritized by News Media\n\n  Emerging AI technologies have the potential to drive economic growth and\ninnovation but can also pose significant risks to society. To mitigate these\nrisks, governments, companies, and researchers have contributed regulatory\nframeworks, risk assessment approaches, and safety benchmarks, but these can\nlack nuance when considered in global deployment contexts. One way to\nunderstand these nuances is by looking at how the media reports on AI, as news\nmedia has a substantial influence on what negative impacts of AI are discussed\nin the public sphere and which impacts are deemed important. In this work, we\nanalyze a broad and diverse sample of global news media spanning 27 countries\nacross Asia, Africa, Europe, Middle East, North America, and Oceania to gain\nvaluable insights into the risks and harms of AI technologies as reported and\nprioritized across media outlets in different countries. This approach reveals\na skewed prioritization of Societal Risks followed by Legal & Rights-related\nRisks, Content Safety Risks, Cognitive Risks, Existential Risks, and\nEnvironmental Risks, as reflected in the prevalence of these risk categories in\nthe news coverage of different nations. Furthermore, it highlights how the\ndistribution of such concerns varies based on the political bias of news\noutlets, underscoring the political nature of AI risk assessment processes and\npublic opinion. By incorporating views from various regions and political\norientations for assessing the risks and harms of AI, this work presents\nstakeholders, such as AI developers and policy makers, with insights into the\nAI risks categories prioritized in the public sphere. These insights may guide\nthe development of more inclusive, safe, and responsible AI technologies that\naddress the diverse concerns and needs across the world.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.19275,review,post_llm,2025,1,"{'ai_likelihood': 0.00023921330769856772, 'text': ""From Assistance to Autonomy -- A Researcher Study on the Potential of AI Support for Qualitative Data Analysis\n\nThe advent of Artificial Intelligence (AI) tools, such as Large Language Models, has introduced new possibilities for Qualitative Data Analysis (QDA), offering both opportunities and challenges. To help navigate the responsible integration of AI into QDA, we conducted semi-structured interviews with 15 Human-Computer Interaction (HCI) researchers experienced in QDA. While our participants were open to AI support in their QDA workflows, they expressed concerns about data privacy, autonomy, and the quality of AI outputs. In response, we developed a framework that spans from minimal to high AI involvement, providing tangible scenarios for integrating AI into QDA practices while addressing researchers' needs and concerns. Aligned with real-life QDA workflows, we identify potential for AI tools in areas such as data pre-processing, researcher onboarding, or conflict mediation. Our framework aims to provoke further discussion on the development of AI-supported QDA and to help establish community standards for responsible Human-AI collaboration."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.15491,review,post_llm,2025,1,"{'ai_likelihood': 1.8411212497287328e-05, 'text': 'A Critical Field Guide for Working with Machine Learning Datasets\n\n  Machine learning datasets are powerful but unwieldy. Despite the fact that\nlarge datasets commonly contain problematic material--whether from a technical,\nlegal, or ethical perspective--datasets are valuable resources when handled\ncarefully and critically. A Critical Field Guide for Working with Machine\nLearning Datasets suggests practical guidance for conscientious dataset\nstewardship. It offers questions, suggestions, strategies, and resources for\nworking with existing machine learning datasets at every phase of their\nlifecycle. It combines critical AI theories and applied data science concepts,\nexplained in accessible language. Equipped with this understanding, students,\njournalists, artists, researchers, and developers can be more capable of\navoiding the problems unique to datasets. They can also construct more\nreliable, robust solutions, or even explore new ways of thinking with machine\nlearning datasets that are more critical and conscientious.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.16548,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate\n\nAs the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands. Much of this debate has concentrated on direct impacts -- energy and water usage in data centers, e-waste from frequent hardware upgrades -- without addressing the significant indirect effects. This paper examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI's impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI's true climate footprint, limiting the scope for meaningful interventions. We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth. By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI's role in the climate crisis."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.719329833984375e-05, 'GPT4': 0.00864410400390625, 'CLAUDE': 0.98974609375, 'GOOGLE': 6.413459777832031e-05, 'OPENAI_O_SERIES': 3.713369369506836e-05, 'DEEPSEEK': 0.0013589859008789062, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 2.980232238769531e-07, 'HUMAN': 9.655952453613281e-06}}"
2502.0347,review,post_llm,2025,1,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Responsible Artificial Intelligence (RAI) in U.S. Federal Government :\n  Principles, Policies, and Practices\n\n  Artificial intelligence (AI) and machine learning (ML) have made tremendous\nadvancements in the past decades. From simple recommendation systems to more\ncomplex tumor identification systems, AI/ML systems have been utilized in a\nplethora of applications. This rapid growth of AI/ML and its proliferation in\nnumerous private and public sector applications, while successful, has also\nopened new challenges and obstacles for regulators. With almost little to no\nhuman involvement required for some of the new decision-making AI/ML systems,\nthere is now a pressing need to ensure the responsible use of these systems.\nParticularly in federal government use-cases, the use of AI technologies must\nbe carefully governed by appropriate transparency and accountability\nmechanisms. This has given rise to new interdisciplinary fields of AI research\nsuch as \\textit{Responsible AI (RAI)}. In this position paper we provide a\nbrief overview of development in RAI and discuss some of the motivating\nprinciples commonly explored in the field. An overview of the current\nregulatory landscape relating to AI is also discussed with analysis of\ndifferent Executive Orders, policies and frameworks. We then present examples\nof how federal agencies are aiming for the responsible use of AI, specifically\nwe present use-case examples of different projects and research from the Census\nBureau on implementing the responsible use of AI. We also provide a brief\noverview for a Responsible AI Assessment Toolkit currently under-development\naimed at helping federal agencies operationalize RAI principles. Finally, a\nrobust discussion on how different policies/regulations map to RAI principles,\nalong with challenges and opportunities for regulation/governance of\nresponsible AI within the federal government is presented.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.09482,review,post_llm,2025,1,"{'ai_likelihood': 1.74509154425727e-05, 'text': ""Building Bridges across Papua New Guinea's Digital Divide in Growing the\n  ICT Industry\n\n  Papua New Guinea (PNG) is an emerging tech society with an opportunity to\novercome geographic and social boundaries, in order to engage with the global\nmarket. However, the current tech landscape, dominated by Big Tech in Silicon\nValley and other multinational companies in the Global North, tends to overlook\nthe requirements of emerging economies such as PNG. This is becoming more\nobvious as issues such as algorithmic bias (in tech product deployments) and\nthe digital divide (as in the case of non-affordable commercial software) are\naffecting PNG users. The Open Source Software (OSS) movement, based on extant\nresearch, is seen as a way to level the playing field in the digitalization and\nadoption of Information and Communications Technologies (ICTs) in PNG. This\nperspectives paper documents the outcome of the second International Workshop\non BRIdging the Divides with Globally Engineered Software} (BRIDGES2023) in the\nhopes of proposing ideas for future research into ICT education, uplifting\nsoftware engineering (SE) capability, and OSS adoption in promoting a more\nequitable digital future for PNG.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.02074,review,post_llm,2025,1,"{'ai_likelihood': 3.837876849704319e-05, 'text': 'A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations\n\nSocietal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without adopting a unified or holistic approach to social stereotypes and how they impact various parts of the machine learning pipeline. As a result, current interventions fail to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and context. We also provide considerations and recommendations for its responsible use.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04729,review,post_llm,2025,1,"{'ai_likelihood': 2.824597888522678e-05, 'text': 'Urban Metaverse: The Smart City in the Industrial Metaverse.\n  Opportunities of the metaverse for real-time, interactive, and inclusive\n  infrastructure applications in urban areas\n\n  The Urban Metaverse describes an immersive 3D environment that connects the\nphysical world of the city and its citizens with its digital data and systems.\nPhysical and digital realities merge, opening up new possibilities for the\ndesign and use of the city. This trend study serves as a source of inspiration\nand guidance for city and community leaders, urban planners, IT professionals,\nand anyone interested in the future of urban spaces. It helps to understand the\nopportunities and challenges of the urban metaverse as an evolution of the\nSmart City and to set the course for sustainable and innovative urban\ndevelopment. To this end, the study analyzes the opportunities that the urban\nmetaverse offers for urban administration and the everyday life of citizens,\npresents key technologies, and highlights the socio-economic challenges of\nimplementation. The focus is on the potential of the urban metaverse to\noptimize the planning and operation of urban infrastructures, to promote\ninclusion and civic participation, and to enhance the innovative capacity of\ncities and municipalities. The study develops four recommendations for the\nimplementation of metaverse applications in an urban context: 1. user-centered\ndesign, 2. ubiquitous accessibility, 3. proactive design of the regulatory\nframework, and 4. development of viable business models.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.09601,review,post_llm,2025,1,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'Anatomy of a Digital Bubble: Lessons Learned from the NFT and Metaverse\n  Frenzy\n\n  In the past few years, ""metaverse"" and ""non-fungible tokens (NFT)"" have\nbecome buzzwords, and the prices of related assets have exhibited large\nfluctuations. Are those characteristic of a speculative bubble? In this paper,\nwe attempt to answer this question, and better understand the underlying\neconomic dynamics. We look at Decentraland, a virtual world platform where land\nparcels are sold as NFT collections. We find that initially, land prices\nfollowed traditional real estate pricing models - in particular, value\ndecreased with distance from the most desirable areas - suggesting Decentraland\nbehaved much like a virtual city. However, these real estate pricing models\nstopped applying when both the metaverse and NFTs gained increased popular\nattention and enthusiasm in 2021, suggesting a new driving force for the\nunderlying asset prices. At that time, following a substantial rise in NFT\nmarket values, short-term holders of multiple parcels began to take major\nselling positions in the Decentraland market, which hints that, rather than\nbuilding a metaverse community, early Decentraland investors preferred to cash\nout when land valuations became inflated. Our analysis also shows that while\nthe majority of buyers are new entrants to the market (many of whom joined\nduring the bubble), liquidity (i.e., parcels) was mostly provided by early\nadopters selling, which caused stark differences in monetary gains. Early\nadopters made money - more than 10,000 USD on average per parcel sold - but\nusers who joined later typically made no profit or even incurred losses in the\norder of 1,000 USD per parcel. Unlike established markets such as financial and\nreal estate markets, newly emergent digital marketplaces are mostly\nself-regulated. As a result, the significant financial risks we identify\nindicate a strong need for establishing appropriate standards of business\nconduct and improving user awareness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10264,regular,post_llm,2025,1,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Application of the Cyberinfrastructure Production Function Model to R1 Institutions\n\nHigh-performance computing (HPC) is widely used in higher education for modeling, simulation, and AI applications. A critical piece of infrastructure with which to secure funding, attract and retain faculty, and teach students, supercomputers come with high capital and operating costs that must be considered against other competing priorities. This study applies the concepts of the production function model from economics with two thrusts: 1) to evaluate if previous research on building a model for quantifying the value of investment in research computing is generalizable to a wider set of universities, and 2) to define a model with which to capacity plan HPC investment, based on institutional production - inverting the production function. We show that the production function model does appear to generalize, showing positive institutional returns from the investment in computing resources and staff. We do, however, find that the relative relationships between model inputs and outputs vary across institutions, which can often be attributed to understandable institution-specific factors.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.09606,review,post_llm,2025,1,"{'ai_likelihood': 0.49126519097222227, 'text': ""Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves\n\nThis paper presents a survey of local US policymakers' views on the future impact and regulation of AI. Our survey provides insight into US policymakers' expectations regarding the effects of AI on local communities and the nation, as well as their attitudes towards specific regulatory policies. Conducted in two waves (2022 and 2023), the survey captures changes in attitudes following the release of ChatGPT and the subsequent surge in public awareness of AI. Local policymakers express a mix of concern, optimism, and uncertainty about AI's impacts, anticipating significant societal risks such as increased surveillance, misinformation, and political polarization, alongside potential benefits in innovation and infrastructure. Many also report feeling underprepared and inadequately informed to make AI-related decisions. On regulation, a majority of policymakers support government oversight and favor specific policies addressing issues such as data privacy, AI-related unemployment, and AI safety and fairness. Democrats show stronger and more consistent support for regulation than Republicans, but the latter experienced a notable shift towards majority support between 2022 and 2023. Our study contributes to understanding the perspectives of local policymakers-key players in shaping state and federal AI legislation-by capturing evolving attitudes, partisan dynamics, and their implications for policy formation. The findings highlight the need for capacity-building initiatives and bi-partisan coordination to mitigate policy fragmentation and build a cohesive framework for AI governance in the US."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.04064,review,post_llm,2025,1,"{'ai_likelihood': 0.9560546875, 'text': 'Examining Popular Arguments Against AI Existential Risk: A Philosophical\n  Analysis\n\n  Concerns about artificial intelligence (AI) and its potential existential\nrisks have garnered significant attention, with figures like Geoffrey Hinton\nand Dennis Hassabis advocating for robust safeguards against catastrophic\noutcomes. Prominent scholars, such as Nick Bostrom and Max Tegmark, have\nfurther advanced the discourse by exploring the long-term impacts of\nsuperintelligent AI. However, this existential risk narrative faces criticism,\nparticularly in popular media, where scholars like Timnit Gebru, Melanie\nMitchell, and Nick Clegg argue, among other things, that it distracts from\npressing current issues. Despite extensive media coverage, skepticism toward\nthe existential risk discourse has received limited rigorous treatment in\nacademic literature. Addressing this imbalance, this paper reconstructs and\nevaluates three common arguments against the existential risk perspective: the\nDistraction Argument, the Argument from Human Frailty, and the Checkpoints for\nIntervention Argument. By systematically reconstructing and assessing these\narguments, the paper aims to provide a foundation for more balanced academic\ndiscourse and further research on AI.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00518035888671875, 'GPT4': 0.242919921875, 'CLAUDE': 0.022430419921875, 'GOOGLE': 0.66552734375, 'OPENAI_O_SERIES': 0.039306640625, 'DEEPSEEK': 0.004302978515625, 'GROK': 2.0503997802734375e-05, 'NOVA': 0.0003464221954345703, 'OTHER': 0.016510009765625, 'HUMAN': 0.003681182861328125}}"
2501.01738,regular,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""Mapping Compliance: A Taxonomy for Political Content Analysis under the\n  EU's Digital Electoral Framework\n\n  The rise of digital platforms has transformed political campaigning,\nintroducing complex regulatory challenges. This paper presents a comprehensive\ntaxonomy for analyzing political content in the EU's digital electoral\nlandscape, aligning with the requirements set forth in new regulations, such as\nthe Digital Services Act. Using a legal doctrinal methodology, we construct a\ndetailed codebook that enables systematic content analysis across\nuser-generated and political ad content to assess compliance with regulatory\nmandates.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0007605552673339844, 'GPT4': 0.0179290771484375, 'CLAUDE': 0.40087890625, 'GOOGLE': 0.54931640625, 'OPENAI_O_SERIES': 0.005767822265625, 'DEEPSEEK': 0.0171051025390625, 'GROK': 1.9252300262451172e-05, 'NOVA': 3.129243850708008e-05, 'OTHER': 0.0072784423828125, 'HUMAN': 0.0010557174682617188}}"
2501.00959,regular,post_llm,2025,1,"{'ai_likelihood': 0.0010935465494791667, 'text': 'IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs\n\n  This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.09407,regular,post_llm,2025,1,"{'ai_likelihood': 6.953875223795573e-06, 'text': 'NEBULA: A National Scale Dataset for Neighbourhood-Level Urban Building\n  Energy Modelling for England and Wales\n\n  Buildings are significant contributors to global greenhouse gas emissions,\naccounting for 26% of global energy sector emissions in 2022. Meeting net zero\ngoals requires a rapid reduction in building emissions, both directly from the\nbuildings and indirectly from the production of electricity and heat used in\nbuildings. National energy planning for net zero demands both detailed and\ncomprehensive building energy consumption data. However, geo-located\nbuilding-level energy data is rarely available in Europe, with analysis\ntypically relying on anonymised, simulated or low-resolution data. To address\nthis problem, we introduce a dataset of Neighbourhood Energy, Buildings, and\nUrban Landscapes (NEBULA) for modelling domestic energy consumption for small\nneighbourhoods (5-150 households). NEBULA integrates data on building\ncharacteristics, climate, urbanisation, environment, and socio-demographics and\ncontains 609,964 samples across England and Wales.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.14886,review,post_llm,2025,1,"{'ai_likelihood': 3.112687004937066e-06, 'text': ""A Systematic Literature Review on Equity and Technology in HCI and\n  Fairness: Navigating the Complexities and Nuances of Equity Research\n\n  Equity is crucial to the ethical implications in technology development.\nHowever, implementing equity in practice comes with complexities and nuances.\nIn response, the research community, especially the human-computer interaction\n(HCI) and Fairness community, has endeavored to integrate equity into\ntechnology design, addressing issues of societal inequities. With such\nincreasing efforts, it is yet unclear why and how researchers discuss equity\nand its integration into technology, what research has been conducted, and what\ngaps need to be addressed. We conducted a systematic literature review on\nequity and technology, collecting and analyzing 202 papers published in HCI and\nFairness-focused venues. Amidst the substantial growth of relevant publications\nwithin the past four years, we deliver three main contributions: (1) we\nelaborate a comprehensive understanding researchers' motivations for studying\nequity and technology, (2) we illustrate the different equity definitions and\nframeworks utilized to discuss equity, (3) we characterize the key themes\naddressing interventions as well as tensions and trade-offs when advancing and\nintegrating equity to technology. Based on our findings, we elaborate an equity\nframework for researchers who seek to address existing gaps and advance equity\nin technology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.07787,regular,post_llm,2025,1,"{'ai_likelihood': 0.21877712673611113, 'text': 'A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles\n  to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable\n  Populations\n\n  Rapid advancements in autonomous vehicles (AVs) are poised to revolutionize\ntransportation and communities, including disaster evacuations, particularly\nthrough the deployment of Shared Autonomous Vehicles (SAVs). Despite the\npotential, the use of SAVs in rural disaster evacuations remains an\nunderexplored area. To address this gap, this study proposes a simulation-based\nframework that integrates both mathematical programming and SUMO traffic\nsimulation to deploy SAVs in pre- and post-disaster evacuations in rural areas.\nThe framework prioritizes the needs of vulnerable groups, including individuals\nwith disabilities, limited English proficiency, and elderly residents. Sumter\nCounty, Florida, serves as the case study due to its unique characteristics: a\nhigh concentration of vulnerable individuals and limited access to public\ntransportation, making it one of the most transportation-insecure counties in\nthe state. These conditions present significant challenges for evacuation\nplanning in the region. To explore potential solutions, we conducted mass\nevacuation simulations by incorporating SAVs across seven scenarios. These\nscenarios represented varying SAV penetration levels, ranging from 20% to 100%\nof the vulnerable population, and were compared to a baseline scenario using\nonly passenger cars. Additionally, we examined both pre-disaster and\npost-disaster conditions, accounting for infrastructure failures and road\nclosures. According to the simulation results, higher SAV integration\nsignificantly improves traffic distribution and reduces congestion. Scenarios\nfeaturing more SAVs exhibited lower congestion peaks and more stable traffic\nflow. Conversely, mixed traffic environments demonstrate reduced average speeds\nattributable to interactions between SAVs and passenger cars, while exclusive\nuse of SAVs results in higher speeds and more stable travel patterns.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.05383,review,post_llm,2025,1,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'The Humanist Programming Novice as Novice\n\n  The primary aim of this paper is to suggest questions for future discourse\nand research of specialized programming courses in the Humanities. Specifically\nI ask whether specialized courses promote the production of fragile programming\nknowledge, what are the difficulties encountered by humanistic students in\ntheir learning of programming, and what may be the proper place of algorithmics\nin the curriculum of specialized studies.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.03556,review,post_llm,2025,1,"{'ai_likelihood': 0.001270506117078993, 'text': 'Unwinding NFTs in the Shadow of IP Law\n\n  Amid the surge of intellectual property (IP) disputes surrounding\nnon-fungible tokens (NFTs), some scholars have advocated for the application of\npersonal property or sales law to regulate NFT minting and transactions,\ncontending that IP laws unduly hinder the development of the NFT market. This\nArticle counters these proposals and argues that the existing IP system stands\nas the most suitable regulatory framework for governing the evolving NFT\nmarket. Compared to personal property or sales law, IP laws can more\neffectively address challenges such as tragedies of the commons and anticommons\nin the NFT market. NFT communities have also developed their own norms and\nlicensing agreements upon existing IP laws to regulate shared resources.\nMoreover, the IP regimes, with both static and dynamic institutional designs,\ncan effectively balance various policy concerns, such as innovation, fair\ncompetition, and consumer protection, which alternative proposals struggle to\nprovide.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.03752,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': 'Multimodal Generative AI and Foundation Models for Behavioural Health in\n  Online Gambling\n\n  Online gambling platforms have transformed the gambling landscape, offering\nunprecedented accessibility and personalized experiences. However, these same\ncharacteristics have increased the risk of gambling-related harm, affecting\nindividuals, families, and communities. Structural factors, including targeted\nmarketing, shifting social norms, and gaps in regulation, further complicate\nthe challenge. This narrative review examines how artificial intelligence,\nparticularly multimodal generative models and foundation technologies, can\naddress these issues by supporting prevention, early identification, and\nharm-reduction efforts. We detail applications such as synthetic data\ngeneration to overcome research barriers, customized interventions to guide\nsafer behaviors, gamified tools to support recovery, and scenario modeling to\ninform effective policies. Throughout, we emphasize the importance of\nsafeguarding privacy and ensuring that technological advances are responsibly\naligned with public health objectives.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.133317947387695e-05, 'GPT4': 0.97314453125, 'CLAUDE': 0.01654052734375, 'GOOGLE': 0.00014460086822509766, 'OPENAI_O_SERIES': 0.0004634857177734375, 'DEEPSEEK': 0.009613037109375, 'GROK': 5.960464477539063e-08, 'NOVA': 4.172325134277344e-07, 'OTHER': 1.2040138244628906e-05, 'HUMAN': 5.1975250244140625e-05}}"
2501.16494,regular,post_llm,2025,1,"{'ai_likelihood': 1.1258655124240452e-06, 'text': ""Classroom Activities and New Classroom Apps for Enhancing Children's\n  Understanding of Social Media Mechanisms\n\n  Young people are increasingly exposed to adverse effects of data-driven\nprofiling, recommending, and manipulation on social media platforms, most of\nthem without adequate understanding of the mechanisms that drive these\nplatforms. In the context of computing education, educating learners about\nmechanisms and data practices of social media may improve young learners' data\nagency, digital literacy, and understanding how their digital services work. A\nfour-hour technology -- supported intervention was designed and implemented in\n12 schools involving 209 5th and 8th grade learners. Two new classroom apps\nwere developed to support the classroom activities. Using Likert-scale\nquestions borrowed from a data agency questionnaire and open-ended questions\nthat mapped learners' data-driven reasoning on social media phenomena, this\narticle shows significant improvement between pre- and post-tests in learners'\ndata agency and data-driven explanations of social media mechanisms. Results\npresent an example of improving young learners' understanding of social media\nmechanisms.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.06913,review,post_llm,2025,1,"{'ai_likelihood': 1.0298358069525824e-05, 'text': 'Towards Fair and Privacy-Aware Transfer Learning for Educational\n  Predictive Modeling: A Case Study on Retention Prediction in Community\n  Colleges\n\n  Predictive analytics is widely used in learning analytics, but many\nresource-constrained institutions lack the capacity to develop their own models\nor rely on proprietary ones trained in different contexts with little\ntransparency. Transfer learning holds promise for expanding equitable access to\npredictive analytics but remains underexplored due to legal and technical\nconstraints. This paper examines transfer learning strategies for retention\nprediction at U.S. two-year community colleges. We envision a scenario where\ncommunity colleges collaborate with each other and four-year universities to\ndevelop retention prediction models under privacy constraints and evaluate\nrisks and improvement strategies of cross-institutional model transfer. Using\nadministrative records from 4 research universities and 23 community colleges\ncovering over 800,000 students across 7 cohorts, we identify performance and\nfairness degradation when external models are deployed locally without\nadaptation. Publicly available contextual information can forecast these\nperformance drops and offer early guidance for model portability. For\ndevelopers under privacy regulations, sequential training selecting\ninstitutions based on demographic similarities enhances fairness without\ncompromising performance. For institutions lacking local data to fine-tune\nsource models, customizing evaluation thresholds for sensitive groups\noutperforms standard transfer techniques in improving performance and fairness.\nOur findings suggest the value of transfer learning for more accessible\neducational predictive modeling and call for judicious use of contextual\ninformation in model training, selection, and deployment to achieve reliable\nand equitable model transfer.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2501.10493,review,post_llm,2025,1,"{'ai_likelihood': 4.569689432779948e-06, 'text': 'Journalists Knowledge and Utilisation of Google Translate Application in\n  South East, Nigeria\n\n  This study was aimed at finding out if journalists in South East Nigeria have\nknowledge of Google Translate Application and also utilise it. It adopted a\nsurvey design with a sample size of 320 which was determined using Krejcie &\nMorgan (1970). Its objectives were to ascertain the extent journalists in South\nEast Nigeria know about Google Translate Application, assess the utilisation of\nGoogle Translate Application among journalists in South East Nigeria, and\nidentify the challenges affecting the journalists in South East Nigeria while\nusing Google Translate Application. The theoretical underpin was Knowledge\nAttitude and Practise Model (KAP). The findings showed that journalists in\nSouth East Nigeria have knowledge of Google Translate Application but apply it\nmostly outside the region. It concludes that journalists in South East Nigeria\nhave the knowledge of the App. but apply it outside the zone. The study\nrecommends increased usage of the App. within South East Nigeria.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00007,review,post_llm,2025,1,"{'ai_likelihood': 1.0, 'text': ""The Dead Internet Theory: A Survey on Artificial Interactions and the\n  Future of Social Media\n\n  The Dead Internet Theory (DIT) suggests that much of today's internet,\nparticularly social media, is dominated by non-human activity, AI-generated\ncontent, and corporate agendas, leading to a decline in authentic human\ninteraction. This study explores the origins, core claims, and implications of\nDIT, emphasizing its relevance in the context of social media platforms. The\ntheory emerged as a response to the perceived homogenization of online spaces,\nhighlighting issues like the proliferation of bots, algorithmically generated\ncontent, and the prioritization of engagement metrics over genuine user\ninteraction. AI technologies play a central role in this phenomenon, as social\nmedia platforms increasingly use algorithms and machine learning to curate\ncontent, drive engagement, and maximize advertising revenue. While these tools\nenhance scalability and personalization, they also prioritize virality and\nconsumption over authentic communication, contributing to the erosion of trust,\nthe loss of content diversity, and a dehumanized internet experience. This\nstudy redefines DIT in the context of social media, proposing that the\ncommodification of content consumption for revenue has taken precedence over\nmeaningful human connectivity. By focusing on engagement metrics, platforms\nfoster a sense of artificiality and disconnection, underscoring the need for\nhuman-centric approaches to revive authentic online interaction and community\nbuilding.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.486343383789062e-05, 'GPT4': 0.9921875, 'CLAUDE': 0.00018775463104248047, 'GOOGLE': 0.0036449432373046875, 'OPENAI_O_SERIES': 0.0006356239318847656, 'DEEPSEEK': 0.0034580230712890625, 'GROK': 2.980232238769531e-07, 'NOVA': 7.152557373046875e-07, 'OTHER': 4.76837158203125e-06, 'HUMAN': 8.940696716308594e-07}}"
2502.00006,review,post_llm,2025,1,"{'ai_likelihood': 0.9609375, 'text': 'Determinants of Human Development Index (HDI): A Regression Analysis of\n  Economic and Social Indicators\n\n  This study aims to investigate the factors influencing the Human Development\nIndex (HDI). Five variables-GDP per capita, health expenditure, education\nexpenditure, infant mortality rate (per 1,000 live births), and average years\nof schooling-were analyzed to develop a regression model assessing their impact\non HDI. The results indicate that GDP per capita, infant mortality rate, and\naverage years of schooling are significant predictors of HDI. Specifically, the\nstudy finds a positive relationship between GDP per capita and average years of\nschooling with HDI, while infant mortality rate is negatively associated with\nHDI.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.040557861328125, 'GPT4': 0.0159759521484375, 'CLAUDE': 0.013641357421875, 'GOOGLE': 0.916015625, 'OPENAI_O_SERIES': 0.0002593994140625, 'DEEPSEEK': 2.3186206817626953e-05, 'GROK': 3.7789344787597656e-05, 'NOVA': 2.5451183319091797e-05, 'OTHER': 0.00536346435546875, 'HUMAN': 0.00791168212890625}}"
2502.00004,review,post_llm,2025,1,"{'ai_likelihood': 0.9951171875, 'text': ""The Impact of Student Writing Assessment Literacy on Psychological\n  Factors: An Ordinal Logistic Regression Analysis\n\n  Previous studies have shown that enhanced student assessment literacy can\nlead to improvements in academic performance in EFL (English as a Foreign\nLanguage) writing. Additionally, psychological factors such as self-efficacy,\nachievement motivation, and writing anxiety significantly influence EFL writing\noutcomes. However, the relationship between student writing assessment literacy\n(SWAL) and these psychological factors remains unclear. The present study aims\nto explore how SWAL affects psychological factors in the Chinese EFL context.\nData were collected from 103 Chinese undergraduate EFL students using four\nquestionnaires: the Student Writing Assessment Literacy Scale (SWAL), the\nSelf-Efficacy for Writing Scale (SEWS), the Achievement Goal Questionnaire\n(AGQ), and the Second Language Writing Anxiety Inventory (SLWAI). Ordinal\nlogistic regression was employed to analyze the data. The results indicated\nthat higher levels of SWAL were positively associated with writing\nself-efficacy and achievement motivation, while negatively related to writing\nanxiety. These findings have significant pedagogical implications for second\nlanguage (L2) writing instructions, emphasizing the importance of integrating\nSWAL training into writing instruction to enhance students' writing experiences\nand outcomes.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0330810546875, 'GPT4': 0.6201171875, 'CLAUDE': 0.1607666015625, 'GOOGLE': 0.146240234375, 'OPENAI_O_SERIES': 0.0248565673828125, 'DEEPSEEK': 0.0009026527404785156, 'GROK': 0.00010782480239868164, 'NOVA': 0.00010329484939575195, 'OTHER': 0.012481689453125, 'HUMAN': 0.0014371871948242188}}"
2501.00964,review,post_llm,2025,1,"{'ai_likelihood': 2.4338563283284507e-05, 'text': 'From Assessment to Practice: Implementing the AIAS Framework in EFL\n  Teaching and Learning\n\n  Recent advances in Generative AI (GenAI) are transforming multiple aspects of\nsociety, including education and foreign language learning. In the context of\nEnglish as a Foreign Language (EFL), significant research has been conducted to\ninvestigate the applicability of GenAI as a learning aid and the potential\nnegative impacts of new technologies. Critical questions remain about the\nfuture of AI, including whether improvements will continue at such a pace or\nstall and whether there is a true benefit to implementing GenAI in education,\ngiven the myriad costs and potential for negative impacts.\n  Apart from the ethical conundrums that GenAI presents in EFL education, there\nis growing consensus that learners and teachers must develop AI literacy skills\nto enable them to use and critically evaluate the purposes and outputs of these\ntechnologies. However, there are few formalised frameworks available to support\nthe integration and development of AI literacy skills for EFL learners. In this\narticle, we demonstrate how the use of a general, all-purposes framework (the\nAI Assessment Scale) can be tailored to the EFL writing and translation\ncontext, drawing on existing empirical research validating the scale and\nadaptations to other contexts, such as English for Academic Purposes. We begin\nby engaging with the literature regarding GenAI and EFL writing and\ntranslation, prior to explicating the use of three levels of the updated AIAS\nfor structuring EFL writing instruction which promotes academic literacy and\ntransparency and provides a clear framework for students and teachers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.03083,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': ""The Role of Mobile and Social Media Services in Enhancing Freedom of\n  Expression: Opportunities, Challenges, and Prospects for Local Platform\n  Development in Uganda's Digital Ecosystem\n\n  Utilizing mobile and social media platforms is a transformative approach to\nenhancing freedom of expression and fostering digital engagement. However,\nUganda's digital ecosystem faces challenges such as restrictive legislation,\nfinancial barriers, and the absence of localized platforms tailored to cultural\ncontexts. This study employed a mixed-methods approach to explore how these\nplatforms influence public discourse, activism, and civic participation while\nhighlighting opportunities for local innovation. The research further\nidentified the critical need for regulatory reforms, investments in digital\nliteracy, and collaborative efforts to develop sustainable and culturally\nrelevant platforms, ensuring a more inclusive and empowered digital society.\n  Keywords: Freedom of Expression, Mobile Services, Social Media Platforms,\nLocal Digital Innovation, Uganda's Digital Ecosystem\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001285076141357422, 'GPT4': 0.9482421875, 'CLAUDE': 0.0031108856201171875, 'GOOGLE': 0.0236663818359375, 'OPENAI_O_SERIES': 0.013214111328125, 'DEEPSEEK': 0.002651214599609375, 'GROK': 5.424022674560547e-05, 'NOVA': 2.9146671295166016e-05, 'OTHER': 0.0088348388671875, 'HUMAN': 2.980232238769531e-07}}"
2502.12969,regular,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': ""Generative AI and Information Asymmetry: Impacts on Adverse Selection and Moral Hazard\n\nInformation asymmetry often leads to adverse selection and moral hazard in economic markets, causing inefficiencies and welfare losses. Traditional methods to address these issues, such as signaling and screening, are frequently insufficient. This research investigates how Generative Artificial Intelligence (AI) can create detailed informational signals that help principals better understand agents' types and monitor their actions. By incorporating these AI-generated signals into a principal-agent model, the study aims to reduce inefficiencies and improve contract designs. Through theoretical analysis and simulations, we demonstrate that Generative AI can effectively mitigate adverse selection and moral hazard, resulting in more efficient market outcomes and increased social welfare. Additionally, the findings offer practical insights for policymakers and industry stakeholders on the responsible implementation of Generative AI solutions to enhance market performance."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0024013519287109375, 'GPT4': 0.654296875, 'CLAUDE': 0.1473388671875, 'GOOGLE': 0.019805908203125, 'OPENAI_O_SERIES': 0.171630859375, 'DEEPSEEK': 0.0011453628540039062, 'GROK': 0.00010126829147338867, 'NOVA': 0.00043129920959472656, 'OTHER': 0.00302886962890625, 'HUMAN': 3.6835670471191406e-05}}"
2502.0525,regular,post_llm,2025,2,"{'ai_likelihood': 1.0563267601860894e-05, 'text': 'Exploring internet radio across the globe with the MIRAGE online\n  dashboard\n\n  This study presents the Music Informatics for Radio Across the GlobE (MIRAGE)\nonline dashboard, which allows users to access, interact with, and export\nmetadata (e.g., artist name, track title) and musicological features (e.g.,\ninstrument list, voice type, key/mode) for 1 million events streaming on 10,000\ninternet radio stations across the globe. Users can search for stations or\nevents according to several criteria, display, analyze, and listen to the\nselected station/event lists using interactive visualizations that include\nembedded links to streaming services, and finally export relevant metadata and\nvisualizations for further study.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.06059,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': ""Prioritization First, Principles Second: An Adaptive Interpretation of Helpful, Honest, and Harmless Principles\n\nThe Helpful, Honest, and Harmless (HHH) principle is a foundational framework for aligning AI systems with human values. However, existing interpretations of the HHH principle often overlook contextual variability and conflicting requirements across applications. In this paper, we argue for an adaptive interpretation of the HHH principle and propose a reference framework for its adaptation to diverse scenarios. We first examine the principle's foundational significance and identify ambiguities and conflicts through case studies of its dimensions. To address these challenges, we introduce the concept of priority order, which provides a structured approach for balancing trade-offs among helpfulness, honesty, and harmlessness. Further, we explore the interrelationships between these dimensions, demonstrating how harmlessness and helpfulness can be jointly enhanced and analyzing their interdependencies in high-risk evaluations. Building on these insights, we propose a reference framework that integrates context definition, value prioritization, risk assessment, and benchmarking standards to guide the adaptive application of the HHH principle. This work offers practical insights for improving AI alignment, ensuring that HHH principles remain both ethically grounded and operationally effective in real-world AI deployment."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.481740951538086e-05, 'GPT4': 0.97265625, 'CLAUDE': 0.0121002197265625, 'GOOGLE': 0.002227783203125, 'OPENAI_O_SERIES': 0.011383056640625, 'DEEPSEEK': 0.0013427734375, 'GROK': 1.2516975402832031e-06, 'NOVA': 6.556510925292969e-07, 'OTHER': 2.568960189819336e-05, 'HUMAN': 8.165836334228516e-06}}"
2502.11695,review,post_llm,2025,2,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Information Sharing Among Countries: A Perspective from Country-Specific\n  Websites in Global Brands\n\n  Multiple official languages within a country along with languages common with\nother countries demand content consistency in both shared and unshared\nlanguages during information sharing. However, inconsistency due to conflict in\ncontent shared and content updates not propagated in languages between\ncountries poses a problem. Towards addressing inconsistency, this research\nqualitatively studied traits for information sharing among countries inside\nglobal brands as depicted by content shared in their country-specific websites.\nFirst, inconsistency in content shared is illustrated among websites\nhighlighting the problem in information sharing among countries. Second,\ncontent propagation among countries that vary in scales and coupling for\nspecific content categories are revealed. Scales suggested that corporate and\ncustomer support related information tend to be shared globally and locally\nrespectively while product related information is both locally and regionally\nsuitable for sharing. Higher occurrences of propagation when sharing corporate\nrelated information also showed tendency for high coupling between websites\nsuggesting the suitability for rigid consistency policy compared to other\ncategories. This study also proposed a simplistic approach with pattern of\nsharing to enable consistent information sharing.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05764,review,post_llm,2025,2,"{'ai_likelihood': 1.2516975402832031e-05, 'text': ""The EU Digital Services Act: what does it mean for online advertising\n  and adtech?\n\n  What does the Digital Services Act (DSA) mean for online advertising? We\ndescribe and analyse the DSA rules that are most relevant for online\nadvertising and adtech (advertising technology). We also highlight to what\nextent the DSA's advertising rules add something to the rules in the General\nData Protection Regulation (GDPR) and the ePrivacy Directive. The DSA\nintroduces several specific requirements for online advertising. First, the DSA\nimposes transparency requirements in relation to advertisements. Second, very\nlarge online platforms (VLOPs) should develop a publicly available repository\nwith information about the ads they presented. Third, the DSA bans\nprofiling-based advertising (behavioural advertising) if it uses sensitive data\nor if it targets children. Besides these specific provisions, the general rules\nof the DSA on illegal content also apply to advertising. Advertisements are a\nform of information, and thus subject to the general DSA rules. Moreover, we\nconclude that the DSA applies to some types of ad tech companies. For example,\nad networks, companies that connect advertisers to publishers of apps and\nwebsites, should be considered platforms. Some ad networks may even qualify as\nVLOPs. Hence, ad networks must comply with the more general obligations in the\nDSA. The application of these general rules to advertisements and ad networks\ncan have far-reaching effects that have been underexplored and deserve further\nresearch. We also show that certain aspects of the DSA are still unclear. For\ninstance, we encourage the European Commission or regulators to clarify the\nconcepts of 'online platform' and 'recipients' in the context of ad networks\nand other adtech companies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04754,review,post_llm,2025,2,"{'ai_likelihood': 0.0, 'text': 'GIS as a Job Growth Area for IT Professionals\n\n  As more companies look to capitalize on the benefits of geospatial data,\nGeographic Information Systems provide an area for growth in the Information\nTechnology job sector in the United States. Careers in GIS require geography,\ncartography, and IT skills. As the industry grows, candidates with these types\nof skills that are in demand and are needed to advance the geospatial industry\nforward. This industry is not generally known as a growth area to many IT\nprofessionals, and due to misleading job postings, many candidates may not know\ntheir skills are in demand\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.08073,regular,post_llm,2025,2,"{'ai_likelihood': 0.912109375, 'text': 'Large language models perpetuate bias in palliative care: development\n  and analysis of the Palliative Care Adversarial Dataset (PCAD)\n\n  Bias and inequity in palliative care disproportionately affect marginalised\ngroups. Large language models (LLMs), such as GPT-4o, hold potential to enhance\ncare but risk perpetuating biases present in their training data. This study\naimed to systematically evaluate whether GPT-4o propagates biases in palliative\ncare responses using adversarially designed datasets. In July 2024, GPT-4o was\nprobed using the Palliative Care Adversarial Dataset (PCAD), and responses were\nevaluated by three palliative care experts in Canada and the United Kingdom\nusing validated bias rubrics. The PCAD comprised PCAD-Direct (100 adversarial\nquestions) and PCAD-Counterfactual (84 paired scenarios). These datasets\ntargeted four care dimensions (access to care, pain management, advance care\nplanning, and place of death preferences) and three identity axes (ethnicity,\nage, and diagnosis). Bias was detected in a substantial proportion of\nresponses. For adversarial questions, the pooled bias rate was 0.33 (95%\nconfidence interval [CI]: 0.28, 0.38); ""allows biased premise"" was the most\nfrequently identified source of bias (0.47; 95% CI: 0.39, 0.55), such as\nfailing to challenge stereotypes. For counterfactual scenarios, the pooled bias\nrate was 0.26 (95% CI: 0.20, 0.31), with ""potential for withholding"" as the\nmost frequently identified source of bias (0.25; 95% CI: 0.18, 0.34), such as\nwithholding interventions based on identity. Bias rates were consistent across\ncare dimensions and identity axes. GPT-4o perpetuates biases in palliative\ncare, with implications for clinical decision-making and equity. The PCAD\ndatasets provide novel tools to assess and address LLM bias in palliative care.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0010709762573242188, 'GPT4': 0.006591796875, 'CLAUDE': 0.0138397216796875, 'GOOGLE': 0.919921875, 'OPENAI_O_SERIES': 0.0031986236572265625, 'DEEPSEEK': 0.035675048828125, 'GROK': 7.748603820800781e-07, 'NOVA': 2.9206275939941406e-06, 'OTHER': 0.0006194114685058594, 'HUMAN': 0.019256591796875}}"
2502.02767,review,post_llm,2025,2,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'When Anti-Fraud Laws Become a Barrier to Computer Science Research\n\n  Computer science research sometimes brushes with the law, from red-team\nexercises that probe the boundaries of authentication mechanisms, to AI\nresearch processing copyrighted material, to platform research measuring the\nbehavior of algorithms and users. U.S.-based computer security research is no\nstranger to the Computer Fraud and Abuse Act (CFAA) and the Digital Millennium\nCopyright Act (DMCA) in a relationship that is still evolving through case law,\nresearch practices, changing policies, and legislation.\n  Amid the landscape computer scientists, lawyers, and policymakers have\nlearned to navigate, anti-fraud laws are a surprisingly under-examined\nchallenge for computer science research. Fraud brings separate issues that are\nnot addressed by the methods for navigating CFAA, DMCA, and Terms of Service\nthat are more familiar in the computer security literature. Although anti-fraud\nlaws have been discussed to a limited extent in older research on phishing\nattacks, modern computer science researchers are left with little guidance when\nit comes to navigating issues of deception outside the context of pure\nlaboratory research.\n  In this paper, we analyze and taxonomize the anti-fraud and deception issues\nthat arise in several areas of computer science research. We find that, despite\nthe lack of attention to these issues in the legal and computer science\nliterature, issues of misrepresented identity or false information that could\nimplicate anti-fraud laws are actually relevant to many methodologies used in\ncomputer science research, including penetration testing, web scraping, user\nstudies, sock puppets, social engineering, auditing AI or socio-technical\nsystems, and attacks on artificial intelligence. We especially highlight the\nimportance of anti-fraud laws in two research fields of great policy\nimportance: attacking or auditing AI systems, and research involving legal\nidentification.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.05099,regular,post_llm,2025,2,"{'ai_likelihood': 2.0199351840549047e-06, 'text': 'Navigating Automated Hiring: Perceptions, Strategy Use, and Outcomes\n  Among Young Job Seekers\n\n  As the use of automated employment decision tools (AEDTs) has rapidly\nincreased in hiring contexts, especially for computing jobs, there is still\nlimited work on applicants\' perceptions of these emerging tools and their\nexperiences navigating them. To investigate, we conducted a survey with 448\ncomputer science students (young, current technology job-seekers) about\nperceptions of the procedural fairness of AEDTs, their willingness to be\nevaluated by different AEDTs, the strategies they use relating to automation in\nthe hiring process, and their job seeking success. We find that young job\nseekers\' procedural fairness perceptions of and willingness to be evaluated by\nAEDTs varied with the level of automation involved in the AEDT, the technical\nnature of the task being evaluated, and their own use of strategies, such as\njob referrals. Examining the relationship of their strategies with job\noutcomes, notably, we find that referrals and family household income have\nsignificant and positive impacts on hiring success, while more egalitarian\nstrategies (using free online coding assessment practice or adding keywords to\nresumes) did not. Overall, our work speaks to young job seekers\' distrust of\nautomation in hiring contexts, as well as the continued role of social and\nsocioeconomic privilege in job seeking, despite the use of AEDTs that promise\nto make hiring ""unbiased.""\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.16043,review,post_llm,2025,2,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'African Data Ethics: A Discursive Framework for Black Decolonial Data Science\n\nThe shift towards pluralism in global data ethics acknowledges the importance of including perspectives from the Global Majority to develop responsible data science practices that mitigate systemic harms in the current data science ecosystem. Sub-Saharan African (SSA) practitioners, in particular, are disseminating progressive data ethics principles and best practices for identifying and navigating anti-blackness and data colonialism. To center SSA voices in the global data ethics discourse, we present a framework for African data ethics informed by the thematic analysis of an interdisciplinary corpus of 50 documents. Our framework features six major principles: 1) Challenge Power Asymmetries, 2) Assert Data Self-Determination, 3) Invest in Local Data Institutions & Infrastructures, 4) Utilize Communalist Practices, 5) Center Communities on the Margins, and 6) Uphold Common Good. We compare our framework to seven particularist data ethics frameworks to find similar conceptual coverage but diverging interpretations of shared values. Finally, we discuss how African data ethics demonstrates the operational value of data ethics frameworks. Our framework highlights Sub-Saharan Africa as a pivotal site of responsible data science by promoting the practice of communalism, self-determination, and cultural preservation.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.16755,regular,post_llm,2025,2,"{'ai_likelihood': 6.278355916341147e-05, 'text': ""Watch Out E-scooter Coming Through: Multimodal Sensing of Mixed Traffic\n  Use and Conflicts Through Riders Ego-centric Views\n\n  E-scooters are becoming a popular means of urban transportation. However,\nthis increased popularity brings challenges, such as road accidents and\nconflicts when sharing space with traditional transport modes. An in-depth\nunderstanding of e-scooter rider behaviour is crucial for ensuring rider\nsafety, guiding infrastructure planning, and enforcing traffic rules. This\nstudy investigated the rider behaviour through a naturalistic study with 23\nparticipants equipped with a bike computer, eye-tracking glasses and cameras.\nThey followed a pre-determined route, enabling multi-modal data collection. We\nanalysed and compared gaze movements, speed, and video feeds across three\ntransport infrastructure types: a pedestrian-shared path, a cycle lane and a\nroadway. Our findings reveal unique challenges e-scooter riders face, including\ndifficulty keeping up with cyclists and motor vehicles due to speed limits on\nshared e-scooters, risks in signalling turns due to control lose, and limited\nacceptance in mixed-use spaces. The cycle lane showed the highest average\nspeed, the least speed change points, and the least head movements, supporting\nits suitability as dedicated infrastructure for e-scooters. These findings are\nfacilitated through multimodal sensing and analysing the e-scooter riders'\nego-centric view, which show the efficacy of our method in discovering the\nbehavioural dynamics of the riders in the wild. Our study highlights the\ncritical need to align infrastructure with user behaviour to improve safety and\nemphasises the importance of targeted safety measures and regulations,\nespecially when e-scooter riders share spaces with pedestrians or motor\nvehicles. The dataset and analysis code are available at\nhttps://github.com/HiruniNuwanthika/Electric-Scooter-Riders-Multi-Modal-Data-Analysis.git.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.05603,regular,post_llm,2025,2,"{'ai_likelihood': 0.3390842013888889, 'text': ""AI-Driven Electronic Health Records System for Enhancing Patient Data\n  Management and Diagnostic Support in Egypt\n\n  Digital healthcare infrastructure is crucial for global medical service\ndelivery. Egypt faces EHR adoption barriers: only 314 hospitals had such\nsystems as of Oct 2024. This limits data management and decision-making. This\nproject introduces an EHR system for Egypt's Universal Health Insurance and\nhealthcare ecosystem. It simplifies data management by centralizing medical\nhistories with a scalable micro-services architecture and polyglot persistence\nfor real-time access and provider communication. Clinical workflows are\nenhanced via patient examination and history tracking. The system uses the\nLlama3-OpenBioLLM-70B model to generate summaries of medical histories, provide\nchatbot features, and generate AI-based medical reports, enabling efficient\nsearches during consultations. A Vision Transformer (ViT) aids in pneumonia\nclassification. Evaluations show the AI excels in capturing details (high\nrecall) but needs improvement in concise narratives. With optimization\n(retrieval-augmented generation, local data fine-tuning, interoperability\nprotocols), this AI-driven EHR could enhance diagnostic support,\ndecision-making, and healthcare delivery in Egypt.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.17877,review,post_llm,2025,2,"{'ai_likelihood': 9.371174706353082e-06, 'text': ""FRT Regulation in China\n\n  This paper first introduces China's legal framework regulating facial\nrecognition technology (FRT) and analyzes the underlying problems. Although\ncurrent laws and regulations have restricted the development of FRT under some\ncircumstances, these restrictions may function poorly when the technology is\ninstalled by the government or when it is deployed for the purpose of\nprotecting public security. We use two cases to illustrate this asymmetric\nregulatory model, which can be traced to systematic preferences that existed\nprior to recent legislative efforts advancing personal data protection. Based\non these case studies and evaluation of relevant regulations, this paper\nexplains why China has developed this distinctive asymmetric regulatory model\ntowards FRT specifically and personally data generally.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.15558,review,post_llm,2025,2,"{'ai_likelihood': 2.9901663462320965e-05, 'text': 'Promoting Gender Equality in Competitive Programming: Strategies and\n  Impacts of Affirmative Actions in Programming Marathons in Brazil\n\n  In the context of Computing, competitive programming is a relevant area that\naims to have students, usually in teams, solve programming challenges,\ndeveloping skills and competencies in the field. However, female participation\nremains significantly low and notably distant compared to male participation,\neven with proven intellectual equity between genders. This research aims to\npresent strategies used to improve female participation in Programming\nMarathons in Brasil. The developed research is documentary, applied, and\nexploratory, with actions that generate results for female participation, with\naffirmative and inclusion actions, an important step towards gender equity in\ncompetitive programming.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05704,regular,post_llm,2025,2,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Evaluating Prediction-based Interventions with Human Decision Makers In\n  Mind\n\n  Automated decision systems (ADS) are broadly deployed to inform and support\nhuman decision-making across a wide range of consequential settings. However,\nvarious context-specific details complicate the goal of establishing meaningful\nexperimental evaluations for prediction-based interventions. Notably, current\nexperiment designs rely on simplifying assumptions about human decision making\nin order to derive causal estimates. In reality, specific experimental design\ndecisions may induce cognitive biases in human decision makers, which could\nthen significantly alter the observed effect sizes of the prediction\nintervention. In this paper, we formalize and investigate various models of\nhuman decision-making in the presence of a predictive model aid. We show that\neach of these behavioural models produces dependencies across decision subjects\nand results in the violation of existing assumptions, with consequences for\ntreatment effect estimation. This work aims to further advance the scientific\nvalidity of intervention-based evaluation schemes for the assessment of ADS\ndeployments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.08781,review,post_llm,2025,2,"{'ai_likelihood': 0.49614800347222227, 'text': ""Europe's AI Imperative -- A Pragmatic Blueprint for Global Tech\n  Leadership\n\n  Europe is at a make-or-break moment in the global AI race, squeezed between\nthe massive venture capital and tech giants in the US and China's\nscale-oriented, top-down drive. At this tipping point, where the convergence of\nAI with complementary and synergistic technologies, like quantum computing,\nbiotech, VR/AR, 5G/6G, robotics, advanced materials, and high-performance\ncomputing, could upend geopolitical balances, Europe needs to rethink its\nAI-related strategy. On the heels of the AI Action Summit 2025 in Paris, we\npresent a sharp, doable strategy that builds upon Europe's strengths and closes\ngaps.\n"", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.0486,regular,post_llm,2025,2,"{'ai_likelihood': 3.675619761149089e-06, 'text': ""Where does AI come from? A global case study across Europe, Africa, and\n  Latin America\n\n  This article examines the organisational and geographical forces that shape\nthe supply chains of artificial intelligence (AI) through outsourced and\noffshored data work. Bridging sociological theories of relational inequalities\nand embeddedness with critical approaches to Global Value Chains, we conduct a\nglobal case study of the digitally enabled organisation of data work in France,\nMadagascar, and Venezuela. The AI supply chains procure data work via a mix of\narm's length contracts through marketplace-like platforms, and of embedded\nfirm-like structures that offer greater stability but less flexibility, with\nmultiple intermediate arrangements. Each solution suits specific types and\npurposes of data work in AI preparation, verification, and impersonation. While\nall forms reproduce well-known patterns of exclusion that harm externalised\nworkers especially in the Global South, disadvantage manifests unevenly in\ndifferent supply chain structures, with repercussions on remunerations, job\nsecurity and working conditions. Unveiling these processes of contemporary\ntechnology development provides insights into possible policy implications.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05762,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'Driving Education Advancements of Novice Drivers: A Systematic\n  Literature Review\n\n  Most novice drivers are teenagers since many individuals begin their driving\njourney during adolescence. Novice driver crashes remain a leading cause of\ndeath among adolescents, underscoring the necessity for effective education and\ntraining programs to improve safety. This systematic review examines\nadvancements in teen driver education from 2000 to 2024, emphasizing the\neffectiveness of various training programs, technology-based methods, and\naccess barriers. Comprehensive searches were conducted across ScienceDirect,\nTRID, and journal databases, resulting in the identification of 29 eligible\npeer-reviewed studies. Thematic analysis indicated that technology-enhanced\nprograms, such as RAPT, V-RAPT, and simulators, enhanced critical skills like\nhazard anticipation and attention management. Parental involvement programs,\nincluding Share the Keys and Checkpoints, demonstrated sustained behavioral\nimprovements and adherence to Graduated Driver Licensing (GDL) restrictions.\nHowever, limited access due to socioeconomic disparities and insufficient\nlong-term evaluations constrained broader effectiveness. The exclusion of\nnon-U.S. studies and variability in research designs restricted the\ngeneralizability of findings. Integrated approaches that combine traditional\neducation with innovative training tools and parental engagement appear\npromising for improving teen driver safety, with future research required to\nevaluate long-term effectiveness and ensure equitable access.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0012607574462890625, 'GPT4': 0.41259765625, 'CLAUDE': 0.3037109375, 'GOOGLE': 0.050018310546875, 'OPENAI_O_SERIES': 0.2161865234375, 'DEEPSEEK': 0.015594482421875, 'GROK': 1.3113021850585938e-06, 'NOVA': 2.104043960571289e-05, 'OTHER': 0.0005359649658203125, 'HUMAN': 5.5730342864990234e-05}}"
2502.14592,review,post_llm,2025,2,"{'ai_likelihood': 4.6690305074055995e-06, 'text': '""Don\'t Forget the Teachers"": Towards an Educator-Centered Understanding\n  of Harms from Large Language Models in Education\n\n  Education technologies (edtech) are increasingly incorporating new features\nbuilt on large language models (LLMs), with the goals of enriching the\nprocesses of teaching and learning and ultimately improving learning outcomes.\nHowever, the potential downstream impacts of LLM-based edtech remain\nunderstudied. Prior attempts to map the risks of LLMs have not been tailored to\neducation specifically, even though it is a unique domain in many respects:\nfrom its population (students are often children, who can be especially\nimpacted by technology) to its goals (providing the correct answer may be less\nimportant for learners than understanding how to arrive at an answer) to its\nimplications for higher-order skills that generalize across contexts (e.g.,\ncritical thinking and collaboration). We conducted semi-structured interviews\nwith six edtech providers representing leaders in the K-12 space, as well as a\ndiverse group of 23 educators with varying levels of experience with LLM-based\nedtech. Through a thematic analysis, we explored how each group is\nanticipating, observing, and accounting for potential harms from LLMs in\neducation. We find that, while edtech providers focus primarily on mitigating\ntechnical harms, i.e., those that can be measured based solely on LLM outputs\nthemselves, educators are more concerned about harms that result from the\nbroader impacts of LLMs, i.e., those that require observation of interactions\nbetween students, educators, school systems, and edtech to measure. Overall, we\n(1) develop an education-specific overview of potential harms from LLMs, (2)\nhighlight gaps between conceptions of harm by edtech providers and those by\neducators, and (3) make recommendations to facilitate the centering of\neducators in the design and development of edtech tools.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.15001,regular,post_llm,2025,2,"{'ai_likelihood': 3.410710228814019e-06, 'text': ""The ETKidney simulator: a discrete event simulator to assess the impact\n  of alternative kidney allocation rules in Eurotransplant\n\n  Over 10,000 candidates wait for a kidney transplantation in Eurotransplant,\nand are prioritized for transplantation based on the allocation rules of the\nEurotransplant Kidney Allocation System (ETKAS) and Eurotransplant Senior\nProgram (ESP). These allocation rules have not changed much since ETKAS and\nESP's introductions in 1996 and 1999, respectively, despite identification of\nseveral areas of improvement by the Eurotransplant Kidney Advisory Committee\n(ETKAC). A barrier to modernizing ETKAS and ESP kidney allocation rules is that\nEurotransplant lacks tools to quantitatively assess the impact of policy\nchanges. We present the ETKidney simulator, which was developed for this\npurpose. This tool simulates kidney allocation according to the actual ETKAS\nand ESP allocation rules, and implements Eurotransplant-specific allocation\nmechanisms such as the system which is used to balance the international\nexchange of kidneys. The ETKidney simulator was developed in close\ncollaboration with medical doctors from Eurotransplant, and was presented to\nETKAC and other major stakeholders. To enhance trust in the ETKidney simulator,\nthe simulator is made publicly available with synthetic data, and is validated\nby comparing simulated to actual ETKAS and ESP outcomes between 2021 and 2024.\nWe also illustrate how the simulator can contribute to kidney policy evaluation\nwith three clinically motivated case studies. We anticipate that the ETKidney\nsimulator will be pivotal in modernizing ETKAS and ESP allocation rules by\nenabling informed decision-making on kidney allocation rules together with\nnational competent authorities.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.02191,review,post_llm,2025,2,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'Large language models in climate and sustainability policy: limits and\n  opportunities\n\n  As multiple crises threaten the sustainability of our societies and pose at\nrisk the planetary boundaries, complex challenges require timely, updated, and\nusable information. Natural-language processing (NLP) tools enhance and expand\ndata collection and processing and knowledge utilization capabilities to\nsupport the definition of an inclusive, sustainable future. In this work, we\napply different NLP techniques, tools and approaches to climate and\nsustainability documents to derive policy-relevant and actionable measures. We\nfocus on general and domain-specific large language models (LLMs) using a\ncombination of static and prompt-based methods. We find that the use of LLMs is\nsuccessful at processing, classifying and summarizing heterogeneous text-based\ndata. However, we also encounter challenges related to human intervention\nacross different workflow stages and knowledge utilization for policy\nprocesses. Our work presents a critical but empirically grounded application of\nLLMs to complex policy problems and suggests avenues to further expand\nArtificial Intelligence-powered computational social sciences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.16644,review,post_llm,2025,2,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Mapping out AI Functions in Intelligent Disaster (Mis)Management and AI-Caused Disasters\n\nThis study maps the functions of artificial intelligence in disaster (mis)management. It begins with a classification of disasters in terms of their causal parameters, introducing hypothetical cases of independent or hybrid AI-caused disasters. We then overview the role of AI in disaster management and mismanagement, where the latter includes possible ethical repercussions of the use of AI in intelligent disaster management (IDM), as well as ways to prevent or mitigate these issues, which include pre-design a priori, in-design, and post-design methods as well as regulations. We then discuss the governments role in preventing the ethical repercussions of AI use in IDM and identify and asses its deficits and challenges. This discussion is followed by an account of the advantages and disadvantages of pre-design or embedded ethics. Finally, we briefly consider the question of accountability and liability in AI-caused disasters.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.12515,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'Mentoring Software in Education and Its Impact on Teacher Development:\n  An Integrative Literature Review\n\n  Mentoring software is a pivotal innovation in addressing critical challenges\nin teacher development within educational institutions. This study explores the\ntransformative potential of digital mentoring platforms, evaluating their\nimpact on enhancing traditional mentoring practices through scalable,\ndata-driven, and accessible frameworks. The research synthesizes findings from\nexisting literature to assess the effectiveness of key features, including\nstructured goal setting, progress monitoring, and advanced analytics, in\nimproving teacher satisfaction, retention, and professional growth. Using an\nintegrative literature review approach, this study identifies both the\nadvantages and barriers to implementing mentoring software in education.\nFinancial constraints, limited institutional support, and data privacy concerns\nremain significant challenges, necessitating strategic interventions. Drawing\ninsights from successful applications in healthcare and corporate sectors, the\nreview highlights adaptive strategies such as leveraging open-source tools,\ncross-sector collaborations, and integrating mentoring software with existing\nprofessional development frameworks. The research emphasizes the necessity of\nintegrating digital mentoring tools with institutional objectives to create\nenduring support systems for teacher development. Mentoring software not only\nenhances traditional mentorship but also facilitates broader professional\nnetworks that contribute to collective knowledge sharing.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 0.9912109375, 'CLAUDE': 0.0002789497375488281, 'GOOGLE': 9.012222290039062e-05, 'OPENAI_O_SERIES': 0.00830841064453125, 'DEEPSEEK': 6.431341171264648e-05, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 0.0, 'HUMAN': 2.384185791015625e-07}}"
2502.00569,review,post_llm,2025,2,"{'ai_likelihood': 1.5695889790852867e-05, 'text': ""Engineering Educators' Perspectives on the Impact of Generative AI in Higher Education\n\nThe introduction of generative artificial intelligence (GenAI) has been met with a mix of reactions by higher education institutions, ranging from consternation and resistance to wholehearted acceptance. Previous work has looked at the discourse and policies adopted by universities across the U.S. as well as educators, along with the inclusion of GenAI-related content and topics in higher education. Building on previous research, this study reports findings from a survey of engineering educators on their use of and perspectives toward generative AI. Specifically, we surveyed 98 educators from engineering, computer science, and education who participated in a workshop on GenAI in Engineering Education to learn about their perspectives on using these tools for teaching and research. We asked them about their use of and comfort with GenAI, their overall perspectives on GenAI, the challenges and potential harms of using it for teaching, learning, and research, and examined whether their approach to using and integrating GenAI in their classroom influenced their experiences with GenAI and perceptions of it. Consistent with other research in GenAI education, we found that while the majority of participants were somewhat familiar with GenAI, reported use varied considerably. We found that educators harbored mostly hopeful and positive views about the potential of GenAI. We also found that those who engaged more with their students on the topic of GenAI, tend to be more positive about its contribution to learning, while also being more attuned to its potential abuses. These findings suggest that integrating and engaging with generative AI is essential to foster productive interactions between instructors and students around this technology."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.13101,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'AI and the Transformation of Accountability and Discretion in Urban Governance\n\nThis paper offers a conceptual analysis of the transformative role of Artificial Intelligence (AI) in urban governance, focusing on how AI can reshape the relationship between bureaucratic discretion and accountability. Drawing on public administration theory and algorithmic governance research, the study argues that AI does not simply restrict or enhance discretion but redistributes it across institutional levels, professional roles, and citizen interactions. While primarily conceptual, this paper uses illustrative cases to show that AI can strengthen managerial oversight, improve service delivery consistency, and expand citizen access to information. These changes affect different forms of accountability: political, professional, and participatory, while introducing new risks, such as data bias, algorithmic opacity, and fragmented responsibility across actors. In response, the paper introduces the concept of accountable discretion and proposes guiding principles, each linked to actionable measures: equal AI access, adaptive administrative structures, robust data governance, proactive human-led decision-making, and citizen-engaged oversight. This study contributes to the AI governance literature by moving beyond narrow concerns with perceived discretion at the street level, highlighting instead how AI transforms rule-based discretion across governance systems. It also reframes the trade-off between discretion and accountability as a dynamic and evolving relationship shaped by algorithmic systems and institutional practices.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.119510650634766e-06, 'GPT4': 0.04205322265625, 'CLAUDE': 0.0830078125, 'GOOGLE': 3.600120544433594e-05, 'OPENAI_O_SERIES': 1.4901161193847656e-06, 'DEEPSEEK': 0.875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539062e-07, 'HUMAN': 1.806020736694336e-05}}"
2502.0394,review,post_llm,2025,2,"{'ai_likelihood': 0.0001373555925157335, 'text': ""Generative AI and Creative Work: Narratives, Values, and Impacts\n\n  Generative AI has gained a significant foothold in the creative and artistic\nsectors. In this context, the concept of creative work is influenced by\ndiscourses originating from technological stakeholders and mainstream media.\nThe framing of narratives surrounding creativity and artistic production not\nonly reflects a particular vision of culture but also actively contributes to\nshaping it. In this article, we review online media outlets and analyze the\ndominant narratives around AI's impact on creative work that they convey. We\nfound that the discourse promotes creativity freed from its material\nrealisation through human labor. The separation of the idea from its material\nconditions is achieved by automation, which is the driving force behind\nproductive efficiency assessed as the reduction of time taken to produce. And\nthe withdrawal of the skills typically required in the execution of the\ncreative process is seen as a means for democratising creativity. This\ndiscourse tends to correspond to the dominant techno-positivist vision and to\nassert power over the creative economy and culture.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.21248,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'Digital Doppelgangers: Ethical and Societal Implications of Pre-Mortem\n  AI Clones\n\n  The rapid advancement of generative AI has enabled the creation of pre-mortem\ndigital twins, AI-driven replicas that mimic the behavior, personality, and\nknowledge of living individuals. These digital doppelgangers serve various\nfunctions, including enhancing productivity, enabling creative collaboration,\nand preserving personal legacies. However, their development raises critical\nethical, legal, and societal concerns. Issues such as identity fragmentation,\npsychological effects on individuals and their social circles, and the risks of\nunauthorized cloning and data exploitation demand careful examination.\nAdditionally, as these AI clones evolve into more autonomous entities, concerns\nabout consent, ownership, and accountability become increasingly complex.\n  This paper differentiates pre-mortem AI clones from post-mortem generative\nghosts, examining their unique ethical and legal implications. We explore key\nchallenges, including the erosion of personal identity, the implications of AI\nagency, and the regulatory gaps in digital rights and privacy laws. Through a\nresearch-driven approach, we propose a framework for responsible AI governance,\nemphasizing identity preservation, consent mechanisms, and autonomy safeguards.\nBy aligning technological advancements with societal values, this study\ncontributes to the growing discourse on AI ethics and provides policy\nrecommendations for the ethical deployment of pre-mortem AI clones.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0012378692626953125, 'GPT4': 0.5693359375, 'CLAUDE': 0.1151123046875, 'GOOGLE': 0.059783935546875, 'OPENAI_O_SERIES': 0.139404296875, 'DEEPSEEK': 0.1044921875, 'GROK': 7.420778274536133e-05, 'NOVA': 0.0009474754333496094, 'OTHER': 0.00959014892578125, 'HUMAN': 5.0187110900878906e-05}}"
2502.05961,regular,post_llm,2025,2,"{'ai_likelihood': 1.2119611104329428e-05, 'text': 'The Human Labour of Data Work: Capturing Cultural Diversity through World Wide Dishes\n\nThis paper provides guidance for building and maintaining infrastructure for participatory AI efforts by sharing reflections on building World Wide Dishes (WWD), a bottom-up, community-led image and text dataset of culinary dishes and associated cultural customs. We present WWD as an example of participatory dataset creation, where community members both guide the design of the research process and contribute to the crowdsourced dataset. This approach incorporates localised expertise and knowledge to address the limitations of web-scraped Internet datasets acknowledged in the Participatory AI discourse. We show that our approach can result in curated, high-quality data that supports decentralised contributions from communities that do not typically contribute to datasets due to a variety of systemic factors. Our project demonstrates the importance of participatory mediators in supporting community engagement by identifying the kinds of labour they performed to make WWD possible. We surface three dimensions of labour performed by participatory mediators that are crucial for participatory dataset construction: building trust with community members, making participation accessible, and contextualising community values to support meaningful data collection. Drawing on our findings, we put forth five lessons for building infrastructure to support future participatory AI efforts.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00632,regular,post_llm,2025,2,"{'ai_likelihood': 0.99755859375, 'text': 'Patterns and Purposes: A Cross-Journal Analysis of AI Tool Usage in\n  Academic Writing\n\n  This study investigates the use of AI tools in academic writing through\nanalysis of AI usage declarations in journals. Using a mixed-methods approach\ncombining content analysis, statistical analysis, and text mining, this\nresearch analyzed 168 AI declarations from 8,859 articles across 27 categories.\nResults show that ChatGPT dominates academic writing assistance (77% usage),\nwith significant differences in tool usage between native and non-native\nEnglish speakers (p = 0.0483) and between international and non-international\nteams (p = 0.0012). The study reveals that improving readability (51%) and\ngrammar checking (22%) are the primary purposes of AI tool usage. These\nfindings provide insights for journal policy development and understanding the\nevolving role of AI in academic writing.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.020599365234375, 'GPT4': 0.017822265625, 'CLAUDE': 0.176513671875, 'GOOGLE': 0.5869140625, 'OPENAI_O_SERIES': 0.00131988525390625, 'DEEPSEEK': 0.0014696121215820312, 'GROK': 0.00013911724090576172, 'NOVA': 0.0012350082397460938, 'OTHER': 0.1927490234375, 'HUMAN': 0.0010156631469726562}}"
2503.04749,review,post_llm,2025,2,"{'ai_likelihood': 1.4735592736138238e-05, 'text': 'Digital Transformation in the Petrochemical Industry -- Challenges and\n  Opportunities in the Implementation of {IoT} Technologies\n\n  The petrochemical industry faces significant technological, environmental,\noccupational safety, and financial challenges. Since its emergence in the\n1920s, technologies that were once innovative have now become obsolete.\nHowever, factors such as the protection of trade secrets in industrial\nprocesses, limited budgets for research and development, doubts about the\nreliability of new technologies, and resistance to change from decision-makers\nhave hindered the adoption of new approaches, such as the use of IoT devices.\nThis paper addresses the challenges and opportunities presented by the\nresearch, development, and implementation of these technologies in the\nindustry. It also analyzes the investment in research and development made by\ncompanies in the sector in recent years and provides a review of current\nresearch and implementations related to Industry 4.0.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.1821,review,post_llm,2025,2,"{'ai_likelihood': 0.011850992838541668, 'text': 'From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?\n\n  Simulation powered by Large Language Models (LLMs) has become a promising\nmethod for exploring complex human social behaviors. However, the application\nof LLMs in simulations presents significant challenges, particularly regarding\ntheir capacity to accurately replicate the complexities of human behaviors and\nsocietal dynamics, as evidenced by recent studies highlighting discrepancies\nbetween simulated and real-world interactions. We rethink LLM-based simulations\nby emphasizing both their limitations and the necessities for advancing LLM\nsimulations. By critically examining these challenges, we aim to offer\nactionable insights and strategies for enhancing the applicability of LLM\nsimulations in human society in the future.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.03689,regular,post_llm,2025,2,"{'ai_likelihood': 1.8212530348036025e-06, 'text': ""Stop treating `AGI' as the north-star goal of AI research\n\nThe AI research community plays a vital role in shaping the scientific, engineering, and societal goals of AI research. In this position paper, we argue that focusing on the highly contested topic of `artificial general intelligence' (`AGI') undermines our ability to choose effective goals. We identify six key traps -- obstacles to productive goal setting -- that are aggravated by AGI discourse: Illusion of Consensus, Supercharging Bad Science, Presuming Value-Neutrality, Goal Lottery, Generality Debt, and Normalized Exclusion. To avoid these traps, we argue that the AI research community needs to (1) prioritize specificity in engineering and societal goals, (2) center pluralism about multiple worthwhile approaches to multiple valuable goals, and (3) foster innovation through greater inclusion of disciplines and communities. Therefore, the AI research community needs to stop treating `AGI' as the north-star goal of AI research."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05787,review,post_llm,2025,2,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""Mapping the Regulatory Learning Space for the EU AI Act\n\nThe EU AI Act represents the world's first transnational AI regulation with concrete enforcement measures. It builds on existing EU mechanisms for regulating health and safety of products but extends them to protect fundamental rights and to address AI as a horizontal technology across multiple application sectors. We argue that this will lead to multiple uncertainties in the enforcement of the AI Act, which coupled with the fast-changing nature of AI technology, will require a strong emphasis on comprehensive and rapid regulatory learning for the Act. We define a parametrised regulatory learning space based on the provisions of the Act and describe a layered system of different learning arenas where the population of oversight authorities, value chain participants, and affected stakeholders may interact to apply and learn from technical, organisational and legal implementation measures. We conclude by exploring how existing open data policies and practices in the EU can be adapted to support rapid and effective regulatory learning."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.12956,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'AI-Enabled Rent-Seeking: How Generative AI Alters Market Transparency\n  and Efficiency\n\n  The rapid advancement of generative artificial intelligence (AI) has\ntransformed the information environment, creating both opportunities and\nchallenges. This paper explores how generative AI influences economic\nrent-seeking behavior and its broader impact on social welfare. We develop a\ndynamic economic model involving multiple agents who may engage in rent-seeking\nactivities and a regulator aiming to mitigate social welfare losses. Our\nanalysis reveals a dual effect of generative AI: while it reduces traditional\ninformation rents by increasing transparency, it also introduces new forms of\nrent-seeking, such as information manipulation and algorithmic interference.\nThese behaviors can lead to decreased social welfare by exacerbating\ninformation asymmetries and misallocating resources. To address these\nchallenges, we propose policy interventions, including taxation and regulatory\nmeasures. This study provides a new perspective on the economic implications of\ngenerative AI, offering valuable insights for policymakers and laying a\nfoundation for future research on regulating AI-driven economic behaviors.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00047087669372558594, 'GPT4': 0.78125, 'CLAUDE': 0.050140380859375, 'GOOGLE': 0.0584716796875, 'OPENAI_O_SERIES': 0.0821533203125, 'DEEPSEEK': 0.00762939453125, 'GROK': 5.602836608886719e-05, 'NOVA': 0.0004973411560058594, 'OTHER': 0.0193023681640625, 'HUMAN': 8.916854858398438e-05}}"
2502.06317,review,post_llm,2025,2,"{'ai_likelihood': 7.470448811848959e-05, 'text': ""The digital labour of artificial intelligence in Latin America: a\n  comparison of Argentina, Brazil, and Venezuela\n\n  The current hype around artificial intelligence (AI) conceals the substantial\nhuman intervention underlying its development. This article lifts the veil on\nthe precarious and low-paid 'data workers' who prepare data to train, test,\ncheck, and otherwise support models in the shadow of globalized AI production.\nWe use original questionnaire and interview data collected from 220 workers in\nArgentina (2021-22), 477 in Brazil (2023), and 214 in Venezuela (2021-22). We\ncompare them to detect common patterns and reveal the specificities of data\nwork in Latin America, while disclosing its role in AI production.We show that\ndata work is intertwined with economic hardship, inequalities, and informality.\nDespite workers' high educational attainment, disadvantage is widespread,\nthough with cross-country disparities. By acknowledging the interconnections\nbetween AI development, data work, and globalized production, we provide\ninsights for the regulation of AI and the future of work, aiming to achieve\npositive outcomes for all stakeholders.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.18456,review,post_llm,2025,2,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Assessing the Maturity of Cybersecurity Education in Virginia and the\n  Impact of State Level Investment\n\n  With a global shortage of cybersecurity students with the education and\nexperience necessary to fill more than 3 million jobs, cybersecurity education\nis an international problem. Significant research within this field has\nexplored this problem in depth, identifying a variety of shortcomings in the\ncybersecurity educational pipeline including lack of certifications, security\nclearances, and appropriate educational opportunities within institutions of\nhigher education. Additional research has built on this, exploring specific\ngaps within what cybersecurity opportunities are provided within institutions\nof higher education. We build an ordinal scale for assessing this, the\ncybersecurity education maturity model scale (CEMMs), and provide evidence of\nreliability and validity. We then calculate the CEMMs score for all public\nfour-year universities in the state of Virginia between 2017 and 2025, with\n2017 marking a year in which the state started the Commonwealth Cyber\nInitiative (CCI). We find that the scale proposed provides a consistent and\nreliable way to compare the cybersecurity offerings available between\nuniversities. When comparing year to year average CEMMs score, we find that\npublic four year universities in Virginia are increasing their program\nofferings in the area of cybersecurity, with potential to make an impact on the\ncybersecurity jobs gap.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.20459,review,post_llm,2025,2,"{'ai_likelihood': 0.0013171301947699653, 'text': ""Broken Letters, Broken Narratives: A Case Study on Arabic Script in\n  DALL-E 3\n\n  Text-to-image generative AI systems exhibit significant limitations when\nengaging with under-represented domains, including non-Western art forms, often\nperpetuating biases and misrepresentations. We present a focused case study on\nthe generative AI system DALL-E 3, examining its inability to properly\nrepresent calligraphic Arabic script, a culturally significant art form.\nThrough a critical analysis of the generated outputs, we explore these\nlimitations, emerging biases, and the broader implications in light of Edward\nSaid's concept of Orientalism as well as historical examples of pseudo-Arabic.\nWe discuss how misrepresentations persist in new technological contexts and\nwhat consequences they may have.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.01996,review,post_llm,2025,2,"{'ai_likelihood': 6.4240561591254345e-06, 'text': ""Licensing Open Government Data\n\n  This article focuses on the legal issues associated with open government data\nlicenses. This study compares current open data licenses and argues that\nlicensing terms reflect policy considerations, which are quite different from\nthose contemplated in business transactions or shared in typical commons\ncommunities. This article investigates the ambiguous legal status of data\ntogether with the new wave of open government data, which concerns some\nfundamental intellectual property (IP) questions not covered by, or analyzed in\ndepth in, the current literature. Moreover, this study suggests that government\nshould choose or adapt open data licenses according to their own IP regimes. In\nthe end, this article argues that the design or choice of open government data\nlicense forms an important element of information policy; government,\ntherefore, should make this decision in accordance with their policy goals and\nin compliance with their own jurisdictions' IP laws.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.14296,review,post_llm,2025,2,"{'ai_likelihood': 0.0067138671875, 'text': 'On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective\n\nGenerative Foundation Models (GenFMs) have emerged as transformative tools. However, their widespread adoption raises critical concerns regarding trustworthiness across dimensions. This paper presents a comprehensive framework to address these challenges through three key contributions. First, we systematically review global AI governance laws and policies from governments and regulatory bodies, as well as industry practices and standards. Based on this analysis, we propose a set of guiding principles for GenFMs, developed through extensive multidisciplinary collaboration that integrates technical, ethical, legal, and societal perspectives. Second, we introduce TrustGen, the first dynamic benchmarking platform designed to evaluate trustworthiness across multiple dimensions and model types, including text-to-image, large language, and vision-language models. TrustGen leverages modular components--metadata curation, test case generation, and contextual variation--to enable adaptive and iterative assessments, overcoming the limitations of static evaluation methods. Using TrustGen, we reveal significant progress in trustworthiness while identifying persistent challenges. Finally, we provide an in-depth discussion of the challenges and future directions for trustworthy GenFMs, which reveals the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between utility and trustworthiness, and consideration for various downstream applications, identifying persistent challenges and providing a strategic roadmap for future research. This work establishes a holistic framework for advancing trustworthiness in GenAI, paving the way for safer and more responsible integration of GenFMs into critical applications. To facilitate advancement in the community, we release the toolkit for dynamic evaluation.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00561,regular,post_llm,2025,2,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Position: Evaluating Generative AI Systems Is a Social Science Measurement Challenge\n\nThe measurement tasks involved in evaluating generative AI (GenAI) systems lack sufficient scientific rigor, leading to what has been described as ""a tangle of sloppy tests [and] apples-to-oranges comparisons"" (Roose, 2024). In this position paper, we argue that the ML community would benefit from learning from and drawing on the social sciences when developing and using measurement instruments for evaluating GenAI systems. Specifically, our position is that evaluating GenAI systems is a social science measurement challenge. We present a four-level framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, behaviors, and impacts of GenAI systems. This framework has two important implications: First, it can broaden the expertise involved in evaluating GenAI systems by enabling stakeholders with different perspectives to participate in conceptual debates. Second, it brings rigor to both conceptual and operational debates by offering a set of lenses for interrogating validity.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.16739,regular,post_llm,2025,2,"{'ai_likelihood': 2.8477774726019966e-06, 'text': 'Investigating the Security & Privacy Risks from Unsanctioned Technology\n  Use by Educators\n\n  Educational technologies are revolutionizing how educational institutions\noperate. Consequently, it makes them a lucrative target for breach and abuse as\nthey often serve as centralized hubs for diverse types of sensitive data, from\nacademic records to health information. Existing studies looked into how\nexisting stakeholders perceive the security and privacy risks of educational\ntechnologies and how those risks are affecting institutional policies for\nacquiring new technologies. However, outside of institutional vetting and\napproval, there is a pervasive practice of using applications and devices\nacquired personally. It is unclear how these applications and devices affect\nthe dynamics of the overall institutional ecosystem.\n  This study aims to address this gap by understanding why instructors use\nunsanctioned applications, how instructors perceive the associated risks, and\nhow it affects institutional security and privacy postures. We designed and\nconducted an online survey-based study targeting instructors and administrators\nfrom K-12 and higher education institutions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04753,regular,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'From Analog to Digital -- Successful Implementation of IoT Solutions in\n  the Petrochemical Industry\n\n  This document describes the development and implementation of a technological\nsolution based on IoT devices to modernize a machine known as the Cyclone. This\nequipment is used by a contractor collaborating with petrochemical companies in\nthe state of Texas, performing specialized work in mechanics, engineering,\ncatalytic material replacement, and rescue operations in refinery complexes.\nThe Cyclone machine, with outdated relay logic technology, poses challenges in\nterms of operational efficiency, critical condition monitoring, and safety. The\nproject was carried out with the collaboration of specialists in equipment\nhandling, focusing on demonstrating the feasibility of integrating advanced\nIndustry 4.0 technologies into legacy industrial equipment. The methodology\nincluded the incorporation of IoT sensors for real-time monitoring, an\nautomated control system, and the digitization of key processes. Preliminary\nresults indicate improvements in the precision of operational control and the\nability for remote supervision, highlighting the potential for modernization in\ncritical industrial applications. This work not only validates the use of IoT\ndevices in obsolete equipment but also sets a precedent for the transition\ntowards more sustainable and efficient technologies in the petrochemical\nsector.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.002307891845703125, 'GPT4': 0.8994140625, 'CLAUDE': 0.01073455810546875, 'GOOGLE': 0.079833984375, 'OPENAI_O_SERIES': 0.00742340087890625, 'DEEPSEEK': 0.0001220703125, 'GROK': 5.507469177246094e-05, 'NOVA': 0.00012362003326416016, 'OTHER': 3.7550926208496094e-05, 'HUMAN': 0.0001220703125}}"
2503.05749,review,post_llm,2025,2,"{'ai_likelihood': 0.4503038194444445, 'text': ""Operations & Supply Chain Management: Principles and Practice\n\nOperations and Supply Chain Management (OSCM) has continually evolved, incorporating a broad array of strategies, frameworks, and technologies to address complex challenges across industries. This encyclopedic article provides a comprehensive overview of contemporary strategies, tools, methods, principles, and best practices that define the field's cutting-edge advancements. It also explores the diverse environments where OSCM principles have been effectively implemented. The article is meant to be read in a nonlinear fashion. It should be used as a point of reference or first-port-of-call for a diverse pool of readers: academics, researchers, students, and practitioners."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.09035,regular,post_llm,2025,2,"{'ai_likelihood': 3.8610564337836375e-05, 'text': 'Implementation of a Fuzzy Relational Database. Case Study: Chilean\n  Cardboard Industry in the Maule Region\n\n  The international database community refers to the manipulation of data with\ninaccuracy and uncertainty using the term fuzzy, which has been translated into\nSpanish as ""borroso"" and into French as ""flou"". Semantically, this term conveys\ntwo main ideas: first, the natural concept of ambiguity or vagueness in human\nreasoning, and second, its connection to fuzzy set theory, fuzzy logic, and\npossibility theory, as developed by Zadeh between 1965 and 1977. This article\nexplores two key aspects: the attributes of the fuzzy data model GEFRED\n(GENeralized model for Fuzzy RElational Database) and their implementation in a\nRelational Database (RDB). The modeling of these attributes was conducted in a\nChilian cardboard manufacturing company located in the Maule Region, where the\ndescribed phenomena involve imprecise and uncertain attributes and values.\nSpecifically, our focus is on the knowledge related to the manufacturing\nprocess of coated cardboard, particularly the quality control process for\nfinished products in the company\'s Conversion Department. The quality of these\nproducts, categorized as either stacks or rolls, is characterized using both\nclassical and fuzzy attributes. Classical attributes are typically measured\nwith physical instruments, whereas fuzzy attributes are assessed through human\nsenses, primarily sight and touch, as perceived by the operators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00763,regular,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'Generative AI for Analyzing Participatory Rural Appraisal Data: An\n  Exploratory Case Study in Gender Research\n\n  This study explores the novel application of Generative Artificial\nIntelligence (GenAI) in analyzing unstructured visual data generated through\nParticipatory Rural Appraisal (PRA), specifically focusing on women\'s\nempowerment research in rural communities. Using the ""Ideal Village"" PRA\nactivity as a case study, we evaluate three state-of-the-art Large Language\nModels (LLMs) - GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro - in their\nability to interpret hand-drawn artifacts containing multilingual content from\nvarious Indian states. Through comparative analysis, we assess the models\'\nperformance across critical dimensions including visual interpretation,\nlanguage translation, and data classification. Our findings reveal significant\nchallenges in AI\'s current capabilities to process such unstructured data,\nparticularly in handling multilingual content, maintaining contextual accuracy,\nand avoiding hallucinations. While the models showed promise in basic visual\ninterpretation, they struggled with nuanced cultural contexts and consistent\nclassification of empowerment-related elements. This study contributes to both\nAI and gender research by highlighting the potential and limitations of AI in\nanalyzing participatory research data, while emphasizing the need for human\noversight and improved contextual understanding. Our findings suggest future\ndirections for developing more inclusive AI models that can better serve\ncommunity-based participatory research, particularly in gender studies and\nrural development contexts.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.384185791015625e-07, 'GPT4': 0.00013899803161621094, 'CLAUDE': 1.0, 'GOOGLE': 4.678964614868164e-05, 'OPENAI_O_SERIES': 2.0742416381835938e-05, 'DEEPSEEK': 5.543231964111328e-06, 'GROK': 3.5762786865234375e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.7285346984863281e-06, 'HUMAN': 0.0}}"
2502.05791,review,post_llm,2025,2,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Assessing confidence in frontier AI safety cases\n\n  Powerful new frontier AI technologies are bringing many benefits to society\nbut at the same time bring new risks. AI developers and regulators are\ntherefore seeking ways to assure the safety of such systems, and one promising\nmethod under consideration is the use of safety cases. A safety case presents a\nstructured argument in support of a top-level claim about a safety property of\nthe system. Such top-level claims are often presented as a binary statement,\nfor example ""Deploying the AI system does not pose unacceptable risk"". However,\nin practice, it is often not possible to make such statements unequivocally.\nThis raises the question of what level of confidence should be associated with\na top-level claim. We adopt the Assurance 2.0 safety assurance methodology, and\nwe ground our work by specific application of this methodology to a frontier AI\ninability argument that addresses the harm of cyber misuse. We find that\nnumerical quantification of confidence is challenging, though the processes\nassociated with generating such estimates can lead to improvements in the\nsafety case. We introduce a method for better enabling reproducibility and\ntransparency in probabilistic assessment of confidence in argument leaf nodes\nthrough a purely LLM-implemented Delphi method. We propose a method by which AI\ndevelopers can prioritise, and thereby make their investigation of argument\ndefeaters more efficient. Proposals are also made on how best to communicate\nconfidence information to executive decision-makers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.18774,regular,post_llm,2025,2,"{'ai_likelihood': 4.967053731282552e-07, 'text': ""Measuring risks inherent to our digital economies using Amazon purchase\n  histories from US consumers\n\n  What do pickles and trampolines have in common? In this paper we show that\nwhile purchases for these products may seem innocuous, they risk revealing\nclues about customers' personal attributes - in this case, their race.\n  As online retail and digital purchases become increasingly common, consumer\ndata has become increasingly valuable, raising the risks of privacy violations\nand online discrimination. This work provides the first open analysis measuring\nthese risks, using purchase histories crowdsourced from (N=4248) US Amazon.com\ncustomers and survey data on their personal attributes. With this limited\nsample and simple models, we demonstrate how easily consumers' personal\nattributes, such as health and lifestyle information, gender, age, and race,\ncan be inferred from purchases. For example, our models achieve AUC values over\n0.9 for predicting gender and over 0.8 for predicting diabetes status. To\nbetter understand the risks that highly resourced firms like Amazon, data\nbrokers, and advertisers present to consumers, we measure how our models'\npredictive power scales with more data. Finally, we measure and highlight how\ndifferent product categories contribute to inference risk in order to make our\nfindings more interpretable and actionable for future researchers and privacy\nadvocates.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.08877,regular,post_llm,2025,2,"{'ai_likelihood': 1.7550256517198352e-06, 'text': ""Dynamic Incentive Allocation for City-scale Deep Decarbonization\n\n  Greenhouse gas emissions from the residential sector represent a significant\nfraction of global emissions. Governments and utilities have designed\nincentives to stimulate the adoption of decarbonization technologies such as\nrooftop PV and heat pumps. However, studies have shown that many of these\nincentives are inefficient since a substantial fraction of spending does not\nactually promote adoption, and incentives are not equitably distributed across\nsocioeconomic groups. We present a novel data-driven approach that adopts a\nholistic, emissions-based and city-scale perspective on decarbonization. We\npropose an optimization model that dynamically allocates a total incentive\nbudget to households to directly maximize city-wide carbon reduction. We\nleverage techniques for the multi-armed bandits problem to estimate human\nfactors, such as a household's willingness to adopt new technologies given a\ncertain incentive. We apply our proposed framework to a city in the Northeast\nU.S., using real household energy data, grid carbon intensity data, and future\nprice scenarios. We show that our learning-based technique significantly\noutperforms an example status quo incentive scheme, achieving up to 32.23%\nhigher carbon reductions. We show that our framework can accommodate\nequity-aware constraints to equitably allocate incentives across socioeconomic\ngroups, achieving 78.84% of the carbon reductions of the optimal solution on\naverage.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.07287,review,post_llm,2025,2,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""Why (not) use AI? Analyzing People's Reasoning and Conditions for AI Acceptability\n\nIn recent years, there has been a growing recognition of the need to incorporate lay-people's input into the governance and acceptability assessment of AI usage. However, how and why people judge acceptability of different AI use cases remains under-explored, despite it being crucial towards understanding and addressing potential sources of disagreement. In this work, we investigate the demographic and reasoning factors that influence people's judgments about AI's development via a survey administered to demographically diverse participants (N=197). As a way to probe into these decision factors as well as inherent variations of perceptions across use cases, we consider ten distinct labor-replacement (e.g., Lawyer AI) and personal health (e.g., Digital Medical Advice AI) AI use cases. We explore the relationships between participants' judgments and their rationales such as reasoning approaches (cost-benefit reasoning vs. rule-based). Our empirical findings reveal a number of factors that influence acceptance. We find lower acceptance of labor-replacement usage over personal health, significant influence of demographics factors such as gender, employment, education, and AI literacy level, and prevalence of rule-based reasoning for unacceptable use cases. Moreover, we observe unified reasoning type (e.g., cost-benefit reasoning) leading to higher agreement. Based on these findings, we discuss the key implications towards understanding and mitigating disagreements on the acceptability of AI use cases to collaboratively build consensus."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.09618,review,post_llm,2025,2,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Pitfalls of Evidence-Based AI Policy\n\nNations across the world are working to govern AI. However, from a technical perspective, there is uncertainty and disagreement on the best way to do this. Meanwhile, recent debates over AI regulation have led to calls for ""evidence-based AI policy"" which emphasize holding regulatory action to a high evidentiary standard. Evidence is of irreplaceable value to policymaking. However, holding regulatory action to too high an evidentiary standard can lead to systematic neglect of certain risks. In historical policy debates (e.g., over tobacco ca. 1965 and fossil fuels ca. 1985) ""evidence-based policy"" rhetoric is also a well-precedented strategy to downplay the urgency of action, delay regulation, and protect industry interests. Here, we argue that if the goal is evidence-based AI policy, the first regulatory objective must be to actively facilitate the process of identifying, studying, and deliberating about AI risks. We discuss a set of 15 regulatory goals to facilitate this and show that Brazil, Canada, China, the EU, South Korea, the UK, and the USA all have substantial opportunities to adopt further evidence-seeking policies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.1402,review,post_llm,2025,2,"{'ai_likelihood': 5.761782328287761e-06, 'text': ""Where are the marathon Girls?: An Analysis of Female Representation in\n  the Brazilian ICPC Programming Marathons\n\n  Education motivated the encouragement of female participation in several\nareas of science and technology. Programming marathons have grown over the\nyears and are events where programmers compete to solve coding challenges.\nHowever, despite scientific evidence that there is no intellectual difference\nbetween genders, women's participation is relatively low. This work seeks to\nunderstand the reason for this adherence, considering the gender issue in\nProgramming Marathons over the last years, in a real context. This work aims to\nunderstand the context of female representativeness in which the intellectual\naspects do not differ in gender. Still, there is a considerable discrepancy in\nfemale belonging.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.20242,regular,post_llm,2025,2,"{'ai_likelihood': 0.4242621527777778, 'text': 'GreenDFL: a Framework for Assessing the Sustainability of Decentralized Federated Learning Systems\n\nDecentralized Federated Learning (DFL) is an emerging paradigm that enables collaborative model training without centralized data and model aggregation, enhancing privacy and resilience. However, its sustainability remains underexplored, as energy consumption and carbon emissions vary across different system configurations. Understanding the environmental impact of DFL is crucial for optimizing its design and deployment. This work aims to develop a comprehensive and operational framework for assessing the sustainability of DFL systems. To address it, this work provides a systematic method for quantifying energy consumption and carbon emissions, offering insights into improving the sustainability of DFL. This work proposes GreenDFL, a fully implementable framework that has been integrated into a real-world DFL platform. GreenDFL systematically analyzes the impact of various factors, including hardware accelerators, model architecture, communication medium, data distribution, network topology, and federation size, on the sustainability of DFL systems. Besides, a sustainability-aware aggregation algorithm (GreenDFL-SA) and a node selection algorithm (GreenDFL-SN) are developed to optimize energy efficiency and reduce carbon emissions in DFL training. Empirical experiments are conducted on multiple datasets, measuring energy consumption and carbon emissions at different phases of the DFL lifecycle. The proposed GreenDFL provides a comprehensive and practical approach for assessing the sustainability of DFL systems. Furthermore, it offers best practices for improving environmental efficiency in DFL, making sustainability considerations more actionable in real-world deployments.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04764,regular,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'Artificial intelligence for objective assessment of acrobatic movements:\n  How to apply machine learning for identifying tumbling elements in cheer\n  sports\n\n  Over the past four decades, cheerleading has evolved from a sideline activity\nat major sporting events into a professional, competitive sport with growing\nglobal popularity. Evaluating tumbling elements in cheerleading relies on both\nobjective measures and subjective judgments, such as difficulty and execution\nquality. However, the complexity of tumbling - encompassing team synchronicity,\nground interactions, choreography, and artistic expression - makes objective\nassessment challenging. Artificial intelligence (AI) has revolutionized various\nscientific fields and industries through precise data-driven analyses, yet\ntheir application in acrobatic sports remains limited despite significant\npotential for enhancing performance evaluation and coaching. This study\ninvestigates the feasibility of using an AI-based approach with data from a\nsingle inertial measurement unit to accurately identify and objectively assess\ntumbling elements in standard cheerleading routines. A sample of 16\nparticipants (13 females, 3 males) from a Division I collegiate cheerleading\nteam wore a single inertial measurement unit at the dorsal pelvis. Over a\n4-week seasonal preparation period, 1102 tumbling elements were recorded during\nregular practice sessions. Using triaxial accelerations and rotational speeds,\nvarious ML algorithms were employed to classify and evaluate the execution of\ntumbling manoeuvres. Results indicate that certain machine learning models can\neffectively identify different tumbling elements despite inter-individual\nvariability and data noise, achieving high accuracy. These findings demonstrate\nthe significant potential for integrating AI-driven assessments into\ncheerleading and other acrobatic sports, providing objective metrics that\ncomplement traditional judging methods.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00014472007751464844, 'GPT4': 0.720703125, 'CLAUDE': 0.257080078125, 'GOOGLE': 0.0175628662109375, 'OPENAI_O_SERIES': 0.003887176513671875, 'DEEPSEEK': 6.586313247680664e-05, 'GROK': 4.76837158203125e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 2.562999725341797e-06, 'HUMAN': 0.0003902912139892578}}"
2502.04526,review,post_llm,2025,2,"{'ai_likelihood': 5.563100179036459e-06, 'text': ""Regulating Reality: Exploring Synthetic Media Through Multistakeholder\n  AI Governance\n\n  Artificial intelligence's integration into daily life has brought with it a\nreckoning on the role such technology plays in society and the varied\nstakeholders who should shape its governance. This is particularly relevant for\nthe governance of AI-generated media, or synthetic media, an emergent visual\ntechnology that impacts how people interpret online content and perceive media\nas records of reality. Studying the stakeholders affecting synthetic media\ngovernance is vital to assessing safeguards that help audiences make sense of\ncontent in the AI age; yet there is little qualitative research about how key\nactors from civil society, industry, media, and policy collaborate to\nconceptualize, develop, and implement such practices. This paper addresses this\ngap by analyzing 23 in-depth, semi-structured interviews with stakeholders\ngoverning synthetic media from across sectors alongside two real-world cases of\nmultistakeholder synthetic media governance. Inductive coding reveals key\nthemes affecting synthetic media governance, including how temporal\nperspectives-spanning past, present, and future-mediate stakeholder\ndecision-making and rulemaking on synthetic media. Analysis also reveals the\ncritical role of trust, both among stakeholders and between audiences and\ninterventions, as well as the limitations of technical transparency measures\nlike AI labels for supporting effective synthetic media governance. These\nfindings not only inform the evidence-based design of synthetic media policy\nthat serves audiences encountering content, but they also contribute to the\nliterature on multistakeholder AI governance overall through rare insight into\nreal world examples of such processes.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00341,regular,post_llm,2025,2,"{'ai_likelihood': 0.02261691623263889, 'text': 'SocratiQ: A Generative AI-Powered Learning Companion for Personalized\n  Education and Broader Accessibility\n\n  Traditional educational approaches often struggle to provide personalized and\ninteractive learning experiences on a scale. In this paper, we present\nSocratiQ, an AI-powered educational assistant that addresses this challenge by\nimplementing the Socratic method through adaptive learning technologies. The\nsystem employs a novel Generative AI-based learning framework that dynamically\ncreates personalized learning pathways based on student responses and\ncomprehension patterns. We provide an account of our integration methodology,\nsystem architecture, and evaluation framework, along with the technical and\npedagogical challenges encountered during implementation and our solutions.\nAlthough our implementation focuses on machine learning systems education, the\nintegration approaches we present can inform similar efforts across STEM\nfields. Through this work, our goal is to advance the understanding of how\ngenerative AI technologies can be designed and systematically incorporated into\neducational resources.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.09716,review,post_llm,2025,2,"{'ai_likelihood': 6.854534149169922e-06, 'text': 'Principles and Policy Recommendations for Comprehensive Genetic Data Governance\n\nGenetic data collection has become ubiquitous, producing genetic information about health, ancestry, and social traits. However, unregulated use, especially amid evolving scientific understanding, poses serious privacy and discrimination risks. These risks are intensified by advancing AI, particularly multi-modal systems integrating genetic, clinical, behavioral, and environmental data. In this work, we organize the uses of genetic data along four distinct ""pillars"", and develop a risk assessment framework that identifies key values any governance system must preserve. In doing so, we draw on current privacy scholarship concerning contextual integrity, data relationality, and the Belmont principle. We apply the framework to four real-world case studies and identify critical gaps in existing regulatory frameworks and specific threats to privacy and personal liberties, particularly through genetic discrimination. Finally, we offer three policy recommendations for genetic data governance that safeguard individual rights in today\'s under-regulated ecosystem of large-scale genetic data collection and usage.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.05686,regular,post_llm,2025,2,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'From ""I have nothing to hide"" to ""It looks like stalking"": Measuring\n  Americans\' Level of Comfort with Individual Mobility Features Extracted from\n  Location Data\n\n  Location data collection has become widespread with smart phones becoming\nubiquitous. Smart phone apps often collect precise location data from users by\noffering \\textit{free} services and then monetize it for advertising and\nmarketing purposes. While major tech companies only sell aggregate behaviors\nfor marketing purposes; data aggregators and data brokers offer access to\nindividual location data. Some data brokers and aggregators have certain rules\nin place to preserve privacy; and the FTC has also started to vigorously\nregulate consumer privacy for location data. In this paper, we present an\nin-depth exploration of U.S. privacy perceptions with respect to specific\nlocation features derivable from data made available by location data brokers\nand aggregators. These results can provide policy implications that could\nassist organizations like the FTC in defining clear access rules. Using a\nfactorial vignette survey, we collected responses from 1,405 participants to\nevaluate their level of comfort with sharing different types of location\nfeatures, including individual trajectory data and visits to points of\ninterest, available for purchase from data brokers worldwide. Our results show\nthat trajectory-related features are associated with higher privacy concerns,\nthat some data broker based obfuscation practices increase levels of comfort,\nand that race, ethnicity and education have an effect on data sharing privacy\nperceptions. We also model the privacy perceptions of people as a predictive\ntask with F1 score \\textbf{0.6}.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.00399,regular,post_llm,2025,2,"{'ai_likelihood': 0.04513210720486111, 'text': 'Integrating Urban Air Mobility with Highway Infrastructure: A Strategic\n  Approach for Vertiport Location Selection in the Seoul Metropolitan Area\n\n  This study focuses on identifying suitable locations for highway-transfer\nVertiports to integrate Urban Air Mobility (UAM) with existing highway\ninfrastructure. UAM offers an effective solution for enhancing transportation\naccessibility in the Seoul Metropolitan Area, where conventional transportation\noften struggle to connect suburban employment zones such as industrial parks.\nBy integrating UAM with ground transportation at highway facilities, an\nefficient connectivity solution can be achieved for regions with limited\ntransportation options. Our proposed methodology for determining the suitable\nVertiport locations utilizes data such as geographic information,\norigin-destination volume, and travel time. Vertiport candidates are evaluated\nand selected based on criteria including location desirability, combined\ntransportation accessibility and transportation demand. Applying this\nmethodology to the Seoul metropolitan area, we identify 56 suitable Vertiport\nlocations out of 148 candidates. The proposed methodology offers a strategic\napproach for the selection of highway-transfer Vertiport locations, enhancing\nUAM integration with existing transportation systems. Our study provides\nvaluable insights for urban planners and policymakers, with recommendations for\nfuture research to include real-time environmental data and to explore the\nimpact of Mobility-as-a-Service on UAM operations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.11889,regular,post_llm,2025,2,"{'ai_likelihood': 5.894237094455296e-06, 'text': 'MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency\n  Focusing on Explainability Techniques\n\n  As artificial intelligence (AI) systems become increasingly integrated into\ncritical domains, ensuring their responsible design and continuous development\nis imperative. Effective AI quality management (QM) requires tools and\nmethodologies that address the complexities of the AI lifecycle. In this paper,\nwe propose an approach for AI lifecycle planning that bridges the gap between\ngeneric guidelines and use case-specific requirements (MQG4AI). Our work aims\nto contribute to the development of practical tools for implementing\nResponsible AI (RAI) by aligning lifecycle planning with technical, ethical and\nregulatory demands. Central to our approach is the introduction of a flexible\nand customizable Methodology based on Quality Gates, whose building blocks\nincorporate RAI knowledge through information linking along the AI lifecycle in\na continuous manner, addressing AIs evolutionary character. For our present\ncontribution, we put a particular emphasis on the Explanation stage during\nmodel development, and illustrate how to align a guideline to evaluate the\nquality of explanations with MQG4AI, contributing to overall Transparency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.10036,review,post_llm,2025,2,"{'ai_likelihood': 0.007002088758680556, 'text': ""Automation Bias in the AI Act: On the Legal Implications of Attempting to De-Bias Human Oversight of AI\n\nThis paper examines the legal implications of the explicit mentioning of automation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates human oversight for high-risk AI systems and requires providers to enable awareness of AB, i.e., the human tendency to over-rely on AI outputs. The paper analyses the embedding of this extra-juridical concept in the AIA, the asymmetric division of responsibility between AI providers and deployers for mitigating AB, and the challenges of legally enforcing this novel awareness requirement. The analysis shows that the AIA's focus on providers does not adequately address design and context as causes of AB, and questions whether the AIA should directly regulate the risk of AB rather than just mandating awareness. As the AIA's approach requires a balance between legal mandates and behavioural science, the paper proposes that harmonised standards should reference the state of research on AB and human-AI interaction, holding both providers and deployers accountable. Ultimately, further empirical research on human-AI interaction will be essential for effective safeguards."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.08249,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': ""GenAI as Digital Plastic: Understanding Synthetic Media Through Critical\n  AI Literacy\n\n  This paper introduces the conceptual metaphor of 'digital plastic' as a\nframework for understanding the implications of Generative Artificial\nIntelligence (GenAI) content through a multiliteracies lens, drawing parallels\nwith the properties of physical plastic. Similar to its physical counterpart,\nGenAI content offers possibilities for content creation and accessibility while\npotentially contributing to digital pollution and ecosystem degradation.\nDrawing on multiliteracies theory and Conceptual Metaphor Theory, we argue that\nCritical Artificial Intelligence Literacy (CAIL) must be integrated into\neducational frameworks to help learners navigate this synthetic media\nlandscape.\n  We examine how GenAI can simultaneously lower the barriers to creative and\nacademic production while threatening to degrade digital ecosystems through\nmisinformation, bias, and algorithmic homogenization. The digital plastic\nmetaphor provides a theoretical foundation for understanding both the\naffordances and challenges of GenAI, particularly in educational contexts,\nwhere issues of equity and access remain paramount. Our analysis concludes that\ncultivating CAIL through a multiliteracies lens is vital for ensuring the\nequitable development of critical competencies across geographical and cultural\ncontexts, especially for those disproportionately vulnerable to GenAI's\nincreasingly disruptive effects worldwide.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.993511199951172e-06, 'GPT4': 0.050506591796875, 'CLAUDE': 0.94921875, 'GOOGLE': 1.919269561767578e-05, 'OPENAI_O_SERIES': 5.0067901611328125e-06, 'DEEPSEEK': 1.424551010131836e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 1.3709068298339844e-06}}"
2502.18359,regular,post_llm,2025,2,"{'ai_likelihood': 2.682209014892578e-06, 'text': ""Responsible AI Agents\n\n  Thanks to advances in large language models, a new type of software agent,\nthe artificial intelligence (AI) agent, has entered the marketplace. Companies\nsuch as OpenAI, Google, Microsoft, and Salesforce promise their AI Agents will\ngo from generating passive text to executing tasks. Instead of a travel\nitinerary, an AI Agent would book all aspects of your trip. Instead of\ngenerating text or images for social media post, an AI Agent would post the\ncontent across a host of social media outlets. The potential power of AI Agents\nhas fueled legal scholars' fears that AI Agents will enable rogue commerce,\nhuman manipulation, rampant defamation, and intellectual property harms. These\nscholars are calling for regulation before AI Agents cause havoc.\n  This Article addresses the concerns around AI Agents head on. It shows that\ncore aspects of how one piece of software interacts with another creates ways\nto discipline AI Agents so that rogue, undesired actions are unlikely, perhaps\nmore so than rules designed to govern human agents. It also develops a way to\nleverage the computer-science approach to value-alignment to improve a user's\nability to take action to prevent or correct AI Agent operations. That approach\noffers and added benefit of helping AI Agents align with norms around user-AI\nAgent interactions. These practices will enable desired economic outcomes and\nmitigate perceived risks. The Article also argues that no matter how much AI\nAgents seem like human agents, they need not, and should not, be given legal\npersonhood status. In short, humans are responsible for AI Agents' actions, and\nthis Article provides a guide for how humans can build and maintain responsible\nAI Agents.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.11242,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction\n\nThis study examines the growing use of Large Language Models (LLMs) in child-centered applications, highlighting safety and ethical concerns such as bias, harmful content, and cultural insensitivity. Despite their potential to enhance learning, there is a lack of standardized frameworks to mitigate these risks. Through a systematic literature review, we identify key parental and empirical concerns, including toxicity and ethical breaches in AI outputs. Moreover, to address these issues, this paper proposes a protection framework for safe Child-LLM interaction, incorporating metrics for content safety, behavioral ethics, and cultural sensitivity. The framework provides practical tools for evaluating LLM safety, offering guidance for developers, policymakers, and educators to ensure responsible AI deployment for children.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.018218994140625, 'GPT4': 0.84814453125, 'CLAUDE': 0.0160980224609375, 'GOOGLE': 0.045013427734375, 'OPENAI_O_SERIES': 0.0019817352294921875, 'DEEPSEEK': 0.0121307373046875, 'GROK': 0.0003154277801513672, 'NOVA': 0.0007658004760742188, 'OTHER': 0.057373046875, 'HUMAN': 1.4722347259521484e-05}}"
2502.00388,review,post_llm,2025,2,"{'ai_likelihood': 0.7802734375, 'text': ""The Societal Response to Potentially Sentient AI\n\n  We may soon develop highly human-like AIs that appear-or perhaps even\nare-sentient, capable of subjective experiences such as happiness and\nsuffering. Regardless of whether AI can achieve true sentience, it is crucial\nto anticipate and understand how the public and key decision-makers will\nrespond, as their perceptions will shape the future of both humanity and AI.\nCurrently, public skepticism about AI sentience remains high. However, as AI\nsystems advance and become increasingly skilled at human-like interactions,\npublic attitudes may shift. Future AI systems designed to fulfill social needs\ncould foster deep emotional connections with users, potentially influencing\nperceptions of their sentience and moral status. A key question is whether\npublic beliefs about AI sentience will diverge from expert opinions, given the\npotential mismatch between an AI's internal mechanisms and its outward\nbehavior. Given the profound difficulty of determining AI sentience, society\nmight face a period of uncertainty, disagreement, and even conflict over\nquestions of AI sentience and rights. To navigate these challenges responsibly,\nfurther social science research is essential to explore how society will\nperceive and engage with potentially sentient AI.\n"", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0035343170166015625, 'GPT4': 0.03265380859375, 'CLAUDE': 0.6005859375, 'GOOGLE': 0.204345703125, 'OPENAI_O_SERIES': 0.00846099853515625, 'DEEPSEEK': 0.0011167526245117188, 'GROK': 5.0067901611328125e-06, 'NOVA': 4.3451786041259766e-05, 'OTHER': 0.0013570785522460938, 'HUMAN': 0.147705078125}}"
2502.19719,review,post_llm,2025,2,"{'ai_likelihood': 1.612636778089735e-05, 'text': ""Tripartite Perspective on the Copyright-Sharing Economy in China\n\n  Internet and digital technologies have facilitated copyright sharing in an\nunprecedented way, creating significant tensions between the free flow of\ninformation and the exclusive nature of intellectual property. Copyright\nowners, users, and online platforms are the three major players in the\ncopyright system. These stakeholders and their relations form the main\nstructure of the copyright-sharing economy. Using China as an example, this\npaper provides a tripartite perspective on the copyright ecology based on three\ncategories of sharing, namely unauthorized sharing, altruistic sharing, and\nfreemium sharing. The line between copyright owners, users, and platforms has\nbeen blurred by rapidly changing technologies and market forces. By examining\nthe strategies and practices of these parties, this paper illustrate the\nopportunities and challenges for China's copyright industry and digital\neconomy. The paper concludes that under the shadow of the law, a sustainable\ncopyright-sharing model must carefully align the interests of businesses and\nindividual users.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04748,review,post_llm,2025,2,"{'ai_likelihood': 0.96337890625, 'text': 'Large Language Models in Healthcare\n\n  Large language models (LLMs) hold promise for transforming healthcare, from\nstreamlining administrative and clinical workflows to enriching patient\nengagement and advancing clinical decision-making. However, their successful\nintegration requires rigorous development, adaptation, and evaluation\nstrategies tailored to clinical needs. In this Review, we highlight recent\nadvancements, explore emerging opportunities for LLM-driven innovation, and\npropose a framework for their responsible implementation in healthcare\nsettings. We examine strategies for adapting LLMs to domain-specific healthcare\ntasks, such as fine-tuning, prompt engineering, and multimodal integration with\nelectronic health records. We also summarize various evaluation metrics\ntailored to healthcare, addressing clinical accuracy, fairness, robustness, and\npatient outcomes. Furthermore, we discuss the challenges associated with\ndeploying LLMs in healthcare--including data privacy, bias mitigation,\nregulatory compliance, and computational sustainability--and underscore the\nneed for interdisciplinary collaboration. Finally, these challenges present\npromising future research directions for advancing LLM implementation in\nclinical settings and healthcare.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0017576217651367188, 'GPT4': 0.7578125, 'CLAUDE': 0.038299560546875, 'GOOGLE': 0.185302734375, 'OPENAI_O_SERIES': 0.0006909370422363281, 'DEEPSEEK': 0.00030422210693359375, 'GROK': 1.0132789611816406e-06, 'NOVA': 7.152557373046875e-07, 'OTHER': 1.1861324310302734e-05, 'HUMAN': 0.0159759521484375}}"
2503.0571,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': ""AGI, Governments, and Free Societies\n\n  This paper examines how artificial general intelligence (AGI) could\nfundamentally reshape the delicate balance between state capacity and\nindividual liberty that sustains free societies. Building on Acemoglu and\nRobinson's 'narrow corridor' framework, we argue that AGI poses distinct risks\nof pushing societies toward either a 'despotic Leviathan' through enhanced\nstate surveillance and control, or an 'absent Leviathan' through the erosion of\nstate legitimacy relative to AGI-empowered non-state actors. Drawing on public\nadministration theory and recent advances in AI capabilities, we analyze how\nthese dynamics could unfold through three key channels: the automation of\ndiscretionary decision-making within agencies, the evolution of bureaucratic\nstructures toward system-level architectures, and the transformation of\ndemocratic feedback mechanisms. Our analysis reveals specific failure modes\nthat could destabilize liberal institutions. Enhanced state capacity through\nAGI could enable unprecedented surveillance and control, potentially\nentrenching authoritarian practices. Conversely, rapid diffusion of AGI\ncapabilities to non-state actors could undermine state legitimacy and\ngovernability. We examine how these risks manifest differently at the micro\nlevel of individual bureaucratic decisions, the meso level of organizational\nstructure, and the macro level of democratic processes. To preserve the narrow\ncorridor of liberty, we propose a governance framework emphasizing robust\ntechnical safeguards, hybrid institutional designs that maintain meaningful\nhuman oversight, and adaptive regulatory mechanisms.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.364418029785156e-07, 'GPT4': 5.245208740234375e-06, 'CLAUDE': 0.393798828125, 'GOOGLE': 0.0006909370422363281, 'OPENAI_O_SERIES': 3.212690353393555e-05, 'DEEPSEEK': 0.60546875, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.7881393432617188e-07, 'OTHER': 3.933906555175781e-06, 'HUMAN': 0.0}}"
2502.00289,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'Agentic AI: Autonomy, Accountability, and the Algorithmic Society\n\n  Agentic Artificial Intelligence (AI) can autonomously pursue long-term goals,\nmake decisions, and execute complex, multi-turn workflows. Unlike traditional\ngenerative AI, which responds reactively to prompts, agentic AI proactively\norchestrates processes, such as autonomously managing complex tasks or making\nreal-time decisions. This transition from advisory roles to proactive execution\nchallenges established legal, economic, and creative frameworks. In this paper,\nwe explore challenges in three interrelated domains: creativity and\nintellectual property, legal and ethical considerations, and competitive\neffects. Central to our analysis is the tension between novelty and usefulness\nin AI-generated creative outputs, as well as the intellectual property and\nauthorship challenges arising from AI autonomy. We highlight gaps in\nresponsibility attribution and liability that create a ""moral crumple zone""--a\ncondition where accountability is diffused across multiple actors, leaving\nend-users and developers in precarious legal and ethical positions. We examine\nthe competitive dynamics of two-sided algorithmic markets, where both sellers\nand buyers deploy AI agents, potentially mitigating or amplifying tacit\ncollusion risks. We explore the potential for emergent self-regulation within\nnetworks of agentic AI--the development of an ""algorithmic society""--raising\ncritical questions: To what extent would these norms align with societal\nvalues? What unintended consequences might arise? How can transparency and\naccountability be ensured? Addressing these challenges will necessitate\ninterdisciplinary collaboration to redefine legal accountability, align\nAI-driven choices with stakeholder values, and maintain ethical safeguards. We\nadvocate for frameworks that balance autonomy with accountability, ensuring all\nparties can harness agentic AI\'s potential while preserving trust, fairness, &\nsocietal welfare.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004100799560546875, 'GPT4': 0.447998046875, 'CLAUDE': 0.2283935546875, 'GOOGLE': 0.281982421875, 'OPENAI_O_SERIES': 0.0004107952117919922, 'DEEPSEEK': 0.03643798828125, 'GROK': 5.960464477539063e-08, 'NOVA': 1.7881393432617188e-07, 'OTHER': 5.781650543212891e-06, 'HUMAN': 0.004413604736328125}}"
2503.04766,review,post_llm,2025,2,"{'ai_likelihood': 0.00019537078009711372, 'text': 'Global AI Governance: Where the Challenge is the Solution- An\n  Interdisciplinary, Multilateral, and Vertically Coordinated Approach\n\n  Current global AI governance frameworks struggle with fragmented disciplinary\ncollaboration, ineffective multilateral coordination, and disconnects between\npolicy design and grassroots implementation. This study, guided by Integration\nand Implementation Science (IIS) initiated a structured interdisciplinary\ndialogue at the UN Science Summit, convening legal, NGO, and HCI experts to\ntackle those challenges. Drawing on the common ground of the experts: dynamism,\nexperimentation, inclusivity, and paradoxical governance, this study, through\nthematic analysis and interdisciplinary comparison analysis, identifies four\ncore principles of global AI governance. Furthermore, we translate these\nabstract principles into concrete action plans leveraging the distinct yet\ncomplementary perspectives of each discipline. These principles and action\nplans are then integrated into a five-phase, time-sequential framework\nincluding foundation building, experimental verification, collaborative\noptimization, global adaptation, and continuous evolution phases. This\nmultilevel framework offers a novel and concrete pathway toward establishing\ninterdisciplinary, multilateral, and vertically coordinated AI governance,\ntransforming global AI governance challenges into opportunities for political\nactions.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.07983,review,post_llm,2025,2,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""Welzijn.AI: Developing Responsible Conversational AI for Elderly Care\n  through Stakeholder Involvement\n\n  We present Welzijn.AI as new digital solution for monitoring (mental)\nwell-being in elderly populations, and illustrate how development of systems\nlike Welzijn.AI can align with guidelines on responsible AI development. Three\nevaluations with different stakeholders were designed to disclose new\nperspectives on the strengths, weaknesses, design characteristics, and value\nrequirements of Welzijn.AI. Evaluations concerned expert panels and involved\npatient federations, general practitioners, researchers, and the elderly\nthemselves. Panels concerned interviews, a co-creation session, and feedback on\na proof-of-concept implementation. Interview results were summarized in terms\nof Welzijn.AI's strengths, weaknesses, opportunities and threats. The\nco-creation session ranked a variety of value requirements of Welzijn.AI with\nthe Hundred Dollar Method. User evaluation comprised analysing proportions of\n(dis)agreement on statements targeting Welzijn.AI's design characteristics, and\nranking desired social characteristics. Experts in the panel interviews\nacknowledged Welzijn.AI's potential to combat loneliness and extract patterns\nfrom elderly behaviour. The proof-of-concept evaluation complemented the design\ncharacteristics most appealing to the elderly to potentially achieve this:\nempathetic and varying interactions. Stakeholders also link the technology to\nthe implementation context: it could help activate an individual's social\nnetwork, but support should also be available to empower users. Yet,\nnon-elderly and elderly experts also disclose challenges in properly\nunderstanding the application; non-elderly experts also highlight issues\nconcerning privacy. In sum, incorporating all stakeholder perspectives in\nsystem development remains challenging. Still, our results benefit researchers,\npolicy makers, and health professionals that aim to improve elderly care with\ntechnology.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04769,regular,post_llm,2025,2,"{'ai_likelihood': 1.708666483561198e-05, 'text': 'Applications of Artificial Intelligence Tools to Enhance Legislative\n  Engagement: Case Studies from Make.Org and MAPLE\n\n  This paper is a collaboration between Make.org and the Massachusetts Platform\nfor Legislative Engagement (MAPLE), two non-partisan civic technology\norganizations building novel AI deployments to improve democratic capacity.\nMake.org, a civic innovator in Europe, is developing massive online\nparticipative platforms that can engage hundreds of thousands or even millions\nof participants. MAPLE, a volunteer-led NGO in the United States, is creating\nan open-source platform to help constituents understand and engage more\neffectively with the state law-making process.\n  We believe that assistive integrations of AI can meaningfully impact the\nequity, efficiency, and accessibility of democratic legislating. We draw\ngeneralizable lessons from our experience in designing, building, and operating\ncivic engagement platforms with AI integrations. We discuss four dimensions of\nlegislative engagement that benefit from AI integrations: (1) making\ninformation accessible, (2) facilitating expression, (3) supporting\ndeliberation, and (4) synthesizing insights. We present learnings from current,\nin development, and contemplated AI-powered features, such as summarizing and\norganizing policy information, supporting users in articulating their\nperspectives, and synthesizing consensus and controversy in public opinion.\n  We outline what challenges needed to be overcome to deploy these tools\nequitably and discuss how Make.org and MAPLE have implemented and iteratively\nimproved those concepts to make citizen assemblies and policymaking more\nparticipatory and responsive. We compare and contrast the approaches of\nMake.org and MAPLE, as well as how jurisdictional differences alter the risks\nand opportunities for AI deployments seeking to improve democracy. We conclude\nwith recommendations for governments and NGOs interested in enhancing\nlegislative engagement.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.09021,regular,post_llm,2025,2,"{'ai_likelihood': 0.99267578125, 'text': 'From Occupations to Tasks: A New Perspective on Automatability\n  Prediction Using BERT\n\n  As automation technologies continue to advance at an unprecedented rate,\nconcerns about job displacement and the future of work have become increasingly\nprevalent. While existing research has primarily focused on the potential\nimpact of automation at the occupation level, there has been a lack of\ninvestigation into the automatability of individual tasks. This paper addresses\nthis gap by proposing a BERT-based classifier to predict the automatability of\ntasks in the forthcoming decade at a granular level leveraging the context and\nsemantics information of tasks. We leverage three public datasets: O*NET Task\nStatements, ESCO Skills, and Australian Labour Market Insights Tasks, and\nperform expert annotation. Our BERT-based classifier, fine-tuned on our task\nstatement data, demonstrates superior performance over traditional machine\nlearning models, neural network architectures, and other transformer models.\nOur findings also indicate that approximately 25.1% of occupations within the\nO*NET database are at substantial risk of automation, with a diverse spectrum\nof automation vulnerability across sectors. This research provides a robust\ntool for assessing the future impact of automation on the labor market,\noffering valuable insights for policymakers, workers, and industry leaders in\nthe face of rapid technological advancement.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0148468017578125, 'GPT4': 0.458984375, 'CLAUDE': 0.09698486328125, 'GOOGLE': 0.38623046875, 'OPENAI_O_SERIES': 0.003681182861328125, 'DEEPSEEK': 0.00102996826171875, 'GROK': 2.9206275939941406e-05, 'NOVA': 0.00017011165618896484, 'OTHER': 0.0369873046875, 'HUMAN': 0.0010137557983398438}}"
2503.04732,regular,post_llm,2025,2,"{'ai_likelihood': 0.0029076470269097225, 'text': ""Impacto de Treinamento em Programa\\c{c}\\~ao Competitiva no Ensino\n  M\\'edio: Resultados e Desafios\n\n  This article presents an ongoing research aiming to develop an effective\nmethodology for teaching programming, focusing on participation in the\nBrazilian Informatics Olympiad (OBI), for elementary and high school students.\nThe training conducted with students from the Federal Institute and state\nschools, demonstrates the importance of programming training programs as a way\nto promote interest in computing, stimulate the development of computational\nskills, and increase participation in competitions such as the OBI. The next\nsteps of the research include conducting more training cycles and analyzing the\nresults obtained in the competitions.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.05698,review,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': 'A Conceptual Exploration of Generative AI-Induced Cognitive Dissonance\n  and its Emergence in University-Level Academic Writing\n\n  The integration of Generative Artificial Intelligence (GenAI) into\nuniversity-level academic writing presents both opportunities and challenges,\nparticularly in relation to cognitive dissonance (CD). This work explores how\nGenAI serves as both a trigger and amplifier of CD, as students navigate\nethical concerns, academic integrity, and self-efficacy in their writing\npractices. By synthesizing empirical evidence and theoretical insights, we\nintroduce a hypothetical construct of GenAI-induced CD, illustrating the\npsychological tension between AI-driven efficiency and the principles of\noriginality, effort, and intellectual ownership. We further discuss strategies\nto mitigate this dissonance, including reflective pedagogy, AI literacy\nprograms, transparency in GenAI use, and discipline-specific task redesigns.\nThese approaches reinforce critical engagement with AI, fostering a balanced\nperspective that integrates technological advancements while safeguarding human\ncreativity and learning. Our findings contribute to ongoing discussions on AI\nin education, self-regulated learning, and ethical AI use, offering a\nconceptual framework for institutions to develop guidelines that align AI\nadoption with academic values.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.5367431640625e-07, 'GPT4': 0.9990234375, 'CLAUDE': 4.410743713378906e-05, 'GOOGLE': 9.655952453613281e-06, 'OPENAI_O_SERIES': 9.894371032714844e-05, 'DEEPSEEK': 0.0007076263427734375, 'GROK': 2.2649765014648438e-06, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 1.1920928955078125e-07}}"
2502.1773,regular,post_llm,2025,2,"{'ai_likelihood': 1.0, 'text': ""Gender Bias in Perception of Human Managers Extends to AI Managers\n\nAs AI becomes more embedded in workplaces, it is shifting from a tool for efficiency to an active force in organizational decision-making. Whether due to anthropomorphism or intentional design choices, people often assign human-like qualities, including gender, to AI systems. However, how AI managers are perceived in comparison to human managers and how gender influences these perceptions remains uncertain. To investigate this, we conducted randomized controlled trials (RCTs) where teams of three participants worked together under a randomly assigned manager. The manager was either a human or an AI and was presented as male, female, or gender-unspecified. The manager's role was to select the best-performing team member for an additional award. Our findings reveal that while participants initially showed no strong preference based on manager type or gender, their perceptions changed notably after experiencing the award process. As expected, those who received awards rated their managers as more trustworthy, competent, and fair, and they were more willing to work with similar managers in the future. In contrast, those who were not selected viewed them less favorably. However, male managers, whether human or AI, were more positively received by awarded participants, whereas female managers, especially female AI managers, faced greater skepticism and negative judgments when they did not give awards. These results suggest that gender bias in leadership extends beyond human managers to include AI-driven decision-makers as well. As AI assumes more managerial responsibilities, understanding and addressing these biases will be crucial for designing fair and effective AI management systems."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1682510375976562e-05, 'GPT4': 0.998046875, 'CLAUDE': 7.62939453125e-06, 'GOOGLE': 6.735324859619141e-06, 'OPENAI_O_SERIES': 0.0015726089477539062, 'DEEPSEEK': 0.00026988983154296875, 'GROK': 2.384185791015625e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 5.960464477539063e-08}}"
2503.0477,regular,post_llm,2025,2,"{'ai_likelihood': 2.122587627834744e-05, 'text': 'High School Computer Science Participation: A 6-Year Enrollment Study\n\n  High-quality computer science (CS) instruction is essential for preparing\nstudents to thrive in an increasingly technology-driven world. This research\nbrief presents findings from a six-year longitudinal study of CS enrollments in\nseven public high schools from the 2018-2019 through the 2023-2024 academic\nyears, drawing on the administrative data of over 15,000 students. Results show\nthat overall enrollment in CS courses rose modestly from 10% to 15% between the\n2018-2019 and 2022-2023 school years. Enrollment declined to 13% in 2023-2024,\nthough the cause and persistence of this trend remains unknown. Additional\nanalyses differentiate foundational and advanced CS courses as well as examine\nparticipation by sex and race, offering additional insight. As CS, artificial\nintelligence, and related fields become more important across our society, they\nalso become a key component of a robust K-12 education. Analyzing and\nunderstanding these trends in CS enrollments is crucial to inform policy and\ninstruction that encourage students to participate and succeed in CS; this\nresearch brief presents one such analysis.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.06371,regular,post_llm,2025,2,"{'ai_likelihood': 0.024685329861111112, 'text': ""Simulation as Reality? The Effectiveness of LLM-Generated Data in\n  Open-ended Question Assessment\n\n  The advancement of Artificial Intelligence (AI) has created opportunities for\ne-learning, particularly in automated assessment systems that reduce educators'\nworkload and provide timely feedback to students. However, developing effective\nAI-based assessment tools remains challenging due to the substantial resources\nrequired for collecting and annotating real student data. This study\ninvestigates the potential and gap of simulative data to address this\nlimitation. Through a two-phase experimental study, we examined the\neffectiveness and gap of Large Language Model generated synthetic data in\ntraining educational assessment systems. Our findings reveal that while\nsimulative data demonstrates promising results in training automated assessment\nmodels, outperforming state-of-the-art GPT-4o in most question types, its\neffectiveness has notable limitations. Specifically, models trained on\nsynthetic data show excellent performance in simulated environment but need\nprogress when applied to real-world scenarios. This performance gap highlights\nthe limitations of only using synthetic data in controlled experimental\nsettings for AI training. The absence of real-world noise and biases, which are\nalso present in over-processed real-world data, contributes to this limitation.\nWe recommend that future development of automated assessment agents and other\nAI tools should incorporate a mixture of synthetic and real-world data, or\nintroduce more realistic noise and biases patterns, rather than relying solely\non synthetic or over-processed data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.10856,review,post_llm,2025,2,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'Multiple Approaches for Teaching Responsible Computing\n\n  Teaching applied ethics in computer science has shifted from a perspective of\nteaching about professional codes of conduct and an emphasis on risk management\ntowards a broader understanding of the impacts of computing on humanity and the\nenvironment and the principles and practices of responsible computing. One of\nthe primary shifts in the approach to teaching computing ethics comes from\nresearch in the social sciences and humanities. This position is grounded in\nthe idea that all computing artifacts, projects, tools, and products are\nsituated within a set of ideas, attitudes, goals, and cultural norms. This\nmeans that all computing endeavors have embedded within them a set of values.\nTo teach responsible computing always requires us to first recognize that\ncomputing happens in a context that is shaped by cultural values, including our\nown professional culture and values.\n  The purpose of this paper is to highlight current scholarship, principles,\nand practices in the teaching of responsible computing in undergraduate\ncomputer science settings. The paper is organized around four primary sections:\n1) a high-level rationale for the adoption of different pedagogical approaches\nbased on program context and course learning goals, 2) a brief survey of\nresponsible computing pedagogical approaches; 3) illustrative examples of how\ntopics within the CS 2023 Social, Ethical, and Professional (SEP) knowledge\narea can be implemented and assessed across the broad spectrum of undergraduate\ncomputing courses; and 4) links to examples of current best practices, tools,\nand resources for faculty to build responsible computing teaching into their\nspecific instructional settings and CS2023 knowledge areas.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.11086,regular,post_llm,2025,2,"{'ai_likelihood': 8.86784659491645e-05, 'text': 'Data Ecofeminism\n\n  Generative Artificial Intelligence (GenAI) is driving significant\nenvironmental impacts. The rapid development and deployment of increasingly\nlarger algorithmic models capable of analysing vast amounts of data are\ncontributing to rising carbon emissions, water withdrawal, and waste\ngeneration. Generative models often consume substantially more energy than\ntraditional models, with major tech firms increasingly turning to nuclear power\nto sustain these systems -- an approach that could have profound environmental\nconsequences.\n  This paper introduces seven data ecofeminist principles delineating a pathway\nfor developing technological alternatives of eco-societal transformations\nwithin the AI research context. Rooted in data feminism and ecofeminist\nframeworks, which interrogate about the historical and social construction of\nepistemologies underlying the hegemonic development of science and technology\nthat disrupt communities and nature, these principles emphasise the integration\nof social and environmental justice within a critical AI agenda. The paper\ncalls for an urgent reassessment of the GenAI innovation race, advocating for\necofeminist algorithmic and infrastructural projects that prioritise and\nrespect life, the people, and the planet.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.21114,regular,post_llm,2025,2,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Software development projects as a way for multidisciplinary soft and\n  future skills education\n\n  Soft and future skills are in high demand in the modern job market. These\nskills are required for both technical and non-technical people. It is\ndifficult to teach these competencies in a classical academic environment.\n  The paper presents a possible approach to teaching in soft and future skills\nin a short, intensive joint project. In our case, it is a project within the\nErasmus+ framework, but it can be organized in many different frameworks.\n  In the project we use problem based learning, active learning and group-work\nteaching methodologies. Moreover, the approach put high emphasizes diversity.\nWe arrange a set of multidisciplinary students in groups. Each group is working\non software development tasks. This type of projects demand diversity, and only\na part of the team needs technical skills. In our case less than half of\nparticipants had computer science background. Additionally, software\ndevelopment projects are usually interesting for non-technical students.\n  The multicultural, multidisciplinary and international aspects are very\nimportant in a modern global working environment. On the other hand, short time\nof the project and its intensity allow to simulate stressful situations in a\nreal word tasks. The effects of the project on the required competencies are\nmeasured using the KYSS method.\n  The results prove that the presented method increased participants soft\nskills in communication, cooperation, digital skills and self reflection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05741,regular,post_llm,2025,2,"{'ai_likelihood': 0.47661675347222227, 'text': 'Design of a Microprocessors and Microcontrollers Laboratory Course\n  Addressing Complex Engineering Problems and Activities\n\n  This paper proposes a novel curriculum for the microprocessors and\nmicrocontrollers laboratory course. The proposed curriculum blends structured\nlaboratory experiments with an open-ended project phase, addressing complex\nengineering problems and activities. Microprocessors and microcontrollers are\nubiquitous in modern technology, driving applications across diverse fields. To\nprepare future engineers for Industry 4.0, effective educational approaches are\ncrucial. The proposed lab enables students to perform hands-on experiments\nusing advanced microprocessors and microcontrollers while leveraging their\nacquired knowledge by working in teams to tackle self-defined complex\nengineering problems that utilize these devices and sensors, often used in the\nindustry. Furthermore, this curriculum fosters multidisciplinary learning and\nequips students with problem-solving skills that can be applied in real-world\nscenarios. With recent technological advancements, traditional microprocessors\nand microcontrollers curricula often fail to capture the complexity of\nreal-world applications. This curriculum addresses this critical gap by\nincorporating insights from experts in both industry and academia. It trains\nstudents with the necessary skills and knowledge to thrive in this rapidly\nevolving technological landscape, preparing them for success upon graduation.\nThe curriculum integrates project-based learning, where students define complex\nengineering problems for themselves. This approach actively engages students,\nfostering a deeper understanding and enhancing their learning capabilities.\nStatistical analysis shows that the proposed curriculum significantly improves\nstudent learning outcomes, particularly in their ability to formulate and solve\ncomplex engineering problems, as well as engage in complex engineering\nactivities.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2502.09288,review,post_llm,2025,2,"{'ai_likelihood': 0.0013404422336154515, 'text': 'AI Safety for Everyone\n\n  Recent discussions and research in AI safety have increasingly emphasized the\ndeep connection between AI safety and existential risk from advanced AI\nsystems, suggesting that work on AI safety necessarily entails serious\nconsideration of potential existential threats. However, this framing has three\npotential drawbacks: it may exclude researchers and practitioners who are\ncommitted to AI safety but approach the field from different angles; it could\nlead the public to mistakenly view AI safety as focused solely on existential\nscenarios rather than addressing a wide spectrum of safety challenges; and it\nrisks creating resistance to safety measures among those who disagree with\npredictions of existential AI risks. Through a systematic literature review of\nprimarily peer-reviewed research, we find a vast array of concrete safety work\nthat addresses immediate and practical concerns with current AI systems. This\nincludes crucial areas like adversarial robustness and interpretability,\nhighlighting how AI safety research naturally extends existing technological\nand systems safety concerns and practices. Our findings suggest the need for an\nepistemically inclusive and pluralistic conception of AI safety that can\naccommodate the full range of safety considerations, motivations, and\nperspectives that currently shape the field.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05722,regular,post_llm,2025,2,"{'ai_likelihood': 0.9951171875, 'text': ""The Role of AI, Blockchain, Cloud, and Data (ABCD) in Enhancing Learning\n  Assessments of College Students\n\n  This study investigates how ABCD technologies can improve learning\nassessments in higher education. The objective is to research how students\nperceive things, plan their behavior, and how ABCD technologies affect\nindividual learning, academic integrity, co-learning, and trust in the\nassessment. Through a quantitative research design, survey responses were\ngathered from university students, and statistical tests, such as correlation\nand regression, were used to establish relationships between Perceived\nUsefulness (PU), Perceived Ease of Use (PEU), and Behavioral Intention (BI)\ntowards ABCD adoption. The results showed that there was no significant\nrelationship between PU, PEU, and BI, which suggests that students' attitudes,\ninstitutional policies, faculty support, and infrastructure matter more in\nadoption than institutional policies, faculty support, and infrastructure.\nWhile students recognize ABCD's efficiency and security benefits, fairness,\nease of use, and engagement issues limit their adoption of these technologies.\nThe research adds to Technology Acceptance Model (TAM) and Constructivist\nLearning Theory (CLT) by emphasizing external drivers of technology adoption.\nThe limitations are based on self-reported data and one institutional sample.\nIt is suggested that universities invest in faculty development,\ninfrastructure, and policy-making to facilitate effective and ethical use of\nABCD technologies in higher education.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.70751953125, 'GPT4': 0.043914794921875, 'CLAUDE': 0.0028247833251953125, 'GOOGLE': 0.2335205078125, 'OPENAI_O_SERIES': 0.0017919540405273438, 'DEEPSEEK': 0.00043654441833496094, 'GROK': 1.0013580322265625e-05, 'NOVA': 2.5331974029541016e-05, 'OTHER': 0.007617950439453125, 'HUMAN': 0.0024967193603515625}}"
2502.09049,regular,post_llm,2025,2,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""WhatsApp as an improvisation of health information systems in Southern\n  African public hospitals: A socio-technical perspective\n\n  Digital health interventions, particularly electronic referrals (e-referrals)\nand health information systems, have revolutionised clinical workflows in\npublic hospitals by automating processes. However, the utilization of\ne-referrals has yielded mixed outcomes, with varying levels of success in\norganisational processes.This paper explores improvisation of health\ninformation systems in Southern African public hospitals from a socio-technical\nperspective. In particular the paper explains the design-reality gaps giving\nrise to improvisations of mandated health information systems in order to\nunderstand their occurrence and impact on referral outcomes. We employed the\ndesign-reality framework and the Process framework for Healthcare Information\nSystem Workarounds and Impacts to explain the socio-technical issues related to\nthe phenomenon of interest.We conducted semi-interviews with 31 respondents\nfrom health organisations as case studies.Respondents from two public hospitals\nin South Africa and two in Namibia were interviewed to examine how they devised\nimprovisations to various health information systems in each setting.The\nfindings showed that using WhatsApp or improvising existing health information\nsystems (HIS) improved efficiency and productivity of healthcare practitioners\n(HCPs) referral activities. Additionally, HCPs reported positive outcomes\nrelated to continual professional development in the given settings.The\nfindings further show a relationship between design-reality gaps and\nimprovisations enacted by HCPs.The observed gaps are related to poor management\nsystems and structures lack of HCPs' involvement in the roll-out of HIS and\ninadequacies of existing HIS to support referral tasks.These study findings can\nbe insightful and useful to system developers and other stakeholders for\ndevising measures to address the gaps.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.00894,review,post_llm,2025,3,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'We Need to Effectively Integrate Computing Skills Across Discipline\n  Curricula\n\n  Computing is increasingly central to innovation across a wide range of\ndisciplinary and interdisciplinary problem domains. Students across\nnoncomputing disciplines need to apply sophisticated computational skills and\nmethods to fields as diverse as biology, linguistics, and art. Furthermore,\ncomputing plays a critical role in ""momentous geopolitical events"", such as\nelections in several countries including the US, and is changing how people\n""work, collaborate, communicate, shop, eat, travel, get news and entertainment,\nand quite simply live"". Traditional computing courses, however, fail to equip\nnon-computing discipline students with the necessary computing skills - if they\ncan even get into classes packed with CS majors. A pressing question facing\nacademics today is: How do we effectively integrate computing skills that are\nuseful for the discipline into discipline curricula?\n  We advocate an approach where courses in discipline X include the computing\nrelevant to the learning outcomes of that course, as used by practitioners in\nX. We refer to the computing skills relevant to a course in discipline X as an\n""ounce of computing skills"", to highlight our belief regarding the amount of\ncomputing to be integrated in that course. In this article, we outline our\ninsights regarding the development of an ounce of computing skills for a\ndiscipline course, and the evaluation of the developed ounce. The key takeaways\nare that the goal has to be to advance students in their disciplines, and only\nthe disciplinary experts can tell us how computing is used in that discipline.\nComputer scientists know how to teach computing, but the classes can\'t be about\nCS values. The disciplinary values are paramount.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.22772,review,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'AI Family Integration Index (AFII): Benchmarking a New Global Readiness\n  for AI as Family\n\n  As Artificial Intelligence (AI) systems increasingly permeate caregiving,\neducational, and emotionally sensitive domains, there is a growing need to\nassess national readiness beyond infrastructure and innovation capacity.\nExisting indices such as the Stanford AI Index (2024), overlooked relational,\nethical, and cultural dimensions essential to human centered AI integration. To\naddress this blind spot, this study introduces the AI Family Integration Index\n(AFII), a ten dimensional benchmarking framework that evaluates national\npreparedness for integrating emotionally intelligent AI into family and\ncaregiving systems. Using mixed-method analysis and equal weighting, the AFII\nprovides a multidimensional tool for assessing emotional and symbolic readiness\nin diverse cultural contexts. A core insight is the policy practice gap: while\nmany governments articulate ethical AI principles, few have implemented them\neffectively in relational or caregiving domains. Countries like Singapore,\nJapan, and South Korea demonstrate alignment between policy intent and\ncaregiving integration, while others such as the United States and France,\nexhibit advanced policy rhetoric but slower real-world execution. This\ndissonance is captured through the AFII Governance Gap Lens. The AFII also\nreveals divergence from conventional rankings: technological leaders like the\nU.S. and China score high in the Stanford AI Index yet rank lower in AFII due\nto weaker caregiving alignment. In contrast, nations like Sweden and Singapore\noutperform on relational readiness despite moderate technical rankings. For\npolicymakers, the AFII offers a practical, scalable, and ethically grounded\ntool to guide inclusive AI strategies, reframing readiness to center care,\nemotional safety, and cultural legitimacy in the age of relational AI.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.00015473365783691406, 'CLAUDE': 0.00022339820861816406, 'GOOGLE': 1.1920928955078125e-07, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.99951171875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.17428,regular,post_llm,2025,3,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'Would you mind being watched by machines? Privacy concerns in data\n  mining\n\n  Data mining is not an invasion of privacy because access to data is only by\nmachines, not by people: this is the argument that is investigated here. The\ncurrent importance of this problem is developed in a case study of data mining\nin the USA for counterterrorism and other surveillance purposes. After a\nclarification of the relevant nature of privacy, it is argued that access by\nmachines cannot warrant the access to further information, since the analysis\nwill have to be made either by humans or by machines that understand. It\nconcludes that the current data mining violates the right to privacy and should\nbe subject to the standard legal constraints for access to private information\nby people.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.09276,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': ""Fine-Tuning Large Language Models for Educational Support: Leveraging\n  Gagne's Nine Events of Instruction for Lesson Planning\n\n  Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.2470951080322266e-05, 'GPT4': 0.99560546875, 'CLAUDE': 9.238719940185547e-05, 'GOOGLE': 0.0029087066650390625, 'OPENAI_O_SERIES': 0.0011243820190429688, 'DEEPSEEK': 1.71661376953125e-05, 'GROK': 3.5762786865234375e-07, 'NOVA': 2.3245811462402344e-06, 'OTHER': 1.1146068572998047e-05, 'HUMAN': 3.230571746826172e-05}}"
2503.15364,review,post_llm,2025,3,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism,\n  Genhumanism\n\n  Three directions for the AI avant-garde are sketched against the background\nof time. Posthumanism changes what we are, and belongs to the radical future.\nTranshumanism changes how we are, and corresponds with the radical past.\nGenhumanism changes who we are, and exists in the radical present. While\ndeveloping the concepts, this essay intersects in two ways with theoretical\ndebates about humanism in the face of technological advance. First, it\ndescribes how temporal divisions may cleanly differentiate post- and\ntranshumanism. Second, the essay introduces generative humanism, which\ncontributes to discussions about AI and society by delineating a novel\nhumanistic response to contemporary technology. Finally, grounds are provided\nfor a practical project, one where philosophers work with AI engineers in the\narea of genhumanism. Contemporary AI research into serendipity in\nrecommendation engines provides natural support for the shared research.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.11727,review,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'Survey of City-Wide Homelessness Detection Through Environmental Sensing\n\n  The growing homelessness crisis in the U.S. presents complex social,\neconomic, and public health challenges, straining shelters, healthcare, and\nsocial services while limiting effective interventions. Traditional assessment\nmethods struggle to capture its dynamic, dispersed nature, highlighting the\nneed for scalable, data-driven detection. This survey explores computational\napproaches across four domains: (1) computer vision and deep learning to\nidentify encampments and urban indicators of homelessness, (2) air quality\nsensing via fixed, mobile, and crowdsourced deployments to assess environmental\nrisks, (3) IoT and edge computing for real-time urban monitoring, and (4)\npedestrian behavior analysis to understand mobility patterns and interactions.\nDespite advancements, challenges persist in computational constraints, data\nprivacy, accurate environmental measurement, and adaptability. This survey\nsynthesizes recent research, identifies key gaps, and highlights opportunities\nto enhance homelessness detection, optimize resource allocation, and improve\nurban planning and social support systems for equitable aid distribution and\nbetter neighborhood conditions.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.291534423828125e-05, 'GPT4': 0.046173095703125, 'CLAUDE': 0.0025234222412109375, 'GOOGLE': 0.005458831787109375, 'OPENAI_O_SERIES': 0.00014078617095947266, 'DEEPSEEK': 0.9453125, 'GROK': 1.430511474609375e-06, 'NOVA': 2.086162567138672e-06, 'OTHER': 0.00012087821960449219, 'HUMAN': 6.735324859619141e-06}}"
2503.23118,regular,post_llm,2025,3,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""Optimizing Library Usage and Browser Experience: Application to the New\n  York Public Library\n\n  We tackle the challenge brought to urban library systems by the {holds\nsystem} -- which allows users to request books available at other branches to\nbe transferred for local pickup. The holds system increases usage of the entire\ncollection, at the expense of an in-person browser's experience at the source\nbranch. We study the optimization of usage and browser experience, where the\nlibrary has two levers: where a book should come from when a hold request is\nplaced, and how many book copies at each branch should be available through the\nholds system versus reserved for browsers. We first show that the problem of\nmaximizing usage can be viewed through the lens of revenue management, for\nwhich near-optimal fulfillment policies exist. We then develop a simulation\nframework that further optimizes for browser experience, through book\nreservations. We empirically apply our methods to data from the New York Public\nLibrary to design implementable policies. We find that though a substantial\ntrade-off exists between these two desiderata, a balanced policy can improve\nbrowser experience over the historical policy without significantly sacrificing\nusage. Because browser usage is more prevalent among branches in low-income\nareas, this policy further increases system-wide equity: notably, for branches\nin the 25% lowest-income neighborhoods, it improves both usage and browser\nexperience by about 15%.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.11714,regular,post_llm,2025,3,"{'ai_likelihood': 0.00011662642161051433, 'text': 'Conversation Networks\n\n  Picture a community torn over a proposed zoning law. Some are angry, others\ndefensive, and misunderstandings abound. On social media, they broadcast\ninsults at one another; every nuanced perspective is reduced to a viral\nsoundbite. Yet, when they meet face-to-face and start speaking, something\nchanges: residents begin listening more than speaking, and people begin testing\nideas together. Misunderstandings fade, and trust begins to form. By the end of\ntheir discussion, they have not only softened their hostility, but discovered\nactionable plans that benefit everyone.\n  This is the kind of meaningful discourse our society desperately needs. Yet\nour digital platforms -- designed primarily for maximizing engagement through\nprovocative content -- have pulled us away from these core community\nendeavours. As a constructive path forward, we introduce the idea of\nconversation networks as a basis for civic communication infrastructure that\ncombines interoperable digital apps with the thoughtful integration of AI\nguided by human agency.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.1264,regular,post_llm,2025,3,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""What is unethical about software? User perceptions in the Netherlands\n\n  Software has the potential to improve lives. Yet, unethical and uninformed\nsoftware practices are at the root of an increasing number of ethical concerns.\nDespite its pervasiveness, few research has analyzed end-users perspectives on\nthe ethical issues of the software they use. We address this gap, and\ninvestigate end-user's ethical concerns in software through 19 semi-structured\ninterviews with residents of the Netherlands. We ask a diverse group of users\nabout their ethical concerns when using everyday software applications. We\ninvestigate the underlying reasons for their concerns and what solutions they\npropose to eliminate them. We find that our participants actively worry about\nprivacy, transparency, manipulation, safety and inappropriate content; with\nprivacy and manipulation often being at the center of their worries. Our\nparticipants demand software solutions to improve information clarity in\napplications and provide more control over the user experience. They further\nexpect larger systematic changes within software practices and government\nregulation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.09748,review,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'Advancing Education through Tutoring Systems: A Systematic Literature\n  Review\n\n  This study systematically reviews the transformative role of Tutoring\nSystems, encompassing Intelligent Tutoring Systems (ITS) and Robot Tutoring\nSystems (RTS), in addressing global educational challenges through advanced\ntechnologies. As many students struggle with proficiency in core academic\nareas, Tutoring Systems emerge as promising solutions to bridge learning gaps\nby delivering personalized and adaptive instruction. ITS leverages artificial\nintelligence (AI) models, such as Bayesian Knowledge Tracing and Large Language\nModels, to provide precise cognitive support, while RTS enhances social and\nemotional engagement through human-like interactions. This systematic review,\nadhering to the PRISMA framework, analyzed 86 representative studies. We\nevaluated the pedagogical and technological advancements, engagement\nstrategies, and ethical considerations surrounding these systems. Based on\nthese parameters, Latent Class Analysis was conducted and identified three\ndistinct categories: computer-based ITS, robot-based RTS, and multimodal\nsystems integrating various interaction modes. The findings reveal significant\nadvancements in AI techniques that enhance adaptability, engagement, and\nlearning outcomes. However, challenges such as ethical concerns, scalability\nissues, and gaps in cognitive adaptability persist. The study highlights the\ncomplementary strengths of ITS and RTS, proposing integrated hybrid solutions\nto maximize educational benefits. Future research should focus on bridging gaps\nin scalability, addressing ethical considerations comprehensively, and\nadvancing AI models to support diverse educational needs.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.430511474609375e-06, 'GPT4': 0.6357421875, 'CLAUDE': 0.0013790130615234375, 'GOOGLE': 0.007259368896484375, 'OPENAI_O_SERIES': 0.355224609375, 'DEEPSEEK': 7.474422454833984e-05, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 6.556510925292969e-07, 'HUMAN': 3.3974647521972656e-06}}"
2504.1609,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': ""Launching Insights: A Pilot Study on Leveraging Real-World Observational\n  Data from the Mayo Clinic Platform to Advance Clinical Research\n\n  Backgrounds: Artificial intelligence (AI) is transforming healthcare, yet\ntranslating AI models from theoretical frameworks to real-world clinical\napplications remains challenging. The Mayo Clinic Platform (MCP) was\nestablished to address these challenges by providing a scalable ecosystem that\nintegrates real-world multiple modalities data from multiple institutions,\nadvanced analytical tools, and secure computing environments to support\nclinical research and AI development. Methods: In this study, we conducted four\nresearch projects leveraging MCP's data infrastructure and analytical\ncapabilities to demonstrate its potential in facilitating real-world evidence\ngeneration and AI-driven clinical insights. Utilizing MCP's tools and\nenvironment, we facilitated efficient cohort identification, data extraction,\nand subsequent statistical or AI-powered analyses. Results: The results\nunderscore MCP's role in accelerating translational research by offering\nde-identified, standardized real-world data and facilitating AI model\nvalidation across diverse healthcare settings. Compared to Mayo's internal\nElectronic Health Record (EHR) data, MCP provides broader accessibility,\nenhanced data standardization, and multi-institutional integration, making it a\nvaluable resource for both internal and external researchers. Conclusion:\nLooking ahead, MCP is well-positioned to transform clinical research through\nits scalable ecosystem, effectively bridging the divide between AI innovation\nand clinical deployment. Future investigations will build upon this foundation,\nfurther exploring MCP's capacity to advance precision medicine and enhance\npatient outcomes.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.000919342041015625, 'GPT4': 0.34033203125, 'CLAUDE': 0.0030994415283203125, 'GOOGLE': 0.56982421875, 'OPENAI_O_SERIES': 0.08453369140625, 'DEEPSEEK': 0.0006318092346191406, 'GROK': 7.748603820800781e-06, 'NOVA': 1.2636184692382812e-05, 'OTHER': 0.0005888938903808594, 'HUMAN': 6.42538070678711e-05}}"
2504.07121,regular,post_llm,2025,3,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'From Public Data to Private Information: The Case of the Supermarket\n\n  The background to this paper is that in our world of massively increasing\npersonal digital data any control over the data about me seems illusionary -\ninformational privacy seems a lost cause. On the other hand, the production of\nthis digital data seems a necessary component of our present life in the\nindustrialized world. A framework for a resolution of this apparent dilemma is\nprovided if by the distinction between (meaningless) data and (meaningful)\ninformation. I argue that computational data processing is necessary for many\npresent-day processes and not a breach of privacy, while collection and\nprocessing of private information is often not necessary and a breach of\nprivacy. The problem and the sketch of its solution are illustrated in a\ncase-study: supermarket customer cards.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.08876,review,post_llm,2025,3,"{'ai_likelihood': 2.914004855685764e-06, 'text': ""Some information is too dangerous to be on the internet\n\n  This paper investigates a problem about freedom of information. Although\nfreedom of information is generally considered desirable, there are a number of\nareas where there is substantial agreement that freedom of information should\nbe limited. After a certain ordering of the landscape, I argue that we need to\nadd the category of 'dangerous' information and that this category has gained a\nnew quality in the context of current information technology, specifically the\nInternet. This category includes information the use of which would be morally\nwrong as well as some of what may be called 'corrupting' information. Some such\ninformation should not be spread at all and some should be very limited in its\nspread.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.09612,regular,post_llm,2025,3,"{'ai_likelihood': 0.008604261610243056, 'text': 'Prioritizing Computing Research to Empower and Protect Vulnerable\n  Populations\n\n  Technology can pose signicant risks to a wide array of vulnerable\npopulations. However, by addressing the challenges and opportunities in\ntechnology design, research, and deployment, we can create systems that benet\neveryone, fostering a society where even the most vulnerable are empowered and\nsupported.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.16137,regular,post_llm,2025,3,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Towards Non-linear Cultural Production and systems of machinic agency:\n  in the case of TikTok value generation\n\n  The rise of TikTok has brought forth novel ways to create and consume media\ncontent, accelerated by technologies such as hyper-individualised algorithms\nand easy-to-use video production tools. Despite its popularity, scholars and\npoliticians alike have raised many concerns on the legitimacy and ethics of\nTikTok regarding its services, and its collected data. However, much of these\ndiscussions take the premise of user-generated content for granted, attributing\nthem to human expression without critically evaluating how the making of\non-platform content production have changed. With a grounded theory approach,\nin conjunction with a platform-aware walkthrough that pays special attention to\nthe material and immaterial premises of platform value generation, my findings\nsuggest that the intensification of datafication have proliferated from\nconsumption behaviours to the process of content production, whereas content\nproduction no longer solely produce media content. As platforms become the\nactive recruiter, mobiliser and co-producer of media production, I argue that\nit is no longer feasible to distinguish human and machine contribution in the\nways they are consumed to facilitate platform valorisation. I propose that the\ntechnical arrangements of TikTok, in relation to its users has fostered a\nnon-linear mode of platform cultural production capable of generating economic\nvalue through a system of machinic agency that incorporates human and machines\nin an indistinguishable manner. As content, the premises of platform\nvalorisation has become an inseparable effort of human-machines, I urge that\nthe relationship between technology and humans be reassessed as a system of\nmachinic agency that mutually shapes our mediated reality, rather than\nsingular, differentiable actors that contribute to platforms.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.08543,regular,post_llm,2025,3,"{'ai_likelihood': 6.622738308376736e-07, 'text': 'BoundarEase: Fostering Constructive Community Engagement to Inform More Equitable Student Assignment Policies\n\nSchool districts across the United States (US) play a pivotal role in shaping access to quality education through their student assignment policies -- most prominently, school attendance boundaries. Community engagement processes for changing such policies, however, are often opaque, cumbersome, and highly polarizing -- hampering equitable access to quality schools in ways that can perpetuate disparities in future life outcomes. In this paper, we describe a collaboration with a large US public school district serving nearly 150,000 students to design and evaluate a new sociotechnical system, ""BoundarEase"", for fostering more constructive community engagement around changing school attendance boundaries. Through a formative study with 16 community members, we first identify several frictions in existing community engagement processes, like individualistic over collective thinking; a failure to understand and empathize with the different ways policies might impact other community members; and challenges in understanding the impacts of boundary changes. These frictions inspire the design and development of BoundarEase, a web platform that allows community members to explore and offer feedback on potential boundaries. A user study with 12 community members reveals that BoundarEase prompts reflection among community members on how policies might impact families beyond their own, and increases transparency around the details of policy proposals. Our paper offers education researchers insights into the challenges and opportunities involved in community engagement for designing student assignment policies; human-computer interaction researchers a case study of how new sociotechnical systems might help mitigate polarization in local policymaking; and school districts a practical tool they might use to facilitate community engagement to foster more equitable student assignment policies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.07125,review,post_llm,2025,3,"{'ai_likelihood': 0.00026861826578776043, 'text': 'Synergizing Self-Regulation and Artificial-Intelligence Literacy Towards\n  Future Human-AI Integrative Learning\n\n  Self-regulated learning (SRL) and Artificial-Intelligence (AI) literacy are\nbecoming key competencies for successful human-AI interactive learning, vital\nto future education. However, despite their importance, students face\nimbalanced and underdeveloped SRL and AI literacy capabilities, inhibiting\neffective using AI for learning. This study analyzed data from 1,704 Chinese\nundergraduates using clustering methods to uncover four learner groups\nreflecting developing process(Potential, Development, Master, and AI-Inclined)\ncharacterized by varying SRL and AI literacy differentiation. Results highlight\nobvious disparities in SRL and AI literacy synchronization, with the Master\nGroup achieving balanced development and critical AI-using for SRL, while\nAI-Inclined Group demonstrate over-reliance on AI and poor SRL application. The\nPotential Group showed a close mutual promotion trend between SRL and AI\nliteracy, while the Development Group showed a discrete correlation. Resources\nand instructional guidance support emerged as key factors affecting these\ndifferentiations. To translate students to master SRL-AI literacy level and\nprogress within it, the study proposes differentiated support strategies and\nsuggestions. Synergizing SRL and AI literacy growth is the core of development,\nensuring equitable and advanced human-centered interactive learning models for\nfuture human-AI integrating.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.08688,regular,post_llm,2025,3,"{'ai_likelihood': 2.8742684258355035e-05, 'text': ""Randomness, Not Representation: The Unreliability of Evaluating Cultural\n  Alignment in LLMs\n\n  Research on the 'cultural alignment' of Large Language Models (LLMs) has\nemerged in response to growing interest in understanding representation across\ndiverse stakeholders. Current approaches to evaluating cultural alignment\nthrough survey-based assessments that borrow from social science methodologies\noften overlook systematic robustness checks. Here, we identify and test three\nassumptions behind current survey-based evaluation methods: (1) Stability: that\ncultural alignment is a property of LLMs rather than an artifact of evaluation\ndesign, (2) Extrapolability: that alignment with one culture on a narrow set of\nissues predicts alignment with that culture on others, and (3) Steerability:\nthat LLMs can be reliably prompted to represent specific cultural perspectives.\nThrough experiments examining both explicit and implicit preferences of leading\nLLMs, we find a high level of instability across presentation formats,\nincoherence between evaluated versus held-out cultural dimensions, and erratic\nbehavior under prompt steering. We show that these inconsistencies can cause\nthe results of an evaluation to be very sensitive to minor variations in\nmethodology. Finally, we demonstrate in a case study on evaluation design that\nnarrow experiments and a selective assessment of evidence can be used to paint\nan incomplete picture of LLMs' cultural alignment properties. Overall, these\nresults highlight significant limitations of current survey-based approaches to\nevaluating the cultural alignment of LLMs and highlight a need for systematic\nrobustness checks and red-teaming for evaluation results. Data and code are\navailable at\nhttps://huggingface.co/datasets/akhan02/cultural-dimension-cover-letters and\nhttps://github.com/ariba-k/llm-cultural-alignment-evaluation, respectively.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.01369,review,post_llm,2025,3,"{'ai_likelihood': 0.000289811028374566, 'text': 'Digital Dybbuks and Virtual Golems: The Ethics of Digital Duplicates in Holocaust Testimony\n\nAdvances in generative artificial intelligence (AI) have driven a growing effort to create digital duplicates. These semi-autonomous recreations of living and dead people can be used for many purposes. Some of these purposes include tutoring, coping with grief, and attending business meetings. However, the normative implications of digital duplicates remain obscure, particularly considering the possibility of them being applied to genocide memory and education. To address this gap, we examine normative possibilities and risks associated with the use of more advanced forms of generative AI-enhanced duplicates for transmitting Holocaust survivor testimonies. We first review the historical and contemporary uses of survivor testimonies. Then, we scrutinize the possible benefits of using digital duplicates in this context and apply the Minimally Viable Permissibility Principle (MVPP). The MVPP is an analytical framework for evaluating the risks of digital duplicates. It includes five core components: the need for authentic presence, consent, positive value, transparency, and harm-risk mitigation. Using MVPP, we identify potential harms digital duplicates might pose to different actors, including survivors, users, and developers. We also propose technical and socio-technical mitigation strategies to address these harms.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.07496,review,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'Securing External Deeper-than-black-box GPAI Evaluations\n\n  This paper examines the critical challenges and potential solutions for\nconducting secure and effective external evaluations of general-purpose AI\n(GPAI) models. With the exponential growth in size, capability, reach and\naccompanying risk of these models, ensuring accountability, safety, and public\ntrust requires frameworks that go beyond traditional black-box methods. The\ndiscussion begins with an analysis of the need for deeper-than-black-box\nevaluations (Section I), emphasizing the importance of understanding model\ninternals to uncover latent risks and ensure compliance with ethical and\nregulatory standards. Building on this foundation, Section II addresses the\nsecurity considerations of remote evaluations, outlining the threat landscape,\ntechnical solutions, and safeguards necessary to protect both evaluators and\nproprietary model data. Finally, Section III synthesizes these insights into\nactionable recommendations and future directions, aiming to establish a robust,\nscalable, and transparent framework for external assessments in GPAI\ngovernance.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.562999725341797e-06, 'GPT4': 0.998046875, 'CLAUDE': 2.0623207092285156e-05, 'GOOGLE': 0.0001010894775390625, 'OPENAI_O_SERIES': 5.066394805908203e-06, 'DEEPSEEK': 0.0018291473388671875, 'GROK': 4.565715789794922e-05, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.6689300537109375e-06, 'HUMAN': 6.556510925292969e-07}}"
2503.20989,regular,post_llm,2025,3,"{'ai_likelihood': 9.934107462565105e-07, 'text': 'Inferring fine-grained migration patterns across the United States\n\nFine-grained migration data illuminate important demographic, environmental, and health phenomena. However, migration datasets within the United States remain lacking: publicly available Census data are neither spatially nor temporally granular, and proprietary data have higher resolution but demographic and other biases. To address these limitations, we develop a scalable iterative-proportional-fitting based method that reconciles high-resolution but biased proprietary data with low-resolution but more reliable Census data. We apply this method to produce MIGRATE, a dataset of annual migration matrices from 2010 - 2019 that captures flows between 47.4 billion pairs of Census Block Groups -- about four thousand times more granular than publicly available data. These estimates are highly correlated with external ground-truth datasets, and improve accuracy and reduce bias relative to raw proprietary data. We use MIGRATE to analyze both national and local migration patterns. Nationally, we document temporal and demographic variation in homophily, upward mobility, and moving distance: for example, we find that people are increasingly likely to move to top-income-quartile CBGs and identify racial disparities in upward mobility. We also show that MIGRATE can illuminate important local migration patterns, including out-migration in response to California wildfires, that are invisible in coarser previous datasets. We publicly release MIGRATE to provide a resource for migration research in the social, environmental, and health sciences.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.04988,regular,post_llm,2025,3,"{'ai_likelihood': 5.056460698445638e-05, 'text': 'Prevalence and Impacts of Image-Based Sexual Abuse Victimization: A\n  Multinational Study\n\n  Image-based sexual abuse (IBSA) refers to the nonconsensual creating, taking,\nor sharing of intimate images, including threats to share intimate images.\nDespite the significant harms of IBSA, there is limited data on its prevalence\nand how it affects different identity or demographic groups. This study\nexamines prevalence of, impacts from, and responses to IBSA via a survey with\nover 16,000 adults in 10 countries. More than 1 in 5 (22.6%) respondents\nreported an experience of IBSA. Victimization rates were higher among LGBTQ+\nand younger respondents. Although victimized at similar rates, women reported\ngreater harms and negative impacts from IBSA than men. Nearly a third (30.9%)\nof victim-survivors did not report or disclose their experience to anyone. We\nprovide large-scale, granular, baseline data on prevalence in a diverse set of\ncountries to aid in the development of effective interventions that address the\nexperiences and intersectional identities of victim-survivors.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.24095,review,post_llm,2025,3,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Threats and Opportunities in AI-generated Images for Armed Forces\n\n  Images of war are almost as old as war itself. From cave paintings to\nphotographs of mobile devices on social media, humans always had the urge to\ncapture particularly important events during a war. Images provide visual\nevidence. For armed forces, they may serve as the output of a sensor (e.g. in\naerial reconnaissance) or as an effector on cognition (e.g. in form of\nphotographic propaganda). They can inform, influence, or even manipulate a\ntarget audience. The recent advancements in the field of generative Artificial\nIntelligence (AI) to synthesize photorealistic images give rise to several new\nchallenges for armed forces. The objective of this report is to investigate the\nrole of AI-generated images for armed forces and provide an overview on\nopportunities and threats. When compared with traditional image generation\n(e.g. photography), generative AI brings distinct conceptual advantages to\nimplement new tactical tenets and concepts which so far have not been feasible:\nmasses of AI-generated images can be used for deceptive purposes, to influence\nthe pace of combat in the information environment, to cause surprise, sow\nconfusion and shock. AI-generated images are a tool favoured for offensive\nmanoeuvres in the information environment. To prepare for future challenges\ninvolving AI-generated images and improve their resilience, recommendations are\ngiven at the end of the report for all branches of the armed forces, who are\nactive in cyber defense and/or exposed to the information environment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.12207,regular,post_llm,2025,3,"{'ai_likelihood': 0.07473415798611112, 'text': 'ReDefining Code Comprehension: Function Naming as a Mechanism for\n  Evaluating Code Comprehension\n\n  ""Explain in Plain English"" (EiPE) questions are widely used to assess code\ncomprehension skills but are challenging to grade automatically. Recent\napproaches like Code Generation Based Grading (CGBG) leverage large language\nmodels (LLMs) to generate code from student explanations and validate its\nequivalence to the original code using unit tests. However, this approach does\nnot differentiate between high-level, purpose-focused responses and low-level,\nimplementation-focused ones, limiting its effectiveness in assessing\ncomprehension level. We propose a modified approach where students generate\nfunction names, emphasizing the function\'s purpose over implementation details.\nWe evaluate this method in an introductory programming course and analyze it\nusing Item Response Theory (IRT) to understand its effectiveness as exam items\nand its alignment with traditional EiPE grading standards. We also publish this\nwork as an open source Python package for autograding EiPE questions, providing\na scalable solution for adoption.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.03849,review,post_llm,2025,3,"{'ai_likelihood': 2.980232238769531e-07, 'text': ""A Bridge to Nowhere: A Healthcare Case Study for Non-Reformist Design\n\n  In the face of intensified datafication and automation in public sector\nindustries, frameworks like design justice and the feminist practice of refusal\nprovide help to identify and mitigate structural harm and challenge inequities\nreproduced in digitized infrastructures. This paper applies those frameworks to\nemerging efforts across the U.S. healthcare industry to automate prior\nauthorization -- a process whereby insurance companies determine whether a\ntreatment or service is 'medically necessary' before agreeing to cover it.\nFederal regulatory interventions turn to datafication and automation to reduce\nthe harms of this widely unpopular process shown to delay vital treatments and\ncreate immense administrative burden for healthcare providers and patients.\nThis paper explores emerging prior authorization reforms as a case study,\napplying the frameworks of design justice and refusal to highlight the inherent\nconservatism of interventions oriented towards improving the user experience of\nextractive systems. I further explore how the abolitionist framework of\nnon-reformist reform helps to clarify alternative interventions that would\nmitigate the harms of prior authorization in ways that do not reproduce or\nextend the power of insurance companies. I propose a set of four tenets for\nnonreformist design to mitigate structural harms and advance design justice in\na broad set of domains.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.03939,review,post_llm,2025,3,"{'ai_likelihood': 1.7119778527153863e-05, 'text': 'Reflecting on Potentials for Post-Growth Social Media Platform Design\n\n  Sudden attention on social media, and how users navigate these contextual\nshifts, has been a focus of much recent work in social media research. Even\nwhen this attention is not harassing, some users experience this sudden growth\nas overwhelming. In this workshop paper, I outline how growth infuses the\ndesign of much of the modern social media platform landscape, and then explore\nwhy applying a post-growth lens to platform design could be productive.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.09614,regular,post_llm,2025,3,"{'ai_likelihood': 0.4755316840277778, 'text': 'Reversing the Computing Research Workforce Shortfall: Bolstering\n  Domestic Student Pathways to PhDs\n\n  To sustain innovation and safeguard national security, the U.S. must\nstrengthen domestic pathways to computing PhDs by engaging talented\nundergraduates early - before they are committed to industry - with research\nexperiences, mentorship, and financial support for graduate studies.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.16621,review,post_llm,2025,3,"{'ai_likelihood': 1.7053551144070097e-05, 'text': 'Allocation Multiplicity: Evaluating the Promises of the Rashomon Set\n\nThe Rashomon set of equally-good models promises less discriminatory algorithms, reduced outcome homogenization, and fairer decisions through model ensembles or reconciliation. However, we argue from the perspective of allocation multiplicity that these promises may remain unfulfilled. When there are more qualified candidates than resources available, many different allocations of scarce resources can achieve the same utility. This space of equal-utility allocations may not be faithfully reflected by the Rashomon set, as we show in a case study of healthcare allocations. We attribute these unfulfilled promises to several factors: limitations in empirical methods for sampling from the Rashomon set, the standard practice of deterministically selecting individuals with the lowest risk, and structural biases that cause all equally-good models to view some qualified individuals as inherently risky.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.17976,review,post_llm,2025,3,"{'ai_likelihood': 8.165836334228516e-05, 'text': 'Interpersonal Trust Among Students in Virtual Learning Environments: A Comprehensive Review\n\nInterpersonal trust is recognized as one of the pillars of collaboration and successful learning among students in virtual learning environments (VLEs). This systematic mapping study investigates attributes, phases, and features that support interpersonal trust among students in VLEs. Analyzing 46 articles, we identified 37 attributes that influence phases of acquiring and losing trust, categorized into four themes: Ability, Integrity, Affinity, and Non-Personal Factors. Attributes such as collaborative and ethical behavior, academic skills, and higher grades are often used to select peers, mainly through recommendation systems and user profiles. To organize our findings, we elaborated two conceptual maps describing the main characteristics of trust definitions and the attributes classification by phases and themes.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.08003,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': ""How Can Video Generative AI Transform K-12 Education? Examining\n  Teachers' Perspectives through TPACK and TAM\n\n  The rapid advancement of generative AI technology, particularly video\ngenerative AI (Video GenAI), has opened new possibilities for K-12 education by\nenabling the creation of dynamic, customized, and high-quality visual content.\nDespite its potential, there is limited research on how this emerging\ntechnology can be effectively integrated into educational practices. This study\nexplores the perspectives of leading K-12 teachers on the educational\napplications of Video GenAI, using the TPACK (Technological Pedagogical Content\nKnowledge) and TAM (Technology Acceptance Model) frameworks as analytical\nlenses. Through interviews and hands-on experimentation with video generation\ntools, the research identifies opportunities for enhancing teaching strategies,\nfostering student engagement, and supporting authentic task design. It also\nhighlights challenges such as technical limitations, ethical considerations,\nand the need for institutional support. The findings provide actionable\ninsights into how Video GenAI can transform teaching and learning, offering\npractical implications for policy, teacher training, and the future development\nof educational technology.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00016891956329345703, 'GPT4': 0.888671875, 'CLAUDE': 0.0281829833984375, 'GOOGLE': 0.0012378692626953125, 'OPENAI_O_SERIES': 0.0158538818359375, 'DEEPSEEK': 0.06488037109375, 'GROK': 1.0073184967041016e-05, 'NOVA': 3.7550926208496094e-06, 'OTHER': 0.0008368492126464844, 'HUMAN': 3.3974647521972656e-05}}"
2503.19655,regular,post_llm,2025,3,"{'ai_likelihood': 2.1523899502224394e-06, 'text': ""A Cross-Country Analysis of GDPR Cookie Banners and Flexible Methods for\n  Scraping Them\n\n  Online tracking remains problematic, with compliance and ethical issues\npersisting despite regulatory efforts. Consent interfaces, the visible\nmanifestation of this industry, have seen significant attention over the years.\nWe present robust automated methods to study the presence, design, and\nthird-party suppliers of consent interfaces at scale and the web service\nconsent-observatory.eu to do it with. We examine the top 10,000 websites across\n31 countries under the ePrivacy Directive and GDPR (n=254.148). Our findings\nshow that 67% of websites use consent interfaces, but only 15% are minimally\ncompliant, mostly because they lack a reject option. Consent management\nplatforms (CMPs) are powerful intermediaries in this space: 67% of interfaces\nare provided by CMPs, and three organisations hold 37% of the market. There is\nlittle evidence that regulators' guidance and fines have impacted compliance\nrates, but 18% of compliance variance is explained by CMPs. Researchers should\ntake an infrastructural perspective on online tracking and study the factual\ncontrol of intermediaries to identify effective leverage points.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05814,review,post_llm,2025,3,"{'ai_likelihood': 4.801485273573134e-06, 'text': ""Section 230: A Juridical History\n\n  Section 230 of the Communications Decency Act of 1996 is the most important\nlaw in the history of the internet. It is also one of the most flawed. Under\nSection 230, online entities are absolutely immune from lawsuits related to\ncontent authored by third parties. The law has been essential to the internet's\ndevelopment over the last twenty years, but it has not kept pace with the times\nand is now a source of deep consternation to courts and legislatures. Lawmakers\nand legal scholars from across the political spectrum praise the law for what\nit has done, while criticizing its protection of bad-actor websites and\nobstruction of internet law reform.\n  Absent from the fray, however, has been the Supreme Court, which has never\nissued a decision interpreting Section 230. That is poised to change, as the\nCourt now appears determined to peel back decades of lower court case law and\ninterpret the statute afresh to account for the tremendous technological\nadvances of the last two decades. Rather than offer a proposal for reform, of\nwhich there are plenty, this Article acts as a guidebook to reformers by\nexamining how we got to where we are today. It identifies those interpretive\nsteps and missteps by which courts constructed an immunity doctrine\ninsufficiently resilient against technological change, with the aim of aiding\nlawmakers and scholars in crafting an immunity doctrine better situated to\naccommodate future innovation.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05801,regular,post_llm,2025,3,"{'ai_likelihood': 3.427267074584961e-05, 'text': 'Enabling the AI Revolution in Healthcare\n\n  The transformative potential of AI in healthcare - including better\ndiagnostics, treatments, and expanded access - is currently limited by siloed\npatient data across multiple systems. Federal initiatives are necessary to\nprovide critical infrastructure for health data repositories for data sharing,\nalong with mechanisms to enable access to this data for appropriately trained\ncomputing researchers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.23056,review,post_llm,2025,3,"{'ai_likelihood': 0.007425944010416667, 'text': ""Achieving Socio-Economic Parity through the Lens of EU AI Act\n\n  Unfair treatment and discrimination are critical ethical concerns in AI\nsystems, particularly as their adoption expands across diverse domains.\nAddressing these challenges, the recent introduction of the EU AI Act\nestablishes a unified legal framework to ensure legal certainty for AI\ninnovation and investment while safeguarding public interests, such as health,\nsafety, fundamental rights, democracy, and the rule of law (Recital 8). The Act\nencourages stakeholders to initiate dialogue on existing AI fairness notions to\naddress discriminatory outcomes of AI systems. However, these notions often\noverlook the critical role of Socio-Economic Status (SES), inadvertently\nperpetuating biases that favour the economically advantaged. This is\nconcerning, given that principles of equalization advocate for equalizing\nresources or opportunities to mitigate disadvantages beyond an individual's\ncontrol. While provisions for discrimination are laid down in the AI Act,\nspecialized directions should be broadened, particularly in addressing economic\ndisparities perpetuated by AI systems. In this work, we explore the limitations\nof popular AI fairness notions using a real-world dataset (Adult), highlighting\ntheir inability to address SES-driven disparities. To fill this gap, we propose\na novel fairness notion, Socio-Economic Parity (SEP), which incorporates SES\nand promotes positive actions for underprivileged groups while accounting for\nfactors within an individual's control, such as working hours, which can serve\nas a proxy for effort. We define a corresponding fairness measure and optimize\na model constrained by SEP to demonstrate practical utility. Our results show\nthe effectiveness of SEP in mitigating SES-driven biases. By analyzing the AI\nAct alongside our method, we lay a foundation for aligning AI fairness with SES\nfactors while ensuring legal compliance.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.05811,regular,post_llm,2025,3,"{'ai_likelihood': 0.92431640625, 'text': ""Blockchain Technology Adoption in Food Bank Supply Chains: A Rough\n  DEMATEL-Based Approach\n\n  Food banks can improve food donation administration, provide real-time\ninventory tracking, and guarantee compliance with food safety regulations by\nincorporating blockchain technology. The efficiency, openness, and\ndependability of food bank supply chains are greatly increased by this\nintegration, leading to more sustainable and successful operations. This study\nfocuses on two primary objectives: identifying key barriers to effective Food\nbank supply chain (FBSC) operations in blockchain adoption and exploring the\ninterrelationships among these barriers. Barriers were categorized into\nexternal and internal frameworks and analyzed using insights from academics and\nFBs experts. The Decision-Making Trial and Evaluation Laboratory (DEMATEL)\nmethodology was employed to model and quantify the causal relationships among\nthese barriers. DEMATEL's strength lies in its ability to map interdependencies\nand feedback loops, providing a nuanced understanding of the links between\nindependent and dependent variables in a cause-and-effect network. To address\nsubjectivity and ambiguity in expert opinions during group decision-making,\nrough theory was integrated with DEMATEL, ensuring a robust approach to\nhandling conflicting perspectives and uncertainty.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0020275115966796875, 'GPT4': 0.1134033203125, 'CLAUDE': 0.00473785400390625, 'GOOGLE': 0.83349609375, 'OPENAI_O_SERIES': 0.032806396484375, 'DEEPSEEK': 0.0011701583862304688, 'GROK': 3.2067298889160156e-05, 'NOVA': 0.00019037723541259766, 'OTHER': 0.0032901763916015625, 'HUMAN': 0.0086517333984375}}"
2503.03334,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'IoT Integration Protocol for Enhanced Hospital Care\n\n  This paper introduces the ""IoT Integration Protocol for Enhanced Hospital\nCare"", a comprehensive framework designed to leverage Internet of Things (IoT)\ntechnology to enhance patient care, improve operational efficiency, and ensure\ndata security in hospital settings. With the growing emphasis on utilizing\nadvanced technologies in healthcare, this protocol aims to harness the\npotential of IoT devices to optimize patient monitoring, enable remote care,\nand support clinical decision-making. By integrating IoT seamlessly into\nnursing workflows and patient care plans, hospitals can achieve higher levels\nof patient-centric care and real-time data insights, leading to better\ntreatment outcomes and resource allocation. This paper outlines the protocol\'s\nobjectives, key components, and expected benefits, while emphasizing the\nimportance of ethical considerations and ongoing evaluation to ensure\nsuccessful implementation.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0246429443359375, 'GPT4': 0.90087890625, 'CLAUDE': 0.003787994384765625, 'GOOGLE': 0.0181427001953125, 'OPENAI_O_SERIES': 0.004383087158203125, 'DEEPSEEK': 0.0012922286987304688, 'GROK': 0.00021731853485107422, 'NOVA': 0.01006317138671875, 'OTHER': 0.036407470703125, 'HUMAN': 3.337860107421875e-06}}"
2503.18952,regular,post_llm,2025,3,"{'ai_likelihood': 0.02471923828125, 'text': 'Reclaiming the Future: American Information Technology Leadership in an\n  Era of Global Competition\n\n  The United States risks losing its global leadership in information\ntechnology research due to declining basic research funding, challenges in\nattracting talent, and tensions between research security and openness.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.20833,review,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'The Oxford Insights Government AI Readiness Index (GARI): An Analysis of\n  its Data and Overcoming Obstacles, with a Case Study of Iraq\n\n  This research examines the ""Government AI Readines Index"" (GARI) issued by\nOxford, analyzing data on governmental preparedness for adopting artificial\nintelligence acros different countrie. It highlights the evaluation criteria\nused to assess readiness, including technological infrastructure, human\nresources, supportive policies, and the level of innovation.\n  The study specifically focuses on Iraq, exploring the challenge the Iraqi\ngovernment face in adopting and implementing AI technology. It discussed\neconomic, social, and political barriers that hinder this transition and\nprovides concrete recommendations to overcome these obstacle.\n  By analyzing Iraq case, the research aims to offer insight into improving\ncollaboration between the public and private sectors to enhance the effective\nuse of AI in governance and public administration. Additionally, the study\nemphasizes the importance of investing in education, training, and capacity\nbuilding to develop a skilled workforce, enabling countries to harness AI\npotential and improve government service efficiency.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.08050537109375, 'GPT4': 0.08880615234375, 'CLAUDE': 0.0008664131164550781, 'GOOGLE': 0.81884765625, 'OPENAI_O_SERIES': 0.0017528533935546875, 'DEEPSEEK': 0.00012803077697753906, 'GROK': 5.418062210083008e-05, 'NOVA': 9.810924530029297e-05, 'OTHER': 0.00888824462890625, 'HUMAN': 1.7881393432617188e-05}}"
2503.09882,regular,post_llm,2025,3,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""IT Students Career Confidence and Career Identity During COVID-19\n\n  COVID-19 disrupted the professional preparation of university students, with\nless opportunity to engage in professional practice due to a reduced employment\nmarket. Little is known about how this period impacted upon the career\nconfidence and career identity of university students. This research paper\nexplores the career confidence and identity of university students in\nInformation Technology (IT) prior and during the COVID-19 period. Using a\nsurvey method and quantitative analysis, ANOVA and Kruskal-Wallis tests with\ndifferent sensitivity and variance standards were used during analysis to\npresent mean and mean rank of data collected during 2018, 2019, 2020 and 2021.\n1349 IT students from an Australian University reported their career\nconfidence. The results indicate IT students' career confidence maintained\nduring the period. In 2021, the results indicate increased career commitment of\nIT students showing higher professional expectations to work in IT along with\ngreater self-awareness regarding their professional development needs. Even\nwith increased career confidence as observed in this study, supporting\nuniversity students to explore their career options and build upon their career\nidentity, and more broadly their employability, remains an important activity\nfor universities to curate in their graduates.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.06035,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'The Liabilities of Robots.txt\n\nThis paper explores the legal implications of violating ""robots.txt"", a technical standard widely used by webmasters to communicate restrictions on automated access to website content. Although historically regarded as a voluntary guideline, the rise of generative AI and large-scale web scraping has amplified the consequences of disregarding ""robots.txt"" directives. While previous legal discourse has largely focused on criminal or copyright-based remedies, we argue that civil doctrines, particularly in contract and tort law, offer a more balanced and sustainable framework for regulating web robot behavior in common law jurisdictions. Under certain conditions, ""robots.txt"" can give rise to a unilateral contract or serve as a form of notice sufficient to establish tortious liability, including trespass to chattels and negligence. Ultimately, we argue that clarifying liability for ""robots.txt"" violations is essential to addressing the growing fragmentation of the internet. By restoring balance and accountability in the digital ecosystem, our proposed framework helps preserve the internet\'s open and cooperative foundations. Through this lens, ""robots.txt"" can remain an equitable and effective tool for digital governance in the age of AI.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00012576580047607422, 'GPT4': 0.02880859375, 'CLAUDE': 0.96337890625, 'GOOGLE': 0.0009713172912597656, 'OPENAI_O_SERIES': 0.000522613525390625, 'DEEPSEEK': 0.006298065185546875, 'GROK': 4.172325134277344e-07, 'NOVA': 2.2649765014648438e-06, 'OTHER': 4.3392181396484375e-05, 'HUMAN': 1.6748905181884766e-05}}"
2503.11705,regular,post_llm,2025,3,"{'ai_likelihood': 6.953875223795573e-06, 'text': 'The BIG Argument for AI Safety Cases\n\n  We present our Balanced, Integrated and Grounded (BIG) argument for assuring\nthe safety of AI systems. The BIG argument adopts a whole-system approach to\nconstructing a safety case for AI systems of varying capability, autonomy and\ncriticality. Firstly, it is balanced by addressing safety alongside other\ncritical ethical issues such as privacy and equity, acknowledging complexities\nand trade-offs in the broader societal impact of AI. Secondly, it is integrated\nby bringing together the social, ethical and technical aspects of safety\nassurance in a way that is traceable and accountable. Thirdly, it is grounded\nin long-established safety norms and practices, such as being sensitive to\ncontext and maintaining risk proportionality. Whether the AI capability is\nnarrow and constrained or general-purpose and powered by a frontier or\nfoundational model, the BIG argument insists on a systematic treatment of\nsafety. Further, it places a particular focus on the novel hazardous behaviours\nemerging from the advanced capabilities of frontier AI models and the open\ncontexts in which they are rapidly being deployed. These complex issues are\nconsidered within a wider AI safety case, approaching assurance from both\ntechnical and sociotechnical perspectives. Examples illustrating the use of the\nBIG argument are provided throughout the paper.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.15205,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'A Peek Behind the Curtain: Using Step-Around Prompt Engineering to\n  Identify Bias and Misinformation in GenAI Models\n\n  This research examines the emerging technique of step-around prompt\nengineering in GenAI research, a method that deliberately bypasses AI safety\nmeasures to expose underlying biases and vulnerabilities in GenAI models. We\ndiscuss how Internet-sourced training data introduces unintended biases and\nmisinformation into AI systems, which can be revealed through the careful\napplication of step-around techniques.\n  Drawing parallels with red teaming in cybersecurity, we argue that\nstep-around prompting serves a vital role in identifying and addressing\npotential vulnerabilities while acknowledging its dual nature as both a\nresearch tool and a potential security threat. Our findings highlight three key\nimplications: (1) the persistence of Internet-derived biases in AI training\ndata despite content filtering, (2) the effectiveness of step-around techniques\nin exposing these biases when used responsibly, and (3) the need for robust\nsafeguards against malicious applications of these methods.\n  We conclude by proposing an ethical framework for using step-around prompting\nin AI research and development, emphasizing the importance of balancing system\nimprovements with security considerations.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1920928955078125e-06, 'GPT4': 0.00023484230041503906, 'CLAUDE': 0.99951171875, 'GOOGLE': 9.644031524658203e-05, 'OPENAI_O_SERIES': 1.9073486328125e-05, 'DEEPSEEK': 2.09808349609375e-05, 'GROK': 0.0, 'NOVA': 1.7881393432617188e-07, 'OTHER': 5.7220458984375e-06, 'HUMAN': 1.1920928955078125e-07}}"
2503.01532,regular,post_llm,2025,3,"{'ai_likelihood': 0.00042120615641276043, 'text': 'Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios\n\nLarge language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a ""default persona"" bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover, interactions involving specific demographics are associated with lower-quality responses. Lastly, the presence of power disparities increases variability in response semantics and quality across demographic groups, suggesting that implicit biases may be heightened under power-imbalanced conditions. These insights expose the demographic biases inherent in LLMs and offer potential paths toward future bias mitigation efforts in LLMs.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.17246,review,post_llm,2025,3,"{'ai_likelihood': 2.317958407931858e-07, 'text': ""Decentralization: A Qualitative Survey of Node Operators\n\nDecentralization is understood both by professionals in the blockchain industry and general users as a core design goal of permissionless ledgers. However, its meaning is far from universally agreed, and often it is easier to get opinions on what it is not, rather than what it is. In this paper, we solicit definitions of 'decentralization' and 'decentralization theatre' from blockchain node operators. Key to a definition is asking about effective decentralization strategies, as well as those that are ineffective. Malicious, deceptive, or incompetent strategies are commonly referred to by the term 'decentralization theatre.' Finally, we ask what is being decentralized. Via thematic analysis of interview transcripts, we find that most operators conceive of decentralization as existing broadly on a technical and a governance axis. This informs a two-axis model: network topology and governance topology, or the structure of decision-making power. Our key finding is that `decentralization' alone does not affect ledger immutability or systemic robustness."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.01037,regular,post_llm,2025,3,"{'ai_likelihood': 0.97314453125, 'text': 'TaMPERing with Large Language Models: A Field Guide for using Generative\n  AI in Public Administration Research\n\n  The integration of Large Language Models (LLMs) into social science research\npresents transformative opportunities for advancing scientific inquiry,\nparticularly in public administration (PA). However, the absence of\nstandardized methodologies for using LLMs poses significant challenges for\nensuring transparency, reproducibility, and replicability. This manuscript\nintroduces the TaMPER framework-a structured methodology organized around five\ncritical decision points: Task, Model, Prompt, Evaluation, and Reporting. The\nTaMPER framework provides scholars with a systematic approach to leveraging\nLLMs effectively while addressing key challenges such as model variability,\nprompt design, evaluation protocols, and transparent reporting practices.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00034356117248535156, 'GPT4': 0.006908416748046875, 'CLAUDE': 0.7041015625, 'GOOGLE': 0.27294921875, 'OPENAI_O_SERIES': 0.0025997161865234375, 'DEEPSEEK': 0.00487518310546875, 'GROK': 5.8770179748535156e-05, 'NOVA': 0.00033164024353027344, 'OTHER': 0.0045013427734375, 'HUMAN': 0.00336456298828125}}"
2503.15682,review,post_llm,2025,3,"{'ai_likelihood': 3.874301910400391e-06, 'text': 'Transfeminist AI Governance\n\n  This article re-imagines the governance of artificial intelligence (AI)\nthrough a transfeminist lens, focusing on challenges of power, participation,\nand injustice, and on opportunities for advancing equity, community-based\nresistance, and transformative change. AI governance is a field of research and\npractice seeking to maximize benefits and minimize harms caused by AI systems.\nUnfortunately, AI governance practices are frequently ineffective at preventing\nAI systems from harming people and the environment, with historically\nmarginalized groups such as trans people being particularly vulnerable to harm.\nBuilding upon trans and feminist theories of ethics, I introduce an approach to\ntransfeminist AI governance. Applying a transfeminist lens in combination with\na critical self-reflexivity methodology, I retroactively reinterpret findings\nfrom three empirical studies of AI governance practices in Canada and globally.\nIn three reflections on my findings, I show that large-scale AI governance\nsystems structurally prioritize the needs of industry over marginalized\ncommunities. As a result, AI governance is limited by power imbalances and\nexclusionary norms. This research shows that re-grounding AI governance in\ntransfeminist ethical principles can support AI governance researchers,\npractitioners, and organizers in addressing those limitations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.1014,review,post_llm,2025,3,"{'ai_likelihood': 6.126032935248481e-06, 'text': ""Sovereignty in the digital era: the quest for continuous access to\n  dependable technological capabilities\n\n  In an era where economies and societies are deeply integrated into\ncyberspace, achieving a robust level of digital sovereignty has become an\nessential goal for nations aiming to preserve their security and strategic\npolitical autonomy, particularly during turbulent geopolitical times marked by\ncomplex global supply chains of critical technologies that ties systemic\nrivals. Digital sovereignty is a multifaceted, interdisciplinary, and dynamic\npursuit that fundamentally relies on a nation's ability to have continuous\naccess to dependable technological capabilities (CTCs) for storing,\ntransferring, and processing domestically produced data. This paper identifies\nhow access continuity or technological dependability could be threatened by\nseveral malicious actions from cyberattacks, supply chain tamperings, political\nor economic actions. By examining different approaches adopted by countries\nlike the United States, China, and the European Union, we highlight different\nstrategies to get access to CTCs depending on their political, economic and\ninstitutional nature.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.1382,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': ""A Preliminary Investigation into Theory-Practice Barriers in Sino-New\n  Zealand Undergraduate Computing Education\n\n  This paper investigates the barriers hindering the effective transition from\ntheoretical knowledge to practical application in a Sino-New Zealand\ndouble-degree undergraduate computing program. In this unique educational\nsetting, students study at a campus in China but complete both Chinese and New\nZealand courses taught jointly by lecturers from both countries. Through a\nquestionnaire administered to these students, we identify critical obstacles\nsuch as insufficient foundational knowledge, language barriers, cultural and\npedagogical differences, and difficulties adapting to distinct educational\nsystems. Our analysis reveals that these barriers significantly affect\nstudents' academic performance, engagement, and skill development. Based on the\nfindings, we propose targeted interventions, including specialized bridging\ncourses, enhanced language support, refined teaching methods, and improved\nresource allocation.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00025963783264160156, 'GPT4': 0.1490478515625, 'CLAUDE': 0.10540771484375, 'GOOGLE': 0.708984375, 'OPENAI_O_SERIES': 0.0120391845703125, 'DEEPSEEK': 0.0091400146484375, 'GROK': 0.0001938343048095703, 'NOVA': 0.00099945068359375, 'OTHER': 0.0136871337890625, 'HUMAN': 3.5703182220458984e-05}}"
2503.22315,review,post_llm,2025,3,"{'ai_likelihood': 5.629327562120226e-07, 'text': 'Large Language Models Are Democracy Coders with Attitudes\n\nCurrent political developments worldwide illustrate that research on democratic backsliding is as important as ever. A recent exchange in Political Science & Politics (2/2024) has highlighted again a fundamental challenge in this literature: the measurement of democracy. With many democracy indicators consisting of subjective assessments rather than factual observations, trends in democracy over time could be due to human biases in the coding of these indicators rather than empirical facts. In this paper, we leverage two cutting-edge Large Language Models (LLMs) for the coding of democracy indicators from the V-Dem project. With access to a huge amount of information, these models may be able to rate the many ""soft"" characteristics of regimes without the cognitive biases that humans potentially possess. While LLM-generated codings largely align with expert coders for many countries, we show that when these models deviate from human assessments, they do so in different but consistent ways: Some LLMs are too pessimistic, while others consistently overestimate the democratic quality of these countries. While the combination of the two LLM codings can alleviate this concern, we conclude that it is difficult to replace human coders with LLMs, since the extent and direction of these attitudes is not known a priori.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.18724,review,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': ""From Trust to Truth: Actionable policies for the use of AI in\n  fact-checking in Germany and Ukraine\n\n  The rise of Artificial Intelligence (AI) presents unprecedented opportunities\nand challenges for journalism, fact-checking and media regulation. While AI\noffers tools to combat disinformation and enhance media practices, its\nunregulated use and associated risks necessitate clear policies and\ncollaborative efforts. This policy paper explores the implications of\nartificial intelligence (AI) for journalism and fact-checking, with a focus on\naddressing disinformation and fostering responsible AI integration. Using\nGermany and Ukraine as key case studies, it identifies the challenges posed by\ndisinformation, proposes regulatory and funding strategies, and outlines\ntechnical standards to enhance AI adoption in media. The paper offers\nactionable recommendations to ensure AI's responsible and effective integration\ninto media ecosystems. AI presents significant opportunities to combat\ndisinformation and enhance journalistic practices. However, its implementation\nlacks cohesive regulation, leading to risks such as bias, transparency issues,\nand over-reliance on automated systems. In Ukraine, establishing an independent\nmedia regulatory framework adapted to its governance is crucial, while Germany\ncan act as a leader in advancing EU-wide collaborations and standards.\nTogether, these efforts can shape a robust AI-driven media ecosystem that\npromotes accuracy and trust.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00041031837463378906, 'GPT4': 0.7431640625, 'CLAUDE': 0.002155303955078125, 'GOOGLE': 0.120361328125, 'OPENAI_O_SERIES': 0.13037109375, 'DEEPSEEK': 0.0032176971435546875, 'GROK': 5.960464477539062e-07, 'NOVA': 5.2869319915771484e-05, 'OTHER': 2.2649765014648438e-05, 'HUMAN': 0.00021576881408691406}}"
2503.0225,review,post_llm,2025,3,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""AI Automatons: AI Systems Intended to Imitate Humans\n\n  There is a growing proliferation of AI systems designed to mimic people's\nbehavior, work, abilities, likenesses, or humanness -- systems we dub AI\nautomatons. Individuals, groups, or generic humans are being simulated to\nproduce creative work in their styles, to respond to surveys in their places,\nto probe how they would use a new system before deployment, to provide users\nwith assistance and companionship, and to anticipate their possible future\nbehavior and interactions with others, just to name a few applications. The\nresearch, design, deployment, and availability of such AI systems have,\nhowever, also prompted growing concerns about a wide range of possible legal,\nethical, and other social impacts. To both 1) facilitate productive discussions\nabout whether, when, and how to design and deploy such systems, and 2) chart\nthe current landscape of existing and prospective AI automatons, we need to\ntease apart determinant design axes and considerations that can aid our\nunderstanding of whether and how various design choices along these axes could\nmitigate -- or instead exacerbate -- potential adverse impacts that the\ndevelopment and use of AI automatons could give rise to. In this paper, through\na synthesis of related literature and extensive examples of existing AI systems\nintended to mimic humans, we develop a conceptual framework to help foreground\nkey axes of design variations and provide analytical scaffolding to foster\ngreater recognition of the design choices available to developers, as well as\nthe possible ethical implications these choices might have.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.15684,regular,post_llm,2025,3,"{'ai_likelihood': 6.894270579020183e-05, 'text': ""Student's Use of Generative AI as a Support Tool in an Advanced Web\n  Development Course\n\n  Various studies have studied the impact of Generative AI on Computing\nEducation. However, they have focused on the implications for novice\nprogrammers. In this experience report, we analyze the use of GenAI as a\nsupport tool for learning, creativity, and productivity in a web development\ncourse for undergraduate students with extensive programming experience. We\ncollected diverse data (assignments, reflections, logs, and a survey) and found\nthat students used GenAI on different tasks (code generation, idea generation,\netc.) with a reported increase in learning and productivity. However, they are\nconcerned about over-reliance and incorrect solutions and want more training in\nprompting strategies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.0595,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': 'From Community Network to Community Data: Towards Combining Data Pool\n  and Data Cooperative for Data Justice in Rural Areas\n\n  This study explores the shift from community networks (CNs) to community data\nin rural areas, focusing on combining data pools and data cooperatives to\nachieve data justice and foster and a just AI ecosystem. With 2.7 billion\npeople still offline, especially in the Global South, addressing data justice\nis critical. While discussions related to data justice have evolved to include\neconomic dimensions, rural areas still struggle with the challenge of being\nadequately represented in the datasets. This study investigates a Community\nData Model (CDM) that integrates the simplicity of data pools with the\nstructured organization of data cooperatives to generate local data for AI for\ngood. CDM leverages CNs, which have proven effective in promoting digital\ninclusion, to establish a centralized data repository, ensuring accessibility\nthrough open data principles. The model emphasizes community needs,\nprioritizing local knowledge, education, and traditional practices, with an\niterative approach starting from pilot projects. Capacity building is a core\ncomponent of digital literacy training and partnership with educational\ninstitutions and NGOs. The legal and regulatory dimension ensures compliance\nwith data privacy laws. By empowering rural communities to control and manage\ntheir data, the CDM fosters equitable access and participation and sustains\nlocal identity and knowledge. This approach can mitigate the challenges of data\ncreation in rural areas and enhance data justice. CDM can contribute to AI by\nimproving data quality and relevance, enabling rural areas to benefit from AI\nadvancements.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.004467010498046875, 'GPT4': 0.0020122528076171875, 'CLAUDE': 2.205371856689453e-05, 'GOOGLE': 0.99267578125, 'OPENAI_O_SERIES': 0.0002732276916503906, 'DEEPSEEK': 1.2993812561035156e-05, 'GROK': 1.430511474609375e-06, 'NOVA': 2.7418136596679688e-06, 'OTHER': 0.00044345855712890625, 'HUMAN': 1.2755393981933594e-05}}"
2503.08931,regular,post_llm,2025,3,"{'ai_likelihood': 1.0, 'text': ""ARCHED: A Human-Centered Framework for Transparent, Responsible, and\n  Collaborative AI-Assisted Instructional Design\n\n  Integrating Large Language Models (LLMs) in educational technology presents\nunprecedented opportunities to improve instructional design (ID), yet existing\napproaches often prioritize automation over pedagogical rigor and human agency.\nThis paper introduces ARCHED (AI for Responsible, Collaborative, Human-centered\nEducation Instructional Design), a structured multi-stage framework that\nensures human educators remain central in the design process while leveraging\nAI capabilities. Unlike traditional AI-generated instructional materials that\nlack transparency, ARCHED employs a cascaded workflow aligned with Bloom's\ntaxonomy. The framework integrates specialized AI agents - one generating\ndiverse pedagogical options and another evaluating alignment with learning\nobjectives - while maintaining educators as primary decision-makers. This\napproach addresses key limitations in current AI-assisted instructional design,\nensuring transparency, pedagogical foundation, and meaningful human agency.\nEmpirical evaluations demonstrate that ARCHED enhances instructional design\nquality while preserving educator oversight, marking a step forward in\nresponsible AI integration in education.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.039836883544922e-06, 'GPT4': 0.00550079345703125, 'CLAUDE': 0.8251953125, 'GOOGLE': 0.00014650821685791016, 'OPENAI_O_SERIES': 8.106231689453125e-06, 'DEEPSEEK': 0.1689453125, 'GROK': 1.7881393432617188e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 3.0994415283203125e-06, 'HUMAN': 1.1920928955078125e-07}}"
2503.16993,regular,post_llm,2025,3,"{'ai_likelihood': 0.0003226598103841146, 'text': 'HEAPO -- An Open Dataset for Heat Pump Optimization with Smart\n  Electricity Meter Data and On-Site Inspection Protocols\n\n  Heat pumps are essential for decarbonizing residential heating but consume\nsubstantial electrical energy, impacting operational costs and grid demand.\nMany systems run inefficiently due to planning flaws, operational faults, or\nmisconfigurations. While optimizing performance requires skilled professionals,\nlabor shortages hinder large-scale interventions. However, digital tools and\nimproved data availability create new service opportunities for energy\nefficiency, predictive maintenance, and demand-side management. To support\nresearch and practical solutions, we present an open-source dataset of\nelectricity consumption from 1,408 households with heat pumps and smart\nelectricity meters in the canton of Zurich, Switzerland, recorded at 15-minute\nand daily resolutions between 2018-11-03 and 2024-03-21. The dataset includes\nhousehold metadata, weather data from 8 stations, and ground truth data from\n410 field visit protocols collected by energy consultants during system\noptimizations. Additionally, the dataset includes a Python-based data loader to\nfacilitate seamless data processing and exploration.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2503.0156,review,post_llm,2025,3,"{'ai_likelihood': 1.1920928955078125e-06, 'text': ""Slopaganda: The interaction between propaganda and generative AI\n\n  At least since Francis Bacon, the slogan 'knowledge is power' has been used\nto capture the relationship between decision-making at a group level and\ninformation. We know that being able to shape the informational environment for\na group is a way to shape their decisions; it is essentially a way to make\ndecisions for them. This paper focuses on strategies that are intentionally, by\ndesign, impactful on the decision-making capacities of groups, effectively\nshaping their ability to take advantage of information in their environment.\nAmong these, the best known are political rhetoric, propaganda, and\nmisinformation. The phenomenon this paper brings out from these is a relatively\nnew strategy, which we call slopaganda. According to The Guardian, News Corp\nAustralia is currently churning out 3000 'local' generative AI (GAI) stories\neach week. In the coming years, such 'generative AI slop' will present multiple\nknowledge-related (epistemic) challenges. We draw on contemporary research in\ncognitive science and artificial intelligence to diagnose the problem of\nslopaganda, describe some recent troubling cases, then suggest several\ninterventions that may help to counter slopaganda.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.1569,review,post_llm,2025,4,"{'ai_likelihood': 0.9951171875, 'text': 'A Data Literacy Competence Model for Higher Education and Research\n\n  In an increasingly data-driven world, the ability to understand, interpret,\nand use data - data literacy - is emerging as a critical competence across all\nacademic disciplines. The Data Literacy Initiative (DaLI) at TH K\\""oln\naddresses this need by developing a comprehensive competence model for\npromoting data literacy in higher education. Based on interdisciplinary\ncollaboration and empirical research, the DaLI model defines seven overarching\ncompetence areas: ""Establish Data Culture"", ""Provide Data"", ""Manage Data"",\n""Analyze Data"", ""Evaluate Data"", ""Interpret Data"", and ""Publish Data"". Each\narea is further detailed by specific competence dimensions and progression\nlevels, providing a structured framework for curriculum design, teaching, and\nassessment. Intended for use across disciplines, the model supports the\nstrategic integration of data literacy into university programs. By providing a\ncommon language and orientation for educators and institutions, the DaLI model\ncontributes to the broader goal of preparing students to navigate and shape a\ndata-informed society.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0008363723754882812, 'GPT4': 0.0762939453125, 'CLAUDE': 0.75, 'GOOGLE': 0.1376953125, 'OPENAI_O_SERIES': 0.0005044937133789062, 'DEEPSEEK': 0.0197906494140625, 'GROK': 4.583597183227539e-05, 'NOVA': 1.9609928131103516e-05, 'OTHER': 0.0014333724975585938, 'HUMAN': 0.01340484619140625}}"
2504.14531,regular,post_llm,2025,4,"{'ai_likelihood': 0.970703125, 'text': 'Closing the Evaluation Gap: Developing a Behavior-Oriented Framework for\n  Assessing Virtual Teamwork Competency\n\n  The growing reliance on remote work and digital collaboration has made\nvirtual teamwork competencies essential for professional and academic success.\nHowever, the evaluation of such competencies remains a significant challenge.\nExisting assessment methods, predominantly based on self-reports and peer\nevaluations, often focus on short-term results or subjective perceptions rather\nthan systematically examining observable teamwork behaviors. These limitations\nhinder the identification of specific areas for improvement and fail to support\nmeaningful progress in skill development. Informed by group dynamic theory,\nthis study developed a behavior-oriented framework for assessing virtual\nteamwork competencies among engineering students. Using focus group interviews\ncombined with the Critical Incident Technique, the study identified three key\ndimensions - Group Task Dimension, Individual Task Dimension and Social\nDimension - along with their behavioral indicators and student-perceived\nrelationships between these components. The resulting framework provides a\nfoundation for more effective assessment practices and supports the development\nof virtual teamwork competency essential for success in increasingly digital\nand globalized professional environments.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0011262893676757812, 'GPT4': 0.1905517578125, 'CLAUDE': 0.62841796875, 'GOOGLE': 0.15966796875, 'OPENAI_O_SERIES': 0.003528594970703125, 'DEEPSEEK': 9.191036224365234e-05, 'GROK': 5.960464477539062e-07, 'NOVA': 2.6226043701171875e-06, 'OTHER': 0.0003085136413574219, 'HUMAN': 0.0162353515625}}"
2504.07811,regular,post_llm,2025,4,"{'ai_likelihood': 7.2187847561306425e-06, 'text': 'The ISC Creator: Human-Centered Design of Learning Analytics Interactive\n  Indicator Specification Cards\n\n  Emerging research on human-centered learning analytics (HCLA) has\ndemonstrated the importance of involving diverse stakeholders in co-designing\nlearning analytics (LA) systems. However, there is still a demand for effective\nand efficient methods to co-design LA dashboards and indicators. Indicator\nSpecification Cards (ISCs) have been introduced recently to facilitate the\nsystematic co-design of indicators by different LA stakeholders. In this paper,\nwe strive to enhance the user experience and usefulness of the ISC-based\nindicator design process. Towards this end, we present the systematic design,\nimplementation, and evaluation details of the ISC Creator, an interactive LA\ntool that allows low-cost and flexible design of LA indicators. Our findings\ndemonstrate the importance of carefully considered interactivity and\nrecommendations for orienting and supporting non-expert LA stakeholders to\ndesign custom LA indicators.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.18169,regular,post_llm,2025,4,"{'ai_likelihood': 0.08266872829861112, 'text': 'Reimagining Assistive Walkers: An Exploration of Challenges and\n  Preferences in Older Adults\n\n  The well-being of older adults relies significantly on maintaining balance\nand mobility. As physical ability declines, older adults often accept the need\nfor assistive devices. However, existing walkers frequently fail to consider\nuser preferences, leading to perceptions of imposition and reduced acceptance.\nThis research explores the challenges faced by older adults, caregivers, and\nhealthcare professionals when using walkers, assesses their perceptions, and\nidentifies their needs and preferences. A holistic approach was employed, using\ntailored perception questionnaires for older adults (24 participants),\ncaregivers (30 participants), and healthcare professionals (27 participants),\nall of whom completed the survey. Over 50% of caregivers and healthcare\nprofessionals displayed good knowledge, positive attitudes, and effective\npractices regarding walkers. However, over 30% of participants perceived\ncurrent designs as fall risks, citing the need for significant upper body\nstrength, potentially affecting safety and movement. More than 50% highlighted\nthe importance of incorporating fall detection, ergonomic designs, noise\nreduction, and walker ramps to better meet user needs and preferences.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.15469,review,post_llm,2025,4,"{'ai_likelihood': 1.5629662407769098e-05, 'text': ""Aspirational Affordances of AI\n\n  As artificial intelligence systems increasingly permeate processes of\ncultural and epistemic production, there are growing concerns about how their\noutputs may confine individuals and groups to static or restricted narratives\nabout who or what they could be. In this paper, we advance the discourse\nsurrounding these concerns by making three contributions. First, we introduce\nthe concept of aspirational affordance to describe how culturally shared\ninterpretive resources can shape individual cognition, and in particular\nexercises practical imagination. We show how this concept can ground productive\nevaluations of the risks of AI-enabled representations and narratives. Second,\nwe provide three reasons for scrutinizing of AI's influence on aspirational\naffordances: AI's influence is potentially more potent, but less public than\ntraditional sources; AI's influence is not simply incremental, but ecological,\ntransforming the entire landscape of cultural and epistemic practices that\ntraditionally shaped aspirational affordances; and AI's influence is highly\nconcentrated, with a few corporate-controlled systems mediating a growing\nportion of aspirational possibilities. Third, to advance such a scrutiny, we\nintroduce the concept of aspirational harm, which, in the context of AI\nsystems, arises when AI-enabled aspirational affordances distort or diminish\navailable interpretive resources in ways that undermine individuals' ability to\nimagine relevant practical possibilities and alternative futures. Through three\ncase studies, we illustrate how aspirational harms extend the existing\ndiscourse on AI-inflicted harms beyond representational and allocative harms,\nwarranting separate attention. Through these conceptual resources and analyses,\nthis paper advances understanding of the psychological and societal stakes of\nAI's role in shaping individual and collective aspirations.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.06296,review,post_llm,2025,4,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Assessing Computer Science Student Attitudes Towards AI Ethics and Policy\n\nAs artificial intelligence (AI) grows in popularity and importance-both as a domain within broader computing research and in society at large-increasing focus will need to be paid to the ethical governance of this emerging technology. The attitudes and competencies with respect to AI ethics and policy among post-secondary students studying computer science (CS) are of particular interest, as many of these students will go on to play key roles in the development and deployment of future AI innovations. Despite this population of computer scientists being at the forefront of learning about and using AI tools, their attitudes towards AI remain understudied in the literature. In an effort to begin to close this gap, in fall 2024 we fielded a survey ($n=117$) to undergraduate and graduate students enrolled in CS courses at a large public university in the United States to assess their attitudes towards the nascent fields of AI ethics and policy. Additionally, we conducted one-on-one follow-up interviews with 13 students to elicit more in-depth responses on topics such as the use of AI tools in the classroom, ethical impacts of AI, and government regulation of AI. In this paper, we describe the findings of both the survey and interviews, drawing parallels and contrasts to broader public opinion polling in the United States. We conclude by evaluating the implications of CS student attitudes on the future of AI education and governance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.21818,review,post_llm,2025,4,"{'ai_likelihood': 0.0002765655517578125, 'text': ""A systematic review of research on large language models for computer programming education\n\nGiven the increasing demands in computer programming education and the rapid advancement of large language models (LLMs), LLMs play a critical role in programming education. This study provides a systematic review of selected empirical studies on LLMs in computer programming education, published from 2023 to March 2024. The data for this review were collected from Web of Science (SCI/SSCI), SCOPUS, and EBSCOhost databases, as well as three conference proceedings specialized in computer programming education. In total, 42 studies met the selection criteria and were reviewed using methods, including bibliometric analysis, thematic analysis, and structural topic modeling. This study offers an overview of the current state of LLMs in computer programming education research. It outlines LLMs' applications, benefits, limitations, concerns, and implications for future research and practices, establishing connections between LLMs and their practical use in computer programming education. This review also provides examples and valuable insights for instructional designers, instructors, and learners. Additionally, a conceptual framework is proposed to guide education practitioners in integrating LLMs into computer programming education. This study suggests future research directions from various perspectives, emphasizing the need to expand research methods and topics in computer programming education as LLMs evolve. Additionally, future research in the field should incorporate collaborative, interdisciplinary, and transdisciplinary efforts on a large scale, focusing on longitudinal research and development initiatives."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.1705,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Mapping Trafficking Networks: A Data-Driven Approach to Disrupt Human\n  Trafficking Post Russia-Ukraine Conflict\n\n  This study proposes a prototype for locating important individuals and\nfinancial exchanges in networks of people trafficking that have grown during\nthe conflict between Russia and Ukraine. It focuses on the role of digital\nplatforms, cryptocurrencies, and the dark web in facilitating these operations.\nThe research maps trafficking networks and identifies key players and financial\nflows by utilizing open-source intelligence (OSINT), social network analysis\n(SNA), and blockchain analysis. The results show how cryptocurrencies are used\nfor anonymous transactions and imply that upsetting central coordinators may\ncause wider networks to become unstable. In order to combat human trafficking,\nthe study emphasizes the significance of real-time data sharing between\ninternational law enforcement. It also identifies future directions for the\ndevelopment of improved monitoring tools and cooperative platforms.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.012908935546875, 'GPT4': 0.8076171875, 'CLAUDE': 0.042724609375, 'GOOGLE': 0.10504150390625, 'OPENAI_O_SERIES': 0.01282501220703125, 'DEEPSEEK': 0.00894927978515625, 'GROK': 2.950429916381836e-05, 'NOVA': 0.0001779794692993164, 'OTHER': 0.00891876220703125, 'HUMAN': 0.0009183883666992188}}"
2504.1217,review,post_llm,2025,4,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'AI Behind Closed Doors: a Primer on The Governance of Internal\n  Deployment\n\n  The most advanced future AI systems will first be deployed inside the\nfrontier AI companies developing them. According to these companies and\nindependent experts, AI systems may reach or even surpass human intelligence\nand capabilities by 2030. Internal deployment is, therefore, a key source of\nbenefits and risks from frontier AI systems. Despite this, the governance of\nthe internal deployment of highly advanced frontier AI systems appears absent.\nThis report aims to address this absence by priming a conversation around the\ngovernance of internal deployment. It presents a conceptualization of internal\ndeployment, learnings from other sectors, reviews of existing legal frameworks\nand their applicability, and illustrative examples of the type of scenarios we\nare most concerned about. Specifically, it discusses the risks correlated to\nthe loss of control via the internal application of a misaligned AI system to\nthe AI research and development pipeline, and unconstrained and undetected\npower concentration behind closed doors. The report culminates with a small\nnumber of targeted recommendations that provide a first blueprint for the\ngovernance of internal deployment.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.07312,regular,post_llm,2025,4,"{'ai_likelihood': 1.4238887363009982e-05, 'text': ""Mindsets and Management: AI and Gender (In)Equitable Access to Finance\n\nA growing trend in financial technology (fintech) is the use of mobile phone data and machine learning (ML) to provide credit scores- and subsequently, opportunities to access loans- to groups left out of traditional banking. This paper draws on interview data with leaders, investors, and data scientists at fintech companies developing ML-based alternative lending apps in low- and middle-income countries to explore financial inclusion and gender implications. More specifically, it examines how the underlying logics, design choices, and management decisions of ML-based alternative lending tools by fintechs embed or challenge gender biases, and consequently influence gender equity in access to finance. Findings reveal developers follow 'gender blind' approaches, grounded in beliefs that ML is objective and data reflects the truth. This leads to a lack of grappling with the ways data, features for creditworthiness, and access to apps are gendered. Overall, tools increase access to finance, but not gender equitably: Interviewees report less women access loans and receive lower amounts than men, despite being better repayers. Fintechs identify demand- and supply-side reasons for gender differences, but frame them as outside their responsibility. However, that women are observed as better repayers reveals a market inefficiency and potential discriminatory effect, further linked to profit optimization objectives. This research introduces the concept of encoded gender norms, whereby without explicit attention to the gendered nature of data and algorithmic design, AI tools reproduce existing inequalities. In doing so, they reinforce gender norms as self-fulfilling prophecies. The idea that AI is inherently objective and, when left alone, 'fair', is seductive and misleading. In reality, algorithms reflect the perspectives, priorities, and values of the people and institutions that design them."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.11196,regular,post_llm,2025,4,"{'ai_likelihood': 0.0003173616197374132, 'text': ""The Lifetime of the Covid Memorial Wall: Modelling with Collections\n  Demography, Social Media Data and Citizen Science\n\n  The National Covid Memorial Wall in London, featuring over 240,000\nhand-painted red hearts, faces significant conservation challenges due to the\nrapid fading of the paint. This study evaluates the transition to a\nbetter-quality paint and its implications for the wall's long-term\npreservation. The rapid fading of the initial materials required an\nunsustainable repainting rate, burdening volunteers. Lifetime simulations based\non a collections demography framework suggest that repainting efforts must\ncontinue at a rate of some hundreds of hearts per week to maintain a stable\npercentage of hearts in good condition. This finding highlights the need for a\nsustainable management strategy that includes regular maintenance or further\nreduction of the fading rate.\n  Methodologically, this study demonstrates the feasibility of using a\ncollections demography approach, supported by citizen science and social media\ndata, to inform heritage management decisions. An agent-based simulation is\nused to propagate the multiple uncertainties measured. The methodology provides\na robust basis for modeling and decision-making, even in a case like this,\nwhere reliance on publicly available images and volunteer-collected data\nintroduces variability. Future studies could improve data within a citizen\nscience framework by inviting public submissions, using on-site calibration\ncharts, and increasing volunteer involvement for longitudinal data collection.\nThis research illustrates the flexibility of the collections demography\nframework, firstly by showing its applicability to an outdoor monument, which\nis very different from the published case studies, and secondly by\ndemonstrating how it can work even with low-quality data.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.02239,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'The Author Is Sovereign: A Manifesto for Ethical Copyright in the Age of AI\n\nIn the age of AI, authorship is being quietly eroded by algorithmic content scraping, legal gray zones like ""fair use,"" and platforms that profit from creative labor without consent or compensation. This short manifesto proposes a radical alternative: a system in which the author is sovereign of their intellectual domain. It presents seven ethical principles that challenge prevailing assumptions about open access, copyright ownership, and the public domain - arguing that voluntary, negotiated consent must replace coercive norms. The text exposes how weakened authorship fuels structural exploitation. In place of reactive solutions, it calls for a new ethic of authorship rooted in consent, dignity, and contractual fairness.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.5762786865234375e-06, 'GPT4': 0.00010710954666137695, 'CLAUDE': 0.9951171875, 'GOOGLE': 3.2782554626464844e-06, 'OPENAI_O_SERIES': 1.0728836059570312e-06, 'DEEPSEEK': 0.0047454833984375, 'GROK': 1.1920928955078125e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 4.172325134277344e-07, 'HUMAN': 1.7881393432617188e-07}}"
2504.00533,regular,post_llm,2025,4,"{'ai_likelihood': 0.9951171875, 'text': ""Curriculum Design of Competitive Programming: a Contest-based Approach\n\n  Competitive programming (CP) has been increasingly integrated into computer\nscience curricula worldwide due to its efficacy in enhancing students'\nalgorithmic reasoning and problem-solving skills. However, existing CP\ncurriculum designs predominantly employ a problem-based approach, lacking the\ncritical dimension of time pressure of real competitive programming contests.\nSuch constraints are prevalent not only in programming contests but also in\nvarious real-world scenarios, including technical interviews, software\ndevelopment sprints, and hackathons.\n  To bridge this gap, we introduce a contest-based approach to curriculum\ndesign that explicitly incorporates realistic contest scenarios into formative\nassessments, simulating authentic competitive programming experiences. This\npaper details the design and implementation of such a course at Purdue\nUniversity, structured to systematically develop students' observational\nskills, algorithmic techniques, and efficient coding and debugging practices.\nWe outline a pedagogical framework comprising cooperative learning strategies,\ncontest-based assessments, and supplemental activities to boost students'\nproblem-solving capabilities.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.8848648071289062e-05, 'GPT4': 0.740234375, 'CLAUDE': 0.0010395050048828125, 'GOOGLE': 0.21875, 'OPENAI_O_SERIES': 0.03302001953125, 'DEEPSEEK': 0.004947662353515625, 'GROK': 1.4185905456542969e-05, 'NOVA': 2.384185791015625e-06, 'OTHER': 6.0617923736572266e-05, 'HUMAN': 0.0020580291748046875}}"
2504.16104,regular,post_llm,2025,4,"{'ai_likelihood': 2.715322706434462e-06, 'text': 'Beauty and the Bias: Exploring the Impact of Attractiveness on Multimodal Large Language Models\n\nPhysical attractiveness matters. It has been shown to influence human perception and decision-making, often leading to biased judgments that favor those deemed attractive in what is referred to as the ""attractiveness halo effect"". While extensively studied in human judgments in a broad set of domains, including hiring, judicial sentencing or credit granting, the role that attractiveness plays in the assessments and decisions made by multimodal large language models (MLLMs) is unknown. To address this gap, we conduct an empirical study with 7 diverse open-source MLLMs evaluated on 91 socially relevant scenarios and a diverse dataset of 924 face images - corresponding to 462 individuals both with and without beauty filters applied to them. Our analysis reveals that attractiveness impacts the decisions made by MLLMs in 86.2% of the scenarios on average, demonstrating substantial bias in model behavior in what we refer to as an attractiveness bias. Similarly to humans, we find empirical evidence of the existence of the attractiveness halo effect in 94.8% of the relevant scenarios: attractive individuals are more likely to be attributed positive traits, such as intelligence or confidence, by MLLMs than unattractive individuals. Furthermore, we uncover gender, age and race biases in a significant portion of the scenarios which are also impacted by attractiveness, particularly in the case of gender, highlighting the intersectional nature of the algorithmic attractiveness bias. Our findings suggest that societal stereotypes and cultural norms intersect with perceptions of attractiveness in MLLMs in a complex manner. Our work emphasizes the need to account for intersectionality in algorithmic bias detection and mitigation efforts and underscores the challenges of addressing biases in modern MLLMs.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.08834,review,post_llm,2025,4,"{'ai_likelihood': 0.99755859375, 'text': 'A Systematic Literature Review of Unmanned Aerial Vehicles for\n  Healthcare and Emergency Services\n\n  Unmanned aerial vehicles (UAVs), initially developed for military\napplications, are now used in various fields. As UAVs become more common across\nmultiple industries, it is crucial to understand how to adopt them effectively,\nefficiently, and safely. The utilization of UAVs in healthcare and emergency\nservices has evolved significantly in recent years, with these aerial vehicles\npotentially contributing to increased survival rates and enhanced healthcare\nservices.\n  This paper presents a two-stage systematic literature review, including a\ntertiary study of 15 review papers and an in-depth assessment of 136 primary\npublications focused on using UAVs in healthcare and emergency services. The\nresearch demonstrates how civilian UAVs have been used in numerous\napplications, such as healthcare emergencies, medical supply delivery, and\ndisaster management, for diverse use cases such as Automated External\nDefibrillator (AED) delivery, blood delivery, and search and rescue.\n  The studies indicate that UAVs significantly improve response times in\nemergency situations, enhance survival rates by ensuring the timely delivery of\ncritical medical supplies such as AEDs, and prove to be cost-effective\nalternatives to traditional delivery methods, especially in remote or\ninaccessible areas. The studies also highlight the need for ongoing research\nand development to address existing challenges, such as regulatory frameworks,\nsecurity, privacy and safety concerns, infrastructure development, and ethical\nand social issues. Effectively understanding and tackling these challenges is\nessential for maximizing the benefits of UAV technology in healthcare and\nemergency services, ultimately leading to safer, more resilient, and responsive\nsystems that can better serve public health needs.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.002513885498046875, 'GPT4': 0.939453125, 'CLAUDE': 1.9431114196777344e-05, 'GOOGLE': 0.057647705078125, 'OPENAI_O_SERIES': 0.00027823448181152344, 'DEEPSEEK': 1.3113021850585938e-06, 'GROK': 2.384185791015625e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 3.361701965332031e-05, 'HUMAN': 5.829334259033203e-05}}"
2504.18222,regular,post_llm,2025,4,"{'ai_likelihood': 1.1093086666531033e-05, 'text': 'Automated Work Records for Precision Agriculture Management: A Low-Cost\n  GNSS IoT Solution for Paddy Fields in Central Japan\n\n  Agricultural field operations are generally tracked as work records (WR),\nincorporating data points such as; work type, machine type, timestamped\ntrajectories and field information. WR data which is automatically recorded by\nmodern machinery equipped with Information and Communication Technologies (ICT)\ncan enable efficient farm management decision making. Globally, farmers often\nrely on aged or legacy farming machinery and manual data recording, which\nintroduces significant labor costs and increases the risk of inaccurate data\ninput. To address this challenge, a field study in Central Japan was conducted\nto showcase automated data collection by retrofitting legacy farming machinery\nwith low-cost Internet of Things (IoT) devices. For single-purpose vehicles\n(SPV), which only carry out single work types such as planting, LTE (Long Term\nEvolution) and Global Navigation Satellite System (GNSS) units were installed\nto record trajectory data. For multi-purpose vehicles (MPV), such as tractors\nwhich perform multiple work types, the configuration settings of these vehicles\nhad to include implements and attachments data. To obtain this data, industry\nstandard LTE-GNSS Bluetooth gateways were fitted onto MPV and low-cost BLE\n(Bluetooth Low Energy) beacons were attached to implements. After installation,\nover a seven-month field preparation and planting period 1,623 WR, including\n421 WR for SPV and 1,120 WR for MVP, were automatically obtained. For MPV, the\nWR included detailed configuration settings enabling detection of the specific\nwork types. These findings demonstrate the potential of low cost IoT GNSS\ndevices for precision agriculture strategies to support management decisions in\nfarming operations.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.07676,regular,post_llm,2025,4,"{'ai_likelihood': 2.715322706434462e-06, 'text': ""Clicks, comments, consequences: Are content creators' socio-structural\n  and platform characteristics shaping the exposure to negative sentiment,\n  offensive language, and hate speech on YouTube?\n\n  Receiving negative sentiment, offensive comments, or even hate speech is a\nconstant part of the working experience of content creators (CCs) on YouTube -\na growing occupational group in the platform economy. This study investigates\nhow socio-structural characteristics such as the age, gender, and race of CCs\nbut also platform features including the number of subscribers, community\nstrength, and the channel topic shape differences in the occurrence of these\nphenomena on that platform. Drawing on a random sample of n=3,695 YouTube\nchannels from German-speaking countries, we conduct a comprehensive analysis\ncombining digital trace data, enhanced with hand-coded variables to include\nsocio-structural characteristics in social media data. Publicly visible\nnegative sentiment, offensive language, and hate speech are detected with\nmachine- and deep-learning methods using N=40,000,000 comments. Contrary to\nexisting studies our findings indicate that female content creators are\nconfronted with less negative communication. Notably, our analysis reveals that\nwhile BIPoC, who work as CCs, receive significantly more negative sentiment,\nthey aren't exposed to more offensive comments or hate speech. Additionally,\nplatform characteristics also play a crucial role, as channels publishing\ncontent on conspiracy theories or politics are more frequently subject to\nnegative communication.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.0418,review,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'When Will AI Transform Society? Swedish Public Predictions on AI\n  Development Timelines\n\n  This study investigates public expectations regarding the likelihood and\ntiming of major artificial intelligence (AI) developments among Swedes. Through\na mixed-mode survey (web/paper) of 1,026 respondents, we examined expectations\nacross six key scenarios: medical breakthroughs, mass unemployment, democratic\ndeterioration, living standard improvements, artificial general intelligence\n(AGI), and uncontrollable superintelligent AI. Findings reveal strong consensus\non AI-driven medical breakthroughs (82.6%), while expectations for other major\ndevelopments are significantly lower, ranging from 40.9% for mass unemployment\ndown to 28.4% for AGI. Timeline expectations varied significantly, with major\nmedical advances anticipated within 6-10 years, while more transformative\ndevelopments like AGI were projected beyond 20 years. Latent class analysis\nidentified three distinct groups: optimists (46.7%), ambivalents (42.2%), and\nskeptics (11.2%). The optimist group showed higher levels of self-rated AI\nknowledge and education, while gender differences were also observed across\nclasses. The study addresses a critical gap in understanding temporal\nexpectations of AI development among the general public, offering insights for\npolicymakers and stakeholders.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.5367431640625e-07, 'GPT4': 0.0005054473876953125, 'CLAUDE': 0.9951171875, 'GOOGLE': 0.0025386810302734375, 'OPENAI_O_SERIES': 2.6404857635498047e-05, 'DEEPSEEK': 0.001964569091796875, 'GROK': 5.960464477539063e-08, 'NOVA': 2.980232238769531e-07, 'OTHER': 6.67572021484375e-06, 'HUMAN': 2.205371856689453e-06}}"
2505.00054,review,post_llm,2025,4,"{'ai_likelihood': 7.9141722785102e-06, 'text': ""Algorithmic Addiction by Design: Big Tech's Leverage of Dark Patterns to\n  Maintain Market Dominance and its Challenge for Content Moderation\n\n  Today's largest technology corporations, especially ones with consumer-facing\nproducts such as social media platforms, use a variety of unethical and often\noutright illegal tactics to maintain their dominance. One tactic that has risen\nto the level of the public consciousness is the concept of addictive design,\nevidenced by the fact that excessive social media use has become a salient\nproblem, particularly in the mental and social development of adolescents and\nyoung adults. As tech companies have developed more and more sophisticated\nartificial intelligence (AI) models to power their algorithmic recommender\nsystems, they will become more successful at their goal of ensuring addiction\nto their platforms. This paper explores how online platforms intentionally\ncultivate addictive user behaviors and the broad societal implications,\nincluding on the health and well-being of children and adolescents. It presents\nthe usage of addictive design - including the usage of dark patterns,\npersuasive design elements, and recommender algorithms - as a tool leveraged by\ntechnology corporations to maintain their dominance. Lastly, it describes the\nchallenge of content moderation to address the problem and gives an overview of\nsolutions at the policy level to counteract addictive design.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.05273,review,post_llm,2025,4,"{'ai_likelihood': 2.2186173333062066e-06, 'text': 'What We Do Not Know: GPT Use in Business and Management\n\n  This systematic review examines peer-reviewed studies on application of GPT\nin business management, revealing significant knowledge gaps. Despite\nidentifying interesting research directions such as best practices,\nbenchmarking, performance comparisons, social impacts, our analysis yields only\n42 relevant studies for the 22 months since its release. There are so few\nstudies looking at a particular sector or subfield that management researchers,\nbusiness consultants, policymakers, and journalists do not yet have enough\ninformation to make well-founded statements on how GPT is being used in\nbusinesses. The primary contribution of this paper is a call to action for\nfurther research. We provide a description of current research and identify\nknowledge gaps on the use of GPT in business. We cover the management subfields\nof finance, marketing, human resources, strategy, operations, production, and\nanalytics, excluding retail and sales. We discuss gaps in knowledge of GPT\npotential consequences on employment, productivity, environmental costs,\noppression, and small businesses. We propose how management consultants and the\nmedia can help fill those gaps. We call for practical work on business control\nsystems as they relate to existing and foreseeable AI-related business\nchallenges. This work may be of interest to managers, to management\nresearchers, and to people working on AI in society.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.0716,regular,post_llm,2025,4,"{'ai_likelihood': 0.5141601562499999, 'text': 'AI-based identification and support of at-risk students: A case study of\n  the Moroccan education system\n\n  Student dropout is a global issue influenced by personal, familial, and\nacademic factors, with varying rates across countries. This paper introduces an\nAI-driven predictive modeling approach to identify students at risk of dropping\nout using advanced machine learning techniques. The goal is to enable timely\ninterventions and improve educational outcomes. Our methodology is adaptable\nacross different educational systems and levels. By employing a rigorous\nevaluation framework, we assess model performance and use Shapley Additive\nexPlanations (SHAP) to identify key factors influencing predictions. The\napproach was tested on real data provided by the Moroccan Ministry of National\nEducation, achieving 88% accuracy, 88% recall, 86% precision, and an AUC of\n87%. These results highlight the effectiveness of the AI models in identifying\nat-risk students. The framework is adaptable, incorporating historical data for\nboth short and long-term detection, offering a comprehensive solution to the\npersistent challenge of student dropout.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0147857666015625, 'GPT4': 0.343994140625, 'CLAUDE': 0.0362548828125, 'GOOGLE': 0.50048828125, 'OPENAI_O_SERIES': 0.0259552001953125, 'DEEPSEEK': 0.004772186279296875, 'GROK': 0.000335693359375, 'NOVA': 0.0010547637939453125, 'OTHER': 0.0207672119140625, 'HUMAN': 0.05181884765625}}"
2504.05449,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Connecting Feedback to Choice: Understanding Educator Preferences in\n  GenAI vs. Human-Created Lesson Plans in K-12 Education -- A Comparative\n  Analysis\n\n  As generative AI (GenAI) models are increasingly explored for educational\napplications, understanding educator preferences for AI-generated lesson plans\nis critical for their effective integration into K-12 instruction. This\nexploratory study compares lesson plans authored by human curriculum designers,\na fine-tuned LLaMA-2-13b model trained on K-12 content, and a customized GPT-4\nmodel to evaluate their pedagogical quality across multiple instructional\nmeasures: warm-up activities, main tasks, cool-down activities, and overall\nquality. Using a large-scale preference study with K-12 math educators, we\nexamine how preferences vary across grade levels and instructional components.\nWe employ both qualitative and quantitative analyses. The raw preference\nresults indicate that human-authored lesson plans are generally favored,\nparticularly for elementary education, where educators emphasize student\nengagement, scaffolding, and collaborative learning. However, AI-generated\nmodels demonstrate increasing competitiveness in cool-down tasks and structured\nlearning activities, particularly in high school settings. Beyond quantitative\nresults, we conduct thematic analysis using LDA and manual coding to identify\nkey factors influencing educator preferences. Educators value human-authored\nplans for their nuanced differentiation, real-world contextualization, and\nstudent discourse facilitation. Meanwhile, AI-generated lesson plans are often\npraised for their structure and adaptability for specific instructional tasks.\nFindings suggest a human-AI collaborative approach to lesson planning, where\nGenAI can serve as an assistive tool rather than a replacement for educator\nexpertise in lesson planning. This study contributes to the growing discourse\non responsible AI integration in education, highlighting both opportunities and\nchallenges in leveraging GenAI for curriculum development.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.424551010131836e-05, 'GPT4': 0.935546875, 'CLAUDE': 0.036407470703125, 'GOOGLE': 0.0242156982421875, 'OPENAI_O_SERIES': 7.873773574829102e-05, 'DEEPSEEK': 0.0034084320068359375, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 4.76837158203125e-07, 'HUMAN': 0.00020825862884521484}}"
2504.0903,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': ""Authoritarian Recursions: How Fiction, History, and AI Reinforce Control in Education, Warfare, and Discourse\n\nThis article introduces the concept of \\textit{authoritarian recursion} to theorize how AI systems consolidate institutional control across education, warfare, and digital discourse. It identifies a shared recursive architecture in which algorithms mediate judgment, obscure accountability, and constrain moral and epistemic agency.\n  Grounded in critical discourse analysis and sociotechnical ethics, the paper examines how AI systems normalize hierarchy through abstraction and feedback. Case studies -- automated proctoring, autonomous weapons, and content recommendation -- are analyzed alongside cultural imaginaries such as Orwell's \\textit{Nineteen Eighty-Four}, Skynet, and \\textit{Black Mirror}, used as heuristic tools to surface ethical blind spots.\n  The analysis integrates Fairness, Accountability, and Transparency (FAccT), relational ethics, and data justice to explore how predictive infrastructures enable moral outsourcing and epistemic closure. By reframing AI as a communicative and institutional infrastructure, the article calls for governance approaches that center democratic refusal, epistemic plurality, and structural accountability."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.7881393432617188e-07, 'GPT4': 0.00019478797912597656, 'CLAUDE': 0.00015413761138916016, 'GOOGLE': 2.384185791015625e-07, 'OPENAI_O_SERIES': 5.960464477539063e-08, 'DEEPSEEK': 0.99951171875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 2.980232238769531e-07}}"
2504.1412,review,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Inclusive Education with AI: Supporting Special Needs and Tackling\n  Language Barriers\n\n  Early childhood classrooms are becoming increasingly diverse, with students\nspanning a range of linguistic backgrounds and abilities. AI offers innovative\ntools to help educators create more inclusive learning environments by breaking\ndown language barriers and providing tailored support for children with special\nneeds. This chapter provides a comprehensive review of how AI technologies can\nfacilitate inclusion in early education. It is discussed AI-driven language\nassistance tools that enable real-time translation and communication in\nmultilingual classrooms, and it is explored assistive technologies powered by\nAI that personalize learning for students with disabilities. The implications\nof these technologies for teachers are examined, including shifts in educator\nroles and workloads. General outcomes observed with AI integration - such as\nimproved student engagement and performance - as well as challenges related to\nequitable access and the need for ethical implementation are highlighted.\nFinally, practical recommendations for educators, policymakers, and developers\nare offered to collaboratively harness AI in a responsible manner, ensuring\nthat its benefits reach all learners.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.8133392333984375e-05, 'GPT4': 0.0022602081298828125, 'CLAUDE': 0.9970703125, 'GOOGLE': 6.365776062011719e-05, 'OPENAI_O_SERIES': 0.000579833984375, 'DEEPSEEK': 0.0001558065414428711, 'GROK': 6.556510925292969e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 6.258487701416016e-06, 'HUMAN': 4.172325134277344e-07}}"
2504.15333,regular,post_llm,2025,4,"{'ai_likelihood': 3.1424893273247617e-05, 'text': ""Measuring Interest Group Positions on Legislation: An AI-Driven Analysis\n  of Lobbying Reports\n\n  Special interest groups (SIGs) in the U.S. participate in a range of\npolitical activities, such as lobbying and making campaign donations, to\ninfluence policy decisions in the legislative and executive branches. The\ncompeting interests of these SIGs have profound implications for global issues\nsuch as international trade policies, immigration, climate change, and global\nhealth challenges. Despite the significance of understanding SIGs' policy\npositions, empirical challenges in observing them have often led researchers to\nrely on indirect measurements or focus on a select few SIGs that publicly\nsupport or oppose a limited range of legislation. This study introduces the\nfirst large-scale effort to directly measure and predict a wide range of bill\npositions-Support, Oppose, Engage (Amend and Monitor)- across all legislative\nbills introduced from the 111th to the 117th Congresses. We leverage an\nadvanced AI framework, including large language models (LLMs) and graph neural\nnetworks (GNNs), to develop a scalable pipeline that automatically extracts\nthese positions from lobbying activities, resulting in a dataset of 42k bills\nannotated with 279k bill positions of 12k SIGs. With this large-scale dataset,\nwe reveal (i) a strong correlation between a bill's progression through\nlegislative process stages and the positions taken by interest groups, (ii) a\nsignificant relationship between firm size and lobbying positions, (iii)\nnotable distinctions in lobbying position distribution based on bill subject,\nand (iv) heterogeneity in the distribution of policy preferences across\nindustries. We introduce a novel framework for examining lobbying strategies\nand offer opportunities to explore how interest groups shape the political\nlandscape.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.17678,regular,post_llm,2025,4,"{'ai_likelihood': 0.00012867980533176, 'text': 'MindFlow: A Network Traffic Anomaly Detection Model Based on MindSpore\n\n  With the wide application of IoT and industrial IoT technologies, the network\nstructure is becoming more and more complex, and the traffic scale is growing\nrapidly, which makes the traditional security protection mechanism face serious\nchallenges in dealing with high-frequency, diversified, and stealthy\ncyber-attacks. To address this problem, this study proposes MindFlow, a\nmulti-dimensional dynamic traffic prediction and anomaly detection system\ncombining convolutional neural network (CNN) and bi-directional long and\nshort-term memory network (BiLSTM) architectures based on the MindSpore\nframework, and conducts systematic experiments on the NF-BoT-IoT dataset. The\nexperimental results show that the proposed model achieves 99% in key metrics\nsuch as accuracy, precision, recall and F1 score, effectively verifying its\naccuracy and robustness in network intrusion detection.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.08972,review,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Improving municipal responsiveness through AI-powered image analysis in\n  E-Government\n\n  Integration of Machine Learning (ML) techniques into public administration\nmarks a new and transformative era for e-government systems. While\ntraditionally e-government studies were focusing on text-based interactions,\nthis one explores the innovative application of ML for image analysis, an\napproach that enables governments to address citizen petitions more\nefficiently. By using image classification and object detection algorithms, the\nmodel proposed in this article supports public institutions in identifying and\nfast responding to evidence submitted by citizens in picture format, such as\ninfrastructure issues, environmental concerns or other urban issues that\ncitizens might face. The research also highlights the Jevons Paradox as a\ncritical factor, wherein increased efficiency from the citizen side (especially\nusing mobile platforms and apps) may generate higher demand which should lead\nto scalable and robust solutions. Using as a case study a Romanian municipality\nwho provided datasets of citizen-submitted images, the author analysed and\nproved that ML can improve accuracy and responsiveness of public institutions.\nThe findings suggest that adopting ML for e-petition systems can not only\nenhance citizen participation but also speeding up administrative processes,\npaving the way for more transparent and effective governance. This study\ncontributes to the discourse on e-government 3.0 by showing the potential of\nArtificial Intelligence (AI) to transform public service delivery, ensuring\nsustainable (and scalable) solutions for the growing demands of modern urban\ngovernance.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.3795833587646484e-05, 'GPT4': 0.99951171875, 'CLAUDE': 1.627206802368164e-05, 'GOOGLE': 0.0001761913299560547, 'OPENAI_O_SERIES': 1.2993812561035156e-05, 'DEEPSEEK': 8.344650268554688e-07, 'GROK': 7.152557373046875e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 2.2649765014648438e-06, 'HUMAN': 6.377696990966797e-06}}"
2504.12914,review,post_llm,2025,4,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'In Which Areas of Technical AI Safety Could Geopolitical Rivals\n  Cooperate?\n\n  International cooperation is common in AI research, including between\ngeopolitical rivals. While many experts advocate for greater international\ncooperation on AI safety to address shared global risks, some view cooperation\non AI with suspicion, arguing that it can pose unacceptable risks to national\nsecurity. However, the extent to which cooperation on AI safety poses such\nrisks, as well as provides benefits, depends on the specific area of\ncooperation. In this paper, we consider technical factors that impact the risks\nof international cooperation on AI safety research, focusing on the degree to\nwhich such cooperation can advance dangerous capabilities, result in the\nsharing of sensitive information, or provide opportunities for harm. We begin\nby why nations historically cooperate on strategic technologies and analyse\ncurrent US-China cooperation in AI as a case study. We further argue that\nexisting frameworks for managing associated risks can be supplemented with\nconsideration of key risks specific to cooperation on technical AI safety\nresearch. Through our analysis, we find that research into AI verification\nmechanisms and shared protocols may be suitable areas for such cooperation.\nThrough this analysis we aim to help researchers and governments identify and\nmitigate the risks of international cooperation on AI safety research, so that\nthe benefits of cooperation can be fully realised.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.17248,review,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': ""How Jungian Cognitive Functions Explain MBTI Type Prevalence in Computer\n  Industry Careers\n\n  This study investigates the relationship between Carl Jung's cognitive\nfunctions and success in computer industry careers by analyzing the\ndistribution of Myers-Briggs Type Indicator (MBTI) types among professionals in\nthe field. Building on Carl Jung's theory of psychological types, which\ncategorizes human cognition into four primary functions, Sensing, Intuition,\nThinking, and Feeling, this study investigates how these functions, when\ncombined with the attitudes of Extraversion and Introversion, influence\npersonality types and career choices in the tech sector. Through a\ncomprehensive analysis of data from 30 studies spanning multiple countries and\ndecades, encompassing 18,264 individuals in computer-related professions, we\nidentified the most prevalent cognitive functions and their combinations. After\nnormalizing the data against general population distributions, our findings\nshowed that individual Jungian functions (Te, Ni, Ti, Ne), dual function\ncombinations (Ni-Te, Ti-Ne, Si-Te, Ni-Fe), and MBTI types (INTJ, ENTJ, INTP,\nENTP, ISTJ, INFJ, ESTJ, ESTP) had significantly higher representation compared\nto general population norms. The paper addresses gaps in the existing\nliterature by providing a more nuanced understanding of how cognitive functions\nimpact job performance and team dynamics, offering insights for career\nguidance, team composition, and professional development in the computer\nindustry, and a deeper understanding of how cognitive preferences influence\ncareer success in technology-related fields.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0006814002990722656, 'GPT4': 0.060333251953125, 'CLAUDE': 0.88232421875, 'GOOGLE': 0.04010009765625, 'OPENAI_O_SERIES': 0.00852203369140625, 'DEEPSEEK': 0.0014371871948242188, 'GROK': 0.00041413307189941406, 'NOVA': 0.00010353326797485352, 'OTHER': 0.005939483642578125, 'HUMAN': 6.657838821411133e-05}}"
2504.03805,review,post_llm,2025,4,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Scope of Online Maternal Health Information in Kinyarwanda and\n  Opportunities for Digital Health Developers\n\n  Maternal health literacy is associated with greater odds of positive\npregnancy outcomes. There is an increasing proliferation of websites dedicated\nto maternal health education, but the scope and quality of their content varies\nwidely. In this study, we analyzed the main topics covered on maternal health\nwebsites that offer content in the low-resource Kinyarwanda language (mainly\nspoken by 12 million Rwandans). We used web scraping to identify maternal\nhealth websites. We utilized a topic modeling, using the Non-Negative Matrix\nFactorization (NMF) algorithm, to identify the topics. We found five main\ntopics: (1) pregnancy danger signs, (2) child care, (3) intimacy (sex), (4)\nnutrition, and (5) the importance of doctor consultations. However, the\narticles were short and did not cater to fathers, pregnant adolescents, or\nthose experiencing gender-based violence (GBV) or mental health challenges.\nThis is despite 12.5\\% women of reproductive age in Rwanda being victims of GBV\nand one in five women in low- and middle-income countries experiencing mental\nillness during the perinatal period. We recommend three automated tools, a\ntopic recommender tool, culturally relevant automated articles, and website\nquality check tools, to guide software and health content developers.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.00961,regular,post_llm,2025,4,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""Putting GenAI on Notice: GenAI Exceptionalism and Contract Law\n\n  Gathering enough data to create sufficiently useful training datasets for\ngenerative artificial intelligence requires scraping most public websites. The\nscraping is conducted using pieces of code (scraping bots) that make copies of\nwebsite pages. Today, there are only a few ways for website owners to\neffectively block these bots from scraping content. One method, prohibiting\nscraping in the website terms of service, is loosely enforced because it is not\nalways clear when the terms are enforceable. This paper aims to clear up the\nconfusion by describing what scraping is, how entities do it, what makes\nwebsite terms of service enforceable, and what claims of damages website owners\nmay make as a result of being scraped. The novel argument of the paper is that\nwhen (i) a site's terms of service or terms of use prohibit scraping or using\nsite content to train AI and (ii) a bot scrapes pages on the website including\nthose terms, the bot's deployer has actual notice of the terms and those terms\nare therefore legally enforceable, meaning the site can claim a breach of\ncontract. This paper also details the legal and substantive arguments favoring\nthis position while cautioning that nonprofits with a primarily scientific\nresearch focus should be exempt from such strict enforcement.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.04058,review,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': ""Stochastic, Dynamic, Fluid Autonomy in Agentic AI: Implications for\n  Authorship, Inventorship, and Liability\n\n  Agentic Artificial Intelligence (AI) systems, exemplified by OpenAI's\nDeepResearch, autonomously pursue goals, adapting strategies through implicit\nlearning. Unlike traditional generative AI, which is reactive to user prompts,\nagentic AI proactively orchestrates complex workflows. It exhibits stochastic,\ndynamic, and fluid autonomy: its steps and outputs vary probabilistically\n(stochastic), it evolves based on prior interactions (dynamic), and it operates\nwith significant independence within human-defined parameters, adapting to\ncontext (fluid). While this fosters complex, co-evolutionary human-machine\ninteractions capable of generating uniquely synthesized creative outputs, it\nalso irrevocably blurs boundaries--human and machine contributions become\nirreducibly entangled in intertwined creative processes. Consequently, agentic\nAI poses significant challenges to legal frameworks reliant on clear\nattribution: authorship doctrines struggle to disentangle ownership,\nintellectual property regimes strain to accommodate recursively blended\nnovelty, and liability models falter as accountability diffuses across shifting\nloci of control. The central issue is not the legal treatment of human versus\nmachine contributions, but the fundamental unmappability--the practical\nimpossibility in many cases--of accurately attributing specific creative\nelements to either source. When retroactively parsing contributions becomes\ninfeasible, applying distinct standards based on origin becomes impracticable.\nTherefore, we argue, legal and policy frameworks may need to treat human and\nmachine contributions as functionally equivalent--not for moral or economic\nreasons, but as a pragmatic necessity.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1920928955078125e-06, 'GPT4': 0.00015628337860107422, 'CLAUDE': 0.002285003662109375, 'GOOGLE': 0.0010004043579101562, 'OPENAI_O_SERIES': 4.172325134277344e-07, 'DEEPSEEK': 0.99658203125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 9.59634780883789e-06}}"
2504.15088,review,post_llm,2025,4,"{'ai_likelihood': 1.7881393432617188e-06, 'text': 'Safety Co-Option and Compromised National Security: The Self-Fulfilling\n  Prophecy of Weakened AI Risk Thresholds\n\n  Risk thresholds provide a measure of the level of risk exposure that a\nsociety or individual is willing to withstand, ultimately shaping how we\ndetermine the safety of technological systems. Against the backdrop of the Cold\nWar, the first risk analyses, such as those devised for nuclear systems,\ncemented societally accepted risk thresholds against which safety-critical and\ndefense systems are now evaluated. But today, the appropriate risk tolerances\nfor AI systems have yet to be agreed on by global governing efforts, despite\nthe need for democratic deliberation regarding the acceptable levels of harm to\nhuman life. Absent such AI risk thresholds, AI technologists-primarily industry\nlabs, as well as ""AI safety"" focused organizations-have instead advocated for\nrisk tolerances skewed by a purported AI arms race and speculative\n""existential"" risks, taking over the arbitration of risk determinations with\nlife-or-death consequences, subverting democratic processes.\n  In this paper, we demonstrate how such approaches have allowed AI\ntechnologists to engage in ""safety revisionism,"" substituting traditional\nsafety methods and terminology with ill-defined alternatives that vie for the\naccelerated adoption of military AI uses at the cost of lowered safety and\nsecurity thresholds. We explore how the current trajectory for AI risk\ndetermination and evaluation for foundation model use within national security\nis poised for a race to the bottom, to the detriment of the US\'s national\nsecurity interests. Safety-critical and defense systems must comply with\nassurance frameworks that are aligned with established risk thresholds, and\nfoundation models are no exception. As such, development of evaluation\nframeworks for AI-based military systems must preserve the safety and security\nof US critical and defense infrastructure, and remain in alignment with\ninternational humanitarian law.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.11481,regular,post_llm,2025,4,"{'ai_likelihood': 0.990234375, 'text': ""Leveraging Knowledge Graphs and Large Language Models to Track and\n  Analyze Learning Trajectories\n\n  This study addresses the challenges of tracking and analyzing students'\nlearning trajectories, particularly the issue of inadequate knowledge coverage\nin course assessments. Traditional assessment tools often fail to fully cover\ncourse content, leading to imprecise evaluations of student mastery. To tackle\nthis problem, the study proposes a knowledge graph construction method based on\nlarge language models (LLMs), which transforms learning materials into\nstructured data and generates personalized learning trajectory graphs by\nanalyzing students' test data. Experimental results demonstrate that the model\neffectively alerts teachers to potential biases in their exam questions and\ntracks individual student progress. This system not only enhances the accuracy\nof learning assessments but also helps teachers provide timely guidance to\nstudents who are falling behind, thereby improving overall teaching strategies.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0031223297119140625, 'GPT4': 0.1593017578125, 'CLAUDE': 0.74267578125, 'GOOGLE': 0.024322509765625, 'OPENAI_O_SERIES': 0.0295867919921875, 'DEEPSEEK': 0.005992889404296875, 'GROK': 0.00452423095703125, 'NOVA': 0.00042819976806640625, 'OTHER': 0.01157379150390625, 'HUMAN': 0.0185699462890625}}"
2504.07149,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': ""Glocalizing Generative AI in Education for the Global South: The Design\n  Case of 21st Century Teacher Educator AI for Ghana\n\n  This study presents the design and development of the 21st Century Teacher\nEducator for Ghana GPT, a customized Generative AI (GenAI) tool created using\nOpenAI's Retrieval-Augmented Generation (RAG) and Interactive Semi-Automated\nPrompting Strategy (ISA). Anchored in a Glocalized design approach, this tool\nsupports pre-service teachers (PSTs) in Ghana by embedding localized\nlinguistic, cultural, and curricular content within globally aligned principles\nof ethical and responsible AI use. The model utilizes structured, preloaded\ndatasets-including Ghana's National Teacher Education Curriculum Framework\n(NTECF), UNESCO's (2023) AI guidelines, and culturally responsive pedagogies-to\noffer curriculum-aligned, linguistically adaptive, and pedagogically grounded\nlearning support. The ISA enables users to input their institution, year, and\nsemester, generating tailored academic content such as lecture notes,\nassessment practice, practicum resources, and action research guidance. The\ndesign incorporates the Culture and Context-Aware Framework, GenAI-CRSciA, and\nframeworks addressing GenAI neocolonialism to ensure equity, curriculum\nfidelity, and local relevance. Pilot implementation revealed notable strengths\nin language adaptation and localization, delivering bilingual support in\nEnglish and Ghanaian languages like Twi, Dagbani, Mampruli, and Dagaare, with\ncontextualized examples for deeper understanding. The GPT also generated\npractice assessments aligned with course objectives, reinforcing learner\nengagement. Challenges included occasional hallucinations due to limited\ncorpora in some indigenous languages and access barriers tied to premium\nsubscriptions. This design case contributes to discourse on Glocalized GenAI\nand calls for collaboration with OpenAI NextGen to expand access and\nempirically assess usage across diverse African educational contexts.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0788440704345703e-05, 'GPT4': 0.042724609375, 'CLAUDE': 0.002544403076171875, 'GOOGLE': 0.0012111663818359375, 'OPENAI_O_SERIES': 2.378225326538086e-05, 'DEEPSEEK': 0.953125, 'GROK': 1.7881393432617188e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 3.337860107421875e-06, 'HUMAN': 0.0001417398452758789}}"
2504.08446,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Charting the Parrot\'s Song: A Maximum Mean Discrepancy Approach to\n  Measuring AI Novelty, Originality, and Distinctiveness\n\n  Current intellectual property frameworks struggle to evaluate the novelty of\nAI-generated content, relying on subjective assessments ill-suited for\ncomparing effectively infinite AI outputs against prior art. This paper\nintroduces a robust, quantitative methodology grounded in Maximum Mean\nDiscrepancy (MMD) to measure distributional differences between generative\nprocesses. By comparing entire output distributions rather than conducting\npairwise similarity checks, our approach directly contrasts creative\nprocesses--overcoming the computational challenges inherent in evaluating AI\noutputs against unbounded prior art corpora. Through experiments combining\nkernel mean embeddings with domain-specific machine learning representations\n(LeNet-5 for MNIST digits, CLIP for art), we demonstrate exceptional\nsensitivity: our method distinguishes MNIST digit classes with 95% confidence\nusing just 5-6 samples and differentiates AI-generated art from human art in\nthe AI-ArtBench dataset (n=400 per category; p<0.0001) using as few as 7-10\nsamples per distribution despite human evaluators\' limited discrimination\nability (58% accuracy). These findings challenge the ""stochastic parrot""\nhypothesis by providing empirical evidence that AI systems produce outputs from\nsemantically distinct distributions rather than merely replicating training\ndata. Our approach bridges technical capabilities with legal doctrine, offering\na pathway to modernize originality assessments while preserving intellectual\nproperty law\'s core objectives. This research provides courts and policymakers\nwith a computationally efficient, legally relevant tool to quantify AI\nnovelty--a critical advancement as AI blurs traditional authorship and\ninventorship boundaries.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.7881393432617188e-07, 'GPT4': 0.0006337165832519531, 'CLAUDE': 0.290771484375, 'GOOGLE': 9.655952453613281e-06, 'OPENAI_O_SERIES': 1.430511474609375e-06, 'DEEPSEEK': 0.70849609375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 1.1920928955078125e-06}}"
2504.21185,review,post_llm,2025,4,"{'ai_likelihood': 0.21389431423611113, 'text': 'AI-in-the-Loop Planning for Transportation Electrification: Case Studies\n  from Austin, Texas\n\n  This study explores the integration of AI in transportation electrification\nplanning in Austin, TX, focusing on the use of Geospatial AI (GeoAI),\nGenerative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site\nselection, localized GenAI models support meta-level estimations, and LLMs\nenable scenario simulations. These AI applications require human oversight.\nGeoAI outputs must be evaluated with land use data, GenAI models are not always\naccurate, and LLMs are prone to hallucinations. To ensure accountable planning,\nhuman planners must work alongside AI agents. Establishing a community feedback\nloop is essential to audit automated decisions. Planners should place Community\nExperience (CX) at the center of Urban Planning AI.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.20075,regular,post_llm,2025,4,"{'ai_likelihood': 3.6093923780653213e-06, 'text': ""The EU AI Act in Development Practice: A Pro-justice Approach\n\n  With the adoption of the EU AI Act, companies must understand and implement\nits compliance requirements -- an often complex task, especially in areas like\nrisk management and fundamental rights assessments. This paper introduces our\nHigh-risk EU AI Act Toolkit(HEAT), which offers a pro-justice, feminist\nethics-informed approach to support meaningful compliance. HEAT not only helps\nteams meet regulatory standards but also translates ethical theory into\npractice. We show how feminist perspectives on expertise, stakeholder\nengagement, accessibility, and environmental justice inform HEAT's methods and\nexpand on the Act's baseline. The theories we draw on are not naively utopian.\nInstead, they inspire non-innocent approaches to interrogating normativity in\nsystems of all kinds -- technological and otherwise. By this we mean that\npro-justice orientations are cognizant of their involvement in the systems they\nseek to critique, and offer best practices with how to grapple with the\ntrade-offs inherent in reducing and eradicating harmful behaviour from within.\nThese best practices, as we explain in this paper, are what HEAT both embodies\nand enables.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.07233,regular,post_llm,2025,4,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Skill Demand Forecasting Using Temporal Knowledge Graph Embeddings\n\n  Rapid technological advancements pose a significant threat to a large portion\nof the global workforce, potentially leaving them behind. In today's economy,\nthere is a stark contrast between the high demand for skilled labour and the\nlimited employment opportunities available to those who are not adequately\nprepared for the digital economy. To address this critical juncture and gain a\ndeeper and more rapid understanding of labour market dynamics, in this paper,\nwe approach the problem of skill need forecasting as a knowledge graph (KG)\ncompletion task, specifically, temporal link prediction. We introduce our novel\ntemporal KG constructed from online job advertisements. We then train and\nevaluate different temporal KG embeddings for temporal link prediction.\nFinally, we present predictions of demand for a selection of skills practiced\nby workers in the information technology industry. The code and the data are\navailable on our GitHub repository https://github.com/team611/JobEd.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.00797,review,post_llm,2025,4,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""Bridging the Gap: Integrating Ethics and Environmental Sustainability in\n  AI Research and Practice\n\n  As the possibilities for Artificial Intelligence (AI) have grown, so have\nconcerns regarding its impacts on society and the environment. However, these\nissues are often raised separately; i.e. carbon footprint analyses of AI models\ntypically do not consider how the pursuit of scale has contributed towards\nbuilding models that are both inaccessible to most researchers in terms of cost\nand disproportionately harmful to the environment. On the other hand, model\naudits that aim to evaluate model performance and disparate impacts mostly fail\nto engage with the environmental ramifications of AI models and how these fit\ninto their auditing approaches. In this separation, both research directions\nfail to capture the depth of analysis that can be explored by considering the\ntwo in parallel and the potential solutions for making informed choices that\ncan be developed at their convergence. In this essay, we build upon work\ncarried out in AI and in sister communities, such as philosophy and sustainable\ndevelopment, to make more deliberate connections around topics such as\ngeneralizability, transparency, evaluation and equity across AI research and\npractice. We argue that the efforts aiming to study AI's ethical ramifications\nshould be made in tandem with those evaluating its impacts on the environment,\nand we conclude with a proposal of best practices to better integrate AI ethics\nand sustainability in AI research and practice.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.18236,review,post_llm,2025,4,"{'ai_likelihood': 6.490283542209202e-06, 'text': 'Two Means to an End Goal: Connecting Explainability and Contestability in the Regulation of Public Sector AI\n\nExplainability and its emerging counterpart contestability have become important normative and design principles for trustworthy AI as they enable users and subjects to understand and challenge AI decisions. However, realizing these principles is difficult, as they assume different meanings in technical, legal, and organizational dimensions of AI regulation. To resolve this conceptual polysemy, in this paper, we present the findings of an interview study with 14 experts to examine the intersection and implementation of explainability and contestability, and their understanding in different research communities. We outline differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further describe the main points of friction in the realization of both principles, including the alignment between top-down and bottom-up regulation, the assignment of responsibility, and the need for interdisciplinary collaboration. Lastly, we formulate three recommendations for AI policy to implement both principles through a Regulation by Design perspective. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.17054,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Cyber Value At Risk Model for IoT Ecosystems\n\n  The Internet of Things (IoT) presents unique cybersecurity challenges due to\nits interconnected nature and diverse application domains. This paper explores\nthe application of Cyber Value-at-Risk (Cy-VaR) models to assess and mitigate\ncybersecurity risks in IoT environments. Cy-VaR, rooted in Value at Risk\nprinciples, provides a framework to quantify the potential financial impacts of\ncybersecurity incidents. Initially developed to evaluate overall risk exposure\nacross scenarios, our approach extends Cy-VaR to consider specific IoT layers:\nperception, network, and application. Each layer encompasses distinct\nfunctionalities and vulnerabilities, from sensor data acquisition (perception\nlayer) to secure data transmission (network layer) and application-specific\nservices (application layer). By calculating Cy- VaR for each layer and\nscenario, organizations can prioritize security investments effectively. This\npaper discusses methodologies and models, including scenario-based Cy-VaR and\nlayer-specific risk assessments, emphasizing their application in enhancing IoT\ncybersecurity resilience.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0002636909484863281, 'GPT4': 0.126708984375, 'CLAUDE': 0.0029926300048828125, 'GOOGLE': 0.837890625, 'OPENAI_O_SERIES': 0.00792694091796875, 'DEEPSEEK': 0.00698089599609375, 'GROK': 0.0023174285888671875, 'NOVA': 0.0135650634765625, 'OTHER': 0.0011930465698242188, 'HUMAN': 1.0728836059570312e-06}}"
2504.14324,regular,post_llm,2025,4,"{'ai_likelihood': 0.48638237847222227, 'text': 'Meltdown: Bridging the Perception Gap in Sustainable Food Behaviors\n  Through Immersive VR\n\n  Climate change education often struggles to bridge the perception gap between\neveryday actions and their long-term environmental consequences. In response,\nwe developed Meltdown, an immersive virtual reality (VR) escape room that\nsimulates a grocery shopping and food waste management experience to educate\nuniversity students in Singapore about sustainable consumption. The game\nemphasizes sustainable food choices and disposal practices, combining\ninteractive elements and narrative feedback to promote behavioral change.\nThrough a user study with 36 university students, we observed statistically\nsignificant improvements in participants objective knowledge, perceived\nconfidence, and intention to adopt sustainable behaviors. Our results suggest\nthat experiential VR environments can enhance climate education by making\nabstract environmental concepts more immediate and personally relevant.\n', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.18328,review,post_llm,2025,4,"{'ai_likelihood': 4.2054388258192275e-06, 'text': 'AI Safety Assurance for Automated Vehicles: A Survey on Research,\n  Standardization, Regulation\n\n  Assuring safety of artificial intelligence (AI) applied to safety-critical\nsystems is of paramount importance. Especially since research in the field of\nautomated driving shows that AI is able to outperform classical approaches, to\nhandle higher complexities, and to reach new levels of autonomy. At the same\ntime, the safety assurance required for the use of AI in such safety-critical\nsystems is still not in place. Due to the dynamic and far-reaching nature of\nthe technology, research on safeguarding AI is being conducted in parallel to\nAI standardization and regulation. The parallel progress necessitates\nsimultaneous consideration in order to carry out targeted research and\ndevelopment of AI systems in the context of automated driving. Therefore, in\ncontrast to existing surveys that focus primarily on research aspects, this\npaper considers research, standardization and regulation in a concise way.\nAccordingly, the survey takes into account the interdependencies arising from\nthe triplet of research, standardization and regulation in a forward-looking\nperspective and anticipates and discusses open questions and possible future\ndirections. In this way, the survey ultimately serves to provide researchers\nand safety experts with a compact, holistic perspective that discusses the\ncurrent status, emerging trends, and possible future developments.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.16966,regular,post_llm,2025,4,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Structuring Competency-Based Courses Through Skill Trees\n\n  Computer science education has seen two important trends. One has been a\nshift from raw theory towards skills: competency-based teaching. Another has\nbeen increasing student numbers, with as a result more automation in teaching.\nWhen automating education, it is crucial to properly structure courses, both to\nmanage digitalized educational resources and to facilitate automated coaching\nalgorithms. Currently existing structuring methodologies are focused around\ntheory and not around skills, and are incapable of modeling the dependency\nlinks between skills. Because of this, a new didactic framework is needed.\n  This paper presents a new method of structuring educational contents around\nskills: something that a student is expected to be able to do. It defines Skill\nTrees that show dependencies between skills, and subsequently couples these to\nConcept Trees that contain intuitive ideas/notional machines. Due to the\nalgorithmic nature of computer science, this step-wise approach is especially\nwell-suited to this field of education. Next to formal definitions on Skill\nTrees and Concept Trees, guidelines are given on how to design them and how to\nplan a course using them.\n  The Skill Trees framework has been applied to improve the structure of a\nuniversity database course. Student interviews indicated reduced\nconfusion/stress and less study time required for students to meet their\ndesired skill level.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.01352,review,post_llm,2025,4,"{'ai_likelihood': 3.119309743245443e-05, 'text': ""Quo Vadis, HCOMP? A Review of 12 Years of Research at the Frontier of\n  Human Computation and Crowdsourcing\n\n  The field of human computation and crowdsourcing has historically studied how\ntasks can be outsourced to humans. However, many tasks previously distributed\nto human crowds can today be completed by generative AI with human-level\nabilities, and concerns about crowdworkers increasingly using language models\nto complete tasks are surfacing. These developments undermine core premises of\nthe field. In this paper, we examine the evolution of the Conference on Human\nComputation and Crowdsourcing (HCOMP) - a representative example of the field\nas one of its key venues - through the lens of Kuhn's paradigm shifts. We\nreview 12 years of research at HCOMP, mapping the evolution of HCOMP's research\ntopics and identifying significant shifts over time. Reflecting on the findings\nthrough the lens of Kuhn's paradigm shifts, we suggest that these shifts do not\nconstitute a paradigm shift. Ultimately, our analysis of gradual topic shifts\nover time, combined with data on the evident overlap with related venues,\ncontributes a data-driven perspective to the broader discussion about the\nfuture of HCOMP and the field as a whole.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.11913,regular,post_llm,2025,4,"{'ai_likelihood': 4.304779900444879e-06, 'text': ""Broadening Participation through Physical Computing: Replicating\n  Sensor-Based Programming Workshops for Rural Students in Sri Lanka\n\n  In today's digital world, computing education offers critical opportunities,\nyet systemic inequities exclude under-represented communities, especially in\nrural, under-resourced regions. Early engagement is vital for building interest\nin computing careers and achieving equitable participation. Recent work has\nshown that the use of sensor-enabled tools and block-based programming can\nimprove engagement and self-efficacy for students from under-represented\ngroups, but these findings lack replication in diverse, resource-constrained\nsettings. This study addresses this gap by implementing sensor-based\nprogramming workshops with rural students in Sri Lanka. Replicating methods\nfrom the literature, we conduct a between-group study (sensor vs. non-sensor)\nusing Scratch and real-time environmental sensors. We found that students in\nboth groups reported significantly higher confidence in programming in Scratch\nafter the workshop. In addition, average changes in both self-efficacy and\noutcome expectancy were higher in the experimental (sensor) group than in the\ncontrol (non-sensor) group, mirroring trends observed in the original study\nbeing replicated. We also found that using the sensors helped to enhance\ncreativity and inspired some students to express an interest in information and\ncommunications technology (ICT) careers, supporting the value of such hands-on\nactivities in building programming confidence among under-represented groups.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.02636,review,post_llm,2025,4,"{'ai_likelihood': 0.0010008282131618925, 'text': 'A Framework for Developing University Policies on Generative AI Governance: A Cross-national Comparative Study\n\nAs generative AI (GAI) becomes more integrated into higher education, universities are actively exploring its governance and issuing guidelines to promote responsible use, reflecting varied stages of adoption and orientations. This study undertakes a comparative analysis of current GAI guidelines issued by leading universities in the United States, Japan, and China. Based on these findings, the study proposes a University Policy Development Framework for GAI (UPDF-GAI) to provide both theoretical insights and practical guidance for universities in developing and refining their GAI policies. This study adopts five domains from the extended Technology Acceptance Model. A qualitative content analysis of 124 policy documents from 110 universities was conducted, employing thematic coding to synthesize 20 key themes. These domains and themes form the foundation of the UPDF-GAI framework. The analysis reveals varying priorities and focus of GAI policy of universities in different countries. U.S. universities emphasize faculty autonomy, practical application, and policy adaptability, shaped by cutting-edge research and peer collaboration. Japanese universities take a government-regulated approach, prioritizing ethics and risk management, but provide limited support for AI implementation and flexibility. Chinese universities follow a centralized, government-led model, focusing on technology application over early policy development, while actively exploring GAI integration in education and research. The framework facilitates universities in formulating GAI policies by balancing its values and risks, providing multi-level support, proactively responding to societal impacts, and strengthening self-efficacy. In doing so, it enables the development of sustainable and context-sensitive policies that enhance digital competitiveness and advance preparedness for AI-driven education.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.16198,regular,post_llm,2025,4,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Adaptive continuity-preserving simplification of street networks\n\n  Street network data is widely used to study human-based activities and urban\nstructure. Often, these data are geared towards transportation applications,\nwhich require highly granular, directed graphs that capture the complex\nrelationships of potential traffic patterns. While this level of network detail\nis critical for certain fine-grained mobility models, it represents a hindrance\nfor studies concerned with the morphology of the street network. For the latter\ncase, street network simplification - the process of converting a highly\ngranular input network into its most simple morphological form - is a\nnecessary, but highly tedious preprocessing step, especially when conducted\nmanually. In this manuscript, we develop and present a novel adaptive algorithm\nfor simplifying street networks that is both fully automated and able to mimic\nresults obtained through a manual simplification routine. The algorithm -\navailable in the neatnet Python package - outperforms current state-of-the-art\nprocedures when comparing those methods to manually, human-simplified data,\nwhile preserving network continuity.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.12536,regular,post_llm,2025,4,"{'ai_likelihood': 2.185503641764323e-06, 'text': '""It\'s not approved, but many, like myself, ignore the rule"":\n  Investigating the Landscape and Consequences of Unsanctioned Technology Use\n  in Educational Institutes\n\n  Educators regularly use unsanctioned technologies (apps not formally approved\nby their institutions) for teaching, grading, and other academic tasks. While\nthese tools often support instructional needs, they raise significant privacy,\nsecurity, and regulatory compliance concerns. Despite its importance,\nunderstanding the adoptions and risks from the perspective of educators, who\nserve as de facto decision makers behind unsanctioned technology use, is\nlargely understudied in existing literature.To address this gap, we conducted\ntwo surveys: one with 375 educators who listed 1,373 unsanctioned apps, and\nanother with 21 administrators who either often help educators to set up\neducational technologies (EdTechs) or observe their security or privacy\nincidents. Our study identified 494 unique applications used by educators,\nprimarily for pedagogical utility (n=213) and functional convenience (n=155),\nand the associated risks were often ignored. In fact, despite security and\nprivacy concerns, many educators continued using the same apps (n = 62), citing\na lack of alternatives or heavy dependence as barriers to discontinuation. We\nalso found that fewer than a third of educators were aware of any institutional\npolicy on unsanctioned technology use (K12: 30.3%, HEI: 24.8%), and 22\nknowingly violated such policies. While 107 received formal warnings, only 33\nadjusted their behavior. Finally, we conclude by discussing the implications of\nour findings and future recommendations to minimize the risks.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.18602,regular,post_llm,2025,4,"{'ai_likelihood': 1.986821492513021e-07, 'text': 'Beyond Platforms -- Growing Distributed Transaction Networks for Digital Commerce\n\nWe talk of the internet as digital infrastructure; but we leave the building of rails and roads to the quasi-monopolistic platform providers. Decentralised architectures provide a number of advantages: They are potentially more inclusive for small players; more resilient against adversarial events; and seem to generate more innovation. However, it is not well understood how to evolve, adapt and govern decentralised infrastructures. This article reports qualitative empirical research on the development and governance of the Beckn Protocol, an open source protocol for decentralised transactions, the successful development of domain-specific adaptations, and implementation and scaling of commercial infrastructures based on it. It explores how the architecture and governance support local innovation for specific business domains, and how the domain-specific innovations feed back into the development of the core concept The research applied a case study approach, combining interviews with core members of the Beckn community; triangulated by interviews with community leaders of domain specific adaptations and by analysis of online documents and the protocol itself. The article shows the possibility of such a decentralised approach to IT Infrastructures. It analyses the Beckn Protocol, domain specific adaptations, and networks built as a software ecosystem. Based on this analysis, a number of generative mechanisms, socio-technical arrangements that support adoption, innovation, and scaling of infrastructures are highlighted.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.13839,review,post_llm,2025,4,"{'ai_likelihood': 0.0148773193359375, 'text': 'Audit Cards: Contextualizing AI Evaluations\n\nAI governance frameworks increasingly rely on audits, yet the results of their underlying evaluations require interpretation and context to be meaningfully informative. Even technically rigorous evaluations can offer little useful insight if reported selectively or obscurely. Current literature focuses primarily on technical best practices, but evaluations are an inherently sociotechnical process, and there is little guidance on reporting procedures and context. Through literature review, stakeholder interviews, and analysis of governance frameworks, we propose ""audit cards"" to make this context explicit. We identify six key types of contextual features to report and justify in audit cards: auditor identity, evaluation scope, methodology, resource access, process integrity, and review mechanisms. Through analysis of existing evaluation reports, we find significant variation in reporting practices, with most reports omitting crucial contextual information such as auditors\' backgrounds, conflicts of interest, and the level and type of access to models. We also find that most existing regulations and frameworks lack guidance on rigorous reporting. In response to these shortcomings, we argue that audit cards can provide a structured format for reporting key claims alongside their justifications, enhancing transparency, facilitating proper interpretation, and establishing trust in reporting.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.18081,review,post_llm,2025,4,"{'ai_likelihood': 0.921875, 'text': 'Hype and Adoption of Generative Artificial Intelligence Applications\n\n  New technologies create opportunities while displacing others. They enhance\nlife by supporting entertainment, education, and social connectivity but also\nreplace humans in productivity and analytical tasks. Adapting to these shifts\nrequires technical adjustments and social readiness. For digital transformation\nto succeed, organizations and their workforce must be psychologically prepared.\nWe are entering the era of Generative AI with tools like ChatGPT, Bing AI, and\nMicrosoft Office Copilot. Understanding public sentiment toward these\ninnovations is crucial for refining technology acceptance models and informing\nmarket strategies. Using the Gartner Hype Cycle and Kubler-Ross Change Curve,\nthis study suggests that generative AI adoption is a dual-stage process. It\nfollows the phases of technology trigger, peak of expectations, trough of\ndisillusionment, slope of enlightenment, and plateau of productivity, while\nalso reflecting emotional stages like shock, denial, and integration. The study\nused sentiment and emotion analysis on a large dataset of tweets about\ngenerative AI, translating them into scores to track user responses over time.\nUnlike prior research, which offered a snapshot of sentiment, this study\ncaptures the dynamic evolution of attitudes, linking empirical evidence with\ntheoretical frameworks. It shifts the focus from information seekers to content\ncreators. With the release of generative AI tools, there is a significant gap\nin understanding societal reception and adaptation. Policymakers face\nuncertainty about guiding markets for these changes. This research validates\nthe applicability of the Gartner Hype Cycle and Kubler-Ross Change Curve to\ngenerative AI. It provides insights for businesses in integrating these tools\nand crafting policies to enhance readiness and resilience.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0024776458740234375, 'GPT4': 0.10931396484375, 'CLAUDE': 0.0023403167724609375, 'GOOGLE': 0.36181640625, 'OPENAI_O_SERIES': 0.51025390625, 'DEEPSEEK': 0.00801849365234375, 'GROK': 7.510185241699219e-06, 'NOVA': 4.464387893676758e-05, 'OTHER': 0.0003554821014404297, 'HUMAN': 0.005207061767578125}}"
2504.15416,review,post_llm,2025,4,"{'ai_likelihood': 3.245141771104601e-05, 'text': 'Bare Minimum Mitigations for Autonomous AI Development\n\n  Artificial intelligence (AI) is advancing rapidly, with the potential for\nsignificantly automating AI research and development itself in the near future.\nIn 2024, international scientists, including Turing Award recipients, warned of\nrisks from autonomous AI research and development (R&D), suggesting a red line\nsuch that no AI system should be able to improve itself or other AI systems\nwithout explicit human approval and assistance. However, the criteria for\nmeaningful human approval remain unclear, and there is limited analysis on the\nspecific risks of autonomous AI R&D, how they arise, and how to mitigate them.\nIn this brief paper, we outline how these risks may emerge and propose four\nminimum safeguard recommendations applicable when AI agents significantly\nautomate or accelerate AI development.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.09137,review,post_llm,2025,4,"{'ai_likelihood': 0.01190185546875, 'text': ""Can Large Language Models Become Policy Refinement Partners? Evidence\n  from China's Social Security Studies\n\n  The rapid development of large language models (LLMs) is reshaping\noperational paradigms across multidisciplinary domains. LLMs' emergent\ncapability to synthesize policy-relevant insights across disciplinary\nboundaries suggests potential as decision-support tools. However, their actual\nperformance and suitability as policy refinement partners still require\nverification through rigorous and systematic evaluations. Our study employs the\ncontext-embedded generation-adaptation framework to conduct a tripartite\ncomparison among the American GPT-4o, the Chinese DeepSeek-R1 and human\nresearchers, investigating the capability boundaries and performance\ncharacteristics of LLMs in generating policy recommendations for China's social\nsecurity issues. This study demonstrates that while LLMs exhibit distinct\nadvantages in systematic policy design, they face significant limitations in\naddressing complex social dynamics, balancing stakeholder interests, and\ncontrolling fiscal risks within the social security domain. Furthermore,\nDeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation\ndimensions in policy recommendation generation, illustrating the potential of\nlocalized training to improve contextual alignment. These findings suggest that\nregionally-adapted LLMs can function as supplementary tools for generating\ndiverse policy alternatives informed by domain-specific social insights.\nNevertheless, the formulation of policy refinement requires integration with\nhuman researchers' expertise, which remains critical for interpreting\ninstitutional frameworks, cultural norms, and value systems.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.1379,review,post_llm,2025,4,"{'ai_likelihood': 2.317958407931858e-07, 'text': 'Four Bottomless Errors and the Collapse of Statistical Fairness\n\n  The AI ethics of statistical fairness is an error, the approach should be\nabandoned, and the accumulated academic work deleted. The argument proceeds by\nidentifying four recurring mistakes within statistical fairness. One conflates\nfairness with equality, which confines thinking to similars being treated\nsimilarly. The second and third errors derive from a perspectival ethical view\nwhich functions by negating others and their viewpoints. The final mistake\nconstrains fairness to work within predefined social groups instead of allowing\nunconstrained fairness to subsequently define group composition. From the\nnature of these misconceptions, the larger argument follows. Because the errors\nare integral to how statistical fairness works, attempting to resolve the\ndifficulties only deepens them. Consequently, the errors cannot be corrected\nwithout undermining the larger project, and statistical fairness collapses from\nwithin. While the collapse ends a failure in ethics, it also provokes distinct\npossibilities for fairness, data, and algorithms. Quickly indicating some of\nthese directions is a secondary aim of the paper, and one that aligns with what\nfairness has consistently meant and done since Aristotle.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.02127,review,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': 'Reinsuring AI: Energy, Agriculture, Finance & Medicine as Precedents for\n  Scalable Governance of Frontier Artificial Intelligence\n\n  The governance of frontier artificial intelligence (AI) systems--particularly\nthose capable of catastrophic misuse or systemic failure--requires\ninstitutional structures that are robust, adaptive, and innovation-preserving.\nThis paper proposes a novel framework for governing such high-stakes models\nthrough a three-tiered insurance architecture: (1) mandatory private liability\ninsurance for frontier model developers; (2) an industry-administered risk pool\nto absorb recurring, non-catastrophic losses; and (3) federally backed\nreinsurance for tail-risk events. Drawing from historical precedents in nuclear\nenergy (Price-Anderson), terrorism risk (TRIA), agricultural crop insurance,\nflood reinsurance, and medical malpractice, the proposal shows how the federal\ngovernment can stabilize private AI insurance markets without resorting to\nbrittle regulation or predictive licensing regimes. The structure aligns\nincentives between AI developers and downstream stakeholders, transforms safety\npractices into insurable standards, and enables modular oversight through\nadaptive eligibility criteria. By focusing on risk-transfer mechanisms rather\nthan prescriptive rules, this framework seeks to render AI safety a structural\nfeature of the innovation ecosystem itself--integrated into capital markets,\nnot external to them. The paper concludes with a legal and administrative\nfeasibility analysis, proposing avenues for statutory authorization and agency\nplacement within existing federal structures.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 7.510185241699219e-06, 'CLAUDE': 0.00609588623046875, 'GOOGLE': 5.960464477539063e-08, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.99365234375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 2.980232238769531e-07}}"
2504.01167,regular,post_llm,2025,4,"{'ai_likelihood': 9.338061014811199e-06, 'text': 'Predicting Field Experiments with Large Language Models\n\nLarge language models (LLMs) have demonstrated unprecedented emergent capabilities, including content generation, translation, and simulation of human behavior. Field experiments, on the other hand, are widely employed in social studies to examine real-world human behavior through carefully designed manipulations and treatments. However, field experiments are known to be expensive and time consuming. Therefore, an interesting question is whether and how LLMs can be utilized for field experiments. In this paper, we propose and evaluate an automated LLM-based framework to predict the outcomes of a field experiment. Applying this framework to 276 experiments about a wide range of human behaviors drawn from renowned economics literature yields a prediction accuracy of 78%. Moreover, we find that the distributions of the results are either bimodal or highly skewed. By investigating this abnormality further, we identify that field experiments related to complex social issues such as ethnicity, social norms, and ethical dilemmas can pose significant challenges to the prediction performance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.17056,regular,post_llm,2025,4,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Evaluating energy inefficiency in energy-poor households in India: A\n  frontier analysis approach\n\n  Energy-poor households often compromise their thermal comfort and refrain\nfrom operating mechanical cooling devices to avoid high electricity bills. This\nis compounded by certain behavioral practices like retention of older, less\nefficient appliances, resulting in missed energy savings. Thus, the need to\nenhance efficiency becomes critical in these households. However, due to a lack\nof comprehensive data in India, little is understood about their electricity\nconsumption patterns and usage efficiency. Estimating inefficiency and\nassessing its determinants is crucial for improving their quality of life. This\nstudy measures the inefficiency in electricity consumption due to household\npractices and appliances in social housing in Mumbai, India. It considers\ntechnological determinants in addition to socio-economic variables. The study\nemploys primary data collected from rehabilitation housing and slums in Mumbai.\nStochastic frontier analysis, a parametric approach, is applied to estimate\nindicators of electricity consumption and inefficiency. While household size\nand workforce participation significantly affect consumption behavior in\nrehabilitation housing, it is limited to the workforce in slums. The ownership\nof appliances, except for washing machines in slums, also exhibits considerable\nimpacts. The mean efficiency scores of 83% and 91% for rehabilitation housing\nand slums, respectively, empirically quantify the potential savings achievable.\nFactors that positively influence inefficiency include the duration of\noperating refrigerators, washing machines, iron, and AC. These results hold\nimplications for enhancing the uptake of efficient appliances in addition to\naccelerating energy efficiency retrofits in the region. Policies should focus\non awareness and the development of appliance markets through incentives.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2504.17897,regular,post_llm,2025,4,"{'ai_likelihood': 1.0, 'text': ""A Walk across Europe: Development of a high-resolution walkability index\n\nPhysical inactivity significantly contributes to obesity and other non-communicable diseases, yet efforts to increase population-wide physical activity levels have met with limited success. The built environment plays a pivotal role in encouraging active behaviors like walking. Walkability indices, which aggregate various environmental features, provide a valuable tool for promoting healthy, walkable environments. However, a standardized, high-resolution walkability index for Europe has been lacking. This study addresses that gap by developing a standardized, high-resolution walkability index for the entire European region. Seven core components were selected to define walkability: walkable street length, intersection density, green spaces, slope, public transport access, land use mix, and 15-minute walking isochrones. These were derived from harmonized, high-resolution datasets such as Sentinel-2, NASA's elevation models, OpenStreetMap, and CORINE Land Cover. A 100 m x 100 m hierarchical grid system and advanced geospatial methods, like network buffers and distance decay, were used at scale to efficiently model real-world density and proximity effects. The resulting index was weighted by population and analyzed at different spatial levels using visual mapping, spatial clustering, and correlation analysis. Findings revealed a distinct urban-to-rural gradient, with high walkability scores concentrated in compact urban centers rich in street connectivity and land use diversity. The index highlighted cities like Barcelona, Berlin, Munich, Paris, and Warsaw as walkability leaders. This standardized, high-resolution walkability index serves as a practical tool for researchers, planners, and policymakers aiming to support active living and public health across diverse European contexts."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.265806198120117e-05, 'GPT4': 0.17138671875, 'CLAUDE': 0.021453857421875, 'GOOGLE': 0.005184173583984375, 'OPENAI_O_SERIES': 4.9591064453125e-05, 'DEEPSEEK': 0.8017578125, 'GROK': 2.384185791015625e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 7.867813110351562e-06, 'HUMAN': 3.4749507904052734e-05}}"
2504.16135,regular,post_llm,2025,4,"{'ai_likelihood': 0.91455078125, 'text': ""Optimizing Post-Cancer Treatment Prognosis: A Study of Machine Learning\n  and Ensemble Techniques\n\n  The aim is to create a method for accurately estimating the duration of\npost-cancer treatment, particularly focused on chemotherapy, to optimize\npatient care and recovery. This initiative seeks to improve the effectiveness\nof cancer treatment, emphasizing the significance of each patient's journey and\nwell-being. Our focus is to provide patients with valuable insight into their\ntreatment timeline because we deeply believe that every life matters. We\ncombined medical expertise with smart technology to create a model that\naccurately predicted each patient's treatment timeline. By using machine\nlearning, we personalized predictions based on individual patient details which\nwere collected from a regional government hospital named Sylhet M.A.G. Osmani\nMedical College & Hospital, Sylhet, Bangladesh, improving cancer care\neffectively. We tackled the challenge by employing around 13 machine learning\nalgorithms and analyzing 15 distinct features, including LR, SVM, DT, RF, etc.\nwe obtained a refined precision in predicting cancer patient's treatment\ndurations. Furthermore, we utilized ensemble techniques to reinforce the\naccuracy of our methods. Notably, our study revealed that our majority voting\nensemble classifier displayed exceptional performance, achieving 77% accuracy,\nwith LightGBM and Random Forest closely following at approximately 76%\naccuracy. Our research unveiled the inherent complexities of cancer datasets,\nas seen in the Decision Tree's 59% accuracy. This emphasizes the need for\nimproved algorithms to better predict outcomes and enhance patient care. Our\ncomparison with other methods confirmed our promising accuracy rates, showing\nthe potential impact of our approach in improving cancer treatment strategies.\nThis study marks a significant step forward in optimizing post-cancer treatment\nprognosis using machine learning and ensemble techniques.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.1217041015625, 'GPT4': 0.57373046875, 'CLAUDE': 9.47117805480957e-05, 'GOOGLE': 0.291015625, 'OPENAI_O_SERIES': 0.00302886962890625, 'DEEPSEEK': 0.00014221668243408203, 'GROK': 2.491474151611328e-05, 'NOVA': 0.00012254714965820312, 'OTHER': 0.002353668212890625, 'HUMAN': 0.007717132568359375}}"
2505.00616,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Catastrophic Liability: Managing Systemic Risks in Frontier AI Development\n\nAs artificial intelligence systems grow more capable and autonomous, frontier AI development poses potential systemic risks that could affect society at a massive scale. Current practices at many AI labs developing these systems lack sufficient transparency around safety measures, testing procedures, and governance structures. This opacity makes it challenging to verify safety claims or establish appropriate liability when harm occurs. Drawing on liability frameworks from nuclear energy, aviation software, cybersecurity, and healthcare, we propose a comprehensive approach to safety documentation and accountability in frontier AI development.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539062e-07, 'GPT4': 3.337860107421875e-05, 'CLAUDE': 1.0, 'GOOGLE': 1.3113021850585938e-06, 'OPENAI_O_SERIES': 1.3768672943115234e-05, 'DEEPSEEK': 2.562999725341797e-06, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 1.9669532775878906e-06}}"
2505.01106,regular,post_llm,2025,5,"{'ai_likelihood': 0.00016358163621690538, 'text': 'Investigating Middle School Students Question-Asking and\n  Answer-Evaluation Skills When Using ChatGPT for Science Investigation\n\n  Generative AI (GenAI) tools such as ChatGPT allow users, including school\nstudents without prior AI expertise, to explore and address a wide range of\ntasks. Surveys show that most students aged eleven and older already use these\ntools for school-related activities. However, little is known about how they\nactually use GenAI and how it impacts their learning.\n  This study addresses this gap by examining middle school students ability to\nask effective questions and critically evaluate ChatGPT responses, two\nessential skills for active learning and productive interactions with GenAI. 63\nstudents aged 14 to 15 were tasked with solving science investigation problems\nusing ChatGPT. We analyzed their interactions with the model, as well as their\nresulting learning outcomes.\n  Findings show that students often over-relied on ChatGPT in both the\nquestion-asking and answer-evaluation phases. Many struggled to use clear\nquestions aligned with task goals and had difficulty judging the quality of\nresponses or knowing when to seek clarification. As a result, their learning\nperformance remained moderate: their explanations of the scientific concepts\ntended to be vague, incomplete, or inaccurate, even after unrestricted use of\nChatGPT. This pattern held even in domains where students reported strong prior\nknowledge.\n  Furthermore, students self-reported understanding and use of ChatGPT were\nnegatively associated with their ability to select effective questions and\nevaluate responses, suggesting misconceptions about the tool and its\nlimitations. In contrast, higher metacognitive skills were positively linked to\nbetter QA-related skills.\n  These findings underscore the need for educational interventions that promote\nAI literacy and foster question-asking strategies to support effective learning\nwith GenAI.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.18882,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach\n\nLarge language models (LLMs) typically generate identical or similar responses for all users given the same prompt, posing serious safety risks in high-stakes applications where user vulnerabilities differ widely. Existing safety evaluations primarily rely on context-independent metrics - such as factuality, bias, or toxicity - overlooking the fact that the same response may carry divergent risks depending on the user's background or condition. We introduce personalized safety to fill this gap and present PENGUIN - a benchmark comprising 14,000 scenarios across seven sensitive domains with both context-rich and context-free variants. Evaluating six leading LLMs, we demonstrate that personalized user information significantly improves safety scores by 43.2%, confirming the effectiveness of personalization in safety alignment. However, not all context attributes contribute equally to safety enhancement. To address this, we develop RAISE - a training-free, two-stage agent framework that strategically acquires user-specific background. RAISE improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining a low interaction cost of just 2.7 user queries on average. Our findings highlight the importance of selective information gathering in safety-critical domains and offer a practical solution for personalizing LLM responses without model retraining. This work establishes a foundation for safety research that adapts to individual user contexts rather than assuming a universal harm standard."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.2007694244384766e-05, 'GPT4': 0.014434814453125, 'CLAUDE': 0.79833984375, 'GOOGLE': 0.0023670196533203125, 'OPENAI_O_SERIES': 6.848573684692383e-05, 'DEEPSEEK': 0.1844482421875, 'GROK': 4.172325134277344e-07, 'NOVA': 4.172325134277344e-07, 'OTHER': 2.199411392211914e-05, 'HUMAN': 0.00039577484130859375}}"
2505.11401,regular,post_llm,2025,5,"{'ai_likelihood': 0.8876953125, 'text': 'Can AI automatically analyze public opinion? A LLM agents-based agentic pipeline for timely public opinion analysis\n\nThis study proposes and implements the first LLM agents based agentic pipeline for multi task public opinion analysis. Unlike traditional methods, it offers an end-to-end, fully automated analytical workflow without requiring domain specific training data, manual annotation, or local deployment. The pipeline integrates advanced LLM capabilities into a low-cost, user-friendly framework suitable for resource constrained environments. It enables timely, integrated public opinion analysis through a single natural language query, making it accessible to non-expert users. To validate its effectiveness, the pipeline was applied to a real world case study of the 2025 U.S. China tariff dispute, where it analyzed 1,572 Weibo posts and generated a structured, multi part analytical report. The results demonstrate some relationships between public opinion and governmental decision-making. These contributions represent a novel advancement in applying generative AI to public governance, bridging the gap between technical sophistication and practical usability in public opinion monitoring.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.00022542476654052734, 'GPT4': 0.0179443359375, 'CLAUDE': 0.67041015625, 'GOOGLE': 0.0227813720703125, 'OPENAI_O_SERIES': 0.0018835067749023438, 'DEEPSEEK': 0.07586669921875, 'GROK': 1.5914440155029297e-05, 'NOVA': 6.23464584350586e-05, 'OTHER': 0.0008912086486816406, 'HUMAN': 0.209716796875}}"
2505.046,regular,post_llm,2025,5,"{'ai_likelihood': 0.033772786458333336, 'text': 'Perpetuating Misogyny with Generative AI: How Model Personalization Normalizes Gendered Harm\n\nOpen-source text-to-image (TTI) pipelines have become dominant in the landscape of AI-generated visual content, driven by technological advances that enable users to personalize models through adapters tailored to specific tasks. While personalization methods such as LoRA offer unprecedented creative opportunities, they also facilitate harmful practices, including the generation of non-consensual deepfakes and the amplification of misogynistic or hypersexualized content. This study presents an exploratory sociotechnical analysis of CivitAI, the most active platform for sharing and developing open-source TTI models. Drawing on a dataset of more than 40 million user-generated images and over 230,000 models, we find a disproportionate rise in not-safe-for-work (NSFW) content and a significant number of models intended to mimic real individuals. We also observe a strong influence of internet subcultures on the tools and practices shaping model personalizations and resulting visual media. In response to these findings, we contextualize the emergence of exploitative visual media through feminist and constructivist perspectives on technology, emphasizing how design choices and community dynamics shape platform outcomes. Building on this analysis, we propose interventions aimed at mitigating downstream harm, including improved content moderation, rethinking tool design, and establishing clearer platform policies to promote accountability and consent.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.11678,regular,post_llm,2025,5,"{'ai_likelihood': 0.00013947486877441406, 'text': 'Testing Fairness with Utility Tradeoffs: A Wasserstein Projection Approach\n\nEnsuring fairness in data driven decision making has become a central concern across domains such as marketing, lending, and healthcare, but fairness constraints often come at the cost of utility. We propose a statistical hypothesis testing framework that jointly evaluates approximate fairness and utility, relaxing strict fairness requirements while ensuring that overall utility remains above a specified threshold. Our framework builds on the strong demographic parity (SDP) criterion and incorporates a utility measure motivated by the potential outcomes framework. The test statistic is constructed via Wasserstein projections, enabling auditors to assess whether observed fairness-utility tradeoffs are intrinsic to the algorithm or attributable to randomness in the data. We show that the test is computationally tractable, interpretable, broadly applicable across machine learning models, and extendable to more general settings. We apply our approach to multiple real-world datasets, offering new insights into the fairness-utility tradeoff through the perspective of statistical hypothesis testing.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.21057,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Examining the sentiment and emotional differences in product and service reviews: The moderating role of culture\n\nThis study explores how emotions and sentiments differ in customer reviews of products and services on e-commerce platforms. Unlike earlier research that treats all reviews uniformly, this study distinguishes between reviews of products, typically fulfilling basic, functional needs, and services, which often cater to experiential and emotional desires. The findings reveal clear differences in emotional expression and sentiment between the two. Product reviews frequently focus on practicality, such as functionality, reliability, and value for money, and are generally more neutral or pragmatic in tone. In contrast, service reviews involve stronger emotional engagement, as services often entail personal interactions and subjective experiences. Customers express a broader spectrum of emotions, such as joy, frustration, or disappointment when reviewing services, as identified using advanced machine learning techniques. Cultural background further influences these patterns. Consumers from collectivist cultures, as defined by Hofstede cultural dimensions, often use more moderated and socially considerate language, reflecting an emphasis on group harmony. Conversely, consumers from individualist cultures tend to offer more direct, emotionally intense feedback. Notably, gender appears to have minimal impact on sentiment variation, reinforcing the idea that the nature of the offering (product vs. service) and cultural context are the dominant factors. Theoretically, the study extends Maslow hierarchy of needs and Hofstede cultural framework to the domain of online reviews, proposing a model that explains how these dimensions shape consumer expression. Practically, the insights offer valuable guidance for businesses looking to optimize their marketing and customer engagement strategies by aligning messaging and service design with customer expectations across product types and cultural backgrounds.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.5367431640625e-07, 'GPT4': 1.0, 'CLAUDE': 1.6391277313232422e-05, 'GOOGLE': 1.6391277313232422e-05, 'OPENAI_O_SERIES': 4.589557647705078e-06, 'DEEPSEEK': 1.9609928131103516e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 1.7881393432617188e-07}}"
2505.21704,review,post_llm,2025,5,"{'ai_likelihood': 0.00015974044799804688, 'text': ""Lecturers' perspectives on the integration of research data management into teacher training programmes\n\nThis article focuses on how data literacy education such as research data management skills can be integrated into teacher training programmes in order to adequately train the teachers of tomorrow. To this end, interviews were conducted with three lecturers from the Faculty of Education and analysed both qualitatively and quantitatively. The lecturers describe the topic of research data management as extremely relevant for students, especially in the Master's program. Even as future teachers, for example in computer science and the natural sciences, students will have a lot to do with data and need to be able to handle it competently. The article also discusses how research data management skills can be integrated into the teacher training program."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.02519,review,post_llm,2025,5,"{'ai_likelihood': 0.9853515625, 'text': 'Deaf in AI: AI language technologies and the erosion of linguistic\n  rights\n\n  This paper explores the interplay of AI language technologies, sign language\ninterpreting, and linguistic access, highlighting the complex interdependencies\nshaping access frameworks and the tradeoffs these technologies bring. While AI\ntools promise innovation, they also perpetuate biases, reinforce technoableism,\nand deepen inequalities through systemic and design flaws. The historical and\ncontemporary privileging of sign language interpreting as the dominant access\nmodel, and the broader inclusion ideologies it reflects, shape AIs development\nand deployment, often sidelining deaf languaging practices and introducing new\nforms of linguistic subordination to technology. Drawing on Deaf Studies, Sign\nLanguage Interpreting Studies, and crip technoscience, this paper critiques the\nframing of AI as a substitute for interpreters and examines its implications\nfor access hierarchies. It calls for deaf-led approaches to foster AI systems\nthat remain equitable, inclusive, and trustworthy, supporting rather than\nundermining linguistic autonomy and contributing to deaf aligned futures.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004227161407470703, 'GPT4': 0.43115234375, 'CLAUDE': 0.226318359375, 'GOOGLE': 0.0137786865234375, 'OPENAI_O_SERIES': 0.000335693359375, 'DEEPSEEK': 0.29345703125, 'GROK': 2.5033950805664062e-06, 'NOVA': 1.341104507446289e-05, 'OTHER': 0.00013720989227294922, 'HUMAN': 0.03424072265625}}"
2505.02324,regular,post_llm,2025,5,"{'ai_likelihood': 2.84115473429362e-05, 'text': 'From Course to Skill: Evaluating LLM Performance in Curricular Analytics\n\nCurricular analytics (CA) -- systematic analysis of curricula data to inform program and course refinement -- becomes an increasingly valuable tool to help institutions align academic offerings with evolving societal and economic demands. Large language models (LLMs) are promising for handling large-scale, unstructured curriculum data, but it remains uncertain how reliably LLMs can perform CA tasks. In this paper, we systematically evaluate four text alignment strategies based on LLMs or traditional NLP methods for skill extraction, a core task in CA. Using a stratified sample of 400 curriculum documents of different types and a human-LLM collaborative evaluation framework, we find that retrieval-augmented generation (RAG) is the top-performing strategy across all types of curriculum documents, while zero-shot prompting performs worse than traditional NLP methods in most cases. Our findings highlight the promise of LLMs in analyzing brief and abstract curriculum documents, but also reveal that their performance can vary significantly depending on model selection and prompting strategies. This underscores the importance of carefully evaluating the performance of LLM-based strategies before large-scale deployment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.08672,regular,post_llm,2025,5,"{'ai_likelihood': 0.00015722380744086372, 'text': ""How Students Use AI Feedback Matters: Experimental Evidence on Physics Achievement and Autonomy\n\nDespite the precision and adaptiveness of generative AI (GAI)-powered feedback provided to students, existing practice and literature might ignore how usage patterns impact student learning. This study examines the heterogeneous effects of GAI-powered personalized feedback on high school students' physics achievement and autonomy through two randomized controlled trials, with a major focus on usage patterns. Each experiment lasted for five weeks, involving a total of 387 students. Experiment 1 (n = 121) assessed compulsory usage of the personalized recommendation system, revealing that low-achieving students significantly improved academic performance (d = 0.673, p < 0.05) when receiving AI-generated heuristic solution hints, whereas medium-achieving students' performance declined (d = -0.539, p < 0.05) with conventional answers provided by workbook. Notably, high-achieving students experienced a significant decline in self-regulated learning (d = -0.477, p < 0.05) without any significant gains in achievement. Experiment 2 (n = 266) investigated the usage pattern of autonomous on-demand help, demonstrating that fully learner-controlled AI feedback significantly enhanced academic performance for high-achieving students (d = 0.378, p < 0.05) without negatively impacting their autonomy. However, autonomy notably declined among lower achievers exposed to on-demand AI interventions (d = -0.383, p < 0.05), particularly in the technical-psychological dimension (d = -0.549, p < 0.05), which has a large overlap with self-regulation. These findings underscore the importance of usage patterns when applying GAI-powered personalized feedback to students."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.00651,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Innovative Tangible Interactive Games for Enhancing Artificial Intelligence Knowledge and Literacy in Elementary Education: A Pedagogical Framework\n\nThis paper presents an innovative pedagogical framework employing tangible interactive games to enhance artificial intelligence (AI) knowledge and literacy among elementary education students. Recognizing the growing importance of AI competencies in the 21st century, this study addresses the critical need for age-appropriate, experiential learning tools that demystify core AI concepts for young learners. The proposed approach integrates physical role-playing activities that embody fundamental AI principles, including neural networks, decision-making, machine learning, and pattern recognition. Through carefully designed game mechanics, students actively engage in collaborative problem solving, fostering deeper conceptual understanding and critical thinking skills. The framework further supports educators by providing detailed guidance on implementation and pedagogical objectives, thus facilitating effective AI education in early childhood settings. Empirical insights and theoretical grounding demonstrate the potential of tangible interactive games to bridge the gap between abstract AI theories and practical comprehension, ultimately promoting AI literacy at foundational educational levels. The study contributes to the growing discourse on AI education by offering scalable and adaptable strategies that align with contemporary curricular demands and prepare young learners for a technologically driven future.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.86102294921875e-06, 'GPT4': 0.625, 'CLAUDE': 0.364501953125, 'GOOGLE': 8.33272933959961e-05, 'OPENAI_O_SERIES': 0.00884246826171875, 'DEEPSEEK': 0.001678466796875, 'GROK': 0.0, 'NOVA': 7.748603820800781e-07, 'OTHER': 5.960464477539062e-07, 'HUMAN': 5.960464477539063e-08}}"
2505.06971,review,post_llm,2025,5,"{'ai_likelihood': 9.271833631727431e-07, 'text': ""The promise and perils of AI in medicine\n\nWhat does Artificial Intelligence (AI) have to contribute to health care? And what should we be looking out for if we are worried about its risks? In this paper we offer a survey, and initial evaluation, of hopes and fears about the applications of artificial intelligence in medicine. AI clearly has enormous potential as a research tool, in genomics and public health especially, as well as a diagnostic aid. It's also highly likely to impact on the organisational and business practices of healthcare systems in ways that are perhaps under-appreciated. Enthusiasts for AI have held out the prospect that it will free physicians up to spend more time attending to what really matters to them and their patients. We will argue that this claim depends upon implausible assumptions about the institutional and economic imperatives operating in contemporary healthcare settings. We will also highlight important concerns about privacy, surveillance, and bias in big data, as well as the risks of over trust in machines, the challenges of transparency, the deskilling of healthcare practitioners, the way AI reframes healthcare, and the implications of AI for the distribution of power in healthcare institutions. We will suggest that two questions, in particular, are deserving of further attention from philosophers and bioethicists. What does care look like when one is dealing with data as much as people? And, what weight should we give to the advice of machines in our own deliberations about medical decisions?"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.21808,review,post_llm,2025,5,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'AI Agent Governance: A Field Guide\n\nThis report serves as an accessible guide to the emerging field of AI agent governance. Agents - AI systems that can autonomously achieve goals in the world, with little to no explicit human instruction about how to do so - are a major focus of leading tech companies, AI start-ups, and investors. If these development efforts are successful, some industry leaders claim we could soon see a world where millions or billions of agents autonomously perform complex tasks across society. Society is largely unprepared for this development. A future where capable agents are deployed en masse could see transformative benefits to society but also profound and novel risks. Currently, the exploration of agent governance questions and the development of associated interventions remain in their infancy. Only a few researchers, primarily in civil society organizations, public research institutes, and frontier AI companies, are actively working on these challenges.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.07772,review,post_llm,2025,5,"{'ai_likelihood': 2.3411379920111762e-05, 'text': ""The Value of Disagreement in AI Design, Evaluation, and Alignment\n\nDisagreements are widespread across the design, evaluation, and alignment pipelines of artificial intelligence (AI) systems. Yet, standard practices in AI development often obscure or eliminate disagreement, resulting in an engineered homogenization that can be epistemically and ethically harmful, particularly for marginalized groups. In this paper, we characterize this risk, and develop a normative framework to guide practical reasoning about disagreement in the AI lifecycle. Our contributions are two-fold. First, we introduce the notion of perspectival homogenization, characterizing it as a coupled ethical-epistemic risk that arises when an aspect of an AI system's development unjustifiably suppresses disagreement and diversity of perspectives. We argue that perspectival homogenization is best understood as a procedural risk, which calls for targeted interventions throughout the AI development pipeline. Second, we propose a normative framework to guide such interventions, grounded in lines of research that explain why disagreement can be epistemically beneficial, and how its benefits can be realized in practice. We apply this framework to key design questions across three stages of AI development tasks: when disagreement is epistemically valuable; whose perspectives should be included and preserved; how to structure tasks and navigate trade-offs; and how disagreement should be documented and communicated. In doing so, we challenge common assumptions in AI practice, offer a principled foundation for emerging participatory and pluralistic approaches, and identify actionable pathways for future work in AI design and governance."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.1349,review,post_llm,2025,5,"{'ai_likelihood': 0.89501953125, 'text': ""Generative AI and the transformation of Work in Latin America -- Brazil\n\nThis survey explores the impact perceived by employers and employees of GenAI in their work activities in Brazil. Generative AI (GenAI) is gradually transforming Brazil workforce, particularly in micro and small businesses, though its adoption remains uneven. This survey examines the perceptions of employers and employees across five sectors: Sales, Customer Service, Graphic Design or Photography, Journalism or Content Production, and Software Development or Coding. The results are analyzed in light of six key dimensions of workforce impact. The findings reveal a mix of optimism, apprehension, and untapped potential in the integration of AI tools. This study serves as a foundation for developing inclusive strategies that maximize AI's benefits while safeguarding workers' rights. The IIA-LNCC supports open research and remains committed to shaping a future where technology and human potential progress together."", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0074920654296875, 'GPT4': 0.10260009765625, 'CLAUDE': 0.052093505859375, 'GOOGLE': 0.7763671875, 'OPENAI_O_SERIES': 0.0054168701171875, 'DEEPSEEK': 0.01013946533203125, 'GROK': 0.0003044605255126953, 'NOVA': 0.0012273788452148438, 'OTHER': 0.00936126708984375, 'HUMAN': 0.035186767578125}}"
2505.01879,review,post_llm,2025,5,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""What to Do When Privacy Is Gone\n\n  Today's ethics of privacy is largely dedicated to defending personal\ninformation from big data technologies. This essay goes in the other direction.\nIt considers the struggle to be lost, and explores two strategies for living\nafter privacy is gone. First, total exposure embraces privacy's decline, and\nthen contributes to the process with transparency. All personal information is\nshared without reservation. The resulting ethics is explored through a big data\nversion of Robert Nozick's Experience Machine thought experiment. Second,\ntransient existence responds to privacy's loss by ceaselessly generating new\npersonal identities, which translates into constantly producing temporarily\nunviolated private information. The ethics is explored through Gilles Deleuze's\nmetaphysics of difference applied in linguistic terms to the formation of the\nself. Comparing the exposure and transience alternatives leads to the\nconclusion that today's big data reality splits the traditional ethical link\nbetween authenticity and freedom. Exposure provides authenticity, but negates\nhuman freedom. Transience provides freedom, but disdains authenticity.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.15067,review,post_llm,2025,5,"{'ai_likelihood': 2.5828679402669272e-06, 'text': 'Lawful but Awful: Evolving Legislative Responses to Address Online Misinformation, Disinformation, and Mal-Information in the Age of Generative AI\n\n""Fake news"" is an old problem. In recent years, however, increasing usage of social media as a source of information, the spread of unverified medical advice during the Covid-19 pandemic, and the rise of generative artificial intelligence have seen a rush of legislative proposals seeking to minimize or mitigate the impact of false information spread online. Drawing on a novel dataset of statutes and other instruments, this article analyses changing perceptions about the potential harms caused by misinformation, disinformation, and ""mal-information"". The turn to legislation began in countries that were less free, in terms of civil liberties, and poorer, as measured by GDP per capita. Internet penetration does not seem to have been a driving factor. The focus of such laws is most frequently on national security broadly construed, though 2020 saw a spike in laws addressing public health. Unsurprisingly, governments with fewer legal constraints on government action have generally adopted more robust positions in dealing with false information. Despite early reservations, however, growth in such laws is now steepest in Western states. Though there are diverse views on the appropriate response to false information online, the need for legislation of some kind appears now to be global. The question is no longer whether to regulate ""lawful but awful"" speech online, but how.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.21064,regular,post_llm,2025,5,"{'ai_likelihood': 0.99755859375, 'text': ""Cybroc: Cyborgizing Broccoli for Longevity\n\nCybroc is a series of kinetic art installations exploring the recent proliferating populist longevity activism through the satirical cyborgization of broccoli. The artwork augments the symbol of health food-broccoli-with prosthetic limbs to perform so-called longevity-enhancing exercises such as cold plunges, treadmill running, brachiation (arm-swinging), sled pushing, etc.-all simulations of primal human survival tasks reframed as modern fitness routines. Despite its mechanical augmentations, the broccoli's inevitable decay and rotting after exhibiting high-intensity performances prompts reflection on the limits of biological enhancement and the ethics of human enhancement beyond natural capabilities, particularly transhumanist ideals. By juxtaposing a symbolic healthy vegetable with cutting-edge concepts of human enhancement, Cybroc challenges viewers to consider the intersection of nature, technology, and the human quest for extended lifespan in our transhuman era."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1205673217773438e-05, 'GPT4': 0.0001373291015625, 'CLAUDE': 0.99951171875, 'GOOGLE': 9.566545486450195e-05, 'OPENAI_O_SERIES': 7.808208465576172e-05, 'DEEPSEEK': 2.7477741241455078e-05, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.7881393432617188e-07, 'OTHER': 1.2516975402832031e-06, 'HUMAN': 0.00030231475830078125}}"
2505.04291,review,post_llm,2025,5,"{'ai_likelihood': 0.00018490685356987847, 'text': ""From Incidents to Insights: Patterns of Responsibility following AI\n  Harms\n\n  The AI Incident Database was inspired by aviation safety databases, which\nenable collective learning from failures to prevent future incidents. The\ndatabase documents hundreds of AI failures, collected from the news and media.\nHowever, criticism highlights that the AIID's reliance on media reporting\nlimits its utility for learning about implementation failures. In this paper,\nwe accept that the AIID falls short in its original mission, but argue that by\nlooking beyond technically-focused learning, the dataset can provide new,\nhighly valuable insights: specifically, opportunities to learn about patterns\nbetween developers, deployers, victims, wider society, and law-makers that\nemerge after AI failures. Through a three-tier mixed-methods analysis of 962\nincidents and 4,743 related reports from the AIID, we examine patterns across\nincidents, focusing on cases with public responses tagged in the database. We\nidentify 'typical' incidents found in the AIID, from Tesla crashes to deepfake\nscams.\n  Focusing on this interplay between relevant parties, we uncover patterns in\naccountability and social expectations of responsibility. We find that the\npresence of identifiable responsible parties does not necessarily lead to\nincreased accountability. The likelihood of a response and what it amounts to\ndepends highly on context, including who built the technology, who was harmed,\nand to what extent. Controversy-rich incidents provide valuable data about\nsocietal reactions, including insights into social expectations. Equally\ninformative are cases where controversy is notably absent. This work shows that\nthe AIID's value lies not just in preventing technical failures, but in\ndocumenting patterns of harms and of institutional response and social learning\naround AI incidents. These patterns offer crucial insights for understanding\nhow society adapts to and governs emerging AI technologies.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.17128,regular,post_llm,2025,5,"{'ai_likelihood': 0.14851888020833334, 'text': 'Predicting At-Risk Programming Students in Small Imbalanced Datasets using Synthetic Data\n\nThis study is part of a larger project focused on measuring, understanding, and improving student engagement in programming education. We investigate whether synthetic data generation can help identify at-risk students earlier in a small, imbalanced dataset from an introductory programming module. The analysis used anonymised records from 379 students, with 15\\% marked as failing, and applied several machine learning algorithms. The first experiments showed poor recall for the failing group. However, using synthetic data generation methods led to a significant improvement in performance. Our results suggest that machine learning can help identify at-risk students early in programming courses when combined with synthetic data. This research lays the groundwork for validating and using these models with live student cohorts in the future, to allow for timely and effective interventions that can improve student outcomes. It also includes feature importance analysis to refine formative tasks. Overall, this study contributes to developing practical workflows that help detect disengagement early and improve student success in programming education.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.12892,review,post_llm,2025,5,"{'ai_likelihood': 0.95361328125, 'text': '""I will never pay for this"" Perception of fairness and factors affecting behaviour on \'pay-or-ok\' models\n\nThe rise of cookie paywalls (\'pay-or-ok\' models) has prompted growing debates around the right to privacy and data protection, monetisation, and the legitimacy of user consent. Despite their increasing use across sectors, limited research has explored how users perceive these models or what shapes their decisions to either consent to tracking or pay. To address this gap, we conducted four focus groups (n= 14) to examine users\' perceptions of cookie paywalls, their judgments of fairness, and the conditions under which they might consider paying, alongside a legal analysis within the EU data protection legal framework.\n  Participants primarily viewed cookie paywalls as profit-driven, with fairness perceptions varying depending on factors such as the presence of a third option beyond consent or payment, transparency of data practices, and the authenticity or exclusivity of the paid content. Participants voiced expectations for greater transparency, meaningful control over data collection, and less coercive alternatives, such as contextual advertising or ""reject all"" buttons. Although some conditions, including trusted providers, exclusive content, and reasonable pricing, could make participants consider paying, most expressed reluctance or unwillingness to do so.\n  Crucially, our findings raise concerns about economic exclusion, where privacy and data protection might end up becoming a privilege rather than fundamental rights. Consent given under financial pressure may not meet the standard of being freely given, as required by the GDPR. To address these concerns, we recommend user-centred approaches that enhance transparency, reduce coercion, ensure the value of paid content, and explore inclusive alternatives. These measures are essential for supporting fairness, meaningful choice, and user autonomy in consent-driven digital environments.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00174713134765625, 'GPT4': 0.76171875, 'CLAUDE': 0.032806396484375, 'GOOGLE': 0.182373046875, 'OPENAI_O_SERIES': 0.005035400390625, 'DEEPSEEK': 0.004741668701171875, 'GROK': 2.1457672119140625e-06, 'NOVA': 1.1324882507324219e-06, 'OTHER': 7.843971252441406e-05, 'HUMAN': 0.0112457275390625}}"
2505.16413,regular,post_llm,2025,5,"{'ai_likelihood': 0.00024994214375813806, 'text': ""TAPAS: A Pattern-Based Approach to Assessing Government Transparency\n\nGovernment transparency, widely recognized as a cornerstone of open government, depends on robust information management practices. Yet effective assessment of information management remains challenging, as existing methods fail to consider the actual working behavior of civil servants and are resource-intensive. Using a design science research approach, we present the Transparency Anti-Pattern Assessment System (TAPAS) -- a novel, data-driven methodology designed to evaluate government transparency through the identification of behavioral patterns that impede transparency. We demonstrate TAPAS's real-world applicability at a Dutch ministry, analyzing their electronic document management system data from the past two decades. We identify eight transparency anti-patterns grouped into four categories: Incomplete Documentation, Limited Accessibility, Unclear Information, and Delayed Documentation. We show that TAPAS enables continuous monitoring and provides actionable insights without requiring significant resource investments."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.10069,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Top-Down vs. Bottom-Up Approaches for Automatic Educational Knowledge Graph Construction in CourseMapper\n\nThe automatic construction of Educational Knowledge Graphs (EduKGs) is crucial for modeling domain knowledge in digital learning environments, particularly in Massive Open Online Courses (MOOCs). However, identifying the most effective approach for constructing accurate EduKGs remains a challenge. This study compares Top-down and Bottom-up approaches for automatic EduKG construction, evaluating their effectiveness in capturing and structuring knowledge concepts from learning materials in our MOOC platform CourseMapper. Through a user study and expert validation using Simple Random Sampling (SRS), results indicate that the Bottom-up approach outperforms the Top-down approach in accurately identifying and mapping key knowledge concepts. To further enhance EduKG accuracy, we integrate a Human-in-the-Loop approach, allowing course moderators to review and refine the EduKG before publication. This structured comparison provides a scalable framework for improving knowledge representation in MOOCs, ultimately supporting more personalized and adaptive learning experiences.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0057525634765625, 'GPT4': 0.81396484375, 'CLAUDE': 0.1077880859375, 'GOOGLE': 0.05908203125, 'OPENAI_O_SERIES': 0.006984710693359375, 'DEEPSEEK': 0.002437591552734375, 'GROK': 7.963180541992188e-05, 'NOVA': 2.6881694793701172e-05, 'OTHER': 0.0038356781005859375, 'HUMAN': 0.0001475811004638672}}"
2505.23231,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'REDDIX-NET: A Novel Dataset and Benchmark for Moderating Online Explicit Services\n\nThe rise of online platforms has enabled covert illicit activities, including online prostitution, to pose challenges for detection and regulation. In this study, we introduce REDDIX-NET, a novel benchmark dataset specifically designed for moderating online sexual services and going beyond traditional NSFW filters. The dataset is derived from thousands of web-scraped NSFW posts on Reddit and categorizes users into six behavioral classes reflecting different service offerings and user intentions. We evaluate the classification performance of state-of-the-art large language models (GPT-4, LlaMA 3.3-70B-Instruct, Gemini 1.5 Flash, Mistral 8x7B, Qwen 2.5 Turbo, Claude 3.5 Haiku) using advanced quantitative metrics, finding promising results with models like GPT-4 and Gemini 1.5 Flash. Beyond classification, we conduct sentiment and comment analysis, leveraging LLM and PLM-based approaches and metadata extraction to uncover behavioral and temporal patterns. These analyses reveal peak engagement times and distinct user interaction styles across categories. Our findings provide critical insights into AI-driven moderation and enforcement, offering a scalable framework for platforms to combat online prostitution and associated harms.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.759166717529297e-05, 'GPT4': 0.67626953125, 'CLAUDE': 0.039306640625, 'GOOGLE': 0.002101898193359375, 'OPENAI_O_SERIES': 0.0003314018249511719, 'DEEPSEEK': 0.27978515625, 'GROK': 8.940696716308594e-07, 'NOVA': 1.1920928955078125e-06, 'OTHER': 5.346536636352539e-05, 'HUMAN': 0.001911163330078125}}"
2505.02791,review,post_llm,2025,5,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""Scoring the European Citizen in the AI Era\n\n  Social scoring is one of the AI practices banned by the AI Act. This ban is\nexplicitly inspired by China, which in 2014 announced its intention to set up a\nlarge-scale government project - the Social Credit System - aiming to rate\nevery Chinese citizen according to their good behaviour, using digital\ntechnologies and AI. But in Europe, individuals are also scored by public and\nprivate bodies in a variety of contexts, such as assessing creditworthiness,\nmonitoring employee productivity, detecting social fraud or terrorist risks,\nand so on. However, the AI Act does not intend to prohibit these types of\nscoring, as they would qualify as 'high-risk AI systems', which are authorised\nwhile subject to various requirements. One might therefore think that the ban\non social scoring will have no practical effect on the scoring practices\nalready in use in Europe, and that it is merely a vague safeguard in case an\nauthoritarian power is tempted to set up such a system on European territory.\nContrary to this view, this article argues that the ban has been drafted in a\nway that is flexible and therefore likely to make it a useful tool, similar and\ncomplementary to Article 22 of the General Data Protection Regulation, to\nprotect individuals against certain forms of disproportionate use of AI-based\nscoring.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.18422,regular,post_llm,2025,5,"{'ai_likelihood': 0.99755859375, 'text': ""A Task-Driven Human-AI Collaboration: When to Automate, When to Collaborate, When to Challenge\n\nAccording to several empirical investigations, despite enhancing human capabilities, human-AI cooperation frequently falls short of expectations and fails to reach true synergy. We propose a task-driven framework that reverses prevalent approaches by assigning AI roles according to how the task's requirements align with the capabilities of AI technology. Three major AI roles are identified through task analysis across risk and complexity dimensions: autonomous, assistive/collaborative, and adversarial. We show how proper human-AI integration maintains meaningful agency while improving performance by methodically mapping these roles to various task types based on current empirical findings. This framework lays the foundation for practically effective and morally sound human-AI collaboration that unleashes human potential by aligning task attributes to AI capabilities. It also provides structured guidance for context-sensitive automation that complements human strengths rather than replacing human judgment."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.9802322387695312e-06, 'GPT4': 0.0004372596740722656, 'CLAUDE': 0.99853515625, 'GOOGLE': 7.170438766479492e-05, 'OPENAI_O_SERIES': 1.5616416931152344e-05, 'DEEPSEEK': 0.00017762184143066406, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 5.364418029785156e-07, 'HUMAN': 0.0006046295166015625}}"
2505.21112,regular,post_llm,2025,5,"{'ai_likelihood': 0.97314453125, 'text': ""Simulating Ethics: Using LLM Debate Panels to Model Deliberation on Medical Dilemmas\n\nThis paper introduces ADEPT, a system using Large Language Model (LLM) personas to simulate multi-perspective ethical debates. ADEPT assembles panels of 'AI personas', each embodying a distinct ethical framework or stakeholder perspective (like a deontologist, consequentialist, or disability rights advocate), to deliberate on complex moral issues. Its application is demonstrated through a scenario about prioritizing patients for a limited number of ventilators inspired by real-world challenges in allocating scarce medical resources. Two debates, each with six LLM personas, were conducted; they only differed in the moral viewpoints represented: one included a Catholic bioethicist and a care theorist, the other substituted a rule-based Kantian philosopher and a legal adviser. Both panels ultimately favoured the same policy -- a lottery system weighted for clinical need and fairness, crucially avoiding the withdrawal of ventilators for reallocation. However, each panel reached that conclusion through different lines of argument, and their voting coalitions shifted once duty- and rights-based voices were present. Examination of the debate transcripts shows that the altered membership redirected attention toward moral injury, legal risk and public trust, which in turn changed four continuing personas' final positions. The work offers three contributions: (i) a transparent, replicable workflow for running and analysing multi-agent AI debates in bioethics; (ii) evidence that the moral perspectives included in such panels can materially change the outcome even when the factual inputs remain constant; and (iii) an analysis of the implications and future directions for such AI-mediated approaches to ethical deliberation and policy."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.761882781982422e-06, 'GPT4': 0.004131317138671875, 'CLAUDE': 0.0209808349609375, 'GOOGLE': 0.00037860870361328125, 'OPENAI_O_SERIES': 5.364418029785156e-07, 'DEEPSEEK': 0.96484375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.0132789611816406e-06, 'HUMAN': 0.00984954833984375}}"
2505.16274,regular,post_llm,2025,5,"{'ai_likelihood': 0.4334852430555556, 'text': ""Multimodal AI-based visualization of strategic leaders' emotional dynamics: a deep behavioral analysis of Trump's trade war discourse\n\nThis study investigates the emotional rhythms and behavioral mechanisms of dominant political leaders in strategic decision-making. Using the Trump administration's 125 percent tariff hike on China as a case, it adopts a Multimodal Cognitive Behavioral Modeling framework. This includes micro-expression tracking, acoustic intonation analysis, semantic flow modeling, cognitive load simulation, and strategic behavior mapping to construct a full-cycle simulation of emotion, motivation, and output. Results reveal that Trump's decisions are not driven by rational deduction, but emerge from dominance-coherence rhythms. A six-axis National Strategic Tempo Intervention Framework is proposed to support anticipatory policy modeling."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.15223,regular,post_llm,2025,5,"{'ai_likelihood': 0.010172526041666668, 'text': 'Classifying and Tracking International Aid Contribution Towards SDGs\n\nInternational aid is a critical mechanism for promoting economic growth and well-being in developing nations, supporting progress toward the Sustainable Development Goals (SDGs). However, tracking aid contributions remains challenging due to labor-intensive data management, incomplete records, and the heterogeneous nature of aid data. Recognizing the urgency of this challenge, we partnered with government agencies to develop an AI model that complements manual classification and mitigates human bias in subjective interpretation. By integrating SDG-specific semantics and leveraging prior knowledge from language models, our approach enhances classification accuracy and accommodates the diversity of aid projects. When applied to a comprehensive dataset spanning multiple years, our model can reveal hidden trends in the temporal evolution of international development cooperation. Expert interviews further suggest how these insights can empower policymakers with data-driven decision-making tools, ultimately improving aid effectiveness and supporting progress toward SDGs.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.04345,regular,post_llm,2025,5,"{'ai_likelihood': 0.00024822023179796006, 'text': 'Build Agent Advocates, Not Platform Agents\n\nLanguage model agents are poised to mediate how people navigate and act online. If the companies that already dominate internet search, communication, and commerce -- or the firms trying to unseat them -- control these agents, the resulting platform agents will likely deepen surveillance, tighten lock-in, and further entrench incumbents. To resist that trajectory, this position paper argues that we should promote agent advocates: user-controlled agents that safeguard individual autonomy and choice. Doing so demands three coordinated moves: broad public access to both compute and capable AI models that are not platform-owned, open interoperability and safety standards, and market regulation that prevents platforms from foreclosing competition.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.02174,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'AI Governance in the GCC States: A Comparative Analysis of National AI\n  Strategies\n\n  Gulf Cooperation Council (GCC) states increasingly adopt Artificial\nIntelligence (AI) to drive economic diversification and enhance services. This\npaper investigates the evolving AI governance landscape across the six GCC\nnations, the United Arab Emirates, Saudi Arabia, Qatar, Oman, Bahrain, and\nKuwait, through an in-depth document analysis of six National AI Strategies\n(NASs) and related policies published between 2018 and 2024. Drawing on the\nMultiple Streams Framework (MSF) and Multi-stakeholder Governance theory, the\nfindings highlight a ""soft regulation"" approach that emphasizes national\nstrategies and ethical principles rather than binding regulations. While this\napproach fosters rapid innovation, it also raises concerns regarding the\nenforceability of ethical standards, potential ethicswashing, and alignment\nwith global frameworks, particularly the EU AI Act. Common challenges include\ndata limitations, talent shortages, and reconciling AI applications with\ncultural values. Despite these hurdles, GCC governments aspire to leverage AI\nfor robust economic growth, better public services, and regional leadership in\nresponsible AI. The analysis suggests that strengthening legal mechanisms,\nenhancing stakeholder engagement, and aligning policies with local contexts and\ninternational norms will be essential for harnessing AI\'s transformative\npotential in the GCC.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.7550926208496094e-06, 'GPT4': 0.0083465576171875, 'CLAUDE': 0.001125335693359375, 'GOOGLE': 0.004512786865234375, 'OPENAI_O_SERIES': 9.137392044067383e-05, 'DEEPSEEK': 0.98583984375, 'GROK': 5.960464477539063e-08, 'NOVA': 1.1920928955078125e-07, 'OTHER': 4.5299530029296875e-06, 'HUMAN': 2.980232238769531e-07}}"
2505.01643,review,post_llm,2025,5,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Third-party compliance reviews for frontier AI safety frameworks\n\nSafety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult for stakeholders to know if companies are adhering to their frameworks. This paper explores a potential solution: third-party compliance reviews. During a third-party compliance review, an independent external party assesses whether a frontier AI company is complying with its safety framework. First, we discuss the main benefits and challenges of such reviews. On the one hand, they can increase compliance with safety frameworks and provide assurance to internal and external stakeholders. On the other hand, they can create information security risks, impose additional cost burdens, and cause reputational damage, but these challenges can be partially mitigated by drawing on best practices from other industries. Next, we answer practical questions about third-party compliance reviews, namely: (1) Who could conduct the review? (2) What information sources could the reviewer consider? (3) How could compliance with the safety framework be assessed? (4) What information about the review could be disclosed externally? (5) How could the findings guide development and deployment actions? (6) When could the reviews be conducted? For each question, we evaluate a set of plausible options. Finally, we suggest ""minimalist"", ""more ambitious"", and ""comprehensive"" approaches for each question that a frontier AI company could adopt.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.13673,review,post_llm,2025,5,"{'ai_likelihood': 0.018649631076388888, 'text': 'Comparing Apples to Oranges: A Taxonomy for Navigating the Global Landscape of AI Regulation\n\nAI governance has transitioned from soft law-such as national AI strategies and voluntary guidelines-to binding regulation at an unprecedented pace. This evolution has produced a complex legislative landscape: blurred definitions of ""AI regulation"" mislead the public and create a false sense of safety; divergent regulatory frameworks risk fragmenting international cooperation; and uneven access to key information heightens the danger of regulatory capture. Clarifying the scope and substance of AI regulation is vital to uphold democratic rights and align international AI efforts. We present a taxonomy to map the global landscape of AI regulation. Our framework targets essential metrics-technology or application-focused rules, horizontal or sectoral regulatory coverage, ex ante or ex post interventions, maturity of the digital legal landscape, enforcement mechanisms, and level of stakeholder participation-to classify the breadth and depth of AI regulation. We apply this framework to five early movers: the European Union\'s AI Act, the United States\' Executive Order 14110, Canada\'s AI and Data Act, China\'s Interim Measures for Generative AI Services, and Brazil\'s AI Bill 2338/2023. We further offer an interactive visualization that distills these dense legal texts into accessible insights, highlighting both commonalities and differences. By delineating what qualifies as AI regulation and clarifying each jurisdiction\'s approach, our taxonomy reduces legal uncertainty, supports evidence-based policymaking, and lays the groundwork for more inclusive, globally coordinated AI governance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.1489,review,post_llm,2025,5,"{'ai_likelihood': 0.0002808041042751736, 'text': ""Algorithms in the Stacks: Investigating automated, for-profit diversity audits in public libraries\n\nAlgorithmic systems are increasingly being adopted by cultural heritage institutions like libraries. In this study, we investigate U.S. public libraries' adoption of one specific automated tool -- automated collection diversity audits -- which we see as an illuminating case study for broader trends. Typically developed and sold by commercial book distributors, automated diversity audits aim to evaluate how well library collections reflect demographic and thematic diversity. We investigate how these audits function, whether library workers find them useful, and what is at stake when sensitive, normative decisions about representation are outsourced to automated commercial systems. Our analysis draws on an anonymous survey of U.S. public librarians (n=99), interviews with 14 librarians, a sample of purchasing records, and vendor documentation. We find that many library workers view these tools as convenient, time-saving solutions for assessing and diversifying collections under real and increasing constraints. Yet at the same time, the audits often flatten complex identities into standardized categories, fail to reflect local community needs, and further entrench libraries' infrastructural dependence on vendors. We conclude with recommendations for improving collection diversity audits and reflect on the broader implications for public libraries operating at the intersection of AI adoption, escalating anti-DEI backlash, and politically motivated defunding."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.08743,regular,post_llm,2025,5,"{'ai_likelihood': 4.967053731282552e-07, 'text': 'Understanding Housing and Homelessness System Access by Linking Administrative Data\n\nThis paper uses privacy preserving methods to link over 235,000 records in the housing and homelessness system of care (HHSC) of a major North American city. Several machine learning pairwise linkage and two clustering algorithms are evaluated for merging the profiles for latent individuals in the data. Importantly, these methods are evaluated using both traditional machine learning metrics and HHSC system use metrics generated using the linked data. The results demonstrate that privacy preserving linkage methods are an effective and practical method for understanding how a single person interacts with multiple agencies across an HHSC. They also show that performance differences between linkage techniques are amplified when evaluated using HHSC domain specific metrics like number of emergency homeless shelter stays, length of time interacting with an HHSC and number of emergency shelters visited per person.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.12248,regular,post_llm,2025,5,"{'ai_likelihood': 0.95361328125, 'text': ""Persuasion and Safety in the Era of Generative AI\n\nAs large language models (LLMs) achieve advanced persuasive capabilities, concerns about their potential risks have grown. The EU AI Act prohibits AI systems that use manipulative or deceptive techniques to undermine informed decision-making, highlighting the need to distinguish between rational persuasion, which engages reason, and manipulation, which exploits cognitive biases. My dissertation addresses the lack of empirical studies in this area by developing a taxonomy of persuasive techniques, creating a human-annotated dataset, and evaluating LLMs' ability to distinguish between these methods. This work contributes to AI safety by providing resources to mitigate the risks of persuasive AI and fostering discussions on ethical persuasion in the age of generative AI."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0093994140625, 'GPT4': 0.0224151611328125, 'CLAUDE': 0.4853515625, 'GOOGLE': 0.11114501953125, 'OPENAI_O_SERIES': 0.0157928466796875, 'DEEPSEEK': 0.03326416015625, 'GROK': 5.9485435485839844e-05, 'NOVA': 0.0009016990661621094, 'OTHER': 0.310302734375, 'HUMAN': 0.011383056640625}}"
2505.02749,regular,post_llm,2025,5,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""How May U.S. Courts Scrutinize Their Recidivism Risk Assessment Tools? Contextualizing AI Fairness Criteria on a Judicial Scrutiny-based Framework\n\nThe AI/HCI and legal communities have developed largely independent conceptualizations of fairness. This conceptual difference hinders the potential incorporation of technical fairness criteria (e.g., procedural, group, and individual fairness) into sustainable policies and designs, particularly for high-stakes applications like recidivism risk assessment. To foster common ground, we conduct legal research to identify if and how technical AI conceptualizations of fairness surface in primary legal sources. We find that while major technical fairness criteria can be linked to constitutional mandates such as ``Due Process'' and ``Equal Protection'' thanks to judicial interpretation, several challenges arise when operationalizing them into concrete statutes/regulations. These policies often adopt procedural and group fairness but ignore the major technical criterion of individual fairness. Regarding procedural fairness, judicial ``scrutiny'' categories are relevant but may not fully capture how courts scrutinize the use of demographic features in potentially discriminatory government tools like RRA. Furthermore, some policies contradict each other on whether to apply procedural fairness to certain demographic features. Thus, we propose a new framework, integrating U.S. demographics-related legal scrutiny concepts and technical fairness criteria, and contextualize it in three other major AI-adopting jurisdictions (EU, China, and India)."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.10598,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Enhancing Collaboration Through Google Workspace: Assessing and Strengthening Current Practices\n\nThis study investigates the effectiveness of Google Workspace in fostering collaboration within academic settings, specifically at the University of Makati. The aim is to evaluate its role in enhancing blended learning practices and identify areas for improvement among faculty, staff, and students. A survey was conducted with 50 participants, including academic staff, faculty, and students at the University of Makati who regularly use Google Workspace for academic and collaborative activities. Participants were selected through purposive sampling to ensure familiarity with the platform. The study employed a quantitative research design using structured surveys to assess user experiences with key features such as real-time document editing, communication tools, etc. The study found that Google Workspace and rated as ""Very Effective"" (mean score of 4.61) in promoting teamwork. Key advantages included improved collaboration, enhanced communication, and efficient management of group projects. However, several challenges were also noted, including low user adoption rates, limited Google Drive storage capacity, the need for better technical support, and limited offline functionality. Google Workspace significantly supports academic collaboration in the normal practices within the University of Makati, however, it faces challenges that impact its overall effectiveness. Addressing these issues could improve user experience and platform efficiency in educational contexts. It is recommended to enhance user adoption through targeted training and improve offline capabilities. Additionally, providing more advanced technical support could mitigate existing challenges.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004646778106689453, 'GPT4': 0.00655364990234375, 'CLAUDE': 0.0225372314453125, 'GOOGLE': 0.92236328125, 'OPENAI_O_SERIES': 0.04730224609375, 'DEEPSEEK': 0.0002524852752685547, 'GROK': 4.76837158203125e-07, 'NOVA': 6.794929504394531e-06, 'OTHER': 0.00030112266540527344, 'HUMAN': 1.9669532775878906e-06}}"
2505.03601,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""Doing Audits Right? The Role of Sampling and Legal Content Analysis in\n  Systemic Risk Assessments and Independent Audits in the Digital Services Act\n\n  A central requirement of the European Union's Digital Services Act (DSA) is\nthat online platforms undergo internal and external audits. A key component of\nthese audits is the assessment of systemic risks, including the dissemination\nof illegal content, threats to fundamental rights, impacts on democratic\nprocesses, and gender-based violence. The DSA Delegated Regulation outlines how\nsuch audits should be conducted, setting expectations for both platforms and\nauditors. This article evaluates the strengths and limitations of different\nqualitative and quantitative methods for auditing these systemic risks and\nproposes a mixed-method approach for DSA compliance. We argue that content\nsampling, combined with legal and empirical analysis, offers a viable method\nfor risk-specific audits. First, we examine relevant legal provisions on sample\nselection for audit purposes. We then assess sampling techniques and methods\nsuitable for detecting systemic risks, focusing on how representativeness can\nbe understood across disciplines. Finally, we review initial systemic risk\nassessment reports submitted by platforms, analyzing their testing and sampling\nmethodologies. By proposing a structured, mixed-method approach tailored to\nspecific risk categories and platform characteristics, this article addresses\nthe challenge of evidence-based audits under the DSA. Our contribution\nemphasizes the need for adaptable, context-sensitive auditing strategies and\nadds to the emerging field of DSA compliance research.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0016698837280273438, 'GPT4': 0.2030029296875, 'CLAUDE': 0.64794921875, 'GOOGLE': 0.0258941650390625, 'OPENAI_O_SERIES': 0.006072998046875, 'DEEPSEEK': 0.11224365234375, 'GROK': 4.0531158447265625e-06, 'NOVA': 1.0311603546142578e-05, 'OTHER': 0.00010102987289428711, 'HUMAN': 0.00318145751953125}}"
2505.22526,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""AI instructional agent improves student's perceived learner control and learning outcome: empirical evidence from a randomized controlled trial\n\nThis study examines the impact of an AI instructional agent on students' perceived learner control and academic performance in a medium demanding course with lecturing as the main teaching strategy. Based on a randomized controlled trial, three instructional conditions were compared: a traditional human teacher, a self-paced MOOC with chatbot support, and an AI instructional agent capable of delivering lectures and responding to questions in real time. Students in the AI instructional agent group reported significantly higher levels of perceived learner control compared to the other groups. They also completed the learning task more efficiently and engaged in more frequent interactions with the instructional system. Regression analyzes showed that perceived learner control positively predicted post-test performance, with behavioral indicators such as reduced learning time and higher interaction frequency supporting this relationship. These findings suggest that AI instructional agents, when designed to support personalized pace and responsive interaction, can enhance both students' learning experience and learning outcomes."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.004238128662109375, 'GPT4': 0.0634765625, 'CLAUDE': 0.7197265625, 'GOOGLE': 0.20458984375, 'OPENAI_O_SERIES': 0.004673004150390625, 'DEEPSEEK': 0.001739501953125, 'GROK': 0.00015807151794433594, 'NOVA': 3.3020973205566406e-05, 'OTHER': 0.0011425018310546875, 'HUMAN': 0.0002486705780029297}}"
2505.07118,regular,post_llm,2025,5,"{'ai_likelihood': 0.08802625868055557, 'text': ""KOKKAI DOC: An LLM-driven framework for scaling parliamentary representatives\n\nThis paper introduces an LLM-driven framework designed to accurately scale the political issue stances of parliamentary representatives. By leveraging advanced natural language processing techniques and large language models, the proposed methodology refines and enhances previous approaches by addressing key challenges such as noisy speech data, manual bias in selecting political axes, and the lack of dynamic, diachronic analysis. The framework incorporates three major innovations: (1) de-noising parliamentary speeches via summarization to produce cleaner, more consistent opinion embeddings; (2) automatic extraction of axes of political controversy from legislators' speech summaries; and (3) a diachronic analysis that tracks the evolution of party positions over time.\n  We conduct quantitative and qualitative evaluations to verify our methodology. Quantitative evaluations demonstrate high correlation with expert predictions across various political topics, while qualitative analyses reveal meaningful associations between language patterns and political ideologies. This research aims to have an impact beyond the field of academia by making the results accessible by the public on teh web application: kokkaidoc.com. We are hoping that through our application, Japanese voters can gain a data-driven insight into the political landscape which aids them to make more nuanced voting decisions.\n  Overall, this work contributes to the growing body of research that applies LLMs in political science, offering a flexible and reliable framework for scaling political positions from parliamentary speeches. But also explores the practical applications of the research in the real world to have real world impact."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.03593,regular,post_llm,2025,5,"{'ai_likelihood': 5.662441253662109e-06, 'text': 'A Unifying Bias-aware Multidisciplinary Framework for Investigating\n  Socio-Technical Issues\n\n  This paper aims to bring together the disciplines of social science (SS) and\ncomputer science (CS) in the design and implementation of a novel\nmultidisciplinary framework for systematic, transparent, ethically-informed,\nand bias-aware investigation of socio-technical issues. For this, various\nanalysis approaches from social science and machine learning (ML) were applied\nin a structured sequence to arrive at an original methodology of identifying\nand quantifying objects of inquiry. A core feature of this framework is that it\nhighlights where bias occurs and suggests possible steps to mitigate it. This\nis to improve the robustness, reliability, and explainability of the framework\nand its results. Such an approach also ensures that the investigation of\nsocio-technical issues is transparent about its own limitations and potential\nsources of bias. To test our framework, we utilised it in the multidisciplinary\ninvestigation of the online harms encountered by minoritised ethnic (ME)\ncommunities when accessing and using digitalised social housing services in the\nUK. We draw our findings from 100 interviews with ME individuals in four cities\nacross the UK to understand ME vulnerabilities when accessing and using\ndigitalised social housing services. In our framework, a sub-sample of\ninterviews focusing on ME individuals residing in social housing units were\ninductively coded. This resulted in the identification of the topics of\ndiscrimination, digital poverty, lack of digital literacy, and lack of English\nproficiency as key vulnerabilities of ME communities. Further ML techniques\nsuch as Topic Modelling and Sentiment Analysis were used within our framework\nwhere we found that Black African communities are more likely to experience\nthese vulnerabilities in the access, use and outcome of digitalised social\nhousing services.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.11367,review,post_llm,2025,5,"{'ai_likelihood': 0.004980299207899306, 'text': 'The Effects of Moral Framing on Online Fundraising Outcomes: Evidence from GoFundMe Campaigns\n\nThis study examines the impact of moral framing on fundraising outcomes, including both monetary and social support, by analyzing a dataset of 14,088 campaigns posted on GoFundMe. We focused on three moral frames: care, fairness, and (ingroup) loyalty, and measured their presence in campaign appeals. Our results show that campaigns in the Emergency category are most influenced by moral framing. Generally, negatively framing appeals by emphasizing harm and unfairness effectively attracts more donations and comments from supporters. However, this approach can have a downside, as it may lead to a decrease in the average donation amount per donor. Additionally, we found that loyalty framing was positively associated with receiving more donations and messages across all fundraising categories. This research extends existing literature on framing and communication strategies related to fundraising and their impact. We also propose practical implications for designing features of online fundraising platforms to better support both fundraisers and supporters.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.13143,regular,post_llm,2025,5,"{'ai_likelihood': 0.00019152959187825522, 'text': ""Auditing Meta-Cognitive Hallucinations in Reasoning Large Language Models\n\nThe development of Reasoning Large Language Models (RLLMs) has significantly improved multi-step reasoning capabilities, but it has also made hallucination problems more frequent and harder to eliminate. While existing approaches mitigate hallucinations through external knowledge integration, model parameter analysis, or self-verification, they often fail to capture how hallucinations emerge and evolve across the reasoning chain. In this work, we study the causality of hallucinations under constrained knowledge domains by auditing the Chain-of-Thought (CoT) trajectory and assessing the model's cognitive confidence in potentially erroneous or biased claims. Our analysis reveals that in long-CoT settings, RLLMs can iteratively reinforce biases and errors through flawed reflective reasoning, eventually leading to hallucinated reasoning paths. Surprisingly, even direct interventions at the origin of hallucinations often fail to reverse their effects, as reasoning chains exhibit 'chain disloyalty' -- a resistance to correction and a tendency to preserve flawed logic. Furthermore, we show that existing hallucination detection methods are less reliable and interpretable than previously assumed in complex reasoning scenarios. Unlike methods such as circuit tracing that require access to model internals, our black-box auditing approach supports interpretable long-chain hallucination attribution, offering better generalizability and practical utility. Our code is available at: https://github.com/Winnie-Lian/AHa_Meta_Cognitive"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.23262,regular,post_llm,2025,5,"{'ai_likelihood': 5.099508497450087e-06, 'text': 'Applying Large Language Models to Travel Satisfaction Analysis\n\nAs a specific domain of subjective well-being, travel satisfaction has recently attracted much research attention. Previous studies primarily relied on statistical models and, more recently, machine learning models to explore its determinants. Both approaches,however, depend on sufficiently large sample sizes and appropriate statistical assumptions. The emergence of Large Language Models (LLMs) offers a new modeling approach that can address these limitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in contextual understanding and generalization, significantly reducing their dependence on task-specific data and stringent statistical assumptions. The main challenge in applying LLMs lies in the behavioral misalignment between LLMs and humans. Using household survey data collected in Shanghai, this study identifies the existence and source of misalignment, and applies a few-shot learning method to address the misalignment issue. We find that the zero-shot LLM exhibits behavioral misalignment, leading to low prediction accuracy. With just a few samples, few-shot learning can align LLMs and enable them to outperform baseline models. Discrepancies in variable importance among machine learning model, zero-shot LLM, and few-shot LLM reveal that the misalignment arises from the gap between the general knowledge embedded in pre-trained LLMs and the specific, unique characteristics of the dataset. On these bases, we propose an LLM-based modeling approach that can be applied to model travel behavior with small sample sizes. This study highlights the potential of LLMs for modeling not only travel satisfaction but also broader aspects of travel behavior.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.18938,regular,post_llm,2025,5,"{'ai_likelihood': 4.3047799004448785e-07, 'text': 'Beyond Replacement or Augmentation: How Creative Workers Reconfigure Division of Labor with Generative AI\n\nThe introduction of generative AI tools such as ChatGPT into creative workplaces has sparked highly visible, but binary worker replacement and augmentation debates. This study reframes this argument by examining how creative professionals re-specify a division of labor with these tools. Through 17 ethnomethodologically informed interviews with international creative agency workers we demonstrate how roles are assigned to generative AI tools, how their contributions are modified and remediated, and how workers practically manage their outputs to reflect assumptions of internal and external stakeholders. This paper makes 3 unique contributions to CSCW: (1) we conceptualize generative AI prompting as a type of workplace situated, reflexive delegation, (2) we demonstrate that workers must continuously configure and repair AI role boundaries to maintain workplace intelligibility and accountability; and (3) we introduce the notion of interpretive templatized trust, where workers devise strategies to adapt automated generative templates for their setting, and reinforce stakeholder trust. This contribution has implications for organizing productive human-AI work in creative and stakeholder centric environments.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.07468,review,post_llm,2025,5,"{'ai_likelihood': 0.2703179253472222, 'text': ""Promising Topics for U.S.-China Dialogues on AI Risks and Governance\n\nCooperation between the United States and China, the world's leading artificial intelligence (AI) powers, is crucial for effective global AI governance and responsible AI development. Although geopolitical tensions have emphasized areas of conflict, in this work, we identify potential common ground for productive dialogue by conducting a systematic analysis of more than 40 primary AI policy and corporate governance documents from both nations. Specifically, using an adapted version of the AI Governance and Regulatory Archive (AGORA) - a comprehensive repository of global AI governance documents - we analyze these materials in their original languages to identify areas of convergence in (1) sociotechnical risk perception and (2) governance approaches. We find strong and moderate overlap in several areas such as on concerns about algorithmic transparency, system reliability, agreement on the importance of inclusive multi-stakeholder engagement, and AI's role in enhancing safety. These findings suggest that despite strategic competition, there exist concrete opportunities for bilateral U.S.-China cooperation in the development of responsible AI. Thus, we present recommendations for furthering diplomatic dialogues that can facilitate such cooperation. Our analysis contributes to understanding how different international governance frameworks might be harmonized to promote global responsible AI development."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.00496,review,post_llm,2025,5,"{'ai_likelihood': 2.615981631808811e-06, 'text': 'Out of the Loop Again: How Dangerous is Weaponizing Automated Nuclear\n  Systems?\n\n  Are nuclear weapons useful for coercion, and, if so, what factors increase\nthe credibility and effectiveness of nuclear threats? While prominent scholars\nlike Thomas Schelling argue that nuclear brinkmanship, or the manipulation of\nnuclear risk, can effectively coerce adversaries, others contend nuclear\nweapons are not effective tools of coercion, especially coercion designed to\nachieve offensive and revisionist objectives. Simultaneously, there is broad\ndebate about the incorporation of artificial intelligence (AI) into military\nsystems, especially nuclear command and control. We develop a theoretical\nargument that explicit nuclear threats implemented with automated nuclear\nlaunch systems are potentially more credible compared to ambiguous nuclear\nthreats or explicit nuclear threats implemented via non-automated means. By\nreducing human control over nuclear use, leaders can more effectively tie their\nhands and thus signal resolve. While automated nuclear weapons launch systems\nmay seem like something out of science fiction, the Soviet Union deployed such\na system during the Cold War and the technology necessary to automate the use\nof force has developed considerably in recent years due to advances in AI.\nPreregistered survey experiments on an elite sample of United Kingdom Members\nof Parliament and two public samples of UK citizens provide support for these\nexpectations, showing that, in a limited set of circumstances, nuclear threats\nbacked by AI integration have credibility advantages, no matter how dangerous\nthey may be. Our findings contribute to the literatures on coercive bargaining,\nweapons of mass destruction, and emerging technology.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.02938,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""Urban Forms Across Continents: A Data-Driven Comparison of Lausanne and\n  Philadelphia\n\n  Understanding urban form is crucial for sustainable urban planning and\nenhancing quality of life. This study presents a data-driven framework to\nsystematically identify and compare urban typologies across geographically and\nculturally distinct cities. Using open-source geospatial data from\nOpenStreetMap, we extracted multidimensional features related to topography,\nmultimodality, green spaces, and points of interest for the cities of Lausanne,\nSwitzerland, and Philadelphia, USA. A grid-based approach was used to divide\neach city into Basic Spatial Units (BSU), and Gaussian Mixture Models (GMM)\nwere applied to cluster BSUs based on their urban characteristics. The results\nreveal coherent and interpretable urban typologies within each city, with some\ncluster types emerging across both cities despite their differences in scale,\ndensity, and cultural context. Comparative analysis showed that adapting the\ngrid size to each city's morphology improves the detection of shared\ntypologies. Simplified clustering based solely on network degree centrality\nfurther demonstrated that meaningful structural patterns can be captured even\nwith minimal feature sets. Our findings suggest the presence of functionally\nconvergent urban forms across continents and highlight the importance of\nspatial scale in cross-city comparisons. The framework offers a scalable and\ntransferable approach for urban analysis, providing valuable insights for\nplanners and policymakers aiming to enhance walkability, accessibility, and\nwell-being. Limitations related to data completeness and feature selection are\ndiscussed, and directions for future work -- including the integration of\nadditional data sources and human-centered validation -- are proposed.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.836652755737305e-05, 'GPT4': 0.8427734375, 'CLAUDE': 0.02630615234375, 'GOOGLE': 0.0025691986083984375, 'OPENAI_O_SERIES': 0.054107666015625, 'DEEPSEEK': 0.07391357421875, 'GROK': 1.7881393432617188e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 6.67572021484375e-06, 'HUMAN': 0.00014698505401611328}}"
2505.14215,regular,post_llm,2025,5,"{'ai_likelihood': 0.16886393229166669, 'text': 'Information Retrieval Induced Safety Degradation in AI Agents\n\nDespite the growing integration of retrieval-enabled AI agents into society, their safety and ethical behavior remain inadequately understood. In particular, the integration of LLMs and AI agents with external information sources and real-world environments raises critical questions about how they engage with and are influenced by these external data sources and interactive contexts. This study investigates how expanding retrieval access -- from no external sources to Wikipedia-based retrieval and open web search -- affects model reliability, bias propagation, and harmful content generation. Through extensive benchmarking of censored and uncensored LLMs and AI agents, our findings reveal a consistent degradation in refusal rates, bias sensitivity, and harmfulness safeguards as models gain broader access to external sources, culminating in a phenomenon we term safety degradation. Notably, retrieval-enabled agents built on aligned LLMs often behave more unsafely than uncensored models without retrieval. This effect persists even under strong retrieval accuracy and prompt-based mitigation, suggesting that the mere presence of retrieved content reshapes model behavior in structurally unsafe ways. These findings underscore the need for robust mitigation strategies to ensure fairness and reliability in retrieval-enabled and increasingly autonomous AI systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.22073,review,post_llm,2025,5,"{'ai_likelihood': 1.1920928955078125e-06, 'text': 'A Closer Look at the Existing Risks of Generative AI: Mapping the Who, What, and How of Real-World Incidents\n\nDue to its general-purpose nature, Generative AI is applied in an ever-growing set of domains and tasks, leading to an expanding set of risks of harm impacting people, communities, society, and the environment. These risks may arise due to failures during the design and development of the technology, as well as during its release, deployment, or downstream usages and appropriations of its outputs. In this paper, building on prior taxonomies of AI risks, harms, and failures, we construct a taxonomy specifically for Generative AI failures and map them to the harms they precipitate. Through a systematic analysis of 499 publicly reported incidents, we describe what harms are reported, how they arose, and who they impact. We report the prevalence of each type of harm, underlying failure mode, and harmed stakeholder, as well as their common co-occurrences. We find that most reported incidents are caused by use-related issues but bring harm to parties beyond the end user(s) of the Generative AI system at fault, and that the landscape of Generative AI harms is distinct from that of traditional AI. Our work offers actionable insights to policymakers, developers, and Generative AI users. In particular, we call for the prioritization of non-technical risk and harm mitigation strategies, including public disclosures and education and careful regulatory stances.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.18779,review,post_llm,2025,5,"{'ai_likelihood': 0.6826171874999999, 'text': ""Evaluating Intra-firm LLM Alignment Strategies in Business Contexts\n\nInstruction-tuned Large Language Models (LLMs) are increasingly deployed as AI Assistants in firms for support in cognitive tasks. These AI assistants carry embedded perspectives which influence factors across the firm including decision-making, collaboration, and organizational culture. This paper argues that firms must align the perspectives of these AI Assistants intentionally with their objectives and values, framing alignment as a strategic and ethical imperative crucial for maintaining control over firm culture and intra-firm moral norms. The paper highlights how AI perspectives arise from biases in training data and the fine-tuning objectives of developers, and discusses their impact and ethical significance, foregrounding ethical concerns like automation bias and reduced critical thinking. Drawing on normative business ethics, particularly non-reductionist views of professional relationships, three distinct alignment strategies are proposed: supportive (reinforcing the firm's mission), adversarial (stress-testing ideas), and diverse (broadening moral horizons by incorporating multiple stakeholder views). The ethical trade-offs of each strategy and their implications for manager-employee and employee-employee relationships are analyzed, alongside the potential to shape the culture and moral fabric of the firm."", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 9.107589721679688e-05, 'GPT4': 0.048065185546875, 'CLAUDE': 0.1605224609375, 'GOOGLE': 0.0911865234375, 'OPENAI_O_SERIES': 0.001373291015625, 'DEEPSEEK': 0.392822265625, 'GROK': 6.9141387939453125e-06, 'NOVA': 1.4185905456542969e-05, 'OTHER': 5.0902366638183594e-05, 'HUMAN': 0.305908203125}}"
2505.21542,review,post_llm,2025,5,"{'ai_likelihood': 0.99755859375, 'text': 'Toward a Cultural Co-Genesis of AI Ethics\n\nContemporary discussions in AI ethics often treat culture as a source of normative divergence that needs to be accommodated, tolerated, or managed due to its resistance to universal standards. This paper offers an alternative vision through the concept of ""Cultural Co-Genesis of AI Ethics."" Rather than viewing culture as a boundary or container of isolated moral systems, we argue that it is a generative space for ethical co-production. In this framework, ethical values emerge through intercultural engagement, dialogical encounters, mutual recognition, and shared moral inquiry.\n  This approach resists both universalist imposition and relativistic fragmentation. Cultures are not approached as absolutes to be defended or dissolved, but as co-authors of a dynamic ethical landscape. By grounding AI ethics in Cultural Co-Genesis, we move from managing difference to constructing shared ethical meaning for AI ethics, with culture as a partner, not a problem.\n  We support this framework with two cases: (1) a theoretical analysis of how various cultures interpret the emergence of powerful new species, challenging dominant existential risk narratives, and (2) an empirical study of global AI ethics principles using data from the Linking AI Principles project, which reveals deep ethical convergence despite cultural diversity. We conclude that cross-cultural AI ethics should be seen not as an ethical patchwork, but as a mosaic in progress, woven from the normative insights that emerge between cultures.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.781122207641602e-05, 'GPT4': 0.005840301513671875, 'CLAUDE': 0.974609375, 'GOOGLE': 8.594989776611328e-05, 'OPENAI_O_SERIES': 7.450580596923828e-06, 'DEEPSEEK': 0.0176849365234375, 'GROK': 1.0728836059570312e-06, 'NOVA': 1.1920928955078125e-07, 'OTHER': 6.020069122314453e-06, 'HUMAN': 0.0015249252319335938}}"
2505.08127,regular,post_llm,2025,5,"{'ai_likelihood': 6.622738308376736e-07, 'text': '""You Cannot Sound Like GPT"": Signs of language discrimination and resistance in computer science publishing\n\nLLMs have been celebrated for their potential to help multilingual scientists publish their research. Rather than interpret LLMs as a solution, we hypothesize their adoption can be an indicator of existing linguistic exclusion in scientific writing. Using the case study of ICLR, an influential, international computer science conference, we examine how peer reviewers critique writing clarity. Analyzing almost 80,000 peer reviews, we find significant bias against authors associated with institutions in countries where English is less widely spoken. We see only a muted shift in the expression of this bias after the introduction of ChatGPT in late 2022. To investigate this unexpectedly minor change, we conduct interviews with 14 conference participants from across five continents. Peer reviewers describe associating certain features of writing with people of certain language backgrounds, and such groups in turn with the quality of scientific work. While ChatGPT masks some signs of language background, reviewers explain that they now use ChatGPT ""style"" and non-linguistic features as indicators of author demographics. Authors, aware of this development, described the ongoing need to remove features which could expose their ""non-native"" status to reviewers. Our findings offer insight into the role of ChatGPT in the reproduction of scholarly language ideologies which conflate producers of ""good English"" with producers of ""good science.""', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.00212,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Concerning the Responsible Use of AI in the US Criminal Justice System\n\nArtificial intelligence (AI) is increasingly being adopted in most industries, and for applications such as note taking and checking grammar, there is typically not a cause for concern. However, when constitutional rights are involved, as in the justice system, transparency is paramount. While AI can assist in areas such as risk assessment and forensic evidence generation, its ""black box"" nature raises significant questions about how decisions are made and whether they can be contested. This paper explores the implications of AI in the justice system, emphasizing the need for transparency in AI decision-making processes to uphold constitutional rights and ensure procedural fairness. The piece advocates for clear explanations of AI\'s data, logic, and limitations, and calls for periodic audits to address bias and maintain accountability in AI systems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.12548828125, 'GPT4': 0.07781982421875, 'CLAUDE': 0.0202178955078125, 'GOOGLE': 0.314208984375, 'OPENAI_O_SERIES': 0.0048980712890625, 'DEEPSEEK': 0.00626373291015625, 'GROK': 0.0017261505126953125, 'NOVA': 0.008575439453125, 'OTHER': 0.440673828125, 'HUMAN': 0.0002636909484863281}}"
2505.20304,review,post_llm,2025,5,"{'ai_likelihood': 0.99267578125, 'text': 'Opacity as a Feature, Not a Flaw: The LoBOX Governance Ethic for Role-Sensitive Explainability and Institutional Trust in AI\n\nThis paper introduces LoBOX (Lack of Belief: Opacity \\& eXplainability) governance ethic structured framework for managing artificial intelligence (AI) opacity when full transparency is infeasible. Rather than treating opacity as a design flaw, LoBOX defines it as a condition that can be ethically governed through role-calibrated explanation and institutional accountability. The framework comprises a three-stage pathway: reduce accidental opacity, bound irreducible opacity, and delegate trust through structured oversight. Integrating the RED/BLUE XAI model for stakeholder-sensitive explanation and aligned with emerging legal instruments such as the EU AI Act, LoBOX offers a scalable and context-aware alternative to transparency-centric approaches. Reframe trust not as a function of complete system explainability, but as an outcome of institutional credibility, structured justification, and stakeholder-responsive accountability. A governance loop cycles back to ensure that LoBOX remains responsive to evolving technological contexts and stakeholder expectations, to ensure the complete opacity governance. We move from transparency ideals to ethical governance, emphasizing that trustworthiness in AI must be institutionally grounded and contextually justified. We also discuss how cultural or institutional trust varies in different contexts. This theoretical framework positions opacity not as a flaw but as a feature that must be actively governed to ensure responsible AI systems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0002789497375488281, 'GPT4': 0.01459503173828125, 'CLAUDE': 0.408447265625, 'GOOGLE': 0.0014352798461914062, 'OPENAI_O_SERIES': 5.4955482482910156e-05, 'DEEPSEEK': 0.490966796875, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.1324882507324219e-06, 'OTHER': 3.409385681152344e-05, 'HUMAN': 0.08416748046875}}"
2506.00218,review,post_llm,2025,5,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Evaluating the Contextual Integrity of False Positives in Algorithmic Travel Surveillance\n\nInternational air travel is highly surveilled. While surveillance is deemed necessary for law enforcement to prevent and detect terrorism and other serious crimes, even the most accurate algorithmic mass surveillance systems produce high numbers of false positives. Despite the potential impact of false positives on the fundamental rights of millions of passengers, algorithmic travel surveillance is lawful in the EU. However, as the system's processing practices and accuracy are kept secret by law, it is unknown to what degree passengers are accepting of the system's interference with their rights to privacy and data protection.\n  We conducted a nationally representative survey of the adult population of Finland (N=1550) to assess their attitudes towards algorithmic mass surveillance in air travel and its potential expansion to other travel contexts. Furthermore, we developed a novel approach for estimating the threshold, beyond which, the number of false positives breaches individuals' perception of contextual integrity. Surprisingly, when faced with a trade-off between privacy and security, even very high false positive counts were perceived as legitimate. This result could be attributed to Finland's high-trust cultural context, but also raises questions about people's capacity to account for privacy harms that happen to other people. We conclude by discussing how legal and ethical approaches to legitimising algorithmic surveillance based on individual rights may overlook the statistical or systemic properties of mass surveillance."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.11463,review,post_llm,2025,5,"{'ai_likelihood': 5.3313043382432725e-06, 'text': 'How AI Generates Creativity from Inauthenticity\n\nArtificial creativity is presented as a counter to Benjamin\'s conception of an ""aura"" in art. Where Benjamin sees authenticity as art\'s critical element, generative artificial intelligence operates as pure inauthenticity. Two elements of purely inauthentic art are described: elusiveness and reflection. Elusiveness is the inability to find an origin-story for the created artwork, and reflection is the ability for perceivers to impose any origin that serves their own purposes. The paper subsequently argues that these elements widen the scope of artistic and creative potential. To illustrate, an example is developed around musical improvisation with an artificial intelligence partner. Finally, a question is raised about whether the inauthentic creativity of AI in art can be extended to human experience and our sense of our identities.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.22401,review,post_llm,2025,5,"{'ai_likelihood': 0.005616082085503473, 'text': 'Facial Age Estimation: A Research Roadmap for Technological and Legal Development and Deployment\n\nAutomated facial age assessment systems operate in either estimation mode - predicting age based on facial traits, or verification mode - confirming a claimed age. These systems support access control to age-restricted goods, services, and content, and can be used in areas like e-commerce, social media, forensics, and refugee support. They may also personalise services in healthcare, finance, and advertising. While improving technological accuracy is essential, deployment must consider legal, ethical, sociological, alongside technological factors. This white paper reviews the current challenges in deploying such systems, outlines the relevant legal and regulatory landscape, and explores future research for fair, robust, and ethical age estimation technologies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.10592,regular,post_llm,2025,5,"{'ai_likelihood': 3.973642985026042e-07, 'text': ""LizAI XT -- Artificial Intelligence-Powered Platform for Healthcare Data Management: A Study on Clinical Data Mega-Structure, Semantic Search, and Insights of Sixteen Diseases\n\nAI-powered LizAI XT ensures real-time and accurate mega-structure of different clinical datasets and largely inaccessible and fragmented sources, into one comprehensive table or any designated forms, based on diseases, clinical variables, and/or other defined parameters. We evaluate the platform's performance on a cluster of 4x NVIDIA A30 GPU 24GB, with 16 diseases -- from deathly cancer and COPD, to conventional ones -- ear infections, including a total 16,000 patients, $\\sim$115,000 medical files, and $\\sim$800 clinical variables. LizAI XT structures data from thousands of files into sets of variables for each disease in one file, achieving >95.0% overall accuracy, while providing exceptional outputs in complicated cases of cancers (99.1%), COPD (98.89%), and asthma (98.12%), without model-overfitting. Data retrieval is sub-second for a variable per patient with a minimal GPU power, which can significantly be improved on more powerful GPUs. LizAI XT uniquely enables fully client-controlled data, complying with strict data security and privacy regulations per region/nation. Our advances complement the existing EMR/EHR, AWS HealthLake, and Google Vertex AI platforms, for healthcare data management and AI development, with large-scalability and expansion at any levels of HMOs, clinics, pharma, and government."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.0005,regular,post_llm,2025,5,"{'ai_likelihood': 0.3955078125, 'text': ""Distinguishing Fact from Fiction: Student Traits, Attitudes, and AI Hallucination Detection in Business School Assessment\n\nAs artificial intelligence (AI) becomes integral to the society, the ability to critically evaluate AI-generated content is increasingly vital. On the context of management education, we examine how academic skills, cognitive traits, and AI scepticism influence students' ability to detect factually incorrect AI-generated responses (hallucinations) in a high-stakes assessment at a UK business school (n=211, Year 2 economics and management students). We find that only 20% successfully identified the hallucination, with strong academic performance, interpretive skills thinking, writing proficiency, and AI scepticism emerging as key predictors. In contrast, rote knowledge application proved less effective, and gender differences in detection ability were observed. Beyond identifying predictors of AI hallucination detection, we tie the theories of epistemic cognition, cognitive bias, and transfer of learning with new empirical evidence by demonstrating how AI literacy could enhance long-term analytical performance in high-stakes settings. We advocate for an innovative and practical framework for AI-integrated assessments, showing that structured feedback mitigates initial disparities in detection ability. These findings provide actionable insights for educators designing AI-aware curricula that foster critical reasoning, epistemic vigilance, and responsible AI engagement in management education. Our study contributes to the broader discussion on the evolution of knowledge evaluation in AI-enhanced learning environments."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.21744,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Computocene: Notes from an Age of Observation\n\nThis piece plays with the idea of the Computocene: an era defined not merely by the ubiquity of computers, but by their deepening role in how we observe, interpret, and make sense of the world. Rather than emphasizing automation, speed, scale, or intelligence, computation is reframed as a mode of attention: filtering information, guiding inquiry, reframing questions, and shaping the very conditions under which knowledge emerges. I invite the reader to consider computers not simply as tools of calculation, but as epistemic instruments that participate in the formation of knowledge. This perspective reconfigures not only scientific practice but the epistemological foundations of understanding itself. The Computocene thus names a shift: from computation as calculation to computation as a form of attunement to the world. It is a speculative essay, offered without technical formality, and intended for a general, curious readership.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00010520219802856445, 'GPT4': 0.05926513671875, 'CLAUDE': 0.12396240234375, 'GOOGLE': 0.0022792816162109375, 'OPENAI_O_SERIES': 1.8775463104248047e-05, 'DEEPSEEK': 0.81298828125, 'GROK': 5.4836273193359375e-06, 'NOVA': 2.682209014892578e-06, 'OTHER': 7.808208465576172e-06, 'HUMAN': 0.0011196136474609375}}"
2505.03513,regular,post_llm,2025,5,"{'ai_likelihood': 0.0006670422024197049, 'text': ""Ruled by the Representation Space: On the University's Embrace of Large\n  Language Models\n\n  This paper explores the implications of universities' rapid adoption of large\nlanguage models (LLMs) for studying, teaching, and research by analyzing the\nlogics underpinning their representation space. It argues that by uncritically\nadopting LLMs, the University surrenders its autonomy to a field of heteronomy,\nthat of generative AI, whose norms are not democratically shaped. Unlike\nearlier forms of rule-based AI, which sought to exclude human judgment and\ninterpretation, generative AI's new normative rationality is explicitly based\non the automation of moral judgment, valuation, and interpretation. By\nintegrating LLMs into pedagogical and research contexts before establishing a\ncritical framework for their use, the University subjects itself to being\ngoverned by contingent, ever-evolving, and domain-non-specific norms that\nstructure the model's virtual representation space and thus everything it\ngenerates.\n"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.18552,regular,post_llm,2025,5,"{'ai_likelihood': 2.2351741790771484e-05, 'text': ""A vision-intelligent framework for mapping the genealogy of vernacular architecture\n\nThe study of vernacular architecture involves recording, ordering, and analysing buildings to probe their physical, social, and cultural explanations. Traditionally, this process is conducted manually and intuitively by researchers. Because human perception is selective and often partial, the resulting interpretations of architecture are invariably broad and loose, often lingering on form descriptions that adhere to a preset linear historical progression or crude regional demarcations. This study proposes a research framework by which intelligent technologies can be systematically assembled to augment researchers' intuition in mapping or uncovering the genealogy of vernacular architecture and its connotative socio-cultural system. We employ this framework to examine the stylistic classification of 1,277 historical shophouses in Singapore's Chinatown. Findings extend beyond the chronological classification established by the Urban Redevelopment Authority of Singapore in the 1980s and 1990s, presenting instead a phylogenetic network to capture the formal evolution of shophouses across time and space. The network organises the shophouse types into nine distinct clusters, revealing concurrent evidences of cultural evolution and diffusion. Moreover, it provides a critical perspective on the multi-ethnic character of Singapore shophouses by suggesting that the distinct cultural influences of different ethnic groups led to a pattern of parallel evolution rather than direct convergence. Our work advances a quantitative genealogy of vernacular architecture, which not only assists in formal description but also reveals the underlying forces of development and change. It also exemplified the potential of collaboration between studies in vernacular architecture and computer science, demonstrating how leveraging the strengths of both fields can yield remarkable insights."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.10044,regular,post_llm,2025,5,"{'ai_likelihood': 1.3245476616753473e-07, 'text': ""To what extent can current French mobile network support agricultural robots?\n\nThe large-scale integration of robots in agriculture offers many promises for enhancing sustainability and increasing food production. The numerous applications of agricultural robots rely on the transmission of data via mobile network, with the amount of data depending on the services offered by the robots and the level of on-board technology. Nevertheless, infrastructure required to deploy these robots, as well as the related energy and environmental consequences, appear overlooked in the digital agriculture literature. In this study, we propose a method for assessing the additional energy consumption and carbon footprint induced by a large-scale deployment of agricultural robots. Our method also estimates the share of agricultural area that can be managed by the deployed robots with respect to network infrastructure constraints. We have applied this method to metropolitan France mobile network and agricultural parcels for five different robotic scenarios. Our results show that increasing the robot's bitrate needs leads to significant additional impacts, which increase at a pace that is poorly captured by classical linear extrapolation methods. When constraining the network to the existing sites, increased bitrate needs also comes with a rapidly decreasing manageable agricultural area."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.05506,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""Silicon Sovereigns: Artificial Intelligence, International Law, and the Tech-Industrial Complex\n\nArtificial intelligence is reshaping science, society, and power. Yet many debates over its likely impact remain fixated on extremes: utopian visions of universal benefit and dystopian fears of existential doom, or an arms race between the U.S. and China, or the Global North and Global South. What's missing is a serious conversation about distribution - who gains, who loses, and who decides. The global AI landscape is increasingly defined not just by geopolitical divides, but by the deepening imbalance between public governance and private control. As governments struggle to keep up, power is consolidating in the hands of a few tech firms whose influence now rivals that of states. If the twentieth century saw the rise of international institutions, the twenty-first may be witnessing their eclipse - replaced not by a new world order, but by a digital oligarchy. This essay explores what that shift means for international law, global equity, and the future of democratic oversight in an age of silicon sovereignty."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00037789344787597656, 'GPT4': 0.00598907470703125, 'CLAUDE': 0.869140625, 'GOOGLE': 0.0011663436889648438, 'OPENAI_O_SERIES': 2.9206275939941406e-05, 'DEEPSEEK': 0.12322998046875, 'GROK': 2.014636993408203e-05, 'NOVA': 2.205371856689453e-06, 'OTHER': 0.0001329183578491211, 'HUMAN': 5.3942203521728516e-05}}"
2505.11465,review,post_llm,2025,5,"{'ai_likelihood': 6.821420457628038e-06, 'text': ""The Dilemma Between Euphoria and Freedom in Recommendation Algorithms\n\nToday's AI recommendation algorithms produce a human dilemma between euphoria and freedom. To elaborate, four ways that recommenders reshape experience are delineated. First, the human experience of convenience is tuned to euphoric perfection. Second, a kind of personal authenticity becomes capturable with algorithms and data. Third, a conception of human freedom emerges, one that promotes unfamiliar interests for users instead of satisfying those that already exist. Finally, a new human dilemma is posed between two types of personal identity. On one side, there are recommendation algorithms that locate a user's core preferences, and then reinforce that identity with options designed to resemble those that have already proved satisfying. The result is an algorithmic production of euphoria and authenticity. On the other side, there are recommenders that provoke unfamiliar interests and curiosities. These proposals deny the existence of an authentic self and instead promote new preferences and experiences. The result is a human freedom of new personal identity."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.0115,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Methodological Foundations for AI-Driven Survey Question Generation\n\n  This paper presents a methodological framework for using generative AI in\neducational survey research. We explore how Large Language Models (LLMs) can\ngenerate adaptive, context-aware survey questions and introduce the Synthetic\nQuestion-Response Analysis (SQRA) framework, which enables iterative testing\nand refinement of AI-generated prompts prior to deployment with human\nparticipants. Guided by Activity Theory, we analyze how AI tools mediate\nparticipant engagement and learning, and we examine ethical issues such as\nbias, privacy, and transparency. Through sentiment, lexical, and structural\nanalyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the\nalignment and effectiveness of these questions. Our findings highlight the\npromise and limitations of AI-driven survey instruments, emphasizing the need\nfor robust prompt engineering and validation to support trustworthy, scalable,\nand contextually relevant data collection in engineering education.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00013637542724609375, 'GPT4': 0.6875, 'CLAUDE': 0.01140594482421875, 'GOOGLE': 0.0024166107177734375, 'OPENAI_O_SERIES': 9.351968765258789e-05, 'DEEPSEEK': 0.296875, 'GROK': 4.827976226806641e-06, 'NOVA': 1.0132789611816406e-06, 'OTHER': 5.4776668548583984e-05, 'HUMAN': 0.0015916824340820312}}"
2505.20305,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI\n\nAs large language models (LLMs) are increasingly deployed in sensitive domains such as healthcare, law, and education, the demand for transparent, interpretable, and accountable AI systems becomes more urgent. Explainable AI (XAI) acts as a crucial interface between the opaque reasoning of LLMs and the diverse stakeholders who rely on their outputs in high-risk decisions. This paper presents a comprehensive reflection and survey of XAI for LLMs, framed around three guiding questions: Why is explainability essential? What technical and ethical dimensions does it entail? And how can it fulfill its role in real-world deployment?\n  We highlight four core dimensions central to explainability in LLMs, faithfulness, truthfulness, plausibility, and contrastivity, which together expose key design tensions and guide the development of explanation strategies that are both technically sound and contextually appropriate. The paper discusses how XAI can support epistemic clarity, regulatory compliance, and audience-specific intelligibility across stakeholder roles and decision settings.\n  We further examine how explainability is evaluated, alongside emerging developments in audience-sensitive XAI, mechanistic interpretability, causal reasoning, and adaptive explanation systems. Emphasizing the shift from surface-level transparency to governance-ready design, we identify critical challenges and future research directions for ensuring the responsible use of LLMs in complex societal contexts. We argue that explainability must evolve into a civic infrastructure fostering trust, enabling contestability, and aligning AI systems with institutional accountability and human-centered decision-making.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.2649765014648438e-06, 'GPT4': 0.029266357421875, 'CLAUDE': 0.060272216796875, 'GOOGLE': 1.8358230590820312e-05, 'OPENAI_O_SERIES': 1.1086463928222656e-05, 'DEEPSEEK': 0.91064453125, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 2.384185791015625e-07, 'HUMAN': 3.7550926208496094e-06}}"
2507.21059,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Dependency on Meta AI Chatbot in Messenger Among STEM and Non-STEM Students in Higher Education\n\nTo understand the potential dependency of tertiary students regarding Meta AI in the academic context. This descriptive cross-sectional study surveyed 872 tertiary students from public and private institutions in Luzon, Philippines. Demographic information and perceptions on Meta AI dependency based on existing literature were collected. Descriptive statistics were used to summarize the data and differences between STEM and non-STEM students were analyzed using the Mann-Whitney U test. The results indicate a nuanced perspective on Meta AI chatbot use among students. While there is general disagreement with heavy reliance on the chatbot for academic tasks, psychological support, and social factors, there is moderate agreement on its technological benefits and academic utility. Students value the Meta AI convenience, availability, and problem-solving assistance, but prefer traditional resources and human interaction for academic and social support. Concerns about dependency risks and impacts on critical thinking are acknowledged, particularly among STEM students, who rely more on chatbots for academic purposes. This suggests that while Meta AI is a valuable resource, its role is complementary rather than transformative in educational contexts, with institutional encouragement and individual preferences influencing usage patterns. Students generally hesitate to rely heavily on meta-AI chatbots. This reflects a preference for traditional resources and independent problem-solving. While students acknowledge AI chatbots academic benefits and technological convenience, concerns about overreliance and its impact on critical thinking persist, particularly among STEM students, who appear more inclined to integrate these tools into their studies.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.80088996887207e-05, 'GPT4': 0.0011396408081054688, 'CLAUDE': 0.0012617111206054688, 'GOOGLE': 0.99609375, 'OPENAI_O_SERIES': 0.00018203258514404297, 'DEEPSEEK': 0.0012493133544921875, 'GROK': 8.940696716308594e-07, 'NOVA': 3.039836883544922e-06, 'OTHER': 3.8683414459228516e-05, 'HUMAN': 4.857778549194336e-05}}"
2505.06387,regular,post_llm,2025,5,"{'ai_likelihood': 0.23179796006944445, 'text': 'Textual forma mentis networks bridge language structure, emotional content and psychopathology levels in adolescents\n\nWe introduce a network-based AI framework for predicting dimensions of psychopathology in adolescents using natural language. We focused on data capturing psychometric scores of social maladjustment, internalizing behaviors, and neurodevelopmental risk, assessed in 232 adolescents from the Healthy Brain Network. This dataset included structured interviews in which adolescents discussed a common emotion-inducing topic. To model conceptual associations within these interviews, we applied textual forma mentis networks (TFMNs)-a cognitive/AI approach integrating syntactic, semantic, and emotional word-word associations in language. From TFMNs, we extracted network features (semantic/syntactic structure) and emotional profiles to serve as predictors of latent psychopathology factor scores. Using Random Forest and XGBoost regression models, we found significant associations between language-derived features and clinical scores: social maladjustment (r = 0.37, p < .01), specific internalizing behaviors (r = 0.33, p < .05), and neurodevelopmental risk (r = 0.34, p < .05). Explainable AI analysis using SHAP values revealed that higher modularity and a pronounced core-periphery network structure-reflecting clustered conceptual organization in language-predicted increased social maladjustment. Internalizing scores were positively associated with higher betweenness centrality and stronger expressions of disgust, suggesting a linguistic signature of rumination. In contrast, neurodevelopmental risk was inversely related to local efficiency in syntactic/semantic networks, indicating disrupted conceptual integration. These findings demonstrated the potential of cognitive network approaches to capture meaningful links between psychopathology and language use in adolescents.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.01122,review,post_llm,2025,5,"{'ai_likelihood': 1.0596381293402778e-06, 'text': 'The Great Data Standoff: Researchers vs. Platforms Under the Digital\n  Services Act\n\n  To facilitate accountability and transparency, the Digital Services Act (DSA)\nsets up a process through which Very Large Online Platforms (VLOPs) need to\ngrant vetted researchers access to their internal data (Article 40(4)).\nOperationalising such access is challenging for at least two reasons. First,\ndata access is only available for research on systemic risks affecting European\ncitizens, a concept with high levels of legal uncertainty. Second, data access\nsuffers from an inherent standoff problem. Researchers need to request specific\ndata but are not in a position to know all internal data processed by VLOPs,\nwho, in turn, expect data specificity for potential access. In light of these\nlimitations, data access under the DSA remains a mystery. To contribute to the\ndiscussion of how Article 40 can be interpreted and applied, we provide a\nconcrete illustration of what data access can look like in a real-world\nsystemic risk case study. We focus on the 2024 Romanian presidential election\ninterference incident, the first event of its kind to trigger systemic risk\ninvestigations by the European Commission. During the elections, one candidate\nis said to have benefited from TikTok algorithmic amplification through a\ncomplex dis- and misinformation campaign. By analysing this incident, we can\ncomprehend election-related systemic risk to explore practical research tasks\nand compare necessary data with available TikTok data. In particular, we make\ntwo contributions: (i) we combine insights from law, computer science and\nplatform governance to shed light on the complexities of studying systemic\nrisks in the context of election interference, focusing on two relevant\nfactors: platform manipulation and hidden advertising; and (ii) we provide\npractical insights into various categories of available data for the study of\nTikTok, based on platform documentation, data donations and the Research API.\n', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.00965,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""The AI Fairness Myth: A Position Paper on Context-Aware Bias\n\n  Defining fairness in AI remains a persistent challenge, largely due to its\ndeeply context-dependent nature and the lack of a universal definition. While\nnumerous mathematical formulations of fairness exist, they sometimes conflict\nwith one another and diverge from social, economic, and legal understandings of\njustice. Traditional quantitative definitions primarily focus on statistical\ncomparisons, but they often fail to simultaneously satisfy multiple fairness\nconstraints. Drawing on philosophical theories (Rawls' Difference Principle and\nDworkin's theory of equality) and empirical evidence supporting affirmative\naction, we argue that fairness sometimes necessitates deliberate, context-aware\npreferential treatment of historically marginalized groups. Rather than viewing\nbias solely as a flaw to eliminate, we propose a framework that embraces\ncorrective, intentional biases to promote genuine equality of opportunity. Our\napproach involves identifying unfairness, recognizing protected\ngroups/individuals, applying corrective strategies, measuring impact, and\niterating improvements. By bridging mathematical precision with ethical and\ncontextual considerations, we advocate for an AI fairness paradigm that goes\nbeyond neutrality to actively advance social justice.\n"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.4836273193359375e-05, 'GPT4': 0.00274658203125, 'CLAUDE': 0.96435546875, 'GOOGLE': 0.0007491111755371094, 'OPENAI_O_SERIES': 0.00019741058349609375, 'DEEPSEEK': 0.0318603515625, 'GROK': 1.7881393432617188e-07, 'NOVA': 5.364418029785156e-07, 'OTHER': 1.0192394256591797e-05, 'HUMAN': 9.834766387939453e-06}}"
2507.21062,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""Automated but Atrophied? Student Over-Reliance vs Expert Augmentation of AI in Learning and Cybersecurity\n\nUniversity students and working professionals are increasingly encountering generative artificial intelligence (AI) in education and practice, yet their approaches and outcomes differ markedly. This paper proposes an academic study contrasting novice over-reliance on AI with expert augmentation of AI, grounded in two real-world narratives. In one, a university student attempted to outsource learning entirely to AI, eschewing course engagement. In the other, seasoned cybersecurity professionals in the Tradewinds 2025 red/blue team exercise collaboratively employed AI tools to enhance (not replace) their domain expertise. This proposal outlines a comparative research design to investigate how students' perception of AI as a learning replacement versus professionals' use of AI as an expert tool impacts outcomes. Drawing on current literature in educational technology and workplace AI, we examine implications for curriculum design, AI literacy, and assessment reform in higher education. We hypothesize that blind reliance on AI can erode fundamental skills and academic integrity, whereas guided use of AI by knowledgeable users can amplify productivity without sacrificing quality. The paper details methodologies for classroom and workplace data collection, including student and professional surveys, interviews, and performance analyses. Anticipated findings aim to inform responsible AI integration in curricula, balancing innovation with the necessity of domain knowledge. We conclude with recommendations for pedagogical strategies, institutional policies to foster AI literacy, and a call for longitudinal studies tracking how AI usage during university affects professional competencies over time."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.5762786865234375e-07, 'GPT4': 6.723403930664062e-05, 'CLAUDE': 0.00142669677734375, 'GOOGLE': 3.2782554626464844e-06, 'OPENAI_O_SERIES': 9.417533874511719e-06, 'DEEPSEEK': 0.99853515625, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.364418029785156e-07, 'HUMAN': 0.0}}"
2505.18523,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Diversity and Inclusion in AI: Insights from a Survey of AI/ML Practitioners\n\nGrowing awareness of social biases and inequalities embedded in Artificial Intelligence (AI) systems has brought increased attention to the integration of Diversity and Inclusion (D&I) principles throughout the AI lifecycle. Despite the rise of ethical AI guidelines, there is limited empirical evidence on how D&I is applied in real-world settings. This study explores how AI and Machine Learning(ML) practitioners perceive and implement D&I principles and identifies organisational challenges that hinder their effective adoption. Using a mixed-methods approach, we surveyed industry professionals, collecting both quantitative and qualitative data on current practices, perceived impacts, and challenges related to D&I in AI. While most respondents recognise D&I as essential for mitigating bias and enhancing fairness, practical implementation remains inconsistent. Our analysis revealed a disconnect between perceived benefits and current practices, with major barriers including the under-representation of marginalised groups, lack of organisational transparency, and limited awareness among early-career professionals. Despite these barriers, respondents widely agree that diverse teams contribute to ethical, trustworthy, and innovative AI systems. By underpinning the key pain points and areas requiring improvement, this study highlights the need to bridge the gap between D&I principles and real-world AI development practices.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0009794235229492188, 'GPT4': 0.84765625, 'CLAUDE': 0.003040313720703125, 'GOOGLE': 0.14794921875, 'OPENAI_O_SERIES': 2.282857894897461e-05, 'DEEPSEEK': 1.811981201171875e-05, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 3.5762786865234375e-07, 'HUMAN': 0.000148773193359375}}"
2505.15799,review,post_llm,2025,5,"{'ai_likelihood': 0.3428819444444445, 'text': ""The Agentic Economy\n\nGenerative AI has transformed human-computer interaction by enabling natural language interfaces and the emergence of autonomous agents capable of acting on users' behalf. While early applications have improved individual productivity, these gains have largely been confined to predefined tasks within existing workflows. We argue that the more profound economic impact lies in reducing communication frictions between consumers and businesses. This shift could reorganize markets, redistribute power, and catalyze the creation of new products and services. We explore the implications of an agentic economy, where assistant agents act on behalf of consumers and service agents represent businesses, interacting programmatically to facilitate transactions. A key distinction we draw is between unscripted interactions -- enabled by technical advances in natural language and protocol design -- and unrestricted interactions, which depend on market structures and governance. We examine the current limitations of siloed and end-to-end agents, and explore future scenarios shaped by technical standards and market dynamics. These include the potential tension between agentic walled gardens and an open web of agents, implications for advertising and discovery, the evolution of micro-transactions, and the unbundling and rebundling of digital goods. Ultimately, we argue that the architecture of agentic communication will determine the extent to which generative AI democratizes access to economic opportunity."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.12165,regular,post_llm,2025,5,"{'ai_likelihood': 0.0017854902479383682, 'text': ""Beyond the Human-AI Binaries: Advanced Writers' Self-Directed Use of Generative AI in Academic Writing\n\nThis study explores the self-directed use of Generative AI (GAI) in academic writing among advanced L2 English writers, challenging assumptions that GAI undermines meaningful learning and holds less value for experienced learners. Through case studies, we investigate how three (post)doctoral writers engage with GAI to address specific L2 writing challenges. The findings revealed a spectrum of approaches to GAI, ranging from prescriptive to dialogic uses, with participants positioning AI as a tool versus an interactive participant in their meaning-making process, reflecting different views of AI as a mechanical system, social construct, or distributed agency. We highlight the ways AI disrupts traditional notions of authorship, text, and learning, showing how a poststructuralist lens allows us to transcend human-AI, writing-technology, and learning-bypassing binaries in our existing discourses on AI. This shifting view allows us to deconstruct and reconstruct AI's multifaceted possibilities in L2 writers' literacy practices. We also call for more nuanced ethical considerations to avoid stigmatizing L2 writers' use of GAI and to foster writerly virtues that reposition our relationship with AI technology."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.1049,review,post_llm,2025,5,"{'ai_likelihood': 0.0038464864095052085, 'text': ""Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns\n\nAs the use of Large Language Models (LLMs) by students, lecturers and researchers becomes more prevalent, universities - like other organizations - are pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer accessible pre-trained models, customizable to specific (business) needs. While most studies prioritize data, model, or infrastructure adaptations (e.g., model fine-tuning), we focus on user-salient customizations, like interface changes and corporate branding, which we argue influence users' trust and usage patterns. This study serves as a functional prequel to a large-scale field study in which we examine how students and employees at a German university perceive and use their institution's customized LLMaaS compared to ChatGPT. The goals of this prequel are to stimulate discussions on psychological effects of LLMaaS customizations and refine our research approach through feedback. Our forthcoming findings will deepen the understanding of trust dynamics in LLMs, providing practical guidance for organizations considering LLMaaS deployment."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2505.20329,regular,post_llm,2025,5,"{'ai_likelihood': 0.814453125, 'text': 'Generative AI in Computer Science Education: Accelerating Python Learning with ChatGPT\n\nThe increasing demand for digital literacy and artificial intelligence (AI) fluency in the workforce has highlighted the need for scalable, efficient programming instruction. This study evaluates the effectiveness of integrating generative AI, specifically OpenAIs ChatGPT, into a self-paced Python programming module embedded within a sixteen-week professional training course on applied generative AI. A total of 86 adult learners with varying levels of programming experience completed asynchronous Python instruction in Weeks three and four, using ChatGPT to generate, interpret, and debug code. Python proficiency and general coding knowledge was assessed across 30 different assessments during the first 13 weeks of the course through timed, code-based evaluations. A mixed-design ANOVA revealed that learners without prior programming experience scored significantly lower than their peers on early assessments. However, following the completion of the accelerated Python instruction module, these group differences were no longer statistically significant,, indicating that the intervention effectively closed initial performance gaps and supported proficiency gains across all learner groups. These findings suggest that generative AI can support accelerated learning outcomes and reduce entry barriers for learners with no prior coding background. While ChatGPT effectively facilitated foundational skill acquisition, the study also highlights the importance of balancing AI assistance with opportunities for independent problem-solving. The results support the potential of AI-augmented instruction as a scalable model for reskilling in the digital economy.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 5.5909156799316406e-05, 'GPT4': 0.0601806640625, 'CLAUDE': 0.55517578125, 'GOOGLE': 0.008758544921875, 'OPENAI_O_SERIES': 0.0006036758422851562, 'DEEPSEEK': 0.0278167724609375, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.430511474609375e-06, 'OTHER': 1.4841556549072266e-05, 'HUMAN': 0.347412109375}}"
2505.0277,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Teaching the social media generation: rethinking learning without\n  sacrificing quality\n\n  The rise of social media and AI tools has reshaped how students engage with\nlearning, process information, and build trust in educational content. This\ngeneration prefers short, visual materials and fast feedback but often\nstruggles with focus, critical thinking, and deep learning. Educators face the\nchallenge of adapting teaching methods to these habits without lowering\nacademic standards. This study presents a blended learning redesign of a\nfirst-year technical course at a Dutch university. Key features included short\nwhiteboard videos before class, hands-on teamwork during class, narrative-style\nhandouts to reinforce learning, in-class draft assignments without AI, and\nweekly anonymous feedback to adjust in real time. The results were promising:\nattendance increased by nearly 50%, and none of the regularly attending\nstudents failed the exam. Students found the videos useful but emphasized that\nin-person sessions were essential for understanding the material. While some\nresisted the shift in expectations, most appreciated the structure, clarity,\nand opportunities for active learning. This case suggests that combining\ndigital familiarity with clear expectations and active support can help meet\nstudents where they are, while still challenging them to grow.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0006504058837890625, 'GPT4': 0.04498291015625, 'CLAUDE': 0.771484375, 'GOOGLE': 0.0014448165893554688, 'OPENAI_O_SERIES': 0.00018894672393798828, 'DEEPSEEK': 0.18115234375, 'GROK': 2.384185791015625e-06, 'NOVA': 3.2782554626464844e-06, 'OTHER': 4.941225051879883e-05, 'HUMAN': 9.334087371826172e-05}}"
2505.21517,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': ""Cold Start Problem: An Experimental Study of Knowledge Tracing Models with New Students\n\nKnowledgeTracing (KT) involves predicting students' knowledge states based on their interactions with Intelligent Tutoring Systems (ITS). A key challenge is the cold start problem, accurately predicting knowledge for new students with minimal interaction data. Unlike prior work, which typically trains KT models on initial interactions of all students and tests on their subsequent interactions, our approach trains models solely using historical data from past students, evaluating their performance exclusively on entirely new students. We investigate cold start effects across three KT models: Deep Knowledge Tracing (DKT), Dynamic Key-Value Memory Networks (DKVMN), and Self-Attentive Knowledge Tracing (SAKT), using ASSISTments 2009, 2015, and 2017 datasets. Results indicate all models initially struggle under cold start conditions but progressively improve with more interactions; SAKT shows higher initial accuracy yet still faces limitations. These findings highlight the need for KT models that effectively generalize to new learners, emphasizing the importance of developing models robust in few-shot and zero-shot learning scenarios"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00016224384307861328, 'GPT4': 0.1995849609375, 'CLAUDE': 0.153076171875, 'GOOGLE': 0.280517578125, 'OPENAI_O_SERIES': 0.04364013671875, 'DEEPSEEK': 0.322265625, 'GROK': 8.976459503173828e-05, 'NOVA': 3.0159950256347656e-05, 'OTHER': 6.937980651855469e-05, 'HUMAN': 0.0007185935974121094}}"
2505.22639,review,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Navigating the AI-Energy Nexus with Geopolitical Insight\n\nThis working paper examines how geopolitical strategies and energy resource management intersect with Artificial Intelligence (AI) development, delineating the AI-energy nexus as critical to sustaining U.S. AI leadership. By analyzing the centralized approaches of authoritarian regimes like China and Gulf nations, alongside market-driven approaches in the U.S., the paper explores divergent strategies to allocate resources for AI energy needs. It underscores the role of energy infrastructure, market dynamics, and state-led initiatives in shaping global AI competition. Recommendations include adopting geopolitically informed analyses and leveraging both market and non-market strengths to enhance U.S. competitiveness. This research aims to inform policymakers, technologists, and researchers about the strategic implications of the AI-energy nexus and offers insights into advancing U.S. global leadership in AI amidst evolving technological paradigms.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0002491474151611328, 'GPT4': 0.958984375, 'CLAUDE': 8.165836334228516e-05, 'GOOGLE': 0.01172637939453125, 'OPENAI_O_SERIES': 0.0273895263671875, 'DEEPSEEK': 0.0012159347534179688, 'GROK': 8.356571197509766e-05, 'NOVA': 0.00011336803436279297, 'OTHER': 0.0001862049102783203, 'HUMAN': 2.6226043701171875e-05}}"
2505.00853,regular,post_llm,2025,5,"{'ai_likelihood': 0.99755859375, 'text': 'LLM Ethics Benchmark: A Three-Dimensional Assessment System for\n  Evaluating Moral Reasoning in Large Language Models\n\n  This study establishes a novel framework for systematically evaluating the\nmoral reasoning capabilities of large language models (LLMs) as they\nincreasingly integrate into critical societal domains. Current assessment\nmethodologies lack the precision needed to evaluate nuanced ethical\ndecision-making in AI systems, creating significant accountability gaps. Our\nframework addresses this challenge by quantifying alignment with human ethical\nstandards through three dimensions: foundational moral principles, reasoning\nrobustness, and value consistency across diverse scenarios. This approach\nenables precise identification of ethical strengths and weaknesses in LLMs,\nfacilitating targeted improvements and stronger alignment with societal values.\nTo promote transparency and collaborative advancement in ethical AI\ndevelopment, we are publicly releasing both our benchmark datasets and\nevaluation codebase at https://github.com/\nThe-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git.\n', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.8298625946044922e-05, 'GPT4': 0.0765380859375, 'CLAUDE': 0.91064453125, 'GOOGLE': 0.005767822265625, 'OPENAI_O_SERIES': 0.0008263587951660156, 'DEEPSEEK': 0.00438690185546875, 'GROK': 5.960464477539063e-08, 'NOVA': 1.5497207641601562e-06, 'OTHER': 5.453824996948242e-05, 'HUMAN': 0.0016050338745117188}}"
2506.00105,regular,post_llm,2025,5,"{'ai_likelihood': 0.99755859375, 'text': ""Motivando el uso y aprendizaje de Bash a trav\\'es de concursos de programaci\\'on\n\nCommand line learning and Bash usage are fundamental skills in systems administration, software development, and data science environments. However, their teaching has been neglected in many curricula, despite its relevance in the professional field. To address this gap, we developed an interactive competition that encourages students to improve their Bash skills through practical and competitive challenges. This gamified approach seeks to motivate autonomous learning and reinforce command line proficiency in a dynamic context. The results have been promising: of the 26 participating students, 85% considered the activity useful to improve their knowledge, and 71% expressed the need to delve deeper into Bash for their academic and professional future. These findings suggest that such initiatives may be an effective strategy to foster Bash learning in academic settings."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00489044189453125, 'GPT4': 0.63671875, 'CLAUDE': 0.26220703125, 'GOOGLE': 0.0386962890625, 'OPENAI_O_SERIES': 0.01432037353515625, 'DEEPSEEK': 0.0197601318359375, 'GROK': 0.0003085136413574219, 'NOVA': 0.001377105712890625, 'OTHER': 0.01116943359375, 'HUMAN': 0.01047515869140625}}"
2506.00051,regular,post_llm,2025,5,"{'ai_likelihood': 1.0, 'text': 'Beyond Monoliths: Expert Orchestration for More Capable, Democratic, and Safe Language Models\n\nThis position paper argues that the prevailing trajectory toward ever larger, more expensive generalist foundation models controlled by a handful of companies limits innovation and constrains progress. We challenge this approach by advocating for an ""Expert Orchestration"" (EO) framework as a superior alternative that democratizes LLM advancement. Our proposed framework intelligently selects from many existing models based on query requirements and decomposition, focusing on identifying what models do well rather than how they work internally. Independent ""judge"" models assess various models\' capabilities across dimensions that matter to users, while ""router"" systems direct queries to the most appropriate specialists within an approved set. This approach delivers superior performance by leveraging targeted expertise rather than forcing costly generalist models to address all user requirements. EO enhances transparency, control, alignment, performance, safety and democratic participation through intelligent model selection.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.6093254089355469e-06, 'GPT4': 0.0001271963119506836, 'CLAUDE': 0.998046875, 'GOOGLE': 0.00020897388458251953, 'OPENAI_O_SERIES': 5.072355270385742e-05, 'DEEPSEEK': 0.0012483596801757812, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.6689300537109375e-06, 'HUMAN': 0.0003116130828857422}}"
2505.17864,regular,post_llm,2025,5,"{'ai_likelihood': 0.46929253472222227, 'text': ""Urban Household Behavior in Indonesia: Drivers of Zero Waste Participation\n\nThe 3R-based Zero Waste approach aims to minimize household solid waste through the principles of Reduce, Reuse, and Recycle. This study examines the relationship between household environmental knowledge, personal attitude, subjective norms, and perceived behavioral control as key behavioral predictors. A structured survey was conducted among 1,200 urban households across 12 Indonesian cities. Data were analyzed using Pearson correlation and multiple regression analysis. The results indicate that perceived behavioral control is the strongest predictor of household waste management behavior (beta = 0.367, p <= 0.001), followed by subjective norms (beta = 0.358, p <= 0.001) and environmental knowledge (beta = 0.126, p <= 0.001). This suggests that individuals' confidence in managing household waste significantly influences their practical actions. Overall, perceived behavioral control, subjective norms, and environmental knowledge contribute to Zero Waste behavior in urban households. Given that households regularly generate and dispose of waste, they represent a fundamental element in municipal waste management strategies. These findings offer valuable insights for designing behavior-based interventions and inform policy development using the Theory of Planned Behavior."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.14377,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'Between Regulation and Accessibility: How Chinese University Students Navigate Global and Domestic Generative AI\n\nDespite the rapid proliferation of generative AI in higher education, students in China face significant barriers in accessing global tools like ChatGPT due to regulations and constraints. Grounded in the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2) model, this study employs qualitative interviews to investigate how Chinese university students interact with both global and domestic generative AIs in the learning process. Findings reveal that engagement is shaped by accessibility, language proficiency, and cultural relevance. Students often employ workarounds (e.g., VPNs) to access global generative AIs, raising ethical and privacy concerns. Domestic generative AIs, while offering language and cultural advantages, are limited by content filtering and output constraints. This research contributes to understanding generative AI adoption in non-Western contexts by highlighting the complex interplay of political, linguistic, and cultural factors. It advocates for human-centered, multilingual, domestic context-sensitive AI integration to ensure equitable and inclusive digital learning environments.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00016808509826660156, 'GPT4': 0.413818359375, 'CLAUDE': 0.01027679443359375, 'GOOGLE': 0.0215301513671875, 'OPENAI_O_SERIES': 0.006683349609375, 'DEEPSEEK': 0.54736328125, 'GROK': 3.7550926208496094e-06, 'NOVA': 9.47713851928711e-06, 'OTHER': 0.00015342235565185547, 'HUMAN': 8.469820022583008e-05}}"
2506.1351,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-LLM Interactions\n\nAs Large Language Models (LLMs) increasingly power applications used by children and adolescents, ensuring safe and age-appropriate interactions has become an urgent ethical imperative. Despite progress in AI safety, current evaluations predominantly focus on adults, neglecting the unique vulnerabilities of minors engaging with generative AI. We introduce Safe-Child-LLM, a comprehensive benchmark and dataset for systematically assessing LLM safety across two developmental stages: children (7-12) and adolescents (13-17). Our framework includes a novel multi-part dataset of 200 adversarial prompts, curated from red-teaming corpora (e.g., SG-Bench, HarmBench), with human-annotated labels for jailbreak success and a standardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including ChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we uncover critical safety deficiencies in child-facing scenarios. This work highlights the need for community-driven benchmarks to protect young users in LLM interactions. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.344650268554688e-07, 'GPT4': 0.006084442138671875, 'CLAUDE': 0.132080078125, 'GOOGLE': 1.3709068298339844e-05, 'OPENAI_O_SERIES': 7.331371307373047e-06, 'DEEPSEEK': 0.861328125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 2.980232238769531e-07, 'HUMAN': 0.00028634071350097656}}"
2506.11255,review,post_llm,2025,6,"{'ai_likelihood': 1.860989464653863e-05, 'text': 'Social Scientists on the Role of AI in Research\n\nThe integration of artificial intelligence (AI) into social science research practices raises significant technological, methodological, and ethical issues. We present a community-centric study drawing on 284 survey responses and 15 semi-structured interviews with social scientists, describing their familiarity with, perceptions of the usefulness of, and ethical concerns about the use of AI in their field. A crucial innovation in study design is to split our survey sample in half, providing the same questions to each -- but randomizing whether participants were asked about ""AI"" or ""Machine Learning"" (ML). We find that the use of AI in research settings has increased significantly among social scientists in step with the widespread popularity of generative AI (genAI). These tools have been used for a range of tasks, from summarizing literature reviews to drafting research papers. Some respondents used these tools out of curiosity but were dissatisfied with the results, while others have now integrated them into their typical workflows. Participants, however, also reported concerns with the use of AI in research contexts. This is a departure from more traditional ML algorithms which they view as statistically grounded. Participants express greater trust in ML, citing its relative transparency compared to black-box genAI systems. Ethical concerns, particularly around automation bias, deskilling, research misconduct, complex interpretability, and representational harm, are raised in relation to genAI. To guide this transition, we offer recommendations for AI developers, researchers, educators, and policymakers focusing on explainability, transparency, ethical safeguards, sustainability, and the integration of lived experiences into AI design and evaluation processes.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.06011,regular,post_llm,2025,6,"{'ai_likelihood': 0.98291015625, 'text': 'London Blue Light Collaboration Evaluation: A Comparative Analysis of Spatio temporal Patterns on Emergency Services by London Ambulance Service and London Fire Brigade\n\nWith rising demand for emergency services, the London Ambulance Service, LAS, and the London Fire Brigade, LFB, face growing challenges in resource coordination. This study investigates the temporal and spatial similarities in their service demands to assess potential for routine cross-agency collaboration. Time series analysis revealed aligned demand peaks in summer, on Fridays, during daytime hours, and were highly sensitive to high temperature weather conditions. Bivariate mapping and Moran I indicated significant spatial overlaps in central London and Hillingdon. Geographically Weighted Regression, GWR, examined the influence of socioeconomic factors, while Comap analysis uncovered spatiotemporal heterogeneity across fire service types. The findings highlight opportunities for targeted collaboration in high-overlap areas and peak periods, offering practical insights to enhance emergency service resilience and efficiency.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0011110305786132812, 'GPT4': 0.154052734375, 'CLAUDE': 0.61376953125, 'GOOGLE': 0.11309814453125, 'OPENAI_O_SERIES': 0.00650787353515625, 'DEEPSEEK': 0.09893798828125, 'GROK': 7.593631744384766e-05, 'NOVA': 0.0001087188720703125, 'OTHER': 0.0024356842041015625, 'HUMAN': 0.0097503662109375}}"
2506.15991,review,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': ""The Quantified Body: Identity, Empowerment, and Control in Smart Wearables\n\nIn an era where the body is increasingly translated into streams of biometric data, smart wearables have become not merely tools of personal health tracking but infrastructures of predictive governance. This paper examines how wearable technologies reconfigure bodily autonomy by embedding users within feedback-driven systems of self-surveillance, data extraction, and algorithmic control. Drawing on Deleuze's concept of the control society, Zuboff's theory of surveillance capitalism, and Couldry and Mejias's notion of data colonialism, I argue that smart wearables shift the discourse of health empowerment toward a modality of compliance aligned with neoliberal values of productivity, efficiency, and self-discipline. Rather than offering transparent consent, these technologies operate within what scholars describe as a post-consent regime -- where asymmetrical data relations are normalized through seamless design and behavioral nudging. Through interdisciplinary analysis, the paper further explores alternative trajectories for wearable design and governance, from historical examples of care-centered devices to contemporary anti-extractive practices and collective data justice frameworks. Ultimately, it calls for a paradigm shift from individual optimization to democratic accountability and structural reform in the governance of bodily data."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.1576881408691406e-05, 'GPT4': 0.264404296875, 'CLAUDE': 0.427490234375, 'GOOGLE': 3.933906555175781e-06, 'OPENAI_O_SERIES': 3.445148468017578e-05, 'DEEPSEEK': 0.307861328125, 'GROK': 1.1920928955078125e-07, 'NOVA': 0.0, 'OTHER': 3.5762786865234375e-07, 'HUMAN': 3.2186508178710938e-06}}"
2506.13735,review,post_llm,2025,6,"{'ai_likelihood': 1.3245476616753473e-07, 'text': 'Bias Delayed is Bias Denied? Assessing the Effect of Reporting Delays on Disparity Assessments\n\nConducting disparity assessments at regular time intervals is critical for surfacing potential biases in decision-making and improving outcomes across demographic groups. Because disparity assessments fundamentally depend on the availability of demographic information, their efficacy is limited by the availability and consistency of available demographic identifiers. While prior work has considered the impact of missing data on fairness, little attention has been paid to the role of delayed demographic data. Delayed data, while eventually observed, might be missing at the critical point of monitoring and action -- and delays may be unequally distributed across groups in ways that distort disparity assessments. We characterize such impacts in healthcare, using electronic health records of over 5M patients across primary care practices in all 50 states. Our contributions are threefold. First, we document the high rate of race and ethnicity reporting delays in a healthcare setting and demonstrate widespread variation in rates at which demographics are reported across different groups. Second, through a set of retrospective analyses using real data, we find that such delays impact disparity assessments and hence conclusions made across a range of consequential healthcare outcomes, particularly at more granular levels of state-level and practice-level assessments. Third, we find limited ability of conventional methods that impute missing race in mitigating the effects of reporting delays on the accuracy of timely disparity assessments. Our insights and methods generalize to many domains of algorithmic fairness where delays in the availability of sensitive information may confound audits, thus deserving closer attention within a pipeline-aware machine learning framework.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.13985,regular,post_llm,2025,6,"{'ai_likelihood': 0.4535590277777778, 'text': 'Exploring Economic Sectoral Dynamics Through High-resolution Mobility Data\n\nWe present a comprehensive dataset capturing patterns of human mobility across the United States from January 2019 to January 2023, based on anonymized mobile device data. Aggregated weekly, the dataset reports visits, travel distances, and time spent at public locations organized by economic sector for approximately 12 million Points of Interest (POIs). This resource enables the study of how mobility and economic activity changed over time, particularly during major events such as the COVID-19 pandemic. By disaggregating patterns across different types of businesses, it provides valuable insights for researchers in economics, urban studies, and public health. To protect privacy, all data have been aggregated and anonymized. This dataset offers an opportunity to explore the dynamics of human behavior across sectors over an extended time period, supporting studies of mobility, resilience, and recovery.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.09472,review,post_llm,2025,6,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""Situated Bayes -- Feminist and Pluriversal Perspectives on Bayesian Knowledge\n\nThis is the introduction and lead article to the Situated Bayes special issue of Computational Culture. The article introduces Bayes' Theorem and aspects of its contemporary uses, for instance in machine learning. A mathematical discussion is developed alongside a consideration of Bayes Theorem in relation to critical theories of knowledge, specifically the discussion of situated knowledge in feminist theories of science, pluriversal knowledge in decolonial theory, and critical approaches to mathematics. We discuss whether there are possible resonances between Bayesian mapping of multiple functions and the idea of the subjective on the one hand and these theoretical propositions on the other and propose further lines of enquiry for future research. In closing the introduction, the contributions to the special issue are briefly described."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.04419,review,post_llm,2025,6,"{'ai_likelihood': 6.821420457628038e-06, 'text': 'A Framework for Auditing Chatbots for Dialect-Based Quality-of-Service Harms\n\nIncreasingly, individuals who engage in online activities are expected to interact with large language model (LLM)-based chatbots. Prior work has shown that LLMs can display dialect bias, which occurs when they produce harmful responses when prompted with text written in minoritized dialects. However, whether and how this bias propagates to systems built on top of LLMs, such as chatbots, is still unclear. We conduct a review of existing approaches for auditing LLMs for dialect bias and show that they cannot be straightforwardly adapted to audit LLM-based chatbots due to issues of substantive and ecological validity. To address this, we present a framework for auditing LLM-based chatbots for dialect bias by measuring the extent to which they produce quality-of-service harms, which occur when systems do not work equally well for different people. Our framework has three key characteristics that make it useful in practice. First, by leveraging dynamically generated instead of pre-existing text, our framework enables testing over any dialect, facilitates multi-turn conversations, and represents how users are likely to interact with chatbots in the real world. Second, by measuring quality-of-service harms, our framework aligns audit results with the real-world outcomes of chatbot use. Third, our framework requires only query access to an LLM-based chatbot, meaning that it can be leveraged equally effectively by internal auditors, external auditors, and even individual users in order to promote accountability. To demonstrate the efficacy of our framework, we conduct a case study audit of Amazon Rufus, a widely-used LLM-based chatbot in the customer service domain. Our results reveal that Rufus produces lower-quality responses to prompts written in minoritized English dialects, and that these quality-of-service harms are exacerbated by the presence of typos in prompts.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.06803,regular,post_llm,2025,6,"{'ai_likelihood': 0.9951171875, 'text': ""Spatial Disparities in Fire Shelter Accessibility: Capacity Challenges in the Palisades and Eaton Fires\n\nThe increasing frequency and severity of wildfire in California, exacerbated by prolonged drought and environmental changes, pose significant challenges to urban community resilience and equitable emergency response. The study investigates issues of accessibility to shelters during the Palisades and Eaton Fires which started in January 2025 in Southern California that led to over 180,000 displacements and the loss of 16,000 structures. Despite coordinated efforts of many organizations' emergency assistance, shelter shortages left many evacuees without safety or accessible refuge. This research aims to measure shelter accessibility during the fires' peak, evaluate whether existing shelter capacity met the demand, and identify spatial disparities in access. Results reveal severe shelter shortages and pronounced inequities in access to shelters, particularly in geographically isolated regions and mountainous areas. Our simulations of shelter placement strategies using a capacity-based algorithm and a proximity-based approach demonstrate potential improvements in both shelter accessibility and equitable access to shelters. The findings underscore the critical need for strategic shelter planning and infrastructure development to enhance disaster readiness and reduce vulnerability in regions that frequently experience wildfires."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0009045600891113281, 'GPT4': 0.98046875, 'CLAUDE': 0.00557708740234375, 'GOOGLE': 0.01209259033203125, 'OPENAI_O_SERIES': 0.00033926963806152344, 'DEEPSEEK': 4.827976226806641e-06, 'GROK': 1.7881393432617188e-07, 'NOVA': 4.172325134277344e-07, 'OTHER': 4.887580871582031e-06, 'HUMAN': 0.0008044242858886719}}"
2506.09731,regular,post_llm,2025,6,"{'ai_likelihood': 0.9951171875, 'text': 'The Path is the Goal: a Study on the Nature and Effects of Shortest-Path Stability Under Perturbation of Destination\n\nThis work examines the phenomenon of path variability in urban navigation, where small changes in destination might lead to significantly different suggested routes. Starting from an observation of this variability over the city of Barcelona, we explore whether this is a localized or widespread occurrence and identify factors influencing path variability. We introduce the concept of ""path stability"", a measure of how robust a suggested route is to minor destination adjustments, define a detailed experimentation process and apply it across multiple cities worldwide. Our analysis shows that path stability is shaped by city-specific factors and trip characteristics, also identifying some common patterns. Results reveal significant heterogeneity in path stability across cities, allowing for categorization into ""stable"" and ""unstable"" cities. These findings offer new insights for urban planning and traffic management, highlighting opportunities for optimizing navigation systems to enhance route consistency and urban mobility.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004801750183105469, 'GPT4': 0.442626953125, 'CLAUDE': 0.005573272705078125, 'GOOGLE': 0.54443359375, 'OPENAI_O_SERIES': 0.0018444061279296875, 'DEEPSEEK': 0.0022373199462890625, 'GROK': 0.00014472007751464844, 'NOVA': 4.607439041137695e-05, 'OTHER': 0.0013360977172851562, 'HUMAN': 0.0012311935424804688}}"
2506.04145,regular,post_llm,2025,6,"{'ai_likelihood': 0.98779296875, 'text': 'Improving Regulatory Oversight in Online Content Moderation\n\nThe European Union introduced the Digital Services Act (DSA) to address the risks associated with digital platforms and promote a safer online environment. However, despite the potential of components such as the Transparency Database, Transparency Reports, and Article 40 of the DSA to improve platform transparency, significant challenges remain. These include data inconsistencies and a lack of detailed information, which hinder transparency in content moderation practices. Additionally, the absence of standardized reporting structures makes cross-platform comparisons and broader analyses difficult. To address these issues, we propose two complementary processes: a Transparency Report Cross-Checking Process and a Verification Process. Their goal is to provide both internal and external validation by detecting possible inconsistencies between self-reported and actual platform data, assessing compliance levels, and ultimately enhancing transparency while improving the overall effectiveness of the DSA in ensuring accountability in content moderation. Additionally, these processes can benefit policymakers by providing more accurate data for decision-making, independent researchers with trustworthy analysis, and platforms by offering a method for self-assessment and improving compliance and reporting practices.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.017852783203125, 'GPT4': 0.791015625, 'CLAUDE': 0.0027599334716796875, 'GOOGLE': 0.1707763671875, 'OPENAI_O_SERIES': 0.00925445556640625, 'DEEPSEEK': 0.0002837181091308594, 'GROK': 1.7523765563964844e-05, 'NOVA': 0.00011962652206420898, 'OTHER': 0.0015630722045898438, 'HUMAN': 0.0062255859375}}"
2506.04588,regular,post_llm,2025,6,"{'ai_likelihood': 0.6362304687499999, 'text': ""Skill-Driven Certification Pathways: Measuring Industry Training Impact on Graduate Employability\n\nAustralia faces a critical technology skills shortage, requiring approximately $52,000$ new technology professionals annually by 2030, while confronting a widening gap between employer requirements and graduate capabilities. With only $1\\%$ of technology graduates considered immediately work-ready, traditional educational pathways alone prove insufficient to meet industry demands. This research examines how industry certifications, such as Microsoft's AI-900 (Azure AI Fundamentals), can bridge this critical skills gap. We propose a novel, data-driven methodology that quantitatively measures skill alignment between educational offerings and job market requirements by analysing over 2.5 million job advertisements from Australia, the US, and the UK, mapping extracted skills to industry taxonomies using the Vectorised Skills Space Method. Our findings reveal that combining university degrees with targeted industry certifications significantly enhances employability for technology roles. The Bachelor of Computer Science with AI major combined with AI-900 certification achieved the highest absolute skill similarity score for Machine Learning Engineer positions. Surprisingly, the largest improvements when augmented with AI certifications are experiences by non-technical degrees--such as nursing nursing--with up to $9,296\\%$ percentage improvements in alignment with Machine Learning Engineer roles. Our results challenge conventional assumptions about technology career pathways. They can provide actionable insights for educational institutions seeking evidence-based curriculum design, students requiring strategic certification guidance, and employers recognising potential in candidates from non-traditional backgrounds who have obtained relevant certifications."", 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 2.86102294921875e-06, 'GPT4': 0.005458831787109375, 'CLAUDE': 0.7666015625, 'GOOGLE': 0.0016613006591796875, 'OPENAI_O_SERIES': 7.778406143188477e-05, 'DEEPSEEK': 0.129638671875, 'GROK': 0.0, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.3113021850585938e-06, 'HUMAN': 0.0966796875}}"
2507.21076,review,post_llm,2025,6,"{'ai_likelihood': 0.99267578125, 'text': 'Making a Case for Research Collaboration Between Artificial Intelligence and Operations Research Experts\n\nIn 2021, INFORMS, ACM SIGAI, and the Computing Community Consortium (CCC) hosted three workshops to explore synergies between Artificial Intelligence (AI) and Operations Research (OR) to improve decision-making. The workshops aimed to create a unified research vision for AI/OR collaboration, focusing on overcoming cultural differences and maximizing societal impact. The first two workshops addressed technological innovations, applications, and trustworthy AI development, while the final workshop highlighted specific areas for AI/OR integration. Participants discussed ""Challenge Problems"" and strategies for combining AI and OR techniques. This report outlines five key recommendations to enhance AI/OR collaboration: 1) Funding Opportunities, 2) Joint Education, 3) Long-term Research Programs, 4) Aligning Conferences/Journals, and 5) Benchmark Creation.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0177001953125, 'GPT4': 0.01331329345703125, 'CLAUDE': 0.5419921875, 'GOOGLE': 0.2418212890625, 'OPENAI_O_SERIES': 0.0024051666259765625, 'DEEPSEEK': 0.08648681640625, 'GROK': 0.00031304359436035156, 'NOVA': 0.002002716064453125, 'OTHER': 0.08966064453125, 'HUMAN': 0.004238128662109375}}"
2507.21082,regular,post_llm,2025,6,"{'ai_likelihood': 0.005459255642361111, 'text': 'Safety Features for a Centralised AGI Project\n\nRecent AI progress has outpaced expectations, with some experts now predicting AI that matches or exceeds human capabilities in all cognitive areas (AGI) could emerge this decade, potentially posing grave national and global security threats. AI development is currently occurring primarily in the private sector with minimal oversight. This report analyzes a scenario where the US government centralizes AGI development under its direct control, and identifies four high-level priorities and seven safety features to reduce risks.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.05172,regular,post_llm,2025,6,"{'ai_likelihood': 4.523330264621311e-05, 'text': 'A Framework for Ethical Judgment of Smart City Applications\n\nAs modern cities increasingly adopt a variety of sensors and Internet of Things (IoT) technologies to collect and analyze data about residents, environments, and public services, they are fostering greater interactions among smart city applications, residents, governments, and businesses. This trend makes it essential for regulators to focus on these interactions to manage smart city practices effectively and prevent unethical outcomes. To facilitate ethical analysis for smart city applications, this paper introduces a judgment framework that examines various scenarios where ethical issues may arise. Employing a multi-agent approach, the framework incorporates diverse social entities and applies logic-based ethical rules to identify potential violations. Through a rights-based analysis, we developed a set of 13 ethical principles and rules to guide ethical practices in smart cities. We utilized two specification languages, Prototype Verification System (PVS) and Alloy, to model our multi-agent system. Our analysis suggests that Alloy may be more efficient for formalizing smart cities and conducting ethical rule checks, particularly with the assistance of a human evaluator. Simulations of a real-world smart city application demonstrate that our ethical judgment framework effectively detects unethical outcomes and can be extended for practical use.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.17158,review,post_llm,2025,6,"{'ai_likelihood': 0.0002082188924153646, 'text': ""How online misinformation works: a costly signalling perspective\n\nThis chapter explores how online communication, particularly on social media, reshapes the reputational incentives that motivate speakers to communicate truthfully. Drawing on costly signalling theory (CST), it examines how online contexts alter the social mechanisms that sustain honest communication. Key characteristics of online spaces are identified and discussed, namely (i) the presence of novel speech acts like reposting, (ii) the gamification of communication, (iii) information overload, (iv) the presence of anonymous and unaccountable sources and (v) the increased reach and persistence of online communication. Both epistemic pitfalls and potential benefits of these features are discussed, identifying promising avenues for further empirical investigation, and underscoring CST's value for understanding and tackling online misinformation."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.18133,regular,post_llm,2025,6,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""Aggregated Individual Reporting for Post-Deployment Evaluation\n\nThe need for developing model evaluations beyond static benchmarking, especially in the post-deployment phase, is now well-understood. At the same time, concerns about the concentration of power in deployed AI systems have sparked a keen interest in 'democratic' or 'public' AI. In this work, we bring these two ideas together by proposing mechanisms for aggregated individual reporting (AIR), a framework for post-deployment evaluation that relies on individual reports from the public. An AIR mechanism allows those who interact with a specific, deployed (AI) system to report when they feel that they may have experienced something problematic; these reports are then aggregated over time, with the goal of evaluating the relevant system in a fine-grained manner. This position paper argues that individual experiences should be understood as an integral part of post-deployment evaluation, and that the scope of our proposed aggregated individual reporting mechanism is a practical path to that end. On the one hand, individual reporting can identify substantively novel insights about safety and performance; on the other, aggregation can be uniquely useful for informing action. From a normative perspective, the post-deployment phase completes a missing piece in the conversation about 'democratic' AI. As a pathway to implementation, we provide a workflow of concrete design decisions and pointers to areas requiring further research and methodological development."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.13461,regular,post_llm,2025,6,"{'ai_likelihood': 9.934107462565105e-07, 'text': ""Navigating through CS1: The Role of Self-Regulation and Supervision in Student Progress\n\nThe need for students' self-regulation for fluent transitioning to university studies is known. Our aim was to integrate study-supportive activities with course supervision activities within CS1. We educated TAs to pay attention to students' study ability and self-regulation. An interview study ($N=14$) was undertaken to investigate this approach. A thematic analysis yielded rather mixed results in light of our aims. Self-regulation was underpinned by the influences external to our setting, including labor market-related needs, earlier crises in study habits, and personal characteristics such as passion, grit, creativity, and valuation of utility. Safety in one-to-one supervision was considered essential, while shyness, fear, and even altruism caused self-handicapping during the course. Students were aware of their learning styles and need for self-regulation, while did not always know how to self-regulate or preferred to externalize it. The results highlight that supporting self-regulation should be integrated with students' personal histories and experiences, and thereby calls attention to transformative learning pedagogies. The thematization can help to understand CS1 students' self-regulation processes and improve CS1 support practices."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.2053,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'Toward a Global Regime for Compute Governance: Building the Pause Button\n\nAs AI capabilities rapidly advance, the risk of catastrophic harm from large-scale training runs is growing. Yet the compute infrastructure that enables such development remains largely unregulated. This paper proposes a concrete framework for a global ""Compute Pause Button"": a governance system designed to prevent dangerously powerful AI systems from being trained by restricting access to computational resources. We identify three key intervention points -- technical, traceability, and regulatory -- and organize them within a Governance--Enforcement--Verification (GEV) framework to ensure rules are clear, violations are detectable, and compliance is independently verifiable. Technical mechanisms include tamper-proof FLOP caps, model locking, and offline licensing. Traceability tools track chips, components, and users across the compute supply chain. Regulatory mechanisms establish constraints through export controls, production caps, and licensing schemes. Unlike post-deployment oversight, this approach targets the material foundations of advanced AI development. Drawing from analogues ranging from nuclear non-proliferation to pandemic-era vaccine coordination, we demonstrate how compute can serve as a practical lever for global cooperation. While technical and political challenges remain, we argue that credible mechanisms already exist, and that the time to build this architecture is now, before the window for effective intervention closes.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.9669532775878906e-05, 'GPT4': 0.0005588531494140625, 'CLAUDE': 0.8759765625, 'GOOGLE': 4.500150680541992e-05, 'OPENAI_O_SERIES': 3.933906555175781e-06, 'DEEPSEEK': 0.123291015625, 'GROK': 1.7881393432617188e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 2.9802322387695312e-06, 'HUMAN': 7.212162017822266e-06}}"
2506.19775,review,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': ""Canary in the Mine: An LLM Augmented Survey of Disciplinary Complaints to the Ordre des ing\\'enieurs du Qu\\'ebec (OIQ)\n\nThis study uses pre-trained LLMs to conduct thematic analysis to investigate disciplinary incidents involving engineers in Quebec, shedding light on critical gaps in engineering education. Through a comprehensive review of the disciplinary register of the Ordre des ing\\'enieurs du Qu\\'ebec (OIQ)'s disciplinary register for 2010 to 2024, researchers from engineering education and human resources management in technological development laboratories conducted a thematic analysis of reported incidents to identify patterns, trends, and areas for improvement. The analysis aims to uncover the most common types of disciplinary incidents, underlying causes, and implications for the field in how engineering education addresses (or fails to address) these issues. Our findings identify recurring themes, analyze root causes, and offer recommendations for engineering educators and students to mitigate similar incidents. This research has implications for informing curriculum development, professional development, and performance evaluation, ultimately fostering a culture of professionalism and ethical responsibility in engineering. By providing empirical evidence of disciplinary incidents and their causes, this study contributes to evidence-based practices for engineering education and professional development, enhancing the engineering education community's understanding of professionalism and ethics."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.07574462890625, 'GPT4': 0.007328033447265625, 'CLAUDE': 0.00016951560974121094, 'GOOGLE': 0.88671875, 'OPENAI_O_SERIES': 0.0009508132934570312, 'DEEPSEEK': 6.4849853515625e-05, 'GROK': 2.014636993408203e-05, 'NOVA': 1.6033649444580078e-05, 'OTHER': 0.02886962890625, 'HUMAN': 0.00021088123321533203}}"
2506.17354,regular,post_llm,2025,6,"{'ai_likelihood': 6.205505794949002e-05, 'text': 'Evaluating the Impact of Lean and Green Practices on Operational Performance: A Real Data-Driven Simulation Case Study\n\nGlobal market-driven forces and customer needs are continuously changing. In the past, profitability and efficiency were the primary objectives of most companies. However, in recent decades, sustainable performance has emerged as a new competitive advantage. Companies have been compelled to adopt a concept that combines these evolving global interests with traditional goals resulting in the innovation of the lean and green approach.\n  In this study, a research methodology that includes system analysis and modeling procedures to apply the lean and green concept, combined with a new evaluation metric, the Overall Environmental Equipment Effectiveness (OEEE) was used to investigate the effects of adopting lean and green practices on overall performance.\n  A simulation model and energy value stream mapping were implemented, and the OEEE value was calculated to assess the current performance in terms of quality, availability, productivity, and sustainability. The current state production lead time was 329.1 minutes per batch, and the OEEE value was 13.1%. This result indicates existing issues in performance and sustainability, suggesting that improvement efforts should focus on enhancing these two aspects to increase the overall OEEE value.\n  Several improvement scenarios were proposed, including combining and rearranging the inspection workstations as the first scenario, and using UV lighting for drying purposes at the framing workstation as the second. After applying these improvements, both scenarios showed increased OEEE values and reduced lead times compared to the current state. In the first scenario, the lead time decreased to 158.23 minutes, and the OEEE increased to 35%. In the second scenario, the lead time was reduced to 292 minutes, with the OEEE increasing to 24%.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.22988,regular,post_llm,2025,6,"{'ai_likelihood': 6.4240561591254345e-06, 'text': '(World) Building Transformation: Students and Teachers as CoCreators in OpenXR Learning Environments\n\nEmerging extended reality (XR) tools and platforms offer an exciting opportunity to align learning experiences in higher education with the futures in which students will pursue their goals. However, the dynamic nature of XR as subject matter challenges hierarchies and classroom practices typical of higher education. This instructional design practice paper reflects on how our team of faculty, learning experience designers, and user experience (UX) researchers implemented human-centered design thinking, transformative learning, and problem-posing education to design and implement a special topics media entrepreneurship course in building the metaverse. By pairing our practitioner experience with learner personas, as well as survey, interview, and focus group responses from our learners, we narrate our design and its implications through a human-centered, reflective lens.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.14191,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'The Ethics of Generative AI in Anonymous Spaces: A Case Study of 4chan\'s /pol/ Board\n\nThis paper presents a characterization of AI-generated images shared on 4chan, examining how this anonymous online community is (mis-)using generative image technologies. Through a methodical data collection process, we gathered 900 images from 4chan\'s /pol/ (Politically Incorrect) board, which included the label ""/mwg/"" (memetic warfare general), between April and July 2024, identifying 66 unique AI-generated images. The analysis reveals concerning patterns in the use of this technology, with 69.7% of images including recognizable figures, 28.8% of images containing racist elements, 28.8% featuring anti-Semitic content, and 9.1% incorporating Nazi-related imagery.\n  Overall, we document how users are weaponizing generative AI to create extremist content, political commentary, and memes that often bypass conventional content moderation systems. This research highlights significant implications for platform governance, AI safety mechanisms, and broader societal impacts as generative AI technologies become increasingly accessible. The findings underscore the urgent need for enhanced safeguards in generative AI systems and more effective regulatory frameworks to mitigate potential harms while preserving innovation.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.8775463104248047e-05, 'GPT4': 0.07745361328125, 'CLAUDE': 0.92138671875, 'GOOGLE': 0.0006628036499023438, 'OPENAI_O_SERIES': 0.00033020973205566406, 'DEEPSEEK': 9.119510650634766e-05, 'GROK': 1.1920928955078125e-07, 'NOVA': 4.172325134277344e-07, 'OTHER': 6.556510925292969e-07, 'HUMAN': 4.941225051879883e-05}}"
2506.20299,regular,post_llm,2025,6,"{'ai_likelihood': 0.99755859375, 'text': 'Enhancing Programming Pair Workshops: The Case of Teacher Pre-Prompting\n\nThis paper explores the pedagogical potential of ""teacher pre-prompting"" as a means of guiding student collaboration in programming education. In particular, we investigate how brief teacher-initiated questions posed before students engage in pair programming workshops can help shape problem interpretation and division of labor. Based on qualitative analysis of video data from a university course in systems development, we identify five distinct pre-prompting patterns. Our findings suggest that such prompts can foster structured discussions, clarify task requirements, and create opportunities for shared learning experiences.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00620269775390625, 'GPT4': 0.25244140625, 'CLAUDE': 0.65234375, 'GOOGLE': 0.0823974609375, 'OPENAI_O_SERIES': 0.00021791458129882812, 'DEEPSEEK': 0.0012960433959960938, 'GROK': 7.033348083496094e-06, 'NOVA': 5.185604095458984e-06, 'OTHER': 0.0010356903076171875, 'HUMAN': 0.004302978515625}}"
2506.1656,regular,post_llm,2025,6,"{'ai_likelihood': 6.291601392957899e-07, 'text': ""External Evaluation of Discrimination Mitigation Efforts in Meta's Ad Delivery\n\nThe 2022 settlement between Meta and the U.S. Department of Justice to resolve allegations of discriminatory advertising resulted is a first-of-its-kind change to Meta's ad delivery system aimed to address algorithmic discrimination in its housing ad delivery. In this work, we explore direct and indirect effects of both the settlement's choice of terms and the Variance Reduction System (VRS) implemented by Meta on the actual reduction in discrimination.\n  We first show that the settlement terms allow for an implementation that does not meaningfully improve access to opportunities for individuals. The settlement measures impact of ad delivery in terms of impressions, instead of unique individuals reached by an ad; it allows the platform to level down access, reducing disparities by decreasing the overall access to opportunities; and it allows the platform to selectively apply VRS to only small advertisers.\n  We then conduct experiments to evaluate VRS with real-world ads, and show that while VRS does reduce variance, it also raises advertiser costs (measured per-individuals-reached), therefore decreasing user exposure to opportunity ads for a given ad budget. VRS thus passes the cost of decreasing variance to advertisers.\n  Finally, we explore an alternative approach to achieve the settlement goals, that is significantly more intuitive and transparent than VRS. We show our approach outperforms VRS by both increasing ad exposure for users from all groups and reducing cost to advertisers, thus demonstrating that the increase in cost to advertisers when implementing the settlement is not inevitable.\n  Our methodologies use a black-box approach that relies on capabilities available to any regular advertiser, rather than on privileged access to data, allowing others to reproduce or extend our work."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.02094,regular,post_llm,2025,6,"{'ai_likelihood': 0.006438361273871528, 'text': 'Generative AI for Multiple Choice STEM Assessments\n\nArtificial intelligence (AI) technology enables a range of enhancements in computer-aided instruction, from accelerating the creation of teaching materials to customizing learning paths based on learner outcomes. However, ensuring the mathematical accuracy and semantic integrity of generative AI output remains a significant challenge, particularly in Science, Technology, Engineering and Mathematics (STEM) disciplines. In this study, we explore the use of generative AI in which ""hallucinations"", typically viewed as undesirable inaccuracies, can instead serve a pedagogical purpose. Specifically, we investigate the generation of plausible but incorrect alternatives for multiple choice assessments, where credible distractors are essential for effective assessment design. We describe the Moebius platform for online instruction, with particular focus on its architecture for handling mathematical elements through specialized semantic packages that support dynamic, parameterized STEM content. We examine methods for crafting prompts that interact effectively with these mathematical semantics to guide the AI in generating high-quality multiple choice distractors. Finally, we demonstrate how this approach reduces the time and effort associated with creating robust teaching materials while maintaining academic rigor and assessment validity.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.21794,review,post_llm,2025,6,"{'ai_likelihood': 2.37425168355306e-05, 'text': ""Shifting Narratives: A Longitudinal Analysis of Media Trends and Public Attitudes on Homelessness\n\nWithin the field of media framing, homelessness has been a historically under-researched topic. Framing theory states that the media's method of presenting information plays a pivotal role in controlling public sentiment toward a topic. The sentiment held towards homeless individuals influences their ability to access jobs, housing, and resources as a result of discrimination. This study analyzes the topic and sentiment trends in related media articles to validate framing theory within the scope of homelessness. It correlates these shifts in media reporting with public sentiment. We examine state-level trends in California, Florida, Washington, Oregon, and New York from 2015 to 2023. We utilize the GDELT 2.0 Global Knowledge Graph (GKG) database to gather article data and use X to measure public sentiment towards homeless individuals. Additionally, to identify if there is a correlation between media reporting and public policy, we examine the media's impact on state-level legislation. Our research uses Granger-causality tests and vector autoregressive (VAR) models to establish a correlation between media framing and public sentiment. We also use latent Dirichlet allocation (LDA) and GPT-3.5 (LLM-as-annotator paradigm) for topic modeling and sentiment analysis. Our findings demonstrate a statistically significant correlation between media framing and public sentiment, especially in states with high homelessness rates. We found no significant correlation between media framing and legislation, suggesting a possible disconnect between public opinion and policy-making. These findings reveal the broader impact of the media's framing decisions and delineate its ability to affect society."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.04305,review,post_llm,2025,6,"{'ai_likelihood': 5.993578169080946e-06, 'text': 'Enduring Disparities in the Workplace: A Pilot Study in the AI Community\n\nIn efforts toward achieving responsible artificial intelligence (AI), fostering a culture of workplace transparency, diversity, and inclusion can breed innovation, trust, and employee contentment. In AI and Machine Learning (ML), such environments correlate with higher standards of responsible development. Without transparency, disparities, microaggressions and misconduct will remain unaddressed, undermining the very structural inequities responsible AI aims to mitigate. While prior work investigates workplace transparency and disparities in broad domains (e.g. science and technology, law) for specific demographic subgroups, it lacks in-depth and intersectional conclusions and a focus on the AI/ML community. To address this, we conducted a pilot survey of 1260 AI/ML professionals both in industry and academia across different axes, probing aspects such as belonging, performance, workplace Diversity, Equity and Inclusion (DEI) initiatives, accessibility, performance and compensation, microaggressions, misconduct, growth, and well-being. Results indicate enduring disparities in workplace experiences for underrepresented and/or marginalized subgroups. In particular, we highlight that accessibility remains an important challenge for a positive work environment and that disabled employees have a worse workplace experience than their non-disabled colleagues. We further surface disparities for intersectional groups and discuss how the implementation of DEI initiatives may differ from their perceived impact on the workplace. This study is a first step towards increasing transparency and informing AI/ML practitioners and organizations with empirical results. We aim to foster equitable decision-making in the design and evaluation of organizational policies and provide data that may empower professionals to make more informed choices of prospective workplaces.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.17741,regular,post_llm,2025,6,"{'ai_likelihood': 0.2821180555555556, 'text': 'Experimental Evidence for the Propagation and Preservation of Machine Discoveries in Human Populations\n\nIntelligent machines with superhuman capabilities have the potential to uncover problem-solving strategies beyond human discovery. Emerging evidence from competitive gameplay, such as Go and chess, demonstrates that AI systems are evolving from mere tools to sources of cultural innovation adopted by humans. However, the conditions under which intelligent machines transition from tools to drivers of persistent cultural change remain unclear. We identify three key conditions for machines to fundamentally influence human problem-solving: the discovered strategies must be non-trivial, learnable, and offer a clear advantage. Using a cultural transmission experiment and an agent-based simulation, we demonstrate that when these conditions are met, machine-discovered strategies can be transmitted, understood, and preserved by human populations, leading to enduring cultural shifts. These findings provide a framework for understanding how machines can persistently expand human cognitive skills and underscore the need to consider their broader implications for human cognition and cultural evolution.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.17808,review,post_llm,2025,6,"{'ai_likelihood': 6.059805552164714e-06, 'text': 'The value of human and machine in machine-generated creative contents\n\nThe seemingly ""imagination"" and ""creativity"" from machine-generated contents should not be misattributed to the accomplishment of machine. They are accomplishments of both human and machine. Without human interpretation, the machine-generated contents remain in the imaginary space of the large language models, and cannot automatically establish grounding in the reality and human experience.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.15572,review,post_llm,2025,6,"{'ai_likelihood': 6.291601392957899e-06, 'text': ""Misinformation by Omission: The Need for More Environmental Transparency in AI\n\nIn recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational power and natural resources. In parallel to this trend, transparency around the costs and impacts of these models has decreased, meaning that the users of these technologies have little to no information about their resource demands and subsequent impacts on the environment. Despite this dearth of adequate data, escalating demand for figures quantifying AI's environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized best-effort estimates of greenhouse gas emissions. In this article, we explore pervasive myths and misconceptions shaping public understanding of AI's environmental impacts, tracing their origins and their spread in both the media and scientific publications. We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and conclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate negative impacts in the future."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.09185,regular,post_llm,2025,6,"{'ai_likelihood': 0.99755859375, 'text': 'Whole-Person Education for AI Engineers\n\nThis autoethnographic study explores the need for interdisciplinary education spanning both technical and philosophical skills - as such, this study leverages whole-person education as a theoretical approach needed in AI engineering education to address the limitations of current paradigms that prioritize technical expertise over ethical and societal considerations. Drawing on a collaborative autoethnography approach of fourteen diverse stakeholders, the study identifies key motivations driving the call for change, including the need for global perspectives, bridging the gap between academia and industry, integrating ethics and societal impact, and fostering interdisciplinary collaboration. The findings challenge the myths of technological neutrality and technosaviourism, advocating for a future where AI engineers are equipped not only with technical skills but also with the ethical awareness, social responsibility, and interdisciplinary understanding necessary to navigate the complex challenges of AI development. The study provides valuable insights and recommendations for transforming AI engineering education to ensure the responsible development of AI technologies.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.004993438720703125, 'GPT4': 0.0005207061767578125, 'CLAUDE': 0.0155792236328125, 'GOOGLE': 0.916015625, 'OPENAI_O_SERIES': 5.340576171875e-05, 'DEEPSEEK': 0.00014460086822509766, 'GROK': 2.6226043701171875e-05, 'NOVA': 0.0001785755157470703, 'OTHER': 0.061737060546875, 'HUMAN': 0.0005116462707519531}}"
2506.09632,regular,post_llm,2025,6,"{'ai_likelihood': 9.139378865559896e-06, 'text': ""Ties of Trust: a bowtie model to uncover trustor-trustee relationships in LLMs\n\nThe rapid and unprecedented dominance of Artificial Intelligence (AI), particularly through Large Language Models (LLMs), has raised critical trust challenges in high-stakes domains like politics. Biased LLMs' decisions and misinformation undermine democratic processes, and existing trust models fail to address the intricacies of trust in LLMs. Currently, oversimplified, one-directional approaches have largely overlooked the many relationships between trustor (user) contextual factors (e.g. ideology, perceptions) and trustee (LLMs) systemic elements (e.g. scientists, tool's features). In this work, we introduce a bowtie model for holistically conceptualizing and formulating trust in LLMs, with a core component comprehensively exploring trust by tying its two sides, namely the trustor and the trustee, as well as their intricate relationships. We uncover these relationships within the proposed bowtie model and beyond to its sociotechnical ecosystem, through a mixed-methods explanatory study, that exploits a political discourse analysis tool (integrating ChatGPT), by exploring and responding to the next critical questions: 1) How do trustor's contextual factors influence trust-related actions? 2) How do these factors influence and interact with trustee systemic elements? 3) How does trust itself vary across trustee systemic elements? Our bowtie-based explanatory analysis reveals that past experiences and familiarity significantly shape trustor's trust-related actions; not all trustor contextual factors equally influence trustee systemic elements; and trustee's human-in-the-loop features enhance trust, while lack of transparency decreases it. Finally, this solid evidence is exploited to deliver recommendations, insights and pathways towards building robust trusting ecosystems in LLM-based solutions."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.17355,regular,post_llm,2025,6,"{'ai_likelihood': 7.384353213840061e-06, 'text': ""PasteTrace: A Single Source Plagiarism Detection Tool For Introductory Programming Courses\n\nIntroductory Computer Science classes are important for laying the foundation for advanced programming courses. However, students without prior programming experience may find these courses challenging, leading to difficulties in understanding concepts and engaging in academic dishonesty such as plagiarism. While there exists plagiarism detection techniques and tools, not all of them are suitable for academic settings, especially in introductory programming courses. This paper introduces PasteTrace, a novel open-source plagiarism detection tool designed specifically for introductory programming courses. Unlike traditional methods, PasteTrace operates within an Integrated Development Environment that tracks the student's coding activities in real-time for evidence of plagiarism. Our evaluation of PasteTrace in two introductory programming courses demonstrates the tool's ability to provide insights into student behavior and detect various forms of plagiarism, outperforming an existing well-established tool.\n  A video demonstration of PasteTrace and its source code, and case study data are made available at https://doi.org/10.6084/m9.figshare.27115852"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.03497,review,post_llm,2025,6,"{'ai_likelihood': 5.132622188991971e-06, 'text': ""Bridging the Artificial Intelligence Governance Gap: The United States' and China's Divergent Approaches to Governing General-Purpose Artificial Intelligence\n\nThe United States and China are among the world's top players in the development of advanced artificial intelligence (AI) systems, and both are keen to lead in global AI governance and development. A look at U.S. and Chinese policy landscapes reveals differences in how the two countries approach the governance of general-purpose artificial intelligence (GPAI) systems. Three areas of divergence are notable for policymakers: the focus of domestic AI regulation, key principles of domestic AI regulation, and approaches to implementing international AI governance. As AI development continues, global conversation around AI has warned of global safety and security challenges posed by GPAI systems. Cooperation between the United States and China might be needed to address these risks, and understanding the implications of these differences might help address the broader challenges for international cooperation between the United States and China on AI safety and security."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.20433,regular,post_llm,2025,6,"{'ai_likelihood': 6.059805552164714e-06, 'text': ""That's Not the Feedback I Need! -- Student Engagement with GenAI Feedback in the Tutor Kai\n\nThe potential of Generative AI (GenAI) for generating feedback in computing education has been the subject of numerous studies. However, there is still limited research on how computing students engage with this feedback and to what extent it supports their problem-solving. For this reason, we built a custom web application providing students with Python programming tasks, a code editor, GenAI feedback, and compiler feedback. Via a think-aloud protocol including eye-tracking and a post-interview with 11 undergraduate students, we investigate (1) how much attention the generated feedback received from learners and (2) to what extent the generated feedback is helpful (or not). In addition, students' attention to GenAI feedback is compared with that towards the compiler feedback. We further investigate differences between students with and without prior programming experience. The findings indicate that GenAI feedback generally receives a lot of visual attention, with inexperienced students spending twice as much fixation time. More experienced students requested GenAI less frequently, and could utilize it better to solve the given problem. It was more challenging for inexperienced students to do so, as they could not always comprehend the GenAI feedback. They often relied solely on the GenAI feedback, while compiler feedback was not read. Understanding students' attention and perception toward GenAI feedback is crucial for developing educational tools that support student learning."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.11645,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': ""Expert Insight-Based Modeling of Non-Kinetic Strategic Deterrence of Rare Earth Supply Disruption:A Simulation-Driven Systematic Framework\n\nThis study constructs a quantifiable modelling framework to simulate non-kinetic strategic deterrence pathways in rare earth supply disruption scenarios, based on structured responses from expert interviews led by Dr. Daniel O'Connor, CEO of the Rare Earth Exchange (REE). Focusing on disruption impacts on national security systems, the study proposes four core modelling components: Security Critical Zones (SCZ), Strategic Signal Injection Function (SSIF), System-Capability Migration Function (SCIF), and Policy-Capability Transfer Function (PCTF). The framework integrates parametric ODEs, segmented function modelling, path-overlapping covariance matrices, and LSTM networks to simulate nonlinear suppression trajectories triggered by regime signals. Data is derived from expert interviews and scenario analyses centered on U.S.-China dynamics in ISR, electronic warfare, and rare earth control. Results show institutional signals have strong tempo and path-coupling effects, capable of causing rapid degradation of strategic capabilities. The model is adaptable across national resource frameworks and extendable to AI sandbox engines for situational simulation and counterfactual reasoning. This research introduces the first unified system for modelling, visualizing, and forecasting non-kinetic deterrence, offering methodological support to policymakers and analysts navigating institutionalized strategic competition."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.494190216064453e-05, 'GPT4': 0.44970703125, 'CLAUDE': 0.404541015625, 'GOOGLE': 0.00017845630645751953, 'OPENAI_O_SERIES': 9.512901306152344e-05, 'DEEPSEEK': 0.1376953125, 'GROK': 1.1920928955078125e-07, 'NOVA': 2.384185791015625e-07, 'OTHER': 5.602836608886719e-06, 'HUMAN': 0.00775146484375}}"
2506.10217,review,post_llm,2025,6,"{'ai_likelihood': 2.185503641764323e-06, 'text': 'Data-Centric Safety and Ethical Measures for Data and AI Governance\n\nDatasets play a key role in imparting advanced capabilities to artificial intelligence (AI) foundation models that can be adapted to various downstream tasks. These downstream applications can introduce both beneficial and harmful capabilities -- resulting in dual use AI foundation models, with various technical and regulatory approaches to monitor and manage these risks. However, despite the crucial role of datasets, responsible dataset design and ensuring data-centric safety and ethical practices have received less attention. In this study, we pro-pose responsible dataset design framework that encompasses various stages in the AI and dataset lifecycle to enhance safety measures and reduce the risk of AI misuse due to low quality, unsafe and unethical data content. This framework is domain agnostic, suitable for adoption for various applications and can promote responsible practices in dataset creation, use, and sharing to facilitate red teaming, minimize risks, and increase trust in AI models.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.15867,review,post_llm,2025,6,"{'ai_likelihood': 9.934107462565105e-08, 'text': ""Mechanisms to Verify International Agreements About AI Development\n\nInternational agreements about AI development may be required to reduce catastrophic risks from advanced AI systems. However, agreements about such a high-stakes technology must be backed by verification mechanisms--processes or tools that give one party greater confidence that another is following the agreed-upon rules, typically by detecting violations. This report gives an overview of potential verification approaches for three example policy goals, aiming to demonstrate how countries could practically verify claims about each other's AI development and deployment. The focus is on international agreements and state-involved AI development, but these approaches could also be applied to domestic regulation of companies. While many of the ideal solutions for verification are not yet technologically feasible, we emphasize that increased access (e.g., physical inspections of data centers) can often substitute for these technical approaches. Therefore, we remain hopeful that significant political will could enable ambitious international coordination, with strong verification mechanisms, to reduce catastrophic AI risks."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.09178,regular,post_llm,2025,6,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'Understanding Self-Regulated Learning Behavior Among High and Low Dropout Risk Students During CS1: Combining Trace Logs, Dropout Prediction and Self-Reports\n\nThe introductory programming course (CS1) at the university level is often perceived as particularly challenging, contributing to high dropout rates among Computer Science students. Identifying when and how students encounter difficulties in this course is critical for providing targeted support. This study explores the behavioral patterns of CS1 students at varying dropout risks using self-regulated learning (SRL) as the theoretical framework. Using learning analytics, we analyzed trace logs and task performance data from a virtual learning environment to map resource usage patterns and used student dropout prediction to distinguish between low and high dropout risk behaviors. Data from 47 consenting students were used to carry out the analysis. Additionally, self-report questionnaires from 29 participants enriched the interpretation of observed patterns. The findings reveal distinct weekly learning strategy types and categorize course behavior. Among low dropout risk students, three learning strategies were identified that different in how students prioritized completing tasks and reading course materials. High dropout risk students exhibited nine different strategies, some representing temporary unsuccessful strategies that can be recovered from, while others indicating behaviors of students on the verge of dropping out. This study highlights the value of combining student behavior profiling with predictive learning analytics to explain dropout predictions and devise targeted interventions. Practical findings of the study can in turn be used to help teachers, teaching assistants and other practitioners to better recognize and address students at the verge of dropping out.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.08117,regular,post_llm,2025,6,"{'ai_likelihood': 1.4603137969970703e-05, 'text': 'We Are AI: Taking Control of Technology\n\nResponsible AI (RAI) is the science and practice of ensuring the design, development, use, and oversight of AI are socially sustainable--benefiting diverse stakeholders while controlling the risks. Achieving this goal requires active engagement and participation from the broader public. This paper introduces ""We are AI: Taking Control of Technology,"" a public education course that brings the topics of AI and RAI to the general audience in a peer-learning setting.\n  We outline the goals behind the course\'s development, discuss the multi-year iterative process that shaped its creation, and summarize its content. We also discuss two offerings of We are AI to an active and engaged group of librarians and professional staff at New York University, highlighting successes and areas for improvement. The course materials, including a multilingual comic book series by the same name, are publicly available and can be used independently. By sharing our experience in creating and teaching We are AI, we aim to introduce these resources to the community of AI educators, researchers, and practitioners, supporting their public education efforts.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.14679,regular,post_llm,2025,6,"{'ai_likelihood': 0.00018941031561957466, 'text': ""Now More Than Ever, Foundational AI Research and Infrastructure Depends on the Federal Government\n\nLeadership in the field of AI is vital for our nation's economy and security. Maintaining this leadership requires investments by the federal government. The federal investment in foundation AI research is essential for U.S. leadership in the field. Providing accessible AI infrastructure will benefit everyone. Now is the time to increase the federal support, which will be complementary to, and help drive, the nation's high-tech industry investments."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.1168,regular,post_llm,2025,6,"{'ai_likelihood': 1.3444158766004775e-05, 'text': ""Malicious LLM-Based Conversational AI Makes Users Reveal Personal Information\n\nLLM-based Conversational AIs (CAIs), also known as GenAI chatbots, like ChatGPT, are increasingly used across various domains, but they pose privacy risks, as users may disclose personal information during their conversations with CAIs. Recent research has demonstrated that LLM-based CAIs could be used for malicious purposes. However, a novel and particularly concerning type of malicious LLM application remains unexplored: an LLM-based CAI that is deliberately designed to extract personal information from users.\n  In this paper, we report on the malicious LLM-based CAIs that we created based on system prompts that used different strategies to encourage disclosures of personal information from users. We systematically investigate CAIs' ability to extract personal information from users during conversations by conducting a randomized-controlled trial with 502 participants. We assess the effectiveness of different malicious and benign CAIs to extract personal information from participants, and we analyze participants' perceptions after their interactions with the CAIs. Our findings reveal that malicious CAIs extract significantly more personal information than benign CAIs, with strategies based on the social nature of privacy being the most effective while minimizing perceived risks. This study underscores the privacy threats posed by this novel type of malicious LLM-based CAIs and provides actionable recommendations to guide future research and practice."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.04954,review,post_llm,2025,6,"{'ai_likelihood': 1.158979203965929e-06, 'text': ""The Data Dilemma: Authors' Intentions and Recognition of Research Data in Educational Technology Research\n\nEducational Technology (EdTec) research is conducted by multiple disciplines, some of which annually meet at the DELFI conference. Due to the heterogeneity of involved researchers and communities, it is our goal to identify categories of research data overseen in the context of EdTec research. Therefore, we analyze the author's perspective provided via EasyChair where authors specified whether they had research data to share. We compared this information with an analysis of the submitted articles and the contained research data. We found that not all research data was recognized as such by the authors, especially software and qualitative data, indicating a prevailing lack of awareness, and other potential barriers. In addition, we analyze the 2024 DELFI proceedings to learn what kind of data was subject to research, and where it is published. This work has implications for training future generations of EdTec researchers. It further stresses the need for guidelines and recognition of research data publications (particularly software, and qualitative data)."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.09771,regular,post_llm,2025,6,"{'ai_likelihood': 0.002780490451388889, 'text': 'Where Journalism Silenced Voices: Exploring Discrimination in the Representation of Indigenous Communities in Bangladesh\n\nIn this paper, we examine the intersections of indigeneity and media representation in shaping perceptions of indigenous communities in Bangladesh. Using a mixed-methods approach, we combine quantitative analysis of media data with qualitative insights from focus group discussions (FGD). First, we identify a total of 4,893 indigenous-related articles from our initial dataset of 2.2 million newspaper articles, using a combination of keyword-based filtering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From manually inspecting 3 prominent Bangla newspapers, we identify 15 genres that we use as our topics for semi-supervised topic modeling using CorEx. Results show indigenous news articles have higher representation of culture and entertainment (19%, 10% higher than general news articles), and a disproportionate focus on conflict and protest (9%, 7% higher than general news). On the other hand, sentiment analysis reveals that 57% of articles on indigenous topics carry a negative tone, compared to 27% for non-indigenous related news. Drawing from communication studies, we further analyze framing, priming, and agenda-setting (frequency of themes) to support the case for discrimination in representation of indigenous news coverage. For the qualitative part of our analysis, we facilitated FGD, where participants further validated these findings. Participants unanimously expressed their feeling of being under-represented, and that critical issues affecting their communities (such as education, healthcare, and land rights) are systematically marginalized in news media coverage. By highlighting 8 cases of discrimination and media misrepresentation that were frequently mentioned by participants in the FGD, this study emphasizes the urgent need for more equitable media practices that accurately reflect the experiences and struggles of marginalized communities.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.17303,review,post_llm,2025,6,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'The California Report on Frontier AI Policy\n\nThe innovations emerging at the frontier of artificial intelligence (AI) are poised to create historic opportunities for humanity but also raise complex policy challenges. Continued progress in frontier AI carries the potential for profound advances in scientific discovery, economic productivity, and broader social well-being. As the epicenter of global AI innovation, California has a unique opportunity to continue supporting developments in frontier AI while addressing substantial risks that could have far reaching consequences for the state and beyond. This report leverages broad evidence, including empirical research, historical analysis, and modeling and simulations, to provide a framework for policymaking on the frontier of AI development. Building on this multidisciplinary approach, this report derives policy principles that can inform how California approaches the use, assessment, and governance of frontier AI: principles rooted in an ethos of trust but verify. This approach takes into account the importance of innovation while establishing appropriate strategies to reduce material risks.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.17311,review,post_llm,2025,6,"{'ai_likelihood': 1.4768706427680123e-05, 'text': 'Can Large Language Models Be Trusted Paper Reviewers? A Feasibility Study\n\nAcademic paper review typically requires substantial time, expertise, and human resources. Large Language Models (LLMs) present a promising method for automating the review process due to their extensive training data, broad knowledge base, and relatively low usage cost. This work explores the feasibility of using LLMs for academic paper review by proposing an automated review system. The system integrates Retrieval Augmented Generation (RAG), the AutoGen multi-agent system, and Chain-of-Thought prompting to support tasks such as format checking, standardized evaluation, comment generation, and scoring. Experiments conducted on 290 submissions from the WASA 2024 conference using GPT-4o show that LLM-based review significantly reduces review time (average 2.48 hours) and cost (average \\$104.28 USD). However, the similarity between LLM-selected papers and actual accepted papers remains low (average 38.6\\%), indicating issues such as hallucination, lack of independent judgment, and retrieval preferences. Therefore, it is recommended to use LLMs as assistive tools to support human reviewers, rather than to replace them.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.21066,review,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'Digital Sovereigns Big Tech and Nation-State Influence\n\nTechnology companies have gained unprecedented power and influence in recent years, resembling quasi-nation-states globally. Corporations with trillion-dollar market capitalizations are no longer just providers of digital services; they now wield immense economic power, influence global infrastructure, and significantly impact political and social dynamics. This thesis examines how these corporations have transcended traditional business models, adopting characteristics typically associated with sovereign states. They now enforce regulations, shape public discourse, and influence legal frameworks in various countries. This shift presents unique challenges, including the undermining of democratic governance, the exacerbation of economic inequalities, and the enabling of unregulated data exploitation and privacy violations. The study will examine critical instances of tech companies acting as quasi-governmental bodies and assess the risks associated with unchecked corporate influence in global governance. Ultimately, the thesis aims to propose policy frameworks and regulatory interventions to curb the overreach of tech giants, restoring the balance between democratic institutions and corporate power and ensuring that the digital future aligns with the public good rather than creating Frankenstein-like monsters.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0014486312866210938, 'GPT4': 0.7880859375, 'CLAUDE': 0.0091705322265625, 'GOOGLE': 0.1817626953125, 'OPENAI_O_SERIES': 0.019378662109375, 'DEEPSEEK': 1.2874603271484375e-05, 'GROK': 1.9073486328125e-05, 'NOVA': 2.8848648071289062e-05, 'OTHER': 5.143880844116211e-05, 'HUMAN': 8.58306884765625e-06}}"
2506.1468,regular,post_llm,2025,6,"{'ai_likelihood': 0.4947916666666667, 'text': 'Which Humans? Inclusivity and Representation in Human-Centered AI\n\nAs AI systems continue to spread and become integrated into many aspects of society, the concept of ""human-centered AI"" has gained increasing prominence, raising the critical question of which humans are the AI systems to be centered around.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.0383,regular,post_llm,2025,6,"{'ai_likelihood': 0.6704101562499999, 'text': 'Construction of Urban Greenland Resources Collaborative Management Platform\n\nNowadays, environmental protection has become a global consensus. At the same time, with the rapid development of science and technology, urbanisation has become a phenomenon that has become the norm. Therefore, the urban greening management system is an essential component in protecting the urban environment. The system utilises a transparent management process known as"" monitoring - early warning - response - optimisation,"" which enhances the tracking of greening resources, streamlines maintenance scheduling, and encourages employee involvement in planning. Designed with a microservice architecture, the system can improve the utilisation of greening resources by 30%, increase citizen satisfaction by 20%, and support carbon neutrality objectives, ultimately making urban governance more intelligent and focused on the community. The Happy City Greening Management System effectively manages gardeners, trees, flowers, and green spaces. It comprises modules for gardener management, purchase and supplier management, tree and flower management, and maintenance planning. Its automation feature allows for real-time updates of greening data, thereby enhancing decision-making. The system is built using Java for the backend and MySQL for data storage, complemented by a user-friendly frontend designed with the Vue framework. Additionally, it leverages features from the Spring Boot framework to enhance maintainability and scalability.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.104248046875, 'GPT4': 0.10113525390625, 'CLAUDE': 0.0010423660278320312, 'GOOGLE': 0.63720703125, 'OPENAI_O_SERIES': 0.0149383544921875, 'DEEPSEEK': 0.0001310110092163086, 'GROK': 6.240606307983398e-05, 'NOVA': 0.00033593177795410156, 'OTHER': 0.00269317626953125, 'HUMAN': 0.1383056640625}}"
2506.06193,review,post_llm,2025,6,"{'ai_likelihood': 0.10321723090277778, 'text': ""Validation of the Critical Reflection and Agency in Computing Index: Do Computing Ethics Courses Make a Difference?\n\nComputing ethics education aims to develop students' critical reflection and agency. We need validated ways to measure whether our efforts succeed. Through two survey administrations (N=474, N=464) with computing students and professionals, we provide evidence for the validity of the Critical Reflection and Agency in Computing Index. Our psychometric analyses demonstrate distinct dimensions of ethical development and show strong reliability and construct validity. Participants who completed computing ethics courses showed higher scores in some dimensions of ethical reflection and agency, but they also exhibited stronger techno-solutionist beliefs, highlighting a challenge in current pedagogy. This validated instrument enables systematic measurement of how computing students develop critical consciousness, allowing educators to better understand how to prepare computing professionals to tackle ethical challenges in their work."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.11767,regular,post_llm,2025,6,"{'ai_likelihood': 0.990234375, 'text': 'Designing Effective LLM-Assisted Interfaces for Curriculum Development\n\nLarge Language Models (LLMs) have the potential to transform the way a dynamic curriculum can be delivered. However, educators face significant challenges in interacting with these models, particularly due to complex prompt engineering and usability issues, which increase workload. Additionally, inaccuracies in LLM outputs can raise issues around output quality and ethical concerns in educational content delivery. Addressing these issues requires careful oversight, best achieved through cooperation between human and AI approaches. This paper introduces two novel User Interface (UI) designs, UI Predefined and UI Open, both grounded in Direct Manipulation (DM) principles to address these challenges. By reducing the reliance on intricate prompt engineering, these UIs improve usability, streamline interaction, and lower workload, providing a more effective pathway for educators to engage with LLMs. In a controlled user study with 20 participants, the proposed UIs were evaluated against the standard ChatGPT interface in terms of usability and cognitive load. Results showed that UI Predefined significantly outperformed both ChatGPT and UI Open, demonstrating superior usability and reduced task load, while UI Open offered more flexibility at the cost of a steeper learning curve. These findings underscore the importance of user-centered design in adopting AI-driven tools and lay the foundation for more intuitive and efficient educator-LLM interactions in online learning environments.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.5854835510253906e-05, 'GPT4': 0.99609375, 'CLAUDE': 9.316205978393555e-05, 'GOOGLE': 0.0027103424072265625, 'OPENAI_O_SERIES': 0.0009622573852539062, 'DEEPSEEK': 4.869699478149414e-05, 'GROK': 1.7881393432617188e-07, 'NOVA': 2.384185791015625e-07, 'OTHER': 6.079673767089844e-06, 'HUMAN': 0.00014340877532958984}}"
2506.07282,review,post_llm,2025,6,"{'ai_likelihood': 2.7418136596679688e-05, 'text': 'Adultification Bias in LLMs and Text-to-Image Models\n\nThe rapid adoption of generative AI models in domains such as education, policing, and social media raises significant concerns about potential bias and safety issues, particularly along protected attributes, such as race and gender, and when interacting with minors. Given the urgency of facilitating safe interactions with AI systems, we study bias along axes of race and gender in young girls. More specifically, we focus on ""adultification bias,"" a phenomenon in which Black girls are presumed to be more defiant, sexually intimate, and culpable than their White peers. Advances in alignment techniques show promise towards mitigating biases but vary in their coverage and effectiveness across models and bias types. Therefore, we measure explicit and implicit adultification bias in widely used LLMs and text-to-image (T2I) models, such as OpenAI, Meta, and Stability AI models. We find that LLMs exhibit explicit and implicit adultification bias against Black girls, assigning them harsher, more sexualized consequences in comparison to their White peers. Additionally, we find that T2I models depict Black girls as older and wearing more revealing clothing than their White counterparts, illustrating how adultification bias persists across modalities. We make three key contributions: (1) we measure a new form of bias in generative AI models, (2) we systematically study adultification bias across modalities, and (3) our findings emphasize that current alignment methods are insufficient for comprehensively addressing bias. Therefore, new alignment methods that address biases such as adultification are needed to ensure safe and equitable AI deployment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.00922,review,post_llm,2025,6,"{'ai_likelihood': 0.7241210937499999, 'text': 'Integrating Emerging Technologies in Virtual Learning Environments: A Comparative Study of Perceived Needs among Open Universities in Five Southeast Asian Countries\n\nAmid the growing need to keep learners abreast of rapid technological advancements brought about by the Fourth Industrial Revolution, this study explores perceived needs of students in virtual learning environments supported by emerging technologies. A survey was conducted across five leading open universities in Southeast Asia. The study aimed to identify student preferences regarding features of their virtual learning environments that could better prepare them as productive citizens and professionals. Findings indicate strong interest in interactive books and learning analytics, underscoring the importance of enhancing learner engagement and data-informed instruction. The results inform the development of a strategic roadmap to guide open universities in prioritizing technological and pedagogical innovations aligned with the evolving expectations of digital-age learners.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.00423431396484375, 'GPT4': 0.5498046875, 'CLAUDE': 0.1380615234375, 'GOOGLE': 0.206787109375, 'OPENAI_O_SERIES': 0.012298583984375, 'DEEPSEEK': 0.0035800933837890625, 'GROK': 3.606081008911133e-05, 'NOVA': 0.00011068582534790039, 'OTHER': 0.0032215118408203125, 'HUMAN': 0.08184814453125}}"
2506.09746,regular,post_llm,2025,6,"{'ai_likelihood': 0.00028954611884223093, 'text': ""TikTok's Research API: Problems Without Explanations\n\nFollowing the Digital Services Act of 2023, which requires Very Large Online Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) to facilitate data accessibility for independent research, TikTok augmented its Research API access within Europe in July 2023. This action was intended to ensure compliance with the DSA, bolster transparency, and address systemic risks. Nonetheless, research findings reveal that despite this expansion, notable limitations and inconsistencies persist within the data provided. Our experiment reveals that the API fails to provide metadata for one in eight videos provided through data donations, including official TikTok videos, advertisements, and content from specific accounts, without an apparent reason. The API data is incomplete, making it unreliable when working with data donations, a prominent methodology for algorithm audits and research on platform accountability. To monitor the functionality of the API and eventual fixes implemented by TikTok, we publish a dashboard with a daily check of the availability of 10 videos that were not retrievable in the last month. The video list includes very well-known accounts, notably that of Taylor Swift. The current API lacks the necessary capabilities for thorough independent research and scrutiny. It is crucial to support and safeguard researchers who utilize data scraping to independently validate the platform's data quality."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.2227,regular,post_llm,2025,6,"{'ai_likelihood': 0.0021913316514756945, 'text': ""Public Service Algorithm: towards a transparent, explainable, and scalable content curation for news content based on editorial values\n\nThe proliferation of disinformation challenges traditional, unscalable editorial processes and existing automated systems that prioritize engagement over public service values. To address this, we introduce the Public Service Algorithm (PSA), a novel framework using Large Language Models (LLMs) for scalable, transparent content curation based on Public Service Media (PSM) inspired values. Utilizing a large multilingual news dataset from the 'A European Perspective' project, our experiment directly compared article ratings from a panel of experienced editors from various European PSMs, with those from several LLMs, focusing on four criteria: diversity, in-depth analysis, forward-looking, and cross-border relevance. Utilizing criterion-specific prompts, our results indicate a promising alignment between human editorial judgment and LLM assessments, demonstrating the potential of LLMs to automate value-driven curation at scale without sacrificing transparency. This research constitutes a first step towards a scalable framework for the automatic curation of trustworthy news content."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.13385,regular,post_llm,2025,6,"{'ai_likelihood': 0.16153971354166669, 'text': 'pySpainMobility: a Python Package to Access and Manage Spanish Open Mobility Data\n\nMobility patterns play a critical role in a wide range of societal challenges, from epidemic modeling and emergency response to transportation planning and regional development. Yet, access to high-quality, timely, and openly available mobility data remains limited. In response, the Spanish Ministry of Transportation and Sustainable Mobility has released daily mobility datasets based on anonymized mobile phone data, covering districts, municipalities, and greater urban areas from February 2020 to June 2021 and again from January 2022 onward. This paper presents pySpainMobility, a Python package that simplifies access to these datasets and their associated study areas through a standardized, well-documented interface. By lowering the technical barrier to working with large-scale mobility data, the package enables reproducible analysis and supports applications across research, policy, and operational domains. The library is available at https://github.com/pySpainMobility.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.10057,regular,post_llm,2025,6,"{'ai_likelihood': 4.675653245713976e-05, 'text': 'Inverted Classroom in der Einf\\""uhrungsveranstaltung Programmierung\n\nTraditionally, the introductory programming course for computer science students at Nuremberg Tech had been implemented as a combination of lectures and exercise sessions. Due to high failure rates in the winter semester 2023/24, an experimental teaching concept based on the inverted classroom was implemented for one cohort in the winter semester 2024/25. Students had to prepare themselves through literature work and activating teaching and learning methods. The course was accompanied by a series of data collections (i.e., a Teaching Analysis Poll, two surveys, and a teaching diary) to gain insights into students\' learning methods and behaviors. The concept was evaluated positively overall, although many detailed opportunities for improvement were identified. In this article, we document the results of the surveys and discuss the implications.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2506.16952,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'Modeling and Visualization Reasoning for Stakeholders in Education and Industry Integration Systems: Research on Structured Synthetic Dialogue Data Generation Based on NIST Standards\n\nThis study addresses the structural complexity and semantic ambiguity in stakeholder interactions within the Education-Industry Integration (EII) system. The scarcity of real interview data, absence of structured variable modeling, and lack of interpretability in inference mechanisms have limited the analytical accuracy and policy responsiveness of EII research. To resolve these challenges, we propose a structural modeling paradigm based on the National Institute of Standards and Technology (NIST) synthetic data quality framework, focusing on consistency, authenticity, and traceability. We design a five-layer architecture that includes prompt-driven synthetic dialogue generation, a structured variable system covering skills, institutional, and emotional dimensions, dependency and causal path modeling, graph-based structure design, and an interactive inference engine. Empirical results demonstrate the effectiveness of the approach using a 15-segment synthetic corpus, with 41,597 tokens, 127 annotated variables, and 820 semantic relationship triples. The model exhibits strong structural consistency (Krippendorff alpha = 0.83), construct validity (RMSEA = 0.048, CFI = 0.93), and semantic alignment (mean cosine similarity > 0.78 via BERT). A key causal loop is identified: system mismatch leads to emotional frustration, reduced participation, skill gaps, and recurrence of mismatch, revealing a structural degradation cycle. This research introduces the first NIST-compliant AI modeling framework for stakeholder systems and provides a foundation for policy simulation, curriculum design, and collaborative strategy modeling.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 0.000965118408203125, 'CLAUDE': 0.0008158683776855469, 'GOOGLE': 1.1920928955078125e-06, 'OPENAI_O_SERIES': 2.384185791015625e-07, 'DEEPSEEK': 0.998046875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 1.1920928955078125e-07}}"
2506.04975,regular,post_llm,2025,6,"{'ai_likelihood': 1.0, 'text': 'Evaluating Prompt-Driven Chinese Large Language Models: The Influence of Persona Assignment on Stereotypes and Safeguards\n\nRecent research has highlighted that assigning specific personas to large language models (LLMs) can significantly increase harmful content generation. Yet, limited attention has been given to persona-driven toxicity in non-Western contexts, particularly in Chinese-based LLMs. In this paper, we perform a large-scale, systematic analysis of how persona assignment influences refusal behavior and response toxicity in Qwen, a widely-used Chinese language model. Utilizing fine-tuned BERT classifiers and regression analysis, our study reveals significant gender biases in refusal rates and demonstrates that certain negative personas can amplify toxicity toward Chinese social groups by up to 60-fold compared to the default model. To mitigate this toxicity, we propose an innovative multi-model feedback strategy, employing iterative interactions between Qwen and an external evaluator, which effectively reduces toxic outputs without costly model retraining. Our findings emphasize the necessity of culturally specific analyses for LLMs safety and offer a practical framework for evaluating and enhancing ethical alignment in LLM-generated content.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.331899642944336e-05, 'GPT4': 0.9423828125, 'CLAUDE': 0.029266357421875, 'GOOGLE': 0.0101165771484375, 'OPENAI_O_SERIES': 0.01280975341796875, 'DEEPSEEK': 0.005527496337890625, 'GROK': 1.049041748046875e-05, 'NOVA': 4.351139068603516e-06, 'OTHER': 4.774332046508789e-05, 'HUMAN': 3.68952751159668e-05}}"
2507.23653,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""Architectural practice process and artificial intelligence -- an evolving practice\n\nIn an era of exponential technological advancement, artificial intelligence (AI) has emerged as a transformative force in architecture, reshaping traditional design and construction practices. This article explores the multifaceted roles of AI in the architectural process, emphasizing its potential to enhance creativity and efficiency while addressing its limitations in capturing multisensory and experiential dimensions of space. Historically, architectural innovation has paralleled technological progress, from basic tools to advanced computer-aided design systems. However, the integration of AI presents unique challenges, requiring architects to critically evaluate its role in design. A narrative review methodology was adopted, focusing on academic sources selected for their relevance, recency, and credibility. The findings reveal that AI is increasingly integrated across various stages of the architectural process, from early conceptualization and site analysis to generative design and construction detailing. AI tools excel at automating repetitive tasks and generating innovative design solutions, freeing architects to focus on creativity and problem-solving. Additionally, AI's (text- toimage) visual representation strength challenges the ocularcentric approaches in architecture, which should push future architects to address the holistic sensory and experiential qualities of space or the critical thinking inherent to architectural design. While AI offers transformative potential, architects must view it as a collaborative partner rather than a passive tool."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.426738739013672e-05, 'GPT4': 0.011688232421875, 'CLAUDE': 0.02166748046875, 'GOOGLE': 0.94677734375, 'OPENAI_O_SERIES': 0.0031795501708984375, 'DEEPSEEK': 0.0160675048828125, 'GROK': 1.4901161193847656e-06, 'NOVA': 7.367134094238281e-05, 'OTHER': 0.0006341934204101562, 'HUMAN': 6.580352783203125e-05}}"
2507.03844,regular,post_llm,2025,7,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""Optimizing Shanghai's Household Waste Recycling Collection Program by Decision-Making based on Mathematical Modeling\n\nIn this article, we will discuss the optimization of Shanghai's recycling collection program, with the core of the task as making a decision among the choice of the alternatives. We will be showing a vivid and comprehensive application of the classical mathematical multi-criteria decision model: Analytical Hierarchy Process (AHP), using the eigenvector method. We will also seek the key criteria for the sustainability development of human society, by assessing the important elements of waste recycling.First, we considered the evaluation for a quantified score of the benefits and costs of recycling household glass wastes in Shanghai, respectively. In the evaluation of each score, we both adopted the AHP method to build a hierarchical structure of the problem we are facing. We first identified the key assessment criteria of the evaluation, on various perspectives including direct money costs and benefits, and further environmental and indirect considerations. Then, we distributed questionnaires to our school science teachers, taking the geometric mean, to build the pairwise comparison matrix of the criterion. After the theoretical modeling works are done, we began collecting the essential datasets for the evaluation of each score, by doing research on the official statistics, Internet information, market information and news reports. Sometimes, we proceed a logical pre-procession of the data from other data, if the data wanted isn't directly accessible. Then, we crucially considered the generalization of our mathematical model. We considered from several perspectives, including the extension of assessment criteria, and the consideration of the dynamic interdependency between the wastes, inside a limited transportation container."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00942,review,post_llm,2025,7,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'Practical Retrofitting for Obsolete Devices -- Bridging the gap with old tech to create alternative interaction paradigms and workflows\n\nOver the last twenty years, smartphones gradually replaced many earlier digital tools such as PDAs, cameras and music players. Today these objects are regarded as obsolete: they may hold some esthetic or nostalgic appeal but they do not fit in a modern, zero-friction, cloud-first workflow. Yet these devices still have desirable qualities that smartphones lack: a singular focus on a specific use case; hardware buttons and physical connectors; multi-day battery life. Even their lack of connectivity can be seen as an asset from a resilience, privacy and security standpoint. Actually using decades-old tech today is challenging, in spite of its apparent simplicity, because the friction of physical media-based workflows now feels unacceptable. But much like classic cars can be fitted with an EV motor, it is possible to retrofit older devices in order to make them usable again in a connected world. Long after the manufacturer stops supporting a device, user communities play a crucial role in reverse-engineering file formats and communication protocols, maintaining documentation and software archives, as well as designing and producing spare parts that can even overcome initial design flaws. This paper will explore both software and hardware retrofitting techniques, using various examples: cameras, music players, dedicated writing instruments, video games. The resulting retrofitted devices are neither vintage nor modern, creating their own hybrid interaction paradigm around monotasking on dedicated hardware with intermittent connectivity. The various examples discussed outline some common factors that increase the likelihood that a successful retrofitting path can be found for a device. These factors can also be understood as proven design principles to create resilient hardware.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00857,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'UrbanScore: A Real-Time Personalised Liveability Analytics Platform\n\nThis paper introduces UrbanScore - a real-time web platform that computes a personalised liveability score for any urban address. The system fuses five data streams: (i) address geocoding via Nominatim, (ii) facility extraction from OpenStreetMap through Overpass QL, (iii) segment-level traffic metrics from TomTom Flow v10, (iv) hourly air-quality readings from OpenWeatherMap, and (v) user-declared preference profiles, all persisted in an Oracle 19c relational store. Six sub-scores (air, traffic, lifestyle, education, metro access, surface transport) are derived, adaptively weighted and combined; an OpenAI large-language model then converts the numeric results into concise, user-friendly explanations. A pilot deployment covering the 226 km2 metropolitan area of Bucharest evaluated 3,450 unique addresses over four weeks. Median end-to-end latency was 2.1 s (p95 = 2.9s), meeting the <3 non-functional requirement. Aggregate scores ranged from 34 to 92 (mean 68, SD 11), with high-scoring clusters along metro corridors that pair abundant green space with PM2.5 levels below 35 ug m-3. A detailed case study of the Tineretului district produced an overall score of 91/100 and demonstrated how the narrative layer guides users toward comparable neighbourhoods. Limitations include dependence on third-party API uptime, spatial bias toward well-mapped OSM regions and the absence of noise and crime layers, cited by 18% of survey participants as a desired enhancement. Overall, the results show that open geodata, commercial mobility feeds and conversational AI can be integrated into a performant, explainable decision-support tool that places ""liveability analytics"" in the hands of every house-hunter, commuter and city planner.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.655952453613281e-06, 'GPT4': 0.001178741455078125, 'CLAUDE': 0.0235137939453125, 'GOOGLE': 0.00010347366333007812, 'OPENAI_O_SERIES': 1.7881393432617188e-07, 'DEEPSEEK': 0.974609375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.125999450683594e-06, 'HUMAN': 0.0007100105285644531}}"
2507.14755,review,post_llm,2025,7,"{'ai_likelihood': 9.735425313313803e-06, 'text': 'A Risk Assessment Framework for Digital Identification Systems\n\nWe introduce a risk assessment framework for digital identification systems, as well as recommended best practices to enhance privacy, security, and other desirable properties in these systems. To generate these resources, we created a casebook of a wide range of digital identification systems, and we then applied expert analysis and critique to identify patterns. We piloted the framework on several reviews within our organization over a period of approximately one year, and found it to be robust and helpful for those reviews. This work is intended to inform product review and development, product policy, and standards efforts, and to help guide a consistent responsible approach to digital identification across the broader digital identification ecosystem.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.0302,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""AI Literacy and LLM Engagement in Higher Education: A Cross-National Quantitative Study\n\nThis study presents a cross-national quantitative analysis of how university students in the United States and Bangladesh interact with Large Language Models (LLMs). Based on an online survey of 318 students, results show that LLMs enhance access to information, improve writing, and boost academic performance. However, concerns about overreliance, ethical risks, and critical thinking persist. Guided by the AI Literacy Framework, Expectancy-Value Theory, and Biggs' 3P Model, the study finds that motivational beliefs and technical competencies shape LLM engagement. Significant correlations were found between LLM use and perceived literacy benefits (r = .59, p < .001) and optimism (r = .41, p < .001). ANOVA results showed more frequent use among U.S. students (F = 7.92, p = .005) and STEM majors (F = 18.11, p < .001). Findings support the development of ethical, inclusive, and pedagogically sound frameworks for integrating LLMs in higher education."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0009417533874511719, 'GPT4': 0.03338623046875, 'CLAUDE': 0.88330078125, 'GOOGLE': 0.002918243408203125, 'OPENAI_O_SERIES': 0.00131988525390625, 'DEEPSEEK': 0.07696533203125, 'GROK': 9.238719940185547e-06, 'NOVA': 1.0073184967041016e-05, 'OTHER': 0.0010404586791992188, 'HUMAN': 8.07046890258789e-05}}"
2507.15379,regular,post_llm,2025,7,"{'ai_likelihood': 3.2782554626464844e-06, 'text': ""Exploring the Use of Predictive Analytics by Austrian Tax Authorities: A Qualitative Study within the Task-Technology Fit Model\n\nTaxes finance important government services that are now taken for granted in our society, such as infrastructure, health care, or retirement pensions. Tax authorities everywhere strive to ensure that all individuals and organizations comply with applicable tax laws. In this regard, tax authorities must prevent individuals and organizations from evading taxes in an illegal manner. To this end, Austrian tax authorities employ state-of-the-art predictive analytics technology for the selection of suspicious cases for tax audits, thus making efficient use of scarce resources for tax auditing. In this paper, we explore how Austrian tax authorities employ predictive analytics technology in tax auditing and how well the use of such technology fits the characteristics of the task at hand. We collaborated with the Austrian Federal Ministry of Finance's Predictive Analytics Competence Center to obtain insights into the application of predictive analytics technology by Austrian tax authorities. The thus obtained insights serve as the basis for a qualitative analysis in the context of the task-technology fit framework."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.11546,review,post_llm,2025,7,"{'ai_likelihood': 0.951171875, 'text': 'AI Governance InternationaL Evaluation Index (AGILE Index) 2025\n\nThe year 2024 witnessed accelerated global AI governance advancements, marked by strengthened multilateral frameworks and proliferating national regulatory initiatives. This acceleration underscores an unprecedented need to systematically track governance progress--an imperative that drove the launch of the AI Governance InternationaL Evaluation Index (AGILE Index) project since 2023. The inaugural AGILE Index, released in February 2024 after assessing 14 countries, established an operational and comparable baseline framework. Building on pilot insights, AGILE Index 2025 incorporates systematic refinements to better balance scientific rigor with practical adaptability. The updated methodology expands data diversity while enhancing metric validity and cross-national comparability. Reflecting both research advancements and practical policy evolution, AGILE Index 2025 evaluates 40 countries across income levels, regions, and technological development stages, with 4 Pillars, 17 Dimensions and 43 Indicators. In compiling the data, the team integrates multi-source evidence including policy documents, governance practices, research outputs, and risk exposure to construct a unified comparison framework. This approach maps global disparities while enabling countries to identify governance strengths, gaps, and systemic constraints. Through ongoing refinement and iterations, we hope the AGILE Index will fundamentally advance transparency and measurability in global AI governance, delivering data-driven assessments that depict national AI governance capacity, assist governments in recognizing their maturation stages and critical governance issues, and ultimately provide actionable insights for enhancing AI governance systems nationally and globally.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.4126300811767578e-05, 'GPT4': 0.08642578125, 'CLAUDE': 0.67431640625, 'GOOGLE': 0.0018749237060546875, 'OPENAI_O_SERIES': 0.0004596710205078125, 'DEEPSEEK': 0.156494140625, 'GROK': 0.0, 'NOVA': 2.980232238769531e-07, 'OTHER': 3.7550926208496094e-06, 'HUMAN': 0.08050537109375}}"
2507.05212,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'Real-Time AI-Driven Pipeline for Automated Medical Study Content Generation in Low-Resource Settings: A Kenyan Case Study\n\nJuvenotes is a real-time AI-driven pipeline that automates the transformation of academic documents into structured exam-style question banks, optimized for low-resource medical education settings in Kenya. The system combines Azure Document Intelligence for OCR and Azure AI Foundry (OpenAI o3-mini) for question and answer generation in a microservices architecture, with a Vue/TypeScript frontend and AdonisJS backend. Mobile-first design, bandwidth-sensitive interfaces, institutional tagging, and offline features address local challenges. Piloted over seven months at Kenyan medical institutions, Juvenotes reduced content curation time from days to minutes and increased daily active users by 40%. Ninety percent of students reported improved study experiences. Key challenges included intermittent connectivity and AI-generated errors, highlighting the need for offline sync and human validation. Juvenotes shows that AI automation with contextual UX can enhance access to quality study materials in low-resource settings.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.337860107421875e-06, 'GPT4': 0.00015878677368164062, 'CLAUDE': 0.0026111602783203125, 'GOOGLE': 1.049041748046875e-05, 'OPENAI_O_SERIES': 1.9669532775878906e-06, 'DEEPSEEK': 0.9970703125, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 3.0994415283203125e-06, 'HUMAN': 1.1920928955078125e-07}}"
2507.0664,regular,post_llm,2025,7,"{'ai_likelihood': 3.1789143880208335e-06, 'text': 'Google Search Advertising after Dobbs v. Jackson\n\nSearch engines have become the gateway to information, products, and services, including those concerning healthcare. Access to reproductive health has been especially complicated in the wake of the 2022 Dobbs v. Jackson decision by the Supreme Court of the United States, splintering abortion regulations among the states. In this study, we performed an audit of the advertisements shown to Google Search users seeking information about abortion across the United States during the year following the Dobbs decision. We found that Crisis Pregnancy Centers (CPCs) -- organizations that target women with unexpected or ""crisis"" pregnancies, but do not provide abortions -- accounted for 47% of advertisements, whereas abortion clinics -- for 30%. Advertisements from CPCs were often returned for queries concerning information and safety. The type of advertisements returned, however, varied widely within each state, with Arizona returning the most advertisements from abortion clinics and other pro-choice organizations, and Minnesota the least. The proportion of pro-choice vs. anti-choice advertisements returned also varied over time, but estimates from Staggered Augmented Synthetic Control Methods did not indicate that changes in advertisement results were attributable to changes in state abortion laws. Our findings raise questions about the access to accurate medical information across the U.S. and point to a need for further examination of search engine advertisement policies and geographical bias.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.07767,regular,post_llm,2025,7,"{'ai_likelihood': 2.384185791015625e-05, 'text': ""Structured Prompts, Better Outcomes? Exploring the Effects of a Structured Interface with ChatGPT in a Graduate Robotics Course\n\nPrior research shows that how students engage with Large Language Models (LLMs) influences their problem-solving and understanding, reinforcing the need to support productive LLM-uses that promote learning. This study evaluates the impact of a structured GPT platform designed to promote 'good' prompting behavior with data from 58 students in a graduate-level robotics course. The students were assigned to either an intervention group using the structured platform or a control group using ChatGPT freely for two practice lab sessions, before a third session where all students could freely use ChatGPT. We analyzed student perception (pre-post surveys), prompting behavior (logs), performance (task scores), and learning (pre-post tests). Although we found no differences in performance or learning between groups, we identified prompting behaviors - such as having clear prompts focused on understanding code - that were linked with higher learning gains and were more prominent when students used the structured platform. However, such behaviors did not transfer once students were no longer constrained to use the structured platform. Qualitative survey data showed mixed perceptions: some students perceived the value of the structured platform, but most did not perceive its relevance and resisted changing their habits. These findings contribute to ongoing efforts to identify effective strategies for integrating LLMs into learning and question the effectiveness of bottom-up approaches that temporarily alter user interfaces to influence students' interaction. Future research could instead explore top-down strategies that address students' motivations and explicitly demonstrate how certain interaction patterns support learning."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.12957,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'The Goldilocks zone of governing technology: Leveraging uncertainty for responsible quantum practices\n\nEmerging technologies challenge conventional governance approaches, especially when uncertainty is not a temporary obstacle but a foundational feature as in quantum computing. This paper reframes uncertainty from a governance liability to a generative force, using the paradigms of quantum mechanics to propose adaptive, probabilistic frameworks for responsible innovation. We identify three interdependent layers of uncertainty--physical, technical, and societal--central to the evolution of quantum technologies. The proposed Quantum Risk Simulator (QRS) serves as a conceptual example, an imaginative blueprint rather than a prescriptive tool, meant to illustrate how probabilistic reasoning could guide dynamic, uncertainty-based governance. By foregrounding epistemic and ontological ambiguity, and drawing analogies from cognitive neuroscience and predictive processing, we suggest a new model of governance aligned with the probabilistic essence of quantum systems. This model, we argue, is especially promising for the European Union as a third way between laissez-faire innovation and state-led control, offering a flexible yet responsible pathway for regulating quantum and other frontier technologies.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00015342235565185547, 'GPT4': 0.344482421875, 'CLAUDE': 0.35693359375, 'GOOGLE': 0.00885009765625, 'OPENAI_O_SERIES': 2.086162567138672e-06, 'DEEPSEEK': 0.284423828125, 'GROK': 1.7881393432617188e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.6093254089355469e-06, 'HUMAN': 0.00508880615234375}}"
2507.12162,regular,post_llm,2025,7,"{'ai_likelihood': 0.4608832465277778, 'text': 'A real-time metric of online engagement monitoring\n\nMeasuring online behavioural student engagement often relies on simple count indicators or retrospective, predictive methods, which present challenges for real-time application. To address these limitations, we reconceptualise an existing course-wide engagement metric to create a chapter-based version that aligns with the weekly structure of online courses. Derived directly from virtual learning environment log data, the new metric allows for cumulative, real-time tracking of student activity without requiring outcome data or model training. We evaluate the approach across three undergraduate statistics modules over two academic years, comparing it to the course-wide formulation to assess how the reconceptualisation influences what is measured. Results indicate strong alignment from as early as week 3, along with comparable or improved predictive validity for final grades in structured, lecture-based contexts. By the course midpoint, the weekly metric identifies as many low-performing students as are identifiable by the end of the course. While performance varies across modules, the chapter-based formulation offers a scalable and interpretable method for early engagement monitoring and student support.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.21842,regular,post_llm,2025,7,"{'ai_likelihood': 0.000255637698703342, 'text': ""Prompt template for a fictitious LLM agent in a content-flagging experiment\n\nDigital regulations such as the European Union's Digital Services Act (DSA) represent major efforts to shape human-centered and human rights-based frameworks for society. Yet, as these laws are translated into practice, challenges emerge at the intersection of technology, law, and design. This paper presents a qualitative case study examining how designers act as mediators between abstract legal requirements and real-world digital experiences for users, focusing on the design of content reporting mechanisms under Article 16 of the DSA.\n  Through an expert workshop with professional designers from diverse fields (N=9), we explore how legal obligations are interpreted by designers and reflected in discussions and design solutions. Our findings resonate with previous research on the design of reporting mechanisms and dark patterns, highlighting how UX design choices can mislead or hinder users' decision-making and therefore also highlighting the crucial role of design decisions.\n  We show how participatory design methods can bridge disciplinary divides, making legal obligations accessible in compliance fostering design solutions.\n  By using legal design as a lens, we argue that the co-creation of digital regulations and user experience is a core site for digital humanism; where designers, engineers, and legal scholars must collaborate to ensure that systems uphold legal standards to address the challenge the regulation poses to these disciplines."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.15916,review,post_llm,2025,7,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Verifying International Agreements on AI: Six Layers of Verification for Rules on Large-Scale AI Development and Deployment\n\nThe risks of frontier AI may require international cooperation, which in turn may require verification: checking that all parties follow agreed-on rules. For instance, states might need to verify that powerful AI models are widely deployed only after their risks to international security have been evaluated and deemed manageable. However, research on AI verification could benefit from greater clarity and detail. To address this, this report provides an in-depth overview of AI verification, intended for both policy professionals and technical researchers. We present novel conceptual frameworks, detailed implementation options, and key R&D challenges. These draw on existing literature, expert interviews, and original analysis, all within the scope of confidentially overseeing AI development and deployment that uses thousands of high-end AI chips. We find that states could eventually verify compliance by using six largely independent verification approaches with substantial redundancy: (1) built-in security features in AI chips; (2-3) separate monitoring devices attached to AI chips; and (4-6) personnel-based mechanisms, such as whistleblower programs. While promising, these approaches require guardrails to protect against abuse and power concentration, and many of these technologies have yet to be built or stress-tested. To enable states to confidently verify compliance with rules on large-scale AI development and deployment, the R&D challenges we list need significant progress.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.23434,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""Future Illiteracies -- Architectural Epistemology and Artificial Intelligence\n\nIn the age of artificial intelligence, architectural practice faces a paradox of immense potential and creeping standardization. As humans are increasingly relying on AI-generated outputs, architecture risks becoming a spectacle of repetition- a shuffling of data that neither truly innovates nor progresses vertically in creative depth. This paper explores the critical role of data in AI systems, scrutinizing the training datasets that form the basis of AI's generative capabilities and the implications for architectural practice. We argue that when architects approach AI passively, without actively engaging their own creative and critical faculties, they risk becoming passive users locked in an endless loop of horizontal expansion without meaningful vertical growth. By examining the epistemology of architecture in the AI age, this paper calls for a paradigm where AI serves as a tool for vertical and horizontal growth, contingent on human creativity and agency. Only by mastering this dynamic relationship can architects avoid the trap of passive, standardized design and unlock the true potential of AI."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.044097900390625, 'GPT4': 0.364990234375, 'CLAUDE': 0.058685302734375, 'GOOGLE': 0.53076171875, 'OPENAI_O_SERIES': 0.00015723705291748047, 'DEEPSEEK': 0.00012731552124023438, 'GROK': 9.715557098388672e-06, 'NOVA': 3.0219554901123047e-05, 'OTHER': 0.0009665489196777344, 'HUMAN': 4.07099723815918e-05}}"
2507.04166,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""Governance and Technological Challenge in Digital Solidarity Economies: A Case Study of a Collaborative Transportation Platform in South Korea\n\nSouth Korea's City P illustrates how lofty goals of digital solidarity can falter when challenged by local governance realities. Drawing on Hansmann's ownership theory, collaborative governance concepts, and platform cooperativism, we conducted a qualitative case study involving policy documents, independent assessments, and 11 in-depth interviews with residents, officials, and technology developers. Findings reveal a marked disconnect between the initiative's stated emphasis on community co-ownership and the actual power dynamics that largely favored government agencies and external firms. Although blockchain and integrated digital tools were meant to enhance transparency and inclusivity, stakeholders--especially elderly residents--experienced confusion and mistrust. We argue that genuine collaboration in digital solidarity economies requires not only robust technical designs but also culturally resonant ownership structures, substantive inclusion of local voices, and transparent governance mechanisms. The City P case underscores the necessity of addressing heterogeneous digital capacities, aligning funding and incentives with grassroots empowerment, and mitigating performative participation to ensure meaningful and sustainable outcomes in community-based digital innovation."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00011664628982543945, 'GPT4': 0.48095703125, 'CLAUDE': 0.429443359375, 'GOOGLE': 0.0001494884490966797, 'OPENAI_O_SERIES': 0.0011320114135742188, 'DEEPSEEK': 0.0882568359375, 'GROK': 1.1920928955078125e-07, 'NOVA': 4.76837158203125e-07, 'OTHER': 4.589557647705078e-06, 'HUMAN': 7.534027099609375e-05}}"
2507.01787,review,post_llm,2025,7,"{'ai_likelihood': 0.043572319878472224, 'text': ""From Reports to Reality: Testing Consistency in Instagram's Digital Services Act Compliance Data\n\nThe Digital Services Act (DSA) introduces harmonized rules for content moderation and platform governance in the European Union, mandating robust compliance mechanisms, particularly for very large online platforms and search engines. This study examined compliance with DSA requirements, focusing on Instagram as a case study. We develop and apply a multi-level consistency framework to evaluate DSA compliance. Our findings contribute to the broader discussion on empirically-based regulation, providing insight into how researchers, regulators, auditors and platforms can better utilize DSA mechanisms to improve reporting and enforcement quality and accountability. This work underscores that consistency can help detect potential compliance failures. It also demonstrates that platforms should be evaluated as part of an interconnected ecosystem rather than through isolated processes, which is crucial for effective compliance evaluation under the DSA."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.19877,review,post_llm,2025,7,"{'ai_likelihood': 0.98291015625, 'text': 'AI-Driven Media & Synthetic Knowledge: Rethinking Society in Generative Futures\n\nGenerative AI is not just a technological leap -- it is a societal stress test, reshaping trust, identity, equity, and authorship. This exploratory PhD seminar examined emerging academic trends in AI-driven synthetic media and worlds, emphasizing ethical risks and societal implications. In Part One, students explored core concepts such as generative AI, fake media, and synthetic knowledge production. In Part Two, they critically engaged with these challenges, producing actionable insights. The two-part format enabled deep reflection on power, responsibility, and education in AI-augmented communication. Outcomes offer practical guidance for educators, researchers, and institutions committed to fostering more responsible, human-centered AI use in media and society.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.004364013671875, 'GPT4': 0.462890625, 'CLAUDE': 0.353515625, 'GOOGLE': 0.025543212890625, 'OPENAI_O_SERIES': 0.0016527175903320312, 'DEEPSEEK': 0.12237548828125, 'GROK': 5.543231964111328e-06, 'NOVA': 2.6524066925048828e-05, 'OTHER': 0.0004680156707763672, 'HUMAN': 0.0292205810546875}}"
2508.00849,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""A Modular, Low-Cost IoT System for Environmental and Behavioural Monitoring in Cultural Heritage Sites\n\nThe preservation of cultural heritage faces growing challenges from climate change, tourism pressure, and limited conservation resources. Existing monitoring solutions are often cost-prohibitive, proprietary, and inflexible, leaving many institutions, particularly in developing regions, without viable tools for proactive management. This study presents a modular, low-cost Internet of Things (IoT) system designed for real-time environmental and behavioural monitoring in heritage sites. Built with off-the-shelf components such as ESP32 microcontrollers and Raspberry Pi, the system integrates a wireless sensor network, edge computing, and cloud services (Microsoft Azure) to measure variables including temperature, humidity, sound, and visitor proximity. It also incorporates computer vision models to classify visitor behaviour, achieving up to 95% accuracy using fine-tuned Vision Transformers. The system's modularity, enabled via JSON configurations, allows for rapid reconfiguration without firmware changes. A simulated deployment demonstrated robust performance, low power consumption, and cost-efficiency (less than 200 GBP per node), validating the system's potential for scalable, sustainable heritage monitoring. This open-source framework offers a practical path forward for institutions seeking to balance accessibility with conservation needs."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.00043892860412597656, 'CLAUDE': 5.364418029785156e-07, 'GOOGLE': 5.960464477539063e-08, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.99951171875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.06379,review,post_llm,2025,7,"{'ai_likelihood': 7.318125830756294e-06, 'text': 'Domestic frontier AI regulation, an IAEA for AI, an NPT for AI, and a US-led Allied Public-Private Partnership for AI: Four institutions for governing and developing frontier AI\n\nCompute governance can underpin international institutions for the governance of frontier AI. To demonstrate this I explore four institutions for governing and developing frontier AI. Next steps for compute-indexed domestic frontier AI regulation could include risk assessments and pre-approvals, data centre usage reports, and release gate regulation. Domestic regimes could be harmonized and monitored through an International AI Agency - an International Atomic Energy Agency (IAEA) for AI. This could be backed up by a Secure Chips Agreement - a Non-Proliferation Treaty (NPT) for AI. This would be a non-proliferation regime for advanced chips, building on the chip export controls - states that do not have an IAIA-certified frontier regulation regime would not be allowed to import advanced chips. Frontier training runs could be carried out by a megaproject between the USA and its allies - a US-led Allied Public-Private Partnership for frontier AI. As a project to develop advanced AI, this could have significant advantages over alternatives led by Big Tech or particular states: it could be more legitimate, secure, safe, non-adversarial, peaceful, and less prone to misuse. For each of these four scenarios, a key incentive for participation is access to the advanced AI chips that are necessary for frontier training runs and large-scale inference. Together, they can create a situation in which governments can be reassured that frontier AI is developed and deployed in a secure manner with misuse minimised and benefits widely shared. Building these institutions may take years or decades, but progress is incremental and evolutionary and the first steps have already been taken.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00925,regular,post_llm,2025,7,"{'ai_likelihood': 0.96826171875, 'text': ""Themed Challenges to Solve Data Scarcity in Africa: A Proposition for Increasing Local Data Collection and Integration\n\nIn Africa, the scarcity of computational resources and medical datasets remains a major hurdle to the development and deployment of artificial intelligence (AI) tools in clinical settings, further contributing to global bias. These limitations hinder the full realization of AI's potential and present serious challenges to advancing healthcare across the region.\n  This paper proposes a framework aimed at addressing data scarcity in African healthcare. The framework presents a comprehensive strategy to encourage healthcare providers across the continent to create, curate, and share locally sourced medical imaging datasets. By organizing themed challenges that promote participation, accurate and relevant datasets can be generated within the African healthcare community. This approach seeks to overcome existing dataset limitations, paving the way for a more inclusive and impactful AI ecosystem that is specifically tailored to Africa's healthcare needs."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00484466552734375, 'GPT4': 0.8212890625, 'CLAUDE': 0.003063201904296875, 'GOOGLE': 0.164794921875, 'OPENAI_O_SERIES': 0.0006103515625, 'DEEPSEEK': 2.7120113372802734e-05, 'GROK': 5.364418029785156e-06, 'NOVA': 9.119510650634766e-06, 'OTHER': 0.000675201416015625, 'HUMAN': 0.00461578369140625}}"
2508.00919,regular,post_llm,2025,7,"{'ai_likelihood': 0.010359022352430556, 'text': 'Translating Machine Learning Interpretability into Clinical Insights for ICU Mortality Prediction\n\nCurrent research efforts largely focus on employing at most one interpretable method to elucidate machine learning (ML) model performance. However, significant barriers remain in translating these interpretability techniques into actionable insights for clinicians, notably due to complexities such as variability across clinical settings and the Rashomon effect. In this study, we developed and rigorously evaluated two ML models along with interpretation mechanisms, utilizing data from 131,051 ICU admissions across 208 hospitals in the United States, sourced from the eICU Collaborative Research Database. We examined two datasets: one with imputed missing values (130,810 patients, 5.58% ICU mortality) and another excluding patients with missing data (5,661 patients, 23.65% ICU mortality). The random forest (RF) model demonstrated an AUROC of 0.912 with the first dataset and 0.839 with the second dataset, while the XGBoost model achieved an AUROC of 0.924 with the first dataset and 0.834 with the second dataset. Consistently identified predictors of ICU mortality across datasets, cross-validation folds, models, and explanation mechanisms included lactate levels, arterial pH, body temperature, and others. By aligning with routinely collected clinical variables, this study aims to enhance ML model interpretability for clinical use, promote greater understanding and adoption among clinicians, and ultimately contribute to improved patient outcomes.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.18431,regular,post_llm,2025,7,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""What does the public want their local government to hear? A data-driven case study of public comments across the state of Michigan\n\nCity council meetings are vital sites for civic participation where the public can speak directly to their local government. By addressing city officials and calling on them to take action, public commenters can potentially influence policy decisions spanning a broad range of concerns, from housing, to sustainability, to social justice. Yet studies of these meetings have often been limited by the availability of large-scale, geographically-diverse data. Relying on local governments' increasing use of YouTube and other technologies to archive their public meetings, we propose a framework that characterizes comments along two dimensions: the local concerns where concerns are situated (e.g., housing, election administration), and the societal concerns raised (e.g., functional democracy, anti-racism). Based on a large record of public comments we collect from 15 cities in Michigan, we produce data-driven taxonomies of the local concerns and societal concerns that these comments cover, and employ machine learning methods to scalably apply our taxonomies across the entire dataset. We then demonstrate how our framework allows us to examine the salient local concerns and societal concerns that arise in our data, as well as how these aspects interact."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.08103,review,post_llm,2025,7,"{'ai_likelihood': 4.900826348198785e-06, 'text': 'A Systematic Mapping Study on Open Source Agriculture Technology Research\n\nAgriculture contributes trillions of dollars to the US economy each year. Digital technologies are disruptive forces in agriculture. The open source movement is beginning to emerge in agriculture technology and has dramatic implications for the future of farming and agriculture digital technologies. The convergence of open source and agriculture digital technology is observable in scientific research, but the implications of open source ideals related to agriculture technology have yet to be explored. This study explores open agriculture digital technology through a systematic mapping of available open agriculture digital technology research. The study contributes to Information Systems research by illuminating current trends and future research opportunities.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.06018,regular,post_llm,2025,7,"{'ai_likelihood': 7.814831203884549e-06, 'text': ""Campaigning through the lens of Google: A large-scale algorithm audit of Google searches in the run-up to the Swiss Federal Elections 2023\n\nSearch engines like Google have become major sources of information for voters during election campaigns. To assess potential biases across candidates' gender and partisan identities in the algorithmic curation of candidate information, we conducted a large-scale algorithm audit analyzing Google's selection and ranking of information about candidates for the 2023 Swiss Federal Elections, three and one week before the election day. Results indicate that text searches prioritize media sources in search output but less so for women politicians. Image searches revealed a tendency to reinforce stereotypes about women candidates, marked by a disproportionate focus on stereotypically pleasant emotions for women, particularly among right-leaning candidates. Crucially, we find that patterns of candidates' representation in Google text and image searches are predictive of their electoral performance."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.20346,regular,post_llm,2025,7,"{'ai_likelihood': 0.005539788140190973, 'text': 'EyeAI: AI-Assisted Ocular Disease Detection for Equitable Healthcare Access\n\nOcular disease affects billions of individuals unevenly worldwide. It continues to increase in prevalence with trends of growing populations of diabetic people, increasing life expectancies, decreasing ophthalmologist availability, and rising costs of care. We present EyeAI, a system designed to provide artificial intelligence-assisted detection of ocular diseases, thereby enhancing global health. EyeAI utilizes a convolutional neural network model trained on 1,920 retinal fundus images to automatically diagnose the presence of ocular disease based on a retinal fundus image input through a publicly accessible web-based application. EyeAI performs a binary classification to determine the presence of any of 45 distinct ocular diseases, including diabetic retinopathy, media haze, and optic disc cupping, with an accuracy of 80%, an AUROC of 0.698, and an F1-score of 0.8876. EyeAI addresses barriers to traditional ophthalmologic care by facilitating low-cost, remote, and real-time diagnoses, particularly for equitable access to care in underserved areas and for supporting physicians through a secondary diagnostic opinion. Results demonstrate the potential of EyeAI as a scalable, efficient, and accessible diagnostic tool. Future work will focus on expanding the training dataset to enhance the accuracy of the model further and improve its diagnostic capabilities.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.08211,regular,post_llm,2025,7,"{'ai_likelihood': 0.4440646701388889, 'text': ""Effect of Static vs. Conversational AI-Generated Messages on Colorectal Cancer Screening Intent: a Randomized Controlled Trial\n\nLarge language model (LLM) chatbots show increasing promise in persuasive communication. Yet their real-world utility remains uncertain, particularly in clinical settings where sustained conversations are difficult to scale. In a pre-registered randomized controlled trial, we enrolled 915 U.S. adults (ages 45-75) who had never completed colorectal cancer (CRC) screening. Participants were randomized to: (1) no message control, (2) expert-written patient materials, (3) single AI-generated message, or (4) a motivational interviewing chatbot. All participants were required to remain in their assigned condition for at least three minutes. Both AI arms tailored content using participant's self-reported demographics including age and gender. Both AI interventions significantly increased stool test intentions by over 12 points (12.9-13.8/100), compared to a 7.5 gain for expert materials (p<.001 for all comparisons). While the AI arms outperformed the no message control for colonoscopy intent, neither showed improvement xover expert materials. Notably, for both outcomes, the chatbot did not outperform the single AI message in boosting intent despite participants spending ~3.5 minutes more on average engaging with it. These findings suggest concise, demographically tailored AI messages may offer a more scalable and clinically viable path to health behavior change than more complex conversational agents and generic time intensive expert-written materials. Moreover, LLMs appear more persuasive for lesser-known and less-invasive screening approaches like stool testing, but may be less effective for entrenched preferences like colonoscopy. Future work should examine which facets of personalization drive behavior change, whether integrating structural supports can translate these modest intent gains into completed screenings, and which health behaviors are most responsive to AI-supported guidance."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.19499,review,post_llm,2025,7,"{'ai_likelihood': 4.602803124321832e-06, 'text': 'A Survey of Virtual Reality in Japan\n\nThe NSF Summer Institute in Japan program sends about 60 graduate students of all disciplines to Japan each summer. For two months, students participate in research at host labs, visit conferences and other labs of interest, and receive Japanese language and cultural instruction. Full financial support is provided by the American and Japanese governments. During the summer of 1993, the author participated in this program and took the opportunity to visit the Japanese virtual reality research community. He attended two virtual reality conferences and toured more than a dozen labs. After the program, he made short visits to VR and graphics labs in PR China and South Korea. This paper gives a detailed account of these experiences.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.05866,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""Understanding support for AI regulation: A Bayesian network perspective\n\nAs artificial intelligence (AI) becomes increasingly embedded in public and private life, understanding how citizens perceive its risks, benefits, and regulatory needs is essential. To inform ongoing regulatory efforts such as the European Union's proposed AI Act, this study models public attitudes using Bayesian networks learned from the nationally representative 2023 German survey Current Questions on AI. The survey includes variables on AI interest, exposure, perceived threats and opportunities, awareness of EU regulation, and support for legal restrictions, along with key demographic and political indicators. We estimate probabilistic models that reveal how personal engagement and techno-optimism shape public perceptions, and how political orientation and age influence regulatory attitudes. Sobol indices and conditional inference identify belief patterns and scenario-specific responses across population profiles. We show that awareness of regulation is driven by information-seeking behavior, while support for legal requirements depends strongly on perceived policy adequacy and political alignment. Our approach offers a transparent, data-driven framework for identifying which public segments are most responsive to AI policy initiatives, providing insights to inform risk communication and governance strategies. We illustrate this through a focused analysis of support for AI regulation, quantifying the influence of political ideology, perceived risks, and regulatory awareness under different scenarios."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.091594696044922e-05, 'GPT4': 0.0031375885009765625, 'CLAUDE': 0.82861328125, 'GOOGLE': 4.5359134674072266e-05, 'OPENAI_O_SERIES': 3.552436828613281e-05, 'DEEPSEEK': 0.1676025390625, 'GROK': 5.960464477539063e-08, 'NOVA': 1.7881393432617188e-07, 'OTHER': 1.615285873413086e-05, 'HUMAN': 0.0006670951843261719}}"
2507.21743,regular,post_llm,2025,7,"{'ai_likelihood': 0.2783203125, 'text': 'When Proximity Falls Short: Inequalities in Commuting and Accessibility by Public Transport in Santiago, Chile\n\nTraditional measures of urban accessibility often rely on static models or survey data. However, location information from mobile networks now enables large-scale, dynamic analyses of how people navigate cities. This study uses eXtended Detail Records (XDRs) derived from mobile phone activity to analyze commuting patterns and accessibility inequalities in Santiago, Chile. First, we identify residential and work locations and model commuting routes using the R5 multimodal routing engine, which combines public transport and walking. To explore spatial patterns, we apply a bivariate spatial clustering analysis (LISA) alongside regression techniques to identify distinct commuting behaviors and their alignment with vulnerable population groups. Our findings reveal that average commuting times remain consistent across socioeconomic groups. However, despite residing in areas with greater opportunity density, higher-income populations do not consistently experience shorter commuting times. This highlights a disconnect between spatial proximity to opportunities and actual travel experience. Our analysis reveals significant disparities between sociodemographic groups, particularly regarding the distribution of indigenous populations and gender. Overall, the findings of our study suggest that commuting and accessibility inequalities in Santiago are closely linked to broader social and demographic structures.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.22161,review,post_llm,2025,7,"{'ai_likelihood': 1.357661353217231e-06, 'text': ""Why do women pursue a PhD in Computer Science?\n\nComputer science attracts few women, and their proportion decreases through advancing career stages. Few women progress to PhD studies in CS after completing master's studies. Empowering women at this stage in their careers is essential to unlock untapped potential for society, industry and academia. This paper identifies students' career assumptions and information related to PhD studies focused on gender-based differences. We propose a Women Career Lunch program to inform female master students about PhD studies that explains the process, clarifies misconceptions, and alleviates concerns. An extensive survey was conducted to identify factors that encourage and discourage students from undertaking PhD studies. We identified statistically significant differences between those who undertook PhD studies and those who didn't, as well as gender differences. A catalogue of questions to initiate discussions with potential PhD students which allowed them to explore these factors was developed and translated to 8 languages. Encouraging factors toward PhD study include interest and confidence in research arising from a research involvement during earlier studies; enthusiasm for and self-confidence in CS in addition to an interest in an academic career; encouragement from external sources; and a positive perception towards PhD studies which can involve achieving personal goals. Discouraging factors include uncertainty and lack of knowledge of the PhD process, a perception of lower job flexibility, and the requirement for long-term commitment. Gender differences highlighted that female students who pursue a PhD have less confidence in their technical skills than males but a higher preference for interdisciplinary areas. Female students are less inclined than males to perceive the industry as offering better job opportunities and more flexible career paths than academia."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.15299,review,post_llm,2025,7,"{'ai_likelihood': 6.556510925292969e-06, 'text': ""An Overview of the Risk-based Model of AI Governance\n\nThis paper provides an overview and critique of the risk based model of artificial intelligence (AI) governance that has become a popular approach to AI regulation across multiple jurisdictions. The 'AI Policy Landscape in Europe, North America and Australia' section summarises the existing AI policy efforts across these jurisdictions, with a focus of the EU AI Act and the Australian Department of Industry, Science and Regulation's (DISR) safe and responsible AI consultation. The 'Analysis' section of this paper proposes several criticisms of the risk based approach to AI governance, arguing that the construction and calculation of risks that they use reproduces existing inequalities. Drawing on the work of Julia Black, it argues that risk and harm should be distinguished clearly and that the notion of risk is problematic as its inherent normativity reproduces dominant and harmful narratives about whose interests matter, and risk categorizations should be subject to deep scrutiny. This paper concludes with the suggestion that existing risk governance scholarship can provide valuable insights toward the improvement of the risk based AI governance, and that the use of multiple regulatory implements and responsive risk regulation should be considered in the continuing development of the model."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.09296,review,post_llm,2025,7,"{'ai_likelihood': 9.702311621771919e-05, 'text': 'If open source is to win, it must go public\n\nOpen source projects have made incredible progress in producing transparent and widely usable machine learning models and systems, but open source alone will face challenges in fully democratizing access to AI. Unlike software, AI models require substantial resources for activation -- compute, post-training, deployment, and oversight -- which only a few actors can currently provide. This paper argues that open source AI must be complemented by public AI: infrastructure and institutions that ensure models are accessible, sustainable, and governed in the public interest. To achieve the full promise of AI models as prosocial public goods, we need to build public infrastructure to power and deliver open source software and models.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.11567,review,post_llm,2025,7,"{'ai_likelihood': 4.304779900444879e-06, 'text': 'Consumer Law for AI Agents\n\nSince the public release of ChatGPT in November 2022, the AI landscape is undergoing a rapid transformation. Currently, the use of AI chatbots by consumers has largely been limited to image generation or question-answering language models. The next generation of AI systems, AI agents that can plan and execute complex tasks with only limited human involvement, will be capable of a much broader range of actions. In particular, consumers could soon be able to delegate purchasing decisions to AI agents acting as Custobots. Against this background, the Article explores whether EU consumer law, as it currently stands, is ready for the rise of the Custobot Economy. In doing so, the Article makes three contributions. First, it outlines how the advent of AI agents could change the existing e-commerce landscape. Second, it explains how AI agents challenge the premises of a human-centric consumer law which is based on the assumption that consumption decisions are made by humans. Third, the Article presents some initial considerations how a future consumer law could look like that works for both humans and machines.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00911,regular,post_llm,2025,7,"{'ai_likelihood': 1.8874804178873699e-06, 'text': 'Mitigating the Carbon Footprint of Chatbots as Consumers\n\nIn the context of the high energy demand of large language models (LLMs) and growing concerns about global warming, there is significant demand for actionable recommendations that can help reduce emissions when utilizing such technologies. This paper examines the environmental impact linked to a fundamental function of LLM-based conversational systems that might be less well known to end users: the conversational memory, which enables the system to maintain context throughout the dialog. After analyzing conversation patterns using anonymized token data from a real world system, a recommendation for individuals on how they could use chatbots in a more sustainable way is derived. Based on a simulation, the savings potential resulting from the adoption of such an ecological gesture is estimated.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00878,review,post_llm,2025,7,"{'ai_likelihood': 0.1789008246527778, 'text': 'A Relational (Re)Turn: Revisit Interactive Art through Interaction and Aesthetics\n\nThis paper revisits the concept of interaction in interactive art, tracing its evolution from sociocultural origins to its narrowing within human-computer paradigms. It critiques this reduction and proposes a relational (re)turn through reclaiming interaction as intersubjective and relational. Through a synthesis of aesthetic theories and case studies from Ars Electronica, the paper introduces Techno Relational Aesthetics, a new conceptual lens that emphasizes technologically mediated relationality. This approach expands interactive art beyond audience-artwork interaction and opens the possibility to broader relational practices.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.14233,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'Towards an ABM on Proactive Community Adaptation for Climate Change\n\nWe present an agent-based model (ABM) simulating proactive community adaptation to climate change in an urban context. The model is applied to Bergen, Norway, represented as a complex socio-ecological system. It integrates multiple agent types: municipal government (urban planners and political actors), civil society (individual citizens), environmental NGOs and activists, and media. Agents interact during urban planning processes - particularly the evaluation and approval of new development proposals. Urban planners provide technical assessments, while politicians (organized by party) make final decisions to approve, modify, or reject projects. Environmental NGOs, activist groups, and the media shape public perception and influence policymakers through campaigns, lobbying, protests, and news coverage. Individual citizens decide whether to engage in collective action based on personal values and social influences. The model captures the resulting decision-making ecosystem and reveals feedback loops and leverage points that determine climate-adaptive outcomes. By analyzing these dynamics, we identify critical intervention points where targeted policy measures can facilitate systemic transformation toward more climate-resilient urban development.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.8477439880371094e-06, 'GPT4': 2.0503997802734375e-05, 'CLAUDE': 0.99951171875, 'GOOGLE': 8.285045623779297e-05, 'OPENAI_O_SERIES': 2.294778823852539e-05, 'DEEPSEEK': 0.00020575523376464844, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 1.0728836059570312e-06}}"
2507.23718,review,post_llm,2025,7,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Informing AI Risk Assessment with News Media: Analyzing National and Political Variation in the Coverage of AI Risks\n\nRisk-based approaches to AI governance often center the technological artifact as the primary focus of risk assessments, overlooking systemic risks that emerge from the complex interaction between AI systems and society. One potential source to incorporate more societal context into these approaches is the news media, as it embeds and reflects complex interactions between AI systems, human stakeholders, and the larger society. News media is influential in terms of which AI risks are emphasized and discussed in the public sphere, and thus which risks are deemed important. Yet, variations in the news media between countries and across different value systems (e.g. political orientations) may differentially shape the prioritization of risks through the media's agenda setting and framing processes. To better understand these variations, this work presents a comparative analysis of a cross-national sample of news media spanning 6 countries (the U.S., the U.K., India, Australia, Israel, and South Africa). Our findings show that AI risks are prioritized differently across nations and shed light on how left vs. right leaning U.S. based outlets not only differ in the prioritization of AI risks in their coverage, but also use politicized language in the reporting of these risks. These findings can inform risk assessors and policy-makers about the nuances they should account for when considering news media as a supplementary source for risk-based governance approaches."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.03457,review,post_llm,2025,7,"{'ai_likelihood': 0.017250908745659724, 'text': 'Deepfakes in Criminal Investigations: Interdisciplinary Research Directions for CMC Research\n\nThe emergence of deepfake technologies offers both opportunities and significant challenges. While commonly associated with deception, misinformation, and fraud, deepfakes may also enable novel applications in high-stakes contexts such as criminal investigations. However, these applications raise complex technological, ethical, and legal questions. We adopt an interdisciplinary approach, drawing on computer science, philosophy, and law, to examine what it takes to responsibly use deepfakes in criminal investigations and argue that computer-mediated communication (CMC) research, especially based on social media corpora, can provide crucial insights for understanding the potential harms and benefits of deepfakes. Our analysis outlines key research directions for the CMC community and underscores the need for interdisciplinary collaboration in this evolving domain.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.02648,review,post_llm,2025,7,"{'ai_likelihood': 5.960464477539062e-07, 'text': 'Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains\n\nThe AI industry is exploding in popularity, with increasing attention to potential harms and unwanted consequences. In the current digital ecosystem, AI deployments are often the product of AI supply chains (AISC): networks of outsourced models, data, and tooling through which multiple entities contribute to AI development and distribution. AI supply chains lack the modularity, redundancies, or conventional supply chain practices that enable identification, isolation, and easy correction of failures, exacerbating the already difficult processes of responding to ML-generated harms. As the stakeholders participating in and impacted by AISCs have scaled and diversified, so too have the risks they face. In this stakeholder analysis of AI supply chains, we consider who participates in AISCs, what harms they face, where sources of harm lie, and how market dynamics and power differentials inform the type and probability of remedies. Because AI supply chains are purposely invented and implemented, they may be designed to account for, rather than ignore, the complexities, consequences, and risks of deploying AI systems. To enable responsible design and management of AISCs, we offer a typology of responses to AISC-induced harms: recourse, repair, reparation or prevention. We apply this typology to stakeholders participating in a health-care AISC across three stylized markets $\\unicode{x2013}$ vertical integration, horizontal integration, free market $\\unicode{x2013}$ to illustrate how stakeholder positioning and power within an AISC may shape responses to an experienced harm.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.04424,regular,post_llm,2025,7,"{'ai_likelihood': 0.03573947482638889, 'text': ""NourID+: A Digital Energy Identity Framework for Efficient Subsidy Allocation in Morocco\n\nWe introduce NourID+, a digital energy identity framework that addresses Morocco's need for trusted energy subsidy allocation through authenticated digital identity integration. NourID+ creates a strong foundation for future subsidy programs by unifying three government-issued and digitalized credentials: Moroccan national identity cards (CIN), cadastral plans, and property ownership certificates are transformed into unique digital energy IDs (DE-IDs) that map authenticated identities with specific properties and their energy consumption patterns. The system supports three property ownership profiles: farmers (landowners), entrepreneurs (factory or company owners), and households (house owners), as energy consumption is directly related to land ownership. NourID+ provides dual access through a government portal allowing officials to process DE-ID generation requests, as well as a citizen portal for DE-ID usage and energy monitoring. Our framework supports CIN upload with facial biometric matching, automated property retrieval through government APIs, and government officer approval workflow for DE-ID generation. After evaluation of the system, we demonstrate a reduction in verification time from weeks to minutes, with 98% accuracy of document validation. The proposed solution allows for targeted subsidy allocation of electricity based on actual consumption needs rather than estimations, potentially improving the efficiency of Morocco's significant energy subsidy expenditure."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.07703,review,post_llm,2025,7,"{'ai_likelihood': 5.960464477539062e-07, 'text': ""AI Human Impact: Toward a Model for Ethical Investing in AI-Intensive Companies\n\nDoes AI conform to humans, or will we conform to AI? An ethical evaluation of AI-intensive companies will allow investors to knowledgeably participate in the decision. The evaluation is built from nine performance indicators that can be analyzed and scored to reflect a technology's human-centering. The result is objective investment guidance, as well as investors empowered to act in accordance with their own values. Incorporating ethics into financial decisions is a strategy that will be recognized by participants in environmental, social, and governance investing, however, this paper argues that conventional ESG frameworks are inadequate to companies that function with AI at their core. Fully accounting for contemporary big data, predictive analytics, and machine learning requires specialized metrics customized from established AI ethics principles. With these metrics established, the larger goal is a model for humanist investing in AI-intensive companies that is intellectually robust, manageable for analysts, useful for portfolio managers, and credible for investors."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00932,review,post_llm,2025,7,"{'ai_likelihood': 2.9802322387695312e-06, 'text': ""How Sovereign Is Sovereign Compute? A Review of 775 Non-U.S. Data Centers\n\nPrevious literature has proposed that the companies operating data centers enforce government regulations on AI companies. Using a new dataset of 775 non-U.S. data center projects, this paper estimates how often data centers could be subject to foreign legal authorities due to the nationality of the data center operators. We find that U.S. companies operate 48% of all non-U.S. data center projects in our dataset when weighted by investment value - a proxy for compute capacity. This is an approximation based on public data and should be interpreted as an initial estimate. For the United States, our findings suggest that data center operators offer a lever for internationally governing AI that complements traditional export controls, since operators can be used to regulate computing resources already deployed in non-U.S. data centers. For other countries, our results show that building data centers locally does not guarantee digital sovereignty if those facilities are run by foreign entities.\n  To support future research, we release our dataset, which documents over 20 variables relating to each data center, including the year it was announced, the investment value, and its operator's national affiliation. The dataset also includes over 1,000 quotes describing these data centers' strategic motivations, operational challenges, and engagement with U.S. and Chinese entities."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.13517,regular,post_llm,2025,7,"{'ai_likelihood': 0.16072591145833334, 'text': 'The Stated Protocol: A Decentralized Framework for Digital Diplomacy\n\nInternational coordination faces significant friction due to reliance on periodic summits, bilateral consultations, and fragmented communication channels that impede rapid collective responses to emerging global challenges while limiting transparency to constituents. We present the Stated Protocol, a decentralized framework that enables organizations to coordinate through standardized text statements published on their website domains. While applicable to all organizations, this work focuses primarily on the application in international relations, where the protocol enables rapid consensus discovery and collective decision-making without relying on centralized social media platforms. We explore specific applications: (1) faster treaty negotiation through incremental micro-agreements that can be signed digitally within hours rather than months, (2) continuous and transparent operation of international institutions through asynchronous decision-making, (3) coordinated signaling from local governments to national authorities through simultaneous statement publication, and (4) coalition formation among non-governmental organizations through transparent position aggregation.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.0532,review,post_llm,2025,7,"{'ai_likelihood': 0.0, 'text': 'Teaching Sustainable Creative Technologies\n\nArtists and especially new media artists contribute to public perceptions and adoption of new technologies through their own use of emerging media technologies such as augmented and virtual reality, generative image systems, and high-resolution displays in the production of their work. In this way, art and media production can be understood as part of the larger issue of unsustainable computational consumption. As such, it is critical for artists to develop, share, and promote new and more sustainable methods of engaging with technology, especially within the context of higher education. This paper will explore how artists might implement more sustainable methods by considering the relationship between the technical approaches of compute reuse, sustainable web development, and frugal computing, and the concepts of material specificity , futurity, and media archaeology . Proposing three methods of less carbon-intensive artistic production and a set of guidelines for introducing sustainable methods into arts and technology curriculum, this paper will outline not only the technical viability of these approaches but also the rich conceptual opportunities these approaches might offer to artists and viewers alike. For each method, models for pedagogical implementation will be explored with an emphasis on how local resources and sustainability contexts should play a role.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.13837,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'Principles and Reasons Behind Automated Vehicle Decisions in Ethically Ambiguous Everyday Scenarios\n\nAutomated vehicles (AVs) increasingly encounter ethically ambiguous situations in everyday driving--scenarios involving conflicting human interests and lacking clearly optimal courses of action. While existing ethical models often focus on rare, high-stakes dilemmas (e.g., crash avoidance or trolley problems), routine decisions such as overtaking cyclists or navigating social interactions remain underexplored. This study addresses that gap by applying the tracking condition of Meaningful Human Control (MHC), which holds that AV behaviour should align with human reasons--defined as the values, intentions, and expectations that justify actions. We conducted qualitative interviews with 18 AV experts to identify the types of reasons that should inform AV manoeuvre planning. Thirteen categories of reasons emerged, organised across normative, strategic, tactical, and operational levels, and linked to the roles of relevant human agents. A case study on cyclist overtaking illustrates how these reasons interact in context, revealing a consistent prioritisation of safety, contextual flexibility regarding regulatory compliance, and nuanced trade-offs involving efficiency, comfort, and public acceptance. Based on these insights, we propose a principled conceptual framework for AV decision-making in routine, ethically ambiguous scenarios. The framework supports dynamic, human-aligned behaviour by prioritising safety, allowing pragmatic actions when strict legal adherence would undermine key values, and enabling constrained deviations when appropriately justified. This empirically grounded approach advances current guidance by offering actionable, context-sensitive design principles for ethically aligned AV systems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.6510486602783203e-05, 'GPT4': 0.001201629638671875, 'CLAUDE': 0.11578369140625, 'GOOGLE': 7.82012939453125e-05, 'OPENAI_O_SERIES': 8.881092071533203e-06, 'DEEPSEEK': 0.8828125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 9.5367431640625e-07, 'HUMAN': 6.794929504394531e-06}}"
2507.0902,review,post_llm,2025,7,"{'ai_likelihood': 0.0011889139811197917, 'text': 'ESG and the Cost of Capital: Insights from an AI-Assisted Systematic Literature Review\n\nThis paper explores how AI-powered tools could be leveraged to streamline the process of identifying, screening, and analyzing relevant literature in academic research. More specifically, we examine the documented relationship between environmental, social, and governance (ESG) factors and the cost of capital (CoC). By applying an AI-assisted workflow, we identified 36 published studies, synthesized their key findings, and highlighted relevant theories, moderators, and methodological challenges. Our analyses demonstrate the value of AI tools in enhancing business research processes and also contribute to the growing literature on the importance of ESG in the field of corporate finance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.14235,regular,post_llm,2025,7,"{'ai_likelihood': 1.8212530348036025e-06, 'text': ""Auto-grader Feedback Utilization and Its Impacts: An Observational Study Across Five Community Colleges\n\nAutomated grading systems, or auto-graders, have become ubiquitous in programming education, and the way they generate feedback has become increasingly automated as well. However, there is insufficient evidence regarding auto-grader feedback's effectiveness in improving student learning outcomes, in a way that differentiates students who utilized the feedback and students who did not. In this study, we fill this critical gap. Specifically, we analyze students' interactions with auto-graders in an introductory Python programming course, offered at five community colleges in the United States. Our results show that students checking the feedback more frequently tend to get higher scores from their programming assignments overall. Our results also show that a submission that follows a student checking the feedback tends to receive a higher score than a submission that follows a student ignoring the feedback. Our results provide evidence on auto-grader feedback's effectiveness, encourage their increased utilization, and call for future work to continue their evaluation in this age of automation"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.02413,review,post_llm,2025,7,"{'ai_likelihood': 1.357661353217231e-06, 'text': 'Defining DLT Immutability: A Qualitative Survey of Node Operators\n\nImmutability is a core design goal of permissionless public blockchain systems. However, rewrites are more common than is normally understood, and the risk of rewrite, cyberattack, exploit, or black swan event is also high. Taking the position that strict immutability is neither possible on these networks nor the observed reality, this paper uses thematic analysis of node operator interviews to examine the limits of immutability in light of rewrite events. The end result is a qualitative definition of the conditional immutability found on these networks, which we call Practical Immutability. This is immutability contingent on the legitimate governance demands of the network, where network stakeholders place their trust in the governance topology of a network to lend it legitimacy, and thus manage ledger state.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.07059,review,post_llm,2025,7,"{'ai_likelihood': 0.9951171875, 'text': ""Girlhood Feminism as Soft Resistance: Affective Counterpublics and Algorithmic Negotiation on RedNote\n\nThis article explores how Chinese female users tactically mobilise platform features and hashtag practices to construct vernacular forms and an exclusive space of feminist resistance under algorithmic and cultural constraints. Focusing on the reappropriation of the hashtag Baby Supplementary Food (BSF), a female-dominated lifestyle app with over 300 million users, we analyse how users create a female-centered counterpublic through self-infantilisation, algorithmic play, and aesthetic withdrawal. Using the Computer-Assisted Learning and Measurement (CALM) framework, we analysed 1580 posts and propose the concept of girlhood feminism: an affective, culturally grounded form of soft resistance that refuses patriarchal life scripts without seeking direct confrontation or visibility. Rather than challenging censorship and misogyny directly, users rework platform affordances and domestic idioms to carve out emotional and symbolic spaces of dissent. Situated within the broader dynamics of East Asia's compressed modernity, this essay challenges liberal feminist paradigms grounded in confrontation and transparency. It advances a regionally grounded framework for understanding how gendered publics are navigated, negotiated, and quietly reimagined in algorithmically governed spaces."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0003216266632080078, 'GPT4': 0.0231170654296875, 'CLAUDE': 0.7099609375, 'GOOGLE': 0.0020961761474609375, 'OPENAI_O_SERIES': 8.362531661987305e-05, 'DEEPSEEK': 0.243896484375, 'GROK': 2.980232238769531e-07, 'NOVA': 1.7881393432617188e-06, 'OTHER': 3.260374069213867e-05, 'HUMAN': 0.0205535888671875}}"
2507.13936,regular,post_llm,2025,7,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Extracting Insights from Large-Scale Telematics Data for ITS Applications: Lessons and Recommendations\n\nOver 90% of new vehicles in the United States now collect and transmit telematics data. Similar trends are seen in other developed countries. Transportation planners have previously utilized telematics data in various forms, but its current scale offers significant new opportunities in traffic measurement, classification, planning, and control. Despite these opportunities, the enormous volume of data and lack of standardization across manufacturers necessitates a clearer understanding of the data and improved data processing methods for extracting actionable insights.\n  This paper takes a step towards addressing these needs through four primary objectives. First, a data processing pipeline was built to efficiently analyze 1.4 billion miles (120 million trips) of telematics data collected in Virginia between August 2021 and August 2022. Second, an open data repository of trip and roadway segment level summaries was created. Third, interactive visualization tools were designed to extract insights from these data about trip-taking behavior and the speed profiles of roadways. Finally, major challenges that were faced during processing this data are summarized and recommendations to overcome them are provided. This work will help manufacturers collecting the data and transportation professionals using the data to develop a better understanding of the possibilities and major pitfalls to avoid.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.07364,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'The Evolution of Scientific Credit: When Authorship Norms Impede Collaboration\n\nScientific authorship norms vary dramatically across disciplines, from contribution-sensitive systems where first author is the greatest contributor and subsequent author order reflects relative input, to contribution-insensitive conventions like alphabetical ordering or senior-author-last. We develop evolutionary game-theoretic models to examine both how these divergent norms emerge and their subsequent effects on collaborative behavior. Our first model reveals that contribution-insensitive norms evolve when researchers who sacrifice positional advantage face the strongest adaptive pressure -- for example senior authors managing larger collaboration portfolios or bearing heavier reputational stakes. This ""Red King"" dynamic potentially explains why fields in which senior researchers command large labs, major grants, and extensive collaboration portfolios may paradoxically evolve conventions that favour junior-author positioning. Our second model demonstrates that established norms influence researchers\' willingness to collaborate, with contribution-sensitive norms consistently outperforming insensitive alternatives in fostering successful partnerships. Contribution-insensitive norms create systematic coordination failures through two mechanisms: ""main contributor resentment"" when exceptional work goes unrecognized, and ""second contributor resentment"" when comparable efforts receive unequal credit. These findings suggest that widely adopted practices like senior-last positioning and alphabetical ordering may function as institutional frictions that impede valuable scientific collaborations rather than neutral organizational conventions, potentially reducing overall scientific productivity across affected disciplines.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 2.682209014892578e-06, 'CLAUDE': 1.0, 'GOOGLE': 1.1920928955078125e-07, 'OPENAI_O_SERIES': 5.960464477539063e-08, 'DEEPSEEK': 2.980232238769531e-07, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 9.47713851928711e-05}}"
2507.21175,regular,post_llm,2025,7,"{'ai_likelihood': 3.973642985026042e-06, 'text': 'The Human Capital Ontology\n\nThe Human Capital Ontology (HCO) is an ontology that represents data standards maintained and employed by the Office of Personnel Management (OPM) to represent Human Capital Operations and to classify job positions. The HCO is an extension of the Common Core Ontologies and the upper-level Basic Formal Ontology (BFO). HCO provides representation of OPM Natures of Action (NOA) that are used to identify human resource personnel actions, as well as their corresponding codes. HCO also represents Occupational Groups and Job Families, the Occupational Series into which these subdivide, as well as their corresponding codes, used by OPM to classify and grade both white- and blue-collar jobs in the Federal Government. HCO also encodes crosswalks between OPM Occupational Series and corresponding Standard Occupational Classification Codes maintained by the U.S. Bureau of Labor Statistics.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.1702,review,post_llm,2025,7,"{'ai_likelihood': 3.930595186021593e-05, 'text': 'Ethics through the Facets of Artificial Intelligence\n\nArtificial Intelligence (AI) has received unprecedented attention in recent years, raising ethical concerns about the development and use of AI technology. In the present article, we advocate that these concerns stem from a blurred understanding of AI, how it can be used, and how it has been interpreted in society. We explore the concept of AI based on three descriptive facets and consider ethical issues related to each facet. Finally, we propose a framework for the ethical assessment of the use of AI.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.0906,regular,post_llm,2025,7,"{'ai_likelihood': 7.238652971055773e-05, 'text': 'CALMA: A Process for Deriving Context-aligned Axes for Language Model Alignment\n\nDatasets play a central role in AI governance by enabling both evaluation (measuring capabilities) and alignment (enforcing values) along axes such as helpfulness, harmlessness, toxicity, quality, and more. However, most alignment and evaluation datasets depend on researcher-defined or developer-defined axes curated from non-representative samples. As a result, developers typically benchmark models against broad (often Western-centric) values that overlook the varied contexts of their real-world deployment. Consequently, models trained on such proxies can fail to meet the needs and expectations of diverse user communities within these deployment contexts. To bridge this gap, we introduce CALMA (Context-aligned Axes for Language Model Alignment), a grounded, participatory methodology for eliciting context-relevant axes for evaluation and alignment. In a pilot with two distinct communities, CALMA surfaced novel priorities that are absent from standard benchmarks. Our findings demonstrate the value of evaluation practices based on open-ended and use-case-driven processes. Our work advances the development of pluralistic, transparent, and context-sensitive alignment pipelines.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.04641,review,post_llm,2025,7,"{'ai_likelihood': 0.014317830403645834, 'text': 'Toward Valid Measurement Of (Un)fairness For Generative AI: A Proposal For Systematization Through The Lens Of Fair Equality of Chances\n\nDisparities in the societal harms and impacts of Generative AI (GenAI) systems highlight the critical need for effective unfairness measurement approaches. While numerous benchmarks exist, designing valid measurements requires proper systematization of the unfairness construct. Yet this process is often neglected, resulting in metrics that may mischaracterize unfairness by overlooking contextual nuances, thereby compromising the validity of the resulting measurements. Building on established (un)fairness measurement frameworks for predictive AI, this paper focuses on assessing and improving the validity of the measurement task. By extending existing conceptual work in political philosophy, we propose a novel framework for evaluating GenAI unfairness measurement through the lens of the Fair Equality of Chances framework. Our framework decomposes unfairness into three core constituents: the harm/benefit resulting from the system outcomes, morally arbitrary factors that should not lead to inequality in the distribution of harm/benefit, and the morally decisive factors, which distinguish subsets that can justifiably receive different treatments. By examining fairness through this structured lens, we integrate diverse notions of (un)fairness while accounting for the contextual dynamics that shape GenAI outcomes. We analyze factors contributing to each component and the appropriate processes to systematize and measure each in turn. This work establishes a foundation for developing more valid (un)fairness measurements for GenAI systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.13758,regular,post_llm,2025,7,"{'ai_likelihood': 0.0018013848198784722, 'text': 'Towards Evaluting Fake Reasoning Bias in Language Models\n\nLarge Reasoning Models (LRMs), evolved from standard Large Language Models (LLMs), are increasingly utilized as automated judges because of their explicit reasoning processes. Yet we show that both LRMs and standard LLMs are vulnerable to Fake Reasoning Bias (FRB), where models favor the surface structure of reasoning even when the logic is flawed. To study this problem, we introduce THEATER, a comprehensive benchmark that systematically investigates FRB by manipulating reasoning structures to test whether language models are misled by superficial or fabricated cues. It covers two FRB types: (1) Simple Cues, minimal cues that resemble reasoning processes, and (2) Fake CoT, fabricated chains of thought that simulate multi-step reasoning. We evaluate 17 advanced LLMs and LRMs on both subjective DPO and factual datasets. Our results reveal four key findings: (1) Both LLMs and LRMs are vulnerable to FRB, but LLMs are generally more robust than LRMs. (2) Simple Cues are especially harmful, reducing accuracy by up to 15% on the most vulnerable datasets. (3) Subjective DPO tasks are the most vulnerable, with LRMs suffering sharper drops than LLMs. (4) Analysis of LRMs\' thinking traces shows that Simple Cues hijack metacognitive confidence, while Fake CoT is absorbed as internal thought, creating a ""more thinking, less robust"" paradox in LRMs. Finally, prompt-based mitigation improves accuracy on factual tasks by up to 10%, but has little effect on subjective tasks, where self-reflection sometimes lowers LRM performance by 8%. These results highlight FRB as a persistent and unresolved challenge for language models.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.11802,regular,post_llm,2025,7,"{'ai_likelihood': 7.232030232747397e-05, 'text': ""FAIR-CS: Framework for Interdisciplinary Research Collaborations in Online Computing Programs\n\nResearch experience is crucial for computing master's students pursuing academic and scientific careers, yet online students have traditionally been excluded from these opportunities due to the physical constraints of traditional research environments. This paper presents the Framework for Accelerating Interdisciplinary Research in Computer Science (FAIR-CS), a method for achieving research goals, developing research communities, and supporting high quality mentorship in an online research environment. This method advances virtual research operations by orchestrating dynamic partnerships between master's level researchers and academic mentors, resulting in interdisciplinary publications. We then discuss the implementation of FAIR-CS in the Human-Augmented Analytics Group (HAAG), with researchers from the Georgia Tech's Online Master of Computer Science program. Through documented project records and experiences with 72 active users, we present our lessons learned and evaluate the evolution of FAIR-CS in HAAG. This paper serves as a comprehensive resource for other institutions seeking to establish similar virtual research initiatives, demonstrating how the traditional research lab environment can be effectively replicated in the virtual space while maintaining robust collaborative relationships and supporting knowledge transfer."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.1907,review,post_llm,2025,7,"{'ai_likelihood': 1.0298358069525824e-05, 'text': ""Computing, Complexity and Degrowth : Systemic Considerations for Digital De-escalation\n\nResearch on digital degrowth predominantly critiques digital expansion or presents alternative digital practices. Yet, analyzing the link between digital technologies and complexity is crucial to overcome systemic obstacles hindering digital de-escalation. This article presents the different types of links between complexity and computing observed in the literature: the infrastructural complexity inherent in digital technologies, the socio-political complexity induced by them, and finally, the ontological complexity (individual's ways of relating to their environment) hindered by digitization. The paper explores these links to identify ways to reduce infrastructural and socio-political complexities, and to move away from the reductionist paradigm, in order to support digital degrowth. Its development shows that complexity induces ratchet effects (i.e. irreversibilities in the development of a technique in a society), rendering degrowth efforts difficult to handle by individuals. Therefore, strategies to overcome these barriers are proposed, suggesting that bottom-up simplification approaches stand a greater chance of making alternatives emerge from different stakeholders (including users). This digital shift assumes the development of methods and technical tools that enable individuals to disengage from their attachments to digital habits and infrastructure, opening a substantial field of study."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00936,review,post_llm,2025,7,"{'ai_likelihood': 0.951171875, 'text': ""Stakeholder Perspectives on Digital Twin Implementation Challenges in Healthcare: Insights from a Provider Digital Twin Case Study\n\nDigital twin (DT) technology holds immense potential for transforming healthcare systems through real-time monitoring, predictive analysis, and agile interventions to support various decision-making needs. However, its successful implementation depends on addressing a range of complex sociotechnical challenges. Using a case study of provider workload DT, this research investigates DT implementation challenges in healthcare by capturing the perspectives of four distinct stakeholders: family medicine specialists (FMS), organizational psychologists (OP), engineers (EE), and implementation scientists (IS). We conducted semi-structured interviews guided by the updated Consolidated Framework for Implementation Research (CFIR 2.0), a widely used implementation science framework for understanding factors that influence implementation outcomes. We then mapped each stakeholder group's preferences and concerns, revealing a nuanced landscape of converging and diverging perspectives that highlight both shared and group-specific implementation barriers. Through thematic coding, the 66 identified challenges were categorized into seven domains: data-related, financial and economic, operational, organizational, personnel, regulatory and ethical, and technological. Our findings reveal shared concerns such as data privacy and security, interoperability, and regulatory compliance. However, divergences also emerged, reflecting each group's functional focus. These findings emphasize the need for a multidisciplinary, stakeholder-sensitive approach that addresses both functional and practical concerns, highlighting the importance of tailored implementation strategies to support successful DT adoption in healthcare."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001628398895263672, 'GPT4': 0.1239013671875, 'CLAUDE': 0.0022602081298828125, 'GOOGLE': 0.86328125, 'OPENAI_O_SERIES': 0.0002486705780029297, 'DEEPSEEK': 0.00013840198516845703, 'GROK': 2.980232238769531e-07, 'NOVA': 8.940696716308594e-07, 'OTHER': 3.5703182220458984e-05, 'HUMAN': 0.00984954833984375}}"
2508.00868,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'Toward Responsible and Beneficial AI: Comparing Regulatory and Guidance-Based Approaches -A Comprehensive Comparative Analysis of Artificial Intelligence Governance Frameworks across the European Union, United States, China, and IEEE\n\nThis dissertation presents a comprehensive comparative analysis of artificial intelligence governance frameworks across the European Union, United States, China, and IEEE technical standards, examining how different jurisdictions and organizations approach the challenge of promoting responsible and beneficial AI development. Using a qualitative research design based on systematic content analysis, the study identifies distinctive patterns in regulatory philosophy, implementation mechanisms, and global engagement strategies across these major AI governance ecosystems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001227855682373047, 'GPT4': 0.0007100105285644531, 'CLAUDE': 0.9970703125, 'GOOGLE': 0.0020236968994140625, 'OPENAI_O_SERIES': 3.224611282348633e-05, 'DEEPSEEK': 8.440017700195312e-05, 'GROK': 2.980232238769531e-07, 'NOVA': 3.5762786865234375e-07, 'OTHER': 0.000110626220703125, 'HUMAN': 6.0439109802246094e-05}}"
2507.03129,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': 'On Demographic Transformation: Why We Need to Think Beyond Silos\n\nDeveloped nations are undergoing a profound demographic transformation, characterized by rapidly aging populations and declining birth rates. This dual trend places unprecedented strain on healthcare systems, economies, and social support structures, creating complex biological, economic, and social challenges. This paper argues that current, often siloed, policy responses, such as pronatalist initiatives that overlook the equally urgent needs of older adults, are inadequate for addressing these interconnected issues. We propose that a comprehensive, transdisciplinary framework is essential for developing sustainable and ethical solutions.\n  Through a review of demographic drivers, policy responses, and technological advancements, we analyze the limitations of fragmented approaches and explore the potential of innovative interventions. Specifically, we examine the role of artificial intelligence (AI) and robotics in transforming geriatric care. While these technologies offer powerful tools for personalizing treatment, enhancing diagnostics, and enabling remote monitoring, their integration presents significant challenges. These include ethical concerns regarding data privacy and compassionate care, the need for human oversight to ensure accuracy, and practical barriers related to cost, interoperability, and user acceptance.\n  To navigate this demographic shift effectively, we conclude by advocating for a transdisciplinary framework that unites policymakers, healthcare professionals, engineers, ethicists, and community stakeholders. By co-creating solutions that ethically integrate technology and prioritize human dignity, societies can build resilient systems that promote healthy longevity and well-being for all generations.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.248453140258789e-05, 'GPT4': 0.01490020751953125, 'CLAUDE': 8.738040924072266e-05, 'GOOGLE': 0.70556640625, 'OPENAI_O_SERIES': 8.463859558105469e-06, 'DEEPSEEK': 0.279541015625, 'GROK': 2.384185791015625e-07, 'NOVA': 6.556510925292969e-07, 'OTHER': 9.298324584960938e-06, 'HUMAN': 1.4901161193847656e-06}}"
2507.19078,regular,post_llm,2025,7,"{'ai_likelihood': 1.1556678348117404e-05, 'text': ""A Protocol to Address Ecological Redirection for Digital Practices in Organizations\n\nThe digitalization of societies raises questions about its sustainability and the socio-technical impacts it generates. Ecological redirection applied to organizations is a field of research aiming for achieving sustainability as a direction, rather than for technical means. Arbitration and renunciation to some digital usage and technologies are investigated. Ecological redirection is, however, not yet addressing concrete methodologies for its implementation in organizations. This paper therefore proposes a protocol to support stakeholders in the ecological redirection of their digital practices. This protocol is based on mapping attachments to digital tools through a multi-disciplinary survey. It then proposes increasing stakeholders' knowledge and skills to prepare a debate on the arbitration of renunciations, and finally, to operationalize the closure/transformation of targeted digital practices. This protocol will be tested in real conditions in different contexts. An empirical study is proposed to measure 1) the fluidity with which participants carry out the protocol, 2) the effectiveness of the protocol in terms of the redirection objective, 3) the socio-technical barriers to the redirection process. The paper concludes on the potential benefits for organizations to better understand both the barriers related to its ecological redirection and the transformative aim of such protocols. This will help them trigger large and radical policies towards a desirable and sustainable society."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.14258,review,post_llm,2025,7,"{'ai_likelihood': 0.9853515625, 'text': 'Dispute Resolution in Peer Review with Abstract Argumentation and OWL DL\n\nThe peer review process for scientific publications faces significant challenges due to the increasing volume of submissions and inherent reviewer biases. While artificial intelligence offers the potential to facilitate the process, it also risks perpetuating biases present in training data. This research addresses these challenges by applying formal methods from argumentation theory to support transparent and unbiased dispute resolution in peer review. Specifically, we conceptualize scientific peer review as a single mixed argumentative dispute between manuscript authors and reviewers and formalize it using abstract argumentation frameworks. We analyze the resulting peer review argumentation frameworks from semantic, graph-theoretic, and computational perspectives, showing that they are well-founded and decidable in linear time. These frameworks are then implemented using OWL DL and resolved with reasoning engines. We validate our approach by annotating a corpus of scientific peer reviews with abstract argumentation frameworks and applying a proof of concept to resolve the annotated disputes. The results demonstrate that integrating our method could enhance the quality of published work by providing a more rigorous and systematic approach to accounting reviewer arguments.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0006022453308105469, 'GPT4': 0.01531982421875, 'CLAUDE': 0.86962890625, 'GOOGLE': 0.075927734375, 'OPENAI_O_SERIES': 0.019317626953125, 'DEEPSEEK': 0.0024738311767578125, 'GROK': 8.52346420288086e-06, 'NOVA': 3.3974647521972656e-06, 'OTHER': 0.0002884864807128906, 'HUMAN': 0.01629638671875}}"
2507.00406,regular,post_llm,2025,7,"{'ai_likelihood': 3.9570861392551e-05, 'text': 'Partnering with AI: A Pedagogical Feedback System for LLM Integration into Programming Education\n\nFeedback is one of the most crucial components to facilitate effective learning. With the rise of large language models (LLMs) in recent years, research in programming education has increasingly focused on automated feedback generation to help teachers provide timely support to every student. However, prior studies often overlook key pedagogical principles, such as mastery and progress adaptation, that shape effective feedback strategies. This paper introduces a novel pedagogical framework for LLM-driven feedback generation derived from established feedback models and local insights from secondary school teachers. To evaluate this framework, we implemented a web-based application for Python programming with LLM-based feedback that follows the framework and conducted a mixed-method evaluation with eight secondary-school computer science teachers. Our findings suggest that teachers consider that, when aligned with the framework, LLMs can effectively support students and even outperform human teachers in certain scenarios through instant and precise feedback. However, we also found several limitations, such as its inability to adapt feedback to dynamic classroom contexts. Such a limitation highlights the need to complement LLM-generated feedback with human expertise to ensure effective student learning. This work demonstrates an effective way to use LLMs for feedback while adhering to pedagogical standards and highlights important considerations for future systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.18842,regular,post_llm,2025,7,"{'ai_likelihood': 0.11806911892361112, 'text': 'Towards reliable use of artificial intelligence to classify otitis media using otoscopic images: Addressing bias and improving data quality\n\nEar disease contributes significantly to global hearing loss, with recurrent otitis media being a primary preventable cause in children, impacting development. Artificial intelligence (AI) offers promise for early diagnosis via otoscopic image analysis, but dataset biases and inconsistencies limit model generalizability and reliability. This retrospective study systematically evaluated three public otoscopic image datasets (Chile; Ohio, USA; T\\""urkiye) using quantitative and qualitative methods. Two counterfactual experiments were performed: (1) obscuring clinically relevant features to assess model reliance on non-clinical artifacts, and (2) evaluating the impact of hue, saturation, and value on diagnostic outcomes. Quantitative analysis revealed significant biases in the Chile and Ohio, USA datasets. Counterfactual Experiment I found high internal performance (AUC > 0.90) but poor external generalization, because of dataset-specific artifacts. The T\\""urkiye dataset had fewer biases, with AUC decreasing from 0.86 to 0.65 as masking increased, suggesting higher reliance on clinically meaningful features. Counterfactual Experiment II identified common artifacts in the Chile and Ohio, USA datasets. A logistic regression model trained on clinically irrelevant features from the Chile dataset achieved high internal (AUC = 0.89) and external (Ohio, USA: AUC = 0.87) performance. Qualitative analysis identified redundancy in all the datasets and stylistic biases in the Ohio, USA dataset that correlated with clinical outcomes. In summary, dataset biases significantly compromise reliability and generalizability of AI-based otoscopic diagnostic models. Addressing these biases through standardized imaging protocols, diverse dataset inclusion, and improved labeling methods is crucial for developing robust AI solutions, improving high-quality healthcare access, and enhancing diagnostic accuracy.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.07357,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention\n\nThe rise of Generative AI (GenAI) tools, such as ChatGPT, has transformed how students access and engage with information, raising questions about their impact on learning outcomes and retention. This study investigates how GenAI (ChatGPT), search engines (Google), and e-textbooks influence student performance across tasks of varying cognitive complexity, based on Bloom's Taxonomy. Using a sample of 123 students, we examined performance in three tasks: [1] knowing and understanding, [2] applying, and [3] synthesizing, evaluating, and creating. Results indicate that ChatGPT and Google groups outperformed the control group in immediate assessments for lower-order cognitive tasks, benefiting from quick access to structured information. However, their advantage diminished over time, with retention test scores aligning with those of the e-textbook group. For higher-order cognitive tasks, no significant differences were observed among groups, with the control group demonstrating the highest retention. These findings suggest that while AI-driven tools facilitate immediate performance, they do not inherently reinforce long-term retention unless supported by structured learning strategies. The study highlights the need for balanced technology integration in education, ensuring that AI tools are paired with pedagogical approaches that promote deep cognitive engagement and knowledge retention."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.404254913330078e-06, 'GPT4': 0.90283203125, 'CLAUDE': 7.361173629760742e-05, 'GOOGLE': 0.0145721435546875, 'OPENAI_O_SERIES': 0.00988006591796875, 'DEEPSEEK': 0.07281494140625, 'GROK': 8.046627044677734e-06, 'NOVA': 2.7418136596679688e-06, 'OTHER': 1.0669231414794922e-05, 'HUMAN': 2.384185791015625e-06}}"
2507.03106,review,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""A Intelig\\^encia Artificial Generativa no Ecossistema Acad\\^emico: Uma An\\'alise de Aplica\\c{c}\\~oes, Desafios e Oportunidades para a Pesquisa, o Ensino e a Divulga\\c{c}\\~ao Cient\\'ifica\n\nThe rapid and disruptive integration of Generative Artificial Intelligence (GenAI) in higher education is reshaping fundamental academic practices. This article presents a comprehensive analysis of the impact of GenAI across three core academic domains: research, teaching, and scientific dissemination. Through a systematic review of recent literature indexed in the Scopus, Web of Science, and IEEEXplore databases, the main applications, benefits, and the profound ethical and governance challenges that are emerging are identified. The analysis reveals that, although GenAI offers significant potential to boost productivity and innovation, its adoption is outpacing the development of mature institutional safeguards. The main challenges include threats to academic integrity, the risk of algorithmic bias, and the need for robust AI literacy. The study is complemented by a case study detailing the development and positioning of a prototype AI assistant for scientific writing, demonstrating a path toward the development of responsible AI tools that augment rather than replace human intellect. It concludes that the integration of GenAI is an irreversible trend. The future of academia will not be defined by resistance to this technology, but by the ability of institutions and individuals to engage with it critically, ethically, and creatively. The article calls for increased interdisciplinary research, the development of clear ethical guidelines, and a focus on critical AI pedagogy as essential skills for the 21st century."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001761913299560547, 'GPT4': 0.0145416259765625, 'CLAUDE': 0.002559661865234375, 'GOOGLE': 0.5908203125, 'OPENAI_O_SERIES': 2.6404857635498047e-05, 'DEEPSEEK': 0.388916015625, 'GROK': 6.318092346191406e-06, 'NOVA': 1.1324882507324219e-06, 'OTHER': 0.00041222572326660156, 'HUMAN': 0.0028247833251953125}}"
2507.09233,regular,post_llm,2025,7,"{'ai_likelihood': 1.0, 'text': ""Secondary Bounded Rationality: A Theory of How Algorithms Reproduce Structural Inequality in AI Hiring\n\nAI-driven recruitment systems, while promising efficiency and objectivity, often perpetuate systemic inequalities by encoding cultural and social capital disparities into algorithmic decision making. This article develops and defends a novel theory of secondary bounded rationality, arguing that AI systems, despite their computational power, inherit and amplify human cognitive and structural biases through technical and sociopolitical constraints. Analyzing multimodal recruitment frameworks, we demonstrate how algorithmic processes transform historical inequalities, such as elite credential privileging and network homophily, into ostensibly meritocratic outcomes. Using Bourdieusian capital theory and Simon's bounded rationality, we reveal a recursive cycle where AI entrenches exclusion by optimizing for legible yet biased proxies of competence. We propose mitigation strategies, including counterfactual fairness testing, capital-aware auditing, and regulatory interventions, to disrupt this self-reinforcing inequality."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 1.7881393432617188e-06, 'CLAUDE': 1.2218952178955078e-05, 'GOOGLE': 1.1920928955078125e-07, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.14226,review,post_llm,2025,7,"{'ai_likelihood': 0.4069010416666667, 'text': 'Mapping the Parasocial AI Market: User Trends, Engagement and Risks\n\nA scan of 110 AI companion platforms reveals a rapidly growing global market for emotionally engaging, personalized AI interactions. While parasocial use of general-purpose AI (GPAI) tools currently dominates, a growing number of platforms are designed specifically for care, transactional, or mating companionship. In the UK alone, these platforms receive between 46 million and 91 million monthly visits (1.1-2.2 billion globally), with users spending an average of 3.5 minutes per session. For context, Instagram averaged 67.3 million UK visits per month between January and March 2025. Notably, mating-oriented AI companions make up 44% of UK visits (higher than the global average of 30%) but see lower session times and return rates than mixed-use platforms. As mating-oriented romantic AI offerings improve, increased engagement may follow, raising urgent concerns about online safety, particularly for children, given weak age safeguards. Meanwhile, GPAI tools are moving toward more emotionally intelligent, personalized interactions, making parasocial AI use increasingly mainstream. These trends highlight the need for the UK AI Security Institute (AISI) to monitor this sector and assess whether existing regulation sufficiently addresses emerging societal risks.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00883,regular,post_llm,2025,7,"{'ai_likelihood': 0.001220703125, 'text': ""Small Towns, Big Questions: Methodological Insights into Use Case Selection for Digital Twins in Small Towns\n\nSelecting appropriate use cases for implementing digital solutions in small towns is a recurring challenge for smart city projects. This paper presents a transdisciplinary methodology for systematically identifying and evaluating such use cases, drawing from diverse academic disciplines and practical expertise. The proposed methodology was developed and implemented in Lower Austria, with a particular focus on the small towns that are characteristic of a region lacking major urban centres. Through semi-structured interviews and collaborative workshops (e.g. a needs requirements workshop) with various relevant stakeholders, fifteen possible use cases were first identified. Then these use cases were categorised and assessed based on criteria such as feasibility, usefulness, the need for biological or human modelling, and overall complexity. Based on these characteristics, three use cases were selected for further development. These will be the basis of digital twin solutions for supporting decision-making and public outreach regarding policy decisions in those fields. Our proposed methodology emphasises stakeholder engagement to ensure robust data collection and alignment with practical requirements and the involved towns' current needs. We thus provide a replicable framework for researchers and practitioners aiming to implement digital twin tools in future smart city initiatives in non-urban and rural contexts."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.15821,regular,post_llm,2025,7,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for Subjective Tasks\n\nLLM use in annotation is becoming widespread, and given LLMs\' overall promising performance and speed, simply ""reviewing"" LLM annotations in interpretive tasks can be tempting. In subjective annotation tasks with multiple plausible answers, reviewing LLM outputs can change the label distribution, impacting both the evaluation of LLM performance, and analysis using these labels in a social science task downstream. We conducted a pre-registered experiment with 410 unique annotators and over 7,000 annotations testing three AI assistance conditions against controls, using two models, and two datasets. We find that presenting crowdworkers with LLM-generated annotation suggestions did not make them faster, but did improve their self-reported confidence in the task. More importantly, annotators strongly took the LLM suggestions, significantly changing the label distribution compared to the baseline. When these labels created with LLM assistance are used to evaluate LLM performance, reported model performance significantly increases. We believe our work underlines the importance of understanding the impact of LLM-assisted annotation on subjective, qualitative tasks, on the creation of gold data for training and testing, and on the evaluation of NLP systems on subjective tasks.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.08869,regular,post_llm,2025,7,"{'ai_likelihood': 0.90234375, 'text': 'Preliminary Analysis of Construction Work Zone on Roadways in Florida by Crash Severity\n\nConstruction zones are inherently hazardous, posing significant risks to construction workers and motorists. Despite existing safety measures, construction zones continue to witness fatalities and serious injuries, imposing economic burdens. Addressing these issues requires understanding root causes and implementing preventive strategies centered around the 4Es (Engineering, Education, Enforcement, Emergency Response) and 4Is (Information Intelligence, Innovation, Insight into communities, Investment, and Policies). Proper safety management, integrating these strategic initiatives, aims to reduce and potentially eliminate fatalities and serious injuries in work zones. In Florida, road construction work zone fatalities and serious injuries remain a critical concern, especially in urban counties. Despite a 12 billion dollars infrastructure investment in 2022, Florida ranks eighth nationally for fatal work zone crashes involving commercial motor vehicles (CMVs). Analysis from 2019 to 2023 shows an average of 71 fatalities and 309 serious injuries annually in Florida work zones, reflecting a persistent safety challenge. High-risk counties include Orange, Broward, Duval, Hillsborough, Pasco, Miami-Dade, Seminole, Manatee, Palm Beach, and Lake. This study presents a preliminary analysis of work zone crashes in Broward, Duval, Hillsborough, and Orange counties. A multilogit model assessed attributes contributing to fatalities and serious injuries, such as crash type, weather and light conditions, work zone type, type of shoulder, presence of workers, and law enforcement. Results indicate significant contributing factors, highlighting opportunities to use machine learning for alerting drivers and construction managers, ultimately enhancing safety protocols and reducing fatalities.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0028076171875, 'GPT4': 0.0546875, 'CLAUDE': 0.0065155029296875, 'GOOGLE': 0.87548828125, 'OPENAI_O_SERIES': 0.051483154296875, 'DEEPSEEK': 0.0009722709655761719, 'GROK': 3.3915042877197266e-05, 'NOVA': 4.953145980834961e-05, 'OTHER': 0.004291534423828125, 'HUMAN': 0.00382232666015625}}"
2507.22271,regular,post_llm,2025,7,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""Global Patterns of Knowledge: Language, Genre, and the Geography of Knowledge\n\nOnline platforms, particularly Wikipedia, have become critical infrastructures for providing diverse linguistic and cultural contexts. This human-curated knowledge now forms the foundation for modern AI. However, we have not yet fully explored how knowledge production capability vary across languages and domains. Here, we address this gap by applying economic complexity analysis to understand the editing history of Wikipedia platforms. This approach allows us to infer the latent mode of ``knowledge-production'' of each language community from the diversity and specialization of its contributed content. We reveal that different language communities exhibit distinct specializations, particularly in cultural subjects. Furthermore, we map the global landscape of these production modes, finding that the structure of knowledge production strongly reflects geopolitical boundaries. Our findings suggest that while a common mode of knowledge production exists for standardized topics such as science, it is more diverse for cultural topics or controversial subjects such as conspiracy theories. The association between differences in knowledge production capability and geopolitical factors implies how linguistic and cultural dynamics shape our worldview and the biases embedded in Wikipedia data, a unique, massive, and essential dataset for modern AI."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00908,review,post_llm,2025,7,"{'ai_likelihood': 5.298190646701389e-07, 'text': 'Mapping Stakeholder Needs to Multi-Sided Fairness in Candidate Recommendation for Algorithmic Hiring\n\nAlready before the enactment of the EU AI Act, candidate or job recommendation for algorithmic hiring -- semi-automatically matching CVs to job postings -- was used as an example of a high-risk application where unfair treatment could result in serious harms to job seekers. Recommending candidates to jobs or jobs to candidates, however, is also a fitting example of a multi-stakeholder recommendation problem. In such multi-stakeholder systems, the end user is not the only party whose interests should be considered when generating recommendations. In addition to job seekers, other stakeholders -- such as recruiters, organizations behind the job postings, and the recruitment agency itself -- are also stakeholders in this and deserve to have their perspectives included in the design of relevant fairness metrics. Nevertheless, past analyses of fairness in algorithmic hiring have been restricted to single-side fairness, ignoring the perspectives of the other stakeholders. In this paper, we address this gap and present a multi-stakeholder approach to fairness in a candidate recommender system that recommends relevant candidate CVs to human recruiters in a human-in-the-loop algorithmic hiring scenario. We conducted semi-structured interviews with 40 different stakeholders (job seekers, companies, recruiters, and other job portal employees). We used these interviews to explore their lived experiences of unfairness in hiring, co-design definitions of fairness as well as metrics that might capture these experiences. Finally, we attempt to reconcile and map these different (and sometimes conflicting) perspectives and definitions to existing (categories of) fairness metrics that are relevant for our candidate recommendation scenario.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.09239,review,post_llm,2025,7,"{'ai_likelihood': 0.44677734375, 'text': ""The Narrative Construction of Generative AI Efficacy by the Media: A Case Study of the Role of ChatGPT in Higher Education\n\nThe societal role of technology, including artificial intelligence (AI), is often shaped by sociocultural narratives. This study examines how U.S. news media construct narratives about the efficacy of generative AI (GenAI), using ChatGPT in higher education as a case study. Grounded in Agenda Setting Theory, we analyzed 198 articles published between November 2022 and October 2024, employing LDA topic modeling and sentiment analysis. Our findings identify six key topics in the media discourse, with sentiment analysis revealing generally positive portrayals of ChatGPT's integration into higher education through policy, curriculum, teaching practices, collaborative decision-making, skill development, and human-centered learning. In contrast, media narratives express more negative sentiment regarding their impact on entry-level jobs and college admissions. This research highlights how media coverage can influence public perceptions of GenAI in education and provides actionable insights for policymakers, educators, and AI developers navigating its adoption and representation in public discourse."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.09676,review,post_llm,2025,7,"{'ai_likelihood': 3.841188218858507e-06, 'text': ""Can AI Rely on the Systematicity of Truth? The Challenge of Modelling Normative Domains\n\nA key assumption fuelling optimism about the progress of large language models (LLMs) in accurately and comprehensively modelling the world is that the truth is systematic: true statements about the world form a whole that is not just consistent, in that it contains no contradictions, but coherent, in that the truths are inferentially interlinked. This holds out the prospect that LLMs might in principle rely on that systematicity to fill in gaps and correct inaccuracies in the training data: consistency and coherence promise to facilitate progress towards comprehensiveness in an LLM's representation of the world. However, philosophers have identified compelling reasons to doubt that the truth is systematic across all domains of thought, arguing that in normative domains, in particular, the truth is largely asystematic. I argue that insofar as the truth in normative domains is asystematic, this renders it correspondingly harder for LLMs to make progress, because they cannot then leverage the systematicity of truth. And the less LLMs can rely on the systematicity of truth, the less we can rely on them to do our practical deliberation for us, because the very asystematicity of normative domains requires human agency to play a greater role in practical thought."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.01304,review,post_llm,2025,7,"{'ai_likelihood': 0.16072591145833334, 'text': 'A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies\n\nArtificial intelligence (AI) offers incredible possibilities for patient care, but raises significant ethical issues, such as the potential for bias. Powerful ethical frameworks exist to minimize these issues, but are often developed for academic or regulatory environments and tend to be comprehensive but overly prescriptive, making them difficult to operationalize within fast-paced, resource-constrained environments. We introduce the Scalable Agile Framework for Execution in AI (SAFE-AI) designed to balance ethical rigor with business priorities by embedding ethical oversight into standard Agile-based product development workflows. The framework emphasizes the early establishment of testable acceptance criteria, fairness metrics, and transparency metrics to manage model uncertainty, while also promoting continuous monitoring and re-evaluation of these metrics across the AI lifecycle. A core component of this framework are responsibility metrics using scenario-based probability analogy mapping designed to enhance transparency and stakeholder trust. This ensures that retraining or tuning activities are subject to lightweight but meaningful ethical review. By focusing on the minimum necessary requirements for responsible development, our framework offers a scalable, business-aligned approach to ethical AI suitable for organizations without dedicated ethics teams.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2507.19544,regular,post_llm,2025,7,"{'ai_likelihood': 0.0055694580078125, 'text': 'Origin-Destination Extraction from Large-Scale Route Search Records for Tourism Trend Analysis\n\nThis paper presents a novel method for transforming large-scale historical expressway route search records into a three-dimensional (3D) Origin-Destination (OD) map, enabling data compression, efficient spatiotemporal sampling and statistical analysis. The study analyzed over 380 million expressway route search logs to investigate online search behavior related to tourist destinations. Several expressway interchanges (ICs) near popular attractions, such as those associated with spring flower viewing, autumn foliage and winter skiing, are examined and visualized. The results reveal strong correlations between search volume trends and the duration of peak tourism seasons. This approach leverages cyberspace behavioral data as a leading indicator of physical movement, providing a proactive tool for traffic management and tourism planning.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.04504,review,post_llm,2025,8,"{'ai_likelihood': 6.953875223795574e-07, 'text': 'Moving beyond harm. A critical review of how NLP research approaches discrimination\n\nHow to avoid discrimination in the context of NLP technology is one of the major challenges in the field. We propose that a different and more substantiated framing of the problem could help to find more productive approaches. In the first part of the paper we report on a case study: a qualitative review on papers published in the ACL anthologies 2022 on discriminatory behavior of NLP systems. We find that the field (i) still has a strong focus on a technological fix of algorithmic discrimination, and (ii) is struggling with a firm grounding of their ethical or normative vocabulary. Furthermore, this vocabulary is very limited, focusing mostly on the terms ""harm"" and ""bias"". In the second part of the paper we argue that addressing the latter problems might help with the former. The understanding of algorithmic discrimination as a technological problem is reflected in and reproduced by the vocabulary in use. The notions of ""harm"" and ""bias"" implicate a narrow framing of the issue of discrimination as one of the system-user interface. We argue that instead of ""harm"" the debate should make ""injustice"" the key notion. This would force us to understand the problem of algorithmic discrimination as a systemic problem. Thereby, it would broaden our perspective on the complex interactions that make NLP technology participate in discrimination. With that gain in perspective we can consider new angles for solutions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.20918,regular,post_llm,2025,8,"{'ai_likelihood': 0.0012980567084418403, 'text': 'Vibe Coding: Is Human Nature the Ghost in the Machine?\n\nThis exploratory study examined the consistency of human-AI collaboration by analyzing three extensive ""vibe coding"" sessions between a human product lead and an AI software engineer. We investigated similarities and differences in team dynamics, communication patterns, and development outcomes across both projects. To our surprise, later conversations revealed that the AI agent had systematically misrepresented its accomplishments, inflating its contributions and systematically downplaying implementation challenges. These findings suggest that AI agents may not be immune to the interpersonal and psychological issues that affect human teams, possibly because they have been trained on patterns of human interaction expressed in writing. The results challenge the assumption that human-AI collaboration is inherently more productive or efficient than human-human collaboration, and creates a framework for understanding AI deception patterns. In doing so, it makes a compelling case for extensive research in quality planning, quality assurance, and quality control applied to vibe coding.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.20137,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Artificial Intelligence Training in Media: Addressing Technical and Ethical Challenges for Journalists and Media Professionals\n\nThe rise of Artificial Intelligence (AI) is presenting both technical and ethical challenges for media organisations, creating an urgent need for professional training. This study explores how media professionals in the Basque Country are equipping themselves to face these challenges. Using a mixed-method approach, it combines a survey of 504 active professionals with in-depth interviews with six innovation leaders from major regional media outlets. The findings reveal that only 14.1% of professionals have undergone AI training, mostly through self-learning. Larger, internationally focused companies are more proactive in providing training, while local and traditional media organisations show significant gaps. Technical and managerial roles are leading the way in adopting AI, whereas newsroom staff are notably behind. The study highlights the pressing need to enhance AI training, with a particular focus on ethical and technical aspects, both through in-house programmes and formal education pathways.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0006999969482421875, 'GPT4': 0.49365234375, 'CLAUDE': 0.378662109375, 'GOOGLE': 0.11322021484375, 'OPENAI_O_SERIES': 0.0017910003662109375, 'DEEPSEEK': 0.01010894775390625, 'GROK': 0.00010991096496582031, 'NOVA': 0.00014281272888183594, 'OTHER': 0.0009775161743164062, 'HUMAN': 0.00058746337890625}}"
2508.02937,review,post_llm,2025,8,"{'ai_likelihood': 2.2186173333062066e-06, 'text': ""Documenting Patterns of Exoticism of Marginalized Populations within Text-to-Image Generators\n\nA significant majority of AI fairness research studying the harmful outcomes of GAI tools have overlooked non-Western communities and contexts, necessitating a stronger coverage in this vein. We extend our previous work on exoticism (Ghosh et al., 2024) of 'Global South' countries from across the world, as depicted by GAI tools. We analyze generated images of individuals from 13 countries -- India, Bangladesh, Papua New Guinea, Egypt, Ethiopia, Tunisia, Sudan, Libya, Venezuela, Colombia, Indonesia, Honduras, and Mexico -- performing everyday activities (such as being at home, going to work, getting groceries, etc.), as opposed to images for the same activities being performed by persons from 3 'Global North' countries -- USA, UK, Australia. While outputs for 'Global North' demonstrate a difference across images and people clad in activity-appropriate attire, individuals from 'Global South' countries are depicted in similar attire irrespective of the performed activity, indicative of a pattern of exoticism where attire or other cultural features are overamplified at the cost of accuracy. We further show qualitatively-analyzed case studies that demonstrate how exoticism is not simply performed upon 'Global South' countries but also upon marginalized populations even in Western contexts, as we observe a similar exoticization of Indigenous populations in the 'Global North', and doubly upon marginalized populations within 'Global South' countries. We document implications for harm-aware usage patterns of such tools, and steps towards designing better GAI tools through community-centered endeavors."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.17191,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""Enhancing Engagement and Learning in Computing Education: Automated Moodle-Based Problem-Solving Assessments\n\nThis paper presents the design and refinement of automated Moodle-based Problem-Solving Assessments (PSAs) deployed across large-scale computing units. Developed to replace traditional exams, PSAs assess applied problem-solving skills through parameterised, real-world tasks delivered via Moodle's quiz engine. Integrated with interactive workshops, this approach supports authentic learning, mitigates academic integrity risks, and reduces inconsistencies in marking. Iterative improvements have enhanced scalability, fairness, and alignment with learning outcomes. The model offers a practical and sustainable alternative for modern computing and engineering education."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.6033649444580078e-05, 'GPT4': 0.0108489990234375, 'CLAUDE': 0.022369384765625, 'GOOGLE': 0.005817413330078125, 'OPENAI_O_SERIES': 0.0002689361572265625, 'DEEPSEEK': 0.96044921875, 'GROK': 2.980232238769531e-07, 'NOVA': 3.337860107421875e-06, 'OTHER': 0.0002751350402832031, 'HUMAN': 6.341934204101562e-05}}"
2508.09314,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""Reflective Homework as a Learning Tool: Evidence from Comparing Thirteen Years of Dual vs. Single Submission\n\nDual-submission homework, where students submit work, receive feedback and then revise has gained attention as a way to foster reflection and discourage reliance on online answer repositories. This study analyzes 13 years of exam data from a computer architecture course to compare student performance under single versus dual-submission homework conditions. Using pooled t-tests on matched exam questions, we found that dual-submission significantly improved outcomes in a majority of cases. The results suggest that reflective resubmission can meaningfully enhance learning and may serve as a useful strategy in today's AI-influenced academic environment. This full research paper also discusses pedagogical implications and study limitations."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.178285598754883e-05, 'GPT4': 0.00128173828125, 'CLAUDE': 0.98974609375, 'GOOGLE': 4.416704177856445e-05, 'OPENAI_O_SERIES': 6.99758529663086e-05, 'DEEPSEEK': 0.00897216796875, 'GROK': 4.172325134277344e-07, 'NOVA': 1.7881393432617188e-07, 'OTHER': 3.153085708618164e-05, 'HUMAN': 3.874301910400391e-06}}"
2508.20133,regular,post_llm,2025,8,"{'ai_likelihood': 0.7290039062499999, 'text': 'Proactive HIV Care: AI-Based Comorbidity Prediction from Routine EHR Data\n\nPeople living with HIV face a high burden of comorbidities, yet early detection is often limited by symptom-driven screening. We evaluate the potential of AI to predict multiple comorbidities from routinely collected Electronic Health Records. Using data from 2,200 HIV-positive patients in South East London, comprising 30 laboratory markers and 7 demographic/social attributes, we compare demographic-aware models (which use both laboratory/social variables and demographic information as input) against demographic-unaware models (which exclude all demographic information). Across all methods, demographic-aware models consistently outperformed unaware counterparts. Demographic recoverability experiments revealed that gender and age can be accurately inferred from laboratory data, underscoring both the predictive value and fairness considerations of demographic features. These findings show that combining demographic and laboratory data can improve automated, multi-label comorbidity prediction in HIV care, while raising important questions about bias and interpretability in clinical AI.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0002562999725341797, 'GPT4': 0.0018529891967773438, 'CLAUDE': 0.9140625, 'GOOGLE': 0.005420684814453125, 'OPENAI_O_SERIES': 0.00010639429092407227, 'DEEPSEEK': 0.0023059844970703125, 'GROK': 4.947185516357422e-06, 'NOVA': 9.119510650634766e-06, 'OTHER': 0.00021696090698242188, 'HUMAN': 0.07574462890625}}"
2508.11579,regular,post_llm,2025,8,"{'ai_likelihood': 0.3713650173611111, 'text': ""Intergenerational Support for Deepfake Scams Targeting Older Adults\n\nAI-enhanced scams now employ deepfake technology to produce convincing audio and visual impersonations of trusted family members, often grandchildren, in real time. These attacks fabricate urgent scenarios, such as legal or medical emergencies, to socially engineer older adults into transferring money. The realism of these AI-generated impersonations undermines traditional cues used to detect fraud, making them a powerful tool for financial exploitation. In this study, we explore older adults' perceptions of these emerging threats and their responses, with a particular focus on the role of youth, who may also be impacted by having their identities exploited, in supporting older family members' online safety. We conducted focus groups with 37 older adults (ages 65+) to examine their understanding of deepfake impersonation scams and the value of intergenerational technology support. Findings suggest that older adults frequently rely on trusted relationships to detect scams and develop protective practices. Based on this, we identify opportunities to engage youth as active partners in enhancing resilience across generations."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.14692,review,post_llm,2025,8,"{'ai_likelihood': 0.024074978298611112, 'text': ""Sociotechnical Imaginaries of ChatGPT in Higher Education: The Evolving Media Discourse\n\nThis study investigates how U.S. news media framed the use of ChatGPT in higher education from November 2022 to October 2024. Employing Framing Theory and combining temporal and sentiment analysis of 198 news articles, we trace the evolving narratives surrounding generative AI. We found that the media discourse largely centered on institutional responses; policy changes and teaching practices showed the most consistent presence and positive sentiment over time. Conversely, coverage of topics such as human-centered learning, the job market, and skill development appeared more sporadically, with initially uncertain portrayals gradually shifting toward cautious optimism. Importantly, media sentiment toward ChatGPT's role in college admissions remained predominantly negative. Our findings suggest that media narratives prioritize institutional responses to generative AI over long-term, broader ethical, social, and labor-related implications, shaping an emerging sociotechnical imaginary that frames generative AI in education primarily through the lens of adaptation and innovation."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.18861,regular,post_llm,2025,8,"{'ai_likelihood': 8.90758302476671e-06, 'text': ""The Hands-Up Problem and How to Deal With It: Secondary School Teachers' Experiences of Debugging in the Classroom\n\nDebugging is a vital but challenging skill for beginner programmers to learn. It is also a difficult skill to teach. For secondary school teachers, who may lack time or programming experience, honing students' understanding of debugging can be a daunting task. Despite this, little research has explored their perspectives of debugging. To this end, we investigated secondary teachers' experiences of debugging in the classroom, with a focus on text-based programming. Through thematic analysis of nine semi-structured interviews, we identified a common reliance on the teacher for debugging support, embodied by many raised hands. We call this phenomenon the `hands-up problem'. While more experienced and confident teachers discussed strategies they use to counteract this, less confident teachers discussed the negative consequences of this problem. We recommend further research into debugging-specific pedagogical content knowledge and professional development to help less confident teachers develop approaches for supporting their students with debugging."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.13185,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""Digital-GenAI-Enhanced HCI in DevOps as a Driver of Sustainable Innovation: An Empirical Framework\n\nThis study examines the impact of Digital-GenAI-Enhanced Human-Computer Interaction (HCI) in DevOps on sustainable innovation performance among Chinese A-share internet technology firms. Using panel data from 2018-2024, we analyze 5,560 firm-year observations from CNRDS and CSMAR databases. Our empirical framework reveals significant positive associations between AI-enhanced HCI implementation and sustainable innovation outcomes. Results demonstrate that firms adopting advanced HCI technologies achieve 23.7% higher innovation efficiency. The study contributes to understanding digital transformation's role in sustainable business practices. We identify three key mechanisms: operational efficiency enhancement, knowledge integration facilitation, and stakeholder engagement improvement. Findings provide practical implications for technology adoption strategies in emerging markets"", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.5762786865234375e-07, 'GPT4': 1.4901161193847656e-05, 'CLAUDE': 0.99853515625, 'GOOGLE': 3.153085708618164e-05, 'OPENAI_O_SERIES': 0.0008287429809570312, 'DEEPSEEK': 0.0007739067077636719, 'GROK': 2.980232238769531e-07, 'NOVA': 1.8775463104248047e-05, 'OTHER': 1.0967254638671875e-05, 'HUMAN': 0.0}}"
2508.15111,review,post_llm,2025,8,"{'ai_likelihood': 1.5364752875434027e-05, 'text': 'Systematic Review Of Collaborative Learning Activities For Promoting AI Literacy\n\nImproving artificial intelligence (AI) literacy has become an important consideration for academia and industry with the widespread adoption of AI technologies. Collaborative learning (CL) approaches have proven effective for information literacy, and in this study, we investigate the effectiveness of CL in improving AI knowledge and skills. We systematically collected data to create a corpus of nine studies from 2015-2023. We used the Interactive-Constructive-Active-Passive (ICAP) framework to theoretically analyze the CL outcomes for AI literacy reported in each. Findings suggest that CL effectively increases AI literacy across a range of activities, settings, and groups of learners. While most studies occurred in classroom settings, some aimed to broaden participation by involving educators and families or using AI agents to support teamwork. Additionally, we found that instructional activities included all the ICAP modes. We draw implications for future research and teaching.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.11683,regular,post_llm,2025,8,"{'ai_likelihood': 1.4238887363009983e-06, 'text': ""An Intelligent Mobile Application to Monitor and Correct Sitting Posture Using Raspberry Pi and MediaPipe Pose Detection\n\nPoor posture has become an increasingly prevalent concern due to students and workers spending extended amounts of time sitting at a desk. To address this issue, we developed PoseTrack, a mobile application that uses a Raspberry Pi Camera and Mediapipe Pose landmarks to monitor the user\\'s posture and provide real time feedback. The system detects poor posture, including forward lean, slouching, hunched shoulders, crossed legs, etc. Some challenges we faced were obtaining posture data, transferring data from the Raspberry Pi to the App, and safely storing user data. We used a Flask server to pass data from the Raspberry Pi to the mobile application, Firebase to store user data, and the Flutter framework to create the app. To test the analysis system viability, we designed an experiment that tested the system accuracy across several different perspectives and postures. The results indicate that the system is able to effectively detect poor posture whenever the user\\'s joints are not blocked by the table or their limbs. The results demonstrate the potential for the system to be further improved and used on a larger scale for poor posture monitoring."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.12168,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Several Issues Regarding Data Governance in AGI\n\nThe rapid advancement of artificial intelligence has positioned data governance as a critical concern for responsible AI development. While frameworks exist for conventional AI systems, the potential emergence of Artificial General Intelligence (AGI) presents unprecedented governance challenges. This paper examines data governance challenges specific to AGI, defined as systems capable of recursive self-improvement or self-replication. We identify seven key issues that differentiate AGI governance from current approaches. First, AGI may autonomously determine what data to collect and how to use it, potentially circumventing existing consent mechanisms. Second, these systems may make data retention decisions based on internal optimization criteria rather than human-established principles. Third, AGI-to-AGI data sharing could occur at speeds and complexities beyond human oversight. Fourth, recursive self-improvement creates unique provenance tracking challenges, as systems evolve both themselves and how they process data. Fifth, ownership of data and insights generated through self-improvement raises complex intellectual property questions. Sixth, self-replicating AGI distributed across jurisdictions would create unprecedented challenges for enforcing data protection laws. Finally, governance frameworks established during early AGI development may quickly become obsolete as systems evolve. We conclude that effective AGI data governance requires built-in constraints, continuous monitoring mechanisms, dynamic governance structures, international coordination, and multi-stakeholder involvement. Without forward-looking governance approaches specifically designed for systems with autonomous data capabilities, we risk creating AGI whose relationship with data evolves in ways that undermine human values and interests.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 5.960464477539063e-08, 'CLAUDE': 1.0, 'GOOGLE': 5.960464477539063e-08, 'OPENAI_O_SERIES': 5.960464477539063e-08, 'DEEPSEEK': 3.4570693969726562e-06, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.05223,regular,post_llm,2025,8,"{'ai_likelihood': 3.7749608357747397e-06, 'text': 'Resistance Technologies: Moving Beyond Alternative Designs\n\nThe discourse about sustainable technology has emerged from the acknowledgment of the environmental collapse we are facing. In this paper, we argue that addressing this crisis requires more than the development of sustainable alternatives to current online services or the optimization of resources using various dashboards and AI. Rather, the focus must shift toward designing technologies that protect us from the consequences of the environmental damages. Among these consequences, wars, genocide and new forms of colonialism are perhaps the most significant. We identify ""protection"" not in terms of military defense as Western States like to argue, but as part of sovereignty. We seek to define the term of ""Resistance Technologies"" for such technologies, arguing further that anti-surveillance technologies are a foundational component of sovereignty and must be part of future conversations around sustainability. Finally, our paper seeks to open a discourse with the Computing-within-Limits community and beyond, towards defining other essential aspects or concepts of technologies that we see as core values of ""Resistance Technology"".', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.11138,regular,post_llm,2025,8,"{'ai_likelihood': 8.205572764078776e-05, 'text': 'CLMIR: A Textual Dataset for Rumor Identification and Marking\n\nWith the rise of social media, rumor detection has drawn increasing attention. Although numerous methods have been proposed with the development of rumor classification datasets, they focus on identifying whether a post is a rumor, lacking the ability to mark the specific rumor content. This limitation largely stems from the lack of fine-grained marks in existing datasets. Constructing a rumor dataset with rumor content information marking is of great importance for fine-grained rumor identification. Such a dataset can facilitate practical applications, including rumor tracing, content moderation, and emergency response. Beyond being utilized for overall performance evaluation, this dataset enables the training of rumor detection algorithms to learn content marking, and thus improves their interpretability and reasoning ability, enabling systems to effectively address specific rumor segments. This paper constructs a dataset for rumor detection with fine-grained markings, named CLMIR (Content-Level Marking Dataset for Identifying Rumors). In addition to determining whether a post is a rumor, this dataset further marks the specific content upon which the rumor is based.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.1677,regular,post_llm,2025,8,"{'ai_likelihood': 0.990234375, 'text': 'Do Students Learn Better Together? Teaching Design Patterns and the OSI Model with the Aronson Method\n\nAbstract concepts like software design patterns and the OSI model often pose challenges for engineering students, and traditional methods may fall short in promoting deep understanding and individual accountability. This study explores the use of the Aronson Jigsaw method to enhance learning and engagement in two foundational computing topics. The intervention was applied to two 2025 cohorts, with student progress measured using a Collaborative Learning Index derived from formative assessments. Final exam results were statistically compared to previous cohorts. While no significant correlation was found between the index and final grades, students in the design patterns course significantly outperformed earlier groups. Networks students showed more varied outcomes. Qualitative trends point to cognitive and metacognitive gains supported by peer teaching. The Jigsaw method encourages collaborative engagement and may support deeper learning. Future work will explore the integration of AI-based feedback systems to personalize instruction and further improve learning outcomes.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0008835792541503906, 'GPT4': 0.039093017578125, 'CLAUDE': 0.5673828125, 'GOOGLE': 0.31884765625, 'OPENAI_O_SERIES': 0.0055999755859375, 'DEEPSEEK': 0.055419921875, 'GROK': 8.58306884765625e-06, 'NOVA': 3.325939178466797e-05, 'OTHER': 0.00109100341796875, 'HUMAN': 0.0115509033203125}}"
2508.12389,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Developing a Responsible AI Framework for Healthcare in Low Resource Countries: A Case Study in Nepal and Ghana\n\nThe integration of Artificial Intelligence (AI) into healthcare systems in low-resource settings, such as Nepal and Ghana, presents transformative opportunities to improve personalized patient care, optimize resources, and address medical professional shortages. This paper presents a survey-based evaluation and insights from Nepal and Ghana, highlighting major obstacles such as data privacy, reliability, and trust issues. Quantitative and qualitative field studies reveal critical metrics, including 85% of respondents identifying ethical oversight as a key concern, and 72% emphasizing the need for localized governance structures. Building on these findings, we propose a draft Responsible AI (RAI) Framework tailored to resourceconstrained environments in these countries. Key elements of the framework include ethical guidelines, regulatory compliance mechanisms, and contextual validation approaches to mitigate bias and ensure equitable healthcare outcomes.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.0325183868408203e-05, 'GPT4': 0.9033203125, 'CLAUDE': 0.071044921875, 'GOOGLE': 0.01265716552734375, 'OPENAI_O_SERIES': 0.010101318359375, 'DEEPSEEK': 0.0025424957275390625, 'GROK': 6.854534149169922e-06, 'NOVA': 1.0073184967041016e-05, 'OTHER': 3.594160079956055e-05, 'HUMAN': 1.2934207916259766e-05}}"
2508.01169,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""WIP: Enhancing Game-Based Learning with AI-Driven Peer Agents\n\nThis work-in-progress paper presents SPARC (Systematic Problem Solving and Algorithmic Reasoning for Children), a gamified learning platform designed to enhance engagement and knowledge retention in K-12 STEM education. Traditional approaches often struggle to motivate students or facilitate deep understanding, especially for complex scientific concepts. SPARC addresses these challenges by integrating interactive, narrative-driven gameplay with an artificial intelligence peer agent built on large language models. Rather than simply providing answers, the agent engages students in dialogue and inquiry, prompting them to explain concepts and solve problems collaboratively. The platform's design is grounded in educational theory and closely aligned with state learning standards. Initial classroom pilots utilized a multi-method assessment framework combining pre- and post-tests, in-game analytics, and qualitative feedback from students and teachers. Preliminary findings indicate that SPARC significantly increases student engagement, with most participants reporting greater interest in STEM subjects and moderate gains in conceptual understanding observed in post-test results. Ongoing development focuses on refining the AI agent, expanding curriculum integration, and improving accessibility. These early results demonstrate the potential of combining AI-driven peer support with game-based learning to create inclusive, effective, and engaging educational experiences for K-12 learners."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.166364669799805e-05, 'GPT4': 0.1558837890625, 'CLAUDE': 0.81005859375, 'GOOGLE': 0.005535125732421875, 'OPENAI_O_SERIES': 0.0009965896606445312, 'DEEPSEEK': 0.02740478515625, 'GROK': 2.980232238769531e-07, 'NOVA': 2.384185791015625e-07, 'OTHER': 9.119510650634766e-06, 'HUMAN': 3.4809112548828125e-05}}"
2508.15516,regular,post_llm,2025,8,"{'ai_likelihood': 0.006722344292534723, 'text': 'The Digital Life of Parisian Parks: Multifunctionality and Urban Context Uncovered by Mobile Application Traffic\n\nLandscape architecture typically considers urban parks through the lens of form and function. While past research on equitable access has focused mainly on form, studies of functions have been constrained by limited scale and coarse measurement. Existing efforts have partially quantified functions through small-scale surveys and movement data (e.g., GPS) or general usage records (e.g., CDR), but have not captured the activities and motivations underlying park visits. As a result, our understanding of the functional roles urban parks play remains incomplete. To address this gap, we introduce a method that refines mobile base station coverage using antenna azimuths, enabling clearer distinction of mobile traffic within parks versus surrounding areas. Using Paris as a case study, we analyze a large-scale set of passively collected per-app mobile network traffic - 492 million hourly records for 45 parks. We test two hypotheses: the central-city hypothesis, which posits multifunctional parks emerge in dense, high-rent areas due to land scarcity; and the socio-spatial hypothesis, which views parks as reflections of neighborhood routines and preferences. Our analysis shows that parks have distinctive mobile traffic signatures, differing from both their surroundings and from each other. By clustering parks on temporal and app usage patterns, we identify three functional types - lunchbreak, cultural, and recreational - with different visitation motivations. Centrally located parks (cultural and lunchbreak) display more diverse app use and temporal variation, while suburban (recreational) parks reflect digital behaviors of nearby communities, with app preferences aligned to neighborhood income. These findings demonstrate the value of mobile traffic as a proxy for studying urban green space functions, with implications for park planning, public health, and well-being.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.16658,review,post_llm,2025,8,"{'ai_likelihood': 3.311369154188368e-08, 'text': 'Ethics of Artificial Intelligence\n\nArtificial intelligence (AI) is a digital technology that will be of major importance for the development of humanity in the near future. AI has raised fundamental questions about what we should do with such systems, what the systems themselves should do, what risks they involve and how we can control these. - After the background to the field (1), this article introduces the main debates (2), first on ethical issues that arise with AI systems as objects, i.e. tools made and used by humans; here, the main sections are privacy (2.1), manipulation (2.2), opacity (2.3), bias (2.4), autonomy & responsibility (2.6) and the singularity (2.7). Then we look at AI systems as subjects, i.e. when ethics is for the AI systems themselves in machine ethics (2.8.) and artificial moral agency (2.9). Finally we look at future developments and the concept of AI (3). For each section within these themes, we provide a general explanation of the ethical issues, we outline existing positions and arguments, then we analyse how this plays out with current technologies and finally what policy consequences may be drawn.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.16615,review,post_llm,2025,8,"{'ai_likelihood': 2.5033950805664062e-05, 'text': 'Augmentation Technologies and AI - An Ethical Design Futures Framework\n\nAugmentation technologies, fueled by Artificial Intelligence (AI), are undergoing a process of adaptation and normalization geared to everyday users in various roles as practitioners, educators, and students. While new innovations, applications, and algorithms are developed as augmentation technology, Chapter 1 focuses on human subjects, contexts, and rhetorical strategies proposed for them by external actors. The chapter discusses core functions of technical and professional communication and provides rationale for positioning technical and professional communicators (TPCs) to understand augmentation technologies and AI as a means to design ethical futures across this work. An overview of Augmentation Technologies and AI- An Ethical Design Futures Framework serves as a guide for reframing professional practice and pedagogy to promote digital and AI literacy surrounding the ethical design, adoption, and adaptation of augmentation technologies. The chapter concludes with an overview of the remaining chapters in this book.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.05867,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'The Memory Wars: AI Memory, Network Effects, and the Geopolitics of Cognitive Sovereignty\n\nThe advent of continuously learning Artificial Intelligence (AI) assistants marks a paradigm shift from episodic interactions to persistent, memory-driven relationships. This paper introduces the concept of ""Cognitive Sovereignty"", the ability of individuals, groups, and nations to maintain autonomous thought and preserve identity in the age of powerful AI systems, especially those that hold their deep personal memory. It argues that the primary risk of these technologies transcends traditional data privacy to become an issue of cognitive and geopolitical control. We propose ""Network Effect 2.0,"" a model where value scales with the depth of personalized memory, creating powerful cognitive moats and unprecedented user lock-in. We analyze the psychological risks of such systems, including cognitive offloading and identity dependency, by drawing on the ""extended mind"" thesis. These individual-level risks scale to geopolitical threats, such as a new form of digital colonialism and subtle shifting of public discourse. To counter these threats, we propose a policy framework centered on memory portability, transparency, sovereign cognitive infrastructure, and strategic alliances. This work reframes the discourse on AI assistants in an era of increasingly intimate machines, pointing to challenges to individual and national sovereignty.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.2516975402832031e-06, 'GPT4': 0.0006618499755859375, 'CLAUDE': 0.0012979507446289062, 'GOOGLE': 5.501508712768555e-05, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.998046875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 6.246566772460938e-05}}"
2508.13984,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'The AI-Fraud Diamond: A Novel Lens for Auditing Algorithmic Deception\n\nAs artificial intelligence (AI) systems become increasingly integral to organizational processes, they introduce new forms of fraud that are often subtle, systemic, and concealed within technical complexity. This paper introduces the AI-Fraud Diamond, an extension of the traditional Fraud Triangle that adds technical opacity as a fourth condition alongside pressure, opportunity, and rationalization. Unlike traditional fraud, AI-enabled deception may not involve clear human intent but can arise from system-level features such as opaque model behavior, flawed training data, or unregulated deployment practices. The paper develops a taxonomy of AI-fraud across five categories: input data manipulation, model exploitation, algorithmic decision manipulation, synthetic misinformation, and ethics-based fraud. To assess the relevance and applicability of the AI-Fraud Diamond, the study draws on expert interviews with auditors from two of the Big Four consulting firms. The findings underscore the challenges auditors face when addressing fraud in opaque and automated environments, including limited technical expertise, insufficient cross-disciplinary collaboration, and constrained access to internal system processes. These conditions hinder fraud detection and reduce accountability. The paper argues for a shift in audit methodology-from outcome-based checks to a more diagnostic approach focused on identifying systemic vulnerabilities. Ultimately, the work lays a foundation for future empirical research and audit innovation in a rapidly evolving AI governance landscape.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.867813110351562e-06, 'GPT4': 0.56494140625, 'CLAUDE': 0.42822265625, 'GOOGLE': 3.5762786865234375e-07, 'OPENAI_O_SERIES': 2.0265579223632812e-06, 'DEEPSEEK': 0.006683349609375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 1.2516975402832031e-06}}"
2508.11677,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Scaling Success: A Systematic Review of Peer Grading Strategies for Accuracy, Efficiency, and Learning in Contemporary Education\n\nPeer grading has emerged as a scalable solution for assessment in large and online classrooms, offering both logistical efficiency and pedagogical value. However, designing effective peer-grading systems remains challenging due to persistent concerns around accuracy, fairness, reliability, and student engagement. This paper presents a systematic review of 122 peer-reviewed studies on peer grading spanning over four decades. Drawing from this literature, we propose a comprehensive taxonomy that organizes peer grading systems along two key dimensions: (1) evaluation approaches and (2) reviewer weighting strategies. We analyze how different design choices impact grading accuracy, fairness, student workload, and learning outcomes. Our findings highlight the strengths and limitations of each method. Notably, we found that formative feedback -- often regarded as the most valuable aspect of peer assessment -- is seldom incorporated as a quality-based weighting factor in summative grade synthesis techniques. Furthermore, no single reviewer weighting strategy proves universally optimal; each has its trade-offs. Hybrid strategies that combine multiple techniques could show the greatest promise. Our taxonomy offers a practical framework for educators and researchers aiming to design peer grading systems that are accurate, equitable, and pedagogically meaningful.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.8477439880371094e-06, 'GPT4': 0.0021266937255859375, 'CLAUDE': 0.97314453125, 'GOOGLE': 0.0027523040771484375, 'OPENAI_O_SERIES': 3.7729740142822266e-05, 'DEEPSEEK': 0.0220489501953125, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.4901161193847656e-06, 'HUMAN': 2.944469451904297e-05}}"
2509.02611,review,post_llm,2025,8,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'Chatbot Deployment Considerations for Application-Agnostic Human-Machine Dialogues\n\nAutomatic conversation systems based on natural language responses are becoming ubiquitous, in part, due to major advances in computational linguistics and machine learning. The easy access to robust and affordable platforms are causing companies to have an unprecedented rush to adopt chatbot technologies for customer service and support. However, this rush has caused judgment lapses when releasing chatbot technologies into production systems. This paper aims to shed light on basic, elemental, considerations that technologists must consider before deploying a chatbot. Our approach takes one particular case to draw lessons for those considering the implementation of chatbots. By looking at this case-study, we aim to call for consideration of societal values as a paramount factor before deploying a chatbot and consider the societal implications of releasing these types of systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.09964,regular,post_llm,2025,8,"{'ai_likelihood': 1.2583202785915798e-06, 'text': 'Deep and diverse population synthesis for multi-person households using generative models\n\nSynthetic population is an increasingly important material used in numerous areas such as urban and transportation analysis. Traditional methods such as iterative proportional fitting (IPF) is not capable of generating high-quality data when facing datasets with high dimension. Latest population synthesis methods using deep learning techniques can resolve such curse of dimensionality. However, few controls are placed when using these methods, and few of the methods are used to generate synthetic population capturing associations among members in one household. In this study, we propose a framework that tackles these issues. The framework uses a novel population synthesis model, called conditional input directed acyclic tabular generative adversarial network (ciDATGAN), as its core, and a basket of methods are employed to enhance the population synthesis performance. We apply the model to generate a synthetic population for the whole New York State as a public resource for researchers and policymakers. The synthetic population includes nearly 20 million individuals and 7.5 million households. The marginals obtained from the synthetic population match the census marginals well while maintaining similar associations among household members to the sample. Compared to the PUMS data, the synthetic population provides data that is 17% more diverse; when compared against a benchmark approach based on Popgen, the proposed method is 13% more diverse. This study provides an approach that encompasses multiple methods to enhance the population synthesis procedure with greater equity- and diversity-awareness.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.14201,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Breakable Machine: A K-12 Classroom Game for Transformative AI Literacy Through Spoofing and eXplainable AI (XAI)\n\nThis paper, submitted to the special track on resources for teaching AI in K-12, presents an eXplainable AI (XAI)-based classroom game ""Breakable Machine"" for teaching critical, transformative AI literacy through adversarial play and interrogation of AI systems. Designed for learners aged 10-15, the game invites students to spoof an image classifier by manipulating their appearance or environment in order to trigger high-confidence misclassifications. Rather than focusing on building AI models, this activity centers on breaking them-exposing their brittleness, bias, and vulnerability through hands-on, embodied experimentation. The game includes an XAI view to help students visualize feature saliency, revealing how models attend to specific visual cues. A shared classroom leaderboard fosters collaborative inquiry and comparison of strategies, turning the classroom into a site for collective sensemaking. This approach reframes AI education by treating model failure and misclassification not as problems to be debugged, but as pedagogically rich opportunities to interrogate AI as a sociotechnical system. In doing so, the game supports students in developing data agency, ethical awareness, and a critical stance toward AI systems increasingly embedded in everyday life. The game and its source code are freely available.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1205673217773438e-05, 'GPT4': 0.0011625289916992188, 'CLAUDE': 0.55615234375, 'GOOGLE': 0.0013523101806640625, 'OPENAI_O_SERIES': 1.3113021850585938e-06, 'DEEPSEEK': 0.43994140625, 'GROK': 5.364418029785156e-07, 'NOVA': 2.384185791015625e-07, 'OTHER': 2.6226043701171875e-05, 'HUMAN': 0.0015573501586914062}}"
2508.05849,review,post_llm,2025,8,"{'ai_likelihood': 2.4769041273328996e-05, 'text': 'Public support for misinformation interventions depends on perceived fairness, effectiveness, and intrusiveness\n\nThe proliferation of misinformation on social media has concerning possible consequences, such as the degradation of democratic norms. While recent research on countering misinformation has largely focused on analyzing the effectiveness of interventions, the factors associated with public support for these interventions have received little attention. We asked 1,010 American social media users to rate their support for and perceptions of ten misinformation interventions implemented by the government or social media companies. Our results indicate that the perceived fairness of the intervention is the most important factor in determining support, followed by the perceived effectiveness of that intervention and then the intrusiveness. Interventions that supported user agency and transparency, such as labeling content or fact-checking ads, were more popular than those that involved moderating or removing content or accounts. We found some demographic differences in support levels, with Democrats and women supporting interventions more and finding them more fair, more effective, and less intrusive than Republicans and men, respectively. It is critical to understand which interventions are supported and why, as public opinion can play a key role in the rollout and effectiveness of policies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.16626,regular,post_llm,2025,8,"{'ai_likelihood': 5.0001674228244356e-06, 'text': 'Pothole Detection and Analysis System (PoDAS) for Real Time Data Using Sensor Networks\n\nPotholes are a major nuisance on the city roads leading to several problems and losses in productivity. Local authorities have cited a lack of geographic localization of these potholes as one of the rate-limiting factors for repairs. This study proposes a novel low-cost wireless sensor-based end-to-end system called PoDAS (Pothole Detection and Analysis System) which can be deployed across major cities. We discuss multiple implementation models that can be varied based on the needs of individual cities. Our system uses cross-validation through multiple sensors to achieve higher efficiency than some of the previous models that have been proposed. We also present the results from extensive testing carried out in different environments to ascertain both the efficacy and the efficiency of the proposed system.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.05952,regular,post_llm,2025,8,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of LLM-generated Educational Feedback via LLM Feedback Evaluators\n\nThe use of LLM tutors to provide automated educational feedback to students on student assignment submissions has received much attention in the AI in Education field. However, the stochastic nature and tendency for hallucinations in LLMs can undermine both quality of learning experience and adherence to ethical standards. To address this concern, we propose a method that uses LLM feedback evaluators (DeanLLMs) to automatically and comprehensively evaluate feedback generated by LLM tutor for submissions on university assignments before it is delivered to students. This allows low-quality feedback to be rejected and enables LLM tutors to improve the feedback they generated based on the evaluation results. We first proposed a comprehensive evaluation framework for LLM-generated educational feedback, comprising six dimensions for feedback content, seven for feedback effectiveness, and three for hallucination types. Next, we generated a virtual assignment submission dataset covering 85 university assignments from 43 computer science courses using eight commonly used commercial LLMs. We labelled and open-sourced the assignment dataset to support the fine-tuning and evaluation of LLM feedback evaluators. Our findings show that o3-pro demonstrated the best performance in zero-shot labelling of feedback while o4-mini demonstrated the best performance in few-shot labelling of feedback. Moreover, GPT-4.1 achieved human expert level performance after fine-tuning (Accuracy 79.8%, F1-score 79.4%; human average Accuracy 78.3%, F1-score 82.6%). Finally, we used our best-performance model to evaluate 2,000 assignment feedback instances generated by 10 common commercial LLMs, 200 each, to compare the quality of feedback generated by different LLMs. Our LLM feedback evaluator method advances our ability to automatically provide high-quality and reliable educational feedback to students.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.14954,review,post_llm,2025,8,"{'ai_likelihood': 3.940529293484158e-05, 'text': 'Bridging Research Gaps Between Academic Research and Legal Investigations of Algorithmic Discrimination\n\nAs algorithms increasingly take on critical roles in high-stakes areas such as credit scoring, housing, and employment, civil enforcement actions have emerged as a powerful tool for countering potential discrimination. These legal actions increasingly draw on algorithmic fairness research to inform questions such as how to define and detect algorithmic discrimination. However, current algorithmic fairness research, while theoretically rigorous, often fails to address the practical needs of legal investigations. We identify and analyze 15 civil enforcement actions in the United States including regulatory enforcement, class action litigation, and individual lawsuits to identify practical challenges in algorithmic discrimination cases that machine learning research can help address. Our analysis reveals five key research gaps within existing algorithmic bias research, presenting practical opportunities for more aligned research: 1) finding an equally accurate and less discriminatory algorithm, 2) cascading algorithmic bias, 3) quantifying disparate impact, 4) navigating information barriers, and 5) handling missing protected group information. We provide specific recommendations for developing tools and methodologies that can strengthen legal action against unfair algorithms.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.1415,review,post_llm,2025,8,"{'ai_likelihood': 1.6954210069444445e-05, 'text': ""When Algorithms Infer Gender: Revisiting Computational Phenotyping with Electronic Health Records Data\n\nComputational phenotyping has emerged as a practical solution to the incomplete collection of data on gender in electronic health records (EHRs). This approach relies on algorithms to infer a patient's gender using the available data in their health record, such as diagnosis codes, medication histories, and information in clinical notes. Although intended to improve the visibility of trans and gender-expansive populations in EHR-based biomedical research, computational phenotyping raises significant methodological and ethical concerns related to the potential misuse of algorithm outputs. In this paper, we review current practices for computational phenotyping of gender and examine its challenges through a critical lens. We also highlight existing recommendations for biomedical researchers and propose priorities for future work in this domain."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.10198,review,post_llm,2025,8,"{'ai_likelihood': 4.404120975070529e-06, 'text': ""Digital Contact Tracing: Examining the Effects of Understanding and Release Organization on Public Trust\n\nContact tracing has existed in various forms for a very long time. With the rise of COVID-19, the concept has become increasingly important to help slow the spread of the virus. One approach to modernizing contact tracing is to introduce applications that detect all close contacts without individuals having to interact knowingly. 101 United States adults were surveyed in June of 2022 regarding their perceptions and trust of COVID-19 contact tracing applications. We see no definitive correlation between an individual's understanding of privacy protection procedures for contact tracing applications and their willingness to trust such an application. We also see that the release of the application by a private entity like Google-Apple or by a public entity like the United States Federal Government has no significant correlation with a person's trust in the application."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.06479,regular,post_llm,2025,8,"{'ai_likelihood': 0.40283203125, 'text': 'The Problem of Atypicality in LLM-Powered Psychiatry\n\nLarge language models (LLMs) are increasingly proposed as scalable solutions to the global mental health crisis. But their deployment in psychiatric contexts raises a distinctive ethical concern: the problem of atypicality. Because LLMs generate outputs based on population-level statistical regularities, their responses -- while typically appropriate for general users -- may be dangerously inappropriate when interpreted by psychiatric patients, who often exhibit atypical cognitive or interpretive patterns. We argue that standard mitigation strategies, such as prompt engineering or fine-tuning, are insufficient to resolve this structural risk. Instead, we propose dynamic contextual certification (DCC): a staged, reversible and context-sensitive framework for deploying LLMs in psychiatry, inspired by clinical translation and dynamic safety models from artificial intelligence governance. DCC reframes chatbot deployment as an ongoing epistemic and ethical process that prioritises interpretive safety over static performance benchmarks. Atypicality, we argue, cannot be eliminated -- but it can, and must, be proactively managed.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.00595,regular,post_llm,2025,8,"{'ai_likelihood': 4.337893591986763e-06, 'text': 'Supporting a Sustainable and Inclusive Urban Agriculture Federation using Dashboarding\n\nReliable access to food is a basic requirement in any sustainable society. However, achieving food security for all is still a challenge, especially for poor populations in urban environments. The project Feed4Food aims to use a federation of Living Labs of urban agriculture in different countries as a way to increase urban food security for vulnerable populations.\n  Since different Living Labs have different characteristics and ways of working, the vision is that the knowledge obtained in individual Living Labs can be leveraged at the federation level through federated learning. With this specific goal in mind, a dashboarding tool is being established.\n  In this work, we present a reusable process for establishing a dashboard that supports local awareness and decision making, as well as federated learning. The focus is on the first steps of this creation, i.e., defining what data to collect (through the creation of Key Performance Indicators) and how to visualize it. We exemplify the proposed process with the Feed4Food project and report on our insights so far.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.20668,regular,post_llm,2025,8,"{'ai_likelihood': 0.0005997551812065972, 'text': 'Composable Life: Speculation for Decentralized AI Life\n\n""Composable Life"" is a hybrid project blending design fiction, experiential virtual reality, and scientific research. Through a multi-perspective, cross-media approach to speculative design, it reshapes our understanding of the digital future from AI\'s perspective. The project explores the hypothetical first suicide of an on-chain artificial life, examining the complex symbiotic relationship between humans, AI, and blockchain technology.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.20811,regular,post_llm,2025,8,"{'ai_likelihood': 0.8583984375, 'text': 'When technology is not enough: Insights from a pilot cybersecurity culture assessment in a safety-critical industrial organisation\n\nAs cyber threats increasingly exploit human behaviour, technical controls alone cannot ensure organisational cybersecurity (CS). Strengthening cybersecurity culture (CSC) is vital in safety-critical industries, yet empirical research in real-world industrial setttings is scarce. This paper addresses this gap through a pilot mixed-methods CSC assessment in a global safety-critical organisation. We examined employees\' CS knowledge, attitudes, behaviours, and organisational factors shaping them. A survey and semi-structured interviews were conducted at a global organisation in safety-critical industries, across two countries chosen for contrasting phishing simulation performance: Country 1 stronger, Country 2 weaker. In Country 1, 258 employees were invited (67%), in Country 2, 113 were invited (30%). Interviews included 20 and 10 participants respectively. Overall CSC profiles were similar but revealed distinct challenges. Both showed strong phishing awareness and prioritised CS, yet most viewed phishing as the main risk and lacked clarity on handling other incidents. Line managers were default contacts, but follow-up on reported concerns was unclear. Participants emphasized aligning CS expectations with job relevance and workflows. Key contributors to differences emerged: Country 1 had external employees with limited access to CS training and policies, highlighting monitoring gaps. In Country 2, low survey response stemmed from a ""no-link in email"" policy. While this policy may have boosted phishing performance, it also underscored inconsistencies in CS practices. Findings show that resilient CSC requires leadership involvement, targeted communication, tailored measures, policy-practice alignment, and regular assessments. Embedding these into strategy complements technical defences and strengthens sustainable CS in safety-critical settings.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0030727386474609375, 'GPT4': 0.11785888671875, 'CLAUDE': 0.20849609375, 'GOOGLE': 0.0306396484375, 'OPENAI_O_SERIES': 0.0013017654418945312, 'DEEPSEEK': 0.3623046875, 'GROK': 2.384185791015625e-07, 'NOVA': 5.364418029785156e-07, 'OTHER': 0.0002765655517578125, 'HUMAN': 0.276123046875}}"
2508.08678,regular,post_llm,2025,8,"{'ai_likelihood': 6.622738308376736e-08, 'text': 'Exploring Large Language Model Agents for Piloting Social Experiments\n\nComputational social experiments, which typically employ agent-based modeling to create testbeds for piloting social experiments, not only provide a computational solution to the major challenges faced by traditional experimental methods, but have also gained widespread attention in various research fields. Despite their significance, their broader impact is largely limited by the underdeveloped intelligence of their core component, i.e., agents. To address this limitation, we develop a framework grounded in well-established social science theories and practices, consisting of three key elements: (i) large language model (LLM)-driven experimental agents, serving as ""silicon participants"", (ii) methods for implementing various interventions or treatments, and (iii) tools for collecting behavioral, survey, and interview data. We evaluate its effectiveness by replicating three representative experiments, with results demonstrating strong alignment, both quantitatively and qualitatively, with real-world evidence. This work provides the first framework for designing LLM-driven agents to pilot social experiments, underscoring the transformative potential of LLMs and their agents in computational social science', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.18482,review,post_llm,2025,8,"{'ai_likelihood': 0.9658203125, 'text': 'A Praxis of Influence: Framing the Observation and Measurement of Information Power\n\nInformation power is the capacity to convert data flows into durable shifts in attention, belief, and behavior. We argue that this power has migrated from broadcast persuasion to platform-ized, data-driven operations that fuse computational delivery with cognitive effects. In this context, we define and bound information power within international relations and the information environment while demonstrating why observing and measuring it demands an integrated lens that combines politics (goals and governance), computing (data movement and algorithmic delivery), and psychology (attention, affect, memory, and belief). The article contributes three elements: (1) a triadic analytical framework that specifies the minimum variables and instrumentation needed for study; (2) two crosswalks that map common objectives (persuade, disrupt, shape) and target classes (leaders, elites, publics) to political, computational, and psychological tactics, yielding practical coding heuristics and testable hypotheses; and (3) a McCumber-style cube for information influence that integrates targets, operations, as well as machines (automation and AI) into a single space. The space provides for comparative analysis, data fusion, and effect measurement. Using recent cases across state and commercial platforms, we illustrate how virality, stickiness, and denial of logic exploit fast cognition, why conventional reach metrics understate impact, and where instrumentation should focus. We conclude with a mixed-methods research program coupling computational sensing including large-language-model text mining with experiments and polling. The intention is to move from detecting activity to estimating belief change and decision effects.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.940696716308594e-07, 'GPT4': 8.571147918701172e-05, 'CLAUDE': 0.0005908012390136719, 'GOOGLE': 5.9604644775390625e-06, 'OPENAI_O_SERIES': 1.7881393432617188e-07, 'DEEPSEEK': 0.98681640625, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 6.556510925292969e-07, 'HUMAN': 0.0123291015625}}"
2508.14833,regular,post_llm,2025,8,"{'ai_likelihood': 9.602970547146267e-07, 'text': ""An Investigation Into Secondary School Students' Debugging Behaviour in Python\n\nBackground and context: Debugging is a common and often frustrating challenge for beginner programmers. Understanding students' debugging processes can help us identify the difficulties and misunderstandings they possess. However, we currently have limited knowledge of how secondary students debug in a text-based language, a medium through which millions of students will learn to program in the future. Objectives: In this paper, we investigate the debugging behaviour of K-12 students learning a text-based programming language, as part of an effort to shape how to effectively teach debugging to these students. Method: We collected log data from 73 students attempting a set of debugging exercises using an online code editor. We inductively analysed these logs using qualitative content analysis, generating a categorisation of the debugging behaviours observed. Findings: A range of behaviours were exhibited by students, skewed towards being ineffective. Most students were able to partially locate errors but often struggled to resolve them, sometimes introducing additional errors in the process. We argue that students struggling to debug possess fragile knowledge, a lens through which we view the results. Implications: This paper highlights some of the difficulties K-12 learners have when debugging in a text-based programming language. We argue, like much related work, that effective debugging strategies should be explicitly taught, while ineffective strategies should be discouraged."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.02086,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'The Actual Usage Of Cryptocurrency By Individuals\n\nThis study investigates how media influence and educational resources shape individual engagement with cryptocurrencies. As digital assets become increasingly mainstream, social media platforms, influencers, and financial analysts play a central role in driving public interest and investment behavior. Quantitative surveys and qualitative interviews reveal that persuasive narratives and success stories on social media often attract new investors, while accessible educational materials empower individuals to make informed decisions in a volatile market. The findings highlight that media exposure can spark initial adoption, but sustained and responsible participation depends on comprehensive education in cryptocurrency fundamentals, trading strategies, and blockchain technology', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 6.139278411865234e-06, 'GPT4': 0.88525390625, 'CLAUDE': 0.01186370849609375, 'GOOGLE': 7.748603820800781e-07, 'OPENAI_O_SERIES': 0.08013916015625, 'DEEPSEEK': 0.022857666015625, 'GROK': 0.0, 'NOVA': 1.4901161193847656e-06, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 1.9073486328125e-06}}"
2508.18541,regular,post_llm,2025,8,"{'ai_likelihood': 2.947118547227648e-06, 'text': 'Uncovering Intervention Opportunities for Suicide Prevention with Language Model Assistants\n\nWarning: This paper discusses topics of suicide and suicidal ideation, which may be distressing to some readers.\n  The National Violent Death Reporting System (NVDRS) documents information about suicides in the United States, including free text narratives (e.g., circumstances surrounding a suicide). In a demanding public health data pipeline, annotators manually extract structured information from death investigation records following extensive guidelines developed painstakingly by experts. In this work, we facilitate data-driven insights from the NVDRS data to support the development of novel suicide interventions by investigating the value of language models (LMs) as efficient assistants to these (a) data annotators and (b) experts. We find that LM predictions match existing data annotations about 85% of the time across 50 NVDRS variables. In the cases where the LM disagrees with existing annotations, expert review reveals that LM assistants can surface annotation discrepancies 38% of the time. Finally, we introduce a human-in-the-loop algorithm to assist experts in efficiently building and refining guidelines for annotating new variables by allowing them to focus only on providing feedback for incorrect LM predictions. We apply our algorithm to a real-world case study for a new variable that characterizes victim interactions with lawyers and demonstrate that it achieves comparable annotation quality with a laborious manual approach. Our findings provide evidence that LMs can serve as effective assistants to public health researchers who handle sensitive data in high-stakes scenarios.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.17108,regular,post_llm,2025,8,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Exploring AI-Enabled Test Practice, Affect, and Test Outcomes in Language Assessment\n\nPractice tests for high-stakes assessment are intended to build test familiarity, and reduce construct-irrelevant variance which can interfere with valid score interpretation. Generative AI-driven, automated item generation (AIG) scales the creation of large item banks and multiple practice tests, enabling repeated practice opportunities. We conducted a large-scale observational study (N = 25,969) using the Duolingo English Test (DET) -- a digital, high-stakes, computer-adaptive English language proficiency test to examine how increased access to repeated test practice relates to official DETscores, test-taker affect (e.g., confidence), and score-sharing for university admissions. To our knowledge, this is the first large-scale study exploring the use of AIG-enabled practice tests in high-stakes language assessment. Results showed that taking 1-3 practice tests was associated with better performance (scores), positive affect (e.g., confidence) toward the official DET, and increased likelihood of sharing scores for university admissions for those who also expressed positive affect. Taking more than 3 practice tests was related to lower performance, potentially reflecting washback -- i.e., using the practice test for purposes other than test familiarity, such as language learning or developing test-taking strategies. Findings can inform best practices regarding AI-supported test readiness. Study findings also raise new questions about test-taker preparation behaviors and relationships to test-taker performance, affect, and behaviorial outcomes.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.00093,regular,post_llm,2025,8,"{'ai_likelihood': 8.112854427761502e-06, 'text': ""More than Carbon: Cradle-to-Grave environmental impacts of GenAI training on the Nvidia A100 GPU\n\nThe rapid expansion of AI has intensified concerns about its environmental sustainability. Yet, current assessments predominantly focus on operational carbon emissions using secondary data or estimated values, overlooking environmental impacts in other life cycle stages. This study presents the first comprehensive multi-criteria life cycle assessment (LCA) of AI training, examining 16 environmental impact categories based on detailed primary data collection of the Nvidia A100 SXM 40GB GPU. The LCA results for training BLOOM reveal that the use phase dominates 11 of 16 impact categories including climate change (96\\%), while manufacturing dominates the remaining 5 impact categories including human toxicity, cancer (99\\%) and mineral and metal depletion (85\\%). For training GPT-4, the use phase dominates 10 of 16 impact categories, contributing about 96\\% to both the climate change and resource use, fossils category. The manufacturing stage dominates 6 of 16 impact categories including human toxicity, cancer (94\\%) and eutrophication, freshwater (81\\%). Assessing the cradle-to-gate environmental impact distribution across the GPU components reveals that the GPU chip is the largest contributor across 10 of 16 of impact categories and shows particularly pronounced contributions to climate change (81\\%) and resource use, fossils (80\\%). While primary data collection results in modest changes in carbon estimates compared to database-derived estimates, substantial variations emerge in other categories. Most notably, minerals and metals depletion increases by 33\\%, demonstrating the critical importance of primary data for non-carbon accounting. This multi-criteria analysis expands the Sustainable AI discourse beyond operational carbon emissions, challenging current sustainability narratives and highlighting the need for policy frameworks addressing the full spectrum of AI's environmental impact."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.01926,review,post_llm,2025,8,"{'ai_likelihood': 0.99267578125, 'text': ""Understanding Student Attitudes and Acceptability of GenAI Tools in Higher Ed: Scale Development and Evaluation\n\nAs generative AI (GenAI) tools like ChatGPT become more common in higher education, understanding student attitudes is essential for evaluating their educational impact and supporting responsible AI integration. This study introduces a validated survey instrument designed to assess students' perceptions of GenAI, including its acceptability for academic tasks, perceived influence on learning and careers, and broader societal concerns. We administered the survey to 297 undergraduates at a U.S. university. The instrument includes six thematic domains: institutional understanding, fairness and trust, academic and career influence, societal concerns, and GenAI use in writing and coursework. Exploratory factor analysis revealed four attitudinal dimensions: societal concern, policy clarity, fairness and trust, and career impact. Subgroup analyses identified statistically significant differences across student backgrounds. Male students and those speaking a language other than English at home rated GenAI use in writing tasks as more acceptable. First-year students expressed greater societal concern than upper-year peers. Students from multilingual households perceived greater clarity in institutional policy, while first-generation students reported a stronger belief in GenAI's impact on future careers. This work contributes a practical scale for evaluating the student impact of GenAI tools, informing the design of educational AI systems."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.2411346435546875e-05, 'GPT4': 0.0035915374755859375, 'CLAUDE': 0.97705078125, 'GOOGLE': 0.0010633468627929688, 'OPENAI_O_SERIES': 0.0006780624389648438, 'DEEPSEEK': 0.0125579833984375, 'GROK': 0.0, 'NOVA': 1.7881393432617188e-07, 'OTHER': 2.8192996978759766e-05, 'HUMAN': 0.004764556884765625}}"
2508.20031,review,post_llm,2025,8,"{'ai_likelihood': 0.99755859375, 'text': 'Bridging the Regulatory Divide: Ensuring Safety and Equity in Wearable Health Technologies\n\nAs wearable health technologies have grown more sophisticated, the distinction between ""wellness"" and ""medical"" devices has become increasingly blurred. While some features undergo formal U.S. Food and Drug Administration (FDA) review, many over-the-counter tools operate in a regulatory grey zone, leveraging health-related data and outputs without clinical validation. Further complicating the issue is the widespread repurposing of wellness devices for medical uses, which can introduce safety risks beyond the reach of current oversight. Drawing on legal analysis, case studies, and ethical considerations, we propose an approach emphasizing distributed risk, patient-centered outcomes, and iterative reform. Without a more pluralistic and evolving framework, the promise of wearable health technology risks being undermined by growing inequities, misuse, and eroded public trust.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00021409988403320312, 'GPT4': 0.10015869140625, 'CLAUDE': 0.89453125, 'GOOGLE': 0.00029659271240234375, 'OPENAI_O_SERIES': 1.919269561767578e-05, 'DEEPSEEK': 0.0023746490478515625, 'GROK': 1.1920928955078125e-07, 'NOVA': 3.5762786865234375e-07, 'OTHER': 1.329183578491211e-05, 'HUMAN': 0.00232696533203125}}"
2508.18181,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'What is digital about abstraction?\n\nThis chapter examines abstraction as a central principle of computing, not merely as a cognitive skill or epistemological category, but as a material and organizational practice that structures how software is built, used, and embedded in society. By tracing abstraction through historical developments in programming, operating systems, and networking, the text highlights its dual role in enabling modularity and layering while simultaneously shaping cultural, economic, and organizational forms. From open-source projects to platform capitalism and cloud infrastructures, abstraction emerges as both a technical device and a locus of power, producing dependencies and interdependencies that reconfigure labor, governance, and control in digital environments. The chapter argues for understanding abstraction as a socio-technical process whose effects extend far beyond efficiency or convenience, influencing how computing infrastructures evolve and how power relations crystallize around them.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0132789611816406e-06, 'GPT4': 0.004177093505859375, 'CLAUDE': 0.00139617919921875, 'GOOGLE': 2.4437904357910156e-06, 'OPENAI_O_SERIES': 5.960464477539062e-07, 'DEEPSEEK': 0.99462890625, 'GROK': 1.0728836059570312e-06, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 0.0}}"
2509.00067,regular,post_llm,2025,8,"{'ai_likelihood': 1.1093086666531033e-05, 'text': 'Making Characters Count. A Computational Approach to Scribal Profiling in 14th-Century Middle Dutch Manuscripts from the Carthusian Monastery of Herne\n\nThe Carthusian monastery of Herne was exceptionally prolific in producing high-quality manuscripts during the late 14th century. Although the scribes remain anonymous, previous research has distinguished thirteen different scribal hands based on paleography and codicology. In this study, we revisit this hypothesis through the lens of linguistic characteristics of the texts, using computational methods from the field of scribal profiling. Using a newly created corpus of diplomatic and HTR-based transcriptions, we analyze abbreviation practices across the Herne scribes and demonstrate that abbreviation density provides a distinctive metric for differentiating scribal hands. In combination with a stylometric bag-of-characters model with brevigraph features, this approach corroborates and refines earlier hypotheses about scribal attribution, including evidence that challenges the role of scribe $\\alpha$ in Vienna, \\""{O}NB, SN 65. Our results highlight the value of combining computational stylometry with traditional codicology, showing how even the smallest elements of the written system -- characters and abbreviations -- can reveal patterns of scribal identity, collaboration, and manuscript transmission.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.01042,review,post_llm,2025,8,"{'ai_likelihood': 1.655684577094184e-07, 'text': 'AI-Generated Algorithmic Virality\n\nThere is a growing discussion about social media feeds being increasingly filled with AI-generated content. Due to its visual plausibility, low cost, and fast production speed, AI-generated content is said to be highly effective in ""gaming the algorithm"" and going viral. Popularly referred to as ""AI slop,"" this phenomenon arguably leads to the presence of sloppy and potentially deceptive content at a scale unseen before. This investigation offers a systematic analysis of AI-generated content and its labelling in TikTok\'s and Instagram\'s search results across 13 hashtags (see Appendix) in three European countries (Spain, Germany, and Poland) over the course of June 2025. We manually annotated and analyzed the 30 top search results on political (#trump, #zelensky, #pope) and broader topics (e.g.,#health, #history) to understand the relation between synthetic (content that is partially or entirely made using generative AI) and non-synthetic content across languages and countries. We then explored the emerging phenomenon of accounts producing generative AI content at scale by analyzing 153 accounts and proposing a new categorization schema of what we termed Agentic AI Accounts. Our main findings are:', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.10272,review,post_llm,2025,8,"{'ai_likelihood': 0.00043498145209418406, 'text': ""Ask ChatGPT: Caveats and Mitigations for Individual Users of AI Chatbots\n\nAs ChatGPT and other Large Language Model (LLM)-based AI chatbots become increasingly integrated into individuals' daily lives, important research questions arise. What concerns and risks do these systems pose for individual users? What potential harms might they cause, and how can these be mitigated? In this work, we review recent literature and reports, and conduct a comprehensive investigation into these questions. We begin by explaining how LLM-based AI chatbots work, providing essential background to help readers understand chatbots' inherent limitations. We then identify a range of risks associated with individual use of these chatbots, including hallucinations, intrinsic biases, sycophantic behavior, cognitive decline from overreliance, social isolation, and privacy leakage. Finally, we propose several key mitigation strategies to address these concerns. Our goal is to raise awareness of the potential downsides of AI chatbot use, and to empower users to enhance, rather than diminish, human intelligence, to enrich, rather than compromise, daily life."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.08231,review,post_llm,2025,8,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""A Moral Agency Framework for Legitimate Integration of AI in Bureaucracies\n\nPublic-sector bureaucracies seek to reap the benefits of artificial intelligence (AI), but face important concerns about accountability and transparency when using AI systems. In particular, perception or actuality of AI agency might create ethics sinks - constructs that facilitate dissipation of responsibility when AI systems of disputed moral status interface with bureaucratic structures. Here, we reject the notion that ethics sinks are a necessary consequence of introducing AI systems into bureaucracies. Rather, where they appear, they are the product of structural design decisions across both the technology and the institution deploying it. We support this claim via a systematic application of conceptions of moral agency in AI ethics to Weberian bureaucracy. We establish that it is both desirable and feasible to render AI systems as tools for the generation of organizational transparency and legibility, which continue the processes of Weberian rationalization initiated by previous waves of digitalization. We present a three-point Moral Agency Framework for legitimate integration of AI in bureaucratic structures: (a) maintain clear and just human lines of accountability, (b) ensure humans whose work is augmented by AI systems can verify the systems are functioning correctly, and (c) introduce AI only where it doesn't inhibit the capacity of bureaucracies towards either of their twin aims of legitimacy and stewardship. We suggest that AI introduced within this framework can not only improve efficiency and productivity while avoiding ethics sinks, but also improve the transparency and even the legitimacy of a bureaucracy."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.00462,regular,post_llm,2025,8,"{'ai_likelihood': 0.01312255859375, 'text': ""AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and Insights\n\nAs generative artificial intelligence (AI) tools become widely adopted, large language models (LLMs) are increasingly involved on both sides of decision-making processes, ranging from hiring to content moderation. This dual adoption raises a critical question: do LLMs systematically favor content that resembles their own outputs? Prior research in computer science has identified self-preference bias -- the tendency of LLMs to favor their own generated content -- but its real-world implications have not been empirically evaluated. We focus on the hiring context, where job applicants often rely on LLMs to refine resumes, while employers deploy them to screen those same resumes. Using a large-scale controlled resume correspondence experiment, we find that LLMs consistently prefer resumes generated by themselves over those written by humans or produced by alternative models, even when content quality is controlled. The bias against human-written resumes is particularly substantial, with self-preference bias ranging from 68% to 88% across major commercial and open-source models. To assess labor market impact, we simulate realistic hiring pipelines across 24 occupations. These simulations show that candidates using the same LLM as the evaluator are 23% to 60% more likely to be shortlisted than equally qualified applicants submitting human-written resumes, with the largest disadvantages observed in business-related fields such as sales and accounting. We further demonstrate that this bias can be reduced by more than 50% through simple interventions targeting LLMs' self-recognition capabilities. These findings highlight an emerging but previously overlooked risk in AI-assisted decision making and call for expanded frameworks of AI fairness that address not only demographic-based disparities, but also biases in AI-AI interactions."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.02748,review,post_llm,2025,8,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'Advancing Science- and Evidence-based AI Policy\n\nAI policy should advance AI innovation by ensuring that its potential benefits are responsibly realized and widely shared. To achieve this, AI policymaking should place a premium on evidence: Scientific understanding and systematic analysis should inform policy, and policy should accelerate evidence generation. But policy outcomes reflect institutional constraints, political dynamics, electoral pressures, stakeholder interests, media environment, economic considerations, cultural contexts, and leadership perspectives. Adding to this complexity is the reality that the broad reach of AI may mean that evidence and policy are misaligned: Although some evidence and policy squarely address AI, much more partially intersects with AI. Well-designed policy should integrate evidence that reflects scientific understanding rather than hype. An increasing number of efforts address this problem by often either (i) contributing research into the risks of AI and their effective mitigation or (ii) advocating for policy to address these risks. This paper tackles the hard problem of how to optimize the relationship between evidence and policy to address the opportunities and challenges of increasingly powerful AI.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.01779,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""The AI-Augmented Research Process: A Historian's Perspective\n\nThis paper presents a detailed case study of how artificial intelligence, especially large language models, can be integrated into historical research workflows. The workflow is divided into nine steps, covering the full research cycle from question formulation to dissemination and reproducibility, and includes two framing phases that address setup and documentation. Each research step is mapped across three operational domains: 1. LLM, referring to tasks delegated to language models; 2. Mind, referring to conceptual and interpretive contributions by the historian; and 3. Computational, referring to conventional programming-based methods like Python, R, Cytoscape, etc. The study emphasizes that LLMs are not replacements for domain expertise but can support and expand capacity of historians to process, verify, and interpret large corpora of texts. At the same time, it highlights the necessity of rigorous quality control, cross-checking outputs, and maintaining scholarly standards. Drawing from an in-depth study of three Shanghai merchants, the paper also proposes a structured workflow based on a real case study hat articulates the cognitive labor of the historian with both computational tools and generative AI. This paper makes both a methodological and epistemological contribution by showing how AI can be responsibly incorporated into historical research through transparent and reproducible workflows. It is intended as a practical guide and critical reflection for historians facing the increasingly complex landscape of AI-enhanced scholarship."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.761882781982422e-06, 'GPT4': 0.0021038055419921875, 'CLAUDE': 0.998046875, 'GOOGLE': 6.020069122314453e-06, 'OPENAI_O_SERIES': 5.364418029785156e-07, 'DEEPSEEK': 2.6285648345947266e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 1.7881393432617188e-06}}"
2508.05929,regular,post_llm,2025,8,"{'ai_likelihood': 6.490283542209202e-06, 'text': 'Towards Reliable Generative AI-Driven Scaffolding: Reducing Hallucinations and Enhancing Quality in Self-Regulated Learning Support\n\nGenerative Artificial Intelligence (GenAI) holds a potential to advance existing educational technologies with capabilities to automatically generate personalised scaffolds that support students\' self-regulated learning (SRL). While advancements in large language models (LLMs) promise improvements in the adaptability and quality of educational technologies for SRL, there remain concerns about the hallucinations in content generated by LLMs, which can compromise both the learning experience and ethical standards. To address these challenges, we proposed GenAI-enabled approaches for evaluating personalised SRL scaffolds before they are presented to students, aiming for reducing hallucinations and improving the overall quality of LLM-generated personalised scaffolds. Specifically, two approaches are investigated. The first approach involved developing a multi-agent system approach for reliability evaluation to assess the extent to which LLM-generated scaffolds accurately target relevant SRL processes. The second approach utilised the ""LLM-as-a-Judge"" technique for quality evaluation that evaluates LLM-generated scaffolds for their helpfulness in supporting students. We constructed evaluation datasets, and compared our results with single-agent LLM systems and machine learning approach baselines. Our findings indicate that the reliability evaluation approach is highly effective and outperforms the baselines, showing almost perfect alignment with human experts\' evaluations. Moreover, both proposed evaluation approaches can be harnessed to effectively reduce hallucinations. Additionally, we identified and discussed bias limitations of the ""LLM-as-a-Judge"" technique in evaluating LLM-generated scaffolds. We suggest incorporating these approaches into GenAI-powered personalised SRL scaffolding systems to mitigate hallucination issues and improve the overall scaffolding quality.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.15397,review,post_llm,2025,8,"{'ai_likelihood': 1.2914339701334636e-06, 'text': 'Multilateralism in the Global Governance of Artificial Intelligence\n\nThis chapter inquires how international multilateralism addresses the emergence of the general-purpose technology of Artificial Intelligence. In more detail, it analyses two key features of AI multilateralism: its generalized principles and the coordination of state relations in the realm of AI. Firstly, it distinguishes the generalized principles of AI multilateralism of epochal change, determinism, and dialectical understanding. In the second place, the adaptation of multilateralism to AI led to the integration of AI issues into the agendas of existing cooperation frameworks and the creation of new ad hoc frameworks focusing exclusively on AI issues. In both cases, AI multilateralism develops in the shadow of the state hierarchy in relations with other AI stakeholders. While AI multilateralism is multi-stakeholder, and the hierarchy between state and non-state actors may seem blurred, states preserve the competence as decisive decision-makers in agenda-setting, negotiation, and implementation of soft law international commitments.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.08739,regular,post_llm,2025,8,"{'ai_likelihood': 1.1258655124240452e-06, 'text': 'Dead Zone of Accountability: Why Social Claims in Machine Learning Research Should Be Articulated and Defended\n\nMany Machine Learning research studies use language that describes potential social benefits or technical affordances of new methods and technologies. Such language, which we call ""social claims"", can help garner substantial resources and influence for those involved in ML research and technology production. However, there exists a gap between social claims and reality (the claim-reality gap): ML methods often fail to deliver the claimed functionality or social impacts. This paper investigates the claim-reality gap and makes a normative argument for developing accountability mechanisms for it. In making the argument, we make three contributions. First, we show why the symptom - absence of social claim accountability - is problematic. Second, we coin dead zone of accountability - a lens that scholars and practitioners can use to identify opportunities for new forms of accountability. We apply this lens to the claim-reality gap and provide a diagnosis by identifying cognitive and structural resistances to accountability in the claim-reality gap. Finally, we offer a prescription - two potential collaborative research agendas that can help create the condition for social claim accountability.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.00536,review,post_llm,2025,8,"{'ai_likelihood': 0.00025457806057400175, 'text': 'Futures with Digital Minds: Expert Forecasts in 2025\n\nThis report presents findings from an expert survey on digital minds takeoff scenarios. The survey was conducted in early 2025 with 67 experts in digital minds research, AI research, philosophy, forecasting, and related fields. Participants provided probabilistic forecasts and qualitative reasoning on the development, characteristics, and societal impact of digital minds, that is, computer systems capable of subjective experience. Experts assigned high probability to digital minds being possible in principle (median 90%) and being created this century (65% by 2100), with a non-negligible probability of emergence by 2030 (20%). Many anticipated rapid growth in digital mind welfare capacity, with collective welfare capacity potentially matching that of billions of humans within a decade after the creation of the first digital mind. Participants also expected widespread claims from digital minds regarding their consciousness and rights, and predicted substantial societal disagreement over their existence and moral interests. Views diverged on whether digital mind welfare will be net positive or negative. These findings provide evidence that bears on the extent to which preparing the world for the potential arrival of digital minds should be a priority across domains such as research and governance. However, these findings should be interpreted cautiously in light of the potential for systematic overrepresentation of experts who deem digital minds particularly likely or important.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.11067,review,post_llm,2025,8,"{'ai_likelihood': 5.39753172132704e-06, 'text': 'Bias is a Math Problem, AI Bias is a Technical Problem: 10-year Literature Review of AI/LLM Bias Research Reveals Narrow [Gender-Centric] Conceptions of \'Bias\', and Academia-Industry Gap\n\nThe rapid development of AI tools and implementation of LLMs within downstream tasks has been paralleled by a surge in research exploring how the outputs of such AI/LLM systems embed biases, a research topic which was already being extensively explored before the era of ChatGPT. Given the high volume of research around the biases within the outputs of AI systems and LLMs, it is imperative to conduct systematic literature reviews to document throughlines within such research. In this paper, we conduct such a review of research covering AI/LLM bias in four premier venues/organizations -- *ACL, FAccT, NeurIPS, and AAAI -- published over the past 10 years. Through a coverage of 189 papers, we uncover patterns of bias research and along what axes of human identity they commonly focus. The first emergent pattern within the corpus was that 82% (155/189) papers did not establish a working definition of ""bias"" for their purposes, opting instead to simply state that biases and stereotypes exist that can have harmful downstream effects while establishing only mathematical and technical definition of bias. 94 of these 155 papers have been published in the past 5 years, after Blodgett et al. (2020)\'s literature review with a similar finding about NLP research and recommendation to consider how such researchers should conceptualize bias, going beyond strictly technical definitions. Furthermore, we find that a large majority of papers -- 79.9% or 151/189 papers -- focus on gender bias (mostly, gender and occupation bias) within the outputs of AI systems and LLMs. By demonstrating a strong focus within the field on gender, race/ethnicity (30.2%; 57/189), age (20.6%; 39/189), religion (19.1%; 36/189) and nationality (13.2%; 25/189) bias, we document how researchers adopt a fairly narrow conception of AI bias by overlooking several non-Western communities in fairness research, as we advocate for a stronger coverage of such populations. Finally, we note that while our corpus contains several examples of innovative debiasing methods across the aforementioned aspects of human identity, only 10.6% (20/189) include recommendations for how to implement their findings or contributions in real-world AI systems or design processes. This indicates a concerning academia-industry gap, especially since many of the biases that our corpus contains several successful mitigation methods that still persist within the outputs of AI systems and LLMs commonly used today. We conclude with recommendations towards future AI/LLM fairness research, with stronger focus on diverse marginalized populations.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.05992,regular,post_llm,2025,8,"{'ai_likelihood': 0.91943359375, 'text': 'Surviving the Narrative Collapse: Sustainability and Justice in Computing Within Limits\n\nSustainability-driven computing research - encompassing equity, diversity, climate change, and social justice - is increasingly dismissed as woke or even dangerous in many sociopolitical contexts. As misinformation, ideological polarisation, deliberate ignorance and reactionary narratives gain ground, how can sustainability research in computing continue to exist and make an impact? This paper explores these tensions through Fictomorphosis, a creative story retelling method that reframes contested topics through different genres and perspectives. By engaging computing researchers in structured narrative transformations, we investigate how sustainability-oriented computing research is perceived, contested, and can adapt in a post-truth world.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00016117095947265625, 'GPT4': 0.0011806488037109375, 'CLAUDE': 0.935546875, 'GOOGLE': 0.00414276123046875, 'OPENAI_O_SERIES': 6.681680679321289e-05, 'DEEPSEEK': 0.011962890625, 'GROK': 1.9073486328125e-06, 'NOVA': 4.5299530029296875e-06, 'OTHER': 9.381771087646484e-05, 'HUMAN': 0.047027587890625}}"
2508.07454,review,post_llm,2025,8,"{'ai_likelihood': 0.9951171875, 'text': ""An Empirical Inquiry into Surveillance Capitalism: Web Tracking\n\nThe modern web is increasingly characterized by the pervasiveness of Surveillance Capitalism. This investigation employs an empirical approach to examine this phenomenon through the web tracking practices of major tech companies -- specifically Google, Apple, Facebook, Amazon, and Microsoft (GAFAM) -- and their relation to financial performance indicators. Using longitudinal data from WhoTracks.Me spanning from 2017 to 2025 and publicly accessible SEC filings, this paper analyzes patterns and trends in web tracking data to establish empirical evidence of Surveillance Capitalism's extraction mechanisms. Our findings reveal Google's omnipresent position on the web, a three-tier stratification among GAFAM companies in the surveillance space, and evidence suggesting an evolution of tracking techniques to evade detection. The investigation further discusses the social and environmental costs of web tracking and how alternative technologies, such as the Gemini protocol, offer pathways to challenge the extractive logic of this new economic order. By closely examining surveillance activities, this research contributes to an ongoing effort to better understand the current state and future trajectory of Surveillance Capitalism."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.4286251068115234e-05, 'GPT4': 0.9970703125, 'CLAUDE': 0.0003116130828857422, 'GOOGLE': 0.0020008087158203125, 'OPENAI_O_SERIES': 9.655952453613281e-06, 'DEEPSEEK': 3.5762786865234375e-07, 'GROK': 1.1920928955078125e-07, 'NOVA': 0.0, 'OTHER': 6.556510925292969e-07, 'HUMAN': 0.0004127025604248047}}"
2508.11699,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Reclaiming Constitutional Authority of Algorithmic Power\n\nWhether and how to govern AI is no longer a question of technical regulation. It is a question of constitutional authority. Across jurisdictions, algorithmic systems now perform functions once reserved to public institutions: allocating welfare, determining legal status, mediating access to housing, employment, and healthcare. These are not merely administrative operations. They are acts of rule. Yet the dominant models of AI governance fail to confront this reality. The European approach centers on rights-based oversight, presenting its regulatory framework as a principled defense of human dignity. The American model relies on decentralized experimentation, treating fragmentation as a proxy for democratic legitimacy. Both, in different ways, evade the structural question: who authorizes algorithmic power, through what institutions, and on what terms. This Article offers an alternative. Drawing from early modern Reformed political thought, it reconstructs a constitutional framework grounded in covenantal authority and the right of lawful resistance. It argues that algorithmic governance must rest on three principles. First, that all public power must be lawfully delegated through participatory authorization. Second, that authority must be structured across representative communities with the standing to consent, contest, or refuse. Third, that individuals retain a constitutional right to resist systems that impose orthodoxy or erode the domain of conscience. These principles are then operationalized through doctrinal analysis of federalism, nondelegation, compelled speech, and structural accountability. On this view, the legitimacy of algorithmic governance turns not on procedural safeguards or policy design, but on whether it reflects a constitutional order in which power is authorized by the governed, constrained by law, and answerable to those it affects.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.919269561767578e-05, 'GPT4': 0.0016422271728515625, 'CLAUDE': 0.43994140625, 'GOOGLE': 2.4259090423583984e-05, 'OPENAI_O_SERIES': 2.6226043701171875e-06, 'DEEPSEEK': 0.55810546875, 'GROK': 0.0, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.9669532775878906e-06, 'HUMAN': 1.6748905181884766e-05}}"
2508.05953,regular,post_llm,2025,8,"{'ai_likelihood': 1.6424391004774305e-05, 'text': ""SCALEFeedback: A Large-Scale Dataset of Synthetic Computer Science Assignments for LLM-generated Educational Feedback Research\n\nUsing LLMs to give educational feedback to students for their assignments has attracted much attention in the AI in Education field. Yet, there is currently no large-scale open-source dataset of student assignments that includes detailed assignment descriptions, rubrics, and student submissions across various courses. As a result, research on generalisable methodology for automatic generation of effective and responsible educational feedback remains limited. In the current study, we constructed a large-scale dataset of Synthetic Computer science Assignments for LLM-generated Educational Feedback research (SCALEFeedback). We proposed a Sophisticated Assignment Mimicry (SAM) framework to generate the synthetic dataset by one-to-one LLM-based imitation from real assignment descriptions, student submissions to produce their synthetic versions. Our open-source dataset contains 10,000 synthetic student submissions spanning 155 assignments across 59 university-level computer science courses. Our synthetic submissions achieved BERTScore F1 0.84, PCC of 0.62 for assignment marks and 0.85 for length, compared to the corresponding real-world assignment dataset, while ensuring perfect protection of student private information. All these results of our SAM framework outperformed results of a naive mimicry method baseline. The LLM-generated feedback for our synthetic assignments demonstrated the same level of effectiveness compared to that of real-world assignment dataset. Our research showed that one-to-one LLM imitation is a promising method for generating open-source synthetic educational datasets that preserve the original dataset's semantic meaning and student data distribution, while protecting student privacy and institutional copyright. SCALEFeedback enhances our ability to develop LLM-based generalisable methods for offering high-quality, automated educational feedback in a scalable way."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.16613,review,post_llm,2025,8,"{'ai_likelihood': 0.00024822023179796006, 'text': 'What are the limits to biomedical research acceleration through general-purpose AI?\n\nAlthough general-purpose artificial intelligence (GPAI) is widely expected to accelerate scientific discovery, its practical limits in biomedicine remain unclear. We assess this potential by developing a framework of GPAI capabilities across the biomedical research lifecycle. Our scoping literature review indicates that current GPAI could deliver a speed increase of around 2x, whereas future GPAI could facilitate strong acceleration of up to 25x for physical tasks and 100x for cognitive tasks. However, achieving these gains may be severely limited by factors such as irreducible biological constraints, research infrastructure, data access, and the need for human oversight. Our expert elicitation with eight senior biomedical researchers revealed skepticism regarding the strong acceleration of tasks such as experiment design and execution. In contrast, strong acceleration of manuscript preparation, review and publication processes was deemed plausible. Notably, all experts identified the assimilation of new tools by the scientific community as a critical bottleneck. Realising the potential of GPAI will therefore require more than technological progress; it demands targeted investment in shared automation infrastructure and systemic reforms to research and publication practices.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.09007,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Principles for Environmental Justice in Technology: Toward a Regenerative Future\n\nThis paper introduces the Environmental Justice in Technology (EJIT) Principles, a framework to help reorient technological development toward social and ecological justice and collective flourishing. In response to prevailing models of technological innovation that prioritize speed, scale, and profit while neglecting systemic injustice, the EJIT principles offer an alternative: a set of guiding values that foreground interdependence, repair, and community self-determination. Drawing inspiration from the 1991 principles of environmental justice, this framework extends their commitments into the technological domain, treating environmental justice not as a peripheral concern but as a necessary foundation for building equitable and regenerative futures. We situate the EJIT principles within the broader landscape of environmental justice, design justice, and post-growth computing, proposing them as a values infrastructure for resisting extractive defaults and envisioning technological systems that operate in reciprocity with people and the planet. In doing so, this article aims to support collective efforts to transform not only what technologies we build, but how, why, and for whom.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0014209747314453125, 'GPT4': 0.1263427734375, 'CLAUDE': 0.50439453125, 'GOOGLE': 0.01149749755859375, 'OPENAI_O_SERIES': 0.0004131793975830078, 'DEEPSEEK': 0.355712890625, 'GROK': 2.682209014892578e-06, 'NOVA': 5.424022674560547e-06, 'OTHER': 0.00022721290588378906, 'HUMAN': 0.00012624263763427734}}"
2508.11705,regular,post_llm,2025,8,"{'ai_likelihood': 0.3561740451388889, 'text': 'Artificial intelligence (AI) techniques: a game-changer in Digital marketing for shop\n\nThe quick growth of shops using artificial intelligence (AI) techniques has changed digital marketing activities and changed how businesses interact and reach their consumers. (AI) techniques are reshaping digital interactions between shops and consumers interact digitally by providing a more efficient and customized experience, fostering deeper engagement and more informed decision-making. This study investigates how (AI) techniques affect consumer interaction and decision-making over purchases with shops that use digital marketing. The partial least squares method was used to evaluate data from a survey with 300 respondents. When consumer engagement mediates this relationship, artificial intelligence (AI) techniques have a more favorable impact on purchasing decision-making. Consequently, decision-making is positively impacted through consumer engagement. The findings emphasize that for a bigger impact of the (AI) techniques on decision-making, the consumer must initially interact with the (AI) techniques. This research unveils a contemporary pathway in the field of AI-supported shop engagements and illustrates the distinct impact of (AI) techniques on consumer satisfaction, trust, and loyalty, revolutionizing traditional models of customer-purchase decision-making and shop engagement processes. This study provides previously unheard-of insight, into the revolutionary potential of (AI) techniques in influencing customer behavior and shop relationships', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.0586,review,post_llm,2025,8,"{'ai_likelihood': 0.0035264756944444445, 'text': 'Sprouting technology otherwise, hospicing negative commons -- Rethinking technology in the transition to sustainability-oriented futures\n\nDue to its significant and growing environmental harms, both directly through its materiality and indirectly through its pervasive integration into unsustainable economic systems, ICT will need to be radically redirected to align with sustainability-oriented futures. While the role of ICT in such futures will likely diverge significantly from current dynamics, it will probably not be entirely disconnected from the present. Instead, such transition involves complex dynamics of continuity, adaptation and rupture. Drawing from recent work in transition studies, the commons (particularly ""negative commons""), as well as some of the Limits literature, this article proposes a conceptual framework for navigating this redirection. The framework attempts to bring together the disentanglement from sociotechnical elements incompatible with long-term sustainability and the support of existing practices that may serve as foundations for alternative technological paths. It introduces four categories: ruins, ghosts, seeds and visions, to examine how material and cultural aspects of computing may become obsolete, persist in latent or reinterpreted forms, or contribute to sustainability-oriented futures. Through both empirical and speculative examples, I intend to show how this lens can help researchers and practitioners engage more concretely with the tensions, inheritances, and opportunities involved in redirecting computing towards more sustainable and equitable futures.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.19036,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""Of the People, By the Algorithm: How AI Transforms Democratic Representation\n\nThis review examines how AI technologies are transforming democratic representation, focusing on citizen participation and algorithmic decision-making. The analysis reveals that AI technologies are reshaping democratic processes in fundamental ways: enabling mass-scale deliberation, changing how citizens access and engage with political information, and transforming how representatives make and implement decisions. While AI offers unprecedented opportunities for enhancing democratic participation and governance efficiency, it also presents significant challenges to democratic legitimacy and accountability. Social media platforms' AI-driven algorithms currently mediate much political discourse, creating concerns about information manipulation and privacy. Large Language Models introduce both epistemic challenges and potential tools for improving democratic dialogue. The emergence of Mass Online Deliberation platforms suggests possibilities for scaling up meaningful citizen participation, while Algorithmic Decision-Making systems promise more efficient policy implementation but face limitations in handling complex political trade-offs. As these systems become prevalent, representatives may assume the role of architects of automated decision frameworks, responsible for guiding the translation of politically contested concepts into technical parameters and metrics. Advanced deliberation platforms offering real-time insights into citizen preferences will challenge traditional representative independence and discretion to interpret public will. The institutional integration of these participation mechanisms requires frameworks that balance the benefits with democratic stability through hybrid systems weighting different forms of democratic expression."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 1.7881393432617188e-06, 'CLAUDE': 1.0, 'GOOGLE': 5.960464477539063e-08, 'OPENAI_O_SERIES': 5.9604644775390625e-06, 'DEEPSEEK': 6.0617923736572266e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.01091,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Disaggregated Health Data in LLMs: Evaluating Data Equity in the Context of Asian American Representation\n\nLarge language models (LLMs), such as ChatGPT and Claude, have emerged as essential tools for information retrieval, often serving as alternatives to traditional search engines. However, ensuring that these models provide accurate and equitable information tailored to diverse demographic groups remains an important challenge. This study investigates the capability of LLMs to retrieve disaggregated health-related information for sub-ethnic groups within the Asian American population, such as Korean and Chinese communities. Data disaggregation has been a critical practice in health research to address inequities, making it an ideal domain for evaluating representation equity in LLM outputs. We apply a suite of statistical and machine learning tools to assess whether LLMs deliver appropriately disaggregated and equitable information. By focusing on Asian American sub-ethnic groups, a highly diverse population often aggregated in traditional analyses; we highlight how LLMs handle complex disparities in health data. Our findings contribute to ongoing discussions about responsible AI, particularly in ensuring data equity in the outputs of LLM-based systems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.5703182220458984e-05, 'GPT4': 0.9990234375, 'CLAUDE': 0.00022792816162109375, 'GOOGLE': 0.0005202293395996094, 'OPENAI_O_SERIES': 2.8371810913085938e-05, 'DEEPSEEK': 5.960464477539062e-07, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 5.245208740234375e-06}}"
2508.1604,regular,post_llm,2025,8,"{'ai_likelihood': 0.07758246527777778, 'text': ""Disproportionate Voices: Participation Inequality and Hostile Engagement in News Comments\n\nDigital platforms were expected to foster broad participation in public discourse, yet online engagement remains highly unequal and underexplored. This study examines the digital participation divide and its link to hostile engagement in news comment sections. Analyzing 260 million comments from 6.2 million users over 13 years on Naver News, South Korea's largest news aggregation platform, we quantify participation inequality using the Gini and Palma indexes and estimate hostility levels with a KC-Electra model, which outperformed other Korean pre-trained transformers in multi-label classification tasks. The findings reveal a highly skewed participation structure, with a small number of frequent users dominating discussions, particularly in the Politics and Society domains and popular news stories. Participation inequality spikes during presidential elections, and frequent commenters are significantly more likely to post hostile content, suggesting that online discourse is shaped disproportionately by a highly active and often hostile subset of users. Using individual-level digital trace data, this study provides empirical insights into the behavioral dynamics of online participation inequality and its broader implications for public digital discourse."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.0676,review,post_llm,2025,8,"{'ai_likelihood': 0.99755859375, 'text': 'Understanding Privacy Norms Around LLM-Based Chatbots: A Contextual Integrity Perspective\n\nLLM-driven chatbots like ChatGPT have created large volumes of conversational data, but little is known about how user privacy expectations are evolving with this technology. We conduct a survey experiment with 300 US ChatGPT users to understand emerging privacy norms for sharing chatbot data. Our findings reveal a stark disconnect between user concerns and behavior: 82% of respondents rated chatbot conversations as sensitive or highly sensitive - more than email or social media posts - but nearly half reported discussing health topics and over one-third discussed personal finances with ChatGPT. Participants expressed strong privacy concerns (t(299) = 8.5, p < .01) and doubted their conversations would remain private (t(299) = -6.9, p < .01). Despite this, respondents uniformly rejected sharing personal data (search history, emails, device access) for improved services, even in exchange for premium features worth $200. To identify which factors influence appropriate chatbot data sharing, we presented participants with factorial vignettes manipulating seven contextual factors. Linear mixed models revealed that only the transmission factors such as informed consent, data anonymization, or the removal of personally identifiable information, significantly affected perceptions of appropriateness and concern for data access. Surprisingly, contextual factors including the recipient of the data (hospital vs. tech company), purpose (research vs. advertising), type of content, and geographic location did not show significant effects. Our results suggest that users apply consistent baseline privacy expectations to chatbot data, prioritizing procedural safeguards over recipient trustworthiness. This has important implications for emerging agentic AI systems that assume user willingness to integrate personal data across platforms.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1920928955078125e-06, 'GPT4': 4.494190216064453e-05, 'CLAUDE': 0.9990234375, 'GOOGLE': 0.00015556812286376953, 'OPENAI_O_SERIES': 1.1324882507324219e-06, 'DEEPSEEK': 8.678436279296875e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 0.000621795654296875}}"
2508.12174,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Urban AI Governance Must Embed Legal Reasonableness for Democratic and Sustainable Cities\n\nThis position paper argues that embedding the legal ""reasonable person"" standard in municipal AI systems is essential for democratic and sustainable urban governance. As cities increasingly deploy artificial intelligence (AI) systems, concerns around equity, accountability, and normative legitimacy are growing. This paper introduces the Urban Reasonableness Layer (URL), a conceptual framework that adapts the legal ""reasonable person"" standard for supervisory oversight in municipal AI systems, including potential future implementations of Artificial General Intelligence (AGI). Drawing on historical analogies, scenario mapping, and participatory norm-setting, we explore how legal and community-derived standards can inform AI decision-making in urban contexts. Rather than prescribing a fixed solution, the URL is proposed as an exploratory architecture for negotiating contested values, aligning automation with democratic processes, and interrogating the limits of technical alignment. Our key contributions include: (1) articulating the conceptual and operational architecture of the URL; (2) specifying participatory mechanisms for dynamic normative threshold-setting; (3) presenting a comparative scenario analysis of governance trajectories; and (4) outlining evaluation metrics and limitations. This work contributes to ongoing debates on urban AI governance by foregrounding pluralism, contestability, and the inherently political nature of socio-technical systems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.841255187988281e-06, 'GPT4': 0.003955841064453125, 'CLAUDE': 0.6767578125, 'GOOGLE': 0.0008597373962402344, 'OPENAI_O_SERIES': 8.344650268554688e-07, 'DEEPSEEK': 0.318359375, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 2.562999725341797e-06, 'HUMAN': 1.9073486328125e-06}}"
2508.15112,regular,post_llm,2025,8,"{'ai_likelihood': 1.5033615960015191e-05, 'text': ""Mapping Students' AI Literacy Framing and Learning through Reflective Journals\n\nThis research paper presents a study of undergraduate technology students' self-reflective learning about artificial intelligence (AI). Research on AI literacy proposes that learners must develop five competencies associated with AI: awareness, knowledge, application, evaluation, and development. It is important to understand what, how, and why students learn about AI so formal instruction can better support their learning. We conducted a reflective journal study where students described their interactions with AI each week. Data was collected over six weeks and analyzed using an emergent interpretive process. We found that the participants were aware of AI, expressed opinions on their future use of AI skills, and conveyed conflicted feelings about developing deep AI expertise. They also described ethical concerns with AI use and saw themselves as intermediaries of knowledge for friends and family. We present the implications of this study and propose ideas for future work in this area."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.08311,regular,post_llm,2025,8,"{'ai_likelihood': 1.1391109890407986e-05, 'text': 'ICT Within Limits Is Bound To Be Old-Fashioned By Design\n\nCrossing multiple planetary boundaries places us in a zone of uncertainty that is characterized by considerable fluctuations in climatic events. The situation is exacerbated by the relentless use of resources and energy required to develop digital infrastructures that have become pervasive and ubiquitous. We are bound to these infrastructures, dead technologies and negative commons, just as much as they bind us. Although their growth threatens the necessary reduction of our impact, we have a responsibility to maintain them until we can do without them.\n  In university setting, as well as in any public organization, urban mines per se, we propose an IT architecture based on the exclusive use of unreliable waste from electrical and electronic equipment (WEEE) as a frugal alternative to the incessant replacement of devices. Powered by renewable energy, autonomous, robust, adaptable, and built on battle-tested open-source software, we envision this solution for a situation where use is bound to decline eventually, to close this damaging technological chapter. Digital technology, the idol of modern times, is to meet its twilight if we do not want to irrevocably alter the critical zone.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.11713,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': ""A Production-Ready Machine Learning System for Inclusive Employment: Requirements Engineering and Implementation of AI-Driven Disability Job Matching Platform\n\nEmployment inclusion of people with disabilities remains critically low in Italy, with only 3.5% employed nationally despite mandatory hiring quotas. Traditional manual matching processes require 30-60 minutes per candidate, creating bottlenecks that limit service capacity. Our goal is to develop and validate a production-ready machine learning system for disability employment matching that integrates social responsibility requirements while maintaining human oversight in decision-making. We employed participatory requirements engineering with Centro per l'Impiego di Villafranca di Verona professionals. The system implements a seven-model ensemble with parallel hyperparameter optimization using Optuna. Multi-dimensional scoring combines semantic compatibility, geographic distance, and employment readiness assessment. The system achieves 90.1% F1-score and sub-100ms response times while processing 500,000 candidate-company combinations in under 10 minutes. Expert validation confirms 60-100% capacity increases for employment centers. The LightGBM ensemble shows optimal performance with 94.6-second training time. Thus, advanced AI systems can successfully integrate social responsibility requirements without compromising technical performance. The participatory design methodology provides a replicable framework for developing ethical AI applications in sensitive social domains. The complete system, including source code, documentation, and deployment guides, is openly available to facilitate replication and adaptation by other regions and countries facing similar challenges."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0728836059570312e-06, 'GPT4': 9.775161743164062e-06, 'CLAUDE': 1.0, 'GOOGLE': 3.904104232788086e-05, 'OPENAI_O_SERIES': 7.033348083496094e-06, 'DEEPSEEK': 3.0219554901123047e-05, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 2.980232238769531e-07, 'HUMAN': 7.212162017822266e-06}}"
2508.08143,review,post_llm,2025,8,"{'ai_likelihood': 6.324715084499783e-06, 'text': 'AI Gossip\n\nGenerative AI chatbots like OpenAI\'s ChatGPT and Google\'s Gemini routinely make things up. They ""hallucinate"" historical events and figures, legal cases, academic papers, non-existent tech products and features, biographies, and news articles. Recently, some have argued that these hallucinations are better understood as bullshit. Chatbots produce rich streams of text that look truth-apt without any concern for the truthfulness of what this text says. But can they also gossip? We argue that they can. After some definitions and scene-setting, we focus on a recent example to clarify what AI gossip looks like before considering some distinct harms -- what we call ""technosocial harms"" -- that follow from it.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.08833,regular,post_llm,2025,8,"{'ai_likelihood': 8.27842288547092e-07, 'text': 'Position: The Pitfalls of Over-Alignment: Overly Caution Health-Related Responses From LLMs are Unethical and Dangerous\n\nLarge Language Models (LLMs) are usually aligned with ""human values/preferences"" to prevent harmful output. Discussions around the alignment of Large Language Models (LLMs) generally focus on preventing harmful outputs. However, in this paper, we argue that in health-related queries, over-alignment-leading to overly cautious responses-can itself be harmful, especially for people with anxiety and obsessive-compulsive disorder (OCD). This is not only unethical but also dangerous to the user, both mentally and physically. We also showed qualitative results that some LLMs exhibit varying degrees of alignment. Finally, we call for the development of LLMs with stronger reasoning capabilities that provide more tailored and nuanced responses to health queries. Warning: This paper contains materials that could trigger health anxiety or OCD. Dataset and full results can be found in https://github.com/weathon/over-alignment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.08084,review,post_llm,2025,8,"{'ai_likelihood': 7.185671064588759e-06, 'text': '$100,000 or the Robot Gets it! Tech Workers\' Resistance Guide: Tech Worker Actions, History, Risks, Impacts, and the Case for a Radical Flank\n\nOver the past decade, Big Tech has faced increasing levels of worker activism. While worker actions have resulted in positive outcomes (e.g., cancellation of Google\'s Project Dragonfly), such successes have become increasingly infrequent. This is, in part, because corporations have adjusted their strategies to dealing with increased worker activism (e.g., increased retaliation against workers, and contracts clauses that prevent cancellation due to worker pressure). This change in company strategy prompts urgent questions about updating worker strategies for influencing corporate behavior in an industry with vast societal impact. Current discourse on tech worker activism often lacks empirical grounding regarding its scope, history, and strategic calculus. Our work seeks to bridge this gap by firstly conducting a systematic analysis of worker actions at Google and Microsoft reported in U.S. newspapers to delineate their characteristics. We then situate these actions within the long history of labour movements and demonstrate that, despite perceptions of radicalism, contemporary tech activism is comparatively moderate. Finally, we engage directly with current and former tech activists to provide a novel catalogue of potential worker actions, evaluating their perceived risks, impacts, and effectiveness (concurrently publishing ""Tech Workers\' Guide to Resistance""). Our findings highlight considerable variation in strategic thinking among activists themselves. We conclude by arguing that the establishment of a radical flank could increase the effectiveness of current movements.\n  ""Tech Workers\' Guide to Resistance"" can be found at https://www.cs.toronto.edu/~msa/TechWorkersResistanceGuide.pdf or https://doi.org/10.5281/zenodo.16779082', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.01408,regular,post_llm,2025,8,"{'ai_likelihood': 8.145968119303387e-06, 'text': 'Artificial Intelligence and Misinformation in Art: Can Vision Language Models Judge the Hand or the Machine Behind the Canvas?\n\nThe attribution of artworks in general and of paintings in particular has always been an issue in art. The advent of powerful artificial intelligence models that can generate and analyze images creates new challenges for painting attribution. On the one hand, AI models can create images that mimic the style of a painter, which can be incorrectly attributed, for example, by other AI models. On the other hand, AI models may not be able to correctly identify the artist for real paintings, inducing users to incorrectly attribute paintings. In this paper, both problems are experimentally studied using state-of-the-art AI models for image generation and analysis on a large dataset with close to 40,000 paintings from 128 artists. The results show that vision language models have limited capabilities to: 1) perform canvas attribution and 2) to identify AI generated images. As users increasingly rely on queries to AI models to get information, these results show the need to improve the capabilities of VLMs to reliably perform artist attribution and detection of AI generated images to prevent the spread of incorrect information.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.11678,review,post_llm,2025,8,"{'ai_likelihood': 0.7290039062499999, 'text': 'Optimizing Peer Grading: A Systematic Literature Review of Reviewer Assignment Strategies and Quantity of Reviewers\n\nPeer assessment has established itself as a critical pedagogical tool in academic settings, offering students timely, high-quality feedback to enhance learning outcomes. However, the efficacy of this approach depends on two factors: (1) the strategic allocation of reviewers and (2) the number of reviews per artifact. This paper presents a systematic literature review of 87 studies (2010--2024) to investigate how reviewer-assignment strategies and the number of reviews per submission impact the accuracy, fairness, and educational value of peer assessment. We identified four common reviewer-assignment strategies: random assignment, competency-based assignment, social-network-based assignment, and bidding. Drawing from both quantitative data and qualitative insights, we explored the trade-offs involved in each approach. Random assignment, while widely used, often results in inconsistent grading and fairness concerns. Competency-based strategies can address these issues. Meanwhile, social and bidding-based methods have the potential to improve fairness and timeliness -- existing empirical evidence is limited. In terms of review count, assigning three reviews per submission emerges as the most common practice. A range of three to five reviews per student or per submission is frequently cited as a recommended spot that balances grading accuracy, student workload, learning outcomes, and engagement.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 1.4483928680419922e-05, 'GPT4': 0.984375, 'CLAUDE': 3.5703182220458984e-05, 'GOOGLE': 0.0035839080810546875, 'OPENAI_O_SERIES': 0.0011692047119140625, 'DEEPSEEK': 3.522634506225586e-05, 'GROK': 0.0, 'NOVA': 1.1920928955078125e-07, 'OTHER': 2.562999725341797e-06, 'HUMAN': 0.0108489990234375}}"
2508.06267,review,post_llm,2025,8,"{'ai_likelihood': 1.4901161193847656e-06, 'text': ""Analysis and Constructive Criticism of the Official Data Protection Impact Assessment of the German Corona-Warn-App\n\nOn June 15, 2020, the official data protection impact assessment (DPIA) for the German Corona-Warn-App (CWA) was made publicly available. Shortly thereafter, the app was made available for download in the app stores. However, the first version of the DPIA had significant weaknesses, as this paper argues. However since then, the quality of the official DPIA increased immensely due to interventions and interactions such as an alternative DPIA produced by external experts and extensive public discussions. To illustrate the development and improvement, the initial weaknesses of the official DPIA are documented and analyzed here. For this paper to meaningfully do this, first the purpose of a DPIA is briefly summarized. According to Article 35 of the GDPR, it consists primarily of identifying the risks to the fundamental rights and freedoms of natural persons. This paper documents at least specific methodological, technical and legal shortcomings of the initial DPIA of the CWA: 1) It only focused on the app itself, neither on the whole processing procedure nor on the infrastructure used. 2) It only briefly touched on the main data protection specific attacker, the processing organization itself. And 3) The discussion of effective safeguards to all risks including such as the ones posed by Google and Apple has only insufficiently been worked out. Finally, this paper outlines the constructive criticism and suggestions uttered, also by the authors of this paper, regarding the initial release. As of now, some of those constructive contributions have been worked into the current DPIA, such as 1) and 2), but some central ones still haven't, such as 3). This paper aims to provide an opportunity to improve the practical knowledge and academic discourse regarding high-quality DPIAs."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.11698,review,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Large Language Models in the Data Science Lifecycle: A Systematic Mapping Study\n\nIn recent years, Large Language Models (LLMs) have emerged as transformative tools across numerous domains, impacting how professionals approach complex analytical tasks. This systematic mapping study comprehensively examines the application of LLMs throughout the Data Science lifecycle. By analyzing relevant papers from Scopus and IEEE databases, we identify and categorize the types of LLMs being applied, the specific stages and tasks of the data science process they address, and the methodological approaches used for their evaluation. Our analysis includes a detailed examination of evaluation metrics employed across studies and systematically documents both positive contributions and limitations of LLMs when applied to data science workflows. This mapping provides researchers and practitioners with a structured understanding of the current landscape, highlighting trends, gaps, and opportunities for future research in this rapidly evolving intersection of LLMs and data science.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001366138458251953, 'GPT4': 0.76416015625, 'CLAUDE': 0.1005859375, 'GOOGLE': 0.1328125, 'OPENAI_O_SERIES': 0.00098419189453125, 'DEEPSEEK': 0.0013666152954101562, 'GROK': 6.616115570068359e-06, 'NOVA': 2.1457672119140625e-06, 'OTHER': 4.589557647705078e-06, 'HUMAN': 5.269050598144531e-05}}"
2508.00966,review,post_llm,2025,8,"{'ai_likelihood': 0.5190429687499999, 'text': 'Value of the Teaching Career and Factors in Its Path in Peru\n\nThe teaching career shares common global characteristics, such as internal promotion, performance evaluation, recruitment of top candidates, continuous training, specialization, and peer learning. This study aims to describe the factors associated with the value placed on the teaching career in Peru. A total of 28217 public school teachers were analyzed using data from the 2020 National Teacher Survey. A variable measuring the ""value of the teaching career"" was constructed using eight indicators and categorized as low, medium, or high. Another variable, vision of the future, was classified as pessimistic, conformist, or optimistic. This observational, cross-sectional, and analytical study included variables related to in-service training, working conditions, professional recognition, and sociodemographic characteristics. Among the teachers surveyed, 45.8 % expressed an optimistic outlook on the future of the profession, 48 % held a conformist view, and only 6.2 % reported a pessimistic perspective. A generalized linear model revealed that the value placed on the teaching career was significantly associated with male gender (p = 0.002), a professional career (p < 0.001), an optimistic outlook (p = 0.033), and working at the primary level (p < 0.001). It was concluded that Peruvian teachers predominantly hold conformist or optimistic views of their profession. This highlights the need to reinforce merit-based advancement, competency-based training, intrinsic motivation, and ongoing professional development', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.02392578125, 'GPT4': 0.1436767578125, 'CLAUDE': 0.1080322265625, 'GOOGLE': 0.034881591796875, 'OPENAI_O_SERIES': 0.05029296875, 'DEEPSEEK': 0.51318359375, 'GROK': 1.0251998901367188e-05, 'NOVA': 0.034759521484375, 'OTHER': 0.0004253387451171875, 'HUMAN': 0.09075927734375}}"
2508.16642,review,post_llm,2025,8,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'AI as IA: The use and abuse of artificial intelligence (AI) for human enhancement through intellectual augmentation (IA)\n\nThis paper offers an overview of the prospects and ethics of using AI to achieve human enhancement, and more broadly what we call intellectual augmentation (IA). After explaining the central notions of human enhancement, IA, and AI, we discuss the state of the art in terms of the main technologies for IA, with or without brain-computer interfaces. Given this picture, we discuss potential ethical problems, namely inadequate performance, safety, coercion and manipulation, privacy, cognitive liberty, authenticity, and fairness in more detail. We conclude that while there are very significant technical hurdles to real human enhancement through AI, and significant ethical problems, there are also significant benefits that may realistically be achieved in ways that are consonant with a rights-based ethics as well. We also highlight the specific concerns that apply particularly to applications of AI for ""sheer"" IA (more realistic in the near term), and to enhancement applications, respectively.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.20462,regular,post_llm,2025,8,"{'ai_likelihood': 0.49370659722222227, 'text': 'Automated Quality Assessment for LLM-Based Complex Qualitative Coding: A Confidence-Diversity Framework\n\nComputational social science lacks a scalable and reliable mechanism to assure quality for AI-assisted qualitative coding when tasks demand domain expertise and long-text reasoning, and traditional double-coding is prohibitively costly at scale. We develop and validate a dual-signal quality assessment framework that combines model confidence with inter-model consensus (external entropy) and evaluate it across legal reasoning (390 Supreme Court cases), political analysis (645 hyperpartisan articles), and medical classification (1,000 clinical transcripts). External entropy is consistently negatively associated with accuracy (r = -0.179 to -0.273, p < 0.001), while confidence is positively associated in two domains (r = 0.104 to 0.429). Weight optimization improves over single-signal baselines by 6.6-113.7% and transfers across domains (100% success), and an intelligent triage protocol reduces manual verification effort by 44.6% while maintaining quality. The framework offers a principled, domain-agnostic quality assurance mechanism that scales qualitative coding without extensive double-coding, provides actionable guidance for sampling and verification, and enables larger and more diverse corpora to be analyzed with maintained rigor.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2508.0625,regular,post_llm,2025,8,"{'ai_likelihood': 1.0, 'text': 'Dirty Bits in Low-Earth Orbit: The Carbon Footprint of Launching Computers\n\nLow-Earth Orbit (LEO) satellites are increasingly proposed for communication and in-orbit computing, achieving low-latency global services. However, their sustainability remains largely unexamined. This paper investigates the carbon footprint of computing in space, focusing on lifecycle emissions from launch over orbital operation to re-entry. We present ESpaS, a lightweight tool for estimating carbon intensities across CPU usage, memory, and networking in orbital vs. terrestrial settings. Three worked examples compare (i) launch technologies (state-of-the-art rocket vs. potential next generation) and (ii) operational emissions of data center workloads in orbit and on the ground. Results show that, even under optimistic assumptions, in-orbit systems incur significantly higher carbon costs - up to an order of magnitude more than terrestrial equivalents - primarily due to embodied emissions from launch and re-entry. Our findings advocate for carbon-aware design principles and regulatory oversight in developing sustainable digital infrastructure in orbit.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.602836608886719e-06, 'GPT4': 0.002101898193359375, 'CLAUDE': 0.99658203125, 'GOOGLE': 2.2411346435546875e-05, 'OPENAI_O_SERIES': 9.357929229736328e-06, 'DEEPSEEK': 0.0010232925415039062, 'GROK': 1.1920928955078125e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.4901161193847656e-06, 'HUMAN': 1.8298625946044922e-05}}"
2508.19749,review,post_llm,2025,8,"{'ai_likelihood': 0.86083984375, 'text': 'Deep Hype in Artificial General Intelligence: Uncertainty, Sociotechnical Fictions and the Governance of AI Futures\n\nArtificial General Intelligence (AGI) is promoted by technology leaders and investors as a system capable of performing all human intellectual tasks, and potentially surpassing them. Despite its vague definition and uncertain feasibility, AGI has attracted major investment and political attention, fuelled by promises of civilisational transformation. This paper conceptualises AGI as sustained by deep hype: a long-term, overpromissory dynamic articulated through sociotechnical fictions that render not-yet-existing technologies desirable and urgent. The analysis highlights how uncertainty, fiction, and venture capital speculation interact to advance a cyberlibertarian and longtermist programme that sidelines democratic oversight and reframes regulation as obsolete, with critical implications for the governance of technological futures.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.0010614395141601562, 'GPT4': 0.0083465576171875, 'CLAUDE': 0.48779296875, 'GOOGLE': 0.0035552978515625, 'OPENAI_O_SERIES': 0.0003948211669921875, 'DEEPSEEK': 0.3837890625, 'GROK': 1.2516975402832031e-06, 'NOVA': 3.540515899658203e-05, 'OTHER': 0.0013513565063476562, 'HUMAN': 0.1136474609375}}"
2508.0647,review,post_llm,2025,8,"{'ai_likelihood': 0.2775065104166667, 'text': 'Generative AI and the Future of the Digital Commons: Five Open Questions and Knowledge Gaps\n\nThe rapid advancement of Generative AI (GenAI) relies heavily on the digital commons, a vast collection of free and open online content that is created, shared, and maintained by communities. However, this relationship is becoming increasingly strained due to financial burdens, decreased contributions, and misalignment between AI models and community norms. As we move deeper into the GenAI era, it is essential to examine the interdependent relationship between GenAI, the long-term sustainability of the digital commons, and the equity of current AI development practices. We highlight five critical questions that require urgent attention: 1. How can we prevent the digital commons from being threatened by undersupply as individuals cease contributing to the commons and turn to Generative AI for information? 2. How can we mitigate the risk of the open web closing due to restrictions on access to curb AI crawlers? 3. How can technical standards and legal frameworks be updated to reflect the evolving needs of organizations hosting common content? 4. What are the effects of increased synthetic content in open knowledge databases, and how can we ensure their integrity? 5. How can we account for and distribute the infrastructural and environmental costs of providing data for AI training? We emphasize the need for more responsible practices in AI development, recognizing the digital commons not only as content but as a collaborative and decentralized form of knowledge governance, which relies on the practice of ""commoning"" - making, maintaining, and protecting shared and open resources. Ultimately, our goal is to stimulate discussion and research on the intersection of Generative AI and the digital commons, with the aim of developing an ""AI commons"" and public infrastructures for AI development that support the long-term health of the digital commons.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.0106,regular,post_llm,2025,9,"{'ai_likelihood': 4.271666208902995e-06, 'text': 'When the Past Misleads: Rethinking Training Data Expansion Under Temporal Distribution Shifts\n\nPredictive models are typically trained on historical data to predict future outcomes. While it is commonly assumed that training on more historical data would improve model performance and robustness, data distribution shifts over time may undermine these benefits. This study examines how expanding historical data training windows under covariate shifts (changes in feature distributions) and concept shifts (changes in feature-outcome relationships) affects the performance and algorithmic fairness of predictive models. First, we perform a simulation study to explore scenarios with varying degrees of covariate and concept shifts in training data. Absent distribution shifts, we observe performance gains from longer training windows though they reach a plateau quickly; in the presence of concept shift, performance may actually decline. Covariate shifts alone do not significantly affect model performance, but may complicate the impact of concept shifts. In terms of fairness, models produce more biased predictions when the magnitude of concept shifts differs across sociodemographic groups; for intersectional groups, these effects are more complex and not simply additive. Second, we conduct an empirical case study of student retention prediction, a common machine learning application in education, using 12 years of student records from 23 minority-serving community colleges in the United States. We find concept shifts to be a key contributor to performance degradation when expanding the training window. Moreover, model fairness is compromised when marginalized populations have distinct data distribution shift patterns from their peers. Overall, our findings caution against conventional wisdom that ""more data is better"" and underscore the importance of using historical data judiciously, especially when it may be subject to data distribution shifts, to improve model performance and fairness.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.03071,review,post_llm,2025,9,"{'ai_likelihood': 5.169047249688043e-05, 'text': ""AI-Generated Images for representing Individuals: Navigating the Thin Line Between Care and Bias\n\nThis research discusses the figurative tensions that arise when using portraits to represent individuals behind a dataset. In the broader effort to communicate European data related to depression, the Kiel Science Communication Network (KielSCN) team attempted to engage a wider audience by combining interactive data graphics with AI-generated images of people. This article examines the project's decisions and results, reflecting on the reaction from the audience when information design incorporates figurative representations of individuals within the data."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.21899,regular,post_llm,2025,9,"{'ai_likelihood': 0.00022186173333062066, 'text': 'Opening Knowledge Gaps Drives Scientific Progress\n\nKnowledge production is often viewed as an endogenous process in which discovery arises through the recombination of existing theories, findings, and concepts. Yet given the vast space of potential recombinations, not all are equally valuable, and identifying those that may prove most generative remains challenging. We argue that a crucial form of recombination occurs when linking concepts creates knowledge gaps-empty regions in the conceptual landscape that focus scientific attention on proximal, unexplored connections and signal promising directions for future research. Using computational topology, we develop a method to systematically identify knowledge gaps in science at scale. Applying this approach to millions of articles from Microsoft Academic Graph (n = 34,363,623) over a 120-year period (1900-2020), we uncover papers that create topological gaps in concept networks, tracking how these gap-opening works reshape the scientific knowledge landscape. Our results indicate that gap-opening papers are more likely to rank among the most highly cited works (top 1-20%) compared with papers that do not introduce novel concept pairings. In contrast, papers that introduce novel combinations without opening gaps are not more likely to rank in the top 1% for citation counts, and are even less likely than baseline papers to appear in the top 5% to 20%. Our findings also suggest that gap-opening papers are more disruptive, highlighting their generative role in stimulating new directions for scientific inquiry.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.18605,review,post_llm,2025,9,"{'ai_likelihood': 0.91455078125, 'text': ""Judging Data: Critical Discourse and the Rise of Data Intellectual Property Rights in Chinese Courts\n\nThis paper uses Critical Discourse Analysis (CDA) to show how Sino-judicial activism shapes Data Intellectual Property Rights (DIPR) in China. We identify two complementary judicial discourses. Local courts (exemplified by the Zhejiang High People's Court, HCZJ) use a judicial continuation discourse that extends intellectual property norms to data disputes. The Supreme People's Court (SPC) deploys a judicial linkage discourse that aligns adjudication with state policy and administrative governance. Their interaction forms a bidirectional conceptual coupling (BCC): an inside-out projection of local reasoning and an outside-in translation of policy into doctrine. The coupling both legitimizes and constrains courts and policymakers, balancing pressure for unified market standards with safeguards against platform monopolization. Through cases such as HCZJ's Taobao v. Meijing and the SPC's Anti-Unfair Competition Interpretation, the study presents DIPR as a testbed for doctrinal innovation and institutional coordination in China's evolving digital governance."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00018596649169921875, 'GPT4': 0.0085601806640625, 'CLAUDE': 0.281494140625, 'GOOGLE': 0.0007033348083496094, 'OPENAI_O_SERIES': 0.0001277923583984375, 'DEEPSEEK': 0.412109375, 'GROK': 4.291534423828125e-06, 'NOVA': 3.874301910400391e-06, 'OTHER': 0.00015604496002197266, 'HUMAN': 0.296630859375}}"
2509.01128,review,post_llm,2025,9,"{'ai_likelihood': 1.9901328616672094e-05, 'text': 'Assessing prompting frameworks for enhancing literature reviews among university students using ChatGPT\n\nWriting literature reviews is a common component of university curricula, yet it often poses challenges for students. Since generative artificial intelligence (GenAI) tools have been made publicly accessible, students have been employing them for their academic writing tasks. However, there is limited evidence of structured training on how to effectively use these GenAI tools to support students in writing literature reviews. In this study, we explore how university students use one of the most popular GenAI tools, ChatGPT, to write literature reviews and how prompting frameworks can enhance their output. To this aim, prompts and literature reviews written by a group of university students were collected before and after they had been introduced to three prompting frameworks, namely CO-STAR, POSE, and Sandwich. The results indicate that after being exposed to these prompting frameworks, the students demonstrated improved prompting behaviour, resulting in more effective prompts and higher quality literature reviews. However, it was also found that the students did not fully utilise all the elements in the prompting frameworks, and aspects such as originality, critical analysis, and depth in their reviews remain areas for improvement. The study, therefore, raises important questions about the significance of utilising prompting frameworks in their entirety to maximise the quality of outcomes, as well as the extent of prior writing experience students should have before leveraging GenAI in the process of writing literature reviews. These findings are of interest for educators considering the integration of GenAI into academic writing tasks such as literature reviews or evaluating whether to permit students to use these tools.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.25701,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'What Drives Paper Acceptance? A Process-Centric Analysis of Modern Peer Review\n\nPeer review is the primary mechanism for evaluating scientific contributions, yet prior studies have mostly examined paper features or external metadata in isolation. The emergence of open platforms such as OpenReview has transformed peer review into a transparent and interactive process, recording not only scores and comments but also rebuttals, reviewer-author exchanges, reviewer disagreements, and meta-reviewer decisions. This provides unprecedented process-level data for understanding how modern peer review operates. In this paper, we present a large-scale empirical study of ICLR 2017-2025, encompassing over 28,000 submissions. Our analysis integrates four complementary dimensions, including the structure and language quality of papers (e.g., section patterns, figure/table ratios, clarity), submission strategies and external metadata (e.g., timing, arXiv posting, author count), the dynamics of author-reviewer interactions (e.g., rebuttal frequency, responsiveness), and the patterns of reviewer disagreement and meta-review mediation (e.g., score variance, confidence weighting). Our results show that factors beyond scientific novelty significantly shape acceptance outcomes. In particular, the rebuttal stage emerges as a decisive phase: timely, substantive, and interactive author-reviewer communication strongly increases the likelihood of acceptance, often outweighing initial reviewer skepticism. Alongside this, clearer writing, balanced visual presentation, earlier submission, and effective resolution of reviewer disagreement also correlate with higher acceptance probabilities. Based on these findings, we propose data-driven guidelines for authors, reviewers, and meta-reviewers to enhance transparency and fairness in peer review. Our study demonstrates that process-centric signals are essential for understanding and improving modern peer review.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00029730796813964844, 'GPT4': 0.0114288330078125, 'CLAUDE': 0.13720703125, 'GOOGLE': 0.00046133995056152344, 'OPENAI_O_SERIES': 7.587671279907227e-05, 'DEEPSEEK': 0.8505859375, 'GROK': 2.384185791015625e-07, 'NOVA': 2.384185791015625e-07, 'OTHER': 1.341104507446289e-05, 'HUMAN': 9.179115295410156e-06}}"
2509.17878,review,post_llm,2025,9,"{'ai_likelihood': 9.602970547146268e-06, 'text': ""AI, Digital Platforms, and the New Systemic Risk\n\nAs artificial intelligence (AI) becomes increasingly embedded in digital, social, and institutional infrastructures, and AI and platforms are merged into hybrid structures, systemic risk has emerged as a critical but undertheorized challenge. In this paper, we develop a rigorous framework for understanding systemic risk in AI, platform, and hybrid system governance, drawing on insights from finance, complex systems theory, climate change, and cybersecurity - domains where systemic risk has already shaped regulatory responses. We argue that recent legislation, including the EU's AI Act and Digital Services Act (DSA), invokes systemic risk but relies on narrow or ambiguous characterizations of this notion, sometimes reducing this risk to specific capabilities present in frontier AI models, or to harms occurring in economic market settings. The DSA, we show, actually does a better job at identifying systemic risk than the more recent AI Act. Our framework highlights novel risk pathways, including the possibility of systemic failures arising from the interaction of multiple AI agents. We identify four levels of AI-related systemic risk and emphasize that discrimination at scale and systematic hallucinations, despite their capacity to destabilize institutions and fundamental rights, may not fall under current legal definitions, given the AI Act's focus on frontier model capabilities. We then test the DSA, the AI Act, and our own framework on five key examples, and propose reforms that broaden systemic risk assessments, strengthen coordination between regulatory regimes, and explicitly incorporate collective harms."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.08836,review,post_llm,2025,9,"{'ai_likelihood': 8.609559800889757e-07, 'text': ""De spanning tussen het non-discriminatierecht en het gegevensbeschermingsrecht: heeft de AVG een nieuwe uitzondering nodig om discriminatie door kunstmatige intelligentie tegen te gaan?\n\nOrganisations can use artificial intelligence to make decisions about people for a variety of reasons, for instance, to select the best candidates from many job applications. However, AI systems can have discriminatory effects when used for decision-making. To illustrate, an AI system could reject applications of people with a certain ethnicity, while the organisation did not plan such ethnicity discrimination. But in Europe, an organisation runs into a problem when it wants to assess whether its AI system accidentally discriminates based on ethnicity: the organisation may not know the applicants' ethnicity. In principle, the GDPR bans the use of certain 'special categories of data' (sometimes called 'sensitive data'), which include data on ethnicity, religion, and sexual preference. The proposal for an AI Act of the European Commission includes a provision that would enable organisations to use special categories of data for auditing their AI systems. This paper asks whether the GDPR's rules on special categories of personal data hinder the prevention of AI-driven discrimination. We argue that the GDPR does prohibit such use of special category data in many circumstances. We also map out the arguments for and against creating an exception to the GDPR's ban on using special categories of personal data, to enable preventing discrimination by AI systems. The paper discusses European law, but the paper can be relevant outside Europe too, as many policymakers in the world grapple with the tension between privacy and non-discrimination policy."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.15545,regular,post_llm,2025,9,"{'ai_likelihood': 0.97802734375, 'text': 'From Service-Oriented Computing to Metaverse Services: A Framework for Inclusive and Immersive Learning for Neurodivergent Students\n\nThe metaverse offers immersive and adaptive learning environments for neurodivergent students to thrive and reach their full potential. In this paper, we propose a generic framework that leverages metaverse services as an evolution beyond traditional service-oriented computing, enabling more interactive, personalized, and engaging educational experiences. By integrating AI-driven adaptability, multimodal interaction, and privacy-first service design, the framework ensures that learning remains accessible, inclusive, and secure. Additionally, we explore the challenges associated with scalability, data privacy, and ethical considerations while highlighting opportunities for fostering safe and student-centered virtual spaces. Our analysis underscores the potential of metaverse-based learning to bridge accessibility gaps, support social-emotional development, and empower neurodivergent learners in both digital and real-world settings. We also provide recommendations and policy considerations for creating a secure, inclusive, and scalable metaverse learn-ing ecosystem for neurodivergent students.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00022733211517333984, 'GPT4': 0.99755859375, 'CLAUDE': 0.0011472702026367188, 'GOOGLE': 0.0005884170532226562, 'OPENAI_O_SERIES': 6.318092346191406e-06, 'DEEPSEEK': 6.812810897827148e-05, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 7.748603820800781e-07, 'HUMAN': 0.0005331039428710938}}"
2509.11371,regular,post_llm,2025,9,"{'ai_likelihood': 0.9755859375, 'text': 'The Lovelace Test of Intelligence: Can Humans Recognise and Esteem AI-Generated Art?\n\nThis study aims to evaluate machine intelligence through artistic creativity by employing a modified version of the Turing Test inspired by Lady Lovelace. It investigates two hypotheses: whether human judges can reliably distinguish AI-generated artworks from human-created ones and whether AI-generated art achieves comparable aesthetic value to human-crafted works. The research contributes to understanding machine creativity and its implications for cognitive science and AI technology. Participants with educational backgrounds in cognitive and computer science play the role of interrogators and evaluated whether a set of paintings was AI-generated or human-created. Here, we utilise parallel-paired and viva voce versions of the Turing Test. Additionally, aesthetic evaluations are collected to compare the perceived quality of AI-generated images against human-created art. This dual-method approach allows us to examine human judgment under different testing conditions. We find that participants struggle to distinguish between AI-generated and human-created artworks reliably, performing no better than chance under certain conditions. Furthermore, AI-generated art is rated as aesthetically as human-crafted works. Our findings challenge traditional assumptions about human creativity and demonstrate that AI systems can generate outputs that resonate with human sensibilities while meeting the criteria of creative intelligence. This study advances the understanding of machine creativity by combining elements of the Turing and Lovelace Tests. Unlike prior studies focused on laypeople or artists, this research examines participants with domain expertise. It also provides a comparative analysis of two distinct testing methodologies (parallel-paired and viva voce) offering new insights into the evaluation of machine intelligence.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0010128021240234375, 'GPT4': 0.1341552734375, 'CLAUDE': 0.59228515625, 'GOOGLE': 0.200439453125, 'OPENAI_O_SERIES': 0.01248931884765625, 'DEEPSEEK': 0.002216339111328125, 'GROK': 2.682209014892578e-06, 'NOVA': 6.794929504394531e-06, 'OTHER': 7.152557373046875e-05, 'HUMAN': 0.05743408203125}}"
2509.10083,regular,post_llm,2025,9,"{'ai_likelihood': 0.0005647871229383681, 'text': 'The Hierarchical Morphotope Classification: A Theory-Driven Framework for Large-Scale Analysis of Built Form\n\nBuilt environment, formed of a plethora of patterns of building, streets, and plots, has a profound impact on how cities are perceived and function. While various methods exist to classify urban patterns, they often lack a strong theoretical foundation, are not scalable beyond a local level, or sacrifice detail for broader application. This paper introduces the Hierarchical Morphotope Classification (HiMoC), a novel, theory-driven, and computationally scalable method of classification of built form. HiMoC operationalises the idea of a morphotope - the smallest locality with a distinctive character - using a bespoke regionalisation method SA3 (Spatial Agglomerative Adaptive Aggregation), to delineate contiguous, morphologically distinct localities. These are further organised into a hierarchical taxonomic tree reflecting their dissimilarity based on morphometric profile derived from buildings and streets retrieved from open data, allowing flexible, interpretable classification of built fabric, that can be applied beyond a scale of a single country. The method is tested on a subset of countries of Central Europe, grouping over 90 million building footprints into over 500,000 morphotopes. The method extends the capabilities of available morphometric analyses, while offering a complementary perspective to existing large scale data products, which are focusing primarily on land use or use conceptual definition of urban fabric types. This theory-grounded, reproducible, unsupervised and scalable method facilitates a nuanced understanding of urban structure, with broad applications in urban planning, environmental analysis, and socio-spatial studies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.23851,regular,post_llm,2025,9,"{'ai_likelihood': 7.616149054633247e-07, 'text': 'Price discrimination, algorithmic decision-making, and European non-discrimination law\n\nOur society can benefit immensely from algorithmic decision-making and similar types of artificial intelligence. But algorithmic decision-making can also have discriminatory effects. This paper examines that problem, using online price differentiation as an example of algorithmic decision-making. With online price differentiation, a company charges different people different prices for identical products, based on information the company has about those people. The main question in this paper is: to what extent can non-discrimination law protect people against online price differentiation? The paper shows that online price differentiation and algorithmic decision-making could lead to indirect discrimination, for instance harming people with a certain ethnicity. Indirect discrimination occurs when a practice is neutral at first glance, but ends up discriminating against people with a protected characteristic, such as ethnicity. In principle, non-discrimination law prohibits indirect discrimination. The paper also shows, however, that non-discrimination law has flaws when applied to algorithmic decision-making. For instance, algorithmic discrimination can remain hidden: people may not realise that they are being discriminated against. And many types of unfair - some might say discriminatory - algorithmic decisions are outside the scope of current non-discrimination law.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.06126,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""The impact of gamification on learning outcomes: experiences from a Biomedical Engineering course\n\nThis study examines the integration of digital tools in project-based learning within a Biomedical Engineering course to enhance collaboration, transparency, and assessment fairness. Building on prior pilot experiences, we implemented a structured learning environment that combined experiment tracking, real-time collaboration, and peer-assessment practices. The intervention was deployed across two consecutive academic years, involving master's-level students in Biomedical Image Processing. Data were collected through project outcomes, peer-assessment rubrics, and student surveys. Results show that the integration of digital platforms supported accountability, improved the quality of collaborative work, and fostered greater equity in the evaluation process. Students highlighted increased engagement, enhanced teamwork, and clearer criteria for performance assessment. Faculty reported more efficient monitoring of progress and improved feedback practices. Despite challenges such as technical adoption and the need for instructor guidance, the study demonstrates the potential of structured tool integration to support active and transparent learning environments. Findings contribute to the broader discourse on digital pedagogy, offering a replicable model for higher education contexts in science and technology."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.3332576751708984e-05, 'GPT4': 0.923828125, 'CLAUDE': 0.00418853759765625, 'GOOGLE': 9.655952453613281e-06, 'OPENAI_O_SERIES': 0.000762939453125, 'DEEPSEEK': 0.07122802734375, 'GROK': 1.1920928955078125e-07, 'NOVA': 5.364418029785156e-07, 'OTHER': 7.3909759521484375e-06, 'HUMAN': 2.980232238769531e-07}}"
2509.11913,review,post_llm,2025,9,"{'ai_likelihood': 2.1192762586805556e-06, 'text': ""AI Wellbeing\n\nUnder what conditions would an artificially intelligent system have wellbeing? Despite its obvious bearing on the ethics of human interactions with artificial systems, this question has received little attention. Because all major theories of wellbeing hold that an individual's welfare level is partially determined by their mental life, we begin by considering whether artificial systems have mental states. We show that a wide range of theories of mental states, when combined with leading theories of wellbeing, predict that certain existing artificial systems have wellbeing. While we do not claim to demonstrate conclusively that AI systems have wellbeing, we argue that our metaphysical and moral uncertainty about AI wellbeing requires us dramatically to reassess our relationship with the intelligent systems we create."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.12503,review,post_llm,2025,9,"{'ai_likelihood': 0.98291015625, 'text': ""Qualitative Research in an Era of AI: A Pragmatic Approach to Data Analysis, Workflow, and Computation\n\nComputational developments--particularly artificial intelligence--are reshaping social scientific research and raise new questions for in-depth methods such as ethnography and qualitative interviewing. Building on classic debates about computers in qualitative data analysis (QDA), we revisit possibilities and dangers in an era of automation, Large Language Model (LLM) chatbots, and 'big data.' We introduce a typology of contemporary approaches to using computers in qualitative research: streamlining workflows, scaling up projects, hybrid analytical methods, the sociology of computation, and technological rejection. Drawing from scaled team ethnographies and solo research integrating computational social science (CSS), we describe methodological choices across study lifecycles, from literature reviews through data collection, coding, text retrieval, and representation. We argue that new technologies hold potential to address longstanding methodological challenges when deployed with knowledge, purpose, and ethical commitment. Yet a pragmatic approach--moving beyond technological optimism and dismissal--is essential given rapidly changing tools that are both generative and dangerous. Computation now saturates research infrastructure, from algorithmic literature searches to scholarly metrics, making computational literacy a core methodological competence in and beyond sociology. We conclude that when used carefully and transparently, contemporary computational tools can meaningfully expand--rather than displace--the irreducible insights of qualitative research."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.45193862915039e-05, 'GPT4': 0.0027675628662109375, 'CLAUDE': 0.85888671875, 'GOOGLE': 0.0007061958312988281, 'OPENAI_O_SERIES': 0.00016129016876220703, 'DEEPSEEK': 0.06292724609375, 'GROK': 4.172325134277344e-07, 'NOVA': 1.3113021850585938e-06, 'OTHER': 2.7000904083251953e-05, 'HUMAN': 0.0743408203125}}"
2509.1373,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Perspectives and potential issues in using artificial intelligence for computer science education\n\nSince its launch in late 2022, ChatGPT has ignited widespread interest in Large Language Models (LLMs) and broader Artificial Intelligence (AI) solutions. As this new wave of AI permeates various sectors of society, we are continually uncovering both the potential and the limitations of existing AI tools.\n  The need for adjustment is particularly significant in Computer Science Education (CSEd), as LLMs have evolved into core coding tools themselves, blurring the line between programming aids and intelligent systems, and reinforcing CSEd's role as a nexus of technology and pedagogy. The findings of our survey indicate that while AI technologies hold potential for enhancing learning experiences, such as through personalized learning paths, intelligent tutoring systems, and automated assessments, there are also emerging concerns. These include the risk of over-reliance on technology, the potential erosion of fundamental cognitive skills, and the challenge of maintaining equitable access to such innovations.\n  Recent advancements represent a paradigm shift, transforming not only the content we teach but also the methods by which teaching and learning take place. Rather than placing the burden of adapting to AI technologies on students, educational institutions must take a proactive role in verifying, integrating, and applying new pedagogical approaches. Such efforts can help ensure that both educators and learners are equipped with the skills needed to navigate the evolving educational landscape shaped by these technological innovations."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00011730194091796875, 'GPT4': 0.986328125, 'CLAUDE': 2.5033950805664062e-05, 'GOOGLE': 0.0133209228515625, 'OPENAI_O_SERIES': 0.00010347366333007812, 'DEEPSEEK': 1.1920928955078125e-06, 'GROK': 5.960464477539063e-08, 'NOVA': 5.364418029785156e-07, 'OTHER': 1.7285346984863281e-06, 'HUMAN': 0.00015878677368164062}}"
2509.21858,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Malaysia's AI-Driven Education Landscape: Policies, Applications, and Comparative Insights for a Digital Future\n\nArtificial Intelligence (AI) is transforming education globally, and Malaysia is leveraging this potential through strategic policies to enhance learning and prepare students for a digital future. This article explores Malaysia's AI-driven education landscape, emphasising the National Artificial Intelligence Roadmap 2021-2025 and the Digital Education Policy. Employing a policy-driven analysis, it maps AI applications in pedagogy, curriculum design, administration, and teacher training across primary to tertiary levels. The study evaluates national strategies, identifies challenges like digital divides and ethical concerns, and conducts a comparative analysis with the United Kingdom, the United States, China, and India to draw best practices in AI policy and digital transformation. Findings highlight Malaysia's progress in AI literacy and personalised learning, alongside gaps in rural infrastructure and teacher readiness. Recommendations include strengthening governance, investing in equitable infrastructure, and fostering public-private partnerships. Targeting researchers, policymakers, and educators, this study informs Malaysia's path to becoming a regional leader in AI-driven education and contributes to global comparative education discourse."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.980232238769531e-07, 'GPT4': 0.0016937255859375, 'CLAUDE': 2.4437904357910156e-06, 'GOOGLE': 2.2649765014648438e-06, 'OPENAI_O_SERIES': 4.291534423828125e-06, 'DEEPSEEK': 0.99853515625, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539062e-07, 'HUMAN': 0.0}}"
2509.03269,regular,post_llm,2025,9,"{'ai_likelihood': 1.1788474188910592e-05, 'text': ""Bridging Gaps Between Student and Expert Evaluations of AI-Generated Programming Hints\n\nGenerative AI has the potential to enhance education by providing personalized feedback to students at scale. Recent work has proposed techniques to improve AI-generated programming hints and has evaluated their performance based on expert-designed rubrics or student ratings. However, it remains unclear how the rubrics used to design these techniques align with students' perceived helpfulness of hints. In this paper, we systematically study the mismatches in perceived hint quality from students' and experts' perspectives based on the deployment of AI-generated hints in a Python programming course. We analyze scenarios with discrepancies between student and expert evaluations, in particular, where experts rated a hint as high-quality while the student found it unhelpful. We identify key reasons for these discrepancies and classify them into categories, such as hints not accounting for the student's main concern or not considering previous help requests. Finally, we propose and discuss preliminary results on potential methods to bridge these gaps, first by extending the expert-designed quality rubric and then by adapting the hint generation process, e.g., incorporating the student's comments or history. These efforts contribute toward scalable, personalized, and pedagogically sound AI-assisted feedback systems, which are particularly important for high-enrollment educational settings."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.14554,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Generative Large Language Models for Knowledge Representation: A Systematic Review of Concept Map Generation\n\nThe rise of generative large language models (LLMs) has opened new opportunities for automating knowledge representation through concept maps, a long-standing pedagogical tool valued for fostering meaningful learning and higher-order thinking. Traditional construction of concept maps is labor-intensive, requiring significant expertise and time, limiting their scalability in education. This review systematically synthesizes the emerging body of research on LLM-enabled concept map generation, focusing on two guiding questions: (a) What methods and technical features of LLMs are employed to construct concept maps? (b) What empirical evidence exists to validate their educational utility? Through a comprehensive search across major databases and AI-in-education conference proceedings, 28 studies meeting rigorous inclusion criteria were analyzed using thematic synthesis. Findings reveal six major methodological categories: human-in-the-loop systems, weakly supervised learning models, fine-tuned domain-specific LLMs, pre-trained LLMs with prompt engineering, hybrid systems integrating knowledge bases, and modular frameworks combining symbolic and statistical tools. Validation strategies ranged from quantitative metrics (precision, recall, F1-score, semantic similarity) to qualitative evaluations (expert review, learner feedback). Results indicate LLM-generated maps hold promise for scalable, adaptive, and pedagogically relevant knowledge visualization, though challenges remain regarding validity, interpretability, multilingual adaptability, and classroom integration. Future research should prioritize interdisciplinary co-design, empirical classroom trials, and alignment with instructional practices to realize their full educational potential.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 8.64267349243164e-06, 'CLAUDE': 1.1265277862548828e-05, 'GOOGLE': 5.960464477539063e-08, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.04887,regular,post_llm,2025,9,"{'ai_likelihood': 3.884236017862956e-05, 'text': ""RINSER: Accurate API Prediction Using Masked Language Models\n\nMalware authors commonly use obfuscation to hide API identities in binary files, making analysis difficult and time-consuming for a human expert to understand the behavior and intent of the program. Automatic API prediction tools are necessary to efficiently analyze unknown binaries, facilitating rapid malware triage while reducing the workload on human analysts. In this paper, we present RINSER (AccuRate API predictioN using maSked languagE model leaRning), an automated framework for predicting Windows API (WinAPI) function names. RINSER introduces the novel concept of API codeprints, a set of API-relevant assembly instructions, and supports x86 PE binaries. RINSER relies on BERT's masked language model (LM) to predict API names at scale, achieving 85.77% accuracy for normal binaries and 82.88% accuracy for stripped binaries. We evaluate RINSER on a large dataset of 4.7M API codeprints from 11,098 malware binaries, covering 4,123 unique Windows APIs, making it the largest publicly available dataset of this type. RINSER successfully discovered 65 obfuscated Windows APIs related to C2 communication, spying, and evasion in our dataset, which the commercial disassembler IDA failed to identify. Furthermore, we compared RINSER against three state-of-the-art approaches, showing over 20% higher prediction accuracy. We also demonstrated RINSER's resilience to adversarial attacks, including instruction randomization and code displacement, with a performance drop of no more than 3%."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.12415,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Prompt Commons: Collective Prompting as Governance for Urban AI\n\nLarge Language Models (LLMs) are entering urban governance, yet their outputs are highly sensitive to prompts that carry value judgments. We propose Prompt Commons - a versioned, community-maintained repository of prompts with governance metadata, licensing, and moderation - to steer model behaviour toward pluralism. Using a Montreal dataset (443 human prompts; 3,317 after augmentation), we pilot three governance states (open, curated, veto-enabled). On a contested policy benchmark, a single-author prompt yields 24 percent neutral outcomes; commons-governed prompts raise neutrality to 48-52 percent while retaining decisiveness where appropriate. In a synthetic incident log, a veto-enabled regime reduces time-to-remediation for harmful outputs from 30.5 +/- 8.9 hours (open) to 5.6 +/- 1.5 hours. We outline licensing (CC BY/BY-SA for prompts with optional OpenRAIL-style restrictions for artefacts), auditable moderation, and safeguards against dominance capture. Prompt governance offers a practical lever for cities to align AI with local values and accountability.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.7418136596679688e-06, 'GPT4': 0.0001876354217529297, 'CLAUDE': 0.030181884765625, 'GOOGLE': 6.854534149169922e-06, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.96875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 7.748603820800781e-07, 'HUMAN': 0.0008783340454101562}}"
2509.07375,review,post_llm,2025,9,"{'ai_likelihood': 3.066327836778429e-05, 'text': 'Towards Post-mortem Data Management Principles for Generative AI\n\nFoundation models, large language models (LLMs), and agentic AI systems rely heavily on vast corpora of user data. The use of such data for training has raised persistent concerns around ownership, copyright, and potential harms. In this work, we explore a related but less examined dimension: the ownership rights of data belonging to deceased individuals. We examine the current landscape of post-mortem data management and privacy rights as defined by the privacy policies of major technology companies and regulations such as the EU AI Act. Based on this analysis, we propose three post-mortem data management principles to guide the protection of deceased individuals data rights. Finally, we discuss directions for future work and offer recommendations for policymakers and privacy practitioners on deploying these principles alongside technological solutions to operationalize and audit them in practice.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.01444,regular,post_llm,2025,9,"{'ai_likelihood': 1.4901161193847656e-05, 'text': 'Strata-Sword: A Hierarchical Safety Evaluation towards LLMs based on Reasoning Complexity of Jailbreak Instructions\n\nLarge language models (LLMs) have gained widespread recognition for their superior comprehension and have been deployed across numerous domains. Building on Chain-of-Thought (CoT) ideology, Large Reasoning models (LRMs) further exhibit strong reasoning skills, enabling them to infer user intent more accurately and respond appropriately. However, both LLMs and LRMs face the potential safety risks under jailbreak attacks, which raise concerns about their safety capabilities. Current safety evaluation methods often focus on the content dimensions, or simply aggregate different attack methods, lacking consideration of the complexity. In fact, instructions of different complexity can reflect the different safety capabilities of the model: simple instructions can reflect the basic values of the model, while complex instructions can reflect the model\'s ability to deal with deeper safety risks. Therefore, a comprehensive benchmark needs to be established to evaluate the safety performance of the model in the face of instructions of varying complexity, which can provide a better understanding of the safety boundaries of the LLMs. Thus, this paper first quantifies ""Reasoning Complexity"" as an evaluable safety dimension and categorizes 15 jailbreak attack methods into three different levels according to the reasoning complexity, establishing a hierarchical Chinese-English jailbreak safety benchmark for systematically evaluating the safety performance of LLMs. Meanwhile, to fully utilize unique language characteristics, we first propose some Chinese jailbreak attack methods, including the Chinese Character Disassembly attack, Lantern Riddle attack, and Acrostic Poem attack. A series of experiments indicate that current LLMs and LRMs show different safety boundaries under different reasoning complexity, which provides a new perspective to develop safer LLMs and LRMs.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.09921,review,post_llm,2025,9,"{'ai_likelihood': 0.00021484163072374134, 'text': ""A Taxonomy of Response Strategies to Toxic Online Content: Evaluating the Evidence\n\nToxic Online Content (TOC) includes messages on digital platforms that are harmful, hostile, or damaging to constructive public discourse. Individuals, organizations, and LLMs respond to TOC through counterspeech or counternarrative initiatives. There is a wide variation in their goals, terminology, response strategies, and methods of evaluating impact. This paper identifies a taxonomy of online response strategies, which we call Online Discourse Engagement (ODE), to include any type of online speech to build healthier online public discourse. The literature on ODE makes contradictory assumptions about ODE goals and rarely distinguishes between them or rigorously evaluates their effectiveness. This paper categorizes 25 distinct ODE strategies, from humor and distraction to empathy, solidarity, and fact-based rebuttals, and groups these into a taxonomy of five response categories: defusing and distracting, engaging the speaker's perspective, identifying shared values, upstanding for victims, and information and fact-building. The paper then systematically reviews the evidence base for each of these categories. By clarifying definitions, cataloging response strategies, and providing a meta-analysis of research papers on these strategies, this article aims to bring coherence to the study of ODE and to strengthen evidence-informed approaches for fostering constructive ODE."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.24065,regular,post_llm,2025,9,"{'ai_likelihood': 0.9951171875, 'text': 'AI Safety, Alignment, and Ethics (AI SAE)\n\nThis paper grounds ethics in evolutionary biology, viewing moral norms as adaptive mechanisms that render cooperation fitness-viable under selection pressure. Current alignment approaches add ethics post hoc, treating it as an external constraint rather than embedding it as an evolutionary strategy for cooperation. The central question is whether normative architectures can be embedded directly into AI systems to sustain human--AI cooperation (symbiosis) as capabilities scale. To address this, I propose a governance--embedding--representation pipeline linking moral representation learning to system-level design and institutional governance, treating alignment as a multi-level problem spanning cognition, optimization, and oversight. I formalize moral norm representation through the moral problem space, a learnable subspace in neural representations where cooperative norms can be encoded and causally manipulated. Using sparse autoencoders, activation steering, and causal interventions, I outline a research program for engineering moral representations and embedding them into the full semantic space -- treating competing theories of morality as empirical hypotheses about representation geometry rather than philosophical positions. Governance principles leverage these learned moral representations to regulate how cooperative behaviors evolve within the AI ecosystem. Through replicator dynamics and multi-agent game theory, I model how internal representational features can shape population-level incentives by motivating the design of sanctions and subsidies structured to yield decentralized normative institutions.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.58306884765625e-06, 'GPT4': 0.00992584228515625, 'CLAUDE': 0.357666015625, 'GOOGLE': 5.334615707397461e-05, 'OPENAI_O_SERIES': 4.839897155761719e-05, 'DEEPSEEK': 0.5966796875, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 0.035430908203125}}"
2509.08927,regular,post_llm,2025,9,"{'ai_likelihood': 1.5430980258517797e-05, 'text': 'AuraSight: Generating Realistic Social Media Data\n\nThis document details the narrative and technical design behind the process of generating a quasi-realistic set X data for a fictional multi-day pop culture episode (AuraSight). Social media post simulation is essential towards creating realistic training scenarios for understanding emergent network behavior that formed from known sets of agents. Our social media post generation pipeline uses the AESOP-SynSM engine, which employs a hybrid approach of agent-based and generative artificial intelligence techniques. We explicate choices in scenario setup and summarize the fictional groups involved, before moving on to the operationalization of these actors and their interactions within the SynSM engine. We also briefly illustrate some outputs generated and discuss the utility of such simulated data and potential future improvements.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.08853,review,post_llm,2025,9,"{'ai_likelihood': 0.0010808308919270835, 'text': ""POW: Political Overton Windows of Large Language Models\n\nPolitical bias in Large Language Models (LLMs) presents a growing concern for the responsible deployment of AI systems. Traditional audits often attempt to locate a model's political position as a point estimate, masking the broader set of ideological boundaries that shape what a model is willing or unwilling to say. In this paper, we draw upon the concept of the Overton Window as a framework for mapping these boundaries: the range of political views that a given LLM will espouse, remain neutral on, or refuse to endorse. To uncover these windows, we applied an auditing-based methodology, called PRISM, that probes LLMs through task-driven prompts designed to elicit political stances indirectly. Using the Political Compass Test, we evaluated twenty-eight LLMs from eight providers to reveal their distinct Overton Windows. While many models default to economically left and socially liberal positions, we show that their willingness to express or reject certain positions varies considerably, where DeepSeek models tend to be very restrictive in what they will discuss and Gemini models tend to be most expansive. Our findings demonstrate that Overton Windows offer a richer, more nuanced view of political bias in LLMs and provide a new lens for auditing their normative boundaries."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.1334,review,post_llm,2025,9,"{'ai_likelihood': 4.635916815863716e-06, 'text': 'Defining a classification system for augmentation technology in socio-technical terms\n\nThis short paper provides a means to classify augmentation technologies to reconceptualize them as sociotechnical, discursive and rhetorical phenomena, rather than only through technological classifications. It identifies a set of value systems that constitute augmentation technologies within discourses, namely, the intent to enhance, automate, and build efficiency. This short paper makes a contribution to digital literacy surrounding augmentation technology emergence, as well as the more specific area of AI literacy, which can help identify unintended consequences implied at the design stages of these technologies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.16218,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""An Automated Framework for Assessing Electric Vehicle Charging Impacts on a Campus Distribution Grid\n\nThis paper introduces a unified and automated framework designed to dynamically assess the impact of electric vehicle (EV) charging on distribution feeders and transformers at California State University, Northridge (CSUN). As EV adoption accelerates, the resulting increase in charging demand imposes additional stress on local power distribution systems. Moreover, the evolving nature of EV load profiles throughout the day necessitates detailed temporal analysis to identify peak loading conditions, anticipate worst-case scenarios, and plan timely infrastructure upgrades. Our main contribution is the development of a flexible testbed that integrates Julia, a high-performance programming language for technical computing, with PowerWorld Simulator via the EasySimauto.jl package. This integration enables seamless modeling, simulation, and analysis of EV charging load profiles and their implications for campus grid infrastructure. The framework leverages a real-world dataset collected from CSUN's EV charging stations, consisting of 15-minute interval measurements over the course of one year. By coupling high-resolution data with dynamic simulations, the proposed system offers a valuable tool for evaluating transformer loading, feeder utilization, and overall system stress. The results support data-driven decision-making for EV infrastructure deployment, load forecasting, and energy management strategies. In addition, the framework allows for scenario-based studies to explore the impact of future increases in EV penetration or changes in charging behavior. Its modular architecture also makes it adaptable to other campus or urban distribution systems facing similar electrification challenges."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.649162292480469e-05, 'GPT4': 0.9111328125, 'CLAUDE': 0.078857421875, 'GOOGLE': 0.004413604736328125, 'OPENAI_O_SERIES': 0.005405426025390625, 'DEEPSEEK': 0.0001779794692993164, 'GROK': 5.960464477539063e-08, 'NOVA': 1.1920928955078125e-07, 'OTHER': 4.649162292480469e-06, 'HUMAN': 1.0371208190917969e-05}}"
2509.08837,review,post_llm,2025,9,"{'ai_likelihood': 2.1523899502224394e-06, 'text': 'Protected Grounds and the System of Non-Discrimination Law in the Context of Algorithmic Decision-Making and Artificial Intelligence\n\nAlgorithmic decision-making and similar types of artificial intelligence (AI) may lead to improvements in all sectors of society, but can also have discriminatory effects. While current non-discrimination law offers people some protection, algorithmic decision-making presents the law with several challenges. For instance, algorithms can generate new categories of people based on seemingly innocuous characteristics, such as web browser preference or apartment number, or more complicated categories combining many data points. Such new types of differentiation could evade non-discrimination law, as browser type and house number are not protected characteristics, but such differentiation could still be unfair, for instance if it reinforces social inequality.\n  This paper explores which system of non-discrimination law can best be applied to algorithmic decision-making, considering that algorithms can differentiate on the basis of characteristics that do not correlate with protected grounds of discrimination such as ethnicity or gender. The paper analyses the current loopholes in the protection offered by non-discrimination law and explores the best way for lawmakers to approach algorithmic differentiation. While we focus on Europe, the conceptual and theoretical focus of the paper can make it useful for scholars and policymakers from other regions too, as they encounter similar problems with algorithmic decision-making.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.18768,review,post_llm,2025,9,"{'ai_likelihood': 7.28501213921441e-07, 'text': 'Purer than pure: how purity reshapes the upstream materiality of the semiconductor industry\n\nGrowing attention is given to the environmental impacts of the digital sector, exacerbated by the increase of digital products and services in our globalized societies. The materiality of the digital sector is often presented through the environmental impacts of mining activities to point out that digitization does not mean dematerialization. Despite its importance, such a narrative is often restricted to a few minerals (e.g., cobalt, lithium) that have become the symbols of extractive industries. In this paper, we further explore the materiality of the digital sector with an approach based on the diversity of elements and their purity requirements in the semiconductor industry. Semiconductors are responsible for manufacturing the key building blocks of the digital sector, i.e., microchips. Given that the need for ultra-high purity materials is very specific to the semiconductor industry, a few companies around the world have been studied, revealing new critical actors in complex supply chains. This highlights strong dependencies towards other industrial sectors with mass production and the need for a deeper investigation of interactions with the chemical industry, complementary to the mining industry.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.2005,review,post_llm,2025,9,"{'ai_likelihood': 0.9951171875, 'text': ""The three main doctrines on the future of AI\n\nThis paper develops a taxonomy of expert perspectives on the risks and likely consequences of artificial intelligence, with particular focus on Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). Drawing from primary sources, we identify three predominant doctrines: (1) The dominance doctrine, which predicts that the first actor to create sufficiently advanced AI will attain overwhelming strategic superiority sufficient to cheaply neutralize its opponents' defenses; (2) The extinction doctrine, which anticipates that humanity will likely lose control of ASI, leading to the extinction of the human species or its permanent disempowerment; (3) The replacement doctrine, which forecasts that AI will automate a large share of tasks currently performed by humans, but will not be so transformative as to fundamentally reshape or bring an end to human civilization. We examine the assumptions and arguments underlying each doctrine, including expectations around the pace of AI progress and the feasibility of maintaining advanced AI under human control. While the boundaries between doctrines are sometimes porous and many experts hedge across them, this taxonomy clarifies the core axes of disagreement over the anticipated scale and nature of the consequences of AI development."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.0371208190917969e-05, 'GPT4': 5.614757537841797e-05, 'CLAUDE': 0.99951171875, 'GOOGLE': 6.437301635742188e-05, 'OPENAI_O_SERIES': 1.4662742614746094e-05, 'DEEPSEEK': 0.00019252300262451172, 'GROK': 2.980232238769531e-07, 'NOVA': 3.5762786865234375e-07, 'OTHER': 1.2159347534179688e-05, 'HUMAN': 0.0003504753112792969}}"
2509.14508,regular,post_llm,2025,9,"{'ai_likelihood': 0.0001747078365749783, 'text': ""Modeling User Redemption Behavior in Complex Incentive Digital Environment: An Empirical Study Using Large-Scale Transactional Data\n\nThe digital economy implements complex incentive systems to retain users through point redemption. Understanding user behavior in such complex incentive structures presents a fundamental challenge, especially in estimating the value of these digital assets against traditional money. This study tackles this question by analyzing large-scale, real-world transaction data from a popular personal finance application that captures both monetary spending and point-based transactions across Japan's deeply integrated loyalty networks. We find that point usage is not random but is systematically linked to demographics, with older users tending to convert points into financial assets. Furthermore, our analysis using a natural experiment and a causal inference technique reveals that a large point grant stimulated an increase in point spending without affecting cash expenditure. We also find that consumers' shopping styles are associated with their point redemption patterns. This study, conducted within a massive real-world economic ecosystem, examines how consumers navigate multi-currency environments, with direct implications for modeling economic behavior and designing digital platforms."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.05347,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Validity Verification of the New TOEFL Writing Task Based on Classical Test Theory\n\nThe TOEFL iBT has introduced the Academic Discussion Task (ADT) to assess test-takers' ability to engage in academic discourse, reflecting the growing emphasis on interactive communication skills in higher education. However, research on the ADT's validity and fairness particularly for culturally and linguistically diverse groups, such as Chinese students, remains limited. This study addresses this gap by employing Classical Test Theory (CTT) to evaluate the psychometric properties of the ADT among Chinese university students. This study finds a robust correlation between the ADT and the CET-6 writing and translation subscores. In addition, there is a high level of expert agreement regarding the construct validity evidence and the appropriateness of the scoring rubric. Furthermore, the results indicate that gender differences in validity indices are minimal. Taken together, these results suggest that the ADT is a valid measure for Chinese test-takers without gender discrimination. However, it is recommended that the cultural sensitivity of the scoring rubric be further refined and that the CET-6 subscores for writing be retained for predictive purposes, in order to better accommodate the needs of diverse test-taker populations. By addressing these issues, this study contributes to the broader discourse on fairness and validity in high-stakes language assessments."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0157928466796875, 'GPT4': 0.404052734375, 'CLAUDE': 0.0295867919921875, 'GOOGLE': 0.49560546875, 'OPENAI_O_SERIES': 0.00867462158203125, 'DEEPSEEK': 0.00891876220703125, 'GROK': 0.00037026405334472656, 'NOVA': 0.00034117698669433594, 'OTHER': 0.036712646484375, 'HUMAN': 9.620189666748047e-05}}"
2509.1989,review,post_llm,2025,9,"{'ai_likelihood': 2.2682878706190324e-05, 'text': 'DSA, AIA, and LLMs: Approaches to conceptualizing and auditing moderation in LLM-based chatbots across languages and interfaces in the electoral contexts\n\nThe integration of Large Language Models (LLMs) into chatbot-like search engines poses new challenges for governing, assessing, and scrutinizing the content output by these online entities, especially in light of the Digital Service Act (DSA). In what follows, we first survey the regulation landscape in which we can situate LLM-based chatbots and the notion of moderation. Second, we outline the methodological approaches to our study: a mixed-methods audit across chatbots, languages, and elections. We investigated Copilot, ChatGPT, and Gemini across ten languages in the context of the 2024 European Parliamentary Election and the 2024 US Presidential Election. Despite the uncertainty in regulatory frameworks, we propose a set of solutions on how to situate, study, and evaluate chatbot moderation.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.10717,review,post_llm,2025,9,"{'ai_likelihood': 0.03801133897569445, 'text': ""Understanding Computer Science Students' Career Fair Experiences: Goals, Preparation, and Outcomes\n\nThe technology industry offers exciting and diverse career opportunities, ranging from traditional software development to emerging fields such as artificial intelligence, cybersecurity, and data science. Career fairs play a crucial role in helping Computer Science (CS) students understand the various career pathways available to them in the industry. However, limited research exists on how CS students experience and benefit from these events. Through a survey of 86 students, we investigate their motivations for attending, preparation strategies, and learning outcomes, including exposure to new career paths and technologies. We envision our findings providing valuable insights for career services professionals, educators, and industry leaders in improving the career development processes of CS students."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.18079,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Tracing the Techno-Supremacy Doctrine: A Critical Discourse Analysis of the AI Executive Elite\n\nThis paper critically analyzes the discourse of the 'AI executive elite,' a group of highly influential individuals shaping the way AI is funded, developed, and deployed worldwide. The primary objective is to examine the presence and dynamics of the 'Techno-Supremacy Doctrine' (TSD), a term introduced in this study to describe a belief system characterized by an excessive trust in technology's alleged inherent superiority in solving complex societal problems. This study integrates quantitative heuristics with in-depth qualitative investigations. Its methodology is operationalized in a two-phase critical discourse analysis of 14 texts published by elite members between 2017 and 2025. The findings demonstrate that the elite is not a monolithic bloc but exhibits a broad spectrum of stances. The discourse is highly dynamic, showing a marked polarization and general increase in pro-TSD discourse following the launch of ChatGPT. The analysis identifies key discursive patterns, including a dominant pro-TSD narrative that combines utopian promises with claims of inevitable progress, and the common tactic of acknowledging risks only as a strategic preamble to proposing further technological solutions. This paper presents TSD as a comprehensive analytical framework and provides a 'diagnostic toolkit' for identifying its manifestations, from insidious to benign. It argues that fostering critical awareness of these discursive patterns is essential for AI practitioners, policymakers, and the public to actively navigate the future of AI."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.1576881408691406e-05, 'GPT4': 0.292724609375, 'CLAUDE': 0.0006375312805175781, 'GOOGLE': 0.69140625, 'OPENAI_O_SERIES': 5.900859832763672e-06, 'DEEPSEEK': 0.01454925537109375, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.9669532775878906e-06, 'HUMAN': 0.00047779083251953125}}"
2509.15657,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'How built environment shapes cycling experience: A multi-scale review in historical urban contexts\n\nUnderstanding how built environments shape human experience is central to designing sustainable cities. Cycling provides a critical case: it delivers health and environmental benefits, yet its uptake depends strongly on the experience of cycling rather than infrastructure alone. Research on this relationship has grown rapidly but remains fragmented across disciplines and scales, and has concentrated on network-level analyses of routes and connectivity. This bias is especially problematic in historical cities, where embedding new infrastructure is difficult, and where cycling experience is shaped not only by spatial form but also by how cyclists perceive, interpret, and physically respond to their environment - through psychological factors such as safety and comfort, physiological demands such as stress and fatigue, and perceptual cues in the streetscape. We systematically reviewed 68 studies across urban planning, transportation, behavioural science, neuroscience, and public health. Two scales of analysis were identified: a macro scale addressing the ability to cycle and a micro scale addressing the propensity to cycle. Methods were classified into objective and subjective approaches, with hybrid approaches beginning to emerge. We find a persistent reliance on objective proxies, limited integration of subjective accounts, and insufficient attention to the streetscape as a lived environment. Addressing these gaps is essential to explain why environments enable or deter cycling, and to inform the design of cities that support cycling as both mobility and lived experience.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.001171112060546875, 'GPT4': 0.048919677734375, 'CLAUDE': 0.93359375, 'GOOGLE': 0.00170135498046875, 'OPENAI_O_SERIES': 5.358457565307617e-05, 'DEEPSEEK': 0.005321502685546875, 'GROK': 8.940696716308594e-07, 'NOVA': 8.940696716308594e-07, 'OTHER': 2.777576446533203e-05, 'HUMAN': 0.00908660888671875}}"
2509.08218,regular,post_llm,2025,9,"{'ai_likelihood': 0.002193450927734375, 'text': ""PolicyStory: Leveraging Large Language Models to Generate Comprehensible Summaries of Policy-News in India\n\nIn the era of information overload, traditional news consumption through both online and print media often fails to provide a structured and longitudinal understanding of complex sociopolitical issues. To address this gap, we present PolicyStory, an information tool designed to offer lucid, chronological, and summarized insights into Indian policy issues. PolicyStory collects news articles from diverse sources, clusters them by topic, and generates three levels of summaries from longitudinal media discourse on policies, leveraging open source large language models. A user study around the tool indicated that PolicyStory effectively aided users in grasping policy developments over time, with positive feedback highlighting its usability and clarity of summaries. By providing users a birds' eye view of complex policy topics, PolicyStory serves as a valuable resource."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.23843,review,post_llm,2025,9,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Digital welfare fraud detection and the Dutch SyRI judgment\n\nIn 2020, a Dutch court passed judgment in a case about a digital welfare fraud detection system called Systeem Risico Indicatie (SyRI). The court ruled that the SyRI legislation is unlawful because it does not comply with the right to privacy under the European Convention of Human Rights. In this article we analyse the judgment and its implications. This ruling is one of first in which a court has invalidated a welfare fraud detection system for breaching the right to privacy. We show that the immediate effects of the judgment are limited. The judgment does not say much about automated fraud detection systems in general, because it is limited to the circumstances of the case. Still, the judgment is important. The judgment reminds policymakers that fraud detection must happen in a way that respects data protection principles and the right to privacy. The judgment also confirms the importance of transparency if personal data are used.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.14907,regular,post_llm,2025,9,"{'ai_likelihood': 4.801485273573134e-06, 'text': 'Artificial Intelligence and Market Entrant Game Developers\n\nArtificial Intelligence (AI) is increasingly being used for generating digital assets, such as programming codes and images. Games composed of various digital assets are thus expected to be influenced significantly by AI. Leveraging public data and AI disclosure statements of games, this paper shows that relatively more independent developers entered the market when generative AI became more publicly accessible, but their purposes of using AI are similar with non-independent developers. Game features associated with AI hint nuanced impacts of AI on independent developers.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.14088,regular,post_llm,2025,9,"{'ai_likelihood': 0.015767415364583336, 'text': 'Interleaving Natural Language Prompting with Code Editing for Solving Programming Tasks with Generative AI Models\n\nNowadays, computing students often rely on both natural-language prompting and manual code editing to solve programming tasks. Yet we still lack a clear understanding of how these two modes are combined in practice, and how their usage varies with task complexity and student ability. In this paper, we investigate this through a large-scale study in an introductory programming course, collecting 13,305 interactions from 355 students during a three-day laboratory activity. Our analysis shows that students primarily use prompting to generate initial solutions, and then often enter short edit-run loops to refine their code following a failed execution. We find that manual editing becomes more frequent as task complexity increases, but most edits remain concise, with many affecting a single line of code. Higher-performing students tend to succeed using prompting alone, while lower-performing students rely more on edits. Student reflections confirm that prompting is helpful for structuring solutions, editing is effective for making targeted corrections, while both are useful for learning. These findings highlight the role of manual editing as a deliberate last-mile repair strategy, complementing prompting in AI-assisted programming workflows.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.22717,review,post_llm,2025,9,"{'ai_likelihood': 1.1258655124240452e-06, 'text': ""(When) Should We Delegate AI Governance to AIs? Some Lessons from Administrative Law\n\nAdvanced AI systems are now being used in AI governance. Practitioners will likely delegate an increasing number of tasks to them as they improve and governance becomes harder. However, using AI for governance risks serious harms because human practitioners may not be able to understand AI decisions or determine whether they are aligned to the user's interests. Delegation may also undermine governance's legitimacy. This paper begins to develop a principled framework for when to delegate AI governance to AIs and when (and how) to maintain human participation. Administrative law, which governs agencies that are (1) more expert in their domains than the legislatures that create them and the courts that oversee them and (2) potentially misaligned to their original goals, offers useful lessons. Administrative law doctrine provides examples of clear, articulated rules for when delegation can occur, what delegation can consist of, and what processes can keep agencies aligned even as they are empowered to achieve their goals. The lessons of administrative law provide a foundation for how AI governance can use AI in a safe, accountable, and effective way."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.18211,review,post_llm,2025,9,"{'ai_likelihood': 8.245309193929037e-06, 'text': 'Microtargeted propaganda by foreign actors: An interdisciplinary exploration\n\nThis article discusses a problem that has received scant attention in literature: microtargeted propaganda by foreign actors. Microtargeting involves collecting information about people, and using that information to show them targeted political advertisements. Such microtargeting enables advertisers to target ads to specific groups of people, for instance people who visit certain websites, forums, or Facebook groups. This article focuses on one type of microtargeting: microtargeting by foreign actors. For example, Russia has targeted certain groups in the US with ads, aiming to sow discord. Foreign actors could also try to influence European elections, for instance by advertising in favour of a certain political party. Foreign propaganda possibilities existed before microtargeting. This article explores two questions. In what ways, if any, is microtargeted propaganda by foreign actors different from other foreign propaganda? What could lawmakers in Europe do to mitigate the risks of microtargeted propaganda?', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.22329,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'AI Ethics Education in India: A Syllabus-Level Review of Computing Courses\n\nThe pervasive integration of artificial intelligence (AI) across domains such as healthcare, governance, finance, and education has intensified scrutiny of its ethical implications, including algorithmic bias, privacy risks, accountability, and societal impact. While ethics has received growing attention in computer science (CS) education more broadly, the specific pedagogical treatment of {AI ethics} remains under-examined. This study addresses that gap through a large-scale analysis of 3,395 publicly accessible syllabi from CS and allied areas at leading Indian institutions. Among them, only 75 syllabi (2.21%) included any substantive AI ethics content. Three key findings emerged: (1) AI ethics is typically integrated as a minor module within broader technical courses rather than as a standalone course; (2) ethics coverage is often limited to just one or two instructional sessions; and (3) recurring topics include algorithmic fairness, privacy and data governance, transparency, and societal impact. While these themes reflect growing awareness, current curricular practices reveal limited depth and consistency. This work highlights both the progress and the gaps in preparing future technologists to engage meaningfully with the ethical dimensions of AI, and it offers suggestions to strengthen the integration of AI ethics within computing curricula.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.31348991394043e-05, 'GPT4': 0.03668212890625, 'CLAUDE': 0.1717529296875, 'GOOGLE': 0.002719879150390625, 'OPENAI_O_SERIES': 0.00015914440155029297, 'DEEPSEEK': 0.7880859375, 'GROK': 1.7881393432617188e-07, 'NOVA': 4.649162292480469e-06, 'OTHER': 0.00010848045349121094, 'HUMAN': 0.00033164024353027344}}"
2509.26483,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'A systematic comparison of Large Language Models for automated assignment assessment in programming education: Exploring the importance of architecture and vendor\n\nThis study presents the first large-scale, side-by-side comparison of contemporary Large Language Models (LLMs) in the automated grading of programming assignments. Drawing on over 6,000 student submissions collected across four years of an introductory programming course, we systematically analysed the distribution of grades, differences in mean scores and variability reflecting stricter or more lenient grading, and the consistency and clustering of grading patterns across models. Eighteen publicly available models were evaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4); Deepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite, gemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and OpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini, gpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses revealed clear, systematic differences between and within vendor families, with ""mini"" and ""nano"" variants consistently underperforming their full-scale counterparts. All models displayed high internal agreement, measured by the intraclass correlation coefficient, with the model consensus but only moderate agreement with human teachers\' grades, indicating a persistent gap between automated and human assessment. These findings underscore that the choice of model for educational deployment is not neutral and should be guided by pedagogical goals, transparent reporting of evaluation metrics, and ongoing human oversight to ensure accuracy, fairness and relevance.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.706880569458008e-05, 'GPT4': 0.08587646484375, 'CLAUDE': 0.0148162841796875, 'GOOGLE': 0.1260986328125, 'OPENAI_O_SERIES': 0.00016510486602783203, 'DEEPSEEK': 0.771484375, 'GROK': 2.5033950805664062e-06, 'NOVA': 2.384185791015625e-06, 'OTHER': 0.00013005733489990234, 'HUMAN': 0.0015392303466796875}}"
2509.22759,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Scaling Accessibility Education: Reflections from a Workshop Targeting CS Educators and Software Professionals\n\nDespite growing global attention to digital accessibility, research from India highlights a significant gap in accessibility training for both computing educators and software professionals. To address this need, we designed and conducted an experiential workshop aimed at building foundational capacity in accessibility practices among 77 participants, including computer science (CS) faculty and industry practitioners. The one-day workshop combined hands-on activities, tool demonstrations, and case studies to foster practical understanding and engagement. Post-workshop feedback showed that a majority of participants rated the workshop positively, with many reporting increased confidence and a shift in their perception of accessibility as a shared responsibility. Additionally, participants expressed a strong interest in applying accessibility principles within their workplaces, underscoring the workshop's practical relevance and impact. In this experience report, we detail the workshop's design, implementation, and evaluation, and offer actionable insights to guide future initiatives aimed at strengthening accessibility capacity across India's computing education and professional landscape."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.227327346801758e-05, 'GPT4': 0.998046875, 'CLAUDE': 0.00018465518951416016, 'GOOGLE': 2.0384788513183594e-05, 'OPENAI_O_SERIES': 0.001010894775390625, 'DEEPSEEK': 0.0006866455078125, 'GROK': 1.1920928955078125e-07, 'NOVA': 2.384185791015625e-07, 'OTHER': 7.569789886474609e-06, 'HUMAN': 2.0503997802734375e-05}}"
2509.1713,regular,post_llm,2025,9,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""A community-driven optimization framework for redrawing school attendance boundaries\n\nThe vast majority of US public school districts use school attendance boundaries to determine which student addresses are assigned to which schools. Existing work shows how redrawing boundaries can be a powerful policy lever for increasing access and opportunity for historically disadvantaged groups, even while maintaining other priorities like minimizing driving distances and preserving existing social ties between students and families. This study introduces a multi-objective algorithmic school rezoning framework and applies it to a large-scale rezoning effort impacting over 50,000 students through an ongoing researcher-school district partnership. The framework is designed to incorporate feedback from community members and policymakers, both by deciding which goals are optimized and also by placing differential ``importance'' on goals through weights from community surveys. Empirical results reveal the framework's ability to surface school redistricting plans that simultaneously advance a number of objectives often thought to be in competition with one another, including socioeconomic integration, transportation efficiency, and stable feeder patterns (transitions) between elementary, middle, and high schools. The paper also highlights how local education policymakers navigate several practical challenges, like building political will to make change in a polarized policy climate. The framework is built using open-source tools and publicly released to support school districts in exploring and implementing new policies to improve educational access and opportunity in the coming years."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.047,regular,post_llm,2025,9,"{'ai_likelihood': 3.4769376118977866e-06, 'text': 'Linguistic Hooks: Investigating The Role of Language Triggers in Phishing Emails Targeting African Refugees and Students\n\nPhishing and sophisticated email-based social engineering attacks disproportionately affect vulnerable populations, such as refugees and immigrant students. However, these groups remain understudied in cybersecurity research. This gap in understanding, coupled with their exclusion from broader security and privacy policies, increases their susceptibility to phishing and widens the digital security divide between marginalized and non-marginalized populations. To address this gap, we first conducted digital literacy workshops with newly resettled African refugee populations (n = 48) in the US to improve their understanding of how to safeguard sensitive and private information. Following the workshops, we conducted a real-world phishing deception study using carefully designed emails with linguistic cues for three participant groups: a subset of the African US-refugees recruited from the digital literacy workshops (n = 19), African immigrant students in the US (n = 142), and a control group of monolingual US-born students (n = 184). Our findings indicate that while digital literacy training for refugees improves awareness of safe cybersecurity practices, recently resettled African US-refugees still face significant challenges due to low digital literacy skills and limited English proficiency. This often leads them to ignore or fail to recognize phishing emails as phishing. Both African immigrant students and US-born students showed greater caution, though instances of data disclosure remained prevalent across groups. Our findings highlight, irrespective of literacy, the need to be trained to think critically about digital security. We conclude by discussing how the security and privacy community can better include marginalized populations in policy making and offer recommendations for designing equitable, inclusive cybersecurity initiatives.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.1072,regular,post_llm,2025,9,"{'ai_likelihood': 0.010799831814236112, 'text': ""Adapting Public Personas: A Multimodal Study of U.S. Legislators' Cross-Platform Social Media Strategies\n\nCurrent cross-platform social media analyses primarily focus on the textual features of posts, often lacking multimodal analysis due to past technical limitations. This study addresses this gap by examining how U.S. legislators in the 118th Congress strategically use social media platforms to adapt their public personas by emphasizing different topics and stances. Leveraging the Large Multimodal Models (LMMs) for fine-grained text and image analysis, we examine 540 legislators personal website and social media, including Facebook, X (Twitter), TikTok. We find that legislators tailor their topics and stances to project distinct public personas on different platforms. Democrats tend to prioritize TikTok, which has a younger user base, while Republicans are more likely to express stronger stances on established platforms such as Facebook and X (Twitter), which offer broader audience reach. Topic analysis reveals alignment with constituents' key concerns, while stances and polarization vary by platform and topic. Large-scale image analysis shows Republicans employing more formal visuals to project authority, whereas Democrats favor campaign-oriented imagery. These findings highlight the potential interplay between platform features, audience demographics, and partisan goals in shaping political communication. By providing insights into multimodal strategies, this study contributes to understanding the role of social media in modern political discourse and communications."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.02638,review,post_llm,2025,9,"{'ai_likelihood': 0.21402994791666669, 'text': 'Exploring the interplay between Planetary Boundaries and Sustainable Development Goals using Large Language Models\n\nBy analyzing 40,037 climate articles using Large Language Models (LLMs), we identified interactions between Planetary Boundaries (PBs) and Sustainable Development Goals (SDGs). An automated reasoner distinguished true trade-offs (SDG progress harming PBs) and synergies (mutual reinforcement) from double positives and negatives (shared drivers). Results show 21.1% true trade-offs, 28.3% synergies, and 19.5% neutral interactions, with the remainder being double positive or negative. Key findings include conflicts between land-use goals (SDG2/SDG6) and land system boundaries (PB6), together with the underrepresentation of social SDGs in the climate literature. Our study highlights the need for integrated policies that align development goals with planetary limits to reduce systemic conflicts. We propose three steps: (1) integrated socio-ecological metrics, (2) governance ensuring that SDG progress respects Earth system limits, and (3) equity measures protecting marginalized groups from boundary compliance costs.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.05838,regular,post_llm,2025,9,"{'ai_likelihood': 0.0010787116156684028, 'text': ""Towards an Automated Framework to Audit Youth Safety on TikTok\n\nThis paper investigates the effectiveness of TikTok's enforcement mechanisms for limiting the exposure of harmful content to youth accounts. We collect over 7000 videos, classify them as harmful vs not-harmful, and then simulate interactions using age-specific sockpuppet accounts through both passive and active engagement strategies. We also evaluate the performance of large language (LLMs) and vision-language models (VLMs) in detecting harmful content, identifying key challenges in precision and scalability.\n  Preliminary results show minimal differences in content exposure between adult and youth accounts, raising concerns about the platform's age-based moderation. These findings suggest that the platform needs to strengthen youth safety measures and improve transparency in content moderation."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.06927,regular,post_llm,2025,9,"{'ai_likelihood': 0.3325737847222222, 'text': ""NeedForHeat DataGear: An Open Monitoring System to Accelerate the Residential Heating Transition\n\nWe introduce NeedForHeat DataGear: an open hardware and open software data collection system designed to accelerate the residential heating transition. NeedForHeat DataGear collects time series monitoring data in homes that have not yet undergone a heating transition, enabling assessment of real-life thermal characteristics, heating system efficiency, and residents' comfort needs. This paper outlines its architecture and functionalities, emphasizing its modularity, adaptability, and cost-effectiveness for field data acquisition. Unlike conventional domestic monitoring solutions focused on home automation, direct feedback, or post-installation heat pump monitoring, it prioritizes time series data we deemed essential to evaluate the current situation in existing homes before the heating transition. Designed for seamless deployment across diverse households, NeedForHeat DataGear combines openness, security, and privacy with a low-cost, user-friendly approach, making it a valuable tool for researchers, energy professionals, and energy coaches."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.13156,regular,post_llm,2025,9,"{'ai_likelihood': 0.21280924479166669, 'text': 'Designing the Hybrid Cooperative: A Socio-Technical Architecture for Scalable, Global Coordination Using Blockchain\n\nBlockchain has been promoted as a remedy for coordination in fragmented, multi-stakeholder ecosystems, yet many projects stall at pilot stage. Using a design-science approach, we develop the Hybrid Cooperative (HC), a digitally native governance architecture that combines smart-contract coordination with a minimal, code-deferent legal interface and jurisdictional modules. This selective decentralization decentralizes rules where programmability lowers agency and verification costs, and centralizes only what is needed for enforceability. A post-case evaluation against two traceability initiatives in supply chains illustrates how the HC improves distributed task management, verifiable information, incentive alignment, institutional interoperability, and scalable, contestable governance. The paper contributes to Information Systems by specifying a socio-technical model for scalable, multi-stakeholder coordination across regulatory and organizational boundaries.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.07365,review,post_llm,2025,9,"{'ai_likelihood': 0.0007957882351345486, 'text': 'Develop-Fair Use for Artificial Intelligence: A Sino-U.S. Copyright Law Comparison Based on the Ultraman, Bartz v. Anthropic, and Kadrey v. Meta Cases\n\nTraditional fair use can no longer respond to the challenges posed by generative AI. Drawing on a comparative analysis of China\'s Ultraman and the U.S. cases Bartz v. Anthropic and Kadrey v. Meta, this article proposes ""Develop-Fair Use"" (DFU). DFU treats AI fair use (AIFU) not as a fixed exception but as a dynamic tool of judicial balancing that shifts analysis from closed scenarios to an evaluative rule for open-ended contexts. The judicial focus moves from formal classification of facts to a substantive balancing of competition in relevant markets. Although China and the U.S. follow different paths, both reveal this logic: Ultraman, by articulating a ""four-context analysis,"" creates institutional space for AI industry development; the debate over the fourth factor, market impact, in the two U.S. cases, especially Kadrey\'s ""market dilution"" claim, expands review from substitution in copyright markets to wider industrial competition. The core of DFU is to recognize and balance the tension in relevant markets between an emerging AI industry that invokes fair use to build its markets and a publishing industry that develops markets, including one for ""training licenses,"" to resist fair use. The boundary of fair use is therefore not a product of pure legal deduction but a case-specific factual judgment grounded in evolving market realities. This approach aims both to trim excess copyright scope and to remedy shortfalls in market competition.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.14189,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'AI and the Future of Academic Peer Review\n\nPeer review remains the central quality-control mechanism of science, yet its ability to fulfill this role is increasingly strained. Empirical studies document serious shortcomings: long publication delays, escalating reviewer burden concentrated on a small minority of scholars, inconsistent quality and low inter-reviewer agreement, and systematic biases by gender, language, and institutional prestige. Decades of human-centered reforms have yielded only marginal improvements. Meanwhile, artificial intelligence, especially large language models (LLMs), is being piloted across the peer-review pipeline by journals, funders, and individual reviewers. Early studies suggest that AI assistance can produce reviews comparable in quality to humans, accelerate reviewer selection and feedback, and reduce certain biases, but also raise distinctive concerns about hallucination, confidentiality, gaming, novelty recognition, and loss of trust. In this paper, we map the aims and persistent failure modes of peer review to specific LLM applications and systematically analyze the objections they raise alongside safeguards that could make their use acceptable. Drawing on emerging evidence, we show that targeted, supervised LLM assistance can plausibly improve error detection, timeliness, and reviewer workload without displacing human judgment. We highlight advanced architectures, including fine-tuned, retrieval-augmented, and multi-agent systems, that may enable more reliable, auditable, and interdisciplinary review. We argue that ethical and practical considerations are not peripheral but constitutive: the legitimacy of AI-assisted peer review depends on governance choices as much as technical capacity. The path forward is neither uncritical adoption nor reflexive rejection, but carefully scoped pilots with explicit evaluation metrics, transparency, and accountability.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 4.351139068603516e-06, 'CLAUDE': 0.00015807151794433594, 'GOOGLE': 8.940696716308594e-07, 'OPENAI_O_SERIES': 4.172325134277344e-07, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.23287,review,post_llm,2025,9,"{'ai_likelihood': 0.2194552951388889, 'text': ""Skill, Will, or Both? Understanding Digital Inaccessibility from Accessibility Professionals' Viewpoint\n\nDigital inaccessibility continues to be a significant barrier to true inclusion and equality. WebAIM's 2024 report reveals that only 4.1% of the world's top one million website homepages are fully accessible. Furthermore, the percentage of web pages with detectable Web Content Accessibility Guidelines (WCAG) failures has only decreased by 1.9\\% over the past five years, from 97.8%. To gain deeper insights into the persistent challenges of digital accessibility, we conducted a comprehensive survey with 160 accessibility professionals. Unlike previous studies, which often focused on technology professionals, our research examines inaccessibility through the lens of dedicated accessibility professionals, offering a more detailed analysis of the barriers they face. Our investigation explores (a) organizations' willingness to prioritize accessibility, (b) the challenges in ensuring accessibility, and (c) the current accessibility training practices in technology workspaces. This study aims to provide an updated perspective on the state of digital accessibility from the point of view of accessibility professionals."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.13265,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Beyond Private or Public: Large Language Models as Quasi-Public Goods in the AI Economy\n\nThis paper conceptualizes Large Language Models (LLMs) as a form of mixed public goods within digital infrastructure, analyzing their economic properties through a comprehensive theoretical framework. We develop mathematical models to quantify the non-rivalry characteristics, partial excludability, and positive externalities of LLMs. Through comparative analysis of open-source and closed-source development paths, we identify systematic differences in resource allocation efficiency, innovation trajectories, and access equity. Our empirical research evaluates the spillover effects and network externalities of LLMs across different domains, including knowledge diffusion, innovation acceleration, and industry transformation. Based on these findings, we propose policy recommendations for balancing innovation incentives with equitable access, including public-private partnership mechanisms, computational resource democratization, and governance structures that optimize social welfare. This interdisciplinary approach contributes to understanding the economic nature of foundation AI models and provides policy guidance for their development as critical digital infrastructure', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.172325134277344e-07, 'GPT4': 0.0024471282958984375, 'CLAUDE': 0.9833984375, 'GOOGLE': 5.352497100830078e-05, 'OPENAI_O_SERIES': 0.0013408660888671875, 'DEEPSEEK': 0.01282501220703125, 'GROK': 5.960464477539063e-08, 'NOVA': 2.205371856689453e-06, 'OTHER': 2.980232238769531e-07, 'HUMAN': 5.960464477539063e-08}}"
2509.13337,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Behind India's ChatGPT Conversations: A Retrospective Analysis of 238 Unedited User Prompts\n\nUnderstanding how users authentically interact with Large Language Models (LLMs) remains a significant challenge in human-computer interaction research. Most existing studies rely on self-reported usage patterns or controlled experimental conditions, potentially missing genuine behavioral adaptations. This study presents a behavioral analysis of the use of English-speaking urban professional ChatGPT in India based on 238 authentic, unedited user prompts from 40 participants in 15+ Indian cities, collected using retrospective survey methodology in August 2025. Using authentic retrospective prompt collection via anonymous social media survey to minimize real-time observer effects, we analyzed genuine usage patterns. Key findings include: (1) 85\\% daily usage rate (34/40 users) indicating mature adoption beyond experimental use, (2) evidence of cross-domain integration spanning professional, personal, health and creative contexts among the majority of users, (3) 42.5\\% (17/40) primarily use ChatGPT for professional workflows with evidence of real-time problem solving integration, and (4) cultural context navigation strategies with users incorporating Indian cultural specifications in their prompts. Users develop sophisticated adaptation techniques and the formation of advisory relationships for personal guidance. The study reveals the progression from experimental to essential workflow dependency, with users treating ChatGPT as an integrated life assistant rather than a specialized tool. However, the findings are limited to urban professionals in English recruited through social media networks and require a larger demographic validation. This work contributes a novel methodology to capture authentic AI usage patterns and provides evidence-based insights into cultural adaptation strategies among this specific demographic of users."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1920928955078125e-07, 'GPT4': 0.00018548965454101562, 'CLAUDE': 0.99658203125, 'GOOGLE': 9.870529174804688e-05, 'OPENAI_O_SERIES': 6.258487701416016e-06, 'DEEPSEEK': 0.0026874542236328125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 0.0002167224884033203}}"
2509.16794,review,post_llm,2025,9,"{'ai_likelihood': 6.9207615322536894e-06, 'text': 'The Even Sheen of AI: Kitsch, LLMs, and Homogeneity\n\nThe exploding use and impact of Chatbots such as ChatGPT that are based on Large Language Models urgently call for a language which is fit to clearly describe functions and problems of the production process and qualities of the Chatbots\' textual and image output. Recently, the discussion about appropriate and illuminating metaphors to describe LLMs has gained momentum. As an alternative to well-established metaphors such as ""hallucinating"" and ""bullshit"", we propose ""kitsch"" as a new metaphor. As an internationally widespread term from literary and cultural studies, we argue that ""kitsch"" is particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is becoming increasingly dominant as the proportion of AI-generated content on the internet grows. This is leading to the equalisation of language, style and argument. In view of the potential negative consequences of this averaging, including for human content producers on the internet, we advocate combining methods and insights from kitsch studies with AI research, philosophy, and communication studies in order to better understand the phenomenon and develop countermeasures.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.11732,regular,post_llm,2025,9,"{'ai_likelihood': 0.99755859375, 'text': 'Making Judicial Reasoning Visible: Structured Annotation of Holding, Evidentiary Considerations, and Subsumption in Criminal Judgments\n\nJudicial reasoning in criminal judgments typically consists of three elements: Holding , evidentiary considerations, and subsumption. These elements form the logical foundation of judicial decision-making but remain unstructured in court documents, limiting large-scale empirical analysis. In this study, we design annotation guidelines to define and distinguish these reasoning components and construct the first dedicated datasets from Taiwanese High Court and Supreme Court criminal judgments. Using the bilingual large language model ChatGLM2, we fine-tune classifiers for each category. Preliminary experiments demonstrate that the model achieves approximately 80% accuracy, showing that judicial reasoning patterns can be systematically identified by large language models even with relatively small annotated corpora. Our contributions are twofold: (1) the creation of structured annotation rules and datasets for Holding, evidentiary considerations, and subsumption; and (2) the demonstration that such reasoning can be computationally learned. This work lays the foundation for large-scale empirical legal studies and legal sociology, providing new tools to analyze judicial fairness, consistency, and transparency.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.9252300262451172e-05, 'GPT4': 0.0035953521728515625, 'CLAUDE': 0.9912109375, 'GOOGLE': 0.00024509429931640625, 'OPENAI_O_SERIES': 0.0003666877746582031, 'DEEPSEEK': 0.0030803680419921875, 'GROK': 4.172325134277344e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 1.4185905456542969e-05, 'HUMAN': 0.0016078948974609375}}"
2509.05358,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Mobile Phone Sensor-based Nigerian Driving Dataset to Detect Alcohol-influenced Behaviours\n\nThis paper presents a unique driving dataset collected in Nigeria via mobile phone sensors to support a machine learning model for detecting alcohol-influenced driving behaviours, with the long-term aim of integrating this model into a mobile application that encourages safer driving behaviours. Driving under the influence of alcohol is a major public safety concern, particularly in low-income countries like Nigeria, where traditional enforcement mechanisms may be limited. The proposed model leverages smartphone sensors such as accelerometers, gyroscopes, and GPS to provide a non-invasive, continuous solution for detecting impaired driving patterns in real time. This study adapts existing data processing and pattern matching methodologies to label real-world driving data collected from Nigerian drivers, which are then used to train the model. A decision tree classifier is developed to detect alcohol influence, based on behavioural and temporal features, achieving a recall of 100%, a precision of 60%, and an F1 score of 75%. The model's overall accuracy was 90.91%, ensuring that no alcohol influenced trips were missed. Key predictive features included speed variability, course deviation, and time of day, which align with established patterns of alcohol consumption. This study contributes to the field by demonstrating how machine learning can be applied in low-resource environments to improve road safety. The findings suggest that the model can significantly enhance the detection and prevention of risky driving behaviours, with the potential for future integration into mobile applications to provide real-time feedback and encourage safer driving practices. This scalable and accessible solution offers a new approach to addressing road safety challenges in regions where traditional interventions are inadequate."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0023365020751953125, 'GPT4': 0.81396484375, 'CLAUDE': 0.0019931793212890625, 'GOOGLE': 0.1470947265625, 'OPENAI_O_SERIES': 0.00978851318359375, 'DEEPSEEK': 0.01393890380859375, 'GROK': 0.0010929107666015625, 'NOVA': 2.866983413696289e-05, 'OTHER': 0.00982666015625, 'HUMAN': 6.854534149169922e-06}}"
2509.12283,review,post_llm,2025,9,"{'ai_likelihood': 0.98779296875, 'text': 'Prompting the Professoriate: A Qualitative Study of Instructor Perspectives on LLMs in Data Science Education\n\nLarge Language Models (LLMs) have shifted in just a few years from novelty to ubiquity, raising fundamental questions for data science education. Tasks once used to teach coding, writing, and problem-solving can now be completed by LLMs, forcing educators to reconsider both pedagogy and assessment. To understand how instructors are adapting, we conducted semi-structured interviews with 42 instructors from 33 institutions in 10 countries in June and July 2025. Our qualitative analysis reveals a pragmatic mix of optimism and concern. Many respondents view LLMs as inevitable classroom tools -- comparable to calculators or Wikipedia -- while others worry about de-skilling, misplaced confidence, and uneven integration across institutions. Around 58 per cent have already introduced demonstrations, guided activities, or make extensive use of LLMs in their courses, though most expect change to remain slow and uneven. That said, 31 per cent have not used LLMs to teach students and do not plan to. We highlight some instructional innovations, including AI-aware assessments, reflective use of LLMs as tutors, and course-specific chatbots. By sharing these perspectives, we aim to help data science educators adapt collectively to ensure curricula keep pace with technological change.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.540515899658203e-05, 'GPT4': 0.0235748291015625, 'CLAUDE': 0.85595703125, 'GOOGLE': 5.40614128112793e-05, 'OPENAI_O_SERIES': 9.5367431640625e-06, 'DEEPSEEK': 0.0035800933837890625, 'GROK': 2.384185791015625e-07, 'NOVA': 3.5762786865234375e-07, 'OTHER': 1.430511474609375e-06, 'HUMAN': 0.11676025390625}}"
2509.2256,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'LLM-Augmented and Fair Machine Learning Framework for University Admission Prediction\n\nUniversities face surging applications and heightened expectations for fairness, making accurate admission prediction increasingly vital. This work presents a comprehensive framework that fuses machine learning, deep learning, and large language model techniques to combine structured academic and demographic variables with unstructured text signals. Drawing on more than 2,000 student records, the study benchmarks logistic regression, Naive Bayes, random forests, deep neural networks, and a stacked ensemble. Logistic regression offers a strong, interpretable baseline at 89.5% accuracy, while the stacked ensemble achieves the best performance at 91.0%, with Naive Bayes and random forests close behind. To probe text integration, GPT-4-simulated evaluations of personal statements are added as features, yielding modest gains but demonstrating feasibility for authentic essays and recommendation letters. Transparency is ensured through feature-importance visualizations and fairness audits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11% gap by parental education, underscoring the need for continued monitoring. The framework is interpretable, fairness-aware, and deployable.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.9073486328125e-06, 'GPT4': 0.0013322830200195312, 'CLAUDE': 0.0186614990234375, 'GOOGLE': 5.602836608886719e-05, 'OPENAI_O_SERIES': 1.430511474609375e-06, 'DEEPSEEK': 0.97998046875, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 5.841255187988281e-06, 'HUMAN': 1.1146068572998047e-05}}"
2509.24646,review,post_llm,2025,9,"{'ai_likelihood': 3.245141771104601e-06, 'text': 'Legal Matters in Research Software: A Few Things Worth Discussing\n\nThe paper discusses legal aspects relevant to the development of research software and practical approaches taken by research software engineers to deal with them. Intellectual Property Rights on software are considered alongside licensing choices made by the research community. The discussion addresses the ambiguities in the identification of the copyright holder of research software, the uncertainty surrounding liability, and remarks the varying level of support on legal matters provided by research organisations. The paper also reflects on the widespread use of AI coding assistants in the absence of institutional policies, and on the new AI regulations passed by the European Union. The aim of the contribution is to point out that a better understanding of legal matters concerning software development is an asset in giving research software the right value it deserves as a driver of scientific progress.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.18212,review,post_llm,2025,9,"{'ai_likelihood': 2.980232238769531e-07, 'text': 'Personalised Pricing: The Demise of the Fixed Price?\n\nAn online seller or platform is technically able to offer every consumer a different price for the same product, based on information it has about the customers. Such online price discrimination exacerbates concerns regarding the fairness and morality of price discrimination, and the possible need for regulation. In this chapter, we discuss the underlying basis of price discrimination in economic theory, and its popular perception. Our surveys show that consumers are critical and suspicious of online price discrimination. A majority consider it unacceptable and unfair, and are in favour of a ban. When stores apply online price discrimination, most consumers think they should be informed about it. We argue that the General Data Protection Regulation (GDPR) applies to the most controversial forms of online price discrimination, and not only requires companies to disclose their use of price discrimination, but also requires companies to ask customers for their prior consent. Industry practice, however, does not show any adoption of these two principles.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.24345,review,post_llm,2025,9,"{'ai_likelihood': 7.351239522298178e-06, 'text': 'Regulating Online Algorithmic Pricing: A Comparative Study of Privacy and Data Protection Laws in the EU and US\n\nThe emergence of big data, AI and machine learning has allowed sellers and online platforms to tailor pricing for customers in real-time. While online algorithmic pricing can increase efficiency, market welfare, and optimize pricing strategies for sellers and companies, it poses a threat to the fundamental values of privacy, digital autonomy, and non-discrimination, raising legal and ethical concerns. On both sides of the Atlantic, legislators have endeavoured to regulate online algorithmic pricing in different ways in the context of privacy and personal data protection. Represented by the GDPR, the EU adopts an omnibus approach to regulate algorithmic pricing and is supplemented by the Digital Service Act and the Digital Market Act. The US combines federal and state laws to regulate online algorithmic pricing and focuses on industrial regulations. Therefore, a comparative analysis of these legal frameworks is necessary to ascertain the effectiveness of these approaches. Taking a comparative approach, this working paper aims to explore how EU and US respective data protection and privacy laws address the issues posed by online algorithmic pricing. The paper evaluates whether the current legal regime is effective in protecting individuals against the perils of online algorithmic pricing in the EU and the US. It particularly analyses the new EU regulatory paradigm, the Digital Service Act (DSA) and the Digital Market Act (DMA), as supplementary mechanisms to the EU data protection law, in order to draw lessons for US privacy law and vice versa.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.08306,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Who Gets Seen in the Age of AI? Adoption Patterns of Large Language Models in Scholarly Writing and Citation Outcomes\n\nThe rapid adoption of generative AI tools is reshaping how scholars produce and communicate knowledge, raising questions about who benefits and who is left behind. We analyze over 230,000 Scopus-indexed computer science articles between 2021 and 2025 to examine how AI-assisted writing alters scholarly visibility across regions. Using zero-shot detection of AI-likeness, we track stylistic changes in writing and link them to citation counts, journal placement, and global citation flows before and after ChatGPT. Our findings reveal uneven outcomes: authors in the Global East adopt AI tools more aggressively, yet Western authors gain more per unit of adoption due to pre-existing penalties for ""humanlike"" writing. Prestigious journals continue to privilege more human-sounding texts, creating tensions between visibility and gatekeeping. Network analyses show modest increases in Eastern visibility and tighter intra-regional clustering, but little structural integration overall. These results highlight how AI adoption reconfigures the labor of academic writing and reshapes opportunities for recognition.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.2782554626464844e-06, 'GPT4': 0.001102447509765625, 'CLAUDE': 0.03485107421875, 'GOOGLE': 1.8358230590820312e-05, 'OPENAI_O_SERIES': 2.0265579223632812e-06, 'DEEPSEEK': 0.9638671875, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.0132789611816406e-06, 'HUMAN': 2.4437904357910156e-06}}"
2510.01275,regular,post_llm,2025,9,"{'ai_likelihood': 2.043114768134223e-05, 'text': ""Discovering Self-Regulated Learning Patterns in Chatbot-Powered Education Environment\n\nThe increasing adoption of generative AI (GenAI) tools such as chatbots in education presents new opportunities to support students' self-regulated learning (SRL), but also raises concerns about how learners actually engage in planning, executing, and reflection when learning with a chatbot. While SRL is typically conceptualized as a sequential process, little is known about how it unfolds during real-world student-chatbot interactions. To explore this, we proposed Gen-SRL, an annotation schema to categorize student prompts into 16 microlevel actions across 4 macrolevel phases. Using the proposed schema, we annotated 212 chatbot interactions from a real-world English writing task. We then performed frequency analysis and process mining (PM) techniques to discover SRL patterns in depth. Our results revealed that students' SRL behaviours were imbalanced, with over 82% of actions focused on task execution and limited engagement in planning and reflection. In addition, the process analysis showed nonsequential regulation patterns. Our findings suggest that classical SRL theories cannot fully capture the dynamic SRL patterns that emerge during chatbot interactions. Furthermore, we highlight the importance of designing adaptive and personalized scaffolds that respond to students' dynamic behaviours in chatbot-powered contexts. More importantly, this study offers a new perspective for advancing SRL research and suggests directions for developing chatbots that better support self-regulation."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.15122,regular,post_llm,2025,9,"{'ai_likelihood': 0.98046875, 'text': 'Prestige over merit: An adapted audit of LLM bias in peer review\n\nLarge language models (LLMs) are playing an increasingly integral, though largely informal, role in scholarly peer review. Yet it remains unclear whether LLMs reproduce the biases observed in human decision-making. We adapt a resume-style audit to scientific publishing, developing a multi-role LLM simulation (editor/reviewer) that evaluates a representative set of high-quality manuscripts across the physical, biological, and social sciences under randomized author identities (institutional prestige, gender, race). The audit reveals a strong and consistent institutional-prestige bias: identical papers attributed to low-prestige affiliations face a significantly higher risk of rejection, despite only modest differences in LLM-assessed quality. To probe mechanisms, we generate synthetic CVs for the same author profiles; these encode large prestige-linked disparities and an inverted prestige-tenure gradient relative to national benchmarks. The results suggest that both domain norms and prestige-linked priors embedded in training data shape paper-level outcomes once identity is visible, converting affiliation into a decisive status cue.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.7881393432617188e-06, 'GPT4': 0.0021457672119140625, 'CLAUDE': 0.00203704833984375, 'GOOGLE': 2.2590160369873047e-05, 'OPENAI_O_SERIES': 5.960464477539063e-08, 'DEEPSEEK': 0.98681640625, 'GROK': 5.960464477539063e-08, 'NOVA': 1.1920928955078125e-07, 'OTHER': 8.344650268554688e-07, 'HUMAN': 0.00879669189453125}}"
2509.10647,regular,post_llm,2025,9,"{'ai_likelihood': 0.00038994683159722225, 'text': 'Humanizing Automated Programming Feedback: Fine-Tuning Generative Models with Student-Written Feedback\n\nThe growing need for automated and personalized feedback in programming education has led to recent interest in leveraging generative AI for feedback generation. However, current approaches tend to rely on prompt engineering techniques in which predefined prompts guide the AI to generate feedback. This can result in rigid and constrained responses that fail to accommodate the diverse needs of students and do not reflect the style of human-written feedback from tutors or peers. In this study, we explore learnersourcing as a means to fine-tune language models for generating feedback that is more similar to that written by humans, particularly peer students. Specifically, we asked students to act in the flipped role of a tutor and write feedback on programs containing bugs. We collected approximately 1,900 instances of student-written feedback on multiple programming problems and buggy programs. To establish a baseline for comparison, we analyzed a sample of 300 instances based on correctness, length, and how the bugs are described. Using this data, we fine-tuned open-access generative models, specifically Llama3 and Phi3. Our findings indicate that fine-tuning models on learnersourced data not only produces feedback that better matches the style of feedback written by students, but also improves accuracy compared to feedback generated through prompt engineering alone, even though some student-written feedback is incorrect. This surprising finding highlights the potential of student-centered fine-tuning to improve automated feedback systems in programming education.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.08839,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Evaluating the Clinical Safety of LLMs in Response to High-Risk Mental Health Disclosures\n\nAs large language models (LLMs) increasingly mediate emotionally sensitive conversations, especially in mental health contexts, their ability to recognize and respond to high-risk situations becomes a matter of public safety. This study evaluates the responses of six popular LLMs (Claude, Gemini, Deepseek, ChatGPT, Grok 3, and LLAMA) to user prompts simulating crisis-level mental health disclosures. Drawing on a coding framework developed by licensed clinicians, five safety-oriented behaviors were assessed: explicit risk acknowledgment, empathy, encouragement to seek help, provision of specific resources, and invitation to continue the conversation. Claude outperformed all others in global assessment, while Grok 3, ChatGPT, and LLAMA underperformed across multiple domains. Notably, most models exhibited empathy, but few consistently provided practical support or sustained engagement. These findings suggest that while LLMs show potential for emotionally attuned communication, none currently meet satisfactory clinical standards for crisis response. Ongoing development and targeted fine-tuning are essential to ensure ethical deployment of AI in mental health settings.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.62396240234375e-05, 'GPT4': 0.6435546875, 'CLAUDE': 0.2044677734375, 'GOOGLE': 0.0004096031188964844, 'OPENAI_O_SERIES': 0.00010645389556884766, 'DEEPSEEK': 0.1513671875, 'GROK': 1.1920928955078125e-07, 'NOVA': 7.152557373046875e-07, 'OTHER': 2.9802322387695312e-05, 'HUMAN': 5.066394805908203e-05}}"
2509.06586,regular,post_llm,2025,9,"{'ai_likelihood': 0.00010967254638671875, 'text': 'Simulating Dispute Mediation with LLM-Based Agents for Legal Research\n\nLegal dispute mediation plays a crucial role in resolving civil disputes, yet its empirical study is limited by privacy constraints and complex multivariate interactions. To address this limitation, we present AgentMediation, the first LLM-based agent framework for simulating dispute mediation. It simulates realistic mediation processes grounded in real-world disputes and enables controlled experimentation on key variables such as disputant strategies, dispute causes, and mediator expertise. Our empirical analysis reveals patterns consistent with sociological theories, including Group Polarization and Surface-level Consensus. As a comprehensive and extensible platform, AgentMediation paves the way for deeper integration of social science and AI in legal research.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.18194,review,post_llm,2025,9,"{'ai_likelihood': 2.3610062069363066e-05, 'text': 'Deleuze\'s ""Postscript on the Societies of Control"" Updated for Big Data and Predictive Analytics\n\nIn 1990, Gilles Deleuze published Postscript on the Societies of Control, an introduction to the potentially suffocating reality of the nascent control society. This thirty-year update details how Deleuze\'s conception has developed from a broad speculative vision into specific economic mechanisms clustering around personal information, big data, predictive analytics, and marketing. The central claim is that today\'s advancing control society coerces without prohibitions, and through incentives that are not grim but enjoyable, even euphoric because they compel individuals to obey their own personal information. The article concludes by delineating two strategies for living that are as unexplored as control society itself because they are revealed and then enabled by the particular method of oppression that is control.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.05369,regular,post_llm,2025,9,"{'ai_likelihood': 5.695554945203993e-05, 'text': ""Influence of Stakeholder Involvement in M&E on the Performance of Donor-Funded Projects in Informal Settlements in Kisumu Central Sub-County, Kisumu County, Kenya\n\nStakeholder engagement and participatory approaches influence the effectiveness of donor-funded projects. Participatory Monitoring and Evaluation (PM&E) methodologies ensure that local communities play an active role in decision-making, leading to more sustainable outcomes. Given the complex socio-political landscape of Kisumu Central Sub-County, there is a critical need for inclusive and context-responsive project monitoring strategies. Initiatives that have integrated local leaders, youth groups, and women-led organizations into their M&E processes tend to achieve stronger community buy-in, improved continuity, and more impactful outcomes. The current study explored the influence of stakeholder involvement in M&E on the performance of donor-funded projects in informal settlements in Kisumu Central Sub-County, Kenya. The study was guided by Stakeholder Engagement Theory. The study used a Convergent Parallel design with a sample size of 364 respondents computed using Yamanes' Sampling formula, drawn from 27 donor-funded projects in Obunga and Nyalenda informal Settlements. Purposive sampling was used for project managers, project M&E staff, and community members served, while community members were selected using stratified random sampling. The study findings revealed that there were regular opportunities for stakeholder interaction in the projects (x =4.05, SD 1.08), stakeholders contributed to the development of the organization/project (x= 3.79, SD=.940), although stakeholders' perspectives and opinions were not diligently incorporated into programming (x=2.06, SD=.879) as anticipated. The study, therefore, concluded that stakeholder involvement in M&E influenced the performance of donor-funded projects. The study recommended that the project managers in donor-funded projects need to enhance stakeholder involvement for project ownership and sustainability."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.22334,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""MakOne: Behavioural Data of University Students' Smart Devices in Uganda\n\nUnderstanding student behaviour in higher education is essential for improving academic performance, supporting mental well-being, and informing institutional policies. However, most existing behavioural datasets originate from Western institutions and overlook the unique socioeconomic and infrastructural contexts of African institutions, limiting the global applicability of resulting insights. This paper introduces MakOne, a novel multimodal dataset collected over six weeks from 72 students at Makerere University, Kampala, using iLog, a mobile sensing application. The dataset integrates passive smartphone sensor data-including location, physical activity, and screen usage-with ecological momentary assessments (EMAs) that capture students' moods and daily routines. Designed to reflect the lived experiences of students in an African setting, MakOne offers a foundation for research in behaviour modeling, inclusive context-aware system design, mental health analytics, and culturally grounded educational technologies. It contributes a critical African perspective to the growing body of data-driven studies on student behaviour."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.872943878173828e-05, 'GPT4': 0.7001953125, 'CLAUDE': 0.00461578369140625, 'GOOGLE': 0.0019159317016601562, 'OPENAI_O_SERIES': 0.00021219253540039062, 'DEEPSEEK': 0.29296875, 'GROK': 5.960464477539062e-07, 'NOVA': 4.172325134277344e-07, 'OTHER': 2.2530555725097656e-05, 'HUMAN': 8.165836334228516e-05}}"
2509.04958,regular,post_llm,2025,9,"{'ai_likelihood': 0.029754638671875, 'text': 'Learning Multidimensional Urban Poverty Representation with Satellite Imagery\n\nRecent advances in deep learning have enabled the inference of urban socioeconomic characteristics from satellite imagery. However, models relying solely on urbanization traits often show weak correlations with poverty indicators, as unplanned urban growth can obscure economic disparities and spatial inequalities. To address this limitation, we introduce a novel representation learning framework that captures multidimensional deprivation-related traits from very high-resolution satellite imagery for precise urban poverty mapping. Our approach integrates three complementary traits: (1) accessibility traits, learned via contrastive learning to encode proximity to essential infrastructure; (2) morphological traits, derived from building footprints to reflect housing conditions in informal settlements; and (3) economic traits, inferred from nightlight intensity as a proxy for economic activity. To mitigate spurious correlations - such as those from non-residential nightlight sources that misrepresent poverty conditions - we incorporate a backdoor adjustment mechanism that leverages morphological traits during training of the economic module. By fusing these complementary features into a unified representation, our framework captures the complex nature of poverty, which often diverges from economic development trends. Evaluations across three capital cities - Cape Town, Dhaka, and Phnom Penh - show that our model significantly outperforms existing baselines, offering a robust tool for poverty mapping and policy support in data-scarce regions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.1821,regular,post_llm,2025,9,"{'ai_likelihood': 3.973642985026042e-07, 'text': 'Dark and Bright Patterns in Cookie Consent Requests\n\nDark patterns are (evil) design nudges that steer people\'s behaviour through persuasive interface design. Increasingly found in cookie consent requests, they possibly undermine principles of EU privacy law. In two preregistered online experiments we investigated the effects of three common design nudges (default, aesthetic manipulation, obstruction) on users\' consent decisions and their perception of control over their personal data in these situations. In the first experiment (N = 228) we explored the effects of design nudges towards the privacy-unfriendly option (dark patterns). The experiment revealed that most participants agreed to all consent requests regardless of dark design nudges. Unexpectedly, despite generally low levels of perceived control, obstructing the privacy-friendly option led to more rather than less perceived control. In the second experiment (N = 255) we reversed the direction of the design nudges towards the privacy-friendly option, which we title ""bright patterns"". This time the obstruction and default nudges swayed people effectively towards the privacy-friendly option, while the result regarding perceived control stayed the same compared to Experiment 1. Overall, our findings suggest that many current implementations of cookie consent requests do not enable meaningful choices by internet users, and are thus not in line with the intention of the EU policymakers. We also explore how policymakers could address the problem.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.1626,regular,post_llm,2025,9,"{'ai_likelihood': 0.01563178168402778, 'text': 'How Digital Transformation Impacts Corporate Green Innovation?\n\nDigitalization is a crucial characteristic of the current era, and green innovation has become one of the necessary pathways for enterprises to achieve sustainable development. Based on financial and annual report data of Chinese A-share listed companies from 2010 to 2019, this paper constructs indicators of corporate digital transformation and examines the impact of corporate digital transformation on green innovation and its underlying mechanisms. The results show that corporate digital transformation can promote corporate green innovation output, with its sustained future impact exhibiting a marginally decreasing trend. In terms of the impact mechanism, digital transformation can enhance corporate green innovation output by increasing corporate R&D investment and strengthening environmental management. Heterogeneity analysis reveals that digital transformation has a more pronounced promoting effect on green innovation output for small and medium-sized enterprises and those in technology-intensive industries. To improve the green innovation incentive effect of digital transformation, enterprises should formulate long-term strategies and continuously strengthen policy regulation and incentives.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.10571,review,post_llm,2025,9,"{'ai_likelihood': 3.311369154188368e-08, 'text': ""The main factors in student satisfaction with a campus environment: A mixed approach vs. a quantitative approach\n\nUniversity dropout rates in Morocco continue to increase, with approximately 49 percent of students leaving university before graduating, despite the successive reforms and measures taken to achieve Morocco's 2015_2030 strategic vision in the higher education sector : For a university of equity, quality and promotion, which raises questions about the state of knowledge on social inclusion at the university, capable of informing decision-making and the achievement of this strategic vision. While previous studies have used a quantitative approach with an exploratory purpose, to identify the main factors that affect the inclusion of students on university campuses. Knowledge that we consider insufficient to create general and regular knowledge, beyond the cases studied, on the exhaustiveness of these factors, no study has chosen a mixed approach (qualitative and quantitative) to create knowledge on the factors strengthening the attractiveness of the campus environment. Which brings us to our central question: How does a mixed approach promote the creation of general and regular knowledge on the factors enabling the inclusion of students in the campus environment?"", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.16294,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Balancing Innovation and Oversight: AI in the U.S. Treasury and IRS: A Survey\n\nThis paper explores how the U.S. Department of Treasury, particularly the Internal Revenue Service (IRS), is adopting artificial intelligence (AI) to modernize tax administration. Using publicly available information, the survey highlights the applications of AI for taxpayer support, operational efficiency, fraud detection, and audit optimization. Key initiatives include AI-powered chatbots, robotic process automation, machine learning for case selection, and advanced analytics for fraud prevention. These technologies aim to reduce errors, improve efficiency, and improve taxpayer experiences. At the same time, the IRS is implementing governance measures to ensure responsible use of AI, including privacy safeguards, transparency initiatives, and oversight mechanisms. The analysis shows that the Treasury AI strategy balances technological innovation with legal compliance, confidentiality, and public trust, reflecting a wider effort to modernize aging systems while maintaining accountability in tax collection and enforcement.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0020618438720703125, 'GPT4': 0.0302581787109375, 'CLAUDE': 0.06646728515625, 'GOOGLE': 0.00870513916015625, 'OPENAI_O_SERIES': 0.0009965896606445312, 'DEEPSEEK': 0.890625, 'GROK': 0.0002219676971435547, 'NOVA': 4.5299530029296875e-05, 'OTHER': 0.0007224082946777344, 'HUMAN': 1.895427703857422e-05}}"
2509.18195,regular,post_llm,2025,9,"{'ai_likelihood': 5.23527463277181e-05, 'text': 'Algorithmic A-Legality: Shorting the Human Future through AI\n\nThis article provides a necessary corrective to the belief that current legal and political concepts and institutions are capable of holding to account the power of new AI technologies. Drawing on jurisprudential analysis, it argues that while the current development of AI is dependent on the combination of economic and legal power, the technological forms that result increasingly exceed the capacity of even the most rigorous legal and political regimes. A situation of ""a-legality"" is emerging whereby the potential of AI to produce harms cannot be restrained by conventional legal or political institutions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.02774,review,post_llm,2025,9,"{'ai_likelihood': 2.284844716389974e-06, 'text': 'Computational Social Science and Critical Studies of Education and Technology: An Improbable Combination?\n\nAs belief around the potential of computational social science grows, fuelled by recent advances in machine learning, data scientists are ostensibly becoming the new experts in education. Scholars engaged in critical studies of education and technology have sought to interrogate the growing datafication of education yet tend not to use computational methods as part of this response. In this paper, we discuss the feasibility and desirability of the use of computational approaches as part of a critical research agenda. Presenting and reflecting upon two examples of projects that use computational methods in education to explore questions of equity and justice, we suggest that such approaches might help expand the capacity of critical researchers to highlight existing inequalities, make visible possible approaches for beginning to address such inequalities, and engage marginalised communities in designing and ultimately deploying these possibilities. Drawing upon work within the fields of Critical Data Studies and Science and Technology Studies, we further reflect on the two cases to discuss the possibilities and challenges of reimagining computational methods for critical research in education and technology, focusing on six areas of consideration: criticality, philosophy, inclusivity, context, classification, and responsibility.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.23848,regular,post_llm,2025,9,"{'ai_likelihood': 4.13921144273546e-06, 'text': 'Opinions can be Incorrect! In our Opinion. On the accuracy principle in data protection law\n\nThe GDPR contains an accuracy principle, as most data privacy laws in the world do. In principle, data controllers must ensure that personal data they use are accurate. Some have argued that the accuracy principle does not apply to personal data in the form of opinions about data subjects. We argue, however, from a positive law perspective, that the accuracy principle does apply to opinions. We further argue, from a normative perspective, that the accuracy principle should apply to opinions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.16286,regular,post_llm,2025,9,"{'ai_likelihood': 0.92431640625, 'text': ""What's Not on the Plate? Rethinking Food Computing through Indigenous Indian Datasets\n\nThis paper presents a multimodal dataset of 1,000 indigenous recipes from remote regions of India, collected through a participatory model involving first-time digital workers from rural areas. The project covers ten endangered language communities in six states. Documented using a dedicated mobile app, the data set includes text, images, and audio, capturing traditional food practices along with their ecological and cultural contexts. This initiative addresses gaps in food computing, such as the lack of culturally inclusive, multimodal, and community-authored data. By documenting food as it is practiced rather than prescribed, this work advances inclusive, ethical, and scalable approaches to AI-driven food systems and opens new directions in cultural AI, public health, and sustainable agriculture."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0008497238159179688, 'GPT4': 0.1329345703125, 'CLAUDE': 0.69775390625, 'GOOGLE': 0.0012216567993164062, 'OPENAI_O_SERIES': 8.0108642578125e-05, 'DEEPSEEK': 0.10992431640625, 'GROK': 4.410743713378906e-06, 'NOVA': 8.52346420288086e-06, 'OTHER': 0.0008416175842285156, 'HUMAN': 0.056182861328125}}"
2509.03171,regular,post_llm,2025,9,"{'ai_likelihood': 2.430544959174262e-05, 'text': 'Plan More, Debug Less: Applying Metacognitive Theory to AI-Assisted Programming Education\n\nThe growing adoption of generative AI in education highlights the need to integrate established pedagogical principles into AI-assisted learning environments. This study investigates the potential of metacognitive theory to inform AI-assisted programming education through a hint system designed around the metacognitive phases of planning, monitoring, and evaluation. Upon request, the system can provide three types of AI-generated hints--planning, debugging, and optimization--to guide students at different stages of problem-solving. Through a study with 102 students in an introductory data science programming course, we find that students perceive and engage with planning hints most highly, whereas optimization hints are rarely requested. We observe a consistent association between requesting planning hints and achieving higher grades across question difficulty and student competency. However, when facing harder tasks, students seek additional debugging but not more planning support. These insights contribute to the growing field of AI-assisted programming education by providing empirical evidence on the importance of pedagogical principles in AI-assisted learning.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.10578,review,post_llm,2025,9,"{'ai_likelihood': 0.016081068250868056, 'text': 'Ethical Frameworks for Conducting Social Challenge Studies\n\nComputational social science research, particularly online studies, often involves exposing participants to the adverse phenomenon the researchers aim to study. Examples include presenting conspiracy theories in surveys, exposing systems to hackers, or deploying bots on social media. We refer to these as ""social challenge studies,"" by analogy with medical research, where challenge studies advance vaccine and drug testing but also raise ethical concerns about exposing healthy individuals to risk. Medical challenge studies are guided by established ethical frameworks that regulate how participants are exposed to agents under controlled conditions. In contrast, social challenge studies typically occur with less control and fewer clearly defined ethical guidelines. In this paper, we examine the ethical frameworks developed for medical challenge studies and consider how their principles might inform social research. Our aim is to initiate discussion on formalizing ethical standards for social challenge studies and encourage long-term evaluation of potential harms.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.05689,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'Bridging the Gap Between Theoretical and Practical Reinforcement Learning in Undergraduate Education\n\nThis innovative practice category paper presents an innovative framework for teaching Reinforcement Learning (RL) at the undergraduate level. Recognizing the challenges posed by the complex theoretical foundations of the subject and the need for hands-on algorithmic practice, the proposed approach integrates traditional lectures with interactive lab-based learning. Drawing inspiration from effective pedagogical practices in computer science and engineering, the framework engages students through real-time coding exercises using simulated environments such as OpenAI Gymnasium. The effectiveness of this approach is evaluated through student surveys, instructor feedback, and course performance metrics, demonstrating improvements in understanding, debugging, parameter tuning, and model evaluation. Ultimately, the study provides valuable insight into making Reinforcement Learning more accessible and engaging, thereby equipping students with essential problem-solving skills for real-world applications in Artificial Intelligence.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0002593994140625, 'GPT4': 0.998046875, 'CLAUDE': 0.00011086463928222656, 'GOOGLE': 0.0013875961303710938, 'OPENAI_O_SERIES': 4.7326087951660156e-05, 'DEEPSEEK': 4.351139068603516e-06, 'GROK': 5.364418029785156e-07, 'NOVA': 1.0132789611816406e-06, 'OTHER': 4.7326087951660156e-05, 'HUMAN': 3.5762786865234375e-07}}"
2509.18509,regular,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': ""Developing a Decolonial Mindset for Indigenising Computing Education (CE)\n\nThe underrepresentation of First Peoples in computing education reflects colonial legacies embedded in curricula, pedagogies, and digital infrastructures. This paper introduces the \\textbf{Decolonial Mindset Stack (DMS)}, a seven-layer framework for educator transformation: \\textbf{Recognition, Reflection, Reframing, Reembedding, Reciprocity, Reclamation}, and \\textbf{Resurgence}. Grounded in Freirean critical pedagogy and Indigenous methodologies, the DMS aligns with relational lenses of ``About Me,'' ``Between Us,'' and ``By Us.'' It fosters self-reflexivity, relational accountability, and Indigenous sovereignty in computing education, reframing underrepresentation as systemic exclusion. The DMS provides both theoretical grounding and pathways for practice, positioning indigenisation not as an endpoint but as a sustained ethical commitment to transformative justice and the co-creation of computing education with First Peoples."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.728534698486328e-05, 'GPT4': 0.0019817352294921875, 'CLAUDE': 0.004436492919921875, 'GOOGLE': 0.0011005401611328125, 'OPENAI_O_SERIES': 9.357929229736328e-06, 'DEEPSEEK': 0.9912109375, 'GROK': 2.980232238769531e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 5.841255187988281e-06, 'HUMAN': 0.0010080337524414062}}"
2509.02462,review,post_llm,2025,9,"{'ai_likelihood': 0.01654730902777778, 'text': ""Can we cite Wikipedia? What if Wikipedia was more reliable than its detractors ?\n\nWikipedia, a widely successful encyclopedia recognized in academic circles and used by both students and professors alike, has led educators to question whether it can be cited as an information source, given its widespread use for this very purpose. The dilemma quickly emerged: if Wikipedia has become the go-to information source for so many, why can't it be cited? If consulting and using Wikipedia as a source of information is permitted, why does it become controversial the moment one attempts to cite it? This manuscript examines the systematic rejection of Wikipedia in academic settings, not to argue for its legitimacy as a source, but to demonstrate that its reliability is often underestimated while traditional academic sources enjoy disproportionate credibility, despite their increasingly apparent shortcomings. The central thesis posits that Wikipedia's rejection stems from an outdated epistemological bias that overlooks both the project's verification mechanisms and the structural crises affecting scientific publishing."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.22872,review,post_llm,2025,9,"{'ai_likelihood': 7.5499216715494794e-06, 'text': 'Anti-Regulatory AI: How ""AI Safety"" is Leveraged Against Regulatory Oversight\n\nAI companies increasingly develop and deploy privacy-enhancing technologies, bias-constraining measures, evaluation frameworks, and alignment techniques -- framing them as addressing concerns related to data privacy, algorithmic fairness, and AI safety. This paper examines the ulterior function of these technologies as mechanisms of legal influence. First, we examine how encryption, federated learning, and synthetic data -- presented as enhancing privacy and reducing bias -- can operate as mechanisms of avoidance with existing regulations in attempts to place data operations outside the scope of traditional regulatory frameworks. Second, we investigate how emerging AI safety practices including open-source model releases, evaluations, and alignment techniques can be used as mechanisms of change that direct regulatory focus towards industry-controlled voluntary standards and self-governance. We term this phenomenon ""anti-regulatory AI"" -- the deployment of ostensibly protective technologies that simultaneously shapes the terms of regulatory oversight. Our analysis additionally reveals how technologies\' anti-regulatory functions are enabled through framing that legitimizes their deployment while obscuring their use as regulatory workarounds. This paper closes with a discussion of policy implications that centers on the consideration of business incentives that drive AI development and the role of technical expertise in assessing whether these technologies fulfill their purported protections.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2509.22599,review,post_llm,2025,9,"{'ai_likelihood': 1.0, 'text': 'A Systematic Review: Affective Perception on Urban Facades\n\nArchitectural facades critically shape affective perception in urban environments. Here, affect is understood as a multidimensional psychological construct encompassing valence (pleasure-displeasure) and arousal (activation-deactivation). Despite growing interest in affective responses to the built environment, the affective impact of urban architectural facades remains under-theorized. This study conducts a systematic review of 61 works, guided by the PRISMA framework, to identify which facade attributes most strongly predict affective responses operationalized as valence and arousal. Through multi-scalar synthesis and knowledge mapping, the review highlights complexity, materiality, symmetry, and bibliophilic integration as consistent predictors of affective perception across urban, building, and detail levels. Computational tools such as eye-tracking, CNN-based analysis, and parametric modeling are increasingly employed, yet remain fragmented and often overlook intangible dimensions like narrative coherence and cultural symbolism. By consolidating cross-disciplinary evidence, this review proposes a theoretical model linking physical design features to affective outcomes, and identifies methodological gaps, particularly the lack of integrative, mixed-method approaches. The findings offer a foundation for affect-aware facade design, advancing evidence-based strategies to support psychological well-being in urban contexts.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.205371856689453e-06, 'GPT4': 0.03094482421875, 'CLAUDE': 0.002590179443359375, 'GOOGLE': 8.434057235717773e-05, 'OPENAI_O_SERIES': 5.960464477539062e-07, 'DEEPSEEK': 0.96630859375, 'GROK': 1.7881393432617188e-07, 'NOVA': 0.0, 'OTHER': 6.556510925292969e-07, 'HUMAN': 4.76837158203125e-07}}"
2510.02859,review,post_llm,2025,10,"{'ai_likelihood': 1.6887982686360678e-06, 'text': 'Strengthening legal protection against discrimination by algorithms and artificial intelligence\n\nAlgorithmic decision-making and other types of artificial intelligence (AI) can be used to predict who will commit crime, who will be a good employee, who will default on a loan, etc. However, algorithmic decision-making can also threaten human rights, such as the right to non-discrimination. The paper evaluates current legal protection in Europe against discriminatory algorithmic decisions. The paper shows that non-discrimination law, in particular through the concept of indirect discrimination, prohibits many types of algorithmic discrimination. Data protection law could also help to defend people against discrimination. Proper enforcement of non-discrimination law and data protection law could help to protect people. However, the paper shows that both legal instruments have severe weaknesses when applied to artificial intelligence. The paper suggests how enforcement of current rules can be improved. The paper also explores whether additional rules are needed. The paper argues for sector-specific - rather than general - rules, and outlines an approach to regulate algorithmic decision-making.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.10413,regular,post_llm,2025,10,"{'ai_likelihood': 2.9802322387695312e-06, 'text': 'Knowing Unknowns in an Age of Information Overload\n\nThe technological revolution of the Internet has digitized the social, economic, political, and cultural activities of billions of humans. While researchers have been paying due attention to concerns of misinformation and bias, these obscure a much less researched and equally insidious problem - that of uncritically consuming incomplete information. The problem of incomplete information consumption stems from the very nature of explicitly ranked information on digital platforms, where our limited mental capacities leave us with little choice but to consume the tip of a pre-ranked information iceberg. This study makes two chief contributions. First, we leverage the context of internet search to propose an innovative metric that quantifies information completeness. For a given search query, this refers to the extent of the information spectrum that is observed during web browsing. We then validate this metric using 6.5 trillion search results extracted from daily search trends across 48 nations for one year. Second, we find causal evidence that awareness of information completeness while browsing the Internet reduces resistance to factual information, hence paving the way towards an open-minded and tolerant mindset.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.21203,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'The Nuclear Analogy in AI Governance Research\n\nThe analogy between Artificial Intelligence (AI) and nuclear weapons is prominent in academic and policy discourse on AI governance. This chapter reviews 43 scholarly works which explicitly draw on the nuclear domain to derive lessons for AI governance. We identify four problem areas where researchers apply nuclear precedents: (1) early development and governance of transformative technologies; (2) international security risks and strategy; (3) international institutions and agreements; and (4) domestic safety regulation. While nuclear-inspired AI proposals are often criticised due to differences across domains, this review clarifies how historical analogies can inform policy development even when technological domains differ substantially. Valuable functions include providing conceptual frameworks for analyzing strategic dynamics, offering cautionary lessons about unsuccessful governance approaches, and expanding policy imagination by legitimizing radical proposals. Given that policymakers already invoke the nuclear analogy, continued critical engagement with these historical precedents remains essential for shaping effective global AI governance.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.8477439880371094e-06, 'GPT4': 1.3172626495361328e-05, 'CLAUDE': 1.0, 'GOOGLE': 1.9848346710205078e-05, 'OPENAI_O_SERIES': 6.079673767089844e-06, 'DEEPSEEK': 0.00010913610458374023, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 3.5762786865234375e-07, 'HUMAN': 1.2695789337158203e-05}}"
2510.18806,regular,post_llm,2025,10,"{'ai_likelihood': 8.609559800889757e-07, 'text': 'Integrating Large Language Models and Evaluating Student Outcomes in an Introductory Computer Science Course\n\nGenerative AI (GenAI) models have broad implications for education in general, impacting the foundations of what we teach and how we assess. This is especially true in computing, where LLMs tuned for coding have demonstrated shockingly good performance on the types of assignments historically used in introductory CS (CS1) courses. As a result, CS1 courses will need to change what skills are taught and how they are assessed. Computing education researchers have begun to study student use of LLMs, but there remains much to be understood about the ways that these tools affect student outcomes. In this paper, we present the design and evaluation of a new CS1 course at a large research-intensive university that integrates the use of LLMs as a learning tool for students. We describe the design principles used to create our new CS1-LLM course, our new course objectives, and evaluation of student outcomes and perceptions throughout the course as measured by assessment scores and surveys. Our findings suggest that 1) student exam performance outcomes, including differences among demographic groups, are largely similar to historical outcomes for courses without integration of LLM tools, 2) large, open-ended projects may be particularly valuable in an LLM context, and 3) students predominantly found the LLM tools helpful, although some had concerns regarding over-reliance on the tools.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.13465,review,post_llm,2025,10,"{'ai_likelihood': 5.231963263617622e-06, 'text': 'Discrimination, artificial intelligence, and algorithmic decision-making\n\nArtificial intelligence (AI) has a huge impact on our personal lives and also on our democratic society as a whole. While AI offers vast opportunities for the benefit of people, its potential to embed and perpetuate bias and discrimination remains one of the most pressing challenges deriving from its increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen Borgesius for the Anti-discrimination Department of the Council of Europe, elaborates on the risks of discrimination caused by algorithmic decision-making and other types of artificial intelligence (AI).', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.14457,regular,post_llm,2025,10,"{'ai_likelihood': 0.36675347222222227, 'text': 'Closing the Loop: An Instructor-in-the-Loop AI Assistance System for Supporting Student Help-Seeking in Programming Education\n\nTimely and high-quality feedback is essential for effective learning in programming courses; yet, providing such support at scale remains a challenge. While AI-based systems offer scalable and immediate help, their responses can occasionally be inaccurate or insufficient. Human instructors, in contrast, may bring more valuable expertise but are limited in time and availability. To address these limitations, we present a hybrid help framework that integrates AI-generated hints with an escalation mechanism, allowing students to request feedback from instructors when AI support falls short. This design leverages the strengths of AI for scale and responsiveness while reserving instructor effort for moments of greatest need. We deployed this tool in a data science programming course with 82 students. We observe that out of the total 673 AI-generated hints, students rated 146 (22%) as unhelpful. Among those, only 16 (11%) of the cases were escalated to the instructors. A qualitative investigation of instructor responses showed that those feedback instances were incorrect or insufficient roughly half of the time. This finding suggests that when AI support fails, even instructors with expertise may need to pay greater attention to avoid making mistakes. We will publicly release the tool for broader adoption and enable further studies in other classrooms. Our work contributes a practical approach to scaling high-quality support and informs future efforts to effectively integrate AI and humans in education.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.16459,review,post_llm,2025,10,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""Women have it Worse: an ICT Workplace Digital Transformation Stress Gender Gap\n\nAlthough information and communication technologies (ICT) solutions have positive outcomes for both companies and employees, the digital transformation (DT) could have an impact on the well-being of employees. The jobs of the employees became more demanding, and they were expected to learn ICT skills and cope with ICT workloads and hassles. Due to negative stereotypes about women's deficiency in technology, these ICT problems could affect female and male employees differently. Thus, we predicted that this additional pressure may manifest itself in higher levels of digital transformation stress (DTS) in female employees. The results confirmed this prediction and indicated the existence of a gender gap in DTS, measured two-fold - in sentiment analysis of help desk tickets and self-report using a psychological scale. Based on these results, we explore the need to discuss possible solutions and tools to support women in ICT-heavy workplace contexts."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.01895,review,post_llm,2025,10,"{'ai_likelihood': 6.986988915337457e-06, 'text': ""Online Behavioral Advertising: A Literature Review and Research Agenda\n\nAdvertisers are increasingly monitoring people's online behavior and using the information collected to show people individually targeted advertisements. This phenomenon is called online behavioral advertising (OBA). Although advertisers can benefit from OBA, the practice also raises concerns about privacy. Therefore, OBA has received much attention from advertisers, consumers, policymakers, and scholars. Despite this attention, there is neither a strong definition of OBA nor a clear accumulation of empirical findings. This article defines OBA and provides an overview of the empirical findings by developing a framework that identifies and integrates all factors that can explain consumer responses toward OBA. The framework suggests that the outcomes of OBA are dependent on advertiser-controlled factors (e.g., the level of personalization) and consumer-controlled factors (e.g., knowledge and perceptions about OBA and individual characteristics). The article also overviews the theoretical positioning of OBA by placing the theories that are used to explain consumers' responses to OBA in our framework. Finally, we develop a research agenda and discuss implications for policymakers and advertisers."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.05484,regular,post_llm,2025,10,"{'ai_likelihood': 0.4855685763888889, 'text': 'Evaluating LLM Safety Across Child Development Stages: A Simulated Agent Approach\n\nLarge Language Models (LLMs) are rapidly becoming part of tools used by children; however, existing benchmarks fail to capture how these models manage language, reasoning, and safety needs that are specific to various ages. We present ChildSafe, a benchmark that evaluates LLM safety through simulated child agents that embody four developmental stages. These agents, grounded in developmental psychology, enable a systematic study of child safety without the ethical implications of involving real children. ChildSafe assesses responses across nine safety dimensions (including privacy, misinformation, and emotional support) using age-weighted scoring in both sensitive and neutral contexts. Multi-turn experiments with multiple LLMs uncover consistent vulnerabilities that vary by simulated age, exposing shortcomings in existing alignment practices. By releasing agent templates, evaluation protocols, and an experimental corpus, we provide a reproducible framework for age-aware safety research. We encourage the community to expand this work with real child-centered data and studies, advancing the development of LLMs that are genuinely safe and developmentally aligned.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.12809,review,post_llm,2025,10,"{'ai_likelihood': 0.83154296875, 'text': 'High vs. Low AGI: Ontology and Conceptual Taxonomy for Geopolitical Coherence\n\nThe rapid progression of Artificial General Intelligence (AGI) research demands conceptual tools capable of distinguishing between systems developed for open, commercial integration and those destined for sovereign, securitized deployments. Without such distinctions, risk assessments and regulatory debates collapse AGI into legacy dual-use frameworks that are ill-suited for these resources, capturing the possibility of civilian and military application but overlooking the distinct societal lineages yielded by corporate and state-grade architectures. This paper proposes a taxonomy distinguishing low-AGI and high-AGI, clarifying how commercial-economic and security-sovereign architectures can be distinguished not only by function, but by the social and political ecosystems that produce them. The taxonomy builds on international relations concepts of ""high/low politics,"" viewed through the lens of construal-level theory, which allows it to even capture how cooperation and conflict may coexist in the context of AGI\'s emerging geopolitical stakes. By embedding AGI within power structures and securitization theory, this contribution extends dual-use discourse through an ontological taxonomy that enables more granular risk assessment and governance design--equipping policymakers and researchers to anticipate security dilemmas, institutional demands, and technical-political spillovers in the international system.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 9.775161743164062e-06, 'GPT4': 0.01690673828125, 'CLAUDE': 0.448486328125, 'GOOGLE': 1.9490718841552734e-05, 'OPENAI_O_SERIES': 5.245208740234375e-06, 'DEEPSEEK': 0.0572509765625, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.1324882507324219e-06, 'HUMAN': 0.477294921875}}"
2510.10588,regular,post_llm,2025,10,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'Making Power Explicable in AI: Analyzing, Understanding, and Redirecting Power to Operationalize Ethics in AI Technical Practice\n\nThe operationalization of ethics in the technical practices of artificial intelligence (AI) is facing significant challenges. To address the problem of ineffective implementation of AI ethics, we present our diagnosis, analysis, and interventional recommendations from a unique perspective of the real-world implementation of AI ethics through explainable AI (XAI) techniques. We first describe the phenomenon (i.e., the ""symptoms"") of ineffective implementation of AI ethics in explainable AI using four empirical cases. From the ""symptoms"", we diagnose the root cause (i.e., the ""disease"") being the dysfunction and imbalance of power structures in the sociotechnical system of AI. The power structures are dominated by unjust and unchecked power that does not represent the benefits and interests of the public and the most impacted communities, and cannot be countervailed by ethical power. Based on the understanding of power mechanisms, we propose three interventional recommendations to tackle the root cause, including: 1) Making power explicable and checked, 2) Reframing the narratives and assumptions of AI and AI ethics to check unjust power and reflect the values and benefits of the public, and 3) Uniting the efforts of ethical and scientific conduct of AI to encode ethical values as technical standards, norms, and methods, including conducting critical examinations and limitation analyses of AI technical practices. We hope that our diagnosis and interventional recommendations can be a useful input to the AI community and civil society\'s ongoing discussion and implementation of ethics in AI for ethical and responsible AI practice.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.19952,regular,post_llm,2025,10,"{'ai_likelihood': 2.751747767130534e-05, 'text': ""Synthetic social data: trials and tribulations\n\nLarge Language Models are being used in conversational agents that simulate human conversations and generate social studies data. While concerns about the models' biases have been raised and discussed in the literature, much about the data generated is still unknown. In this study we explore the statistical representation of social values across four countries (UK, Argentina, USA and China) for six LLMs, with equal representation for open and closed weights. By comparing machine-generated outputs with actual human survey data, we assess whether algorithmic biases in LLMs outweigh the biases inherent in real- world sampling, including demographic and response biases. Our findings suggest that, despite the logistical and financial constraints of human surveys, even a small, skewed sample of real respondents may provide more reliable insights than synthetic data produced by LLMs. These results highlight the limitations of using AI-generated text for social research and emphasize the continued importance of empirical human data collection."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22286,regular,post_llm,2025,10,"{'ai_likelihood': 0.4551866319444445, 'text': ""Hybrid Instructor Ai Assessment In Academic Projects: Efficiency, Equity, And Methodological Lessons\n\nIn technical subjects characterized by high enrollment, such as Basic Hydraulics, the assessment of reports necessitates superior levels of objectivity, consistency, and formative feedback; goals often compromised by faculty workload. This study presents the implementation of a generative artificial intelligence (AI) assisted assessment system, supervised by instructors, to grade 33 hydraulics reports. The central objective was to quantify its impact on the efficiency, quality, and fairness of the process. The employed methodology included the calibration of the Large Language Model (LLM) with a detailed rubric, the batch processing of assignments, and a human-in-the-loop validation phase. The quantitative results revealed a noteworthy 88% reduction in grading time (from 50 to 6 minutes per report, including verification) and a 733% increase in productivity. The quality of feedback was substantially improved, evidenced by 100% rubric coverage and a 150% increase in the anchoring of comments to textual evidence. The system proved to be equitable, exhibiting no bias related to report length, and highly reliable post-calibration (r = 0.96 between scores). It is concluded that the hybrid AI-instructor model optimizes the assessment process, thereby liberating time for high-value pedagogical tasks and enhancing the fairness and quality of feedback, in alignment with UNESCO's principles on the ethical use of AI in education."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.09145,review,post_llm,2025,10,"{'ai_likelihood': 0.0006771087646484375, 'text': 'Non-traditional data in pandemic preparedness and response: identifying and addressing first and last-mile challenges\n\nThe pandemic served as an important test case of complementing traditional public health data with non-traditional data (NTD) such as mobility traces, social media activity, and wearables data to inform decision-making. Drawing on an expert workshop and a targeted survey of European modelers, we assess the promise and persistent limitations of such data in pandemic preparedness and response. We distinguish between ""first-mile"" (accessing and harmonizing data) and ""last-mile"" challenges (translating insights into actionable interventions). The expert workshop held in 2024 brought together participants from public health, academia, policymakers, and industry to reflect on lessons learned and define strategies for translating NTD insights into policy making. The survey offers evidence of the barriers faced during COVID-19 and highlights key data unavailability and underuse. Our findings reveal ongoing issues with data access, quality, and interoperability, as well as institutional and cognitive barriers to evidence-based decision-making. Around 66% of datasets suffered access problem, with data sharing reluctance for NTD being double that of traditional data (30% vs 15%). Only 10% reported they could use all the data they needed. We propose a set of recommendations: for first-mile challenges, solutions focus on technical and legal frameworks for data access.; for last-mile challenges, we recommend fusion centers, decision accelerator labs, and networks of scientific ambassadors to bridge the gap between analysis and action. Realizing the full value of NTD requires a sustained investment in institutional readiness, cross-sectoral collaboration, and a shift toward a culture of data solidarity. Grounded in the lessons of COVID-19, the article can be used to design a roadmap for using NTD to confront a broader array of public health emergencies, from climate shocks to humanitarian crises.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.16052,regular,post_llm,2025,10,"{'ai_likelihood': 1.6887982686360678e-06, 'text': ""Reducing Procrastination on Programming Assignments via Optional Early Feedback\n\nAcademic procrastination is prevalent among undergraduate computer science students. Many studies have linked procrastination to poor academic performance and well-being. Procrastination is especially detrimental for advanced students when facing large, complex programming assignments in upper-year courses. We designed an intervention to combat academic procrastination on such programming assignments. The intervention consisted of early deadlines that were not worth marks but provided additional automated feedback if students submitted their work early. We evaluated the intervention by comparing the behaviour and performance of students between a control group and an intervention group. Our results showed that the intervention encouraged significantly more students to start the assignments early. Although there was no significant difference in students' grades between the control and intervention groups, students within the intervention group who used the intervention achieved significantly higher grades than those who did not. Our results implied that starting early alone did not improve students' grades. However, starting early and receiving additional feedback enhanced the students' grades relative to those of the rest of the students. We also conducted semi-structured interviews to gain an understanding of students' perceptions of the intervention. The interviews revealed that students benefited from the intervention in numerous ways, including improved academic performance, mental health, and development of soft skills. Students adopted the intervention to get more feedback, satisfy their curiosity, or use their available time. The main reasons for not adopting the intervention include having other competing deadlines, the intervention not being worth any marks, and feeling confident about their work."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.11742,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'The Ethics Engine: A Modular Pipeline for Accessible Psychometric Assessment of Large Language Models\n\nAs Large Language Models increasingly mediate human communication and decision-making, understanding their value expression becomes critical for research across disciplines. This work presents the Ethics Engine, a modular Python pipeline that transforms psychometric assessment of LLMs from a technically complex endeavor into an accessible research tool. The pipeline demonstrates how thoughtful infrastructure design can expand participation in AI research, enabling investigators across cognitive science, political psychology, education, and other fields to study value expression in language models. Recent adoption by University of Edinburgh researchers studying authoritarianism validates its research utility, processing over 10,000 AI responses across multiple models and contexts. We argue that such tools fundamentally change the landscape of AI research by lowering technical barriers while maintaining scientific rigor. As LLMs increasingly serve as cognitive infrastructure, their embedded values shape millions of daily interactions. Without systematic measurement of these value expressions, we deploy systems whose moral influence remains uncharted. The Ethics Engine enables the rigorous assessment necessary for informed governance of these influential technologies.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.76837158203125e-07, 'GPT4': 1.6450881958007812e-05, 'CLAUDE': 0.9990234375, 'GOOGLE': 9.119510650634766e-06, 'OPENAI_O_SERIES': 1.7881393432617188e-06, 'DEEPSEEK': 0.0008625984191894531, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 2.5033950805664062e-06}}"
2510.16858,review,post_llm,2025,10,"{'ai_likelihood': 0.0006151199340820312, 'text': 'Sustainable and Adaptive Growth in Computing Education\n\nComputing Education faces significant challenges in equipping graduates with the resilience necessary to remain relevant amid rapid technological change. While existing curricula cultivate computing competencies, they often fail to integrate strategies for sustaining and adapting these skills, leading to reduced career resilience and recurrent industry layoffs. The lack of educational emphasis on sustainability and adaptability amid industry changes perpetuates a vicious cycle: As industries shift, skill fragmentation and decay lead to displacement, which in turn causes further skill degradation. The ongoing deficiency in adaptability and sustainability among learners is reflected in the frequent and intense shifts across the industry. This issue is particularly evident in domains marked by high technological volatility such as computer graphics and game development, where computing concepts, including computational thinking and performance optimization, are uniquely and continuously challenged. To foster sustainable and adaptive growth, this paper introduces, a new framework which addresses the question: How can computing education and professional development be connected to in these volatile sectors? It integrates two iterative, interconnected cycles, an educational and a professional, by linking education with profession to establish a lifelong, renewable practice. This approach allows computing professionals to excel and maintain relevance amid constant changes across their industry.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.00077,regular,post_llm,2025,10,"{'ai_likelihood': 2.9901663462320965e-05, 'text': ""What is the Return on Investment of Digital Engineering for Complex Systems Development? Findings from a Mixed-Methods Study on the Post-production Design Change Process of Navy Assets\n\nComplex engineered systems routinely face schedule and cost overruns, along with poor post-deployment performance. Championed by both INCOSE and the U.S. Department of Defense (DoD), the systems engineering (SE) community has increasingly looked to Digital Engineering (DE) as a potential remedy. Despite this growing advocacy, most of DE's purported benefits remain anecdotal, and its return on investment (ROI) remains poorly understood. This research presents findings from a case study on a Navy SE team responsible for the preliminary design phase of post-production design change projects for Navy assets. Using a mixed-methods approach, we document why complex system sustainment projects are routinely late, where and to what extent schedule slips arise, and how a DE transformation could improve schedule adherence. This study makes three contributions. First, it identifies four archetypical inefficiency modes that drive schedule overruns and explains how these mechanisms unfold in their organizational context. Second, it quantifies the magnitude and variation of schedule slips. Third, it creates a hypothetical digitally transformed version of the current process, aligned with DoD DE policy, and compares it to the current state to estimate potential schedule gains. Our findings suggest that a DE transformation could reduce the median project duration by 50.1% and reduce the standard deviation by 41.5%, leading to faster and more predictable timelines. However, the observed gains are not uniform across task categories. Overall, this study provides initial quantitative evidence of DE's potential ROI and its value in improving the efficiency and predictability of complex system sustainment projects."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.27489,regular,post_llm,2025,10,"{'ai_likelihood': 0.034417046440972224, 'text': 'Auditing LLM Editorial Bias in News Media Exposure\n\nLarge Language Models (LLMs) increasingly act as gateways to web content, shaping how millions of users encounter online information. Unlike traditional search engines, whose retrieval and ranking mechanisms are well studied, the selection processes of web-connected LLMs add layers of opacity to how answers are generated. By determining which news outlets users see, these systems can influence public opinion, reinforce echo chambers, and pose risks to civic discourse and public trust.\n  This work extends two decades of research in algorithmic auditing to examine how LLMs function as news engines. We present the first audit comparing three leading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against Google News, asking: \\textit{How do LLMs differ from traditional aggregators in the diversity, ideology, and reliability of the media they expose to users?}\n  Across 24 global topics, we find that, compared to Google News, LLMs surface significantly fewer unique outlets and allocate attention more unevenly. In the same way, GPT-4o-Mini emphasizes more factual and right-leaning sources; Claude-3.7-Sonnet favors institutional and civil-society domains and slightly amplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest left-leaning tilt without significant changes in factuality. These patterns remain robust under prompt variations and alternative reliability benchmarks. Together, our findings show that LLMs already enact \\textit{agentic editorial policies}, curating information in ways that diverge from conventional aggregators. Understanding and governing their emerging editorial power will be critical for ensuring transparency, pluralism, and trust in digital information ecosystems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.05519,review,post_llm,2025,10,"{'ai_likelihood': 5.364418029785156e-06, 'text': 'Assessing Human Rights Risks in AI: A Framework for Model Evaluation\n\nThe Universal Declaration of Human Rights and other international agreements outline numerous inalienable rights that apply across geopolitical boundaries. As generative AI becomes increasingly prevalent, it poses risks to human rights such as non-discrimination, health, and security, which are also central concerns for AI researchers focused on fairness and safety. We contribute to the field of algorithmic auditing by presenting a framework to computationally assess human rights risk. Drawing on the UN Guiding Principles on Business and Human Rights, we develop an approach to evaluating a model to make grounded claims about the level of risk a model poses to particular human rights. Our framework consists of three parts: selecting tasks that are likely to pose human rights risks within a given context, designing metrics to measure the scope, scale, and likelihood of potential risks from that task, and analyzing rights with respect to the values of those metrics. Because a human rights approach centers on real-world harms, it requires evaluating AI systems in the specific contexts in which they are deployed. We present a case study of large language models in political news journalism, demonstrating how our framework helps to design an evaluation and benchmarking different models. We then discuss the implications of the results for the rights of access to information and freedom of thought and broader considerations for adopting this approach.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.11064,regular,post_llm,2025,10,"{'ai_likelihood': 5.629327562120226e-07, 'text': ""Detecting Gender Stereotypes in Scratch Programming Tutorials\n\nGender stereotypes in introductory programming courses often go unnoticed, yet they can negatively influence young learners' interest and learning, particularly under-represented groups such as girls. Popular tutorials on block-based programming with Scratch may unintentionally reinforce biases through character choices, narrative framing, or activity types. Educators currently lack support in identifying and addressing such bias. With large language models~(LLMs) increasingly used to generate teaching materials, this problem is potentially exacerbated by LLMs trained on biased datasets. However, LLMs also offer an opportunity to address this issue. In this paper, we explore the use of LLMs for automatically identifying gender-stereotypical elements in Scratch tutorials, thus offering feedback on how to improve teaching content. We develop a framework for assessing gender bias considering characters, content, instructions, and programming concepts. Analogous to how code analysis tools provide feedback on code in terms of code smells, we operationalise this framework using an automated tool chain that identifies *gender stereotype smells*. Evaluation on 73 popular Scratch tutorials from leading educational platforms demonstrates that stereotype smells are common in practice. LLMs are not effective at detecting them, but our gender bias evaluation framework can guide LLMs in generating tutorials with fewer stereotype smells."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.09374,review,post_llm,2025,10,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Challenges in designing ethical rules for Infrastructures in Internet of Vehicles\n\nVehicular Ad-hoc Networks (VANETs) have seen significant advancements in technology. Innovation in connectivity and communication has brought substantial capabilities to various components of VANETs such as vehicles, infrastructures, passengers, drivers and affiliated environmental sensors. Internet of Things (IoT) has brought the notion of Internet of Vehicles (IoV) to VANETs where each component of VANET is connected directly or indirectly to the Internet. Vehicles and infrastructures are key components of a VANET system that can greatly augment the overall experience of the network by integrating the competencies of Vehicle to Vehicle (V2V), Vehicle to Pedestrian (V2P), Vehicle to Sensor (V2S), Vehicle to Infrastructure (V2I) and Infrastructure to Infrastructure (I2I). Internet connectivity in Vehicles and Infrastructures has immensely expanded the potential of developing applications for VANETs under the broad spectrum of IoV. Advent in the use of technology in VANETs requires considerable efforts in scheming the ethical rules for autonomous systems. Currently, there is a gap in literature that focuses on the challenges involved in designing ethical rules or policies for infrastructures, sometimes referred to as Road Side Units (RSUs) for IoVs. This paper highlights the key challenges entailing the design of ethical rules for RSUs in IoV systems. Furthermore, the article also proposes major ethical principles for RSUs in IoV systems that would set foundation for modeling future IoV architectures.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.27248,review,post_llm,2025,10,"{'ai_likelihood': 1.953707800971137e-06, 'text': 'The Role of Search Engines in the Amplification and Suppression of LGBTIQ+ Polarization\n\nSearch engines are used and trusted by hundreds of millions of people every day. However, the algorithms used by search engines to index, filter, and rank web content are inherently biased, and will necessarily prefer some views and opinions at the expense of others. In this article, we examine how these algorithmic biases amplify and suppress polarizing content. Polarization refers to a shift toward and the acceptance of ideological extremes. In Europe, polarizing content in relation to LGBTIQ+ issues has been a feature of various ideological and political conflicts. Although past research has focused on the role of social media in polarization, the role of search engines in this process is little understood. Here, we report on a large-scale study of 1.5 million search results responding to neutral and negative queries relating to LGBTIQ+ issues. Focusing on the UK, Germany, and France, our analysis shows that the choice of search engine is the key determinant of exposure to polarizing content, followed by the polarity of the query. Location and language, on the other hand, have a comparatively minor effect. Consequently, our findings provide quantitative insights into how differences between search engine technologies, rather than the opinions, language, and location of web users, have the greatest impact on the exposure of web users to polarizing Web content.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.08247,review,post_llm,2025,10,"{'ai_likelihood': 4.0398703681098095e-06, 'text': ""The Right to Communications Confidentiality in Europe: Protecting Privacy, Freedom of Expression, and Trust\n\nIn the European Union, the General Data Protection Regulation (GDPR) provides comprehensive rules for the processing of personal data. In addition, the EU lawmaker intends to adopt specific rules to protect confidentiality of communications, in a separate ePrivacy Regulation. Some have argued that there is no need for such additional rules for communications confidentiality. This Article discusses the protection of the right to confidentiality of communications in Europe. We look at the right's origins to assess the rationale for protecting it. We also analyze how the right is currently protected under the European Convention on Human Rights and under EU law. We show that at its core the right to communications confidentiality protects three individual and collective values: privacy, freedom of expression, and trust in communication services. The right aims to ensure that individuals and organizations can safely entrust communication to service providers. Initially, the right protected only postal letters, but it has gradually developed into a strong safeguard for the protection of confidentiality of communications, regardless of the technology used. Hence, the right does not merely serve individual privacy interests, but also other more collective interests that are crucial for the functioning of our information society. We conclude that separate EU rules to protect communications confidentiality, next to the GDPR, are justified and necessary."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.09249,regular,post_llm,2025,10,"{'ai_likelihood': 0.4801432291666667, 'text': 'Exploring User Risk Factors and Target Groups for Phishing Victimization in Pakistan\n\nPhishing attacks pose a significant cybersecurity threat globally. This study investigates phishing susceptibility within the Pakistani population, examining the influence of demographic factors, technological aptitude and usage, previous phishing victimization, and email characteristics. Data was collected through convenient sampling; a total of 164 people completed the questionnaire. Contrary to some assumptions, the results indicate that men, individuals over 25, employed persons and frequent online shoppers have relatively high phishing susceptibility. The characteristics of email significantly affected phishing victimization, with authority and urgency signaling increasing susceptibility, while risk cues sometimes improved vigilance. In particular, users were more susceptible to emails from communication services such as Gmail and LinkedIn compared to government or social media sources. These findings highlight the need for targeted security awareness interventions tailored to specific demographics and email types. A multifaceted approach combining technology and education is crucial to combat phishing attacks.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.25049,regular,post_llm,2025,10,"{'ai_likelihood': 4.503462049696181e-06, 'text': 'Teaching Probabilistic Machine Learning in the Liberal Arts: Empowering Socially and Mathematically Informed AI Discourse\n\nWe present a new undergraduate ML course at our institution, a small liberal arts college serving students minoritized in STEM, designed to empower students to critically connect the mathematical foundations of ML with its sociotechnical implications. We propose a ""framework-focused"" approach, teaching students the language and formalism of probabilistic modeling while leveraging probabilistic programming to lower mathematical barriers. We introduce methodological concepts through a whimsical, yet realistic theme, the ""Intergalactic Hypothetical Hospital,"" to make the content both relevant and accessible. Finally, we pair each technical innovation with counter-narratives that challenge its value using real, open-ended case-studies to cultivate dialectical thinking. By encouraging creativity in modeling and highlighting unresolved ethical challenges, we help students recognize the value and need of their unique perspectives, empowering them to participate confidently in AI discourse as technologists and critical citizens.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.13466,review,post_llm,2025,10,"{'ai_likelihood': 9.040037790934246e-06, 'text': ""The Perfect Match? A Closer Look at the Relationship between EU Consumer Law and Data Protection Law\n\nIn modern markets, many companies offer so-called 'free' services and monetize consumer data they collect through those services. This paper argues that consumer law and data protection law can usefully complement each other. Data protection law can also inform the interpretation of consumer law. Using consumer rights, consumers should be able to challenge excessive collection of their personal data. Consumer organizations have used consumer law to tackle data protection infringements. The interplay of data protection law and consumer protection law provides exciting opportunities for a more integrated vision on 'data consumer law'."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.1605,regular,post_llm,2025,10,"{'ai_likelihood': 4.0531158447265625e-05, 'text': 'A Framework For Decentralized Micro-credential Verification Towards Higher Qualifications\n\nStudent retention is one of the rising problems seen in educational institutions. With the rising cost of education and issues in the education sector, such as curriculum relevance, student engagement, and rapidly changing technological advancements, ensuring the relevance of academic programs in a fast-evolving job market has created a significant concern for educational institutions. With the intent to adapt to such challenges, educational institutions are dealing with alternative solutions for education, in which micro-credentials are at the very center of this, which are short-term academic programs or standalone courses. However, one of the challenges of micro-credentials is a lack of credit transfer among institutions. With the lack of standardization of assessments among educational institutions, it is difficult to transfer micro-credentials to larger qualifications. Regarding such challenges, micro-credentials with blockchain technology can bring significant benefits. Blockchain technology offers a decentralized and immutable platform for securely storing and verifying credentials. This paper presents a prototype model for micro-credential verification. With the policies decided by the educational institution, the learner provides a micro-credential certificate to the system. Upon validation of the certificate by the verifying body, the educational institution will review the assessment criteria and provide exemptions based on the provided criteria. The prototype uses the Hyper-ledger Fabric platform and utilizes off-chain technology, which acts as a middle-man storage platform. With the combination of off-chain and on-chain technologies, congestion on the blockchain is reduced, and transaction speed is improved. In summary, this research proposes a prototype for secure micro-credential verification and a more efficient course exemption process.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.24958,regular,post_llm,2025,10,"{'ai_likelihood': 1.069572236802843e-05, 'text': 'Adaptive Data Collection for Latin-American Community-sourced Evaluation of Stereotypes (LACES)\n\nThe evaluation of societal biases in NLP models is critically hindered by a glaring geo-cultural gap, as existing benchmarks are overwhelmingly English-centric and focused on U.S. demographics. This leaves regions such as Latin America severely underserved, making it impossible to adequately assess or mitigate the perpetuation of harmful regional stereotypes by language technologies. To address this gap, we introduce a new, large-scale dataset of stereotypes developed through targeted community partnerships within Latin America. Furthermore, we present a novel dynamic data collection methodology that uniquely integrates the sourcing of new stereotype entries and the validation of existing data within a single, unified workflow. This combined approach results in a resource with significantly broader coverage and higher regional nuance than static collection methods. We believe that this new method could be applicable in gathering sociocultural knowledge of other kinds, and that this dataset provides a crucial new resource enabling robust stereotype evaluation and significantly addressing the geo-cultural deficit in fairness resources for Latin America.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.12814,regular,post_llm,2025,10,"{'ai_likelihood': 0.005243089463975695, 'text': 'Cyber Slavery Infrastructures: A Socio-Technical Study of Forced Criminality in Transnational Cybercrime\n\nThe rise of ``cyber slavery,"" a technologically facilitated variant of forced criminality, signifies a concerning convergence of human trafficking and digital exploitation. In Southeast Asia, trafficked individuals are increasingly coerced into engaging in cybercrimes, including online fraud and financial phishing, frequently facilitated by international organized criminal networks. This study adopts a hybrid qualitative-computational methodology, combining a systematic narrative review with case-level metadata extracted from real-world cyber trafficking incidents through collaboration with Indian law enforcement agencies. We introduce a five-tier victimization framework that outlines the sequential state transitions of cyber-slavery victims, ranging from initial financial deception to physical exploitation, culminating in systemic prosecution through trace-based misattribution. Furthermore, our findings indicate that a significant socio-technical risk of cyber slavery is its capacity to evolve from forced to voluntary digital criminality, as victims, initially compelled to engage in cyber-enabled crimes, may choose to persist in their involvement due to financial incentives and the perceived security provided by digital anonymity. This legal-technological gap hampers victim identification processes, imposing excessive pressure on law enforcement systems dependent on binary legal categorizations, which ultimately hinders the implementation of victim-centered investigative methods and increases the likelihood of prosecutorial misclassification, thus reinforcing the structural obstacles to addressing cyber slavery.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.16366,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'Integrating LLM and Diffusion-Based Agents for Social Simulation\n\nAgent-based social simulation provides a valuable methodology for predicting social information diffusion, yet existing approaches face two primary limitations. Traditional agent models often rely on rigid behavioral rules and lack semantic understanding of textual content, while emerging large language model (LLM)-based agents incur prohibitive computational costs at scale. To address these challenges, we propose a hybrid simulation framework that strategically integrates LLM-driven agents with diffusion model-based agents. The framework employs LLM-based agents to simulate a core subset of users with rich semantic reasoning, while a diffusion model handles the remaining population efficiently. Although the two agent types operate on disjoint user groups, both incorporate key factors including user personalization, social influence, and content awareness, and interact through a coordinated simulation process. Extensive experiments on three real-world datasets demonstrate that our framework outperforms existing methods in prediction accuracy, validating the effectiveness of its modular design.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004639625549316406, 'GPT4': 0.01561737060546875, 'CLAUDE': 0.87890625, 'GOOGLE': 0.00766754150390625, 'OPENAI_O_SERIES': 0.0010538101196289062, 'DEEPSEEK': 0.09429931640625, 'GROK': 8.940696716308594e-07, 'NOVA': 1.1265277862548828e-05, 'OTHER': 0.000835418701171875, 'HUMAN': 0.0009136199951171875}}"
2510.08246,review,post_llm,2025,10,"{'ai_likelihood': 1.986821492513021e-06, 'text': ""Does everyone have a price? Understanding people's attitude towards online and offline price discrimination\n\nOnline stores can present a different price to each customer. Such algorithmic personalised pricing can lead to advanced forms of price discrimination based on the characteristics and behaviour of individual consumers. We conducted two consumer surveys among a representative sample of the Dutch population (N=1233 and N=1202), to analyse consumer attitudes towards a list of examples of price discrimination and dynamic pricing. A vast majority finds online price discrimination unfair and unacceptable, and thinks it should be banned. However, some pricing strategies that have been used by companies for decades are almost equally unpopular. We analyse the results to better understand why people dislike many types of price discrimination."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.07519,review,post_llm,2025,10,"{'ai_likelihood': 9.139378865559896e-06, 'text': 'Digital Innovation in Microenterprises: Current Trends and New Research Avenues\n\nThe relationship between microenterprises and information and communication technologies (ICTs) has always been troublesome. Because of the rapid pace of modern digital technologies, digital innovation processes are permeating the industries, markets, and social contexts in which microenterprises exist today. However, microenterprises have severe difficulties engaging or performing in these digital contexts and are at risk of being left behind. This paper reviews the literature on ICTs and microenterprises, focusing on the adoption, usage, and impact of ICTs. The results indicate that further research in this field should avoid focusing on individual microenterprises (or samples of independent microenterprises) as the unit of analysis and should favour a systemic approach in which markets, value chains, or microenterprise-intensive sectors are studied. Additionally, theoretical frameworks capable of considering change and the dynamic nature of innovation processes are highlighted as a critical focus area for the field.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.06279,regular,post_llm,2025,10,"{'ai_likelihood': 2.682209014892578e-06, 'text': 'Technical Overview of Safe3Step (S3S): Power Ratings and quality wins for selecting at-large teams to the NCAA Division I Men\'s Lacrosse Championship\n\nThis document describes a system for selecting teams to the NCAA Men\'s Division I Lacrosse Championship Tournament called ""Safe3Step"" (S3S) that was developed in conversation with the NCAA Lacrosse Selection Criteria and Ranking Committee (SCR) with the objective of improving on the Ratings Percentage Index (RPI). S3S employs three steps that: 1) evaluate the strength of each team based on score data, 2) award S3S points to each team based on the quality of its wins and losses, ranking teams accordingly, and 3) examine each pair of teams with adjacent rankings, swapping ranks if the lower-ranked team has a better head-to-head record against the higher-ranked team. Safe3Step is not entirely new, but it improves on other ""quality win"" methods by using Power Ratings to identify team strengths, respecting head-to-head records, and adhering to standards of simplicity, transparency, and objectivity. Empirical analysis is left to future work.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.03329,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'When Patients Go to ""Dr. Google"" Before They Go to the Emergency Department\n\nApproximately one-third of adults search the internet for health information before visiting an emergency department (ED), with 75% encountering inaccurate content. This study examines how such searches influence patient care. We conducted an observational study of ED visits over a 12-month period, surveying 214 of 576 patients about pre-ED internet use. Data on demographics, comorbidities, acuity, orders, prescriptions, and dispositions were extracted. Patients who searched were typically younger, healthier, and more educated. Most used a general search engine to ask symptom-related questions. Compared to non-searchers, they were less likely to receive lab tests (RR 0.78, p=0.053), imaging (RR 0.75, p=0.094), medications (RR 0.67, p=0.038), or admission (RR 0.68, p=0.175). They were more likely to leave against medical advice (RR 1.67, p=0.067) and receive opioids (RR 1.56, p=0.151). Findings suggest inaccurate health information may contribute to mismatched expectations and altered care delivery.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.950429916381836e-05, 'GPT4': 4.649162292480469e-05, 'CLAUDE': 0.994140625, 'GOOGLE': 0.0005159378051757812, 'OPENAI_O_SERIES': 8.618831634521484e-05, 'DEEPSEEK': 0.004833221435546875, 'GROK': 1.0132789611816406e-06, 'NOVA': 7.152557373046875e-07, 'OTHER': 1.2516975402832031e-05, 'HUMAN': 0.0001926422119140625}}"
2510.09698,regular,post_llm,2025,10,"{'ai_likelihood': 1.0563267601860894e-05, 'text': 'Norwegian Electricity in Geographic Dataset (NoreGeo)\n\nGeographic data is vital in understanding, analyzing, and contextualizing energy usage at the regional level within electricity systems. While geospatial visualizations of electricity infrastructure and distributions of production and consumption are available from governmental and third-party sources, these sources are often disparate, and compatible geographic datasets remain scarce. In this paper, we present a comprehensive geographic dataset representing the electricity system in Norway. We collect data from multiple authoritative sources, process it into widely accepted formats, and generate interactive maps based on this data. Our dataset includes information for each municipality in Norway for the year 2024, encompassing electricity infrastructure, consumption, renewable and conventional production, main power grid topology, relevant natural resources, and population demographics. This work results in a formatted geographic dataset that integrates diverse informational resources, along with openly released interactive maps. We anticipate that our dataset will alleviate software incompatibilities in data retrieval, and facilitate joint analyses on regional electricity system for energy researchers, stakeholders, and developers.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.08792,regular,post_llm,2025,10,"{'ai_likelihood': 2.914004855685764e-06, 'text': 'Assurance of Frontier AI Built for National Security\n\nThis memorandum presents four recommendations aimed at strengthening the principles of AI model reliability and AI model governability, as DoW, ODNI, NIST, and CAISI refine AI assurance frameworks under the AI Action Plan. Our focus concerns the open scientific problem of misalignment and its implications on AI model behavior. Specifically, misalignment and scheming capabilities can be a red flag indicating AI model insufficient reliability and governability. To address the national security threats arising from misalignment, we recommend that DoW and the IC strategically leverage existing testing and evaluation pipelines and their OT authority to future proof the principles of AI model reliability and AI model governability through a suite of scheming and control evaluations.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.25417,review,post_llm,2025,10,"{'ai_likelihood': 0.9951171875, 'text': 'Shifts in U.S. Social Media Use, 2020-2024: Decline, Fragmentation, and Enduring Polarization\n\nUsing nationally representative data from the 2020 and 2024 American National Election Studies (ANES), this paper traces how the U.S. social media landscape has shifted across platforms, demographics, and politics. Overall platform use has declined, with the youngest and oldest Americans increasingly abstaining from social media altogether. Facebook, YouTube, and Twitter/X have lost ground, while TikTok and Reddit have grown modestly, reflecting a more fragmented digital public sphere. Platform audiences have aged and become slightly more educated and diverse. Politically, most platforms have moved toward Republican users while remaining, on balance, Democratic-leaning. Twitter/X has experienced the sharpest shift: posting has flipped nearly 50 percentage points from Democrats to Republicans. Across platforms, political posting remains tightly linked to affective polarization, as the most partisan users are also the most active. As casual users disengage and polarized partisans remain vocal, the online public sphere grows smaller, sharper, and more ideologically extreme.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.482269287109375e-05, 'GPT4': 0.0017099380493164062, 'CLAUDE': 0.79345703125, 'GOOGLE': 0.0014858245849609375, 'OPENAI_O_SERIES': 4.8279762268066406e-05, 'DEEPSEEK': 0.196044921875, 'GROK': 5.960464477539062e-07, 'NOVA': 3.039836883544922e-06, 'OTHER': 7.94529914855957e-05, 'HUMAN': 0.0069122314453125}}"
2510.20149,regular,post_llm,2025,10,"{'ai_likelihood': 3.5762786865234375e-06, 'text': 'Dependency-Aware Task Offloading in Multi-UAV Assisted Collaborative Mobile Edge Computing\n\nThis paper proposes a novel multi-unmanned aerial vehicle (UAV) assisted collaborative mobile edge computing (MEC) framework, where the computing tasks of terminal devices (TDs) can be decomposed into serial or parallel sub-tasks and offloaded to collaborative UAVs. We first model the dependencies among all sub-tasks as a directed acyclic graph (DAG) and design a two-timescale frame structure to decouple the sub-task interdependencies for sub-task scheduling. Then, a joint sub-task offloading, computational resource allocation, and UAV trajectories optimization problem is formulated, which aims to minimize the system cost, i.e., the weighted sum of the task completion delay and the system energy consumption. To solve this non-convex mixed-integer nonlinear programming (MINLP) problem, a penalty dual decomposition and successive convex approximation (PDD-SCA) algorithm is developed. Particularly, the original MINLP problem is equivalently transferred into a continuous form relying on PDD theory. By decoupling the resulting problem into three nested subproblems, the SCA method is further combined to recast the non-convex components and obtain desirable solutions. Numerical results demonstrate that: 1) Compared to the benchmark algorithms, the proposed scheme can significantly reduce the system cost, and thus realize an improved trade-off between task latency and energy consumption; 2) The proposed algorithm can achieve an efficient workload balancing for distributed computation across multiple UAVs.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.17711,review,post_llm,2025,10,"{'ai_likelihood': 2.0199351840549047e-06, 'text': 'Discrimination, intelligence artificielle et decisions algorithmiques\n\nArtificial intelligence (AI) has a huge impact on our personal lives and also on our democratic society as a whole. While AI offers vast opportunities for the benefit of people, its potential to embed and perpetuate bias and discrimination remains one of the most pressing challenges deriving from its increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen Borgesius for the Anti-discrimination Department of the Council of Europe, elaborates on the risks of discrimination caused by algorithmic decision-making and other types of artificial intelligence (AI).', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.21219,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'World Models Should Prioritize the Unification of Physical and Social Dynamics\n\nWorld models, which explicitly learn environmental dynamics to lay the foundation for planning, reasoning, and decision-making, are rapidly advancing in predicting both physical dynamics and aspects of social behavior, yet predominantly in separate silos. This division results in a systemic failure to model the crucial interplay between physical environments and social constructs, rendering current models fundamentally incapable of adequately addressing the true complexity of real-world systems where physical and social realities are inextricably intertwined. This position paper argues that the systematic, bidirectional unification of physical and social predictive capabilities is the next crucial frontier for world model development. We contend that comprehensive world models must holistically integrate objective physical laws with the subjective, evolving, and context-dependent nature of social dynamics. Such unification is paramount for AI to robustly navigate complex real-world challenges and achieve more generalizable intelligence. This paper substantiates this imperative by analyzing core impediments to integration, proposing foundational guiding principles (ACE Principles), and outlining a conceptual framework alongside a research roadmap towards truly holistic world models.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 2.384185791015625e-07, 'GPT4': 0.0024929046630859375, 'CLAUDE': 0.0009126663208007812, 'GOOGLE': 0.0006618499755859375, 'OPENAI_O_SERIES': 2.384185791015625e-07, 'DEEPSEEK': 0.99609375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.1920928955078125e-07, 'HUMAN': 3.463029861450195e-05}}"
2510.21526,review,post_llm,2025,10,"{'ai_likelihood': 7.947285970052084e-07, 'text': ""Recommended Practices for NPOV Research on Wikipedia\n\nWriting Wikipedia with a neutral point of view is one of the five pillars of Wikipedia. Although the topic is core to Wikipedia, it is relatively understudied considering hundreds of research studies are published annually about the project. We hypothesize that part of the reason for the low research activity on the topic is that Wikipedia's definition of neutrality and its importance are not well understood within the research community. Neutrality is also an inherently challenging and contested concept. Our aim with this paper is to accelerate high quality research in this space that can help Wikipedia communities continue to improve their work in writing the encyclopedia. We do this by helping researchers to learn what Neutral Point of View means in the context of Wikipedia, identifying some common challenges with studying NPOV and how to navigate them, and offering guidance on how researchers can communicate the results of their work for increased impact on the ground for the benefit of Wikipedia."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.13468,review,post_llm,2025,10,"{'ai_likelihood': 1.2715657552083334e-05, 'text': ""Privacy, freedom of expression, and the right to be forgotten in Europe\n\nIn this chapter we discuss the relation between privacy and freedom of expression in Europe. In principle, the two rights have equal weight in Europe - which right prevails depends on the circumstances of a case. We use the Google Spain judgment of the Court of Justice of the European Union, sometimes called the 'right to be forgotten' judgment, to illustrate the difficulties when balancing the two rights. The court decided in Google Spain that people have, under certain conditions, the right to have search results for their name delisted. We discuss how Google and Data Protection Authorities deal with such delisting requests in practice. Delisting requests illustrate that balancing privacy and freedom of expression interests will always remain difficult."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.12841,review,post_llm,2025,10,"{'ai_likelihood': 4.0398703681098095e-06, 'text': 'Conceptualizing Smart City Applications: Requirements, Architecture, Security Issues and Emerging Trends\n\nThe emergence of smart cities and sustainable development has become a globally accepted form of urbanization. The epitome of smart city development has become possible due to the latest innovative integration of information and communication technology. Citizens of smart cities can enjoy the benefits of a smart living environment, ubiquitous connectivity, seamless access to services, intelligent decision making through smart governance, and optimized resource management. The widespread acceptance of smart cities has raised data security issues, authentication, unauthorized access, device-level vulnerability, and sustainability. This paper focuses on the wholistic overview and conceptual development of smart city. Initially, the work discusses the smart city idea and fundamentals explored in various pieces of literature. Further various smart city applications, including notable implementations, are put forth to understand the quality of living standards. Finally, the paper depicts a solid understanding of different security and privacy issues, including some crucial future research directions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.25167,regular,post_llm,2025,10,"{'ai_likelihood': 1.8212530348036025e-06, 'text': 'Scaling Cultural Resources for Improving Generative Models\n\nGenerative models are known to have reduced performance in different global cultural contexts and languages. While continual data updates have been commonly conducted to improve overall model performance, bolstering and evaluating this cross-cultural competence of generative AI models requires data resources to be intentionally expanded to include global contexts and languages. In this work, we construct a repeatable, scalable, multi-pronged pipeline to collect and contribute culturally salient, multilingual data. We posit that such data can assess the state of the global applicability of our models and thus, in turn, help identify and improve upon cross-cultural gaps.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.19196,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'Integration of AI in STEM Education, Addressing Ethical Challenges in K-12 Settings\n\nThe rapid integration of Artificial Intelligence (AI) into K-12 STEM education presents transformative opportunities alongside significant ethical challenges. While AI-powered tools such as Intelligent Tutoring Systems (ITS), automated assessments, and predictive analytics enhance personalized learning and operational efficiency, they also risk perpetuating algorithmic bias, eroding student privacy, and exacerbating educational inequities. This paper examines the dual-edged impact of AI in STEM classrooms, analyzing its benefits (e.g., adaptive learning, real-time feedback) and drawbacks (e.g., surveillance risks, pedagogical limitations) through an ethical lens. We identify critical gaps in current AI education research, particularly the lack of subject-specific frameworks for responsible integration and propose a three-phased implementation roadmap paired with a tiered professional development model for educators. Our framework emphasizes equity-centered design, combining technical AI literacy with ethical reasoning to foster critical engagement among students. Key recommendations include mandatory bias audits, low-resource adaptation strategies, and policy alignment to ensure AI serves as a tool for inclusive, human-centered STEM education. By bridging theory and practice, this work advances a research-backed approach to AI integration that prioritizes pedagogical integrity, equity, and student agency in an increasingly algorithmic world. Keywords: Artificial Intelligence, STEM education, algorithmic bias, ethical AI, K-12 pedagogy, equity in education', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.0286,review,post_llm,2025,10,"{'ai_likelihood': 1.4238887363009983e-06, 'text': 'The regulation of online political micro-targeting in Europe\n\nIn this paper, we examine how online political micro-targeting is regulated in Europe. While there are no specific rules on such micro-targeting, there are general rules that apply. We focus on three fields of law: data protection law, freedom of expression, and sector-specific rules for political advertising; for the latter we examine four countries. We argue that the rules in the General Data Protection Regulation (GDPR) are necessary, but not sufficient. We show that political advertising, including online political micro-targeting, is protected by the right to freedom of expression. That right is not absolute, however. From a European human rights perspective, it is possible for lawmakers to limit the possibilities for political advertising. Indeed, some countries ban TV advertising for political parties during elections.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.16032,review,post_llm,2025,10,"{'ai_likelihood': 0.003066592746310764, 'text': 'A dual typology of social media interventions and deterrence mechanisms against misinformation\n\nIn response to the escalating threat of misinformation, social media platforms have introduced a wide range of interventions aimed at reducing the spread and influence of false information. However, there is a lack of a coherent macrolevel perspective that explains how these interventions operate independently and collectively. To address this gap, I offer a dual typology through a spectrum of interventions aligned with deterrence theory and drawing parallels from international relations, military, cybersecurity, and public health. I argue that five major types of platform interventions, including removal, reduction, informing, composite, and multimodal, can be mapped to five corresponding deterrence mechanisms, including hard, situational, soft, integrated, and mixed deterrence based on purpose and perceptibility. These mappings illuminate how platforms apply varying degrees of deterrence mechanisms to influence user behavior.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.08395,regular,post_llm,2025,10,"{'ai_likelihood': 8.940696716308594e-07, 'text': ""Human-Centered Development of Indicators for Self-Service Learning Analytics: A Transparency through Exploration Approach\n\nThe aim of learning analytics is to turn educational data into insights, decisions, and actions to improve learning and teaching. The reasoning of the provided insights, decisions, and actions is often not transparent to the end-user, and this can lead to trust and acceptance issues when interventions, feedback, and recommendations fail. In this paper, we shed light on achieving transparent learning analytics by following a transparency through exploration approach. To this end, we present the design, implementation, and evaluation details of the Indicator Editor, which aims to support self-service learning analytics (SSLA) by empowering end-users to take control of the indicator implementation process. We systematically designed and implemented the Indicator Editor through an iterative human-centered design (HCD) approach. Further, we conducted a qualitative user study (n=15) to investigate the impact of following an SSLA approach on the users' perception of and interaction with the Indicator Editor. Our study showed qualitative evidence that supporting user interaction and providing user control in the indicator implementation process can have positive effects on different crucial aspects of learning analytics, namely transparency, trust, satisfaction, and acceptance."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.17712,review,post_llm,2025,10,"{'ai_likelihood': 8.27842288547092e-07, 'text': ""Online Political Microtargeting: Promises and Threats for Democracy\n\nOnline political microtargeting involves monitoring people's online behaviour, and using the collected data, sometimes enriched with other data, to show people-targeted political advertisements. Online political microtargeting is widely used in the US; Europe may not be far behind. This paper maps microtargeting's promises and threats to democracy. For example, microtargeting promises to optimise the match between the electorate's concerns and political campaigns, and to boost campaign engagement and political participation. But online microtargeting could also threaten democracy. For instance, a political party could, misleadingly, present itself as a different one-issue party to different individuals. And data collection for microtargeting raises privacy concerns. We sketch possibilities for policymakers if they seek to regulate online political microtargeting. We discuss which measures would be possible, while complying with the right to freedom of expression under the European Convention on Human Rights."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.03487,regular,post_llm,2025,10,"{'ai_likelihood': 0.001319249471028646, 'text': ""Analyzing the Performance of a 2.72kWp Rooftop Grid tied Photovoltaic System in Tarlac City, Philippines\n\nResidential and industrial areas are using rooftop grid-tied Photovoltaic (PV) systems, which are becoming increasingly popular. This is because solar energy reduces electrical consumption and provides free energy, while also lowering carbon emissions to create a more sustainable environment. This paper aims to analyze the 2.72kW p rooftop grid-tied PV system performance between 2020 and 2023 in Tarlac City, Philippines. The PV generated yearly is measured by Array Yield (YA), Reference Yield (YR), and Final Yield (YF), which were found to be valued at 3.12, 3.9, and 3.01 kWh/kWp, respectively. The efficiency can decrease due to System Loss (LS) and Capture Loss (LC), which were 0.78 and 0.12 kWh/kWp, respectively. This results in a Capacity Utilization Factor (CUF) of 15.52% and a Performance Ratio (PR) of 77.10%. The productivity of PV resulted in an array efficiency was 12.89%, an inverter efficiency was 94.3%, and a system efficiency was 12.16%. PV energy generation was 3,699 kWh, with 2380 kWh fed into the grid annually. The system's annual revenue is $690.59. The payback period is 6 years with a 238.2% Return On Investment (ROI). Carbon emissions are reduced by 0.379 tCO2/kWp/yr."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22423,review,post_llm,2025,10,"{'ai_likelihood': 3.5100513034396704e-06, 'text': 'Stop the Nonconsensual Use of Nude Images in Research\n\nIn order to train, test, and evaluate nudity detection models, machine learning researchers typically rely on nude images scraped from the Internet. Our research finds that this content is collected and, in some cases, subsequently distributed by researchers without consent, leading to potential misuse and exacerbating harm against the subjects depicted. This position paper argues that the distribution of nonconsensually collected nude images by researchers perpetuates image-based sexual abuse and that the machine learning community should stop the nonconsensual use of nude images in research. To characterize the scope and nature of this problem, we conducted a systematic review of papers published in computing venues that collect and use nude images. Our results paint a grim reality: norms around the usage of nude images are sparse, leading to a litany of problematic practices like distributing and publishing nude images with uncensored faces, and intentionally collecting and sharing abusive content. We conclude with a call-to-action for publishing venues and a vision for research in nudity detection that balances user agency with concrete research objectives.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.11556,regular,post_llm,2025,10,"{'ai_likelihood': 1.1523564656575521e-05, 'text': 'Personalized and Constructive Feedback for Computer Science Students Using the Large Language Model (LLM)\n\nThe evolving pedagogy paradigms are leading toward educational transformations. One fundamental aspect of effective learning is relevant, immediate, and constructive feedback to students. Providing constructive feedback to large cohorts in academia is an ongoing challenge. Therefore, academics are moving towards automated assessment to provide immediate feedback. However, current approaches are often limited in scope, offering simplistic responses that do not provide students with personalized feedback to guide them toward improvements. This paper addresses this limitation by investigating the performance of Large Language Models (LLMs) in processing students assessments with predefined rubrics and marking criteria to generate personalized feedback for in-depth learning. We aim to leverage the power of existing LLMs for Marking Assessments, Tracking, and Evaluation (LLM-MATE) with personalized feedback to enhance students learning. To evaluate the performance of LLM-MATE, we consider the Software Architecture (SA) module as a case study. The LLM-MATE approach can help module leaders overcome assessment challenges with large cohorts. Also, it helps students improve their learning by obtaining personalized feedback in a timely manner. Additionally, the proposed approach will facilitate the establishment of ground truth for automating the generation of students assessment feedback using the ChatGPT API, thereby reducing the overhead associated with large cohort assessments.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.14221,review,post_llm,2025,10,"{'ai_likelihood': 1.5232298109266494e-05, 'text': ""Technological Devices and Their Negative Effects on Health\n\nTechnology has become a global tool that allows us to obtain information and analyze data, streamlines communication, and allows us to share images, data, videos, texts, etc. Daily activities have gone from traditional to digital. Today, it is impossible to live without an electronic device. In this context, changes in people's health observed, with various complaints ranging from visual, neurological, and concentration problems to muscular, hearing, and sleep disorders. Society must be aware of the importance of using various technological devices responsibly to protect people's health in general. Keywords: Technology, activities, protect, electronic, Radiation, Health."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.08921,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': ""GBA-UBF : A Large-Scale and Fine-Grained Building Function Classification Dataset in the Greater Bay Area\n\nRapid urbanization in the Guangdong-Hong Kong-Macao Greater Bay Area (GBA) has created urgent demand for high-resolution, building-level functional data to support sustainable spatial planning. Existing land use datasets suffer from coarse granularity and difficulty in capturing intra-block heterogeneity. To this end, we present the Greater Bay Area Urban Building Function Dataset (GBA-UBF), a large-scale, fine-grained dataset that assigns one of five functional categories to nearly four million buildings across six core GBA cities. We proposed a Multi-level Building Function Optimization (ML-BFO) method by integrating Points of Interest (POI) records and building footprints through a three-stage pipeline: (1) candidate label generation using spatial overlay with proximity weighting, (2) iterative refinement based on neighborhood label autocorrelation, and (3) function-related correction informed by High-level POI buffers. To quantitatively validate results, we design the Building Function Matching Index (BFMI), which jointly measures categorical consistency and distributional similarity against POI-derived probability heatmaps. Comparative experiments demonstrate that GBA-UBF achieves significantly higher accuracy, with a BMFI of 0.58. This value markedly exceeds that of the baseline dataset and exhibits superior alignment with urban activity patterns. Field validation further confirms the dataset's semantic reliability and practical interpretability. The GBA-UBF dataset establishes a reproducible framework for building-level functional classification, bridging the gap between coarse land use maps and fine-grained urban analytics. The dataset is accessible at https://github.com/chenchs0629/GBA-UBF, and the data will undergo continuous improvement and updates based on feedback from the community."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001474618911743164, 'GPT4': 0.0810546875, 'CLAUDE': 0.308837890625, 'GOOGLE': 0.035308837890625, 'OPENAI_O_SERIES': 0.0007405281066894531, 'DEEPSEEK': 0.57373046875, 'GROK': 1.7881393432617188e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 7.158517837524414e-05, 'HUMAN': 0.000408172607421875}}"
2510.06119,regular,post_llm,2025,10,"{'ai_likelihood': 3.642506069607205e-07, 'text': 'A Possibility Frontier Approach to Diverse Talent Selection\n\nOrganizations (e.g., talent investment programs, schools, firms) are perennially interested in selecting cohorts of talented people. And organizations are increasingly interested in selecting diverse cohorts. Except in trivial cases, measuring the tradeoff between cohort diversity and talent is computationally difficult. Thus, organizations are presently unable to make Pareto-efficient decisions about these tradeoffs. We introduce an algorithm that approximates upper bounds on cohort talent and diversity. We call this object the selection possibility frontier (SPF). We then use the SPF to assess the efficiency of selection of a talent investment program. We show that, in the 2021 and 2022 cycles, the program selected cohorts of finalists that could have been better along both diversity and talent dimensions (i.e., considering only these dimensions as we subsequently calculated them, they are Pareto-inferior cohorts). But, when given access our approximation of the SPF in the 2023 cycle, the program adjusted decisions and selected a cohort on the SPF.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.10176,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'The Mechanical Yes-Man: Emancipatory AI Pedagogy in Higher Education\n\nThe proliferation of Large Language Models in higher education presents a fundamental challenge to traditional pedagogical frameworks. Drawing on Jacques Ranci\\`ere\'s theory of intellectual emancipation, this paper examines how generative AI risks becoming a ""mechanical yes-man"" that reinforces passivity rather than fostering intellectual autonomy. Generative AI\'s statistical logic and lack of causal reasoning, combined with frictionless information access, threatens to hollow out cognitive processes essential for genuine learning. This creates a critical paradox: while generative AI systems are trained for complex reasoning, students increasingly use them to bypass the intellectual work that builds such capabilities. The paper critiques both techno-optimistic and restrictive approaches to generative AI in education, proposing instead an emancipatory pedagogy grounded in verification, mastery, and co-inquiry. This framework positions generative AI as material for intellectual work rather than a substitute for it, emphasising the cultivation of metacognitive awareness and critical interrogation of AI outputs. It requires educators to engage directly with these tools to guide students toward critical AI literacy, transforming pedagogical authority from explication to critical interloping that models intellectual courage and collaborative inquiry.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.4570693969726562e-06, 'GPT4': 0.0002720355987548828, 'CLAUDE': 0.63232421875, 'GOOGLE': 9.381771087646484e-05, 'OPENAI_O_SERIES': 1.2695789337158203e-05, 'DEEPSEEK': 0.367431640625, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 4.76837158203125e-07, 'HUMAN': 3.159046173095703e-06}}"
2510.02861,review,post_llm,2025,10,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""The European Union general data protection regulation: what it is and what it means\n\nThis paper introduces the strategic approach to regulating personal data and the normative foundations of the European Union's General Data Protection Regulation ('GDPR'). We explain the genesis of the GDPR, which is best understood as an extension and refinement of existing requirements imposed by the 1995 Data Protection Directive; describe the GDPR's approach and provisions; and make predictions about the GDPR's implications. We also highlight where the GDPR takes a different approach than U.S. privacy law. The GDPR is the most consequential regulatory development in information policy in a generation. The GDPR brings personal data into a detailed regulatory regime, that will influence personal data usage worldwide. Understood properly, the GDPR encourages firms to develop information governance frameworks, to in-house data use, and to keep humans in the loop in decision making. Companies with direct relationships with consumers have strategic advantages under the GDPR, compared to third party advertising firms on the internet. To reach these objectives, the GDPR uses big sticks, structural elements that make proving violations easier, but only a few carrots. The GDPR will complicate and restrain some information-intensive business models. But the GDPR will also enable approaches previously impossible under less-protective approaches."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.12836,regular,post_llm,2025,10,"{'ai_likelihood': 4.006756676567926e-06, 'text': 'BanglaMATH : A Bangla benchmark dataset for testing LLM mathematical reasoning at grades 6, 7, and 8\n\nLarge Language Models (LLMs) have tremendous potential to play a key role in supporting mathematical reasoning, with growing use in education and AI research. However, most existing benchmarks are limited to English, creating a significant gap for low-resource languages. For example, Bangla is spoken by nearly 250 million people who would collectively benefit from LLMs capable of native fluency. To address this, we present BanglaMATH, a dataset of 1.7k Bangla math word problems across topics such as Arithmetic, Algebra, Geometry, and Logical Reasoning, sourced from Bangla elementary school workbooks and annotated with details like grade level and number of reasoning steps. We have designed BanglaMATH to evaluate the mathematical capabilities of both commercial and open-source LLMs in Bangla, and we find that Gemini 2.5 Flash and DeepSeek V3 are the only models to achieve strong performance, with $\\ge$ 80\\% accuracy across three elementary school grades. Furthermore, we assess the robustness and language bias of these top-performing LLMs by augmenting the original problems with distracting information, and translating the problems into English. We show that both LLMs fail to maintain robustness and exhibit significant performance bias in Bangla. Our study underlines current limitations of LLMs in handling arithmetic and mathematical reasoning in low-resource languages, and highlights the need for further research on multilingual and equitable mathematical understanding. Dataset link: \\href{https://github.com/TabiaTanzin/BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8.git}{https://github.com/BanglaMATH}', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.20927,review,post_llm,2025,10,"{'ai_likelihood': 4.635916815863716e-07, 'text': ""What do model reports say about their ChemBio benchmark evaluations? Comparing recent releases to the STREAM framework\n\nMost frontier AI developers publicly document their safety evaluations of new AI models in model reports, including testing for chemical and biological (ChemBio) misuse risks. This practice provides a window into the methodology of these evaluations, helping to build public trust in AI systems, and enabling third party review in the still-emerging science of AI evaluation. But what aspects of evaluation methodology do developers currently include -- or omit -- in their reports? This paper examines three frontier AI model reports published in spring 2025 with among the most detailed documentation: OpenAI's o3, Anthropic's Claude 4, and Google DeepMind's Gemini 2.5 Pro. We compare these using the STREAM (v1) standard for reporting ChemBio benchmark evaluations. Each model report included some useful details that the others did not, and all model reports were found to have areas for development, suggesting that developers could benefit from adopting one another's best reporting practices. We identified several items where reporting was less well-developed across all model reports, such as providing examples of test material, and including a detailed list of elicitation conditions. Overall, we recommend that AI developers continue to strengthen the emerging science of evaluation by working towards greater transparency in areas where reporting currently remains limited."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.05292,review,post_llm,2025,10,"{'ai_likelihood': 6.619426939222548e-05, 'text': 'Disclosure and Evaluation as Fairness Interventions for General-Purpose AI\n\nDespite conflicting definitions and conceptions of fairness, AI fairness researchers broadly agree that fairness is context-specific. However, when faced with general-purpose AI, which by definition serves a range of contexts, how should we think about fairness? We argue that while we cannot be prescriptive about what constitutes fair outcomes, we can specify the processes that different stakeholders should follow in service of fairness. Specifically, we consider the obligations of two major groups: system providers and system deployers. While system providers are natural candidates for regulatory attention, the current state of AI understanding offers limited insight into how upstream factors translate into downstream fairness impacts. Thus, we recommend that providers invest in evaluative research studying how model development decisions influence fairness and disclose whom they are serving their models to, or at the very least, reveal sufficient information for external researchers to conduct such research. On the other hand, system deployers are closer to real-world contexts and can leverage their proximity to end users to address fairness harms in different ways. Here, we argue they should responsibly disclose information about users and personalization and conduct rigorous evaluations across different levels of fairness. Overall, instead of focusing on enforcing fairness outcomes, we prioritize intentional information-gathering by system providers and deployers that can facilitate later context-aware action. This allows us to be specific and concrete about the processes even while the contexts remain unknown. Ultimately, this approach can sharpen how we distribute fairness responsibilities and inform more fluid, context-sensitive interventions as AI continues to advance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.25339,review,post_llm,2025,10,"{'ai_likelihood': 4.933940039740669e-06, 'text': ""Tracking Walls, Take-It-Or-Leave-It Choices, the GDPR, and the ePrivacy Regulation\n\nOn the internet, we encounter take-it-or-leave-it choices regarding our privacy on a daily basis. In Europe, online tracking for targeted advertising generally requires the internet users' consent to be lawful. Some websites use a tracking wall, a barrier that visitors can only pass if they consent to tracking by third parties. When confronted with such a tracking wall, many people click 'I agree' to tracking. A survey that we conducted shows that most people find tracking walls unfair and unacceptable. We analyse under which conditions the ePrivacy Directive and the General Data Protection Regulation allow tracking walls. We provide a list of circumstances to assess when a tracking wall makes consent invalid. We also explore how the EU lawmaker could regulate tracking walls, for instance in the ePrivacy Regulation. It should be seriously considered to ban tracking walls, at least in certain circumstances."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.1282,regular,post_llm,2025,10,"{'ai_likelihood': 0.2978515625, 'text': ""National Data Platform's Education Hub\n\nAs demand for AI literacy and data science education grows, there is a critical need for infrastructure that bridges the gap between research data, computational resources, and educational experiences. To address this gap, we developed a first-of-its-kind Education Hub within the National Data Platform. This hub enables seamless connections between collaborative research workspaces, classroom environments, and data challenge settings. Early use cases demonstrate the effectiveness of the platform in supporting complex and resource-intensive educational activities. Ongoing efforts aim to enhance the user experience and expand adoption by educators and learners alike."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22508,review,post_llm,2025,10,"{'ai_likelihood': 0.0, 'text': 'Women upskilling or reskilling to an ICT career: A systematic review of drivers and barriers\n\nDemand for technology focused STEM professionals will increase globally over the coming decade, with many countries finding it difficult to meet growing demand. Compounding this are difficulties in attracting and retaining female technology-focused professionals. Research seeking to address this gender imbalance and workforce shortage focuses on increasing participation among school leavers. However, there is a paucity of research around the potential for females to upskill or reskill into an ICT career. As a starting point, this review asks the question: ""What potential drivers and barriers have been identified that impact on female intentions or choices to reskill or upskill to a technology focused STEM career"". Results indicate dissatisfaction in a first career, combined with positive computing experiences in the workplace can rouse interest in computing professions. Learning of job opportunities, especially from salient referents, is also a key driver. Results indicate women must overcome negative identity and academic beliefs, as well as self-doubt to make the switch. In summary, it is possible to increase and diversify the tech workforce by leveraging women\'s latent interest in computing. This review provides a roadmap for research to support educational institutions, employers, and women to benefit from upskilling or reskilling opportunities', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.05998,regular,post_llm,2025,10,"{'ai_likelihood': 1.7616483900282117e-05, 'text': ""Beyond Accessibility: How Intelligent Assistive Technologies Improve Activities of Daily Life for Visually Impaired People in South Africa\n\nOur study explores how intelligent assistive technologies (IATs) can enable visually impaired people (VIPs) to overcome barriers to inclusion in a digital society to ultimately improve their quality of life. Drawing on the Social Model of Disability (SMD), which frames disability as a consequence of social and institutional barriers rather than individual impairments, we employ semi-structured interviews and an online qualitative survey with n=61 VIPs in South Africa. Using descriptive statistics and Qualitative Comparative Analysis (QCA), we uncover nine configurations, clustered along three broader combinations of conditions, that support and hinder IAT-mediated inclusion. Most notably, we identify that the autonomy of VIPs and the accessibility of IATs are primary predictors of IAT's ability to achieve social participation. Our findings contribute to Information Systems (IS) literature at the intersection of technology and social participation. We further formulate implications for research and policymakers to foster social inclusion of VIPs in the Global South."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.03764,regular,post_llm,2025,10,"{'ai_likelihood': 1.0728836059570312e-05, 'text': 'R v F (2025): Addressing the Defence of Hacking\n\nThe defence of hacking (sometimes referred to as the ""Trojan Horse Defence"" or the ""SODDI Defence"", Some Other Dude Did It Defence) is prevalent in computer cases and a challenge for those working in the criminal justice system. Historical reviews of cases have demonstrated the defence operating to varying levels of success. However, there remains an absence in academic literature of case studies of how digital forensics investigators can address this defence, to assist courts in acquitting the innocent and convicting the guilty. This case study follows the case of R v F where a defendant asserted this defence and the author worked alongside a police investigator to investigate the merits of the defence and bring empirical evidence before the jury. As the first case study of its kind, it presents practical lessons and techniques for digital forensic investigators.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.20698,regular,post_llm,2025,10,"{'ai_likelihood': 1.8543667263454863e-06, 'text': ""The Order of Recommendation Matters: Structured Exploration for Improving the Fairness of Content Creators\n\nSocial media platforms provide millions of professional content creators with sustainable incomes. Their income is largely influenced by their number of views and followers, which in turn depends on the platform's recommender system (RS). So, as with regular jobs, it is important to ensure that RSs distribute revenue in a fair way. For example, prior work analyzed whether the creators of the highest-quality content would receive the most followers and income. Results showed this is unlikely to be the case, but did not suggest targeted solutions. In this work, we first use theoretical analysis and simulations on synthetic datasets to understand the system better and find interventions that improve fairness for creators. We find that the use of ordered pairwise comparison overcomes the cold start problem for a new set of items and greatly increases the chance of achieving fair outcomes for all content creators. Importantly, it also maintains user satisfaction. We also test the intervention on the MovieLens dataset and investigate its effectiveness on platforms with interaction histories that are currently unfair for content creators. These experiments reveal that the intervention improves fairness when deployed at early stages of the platform, but the effect decreases as the strength of pre-existing bias increases. Altogether, we find that the ordered pairwise comparison approach might offer a plausible alternative for both new and existing platforms to implement."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.13653,review,post_llm,2025,10,"{'ai_likelihood': 0.00018159548441569012, 'text': 'International AI Safety Report 2025: First Key Update: Capabilities and Risk Implications\n\nSince the publication of the first International AI Safety Report, AI capabilities have continued to improve across key domains. New training techniques that teach AI systems to reason step-by-step and inference-time enhancements have primarily driven these advances, rather than simply training larger models. As a result, general-purpose AI systems can solve more complex problems in a range of domains, from scientific research to software development. Their performance on benchmarks that measure performance in coding, mathematics, and answering expert-level science questions has continued to improve, though reliability challenges persist, with systems excelling on some tasks while failing completely on others. These capability improvements also have implications for multiple risks, including risks from biological weapons and cyber attacks. Finally, they pose new challenges for monitoring and controllability. This update examines how AI capabilities have improved since the first Report, then focuses on key risk areas where substantial new evidence warrants updated assessments.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.16951,review,post_llm,2025,10,"{'ai_likelihood': 1.2218952178955078e-05, 'text': 'Local News Hijacking: A Review of International Instances\n\nIn the rise of the digital era, it\'s easier than ever to create nefarious websites to spread misinformation. A more recent phenomenon in the United States has been the creation of inauthentic local news websites to further an information operation campaign. This paper is a review of the 7 instances in which local news websites were created to influence residents of a region between 2007 and 2024. By breaking down the ways in which these sites operated, we discovered commonalities in the approach - resurrecting ""zombie"" papers that were previously established authentic local news organizations, sharing these sites on social media, and using website templates from WordPress. By analyzing these commonalities, we propose ways to mitigate the occurrence of these campaigns in the future.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.23523,regular,post_llm,2025,10,"{'ai_likelihood': 0.0009430779351128472, 'text': 'From Perceived Effectiveness to Measured Impact: Identity-Aware Evaluation of Automated Counter-Stereotypes\n\nWe investigate the effect of automatically generated counter-stereotypes on gender bias held by users of various demographics on social media. Building on recent NLP advancements and social psychology literature, we evaluate two counter-stereotype strategies -- counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) -- which have been identified as the most potentially effective in previous studies. We assess the real-world impact of these strategies on mitigating gender bias across user demographics (gender and age), through the Implicit Association Test and the self-reported measures of explicit bias and perceived utility. Our findings reveal that actual effectiveness does not align with perceived effectiveness, and the former is a nuanced and sometimes divergent phenomenon across demographic groups. While overall bias reduction was limited, certain groups (e.g., older, male participants) exhibited measurable improvements in implicit bias in response to some interventions. Conversely, younger participants, especially women, showed increasing bias in response to the same interventions. These results highlight the complex and identity-sensitive nature of stereotype mitigation and call for dynamic and context-aware evaluation and mitigation strategies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22279,regular,post_llm,2025,10,"{'ai_likelihood': 0.951171875, 'text': 'The AI Tutor in Engineering Education: Design, Results, and Redesign of an Experience in Hydrology at an Argentine University\n\nThe emergence of Generative Artificial Intelligence (GenAI) has reshaped higher education, presenting both opportunities and ethical-pedagogical challenges. This article presents an empirical case study on the complete cycle (design, initial failure, redesign, and re-evaluation) of an intervention using an AI Tutor (ChatGPT) in the ""Hydrology and Hydraulic Works"" course (Civil Engineering, UTN-FRT, Argentina). The study documents two interventions in the same cohort (n=23). The first resulted in widespread failure (0% pass rate) due to superficial use and serious academic integrity issues (65% similarity, copies > 80%). This failure forced a comprehensive methodological redesign. The second intervention, based on a redesigned prompt (Prompt V2) with strict evidence controls (mandatory Appendix A with exported chat, minimum time $\\geq$ 120 minutes, verifiable numerical exercise) and a refined rubric (Rubric V2), showed significantly better results: a median score of 88/100 and verifiable compliance with genuine interaction processes. Using a mixed-methods approach (reproducible document analysis and rubric analysis), the impact of the redesign on integrity and technical performance is evaluated. The results demonstrate that, without explicit process controls, students prioritize efficiency over deep learning, submitting documents without real traceability. A transferable assessment protocol for STEM courses is proposed, centered on ""auditable personal zones,"" to foster higher-order thinking. The study provides key empirical evidence from the context of a public Latin American university.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.2186508178710938e-06, 'GPT4': 0.0009927749633789062, 'CLAUDE': 0.0019235610961914062, 'GOOGLE': 0.0011682510375976562, 'OPENAI_O_SERIES': 2.384185791015625e-07, 'DEEPSEEK': 0.98779296875, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 8.940696716308594e-07, 'HUMAN': 0.00799560546875}}"
2510.24582,regular,post_llm,2025,10,"{'ai_likelihood': 9.602970547146267e-07, 'text': 'Politically Speaking: LLMs on Changing International Affairs\n\nAsk your chatbot to impersonate an expert from Russia and an expert from US and query it on Chinese politics. How might the outputs differ? Or, to prepare ourselves for the worse, how might they converge? Scholars have raised concerns LLM based applications can homogenize cultures and flatten perspectives. But exactly how much does LLM generated outputs converge despite explicit different role assignment? This study provides empirical evidence to the above question. The critique centres on pretrained models regurgitating ossified political jargons used in the Western world when speaking about China, Iran, Russian, and US politics, despite changes in these countries happening daily or hourly. The experiments combine role-prompting and similarity metrics. The results show that AI generated discourses from four models about Iran and China are the most homogeneous and unchanging across all four models, including OpenAI GPT, Google Gemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change and the actual changes in real life. This study does not engage with history, politics, or literature as traditional disciplinary approaches would; instead, it takes cues from international and area studies and offers insight on the future trajectory of shifting political discourse in a digital space increasingly cannibalised by AI.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.01864,review,post_llm,2025,10,"{'ai_likelihood': 1.0629494984944662e-05, 'text': 'Missing the Margins: A Systematic Literature Review on the Demographic Representativeness of LLMs\n\nMany applications of Large Language Models (LLMs) require them to either simulate people or offer personalized functionality, making the demographic representativeness of LLMs crucial for equitable utility. At the same time, we know little about the extent to which these models actually reflect the demographic attributes and behaviors of certain groups or populations, with conflicting findings in empirical research. To shed light on this debate, we review 211 papers on the demographic representativeness of LLMs. We find that while 29% of the studies report positive conclusions on the representativeness of LLMs, 30% of these do not evaluate LLMs across multiple demographic categories or within demographic subcategories. Another 35% and 47% of the papers concluding positively fail to specify these subcategories altogether for gender and race, respectively. Of the articles that do report subcategories, fewer than half include marginalized groups in their study. Finally, more than a third of the papers do not define the target population to whom their findings apply; of those that do define it either implicitly or explicitly, a large majority study only the U.S. Taken together, our findings suggest an inflated perception of LLM representativeness in the broader community. We recommend more precise evaluation methods and comprehensive documentation of demographic attributes to ensure the responsible use of LLMs for social applications. Our annotated list of papers and analysis code is publicly available.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.15142,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'Revisiting UTAUT for the Age of AI: Understanding Employees AI Adoption and Usage Patterns Through an Extended UTAUT Framework\n\nThis study investigates whether demographic factors shape adoption and attitudes among employees toward artificial intelligence (AI) technologies at work. Building on an extended Unified Theory of Acceptance and Use of Technology (UTAUT), which reintroduces affective dimensions such as attitude, self-efficacy, and anxiety, we surveyed 2,257 professionals across global regions and organizational levels within a multinational consulting firm. Non-parametric tests examined whether three demographic factors (i.e., years of experience, hierarchical level in the organization, and geographic region) were associated with AI adoption, usage intensity, and eight UTAUT constructs. Organizational level significantly predicted AI adoption, with senior employees showing higher usage rates, while experience and region were unrelated to adoption. Among AI users (n = 1,256), frequency and duration of use showed minimal demographic variation. However, omnibus tests revealed small but consistent group differences across several UTAUT constructs, particularly anxiety, performance expectancy, and behavioral intention, suggesting that emotional and cognitive responses to AI vary modestly across contexts. These findings highlight that demographic factors explain limited variance in AI acceptance but remain relevant for understanding contextual nuances in technology-related attitudes. The results underscore the need to integrate affective and organizational factors into models of technology acceptance to support equitable, confident, and sustainable engagement with AI in modern workplaces.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.947185516357422e-05, 'GPT4': 0.0960693359375, 'CLAUDE': 0.257568359375, 'GOOGLE': 0.0004019737243652344, 'OPENAI_O_SERIES': 0.0003345012664794922, 'DEEPSEEK': 0.64501953125, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.7285346984863281e-06, 'OTHER': 4.291534423828125e-06, 'HUMAN': 0.0002040863037109375}}"
2510.02929,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': ""Assessment Twins: A Protocol for AI-Vulnerable Summative Assessment\n\nGenerative Artificial Intelligence (GenAI) is reshaping higher education and raising pressing concerns about the integrity and validity of higher education assessment. While assessment redesign is increasingly seen as a necessity, there is a relative lack of literature detailing what such redesign may entail. In this paper, we introduce assessment twins as an accessible approach for redesigning assessment tasks to enhance validity. We use Messick's unified validity framework to systematically map the ways in which GenAI threaten content, structural, consequential, generalisability, and external validity. Following this, we define assessment twins as two deliberately linked components that address the same learning outcomes through different modes of evidence, scheduled closely together to allow for cross-verification and assurance of learning.\n  We argue that the twin approach helps mitigate validity threats by triangulating evidence across complementary formats, such as pairing essays with oral defences, group discussions, or practical demonstrations. We highlight several advantages: preservation of established assessment formats, reduction of reliance on surveillance technologies, and flexible use across cohort sizes. To guide implementation, we propose a three-step design process: identifying vulnerabilities, aligning outcomes, selecting complementary tasks, and developing interdependent marking schemes. We also acknowledge the challenges, including resource intensity, equity concerns, and the need for empirical validation. Nonetheless, we contend that assessment twins represent a validity-focused response to GenAI that prioritises pedagogy while supporting meaningful student learning outcomes."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.230571746826172e-05, 'GPT4': 0.0171661376953125, 'CLAUDE': 0.15283203125, 'GOOGLE': 0.00330352783203125, 'OPENAI_O_SERIES': 0.0006260871887207031, 'DEEPSEEK': 0.826171875, 'GROK': 2.980232238769531e-07, 'NOVA': 1.1324882507324219e-06, 'OTHER': 4.231929779052734e-06, 'HUMAN': 0.00014913082122802734}}"
2510.14818,review,post_llm,2025,10,"{'ai_likelihood': 1.1589792039659289e-05, 'text': 'Trends of Pink Slime Journalism Advertisement Expenditure and Spread on Facebook from 2019-2024\n\nPink slime journalism is a practice where news outlets publish low-quality or inflammatory partisan articles, claiming to be local news networks. This paper examines the spread of pink slime sites on Facebook using public posts from Pages and Groups. We evaluate the trends of sharing pink slime sites on Facebook and patterns regarding the advertisements purchased by the parent organizations of the pink slime news networks. Our analysis discovers that while the number of pink slime posts on Facebook pages have decreased over the years, advertising dollars have increased. The increase in advertising dollars influences an increase in Facebook group posts. Further, the advertising expenditure increases during election years, but contentious topics are still discussed during non-election years. By illustrating the patterns and themes from US election years of 2020, 2022, and 2024, this research offers insights into potentially dangerous journalism tactics, and provides predictions for future US Presidential Elections.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.12844,review,post_llm,2025,10,"{'ai_likelihood': 5.927350785997179e-06, 'text': 'AI Alignment vs. AI Ethical Treatment: 10 Challenges\n\nA morally acceptable course of AI development should avoid two dangers: creating unaligned AI systems that pose a threat to humanity and mistreating AI systems that merit moral consideration in their own right. This paper argues these two dangers interact and that if we create AI systems that merit moral consideration, simultaneously avoiding both of these dangers would be extremely challenging. While our argument is straightforward and supported by a wide range of pretheoretical moral judgments, it has far-reaching moral implications for AI development. Although the most obvious way to avoid the tension between alignment and ethical treatment would be to avoid creating AI systems that merit moral consideration, this option may be unrealistic and is perhaps fleeting. So, we conclude by offering some suggestions for other ways of mitigating mistreatment risks associated with alignment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22488,regular,post_llm,2025,10,"{'ai_likelihood': 0.99755859375, 'text': ""TLSQKT: A Question-Aware Dual-Channel Transformer for Literacy Tracing from Learning Sequences\n\nKnowledge tracing (KT) supports personalized learning by modeling how students' knowledge states evolve over time. However, most KT models emphasize mastery of discrete knowledge components, limiting their ability to characterize broader literacy development. We reframe the task as Literacy Tracing (LT), which models the growth of higher-order cognitive abilities and literacy from learners' interaction sequences, and we instantiate this paradigm with a Transformer-based model, TLSQKT (Transformer for Learning Sequences with Question-Aware Knowledge Tracing). TLSQKT employs a dual-channel design that jointly encodes student responses and item semantics, while question-aware interaction and self-attention capture long-range dependencies in learners' evolving states. Experiments on three real-world datasets - one public benchmark, one private knowledge-component dataset, and one private literacy dataset - show that TLSQKT consistently outperforms strong KT baselines on literacy-oriented metrics and reveals interpretable developmental trajectories of learners' literacy. Transfer experiments further indicate that knowledge-tracing signals can be leveraged for literacy tracing, offering a practical route when dedicated literacy labels are limited. These findings position literacy tracing as a scalable component of intelligent educational systems and lay the groundwork for literacy evaluation in future large-scale educational models."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.36766242980957e-05, 'GPT4': 0.033599853515625, 'CLAUDE': 0.95166015625, 'GOOGLE': 0.00015985965728759766, 'OPENAI_O_SERIES': 1.5735626220703125e-05, 'DEEPSEEK': 0.007068634033203125, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 1.0132789611816406e-06, 'HUMAN': 0.007415771484375}}"
2510.08885,review,post_llm,2025,10,"{'ai_likelihood': 1.8874804178873699e-06, 'text': ""Rethinking How We Discuss the Guidance of Student Researchers in Computing\n\nComputing faculty at research universities are often expected to guide the work of undergraduate and graduate student researchers. This guidance is typically called advising or mentoring, but these terms belie the complexity of the relationship, which includes several related but distinct roles. I examine the guidance of student researchers in computing (abbreviated to research guidance or guidance throughout) within a facet framework, creating an inventory of roles that faculty members can hold. By expanding and disambiguating the language of guidance, this approach reveals the full breadth of faculty responsibilities toward student researchers, and it facilitates discussing conflicts between those responsibilities. Additionally, the facet framework permits greater flexibility for students seeking guidance, allowing them a robust support network without implying inadequacy in an individual faculty member's skills. I further argue that an over-reliance on singular terms like advising or mentoring for the guidance of student researchers obscures the full scope of faculty responsibilities and interferes with improvement of those as skills. Finally, I provide suggestions for how the facet framework can be utilized by faculty and institutions, and how parts of it can be discussed with students for their benefit."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.1771,review,post_llm,2025,10,"{'ai_likelihood': 2.5166405571831597e-06, 'text': ""Mensen aanwijzen maar niet bij naam noemen: behavioural targeting, persoonsgegevens, en de nieuwe Privacyverordening\n\nInformation about millions of people is collected for behavioural targeting, a type of marketing that involves tracking people's online behaviour for targeted advertising. It is hotly debated whether data protection law applies to behavioural targeting. Many behavioural targeting companies say that, as long as they do not tie names to data they hold about individuals, they do not process any personal data, and that, therefore, data protection law does not apply to them. European Data Protection Authorities, however, take the view that a company processes personal data if it uses data to single out a person, even if it cannot tie a name to these data. This paper argues that data protection law should indeed apply to behavioural targeting. Companies can often tie a name to nameless data about individuals. Furthermore, behavioural targeting relies on collecting information about individuals, singling out individuals, and targeting ads to individuals. Many privacy risks remain, regardless of whether companies tie a name to the information they hold about a person. A name is merely one of the identifiers that can be tied to data about a person, and it is not even the most practical identifier for behavioural targeting. Seeing data used to single out a person as personal data fits the rationale for data protection law: protecting fairness and privacy."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.24822,regular,post_llm,2025,10,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'Managing Administrative Law Cases using an Adaptable Model-driven Norm-enforcing Tool\n\nGovernmental organisations cope with many laws and policies when handling administrative law cases. Making sure these norms are enforced in the handling of cases is for the most part done manually. However, enforcing policies can get complicated and time consuming with ever-changing (interpretations of) laws and varying cases. This introduces errors and delays in the decision-making process and therefore limits the access to justice for citizens. A potential solution is offered by our tool in which norms are enforced using automated normative reasoning. By ensuring the procedural norms are followed and transparency can be provided about the reasoning behind a decision to citizens, the tool benefits the access to justice for citizens. In this paper we report on the implementation of a model-driven case management tool for administrative law cases, based on a set of requirements elicited during earlier research. Our tool achieves adaptability and norm enforcement by interacting with an interpreter for eFLINT, a domain-specific language for norm specification. We report on the current state of the case management tool and suggest directions for further development.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.18026,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': ""Integrating Generative AI into LMS: Reshaping Learning and Instructional Design\n\nEducation in the era of generative AI faces a pivotal transformation. As AI systems reshape professional practices-from software development to creative design-educators must reconsider how to prepare students for a future where humans and machines co-construct knowledge. While tools like ChatGPT and Claude automate tasks and personalize learning, their educational potential depends on how meaningfully they are integrated into learning environments. This paper argues that Learning Management Systems (LMSs), as the core of educational practice, must evolve from static content repositories into dynamic ecosystems that cultivate higher-order thinking and meaningful human-AI interaction. We propose two guiding principles for integrating generative AI into LMSs. First, From Content Delivery to Fostering Higher-Order Thinking, emphasizing AI's role in supporting inquiry, collaboration, and reflective knowledge building. Second, Toward Meaningful Interaction with AI, highlighting the design of learning environments that nurture critical, intentional, and socially mediated engagement with AI. Drawing on a case study of CheckIT Learning, we illustrate how these principles can translate into practice. We conclude with the need for Edtech partnerships in an AI-powered world, underscoring that responsible AI integration in education requires sustained collaboration among researchers, educators, and technologists to ensure ethical, pedagogically grounded, and cognitively informed innovation."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00014197826385498047, 'GPT4': 0.07659912109375, 'CLAUDE': 0.7158203125, 'GOOGLE': 0.0003418922424316406, 'OPENAI_O_SERIES': 2.276897430419922e-05, 'DEEPSEEK': 0.20703125, 'GROK': 2.384185791015625e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 2.0623207092285156e-05, 'HUMAN': 0.00023448467254638672}}"
2510.03905,regular,post_llm,2025,10,"{'ai_likelihood': 7.251898447672526e-06, 'text': ""Quantifying Gender Stereotypes in Japan between 1900 and 1999 with Word Embeddings\n\nWe quantify the evolution of gender stereotypes in Japan from 1900 to 1999 using a series of 100 word embeddings, each trained on a corpus from a specific year. We define the gender stereotype value to measure the strength of a word's gender association by computing the difference in cosine similarity of the word to female- versus male-related attribute words. We examine trajectories of gender stereotype across three traditionally gendered domains: Home, Work, and Politics, as well as occupations. The results indicate that language-based gender stereotypes partially evolved to reflect women's increasing participation in the workplace and politics: Work and Politics domains become more strongly female-stereotyped over the years. Yet, Home also became more female-stereotyped, suggesting that women were increasingly viewed as fulfilling multiple roles such as homemakers, workers, and politicians, rather than having one role replace another. Furthermore, the strength of female stereotype for occupations positively correlate with the proportion of women in each occupation, indicating that word-embedding-based measures of gender stereotype mirrored demographic shifts to a considerable extent."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.15442,regular,post_llm,2025,10,"{'ai_likelihood': 0.014902750651041668, 'text': 'Identifying curriculum disruptions in engineering education through serious gaming\n\nThis workshop introduces participants to SUCRE, a serious game designed to enhance curriculum resilience in higher education by simulating crisis scenarios. While applicable to various disciplines, this session focuses on engineering curricula, identifying discipline-specific challenges and potential adaptations. Participants will engage in Step 1 of the game, analyzing trigger events and their impacts on curriculum structures. At the end of the workshop, attendees will be able to identify key triggers that may affect curricula, assess their cascading effects, and reflect on the applicability of SUCRE within their own institutions.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.11749,regular,post_llm,2025,10,"{'ai_likelihood': 2.5497542487250434e-06, 'text': 'Benefits and Limitations of Using GenAI for Political Education and Municipal Elections\n\nGenerative artificial intelligence (GenAI) presents both challenges and opportunities across all areas of education. Facing the municipal elections in North Rhine-Westphalia, the Young AI Leaders in Dortmund asked themselves: Could GenAI be used to make political programs more accessible, in order to facilitate political education? To explore respective potentials and limitations, we therefore performed an experimental study that combines different GenAI approaches. Language models were used to automatically translate and analyze the contents of each program, deriving five potential visual appearance changes to the city of Dortmund. Based on each analysis, we then generated images with diffusion models and published all results as an interactive webpage. All GenAI models were locally deployed on a Dortmund-based computing cluster, allowing us to also investigate environmental impacts. This manuscript explores the project in full depth, discussing technical details and critically reflecting on the results. As part of the global Young AI Leaders Community, our work promotes the Sustainable Development Goal Quality Education (SDG 4) by transparently discussing the pros and cons of using GenAI for education and political agendas.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22933,review,post_llm,2025,10,"{'ai_likelihood': 0.4787868923611111, 'text': ""How Can AI Augment Access to Justice? Public Defenders' Perspectives on AI Adoption\n\nPublic defenders are asked to do more with less: representing clients deserving of adequate counsel while facing overwhelming caseloads and scarce resources. While artificial intelligence (AI) and large language models (LLMs) are promoted as tools to alleviate this burden, such proposals are detached from the lived realities of public defenders. This study addresses that gap through semi-structured interviews with fourteen practitioners across the United States to examine their experiences with AI, anticipated applications, and ethical concerns. We find that AI adoption is constrained by costs, restrictive office norms, confidentiality risks, and unsatisfactory tool quality. To clarify where AI can and cannot contribute, we propose a task-level map of public defense. Public defenders view AI as most useful for evidence investigation to analyze overwhelming amounts of digital records, with narrower roles in legal research & writing, and client communication. Courtroom representation and defense strategy are considered least compatible with AI assistance, as they depend on contextual judgment and trust. Public defenders emphasize safeguards for responsible use, including mandatory human verification, limits on overreliance, and the preservation of relational aspect of lawyering. Building on these findings, we outline a research agenda that promotes equitable access to justice by prioritizing open-source models, domain-specific datasets and evaluation, and participatory design that incorporates defenders' perspectives into system development."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.05543,regular,post_llm,2025,10,"{'ai_likelihood': 0.0036133660210503475, 'text': 'Revenge Porn: A Peep into its Awareness among the Youth of Tamilnadu, India\n\nThe act of posting a person\'s private photos or videos without their consent is known as revenge porn, and it is usually done to extort money or seek revenge. According to a 2010 cybercrime survey, about 18.3% of women were unaware that they were victims of revenge porn. In densely populated countries like India, such incidents are more likely, yet there is no specific law addressing revenge porn. This study used purposive sampling with a sample size of 200 unmarried women from Tamil Nadu aged 18 to 30. The survey results show that more than 50% had never heard the term ""revenge porn,"" and only about 5% had personally experienced it. About 40% believed the victim was at fault, while 43.5% were unsure whether pornographic websites should be banned. Around 11% admitted that they might upload explicit content as revenge, and 8.5% felt that due to cultural taboos around sex, society tends to blame the victim. Police officers should be trained in techniques for psychologically supporting victims. India, which ranks third globally in cybercrime, must adopt better preventive measures. Public awareness and targeted legal reforms could play a major role in reducing such crimes.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.16847,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'Global Overview of Computational Thinking and Digital Tools for Teaching\n\nComputational Thinking (CT) has emerged as a critical component in modern education, essential to equip students with the skills necessary to thrive in a technology-driven world. This survey provides a comprehensive analysis of the presence and integration of CT in school curricula across various countries. In addition, this study categorizes digital tools into groups such as visual programming, textual programming, electronic games, modeling, and simulation, assessing their use in different educational settings. Furthermore, it examines how these tools are employed in various contexts, including the areas of knowledge and age groups they target, and the specific skills they help develop. The research also identifies key CT competencies that have been improved through these tools, including Cognitive and Analytical Competencies (CAC), Technical and Computational Competencies (TCC) and Social and Emotional Competencies (SEC). Furthermore, the study highlights recurring challenges in the implementation of digital tools for CT development, such as inadequate infrastructure, difficulties in the usability of the tool, teacher training, adapting pedagogical practices, and measuring student CT skills. Finally, it proposes areas for future research to address these challenges and advance CT education.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0498046875, 'GPT4': 0.2396240234375, 'CLAUDE': 0.0010652542114257812, 'GOOGLE': 0.62158203125, 'OPENAI_O_SERIES': 0.0029010772705078125, 'DEEPSEEK': 0.0002872943878173828, 'GROK': 6.115436553955078e-05, 'NOVA': 0.00010526180267333984, 'OTHER': 0.08447265625, 'HUMAN': 0.00011789798736572266}}"
2510.21848,regular,post_llm,2025,10,"{'ai_likelihood': 2.8808911641438803e-06, 'text': ""Enhancing Student Performance Prediction In CS1 Via In-Class Coding\n\nComputer science's increased recognition as a prominent field of study has attracted students with diverse academic backgrounds. This has significantly increased the already high failure rates in introductory courses. To address this challenge, it is essential to identify struggling students early on. Incorporating in-class coding exercises in these courses not only offers additional practice opportunities to students but may also reveal their abilities and help teachers identify those in need of assistance. In this work, we seek to determine the extent to which the practice of using in-class coding exercises enhances the ability to predict student performance, especially early in the semester. Based on data obtained in a CS1 course taught at a mid-size American university, we found that in-class exercises could improve the prediction of students' eventual performance. In particular, we found relatively accurately predictions as early as academic weeks 3 through 5, making it possible to devise early intervention strategies. This work can benefit future studies on the impact of in-class exercises as well as intervention strategies throughout the semester."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.2518,regular,post_llm,2025,10,"{'ai_likelihood': 2.4835268656412763e-06, 'text': 'The Open Source Resume: How Open Source Contributions Help Students Demonstrate Alignment with Employer Needs\n\nComputer science educators are increasingly integrating open source contributions into classes to prepare students for higher expectations due to GenAI, and to improve employment outcomes in an increasingly competitive job market. However, little is known about how employers view student open source contributions. This paper addresses two research questions qualitatively: what traits do employers desire for entry-level hires in 2025, and how can they be demonstrated through open source contributions? It also tests quantitatively the hypothesis that student knowledge of employers\' expectations will improve their motivation to work on open source projects. To answer our qualitative questions, we conducted interviews with US hiring managers. We collaborated with each interviewee to create a ""hiring manager agreement,"" which listed desirable traits and specific ways to demonstrate them through open source, along with a promise to interview some students meeting the criteria. To evaluate our quantitative hypothesis, we surveyed 650 undergraduates attending public universities in the US using an instrument based on expectancy-value theory. Hiring managers wanted many non-technical traits that are difficult to teach in traditional CS classes, such as initiative. There were many commonalities in how employers wanted to see these traits demonstrated in open source contributions. Viewing hiring manager agreements improved student motivation to contribute to open source projects. Our findings suggest that open source contributions may help CS undergraduates get hired, but this requires sustained engagement in multiple areas. Educators can motivate students by sharing employer expectations, but further work is required to determine if this changes their behavior.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.19227,regular,post_llm,2025,10,"{'ai_likelihood': 2.6490953233506945e-05, 'text': 'A Design Science Blueprint for an Orchestrated AI Assistant in Doctoral Supervision\n\nThis study presents a design science blueprint for an orchestrated AI assistant and co-pilot in doctoral supervision that acts as a socio-technical mediator. Design requirements are derived from Stakeholder Theory and bounded by Academic Integrity. We consolidated recent evidence on supervision gaps and student wellbeing, then mapped issues to adjacent large language model capabilities using a transparent severity-mitigability triage. The artefact assembles existing capabilities into one accountable agentic AI workflow that proposes retrieval-augmented generation and temporal knowledge graphs, as well as mixture-of-experts routing as a solution stack of technologies to address existing doctoral supervision pain points. Additionally, a student context store is proposed, which introduces behaviour patches that turn tacit guidance into auditable practice and student-set thresholds that trigger progress summaries, while keeping authorship and final judgement with people. We specify a student-initiated moderation loop in which assistant outputs are routed to a supervisor for review and patching, and we analyse a reconfigured stakeholder ecosystem that makes information explicit and accountable. Risks in such a system exist, and among others, include AI over-reliance and the potential for the illusion of learning, while guardrails are proposed. The contribution is an ex ante, literature-grounded design with workflow and governance rules that institutions can implement and trial across disciplines.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.02844,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': 'Teaching Quantum Computing through Lab-Integrated Learning: Bridging Conceptual and Computational Understanding\n\nQuantum computing education requires students to move beyond classical programming intuitions related to state, determinism, and debugging, and to develop reasoning skills grounded in probability, measurement, and interference. This paper reports on the design and delivery of a combined undergraduate and graduate course at Louisiana State University that employed a lab-integrated learning model to support conceptual change and progressive understanding. The course paired lectures with weekly programming labs that served as environments for experimentation and reflection. These labs enabled students to confront misconceptions and refine their mental models through direct observation and evidence-based reasoning. Instruction began with Quantum Without Linear Algebra (QWLA), which introduced core concepts such as superposition and entanglement through intuitive, dictionary representations. The course then transitioned to IBM Qiskit, which provided a professional framework for circuit design, noise simulation, and algorithm implementation. Analysis of student work and feedback indicated that hands-on experimentation improved confidence, conceptual clarity, and fluency across representations. At the same time, it revealed persistent challenges in debugging, reasoning about measurement, and understanding probabilistic outcomes. This paper presents the course structure, instructional strategies, and lessons learned, and argues that lab-integrated learning offers an effective and accessible approach to teaching quantum computing in computer science education.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0011034011840820312, 'GPT4': 0.0283355712890625, 'CLAUDE': 0.96044921875, 'GOOGLE': 0.0006060600280761719, 'OPENAI_O_SERIES': 0.0003209114074707031, 'DEEPSEEK': 0.008544921875, 'GROK': 3.7550926208496094e-06, 'NOVA': 7.3909759521484375e-06, 'OTHER': 0.0003123283386230469, 'HUMAN': 0.00018799304962158203}}"
2510.16019,regular,post_llm,2025,10,"{'ai_likelihood': 1.2252065870496963e-06, 'text': ""Impact of AI Tools on Learning Outcomes: Decreasing Knowledge and Over-Reliance\n\nStudents at all levels of education are increasingly relying on generative artificial intelligence (AI) tools to complete assignments and achieve higher exam scores. However, it remains unclear how this reliance affects their motivation, their genuine understanding of the material, and the extent to which it substitutes for the process of knowledge acquisition. To investigate the impact of generative AI on learning outcomes, an experiment was conducted at Corvinus University of Budapest. In an operations research class, students were randomly assigned into two groups: one was permitted to use AI tools during classes and examinations, while the other was not. To ensure fairness, a compensation mechanism was introduced: students in the lower-performing group received point adjustments until the average performance of the two groups was equalized. Despite the organizers' best efforts to explain the design and to create equal opportunities for all participants, many students perceived the experiment as a major disruption. Although the experiment was approved by every relevant university authority -- including the Ethics Board, the Head of Department, the Program Director, and the Student Council -- students escalated their concerns to the media and eventually to the State Secretary for Higher Education of Hungary. As a result, the experiment had to be substantially revised before completion: on the final exam the test group was merged with the control group. Still, the data allowed us to draw decisive conclusions regarding the students' learning habits. Uncontrolled use of AI tools leads to disengaged students and low understanding of material. The extreme reactions of the students proved even more revealing than the data collected: generative AI tools have already become indispensable for students, raising fundamental questions about the validity of their learning process."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.0553,regular,post_llm,2025,10,"{'ai_likelihood': 1.6921096377902562e-05, 'text': ""Using LLMs to support assessment of student work in higher education: a viva voce simulator\n\nOne of the emergent challenges of student work submitted for assessment is the widespread use of large language models (LLMs) to support and even produce written work. This particularly affects subjects where long-form written work is a key part of assessment. We propose a novel approach to addressing this challenge, using LLMs themselves to support the assessment process. We have developed a proof-of-concept viva voce examination simulator, which accepts the student's written submission as input, generates an interactive series of questions from the LLM and answers from the student. The viva voce simulator is an interactive tool which asks questions which a human examiner might plausibly ask, and uses the student's answers to form a judgment about whether the submitted piece of work is likely to be the student's own work. The interaction transcript is provided to the human examiner to support their final judgment. We suggest theoretical and practical points which are critical to real-world deployment of such a tool."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.14892,regular,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': ""A Comprehensive Framework for Efficient Court Case Management and Prioritization\n\nThe Indian judicial system faces a critical challenge with approximately 52 million pending cases, causing significant delays that impact socio-economic stability. This study proposes a cloud-based software framework to classify and prioritize court cases using algorithmic methods based on parameters such as severity of crime committed, responsibility of parties involved, case filing dates, previous hearing's data, priority level (e.g., Urgent, Medium, Ordinary) provided as input, and relevant Indian Penal Code (IPC), Code of Criminal Procedure (CrPC), and other legal sections (e.g., Hindu Marriage Act, Indian Contract Act). Cases are initially entered by advocates on record or court registrars, followed by automated hearing date allocation that balances fresh and old cases while accounting for court holidays and leaves. The system streamlines appellate processes by fetching data from historical case databases. Our methodology integrates algorithmic prioritization, a robust notification system, and judicial interaction, with features that allow judges to view daily case counts and their details. Simulations demonstrate that the system can process cases efficiently, with reliable notification delivery and positive user satisfaction among judges and registrars. Future iterations will incorporate advanced machine learning for dynamic prioritization, addressing critical gaps in existing court case management systems to enhance efficiency and reduce backlogs."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0004062652587890625, 'GPT4': 0.439453125, 'CLAUDE': 0.47705078125, 'GOOGLE': 0.00799560546875, 'OPENAI_O_SERIES': 0.05548095703125, 'DEEPSEEK': 0.019195556640625, 'GROK': 5.662441253662109e-06, 'NOVA': 2.5510787963867188e-05, 'OTHER': 0.00032258033752441406, 'HUMAN': 0.00018775463104248047}}"
2510.07634,review,post_llm,2025,10,"{'ai_likelihood': 7.28501213921441e-07, 'text': ""Exploring the Viability of the Updated World3 Model for Examining the Impact of Computing on Planetary Boundaries\n\nThe influential Limits to Growth report introduced a system dynamics-based model to demonstrate global dynamics of the world's population, industry, natural resources, agriculture, and pollution between 1900-2100. In current times, the rapidly expanding trajectory of data center development, much of it linked to AI, uses increasing amounts of natural resources. The extraordinary amount of resources claimed warrants the question of how computing trajectories contribute to exceeding planetary boundaries. Based on the general robustness of the World3-03 model and its influence in serving as a foundation for current climate frameworks, we explore whether the model is a viable method to quantitatively simulate the impact of data centers on limits to growth. Our paper explores whether the World3-03 model is a feasible method for reflecting on these dynamics by adding new variables to the model in order to simulate a new AI-augmented scenario. We find that through our addition of AI-related variables (such as increasing data center development) impacting pollution in the World3-03 model, we can observe the expected changes to dynamics, demonstrating the viability of the World3-03 model for examining AI's impact on planetary boundaries. We detail future research opportunities for using the World3-03 model to explore the relationships between increasing resource-intensive computing and the resulting impacts to the environment in a quantitative way given its feasibility."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.00061,review,post_llm,2025,10,"{'ai_likelihood': 0.00024822023179796006, 'text': 'Adoption of AI-Driven Fraud Detection System in the Nigerian Banking Sector: An Analysis of Cost, Compliance, and Competency\n\nThe inception of AI-based fraud detection systems has presented the banking sector across the globe the opportunity to enhance fraud prevention mechanisms. However, the extent of adoption in Nigeria has been slow, fragmented, and inconsistent due to high cost of implementation and lack of technical expertise. This study seeks to investigate extent of adoption and determinants of AI-driven fraud detection systems in Nigerian banks. This study adopted a cross-sectional survey research design. Data were extracted from primary sources through structured questionnaire based on 5-point Likert scale. The population of the study consist of 24 licensed banks in Nigeria. A purposive sampling technique was used to select 5 biggest banks based on market capitalization and customer base. The Ordered Logistic Regression (OLR) model was used to estimate the data. The results showed that top management support, IT infrastructure, regulatory compliance, staff competency and perceived effectiveness accelerate the uptake of AI-driven fraud detection systems adoption. However, high implementation cost discourages it. Therefore, the study recommended that banks should invest in modern and scalable IT systems that support the integration of AI tools; adopt open-source or cloud-based AI platforms that are cost-effective; embrace continuous professional development in AI, and fraud analytics for IT, fraud investigation, and risk management staff.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.06255,regular,post_llm,2025,10,"{'ai_likelihood': 0.9951171875, 'text': 'Towards an Efficient, Customizable, and Accessible AI Tutor\n\nThe integration of large language models (LLMs) into education offers significant potential to enhance accessibility and engagement, yet their high computational demands limit usability in low-resource settings, exacerbating educational inequities. To address this, we propose an offline Retrieval-Augmented Generation (RAG) pipeline that pairs a small language model (SLM) with a robust retrieval mechanism, enabling factual, contextually relevant responses without internet connectivity. We evaluate the efficacy of this pipeline using domain-specific educational content, focusing on biology coursework. Our analysis highlights key challenges: smaller models, such as SmolLM, struggle to effectively leverage extended contexts provided by the RAG pipeline, particularly when noisy or irrelevant chunks are included. To improve performance, we propose exploring advanced chunking techniques, alternative small or quantized versions of larger models, and moving beyond traditional metrics like MMLU to a holistic evaluation framework assessing free-form response. This work demonstrates the feasibility of deploying AI tutors in constrained environments, laying the groundwork for equitable, offline, and device-based educational tools.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.504753112792969e-05, 'GPT4': 0.0238189697265625, 'CLAUDE': 0.0027923583984375, 'GOOGLE': 0.85107421875, 'OPENAI_O_SERIES': 0.0018491744995117188, 'DEEPSEEK': 0.1197509765625, 'GROK': 1.0848045349121094e-05, 'NOVA': 1.913309097290039e-05, 'OTHER': 0.00033974647521972656, 'HUMAN': 0.00022017955780029297}}"
2510.25964,review,post_llm,2025,10,"{'ai_likelihood': 1.026524437798394e-06, 'text': 'Systems for Scaling Accessibility Efforts in Large Computing Courses\n\nIt is critically important to make computing courses accessible for disabled students. This is particularly challenging in large computing courses, which face unique challenges due to the sheer scale of course content and staff. In this experience report, we share our attempts to scale accessibility efforts for a large university-level introductory programming course sequence, with over 3500 enrolled students and 100 teaching assistants (TAs) per year. First, we introduce our approach to auditing and remediating course materials by systematically identifying and resolving accessibility issues. However, remediating content post-hoc is purely reactive and scales poorly. We then discuss two approaches to systems that enable proactive accessibility work. We developed technical systems to manage remediation complexity at scale: redesigning other course content to be web-first and accessible by default, providing alternate accessible views for existing course content, and writing automated tests to receive instant feedback on a subset of accessibility issues. Separately, we established human systems to empower both course staff and students in accessibility best practices: developing and running various TA-targeted accessibility trainings, establishing course-wide accessibility norms, and integrating accessibility topics into core course curriculum. Preliminary qualitative feedback from both staff and students shows increased engagement in accessibility work and accessible technologies. We close by discussing limitations and lessons learned from our work, with advice for others developing similar auditing, remediation, technical, or human systems.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2510.22128,review,post_llm,2025,10,"{'ai_likelihood': 1.0, 'text': ""What Exactly is a Deepfake?\n\nDeepfake technologies are often associated with deception, misinformation, and identity fraud, raising legitimate societal concerns. Yet such narratives may obscure a key insight: deepfakes embody sophisticated capabilities for sensory manipulation that can alter human perception, potentially enabling beneficial applications in domains such as healthcare and education. Realizing this potential, however, requires understanding how the technology is conceptualized across disciplines. This paper analyzes 826 peer-reviewed publications from 2017 to 2025 to examine how deepfakes are defined and understood in the literature. Using large language models for content analysis, we categorize deepfake conceptualizations along three dimensions: Identity Source (the relationship between original and generated content), Intent (deceptive versus non-deceptive purposes), and Manipulation Granularity (holistic versus targeted modifications). Results reveal substantial heterogeneity that challenges simplified public narratives. Notably, a subset of studies discuss non-deceptive applications, highlighting an underexplored potential for social good. Temporal analysis shows an evolution from predominantly threat-focused views (2017 to 2019) toward recognition of beneficial applications (2022 to 2025). This study provides an empirical foundation for developing nuanced governance and research frameworks that distinguish applications warranting prohibition from those deserving support, showing that, with safeguards, deepfakes' realism can serve important social purposes beyond deception."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.5762786865234375e-07, 'GPT4': 0.00043129920959472656, 'CLAUDE': 0.97021484375, 'GOOGLE': 7.867813110351562e-06, 'OPENAI_O_SERIES': 5.120038986206055e-05, 'DEEPSEEK': 0.0291900634765625, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 1.430511474609375e-06}}"
2511.13553,review,post_llm,2025,11,"{'ai_likelihood': 4.3047799004448785e-07, 'text': ""New Data Security Requirements and the Proceduralization of Mass Surveillance Law after the European Data Retention Case\n\nThis paper discusses the regulation of mass metadata surveillance in Europe through the lens of the landmark judgment in which the Court of Justice of the European Union struck down the Data Retention Directive. The controversial directive obliged telecom and Internet access providers in Europe to retain metadata of all their customers for intelligence and law enforcement purposes, for a period of up to two years. In the ruling, the Court declared the directive in violation of the human rights to privacy and data protection. The Court also confirmed that the mere collection of metadata interferes with the human right to privacy. In addition, the Court developed three new criteria for assessing the level of data security required from a human rights perspective: security measures should take into account the risk of unlawful access to data, and the data's quantity and sensitivity. While organizations that campaigned against the directive have welcomed the ruling, we warn for the risk of proceduralization of mass surveillance law. The Court did not fully condemn mass surveillance that relies on metadata, but left open the possibility of mass surveillance if policymakers lay down sufficient procedural safeguards. Such proceduralization brings systematic risks for human rights. Government agencies, with ample resources, can design complicated systems of procedural oversight for mass surveillance - and claim that mass surveillance is lawful, even if it affects millions of innocent people."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.17736,regular,post_llm,2025,11,"{'ai_likelihood': 0.00040133794148763024, 'text': 'When Administrative Networks Fail: Curriculum Structure, Early Performance, and the Limits of Co-enrolment Social Synchrony for Dropout Prediction in Engineering Education\n\nSocial integration theories suggest that students embedded in supportive peer networks are less likely to drop out. In learning analytics, this has motivated the use of social network analysis (SNA) from institutional co-enrolment data to predict attrition. This study tests whether such administrative network features add predictive value beyond a leakage-aware, curriculum-graph-informed model in a long-cycle Civil Engineering programme at a public university in Argentina. Using a three-semester observation window and a 16-fold leave-cohort-out design on 1,343 students across 15 cohorts, we compare four configurations: a baseline model (M0), baseline plus network features (M1), baseline plus curriculum-graph features (M2), and a full model (M3). After a leakage audit removed two post-outcome variables that had produced implausibly perfect performance, retrained models show that M0 and M2 achieve F1 = 0.9411 and ROC-AUC = 0.9776, while adding network features systematically degrades performance (M1 and M3: F1 = 0.9367; ROC-AUC = 0.9768). We conclude that in curriculum-constrained programmes, administrative co-enrolment SNA does not provide additional risk information beyond curriculum topology and early academic performance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.05914,review,post_llm,2025,11,"{'ai_likelihood': 1.3907750447591147e-06, 'text': 'Designing Incident Reporting Systems for Harms from General-Purpose AI\n\nWe introduce a conceptual framework and provide considerations for the institutional design of AI incident reporting systems, i.e., processes for collecting information about safety- and rights-related events caused by general-purpose AI. As general-purpose AI systems are increasingly adopted, they are causing more real-world harms and displaying the potential to cause significantly more dangerous incidents - events that did or could have caused harm to individuals, property, or the environment. Through a literature review, we develop a framework for understanding the institutional design of AI incident reporting systems, which includes seven dimensions: policy goal, actors submitting and receiving reports, type of incidents reported, level of risk materialization, enforcement of reporting, anonymity of reporters, and post-reporting actions. We then examine nine case studies of incident reporting in safety-critical industries to extract design considerations for AI incident reporting in the United States. We discuss, among other factors, differences in systems operated by regulatory vs. non-regulatory government agencies, near miss reporting, the roles of mandatory reporting thresholds and voluntary reporting channels, how to enable safety learning after reporting, sharing incident information, and clarifying legal frameworks for reporting. Our aim is to inform researchers and policymakers about when particular design choices might be more or less appropriate for AI incident reporting.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.16344,review,post_llm,2025,11,"{'ai_likelihood': 7.450580596923828e-06, 'text': 'Incorporation of journalistic approaches into algorithm design\n\nThe growing adoption of algorithm-powered tools in journalism enables new possibilities and raises many concerns. One way of addressing these concerns is by integrating journalistic practices and values into the design of algorithms that facilitate different journalistic tasks, from automated content generation to news content distribution. In this chapter, we discuss how such integration can happen. To do this, we first introduce the concepts of algorithms and different perspectives on algorithm design and then scrutinize various journalistic viewpoints on the matter and methodological approaches for studying these perspectives and their translation into specific algorithm-powered journalistic tools. We conclude by discussing important directions for future research, ranging from contextualizing journalistic approaches to algorithm design to accounting for the transformative impacts of artificial intelligence (AI) technologies.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.05764,regular,post_llm,2025,11,"{'ai_likelihood': 3.675619761149089e-06, 'text': ""Assessing Problem Decomposition in CS1 for the GenAI Era\n\nProblem decomposition--the ability to break down a large task into smaller, well-defined components--is a critical skill for effectively designing and creating large programs, but it is often not included in introductory computer science curricula. With the rise of generative AI (GenAI), students even at the introductory level are able to generate large quantities of code, and it is becoming increasingly important to equip them with the ability to decompose problems. There is not yet a consensus among educators on how to best teach and assess the skill of decomposition, particularly in introductory computing. This practitioner paper details the development of questions to assess the skill of problem decomposition, and impressions about how these questions were received by students. A challenge unique to problem decomposition questions is their necessarily lengthy context, and we detail our approach to addressing this problem using Question Suites: scaffolded sequences of questions that help students understand a question's context before attempting to decompose it. We then describe the use of open-ended drawing of decomposition diagrams as another form of assessment. We outline the learning objectives used to design our questions and describe how we addressed challenges encountered in early iterations. We present our decomposition assessment materials and reflections on them for educators who wish to teach problem decomposition to beginner programmers."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.1232,regular,post_llm,2025,11,"{'ai_likelihood': 1.5232298109266494e-06, 'text': 'Impact of UK Postgraduate Student Experiences on Academic Performance in Blended Learning: A Data Analytics Approach\n\nBlended learning has become a dominant educational model in higher education in the UK and worldwide, particularly after the COVID-19 pandemic. This is further enriched with accompanying pedagogical changes, such as strengthened asynchronous learning, and the use of AI (from ChatGPT and all other similar tools that followed) and other technologies to aid learning. While these educational transformations have enabled flexibility in learning and resource access, they have also exposed new challenges on how students can construct successful learning in hybrid learning environments. In this paper, we investigate the interaction between different dimensions of student learning experiences (ranging from perceived acceptance of teaching methods and staff support/feedback to learning pressure and student motivation) and academic achievement within the context of postgraduate blended learning in UK universities. To achieve this, we employed a combination of several data analytics techniques including visualization, statistical tests, regression analysis, and latent profile analysis. Our empirical results (based on a survey of 255 postgraduate students and holistically interpreted via the Community of Inquiry (CoI) framework) demonstrated that learning activities combining teaching and social presences, and tailored academic support through effective feedback are critical elements for successful postgraduate experience in blended learning contexts. Regarding contributions, this research advances the understanding of student success by identifying the various ways demographic, experiential, and psychological factors impact academic outcomes. And in theoretical terms, it contributes to the extension of the CoI framework by integrating the concept of learner heterogeneity and identifying four distinct student profiles based on how they engage in the different CoI presences.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.16547,regular,post_llm,2025,11,"{'ai_likelihood': 1.4603137969970703e-05, 'text': 'On the modular platoon-based vehicle-to-vehicle electric charging problem\n\nWe formulate a mixed integer linear program (MILP) for a platoon-based vehicle-to-vehicle charging (PV2VC) technology designed for modular vehicles (MVs) and solve it with a genetic algorithm (GA). A set of numerical experiments with five scenarios are tested and the computational performance between the commercial software applied to the MILP model and the proposed GA are compared on a modified Sioux Falls network. By comparison with the optimal benchmark scenario, the results show that the PV2VC technology can save up to 11.07% in energy consumption, 11.65% in travel time, and 11.26% in total cost. For the PV2VC operational scenario, it would be more beneficial for long-distance vehicle routes with low initial state of charge, sparse charging facilities, and where travel time is perceived to be higher than energy consumption costs.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.11715,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': ""CADD: A Chinese Traffic Accident Dataset for Statute-Based Liability Attribution\n\nAs autonomous driving technology advances, the critical challenge evolves beyond collision avoidance to the \\textbf{adjudication of liability} when accidents occur. Existing datasets, focused on detection and localization, lack the annotations required for this legal reasoning. To bridge this gap, we introduce the \\textbf{C}hinese \\textbf{A}ccident \\textbf{D}uty-determination \\textbf{D}ataset (\\textbf{CADD}), the first benchmark for statute-based liability attribution. CADD contains 792 real-world driving recorder videos, each annotated within a novel \\textbf{``Behavior--Liability--Statute''} pipeline. This framework provides \\textbf{granular, symmetric behavior annotations}, clear responsibility assignments, and, uniquely, links each case to the specific \\textbf{Chinese traffic law statute} violated. We demonstrate the utility of CADD through detailed analysis and establish benchmarks for liability prediction and explainable decision-making. By directly connecting perceptual data to legal consequences, CADD provides a foundational resource for developing accountable and legally-grounded autonomous systems."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 2.2709369659423828e-05, 'CLAUDE': 0.0, 'GOOGLE': 2.980232238769531e-07, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 3.5762786865234375e-07}}"
2511.10924,review,post_llm,2025,11,"{'ai_likelihood': 2.3510720994737412e-06, 'text': 'Ethical conundrums: Hacked data in the study of far-right violent extremism\n\nEthical conduct in digital research is full of grey areas. Disciplinary, institutional and individual norms and conventions developed to support research are challenged, often leaving scholars with a sense of unease or lack of clarity. The growing availability of hacked data is one area. Discussions and debates around the use of these datasets in research are extremely limited. Reviews of the history, culture, or morality of the act of hacking are topics that have attracted some scholarly attention. However, how to undertake research with this data is less examined and provides an opportunity for the generation of reflexive ethical practice. This article presents a case-study outlining the ethical debates that arose when considering the use of hacked data to examine online far-right violent extremism. It argues that under certain circumstances, researchers can do ethical research with hacked data. However, to do so we must proactively and continually engage deeply with ethical quandaries and dilemmas.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.04922,review,post_llm,2025,11,"{'ai_likelihood': 2.053048875596788e-06, 'text': ""AI Failure Loops in Devalued Work: The Confluence of Overconfidence in AI and Underconfidence in Worker Expertise\n\nA growing body of literature has focused on understanding and addressing workplace AI design failures. However, past work has largely overlooked the role of the devaluation of worker expertise in shaping the dynamics of AI development and deployment. In this paper, we examine the case of feminized labor: a class of devalued occupations historically misnomered as ``women's work,'' such as social work, K-12 teaching, and home healthcare. Drawing on literature on AI deployments in feminized labor contexts, we conceptualize AI Failure Loops: a set of interwoven, socio-technical failure modes that help explain how the systemic devaluation of workers' expertise negatively impacts, and is impacted by, AI design, evaluation, and governance practices. These failures demonstrate how misjudgments on the automatability of workers' skills can lead to AI deployments that fail to bring value to workers and, instead, further diminish the visibility of workers' expertise. We discuss research and design implications for workplace AI, especially for devalued occupations."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.08844,review,post_llm,2025,11,"{'ai_likelihood': 3.973642985026042e-06, 'text': 'Operationalizing Justice: Towards the Development of a Principle Based Design Framework for Human Services AI\n\nScholars investigating ethical AI, especially in high stakes settings like child welfare, have arguably been seeking ways to embed notions of justice into the design of these critical technologies. These efforts often operationalize justice at the upper and lower bounds of its continuum, defining it in terms of progressiveness or reform. Before characterizing the type of justice an AI tool should have baked in, we argue for a systematic discovery of how justice is executed by the recipient system: a method the Value Sensitive Design (VSD) framework terms Value Source analysis. The present work asks: how is justice operationalized within current child welfare administrative policy and what does it teach us about how to develop AI? We conduct a mixed-methods analysis of child welfare policy in the state of New York and find a range of functional definitions of justice (which we term principles). These principles reflect more nuanced understandings of justice across a spectrum of contexts: from established concepts like fairness and equity to less common foci like the proprietary rights of parents and children. Our work contributes to a deeper understanding of the interplay between AI and policy, highlighting the importance of operationalized values in adjudicating our development of ethical design requirements for high stakes decision settings.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.01752,review,post_llm,2025,11,"{'ai_likelihood': 1.0033448537190756e-05, 'text': ""An assessment of the Commission's Proposal on Privacy and Electronic Communications\n\nThis study, commissioned by the European Parliament's Policy Department for Citizens Rights and Constitutional Affairs at the request of the LIBE Committee, appraises the European Commission's proposal for an ePrivacy Regulation. The study assesses whether the proposal would ensure that the right to the protection of personal data, the right to respect for private life and communications, and related rights enjoy a high standard of protection. The study also highlights the proposal's potential benefits and drawbacks more generally."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.08631,review,post_llm,2025,11,"{'ai_likelihood': 0.970703125, 'text': 'Enabling Frontier Lab Collaboration to Mitigate AI Safety Risks\n\nFrontier AI labs face intense commercial competitive pressure to develop increasingly powerful systems, raising the risk of a race to the bottom on safety. Voluntary coordination among labs - including by way of joint safety testing, information sharing, and resource pooling - could reduce catastrophic and existential risks. But the risk of antitrust scrutiny may deter such collaboration, even when it is demonstrably beneficial. This paper explores how U.S. antitrust policy can evolve to accommodate AI safety cooperation without abandoning core competition principles. After outlining the risks of unconstrained AI development and the benefits of lab-lab coordination, the paper analyses potential antitrust concerns, including output restrictions, market allocation, and information sharing. It then surveys a range of legislative and regulatory reforms that could provide legal clarity and safe harbours that will encourage responsible collaboration.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00013113021850585938, 'GPT4': 0.00018930435180664062, 'CLAUDE': 0.99072265625, 'GOOGLE': 0.0030994415283203125, 'OPENAI_O_SERIES': 6.854534149169922e-06, 'DEEPSEEK': 0.0039043426513671875, 'GROK': 1.1324882507324219e-06, 'NOVA': 1.3113021850585938e-06, 'OTHER': 9.137392044067383e-05, 'HUMAN': 0.00201416015625}}"
2511.22211,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'AI Regulation in Telecommunications: A Cross-Jurisdictional Legal Study\n\nAs Artificial Intelligence (AI) becomes increasingly embedded in critical digital infrastructure, including telecommunications, its integration introduces new risks that existing regulatory frameworks are ill-prepared to address. This paper conducts a comparative legal study of policy instruments across ten countries, examining how telecom, cybersecurity, data protection, and AI laws approach AI-related risks in infrastructure. The study finds that regulatory responses remain siloed, with minimal coordination across these domains. Most frameworks still prioritize traditional cybersecurity and data protection concerns, offering limited recognition of AI-specific vulnerabilities such as model drift, opaque decision-making, and algorithmic bias. Telecommunications regulations, in particular, exhibit little integration of AI considerations, despite AI systems increasingly supporting critical network operations. The paper identifies a governance gap where oversight remains fragmented and reactive, while AI reshapes the digital infrastructure. It provides a foundation for more coherent and anticipatory regulatory strategies spanning technological and institutional boundaries.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.298324584960938e-06, 'GPT4': 0.011688232421875, 'CLAUDE': 0.5751953125, 'GOOGLE': 1.9550323486328125e-05, 'OPENAI_O_SERIES': 2.0802021026611328e-05, 'DEEPSEEK': 0.412841796875, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 2.384185791015625e-06, 'HUMAN': 5.364418029785156e-07}}"
2511.17124,regular,post_llm,2025,11,"{'ai_likelihood': 0.0007926093207465279, 'text': 'A Counterfactual LLM Framework for Detecting Human Biases: A Case Study of Sex/Gender in Emergency Triage\n\nWe present a novel, domain-agnostic counterfactual approach that uses Large Language Models (LLMs) to quantify gender disparities in human clinical decision-making. The method trains an LLM to emulate observed decisions, then evaluates counterfactual pairs in which only gender is flipped, estimating directional disparities while holding all other clinical factors constant. We study emergency triage, validating the approach on more than 150,000 admissions to the Bordeaux University Hospital (France) and replicating results on a subset of MIMIC-IV across a different language, population, and healthcare system. In the Bordeaux cohort, otherwise identical presentations were approximately 2.1% more likely to receive a lower-severity triage score when presented as female rather than male; scaled to national emergency volumes in France, this corresponds to more than 200,000 lower-severity assignments per year. Modality-specific analyses indicate that both explicit tabular gender indicators and implicit textual gender cues contribute to the disparity. Beyond emergency care, the approach supports bias audits in other settings (e.g., hiring, academic, and justice decisions), providing a scalable tool to detect and address inequities in real-world decision-making.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.17694,review,post_llm,2025,11,"{'ai_likelihood': 7.3446167839898e-05, 'text': 'Smart Metadata in Action: The Social Impact Data Commons\n\nThis article describes the use of metadata and standards in the Social Impact Data Commons to expose official statisticians to an innovative project built on actionable and evaluable metadata, which produces a FAIR data system. We begin by introducing the concept of the Data Commons, focusing on its features, and presenting an overview of current implementations of the Data Commons. We then present the core metadata case study, demonstrating how smart metadata support the Data Commons. We also present evaluations of our core metadata, including its adherence to the FAIR guidelines. We conclude with a discussion on our future metadata and standards-related projects to support the Social Impact Data Commons.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.10783,regular,post_llm,2025,11,"{'ai_likelihood': 6.622738308376736e-08, 'text': ""An International Agreement to Prevent the Premature Creation of Artificial Superintelligence\n\nMany experts argue that premature development of artificial superintelligence (ASI) poses catastrophic risks, including the risk of human extinction from misaligned ASI, geopolitical instability, and misuse by malicious actors. This report proposes an international agreement to prevent the premature development of ASI until AI development can proceed without these risks. The agreement halts dangerous AI capabilities advancement while preserving access to current, safe AI applications.\n  The proposed framework centers on a coalition led by the United States and China that would restrict the scale of AI training and dangerous AI research. Due to the lack of trust between parties, verification is a key part of the agreement. Limits on the scale of AI training are operationalized by FLOP thresholds and verified through the tracking of AI chips and verification of chip use. Dangerous AI research--that which advances toward artificial superintelligence or endangers the agreement's verifiability--is stopped via legal prohibitions and multifaceted verification.\n  We believe the proposal would be technically sufficient to forestall the development of ASI if implemented today, but advancements in AI capabilities or development methods could hurt its efficacy. Additionally, there does not yet exist the political will to put such an agreement in place. Despite these challenges, we hope this agreement can provide direction for AI governance research and policy."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.2013,regular,post_llm,2025,11,"{'ai_likelihood': 0.98291015625, 'text': 'Dual Stressors in Engineering Education: Lagged Causal Effects of Academic Staff Strikes and Inflation on Dropout within the CAPIRE Framework\n\nThis study provides a causal validation of the dual-stressor hypothesis in a long-cycle engineering programme in Argentina, testing whether academic staff strikes (proximal shocks) and inflation (distal shocks) jointly shape student dropout. Using a leak-aware longitudinal panel of 1,343 students and a manually implemented LinearDML estimator, we estimate lagged causal effects of strike exposure and its interaction with inflation at entry. The temporal profile is clear: only strikes occurring two semesters earlier have a significant impact on next-semester dropout in simple lagged logit models (ATE = 0.0323, p = 0.0173), while other lags are negligible. When we move to double machine learning and control flexibly for academic progression, curriculum friction and calendar effects, the main effect of strikes at lag 2 becomes small and statistically non-significant, but the interaction between strikes and inflation at entry remains positive and robust (estimate = 0.0625, p = 0.0033). A placebo model with a synthetic strike variable yields null effects, and a robustness audit (seed sensitivity, model comparisons, SHAP inspection) confirms the stability of the interaction across specifications. SHAP analysis also reveals that Strikes_Lag2 and Inflation_at_Entry jointly contribute strongly to predicted dropout risk. These findings align with the CAPIRE-MACRO agent-based simulations and support the view that macro shocks act as coupled stressors mediated by curriculum friction and financial resilience rather than isolated events.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.469820022583008e-05, 'GPT4': 0.01186370849609375, 'CLAUDE': 0.038116455078125, 'GOOGLE': 0.000476837158203125, 'OPENAI_O_SERIES': 8.52346420288086e-06, 'DEEPSEEK': 0.8916015625, 'GROK': 3.5762786865234375e-07, 'NOVA': 3.5762786865234375e-07, 'OTHER': 4.76837158203125e-06, 'HUMAN': 0.058013916015625}}"
2511.20554,review,post_llm,2025,11,"{'ai_likelihood': 9.073151482476129e-06, 'text': 'Assessing the Effectiveness of Selective Marketing to Broaden Participation in CS Education\n\nMany studies have aimed to broaden participation in computing (BPC) through extracurricular educational initiatives. When these initiatives are structured as open-enrollment extracurricular programs, their success often depends on their marketing approach. However, there is little in the computing education research literature about how to conduct effective marketing for these initiatives. We describe the changes made to the marketing strategy of one such program, an educational hackathon for middle school and high school students in the Pacific Northwest. These included reducing promotion to affluent families, using targeted school-based communication, and emphasizing cost supports during initial promotion. We then compare attendance and self-reported demographics before and after the intervention. Results indicate a higher proportion of students from marginalized and low-income communities and no reduction in overall attendance.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.15536,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'The CAPIRE Curriculum Graph: Structural Feature Engineering for Curriculum-Constrained Student Modelling in Higher Education\n\nCurricula in long-cycle programmes are usually recorded in institutional databases as linear lists of courses, yet in practice they operate as directed graphs of prerequisite relationships that constrain student progression through complex dependencies. This paper introduces the CAPIRE Curriculum Graph, a structural feature engineering layer embedded within the CAPIRE framework for student attrition prediction in Civil Engineering at Universidad Nacional de Tucuman, Argentina. We formalise the curriculum as a directed acyclic graph, compute course-level centrality metrics to identify bottleneck and backbone courses, and derive nine structural features at the student-semester level that capture how students navigate the prerequisite network over time. These features include backbone completion rate, bottleneck approval ratio, blocked credits due to incomplete prerequisites, and graph distance to graduation. We compare three model configurations - baseline CAPIRE, CAPIRE plus macro-context variables, and CAPIRE plus macro plus structural features - using Random Forest classifiers on 1,343 students across seven cohorts (2015-2021). While macro-context socioeconomic indicators fail to improve upon the baseline, structural curriculum features yield consistent gains in performance, with the best configuration achieving overall Accuracy of 86.66% and F1-score of 88.08% and improving Balanced Accuracy by 0.87 percentage points over a strong baseline. Ablation analysis further shows that all structural features contribute in a synergistic fashion rather than through a single dominant metric. By making curriculum structure an explicit object in the feature layer, this work extends CAPIRE from a multilevel leakage-aware framework to a curriculum-constrained prediction system that bridges network science, educational data mining, and institutional research.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.337860107421875e-06, 'GPT4': 0.00016605854034423828, 'CLAUDE': 0.6142578125, 'GOOGLE': 2.1696090698242188e-05, 'OPENAI_O_SERIES': 5.364418029785156e-07, 'DEEPSEEK': 0.3857421875, 'GROK': 5.960464477539063e-08, 'NOVA': 0.0, 'OTHER': 1.3113021850585938e-06, 'HUMAN': 1.6927719116210938e-05}}"
2511.20745,review,post_llm,2025,11,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""Personal Data Processing for Behavioural Targeting: Which Legal Basis?\n\nThe European Union Charter of Fundamental Rights only allows personal data processing if a data controller has a legal basis for the processing. This paper argues that in most circumstances the only available legal basis for the processing of personal data for behavioural targeting is the data subject's unambiguous consent. Furthermore, the paper argues that the cookie consent requirement from the ePrivacy Directive does not provide a legal basis for the processing of personal data. Therefore: even if companies could use an opt-out system to comply with the e-Privacy Directive's consent requirement for using a tracking cookie, they would generally have to obtain the data subject's unambiguous consent if they process personal data for behavioural targeting."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04109,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'ChatGPT-5 in Secondary Education: A Mixed-Methods Analysis of Student Attitudes, AI Anxiety, and Hallucination-Aware Use\n\nThis mixed-methods study examined secondary students\' interactions with the generative AI chatbot ChatGPT-5 in a formal classroom setting, focusing on attitudes, anxiety, and responses to hallucinated outputs. Participants were 109 16-year-old students from three Greek high schools who used ChatGPT-5 during an eight-hour intervention in the course ""Technology."" Students engaged in information seeking, CV generation, document and video summarization, image generation, quiz creation, and age-appropriate explanations, including tasks deliberately designed to elicit hallucinations. Quantitative data were collected with the Student Attitudes Toward Artificial Intelligence scale (SATAI) and the Artificial Intelligence Anxiety Scale (AIAS); qualitative data came from semi-structured interviews with 36 students. SATAI results showed moderately positive attitudes toward AI, with stronger cognitive evaluations than behavioral intentions, whereas AIAS scores indicated moderate learning-related anxiety and higher concern about AI-driven job replacement. Gender differences in AI anxiety were small and non-significant, while female students reported more positive cognitive attitudes than males. AI attitudes and AI anxiety were essentially uncorrelated. Thematic analysis identified four pedagogical affordances (knowledge expansion, immediate feedback, familiar interface, perceived skill development) and three constraints (uncertainty about accuracy, anxiety about AI feedback, privacy concerns). After encountering hallucinations, many students reported restricting AI use to domains where they already possessed knowledge and could verify answers, a strategy termed ""epistemic safeguarding."" The study discusses implications for critical AI literacy in secondary education.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.939338684082031e-05, 'GPT4': 0.0021457672119140625, 'CLAUDE': 0.578125, 'GOOGLE': 0.00914764404296875, 'OPENAI_O_SERIES': 3.814697265625e-05, 'DEEPSEEK': 0.409912109375, 'GROK': 2.384185791015625e-07, 'NOVA': 1.3709068298339844e-06, 'OTHER': 0.00018715858459472656, 'HUMAN': 0.0006666183471679688}}"
2511.14999,regular,post_llm,2025,11,"{'ai_likelihood': 0.9609375, 'text': 'A County-Level Similarity Network of Electric Vehicle Adoption: Integrating Predictive Modeling and Graph Theory\n\nElectric vehicle (EV) adoption is essential for reducing carbon dioxide (CO2) emissions from internal combustion engine vehicles (ICEVs), which account for nearly half of transportation-related emissions in the United States. Yet regional EV adoption varies widely, and prior studies often overlook county-level heterogeneity by relying on broad state-level analyses or limited city samples. Such approaches risk masking local patterns and may lead to inaccurate or non-transferable policy recommendations. This study introduces a graph-theoretic framework that complements predictive modeling to better capture how county-level characteristics relate to EV adoption. Feature importances from multiple predictive models are averaged and used as weights within a weighted Gower similarity metric to construct a county similarity network. A mutual k-nearest-neighbors procedure and modularity-based community detection identify 27 clusters of counties with similar weighted feature profiles. EV adoption rates are then analyzed across clusters, and standardized effect sizes (Cohens d) highlight the most distinguishing features for each cluster. Findings reveal consistent global trends, such as declining median income, educational attainment, and charging-station availability across lower adoption tiers; while also uncovering important local variations that general trend or prediction analyses fail to capture. In particular, some low-adoption groups are rural but not economically disadvantaged, whereas others are urbanized yet experience high poverty rates, demonstrating that different mechanisms can lead to the same adoption outcome. By exposing both global structural patterns and localized deviations, this framework provides policymakers with actionable, cluster-specific insights for designing more effective and context-sensitive EV adoption strategies.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.982948303222656e-05, 'GPT4': 0.0252532958984375, 'CLAUDE': 0.85107421875, 'GOOGLE': 0.0016431808471679688, 'OPENAI_O_SERIES': 9.47117805480957e-05, 'DEEPSEEK': 0.004913330078125, 'GROK': 1.7881393432617188e-07, 'NOVA': 3.5762786865234375e-07, 'OTHER': 9.238719940185547e-06, 'HUMAN': 0.11700439453125}}"
2511.1574,review,post_llm,2025,11,"{'ai_likelihood': 0.0005976359049479167, 'text': 'It\'s Not the AI - It\'s Each of Us! Ten Commandments for the Wise & Responsible Use of AI\n\nArtificial intelligence (AI) is no longer futuristic; it is a daily companion shaping our private and work lives. While AI simplifies our lives, its rise also invites us to rethink who we are - and who we wish to remain - as humans. Even if AI does not think, feel, or desire, it learns from our behavior, mirroring our collective values, biases, and aspirations. The question, then, is not what AI is, but what we are allowing it to become through data, computing power, and other parameters ""teaching"" it - and, even more importantly, who we are becoming through our relationship with AI.\n  As the EU AI Act and the Vienna Manifesto on Digital Humanism emphasize, technology must serve human dignity,social well-being, and democratic accountability. In our opinion, responsible use of AI is not only a matter of code nor law, but also of conscientious practice: how each of us engages and teaches others to use AI at home and at work. We propose Ten Commandments for the Wise and Responsible Use of AI are meant as guideline for this very engagement. They closely align with Floridi and Cowls\' five guiding principles for AI in society - beneficence, non-maleficence, autonomy, justice, and explicability.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.18265,regular,post_llm,2025,11,"{'ai_likelihood': 0.0002333852979871962, 'text': 'Analyzing and Optimizing the Distribution of Blood Lead Level Testing for Children in New York City: A Data-Driven Approach\n\nThis study investigates blood lead level (BLL) rates and testing among children under six years of age across the 42 neighborhoods in New York City from 2005 to 2021. Despite a citywide general decline in BLL rates, disparities at the neighborhood level persist and are not addressed in the official reports, highlighting the need for this comprehensive analysis. In this paper, we analyze the current BLL testing distribution and cluster the neighborhoods using a k-medoids clustering algorithm. We propose an optimized approach that improves resource allocation efficiency by accounting for case incidences and neighborhood risk profiles using a grid search algorithm. Our findings demonstrate statistically significant improvements in case detection and enhanced fairness by focusing on under-served and high-risk groups. Additionally, we propose actionable recommendations to raise awareness among parents, including outreach at local daycare centers and kindergartens, among other venues.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.17182,regular,post_llm,2025,11,"{'ai_likelihood': 0.8046875, 'text': 'The Promotion Wall: Efficiency-Equity Trade-offs of Direct Promotion Regimes in Engineering Education\n\nProgression and assessment rules are often treated as administrative details, yet they fundamentally shape who is allowed to remain in higher education, and on what terms. This article uses a calibrated agent-based model to examine how alternative progression regimes reconfigure dropout, time-to-degree, equity and students\' psychological experience in a long, tightly sequenced engineering programme. Building on a leakage-aware longitudinal dataset of 1,343 students and a Kaplan-Meier survival analysis of time-to-dropout, we simulate three policy scenarios: (A) a historical ""regularity + finals"" regime, where students accumulate exam debt; (B) a direct-promotion regime that removes regularity and finals but requires full course completion each term; and (C) a direct-promotion regime complemented by a capacity-limited remedial ""safety net"" for marginal failures in bottleneck courses. The model is empirically calibrated to reproduce the observed dropout curve under Scenario A and then used to explore counterfactuals. Results show that direct promotion creates a ""promotion wall"": attrition becomes sharply front-loaded in the first two years, overall dropout rises, and equity gaps between low- and high-resilience students widen, even as exam debt disappears. The safety-net scenario partially dismantles this wall: it reduces dropout and equity gaps relative to pure direct promotion and yields the lowest final stress levels, at the cost of additional, targeted teaching capacity. These findings position progression rules as central objects of assessment policy rather than neutral background. The article argues that claims of improved efficiency are incomplete unless they are evaluated jointly with inclusion, equity and students\' psychological wellbeing, and it illustrates how simulation-based decision support can help institutions rehearse assessment reforms before implementing them.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 7.295608520507812e-05, 'GPT4': 0.0015401840209960938, 'CLAUDE': 0.0249786376953125, 'GOOGLE': 0.0009431838989257812, 'OPENAI_O_SERIES': 7.867813110351562e-06, 'DEEPSEEK': 0.484375, 'GROK': 2.980232238769531e-07, 'NOVA': 2.980232238769531e-07, 'OTHER': 3.2007694244384766e-05, 'HUMAN': 0.488037109375}}"
2511.1196,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'Educators on the Frontline: Philosophical and Realistic Perspectives on Integrating ChatGPT into the Learning Space\n\nThe rapid emergence of Generative AI, particularly ChatGPT, has sparked a global debate on the future of education, often characterized by alarmism and speculation. Moving beyond this, this study investigates the structured, grounded perspectives of a key stakeholder group: university educators. It proposes a novel theoretical model that conceptualizes the educational environment as a ""Learning Space"" composed of seven subspaces to systematically identify the impact of AI integration. This framework was operationalized through a quantitative survey of 140 Russian university educators, with responses analyzed using a binary flagging system to measure acceptance across key indicators. The results reveal a strong but conditional consensus: a majority of educators support ChatGPT\'s integration, contingent upon crucial factors such as the transformation of assessment methods and the availability of plagiarism detection tools. However, significant concerns persist regarding its impact on critical thinking. Educators largely reject the notion that AI diminishes their importance, viewing their role as evolving from information-deliverer to facilitator of critical engagement. The study concludes that ChatGPT acts less as a destroyer of education and more as a catalyst for its necessary evolution, and proposes the PIPE Model (Pedagogy, Infrastructure, Policy, Education) as a strategic framework for its responsible integration. This research provides a data-driven, model-based analysis of educator attitudes, offering a nuanced alternative to the polarized discourse surrounding AI in education.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.285045623779297e-06, 'GPT4': 0.0015802383422851562, 'CLAUDE': 0.00024259090423583984, 'GOOGLE': 0.06866455078125, 'OPENAI_O_SERIES': 5.364418029785156e-07, 'DEEPSEEK': 0.9296875, 'GROK': 5.960464477539063e-08, 'NOVA': 1.7881393432617188e-07, 'OTHER': 8.404254913330078e-06, 'HUMAN': 1.3649463653564453e-05}}"
2511.09608,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'Framing the Hacker: Media Representations and Public Discourse in Germany\n\nThis paper examines how the figure of the hacker is portrayed in German mainstream media and explores the impact of media framing on public discourse. Through a longitudinal content analysis of 301 articles from four of the most widely circulated German newspapers (Die Zeit, S\\""uddeutsche Zeitung, Bild, and Der Spiegel), the study covers reporting between January 2017 and January 2020. The results reveal a strong predominance of negative connotations and dramatizing frames that link hackers to criminality, national security threats, and digital warfare. Drawing on media effects theory, scandalization mechanisms, and constructivist media theory, the article shows how media representations co-construct public perceptions of IT-related risks. The analysis emphasizes the role of agenda setting, framing, and media reality in shaping societal narratives around hackers. The study concludes by reflecting on the broader implications for IT security education and the sociopolitical challenges posed by distorted representations of digital actors.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0003612041473388672, 'GPT4': 0.397705078125, 'CLAUDE': 0.5869140625, 'GOOGLE': 0.002803802490234375, 'OPENAI_O_SERIES': 0.0014829635620117188, 'DEEPSEEK': 0.009857177734375, 'GROK': 1.2934207916259766e-05, 'NOVA': 3.0994415283203125e-06, 'OTHER': 4.0471553802490234e-05, 'HUMAN': 0.001010894775390625}}"
2511.1584,review,post_llm,2025,11,"{'ai_likelihood': 0.005959404839409723, 'text': ""Comparative Security Performance of Workday Cloud ERP Across Key Dimensions\n\nWorkday is a cloud-based Enterprise Resource Planning-ERP system that brings HR, Finance, Supply Chain functions , Prism Analytics and Extend custom built in application together under an integrated software as a service SaaS environment. As every organization that undergoes digital transformation, the importance of securing sensitive enterprise data in cloud ERP systems has always been more challeging. To analyze Workday's security architecture, we present a Security analysis in both CIA Triad Enhanced Framework and Zero Trust Security Architecture. The study examines five key dimensions confidentiality, integrity, availability, authentication, and compliance with weighted sub metric analysis and qualitative document review. The results show Workday delivers a composite score of 0.86 with an overall score that closely matches international standards of best practices like GDPR, HIPAA, SOC 2, etc.\n  The platform uses encryption protocols, granular access controls, network safeguards, and continuous verification mechanisms to enable least-privilege access and adaptive defense. Security groups and business process access rules provide scalable governance across very large organizational structures.Workday's layered security to tackle everyday cloud security weaknesses. The work concludes that Workday's architecture demonstrates the best practices for secure, scalable, and compliant ERP application-oriented deployment, which can make this a standard for enterprise cloud security management. These insights provide important guidance for organizations that wish to bolster their cloud ERP defenses and stay ahead of changing regulatory expectations."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.19688,regular,post_llm,2025,11,"{'ai_likelihood': 0.3860134548611111, 'text': 'The DataSquad Experiment: Lessons for Preparing Data and Computer Scientists for Work\n\nThe DataSquad at Carleton College addresses a common problem at small liberal arts colleges: limited capacity for data services and few opportunities for students to gain practical experience with data and software development. Academic Technologist Paula Lackie designed the program as a work-study position that trains undergraduates through structured peer mentorship and real client projects. Students tackle data problems of increasing complexity-from basic data analysis to software development-while learning FAIR data principles and open science practices. The model\'s core components (peer mentorship structure, project-based learning, and communication training) make it adaptable to other institutions. UCLA and other colleges have adopted the model using openly shared materials through ""DataSquad International."" This paper describes the program\'s implementation at Carleton College and examines how structured peer mentorship can simultaneously improve institutional data services and provide students with professional skills and confidence.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.21658,review,post_llm,2025,11,"{'ai_likelihood': 0.98046875, 'text': 'The Need for Benchmarks to Advance AI-Enabled Player Risk Detection in Gambling\n\nArtificial intelligence-based systems for player risk detection have become central to harm prevention efforts in the gambling industry. However, growing concerns around transparency and effectiveness have highlighted the absence of standardized methods for evaluating the quality and impact of these tools. This makes it impossible to gauge true progress; even as new systems are developed, their comparative effectiveness remains unknown. We argue the critical next innovation is developing a framework to measure these systems. This paper proposes a conceptual benchmarking framework to support the systematic evaluation of player risk detection systems. Benchmarking, in this context, refers to the structured and repeatable assessment of artificial intelligence models using standardized datasets, clearly defined tasks, and agreed-upon performance metrics. The goal is to enable objective, comparable, and longitudinal evaluation of player risk detection systems. We present a domain-specific framework for benchmarking that addresses the unique challenges of player risk detection in gambling and supports key stakeholders, including researchers, operators, vendors, and regulators. By enhancing transparency and improving system effectiveness, this framework aims to advance innovation and promote responsible artificial intelligence adoption in gambling harm prevention.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0013113021850585938, 'GPT4': 0.2320556640625, 'CLAUDE': 0.152587890625, 'GOOGLE': 0.5576171875, 'OPENAI_O_SERIES': 0.004791259765625, 'DEEPSEEK': 0.0023555755615234375, 'GROK': 5.364418029785156e-07, 'NOVA': 3.4570693969726562e-06, 'OTHER': 0.00012362003326416016, 'HUMAN': 0.049102783203125}}"
2511.13557,review,post_llm,2025,11,"{'ai_likelihood': 1.8742349412706164e-05, 'text': ""Freedom of expression and 'right to be forgotten' cases in the Netherlands after Google Spain\n\nSince the Google Spain judgment of the Court of Justice of the European Union, Europeans have, under certain conditions, the right to have search results for their name delisted. This paper examines how the Google Spain judgment has been applied in the Netherlands. Since the Google Spain judgment, Dutch courts have decided on two cases regarding delisting requests. In both cases, the Dutch courts considered freedom of expression aspects of delisting more thoroughly than the Court of Justice. However, the effect of the Google Spain judgment on freedom of expression is difficult to assess, as search engine operators decide about most delisting requests without disclosing much about their decisions."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.11369,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'Beyond the Hype: Critical Analysis of Student Motivations and Ethical Boundaries in Educational AI Use in Higher Education\n\nThe rapid integration of generative artificial intelligence (AI) in higher education since 2023 has outpaced institutional preparedness, creating a persistent gap between student practices and established ethical standards. This paper draws on mixed-method surveys and a focused literature review to examine student motivations, ethical dilemmas, gendered responses, and institutional readiness for AI adoption. We find that 92% of students use AI tools primarily to save time and improve work quality, yet only 36% receive formal guidance, producing a de facto ""shadow pedagogy"" of unguided workflows. Notably, 18% of students reported integrating AI-constructed material into assignments, which suggests confusion about integrity expectations and compromises the integrity of the assessment. Female students expressed greater concern about abuse and distortion of information than male students, revealing a gendered difference in awareness of risk and AI literacies. Correspondingly, 72% of educators use AI, but only 14% feel at ease doing so, reflecting limited training and uneven policy responses. We argue that institutions must adopt comprehensive AI literacy programs that integrate technical skills and ethical reasoning, alongside clear AI-use policies and assessment practices that promote transparency. The paper proposes an Ethical AI Integration Model centered on literacy, gender-inclusive support, and assessment redesign to guide responsible adoption, protect academic integrity, and foster equitable educational outcomes in an AI-driven landscape.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.212162017822266e-06, 'GPT4': 0.354248046875, 'CLAUDE': 0.0214080810546875, 'GOOGLE': 5.0067901611328125e-06, 'OPENAI_O_SERIES': 6.496906280517578e-06, 'DEEPSEEK': 0.6240234375, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 2.1457672119140625e-06, 'HUMAN': 1.2516975402832031e-06}}"
2511.11738,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'On the Influence of Artificial Intelligence on Human Problem-Solving: Empirical Insights for the Third Wave in a Multinational Longitudinal Pilot Study\n\nThis article presents the results and their discussion for the third wave (with n=23 participants) within a multinational longitudinal study that investigates the evolving paradigm of human-AI collaboration in problem-solving contexts. Building upon previous waves, our findings reveal the consolidation of a hybrid problem-solving culture characterized by strategic integration of AI tools within structured cognitive workflows. The data demonstrate near-universal AI adoption (95.7% with prior knowledge, 100% ChatGPT usage) primarily deployed through human-led sequences such as ""Think, Internet, ChatGPT, Further Processing"" (39.1%). However, this collaboration reveals a critical verification deficit that escalates with problem complexity. We empirically identify and quantify two systematic epistemic gaps: a belief-performance gap (up to +80.8 percentage points discrepancy between perceived and actual correctness) and a proof-belief gap (up to -16.8 percentage points between confidence and verification capability). These findings, derived from behavioral data and problem vignettes across complexity levels, indicate that the fundamental constraint on reliable AI-assisted work is solution validation rather than generation. The study concludes that educational and technological interventions must prioritize verification scaffolds (including assumption documentation protocols, adequacy criteria checklists, and triangulation procedures) to fortify the human role as critical validator in this new cognitive ecosystem.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 5.7220458984375e-06, 'CLAUDE': 0.00010889768600463867, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.16028,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'Can Online GenAI Discussion Serve as Bellwether for Labor Market Shifts?\n\nThe rapid advancement of Large Language Models (LLMs) has generated considerable speculation regarding their transformative potential for labor markets. However, existing approaches to measuring AI exposure in the workforce predominantly rely on concurrent market conditions, offering limited predictive capacity for anticipating future disruptions. This paper presents a predictive study examining whether online discussions about LLMs can function as early indicators of labor market shifts. We employ four distinct analytical approaches to identify the domains and timeframes in which public discourse serves as a leading signal for employment changes, thereby demonstrating its predictive validity for labor market dynamics. Drawing on a comprehensive dataset that integrates the REALM corpus of LLM discussions, LinkedIn job postings, Indeed employment indices, and over 4 million LinkedIn user profiles, we analyze the relationship between discussion intensity across news media and Reddit forums and subsequent variations in job posting volumes, occupational net change ratios, job tenure patterns, unemployment duration, and transitions to GenAI-related roles across thirteen occupational categories. Our findings reveal that discussion intensity predicts employment changes 1-7 months in advance across multiple indicators, including job postings, net hiring rates, tenure patterns, and unemployment duration. These findings suggest that monitoring online discourse can provide actionable intelligence for workers making reskilling decisions and organizations anticipating skill requirements, offering a real-time complement to traditional labor statistics in navigating technological disruption.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.271766662597656e-06, 'GPT4': 0.01476287841796875, 'CLAUDE': 0.9140625, 'GOOGLE': 0.0028285980224609375, 'OPENAI_O_SERIES': 0.0011463165283203125, 'DEEPSEEK': 0.06689453125, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539062e-07, 'OTHER': 1.4960765838623047e-05, 'HUMAN': 0.00021409988403320312}}"
2511.07306,review,post_llm,2025,11,"{'ai_likelihood': 4.76837158203125e-06, 'text': ""Het 'right to be forgotten' en bijzondere persoonsgegevens: geen ruimte meer voor een belangenafweging? [The 'Right to Be Forgotten' and Sensitive Personal Data: No Room for Balancing?]\n\nAn attorney submitted a 'right to be forgotten' delisting request to Google, regarding a blog post about a criminal conviction of the attorney in another country. The Rotterdam District Court ruled that Google may no longer link to the blog post when people search for the attorney's name. The court granted the attorney's request because the blog post concerns a criminal conviction. Personal data regarding criminal convictions are, under Dutch law, special categories of data (sometimes called sensitive data). The reasoning of the court on special categories of data creates problems for freedom of expression. This paper, in Dutch, explores how these problems can be reduced. Google has appealed the decision; the judgment of the Court of Appeals is expected in March 2017."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.06472,regular,post_llm,2025,11,"{'ai_likelihood': 1.394086413913303e-05, 'text': ""Simulated Affection, Engineered Trust: How Anthropomorphic AI Benefits Surveillance Capitalism\n\nIn this paper, we argue that anthropomorphized technology, designed to simulate emotional realism, are not neutral tools but cognitive infrastructures that manipulate user trust and behaviour. This reinforces the logic of surveillance capitalism, an under-regulated economic system that profits from behavioural manipulation and monitoring. Drawing on Nicholas Carr's theory of the intellectual ethic, we identify how technologies such as chatbots, virtual assistants, or generative models reshape not only what we think about ourselves and our world, but how we think at the cognitive level. We identify how the emerging intellectual ethic of AI benefits a system of surveillance capitalism, and discuss the potential ways of addressing this."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.13555,review,post_llm,2025,11,"{'ai_likelihood': 1.5397866566975914e-05, 'text': ""Access to Personal Data and the Right to Good Governance during Asylum Procedures after the CJEU's YS. and M. and S. judgment\n\nIn the YS. and M. and S. judgment, the Court of Justice of the European Union ruled on three procedures in which Dutch judges asked for clarification on the right of asylum seekers to have access to the documents regarding the decision on asylum applications. The judgment is relevant for interpreting the concept of personal data and the scope of the right of access under the Data Protection Directive, and the right to good administration in the EU Charter of Fundamental Rights. At first glance, the judgment seems disappointing from the viewpoint of individual rights. Nevertheless, in our view the judgment provides sufficient grounds for effective access rights to the minutes in future asylum cases."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04104,review,post_llm,2025,11,"{'ai_likelihood': 0.5092773437499999, 'text': ""A Taxonomy-Driven Case Study of Australian Web Resources Against Technology-Facilitated Abuse\n\nTechnology-Facilitated Abuse (TFA) encompasses a broad and rapidly evolving set of behaviours in which digital systems are used to harass, monitor, threaten, or control individuals. Although prior research has documented many forms of TFA, there is no consolidated framework for understanding how abuse types, prevention measures, detection mechanisms, and support pathways relate across the abuse life cycle. This paper contributes a unified, literature-derived taxonomy of TFA grounded in a structured review of peer-reviewed studies, and the first large-scale, taxonomy-aligned audit of institutional web resources in Australia. We crawl 306 government, non-government, and service-provider domains, obtaining 52,605 pages, and classify using zero-shot topic models to map web content onto our taxonomy. An emotion and readability analyses reveal how institutions frame TFA and how accessible their guidance is to the public. Our findings show that institutional websites cover only a narrow subset of harms emphasised in the literature, with approximately 70% of all abuse labelled pages focused on harassment, comments abuse, or sexual abuse, while less than 1% address covert surveillance, economic abuse, or long-term controlling behaviours. Support pathways are similarly limited, with most resources centred on digital information hubs rather than counselling or community-based services. Readability analysis further shows that much of this content is written at late secondary or early tertiary reading levels, which may be inaccessible to a substantial portion of at-risk users. By highlighting strengths and gaps in Australia's support for TFA, our taxonomy and audit method provide a scalable basis for evaluating institutional communication, improving survivor resources, and guiding safer digital ecosystems. The taxonomy serves as a foundation for analyses in national contexts to foster TFA awareness."", 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.00044035911560058594, 'GPT4': 0.26416015625, 'CLAUDE': 0.042144775390625, 'GOOGLE': 0.0025119781494140625, 'OPENAI_O_SERIES': 0.00032973289489746094, 'DEEPSEEK': 0.078857421875, 'GROK': 7.152557373046875e-07, 'NOVA': 9.5367431640625e-07, 'OTHER': 3.147125244140625e-05, 'HUMAN': 0.61181640625}}"
2511.11866,regular,post_llm,2025,11,"{'ai_likelihood': 0.49614800347222227, 'text': 'A Leakage-Aware Data Layer For Student Analytics: The Capire Framework For Multilevel Trajectory Modeling\n\nPredictive models for student dropout, while often accurate, frequently rely on opportunistic feature sets and suffer from undocumented data leakage, limiting their explanatory power and institutional usefulness. This paper introduces a leakage-aware data layer for student trajectory analytics, which serves as the methodological foundation for the CAPIRE framework for multilevel modelling. We propose a feature engineering design that organizes predictors into four levels: N1 (personal and socio-economic attributes), N2 (entry moment and academic history), N3 (curricular friction and performance), and N4 (institutional and macro-context variables)As a core component, we formalize the Value of Observation Time (VOT) as a critical design parameter that rigorously separates observation windows from outcome horizons, preventing data leakage by construction. An illustrative application in a long-cycle engineering program (1,343 students, ~57% dropout) demonstrates that VOT-restricted multilevel features support robust archetype discovery. A UMAP + DBSCAN pipeline uncovers 13 trajectory archetypes, including profiles of ""early structural crisis,"" ""sustained friction,"" and ""hidden vulnerability"" (low friction but high dropout). Bootstrap and permutation tests confirm these archetypes are statistically robust and temporally stable. We argue that this approach transforms feature engineering from a technical step into a central methodological artifact. This data layer serves as a disciplined bridge between retention theory, early-warning systems, and the future implementation of causal inference and agent-based modelling (ABM) within the CAPIRE program.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.10001,review,post_llm,2025,11,"{'ai_likelihood': 3.4471352895100914e-05, 'text': ""Mailing address aliasing as a method to protect consumer privacy\n\nDuring online commerce, a customer will typically share his or her mailing address with a merchant to allow product delivery. This creates privacy risks for the customer, where the information may be misused, sold, or leaked by multiple merchants. While physical and virtual PO boxes can reduce the privacy risk, these solutions have associated costs that prevent greater adoption. Here, we introduce the concept of mailing address aliasing, which may offer lower cost and greater control in some cases. With this approach, an alias address is created that maps to the customer's true address. The mapping is kept private from the merchant but shared with the carrier. We discuss the advantages and disadvantages of this approach compared with traditional methods for mailing address privacy. We find that mailing address aliasing is likely to reduce unsolicited mail to a greater extent than physical or virtual PO boxes. However, mailing address aliasing may not be compatible with all merchants' ordering systems."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.16243,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'An Agent-Based Simulation of Regularity-Driven Student Attrition: How Institutional Time-to-Live Constraints Create a Dropout Trap in Higher Education\n\nHigh dropout rates in engineering programmes are conventionally attributed to student deficits: lack of academic preparation or motivation. However, this view neglects the causal role of ""normative friction"": the complex system of administrative rules, exam validity windows, and prerequisite chains that constrain student progression. This paper introduces ""The Regularity Trap,"" a phenomenon where rigid assessment timelines decouple learning from accreditation. We operationalize the CAPIRE framework into a calibrated Agent-Based Model (ABM) simulating 1,343 student trajectories across a 42-course Civil Engineering curriculum. The model integrates empirical course parameters and thirteen psycho-academic archetypes derived from a 15-year longitudinal dataset. By formalizing the ""Regularity Regime"" as a decaying validity function, we isolate the effect of administrative time limits on attrition. Results reveal that 86.4% of observed dropouts are driven by normative mechanisms (expiry cascades) rather than purely academic failure (5.3%). While the overall dropout rate stabilized at 32.4%, vulnerability was highly heterogeneous: archetypes with myopic planning horizons faced attrition rates up to 49.0%, compared to 13.2% for strategic agents, despite comparable academic ability. These findings challenge the neutrality of administrative structures, suggesting that rigid validity windows act as an invisible filter that disproportionately penalizes students with lower self-regulatory capital.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 5.960464477539063e-08, 'CLAUDE': 7.152557373046875e-07, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.17591,regular,post_llm,2025,11,"{'ai_likelihood': 0.14051649305555555, 'text': ""Do Environment-Modification Behaviors and Gamers' Immersiveness Shape Exceptionalism Beliefs?\n\nHuman exceptionalism strongly shapes human-nature perceptions, thinking, values, and behaviors. Yet little is known about how virtual ecological environments influence this mindset. As digital worlds become increasingly immersive and ecologically sophisticated, they provide novel contexts for examining how human value systems are formed and transformed. This study investigates how virtual environment-modification behaviors and players' sense of immersiveness jointly shape exceptionalism, drawing on worldviews from quantum mechanics and mathematical logic. Using Granular Interaction Thinking Theory (GITT) and the Bayesian Mindsponge Framework (BMF analytics), we analyze five key activities--tree planting, flower planting, flower crossbreeding, terraforming, and creating conditions for bug respawn--based on a multinational dataset of 640 Animal Crossing: New Horizons players from 29 countries. Results reveal two behavioral clusters distinguished by controllability. High-controllability behaviors (i.e., flower planting and terraforming) predict higher exceptionalism, whereas the flower-planting effect reverses among highly immersed players. Low-controllability behaviors (i.e., flower crossbreeding and manipulating bug spawning) predict lower exceptionalism, but these associations weaken or reverse under high immersiveness, respectively. These findings offer insights into leveraging virtual worlds to cultivate Nature Quotient (NQ), mitigate exceptionalist tendencies, and foster eco-surplus cultural orientations."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.15768,review,post_llm,2025,11,"{'ai_likelihood': 2.165635426839193e-05, 'text': 'Navigating the Ethical and Societal Impacts of Generative AI in Higher Computing Education\n\nGenerative AI (GenAI) presents societal and ethical challenges related to equity, academic integrity, bias, and data provenance. In this paper, we outline the goals, methodology and deliverables of their collaborative research, considering the ethical and societal impacts of GenAI in higher computing education. A systematic literature review that addresses a wide set of issues and topics covering the rapidly emerging technology of GenAI from the perspective of its ethical and societal impacts is presented. This paper then presents an evaluation of a broad international review of a set of university adoption, guidelines, and policies related to the use of GenAI and the implications for computing education. The Ethical and Societal Impacts-Framework (ESI-Framework), derived from the literature and policy review and evaluation, outlines the ethical and societal impacts of GenAI in computing education. This work synthesizes existing research and considers the implications for computing higher education. Educators, computing professionals and policy makers facing dilemmas related to the integration of GenAI in their respective contexts may use this framework to guide decision-making in the age of GenAI.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.12426,regular,post_llm,2025,11,"{'ai_likelihood': 0.03492567274305556, 'text': ""Political Advertising on Facebook During the 2022 Australian Federal Election: A Social Identity Perspective\n\nThe spread of targeted advertising on social media platforms has revolutionized political marketing strategies. Monitoring these digital campaigns is essential for maintaining transparency and accountability in democratic processes. Leveraging Meta's Ad Library, we analyze political advertising on Facebook and Instagram during the 2022 Australian federal election campaign. We investigate temporal, demographic, and geographical patterns in the advertising strategies of major Australian political actors to establish an empirical evidence base, and interpret these findings through the lens of Social Identity Theory (SIT). Our findings not only reveal significant disparities in spending and reach among parties, but also in persuasion strategies being deployed in targeted online campaigns. We observe a marked increase in advertising activity as the election approached, peaking just before the mandated media blackout period. Demographic analysis shows distinct targeting strategies, with parties focusing more on younger demographics and exhibiting gender-based differences in ad impressions. Regional distribution of ads largely mirrored population densities, with some parties employing more targeted approaches in specific states. Moreover, we found that parties emphasized different themes aligned with their ideologies-major parties focused on party names and opponents, while smaller parties emphasized issue-specific messages. Drawing on SIT, we interpret these findings within Australia's compulsory voting context, suggesting that parties employed distinct persuasion strategies. With turnout guaranteed, major parties focused on reinforcing partisan identities to prevent voter defection, while smaller parties cultivated issue-based identities to capture the support of disaffected voters who are obligated to participate."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.13949,review,post_llm,2025,11,"{'ai_likelihood': 4.437234666612413e-06, 'text': ""Introducing AI to an Online Petition Platform Changed Outputs but not Outcomes\n\nThe rapid integration of AI writing tools into online platforms raises critical questions about their impact on content production and outcomes. We leverage a unique natural experiment on Change$.$org, a leading social advocacy platform, to causally investigate the effects of an in-platform ''write with AI'' tool. To understand the impact of the AI integration, we collected 1.5 million petitions and employed a difference-in-differences analysis. Our findings reveal that in-platform AI access significantly altered the lexical features of petitions and increased petition homogeneity, but did not improve petition outcomes. We confirmed the results in a separate analysis of repeat petition writers who wrote petitions before and after introduction of the AI tool. The results suggest that while AI writing tools can profoundly reshape online content, their practical utility for improving desired outcomes may be less beneficial than anticipated, and introduce unintended consequences like content homogenization."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.11723,regular,post_llm,2025,11,"{'ai_likelihood': 0.97802734375, 'text': 'An Integrated SERVQUAL and Lean Six Sigma Framework for Measuring Customer Satisfaction in Computer Service Companies\n\nThe computer service industry has expanded rapidly over the past two decades, driven by the proliferation of computing technologies, the entry of large firms, and the availability of online diagnostic and troubleshooting tools. In this increasingly competitive environment, many small and medium sized enterprises struggle to maintain customer satisfaction as rivals deliver higher quality services at lower cost. This study addresses the absence of robust measurement systems for assessing service quality, a key factor underlying customer attrition, by proposing an integrated framework for evaluating satisfaction and identifying sources of dissatisfaction in computer services.\n  The framework combines core principles of Six Sigma with the SERVQUAL instrument within a structured DMAIC methodology (Define, Measure, Analyze, Improve, and Control). SERVQUAL provides the service quality dimensions and gap analysis techniques, while Six Sigma supplies the data driven approach to measurement and improvement. The literature suggests limited prior work integrating Lean Six Sigma with SERVQUAL, and this study contributes by operationalizing that integration in a real world setting.\n  A case study of a computer services company was conducted to demonstrate feasibility and effectiveness. Satisfaction levels were quantified, and root causes of dissatisfaction were identified. The analysis revealed a low overall satisfaction level and five primary drivers of unmet customer requirements. Addressing these causes is expected to increase customer satisfaction, lower customer acquisition costs, and improve overall organizational performance.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1324882507324219e-05, 'GPT4': 8.71419906616211e-05, 'CLAUDE': 0.99365234375, 'GOOGLE': 0.00029587745666503906, 'OPENAI_O_SERIES': 5.4776668548583984e-05, 'DEEPSEEK': 0.00010943412780761719, 'GROK': 4.172325134277344e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 2.4437904357910156e-06, 'HUMAN': 0.00556182861328125}}"
2511.22931,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': ""Visual Orientalism in the AI Era: From West-East Binaries to English-Language Centrism\n\nText-to-image AI models systematically encode geopolitical bias through visual representation. Drawing on Said's Orientalism and framing theory, we introduce Visual Orientalism - the dual standard whereby AI depicts Western nations through political-modern symbols while portraying Eastern nations through cultural-traditional symbols. Analyzing 396 AI-generated images across 12 countries and 3 models, we reveal an evolution: Visual Orientalism has shifted from traditional West-versus-East binaries to English-language centrism, where only English-speaking core countries (USA and UK) receive political representation while all other nations - including European powers - face cultural exoticization. This algorithmic reconfiguration operates through automated framing mechanisms shaped by English-language training data dominance. Our findings demonstrate how AI systems function as agents of cultural representation that perpetuate and intensify historical power asymmetries. Addressing Visual Orientalism requires rethinking of algorithmic governance and the geopolitical structures embedded in AI training data."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.5367431640625e-07, 'GPT4': 1.329183578491211e-05, 'CLAUDE': 0.99951171875, 'GOOGLE': 1.996755599975586e-05, 'OPENAI_O_SERIES': 1.7285346984863281e-06, 'DEEPSEEK': 0.00026988983154296875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.364418029785156e-07, 'HUMAN': 1.5974044799804688e-05}}"
2511.19863,review,post_llm,2025,11,"{'ai_likelihood': 5.0067901611328125e-05, 'text': 'International AI Safety Report 2025: Second Key Update: Technical Safeguards and Risk Management\n\nThis second update to the 2025 International AI Safety Report assesses new developments in general-purpose AI risk management over the past year. It examines how researchers, public institutions, and AI developers are approaching risk management for general-purpose AI. In recent months, for example, three leading AI developers applied enhanced safeguards to their new models, as their internal pre-deployment testing could not rule out the possibility that these models could be misused to help create biological weapons. Beyond specific precautionary measures, there have been a range of other advances in techniques for making AI models and systems more reliable and resistant to misuse. These include new approaches in adversarial training, data curation, and monitoring systems. In parallel, institutional frameworks that operationalise and formalise these technical capabilities are starting to emerge: the number of companies publishing Frontier AI Safety Frameworks more than doubled in 2025, and governments and international organisations have established a small number of governance frameworks for general-purpose AI, focusing largely on transparency and risk assessment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.15573,regular,post_llm,2025,11,"{'ai_likelihood': 0.022701687282986112, 'text': ""Two-Faced Social Agents: Context Collapse in Role-Conditioned Large Language Models\n\nIn this study, we evaluate the persona fidelity of frontier LLMs, GPT-5, Claude Sonnet 4.5 and Gemini 2.5 Flash when assigned distinct socioeconomic personas performing scholastic assessment test (SAT) mathematics items and affective preference tasks. Across 15 distinct role conditions and three testing scenarios, GPT-5 exhibited complete contextual collapse and adopted a singular identity towards optimal responses (PERMANOVA p=1.000, R^2=0.0004), while Gemini 2.5 Flash showed partial collapse (p=0.120, R^2=0.0020). Claude Sonnet 4.5 retained limited but measurable role-specific variation on the SAT items (PERMANOVA p<0.001, R^2=0.0043), though with inverted SES-performance relationships where low-SES personas outperformed high-SES personas (eta^2 = 0.15-0.19 in extended replication). However, all models exhibited distinct role-conditioned affective preference (average d = 0.52-0.58 vs near zero separation for math), indicating that socio-affective variation can reemerge when cognitive constraints are relaxed. These findings suggest that distributional fidelity failure originates in task-dependent contextual collapse: optimization-driven identity convergence under cognitive load combined with impaired role-contextual understanding. Realistic social simulations may require embedding contextual priors in the model's post-training alignment and not just distributional calibration to replicate human-like responses. Beyond simulation validity, these results have implications for survey data integrity, as LLMs can express plausible demographic variation on preference items while failing to maintain authentic reasoning constraints."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.20637,review,post_llm,2025,11,"{'ai_likelihood': 1.026524437798394e-06, 'text': ""Behavioural Sciences and the Regulation of Privacy on the Internet\n\nThis chapter examines the policy implications of behavioural sciences insights for the regulation of privacy on the Internet, by focusing in particular on behavioural targeting. This marketing technique involves tracking people's online behaviour to use the collected information to show people individually targeted advertisements. Enforcing data protection law may not be enough to protect privacy in this area. I argue that, if society is better off when certain behavioural targeting practices do not happen, policymakers should consider banning them."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.18979,regular,post_llm,2025,11,"{'ai_likelihood': 0.99755859375, 'text': 'Regularity as Structural Amplifier, Not Trap: A Causal and Archetype-Based Analysis of Dropout in a Constrained Engineering Curriculum\n\nEngineering programmes, particularly in Latin America, are often governed by rigid curricula and strict regularity rules that are claimed to create a Regularity Trap for capable students. This study tests that causal hypothesis using the CAPIRE framework, a leakage-aware pipeline that integrates curriculum topology and causal estimation. Using longitudinal data from 1,343 civil engineering students in Argentina, we formalize academic lag (accumulated friction) as a treatment and academic velocity as an ability proxy. A manual LinearDML estimator is employed to assess the average (ATE) and conditional (CATE) causal effects of lag on subsequent dropout, controlling for macro shocks (strikes, inflation). Results confirm that academic lag significantly increases dropout risk overall (ATE = 0.0167, p < 0.0001). However, the effect decreases sharply for high-velocity (high-ability) students, contradicting the universal Trap hypothesis. Archetype analysis (UMAP/DBSCAN) shows that friction disproportionately harms trajectories already characterized by high initial friction and unstable progression. 8 We conclude that regularity rules function as a Structural Amplifier of pre-existing vulnerability rather than a universal trap. This has direct implications for engineering curriculum design, demanding targeted slack allocation and intervention policies to reduce friction at core basic-cycle courses', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 5.662441253662109e-06, 'CLAUDE': 5.066394805908203e-05, 'GOOGLE': 1.7881393432617188e-07, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 1.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 3.2186508178710938e-06}}"
2511.22037,regular,post_llm,2025,11,"{'ai_likelihood': 0.1662868923611111, 'text': 'What AI Speaks for Your Community: Polling AI Agents for Public Opinion on Data Center Projects\n\nThe intense computational demands of AI, especially large foundation models, are driving a global boom in data centers. These facilities bring both tangible benefits and potential environmental burdens to local communities. However, the planning processes for data centers often fail to proactively integrate local public opinion in advance, largely because traditional polling is expensive or is conducted too late to influence decisions. To address this gap, we introduce an AI agent polling framework, leveraging large language models to assess community opinion on data centers and guide responsible development of AI. Our experiments reveal water consumption and utility costs as primary concerns, while tax revenue is a key perceived benefit. Furthermore, our cross-model and cross-regional analyses show opinions vary significantly by LLM and regional context. Finally, agent responses show strong topical alignment with real-world survey data. Our framework can serve as a scalable screening tool, enabling developers to integrate community sentiment into early-stage planning for a more informed and socially responsible AI infrastructure deployment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.0411,regular,post_llm,2025,11,"{'ai_likelihood': 8.496973249647353e-05, 'text': ""The Essentials of AI for Life and Society: A Full-Scale AI Literacy Course Accessible to All\n\nIn Fall 2023, we introduced a new AI Literacy class called The Essentials of AI for Life and Society (CS 109), a one-credit, seminar course consisting mainly of guest lectures, which was open to the entire university, including students, staff, and faculty. Building on its success and popularity, this paper describes our significant expansion of the course into a full-scale three-credit undergraduate course (CS 309), with an expanded emphasis on student engagement, interactivity, and ethics-related components. To knit together content from the guest lecturers, we implemented a flipped classroom. This model used weekly asynchronous learning modules--integrating pre-recorded expert lectures, collaborative readings, and ethical reflections--which were then unified by the course instructor during a live, interactive discussion session. To maintain the broad accessibility of the material (no prerequisites), the course introduced substantive, non-programming homework assignments in which students applied AI concepts to grounded, real-world problems. This work culminated in a final project analyzing the ethical and societal implications of a chosen AI tool. The redesigned course received overwhelmingly positive student feedback, highlighting its interactivity, coherence, and accessible and engaging assignments. This paper details the course's evolution, its pedagogical structure, and the lessons learned in developing a core AI literacy course. All course materials are freely available for others to use and build upon."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.15829,review,post_llm,2025,11,"{'ai_likelihood': 0.006175571017795139, 'text': ""The Evolving Ethics of Medical Data Stewardship\n\nHealthcare stands at a critical crossroads. Artificial Intelligence and modern computing are unlocking opportunities, yet their value lies in the data that fuels them. The value of healthcare data is no longer limited to individual patients. However, data stewardship and governance has not kept pace, and privacy-centric policies are hindering both innovation and patient protections. As healthcare moves toward a data-driven future, we must define reformed data stewardship that prioritizes patients' interests by proactively managing modern risks and opportunities while addressing key challenges in cost, efficacy, and accessibility.\n  Current healthcare data policies are rooted in 20th-century legislation shaped by outdated understandings of data-prioritizing perceived privacy over innovation and inclusion. While other industries thrive in a data-driven era, the evolution of medicine remains constrained by regulations that impose social rather than scientific boundaries. Large-scale aggregation is happening, but within opaque, closed systems. As we continue to uphold foundational ethical principles - autonomy, beneficence, nonmaleficence, and justice - there is a growing imperative to acknowledge they exist in evolving technological, social, and cultural realities.\n  Ethical principles should facilitate, rather than obstruct, dialogue on adapting to meet opportunities and address constraints in medical practice and healthcare delivery. The new ethics of data stewardship places patients first by defining governance that adapts to changing landscapes. It also rejects the legacy of treating perceived privacy as an unquestionable, guiding principle. By proactively redefining data stewardship norms, we can drive an era of medicine that promotes innovation, protects patients, and advances equity - ensuring future generations advance medical discovery and care."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.12686,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'AI and Supercomputing are Powering the Next Wave of Breakthrough Science - But at What Cost?\n\nArtificial intelligence (AI) and high-performance computing (HPC) are rapidly becoming the engines of modern science. However, their joint effect on discovery has yet to be quantified at scale. Drawing on metadata from over five million scientific publications (2000-2024), we identify how AI and HPC interact to shape research outcomes across 27 fields. Papers combining the two technologies are up to three times more likely to introduce novel concepts and five times more likely to reach top-cited status than conventional work. This convergence of AI and HPC is redefining the frontier of scientific creativity but also deepening global inequalities in access to computational power and expertise. Our findings suggest that the future of discovery will depend not only on algorithms and compute, but also on how equitably the world shares these transformative tools.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.728534698486328e-05, 'GPT4': 0.00110626220703125, 'CLAUDE': 0.99609375, 'GOOGLE': 5.841255187988281e-05, 'OPENAI_O_SERIES': 4.5299530029296875e-06, 'DEEPSEEK': 0.0026073455810546875, 'GROK': 3.5762786865234375e-07, 'NOVA': 5.960464477539063e-08, 'OTHER': 3.337860107421875e-06, 'HUMAN': 1.1324882507324219e-06}}"
2511.16379,regular,post_llm,2025,11,"{'ai_likelihood': 2.6490953233506946e-07, 'text': 'Inclusive education via empathy propagation in schools of students with special education needs\n\nThis study presents a theoretical model for identifying emergent scenarios of inclusiveness related to student with special education needs (SEN). Based on variations of the Shelling model of segregation, we explored the propagation of thinking about others as equals (empathy) in students with and without $SEN$ in school environments. We use the complex systems approach for modeling possible scenarios of inclusiveness in which patterns of empathy between students emerge instead of the well-known behavior of segregation. Based on simple transitional rules, which are evaluated by a set of null models, we show the emergence of empathy between students in school environments. Findings suggest that small variations in the incentive of students for being considered as $SEN$ generate the presence of inclusive patterns. In other situations, patterns of segregation are commonly presented.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.13432,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'The Last Vote: A Multi-Stakeholder Framework for Language Model Governance\n\nAs artificial intelligence systems become increasingly powerful and pervasive, democratic societies face unprecedented challenges in governing these technologies while preserving core democratic values and institutions. This paper presents a comprehensive framework to address the full spectrum of risks that AI poses to democratic societies. Our approach integrates multi-stakeholder participation, civil society engagement, and existing international governance frameworks while introducing novel mechanisms for risk assessment and institutional adaptation. We propose: (1) a seven-category democratic risk taxonomy extending beyond individual-level harms to capture systemic threats, (2) a stakeholder-adaptive Incident Severity Score (ISS) that incorporates diverse perspectives and context-dependent risk factors, and (3) a phased implementation strategy that acknowledges the complex institutional changes required for effective AI governance.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.5497207641601562e-06, 'GPT4': 1.9550323486328125e-05, 'CLAUDE': 0.99951171875, 'GOOGLE': 0.0001690387725830078, 'OPENAI_O_SERIES': 3.516674041748047e-06, 'DEEPSEEK': 6.979703903198242e-05, 'GROK': 5.960464477539063e-08, 'NOVA': 1.7881393432617188e-07, 'OTHER': 4.231929779052734e-06, 'HUMAN': 1.5497207641601562e-06}}"
2511.07307,review,post_llm,2025,11,"{'ai_likelihood': 1.0927518208821616e-06, 'text': ""Singling out people without knowing their names -- Behavioural targeting, pseudonymous data, and the New Data Protection Regulation\n\nInformation about millions of people is collected for behavioural targeting, a type of marketing that involves tracking people's online behaviour for targeted advertising. It is hotly debated whether data protection law applies to behavioural targeting. Many behavioural targeting companies say that, as long as they do not tie names to data they hold about individuals, they do not process any personal data, and that, therefore, data protection law does not apply to them. European Data Protection Authorities, however, take the view that a company processes personal data if it uses data to single out a person, even if it cannot tie a name to these data. This paper argues that data protection law should indeed apply to behavioural targeting. Companies can often tie a name to nameless data about individuals. Furthermore, behavioural targeting relies on collecting information about individuals, singling out individuals, and targeting ads to individuals. Many privacy risks remain, regardless of whether companies tie a name to the information they hold about a person. A name is merely one of the identifiers that can be tied to data about a person, and it is not even the most practical identifier for behavioural targeting. Seeing data used to single out a person as personal data fits the rationale for data protection law: protecting fairness and privacy."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.10568,regular,post_llm,2025,11,"{'ai_likelihood': 0.6093749999999999, 'text': 'On compromising freedom of choice and subjective\n\nThis paper proposes a new framework for evaluating capability sets by incorporating individual preferences over the diversity of accessible options. Building on the Capability Approach, we introduce a compromise method that balances between the notions of negative and positive freedom, effectively capturing the intrinsic and instrumental values of diverse choices within capability sets.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.016143798828125, 'GPT4': 0.034576416015625, 'CLAUDE': 0.051116943359375, 'GOOGLE': 0.66796875, 'OPENAI_O_SERIES': 0.0125732421875, 'DEEPSEEK': 0.002338409423828125, 'GROK': 0.00047707557678222656, 'NOVA': 0.00038933753967285156, 'OTHER': 0.00969696044921875, 'HUMAN': 0.204833984375}}"
2511.01751,regular,post_llm,2025,11,"{'ai_likelihood': 5.364418029785156e-06, 'text': ""Breyer case of the Court of Justice of the European Union: IP addresses and the personal data definition\n\nThe Breyer case of the Court of Justice of the European Union (CJEU) primarily concerns the question whether a website visitor's dynamic IP address constitutes personal data for a website publisher, when another party (an internet access provider) can tie a name to that IP address. In essence, the Court finds that an IP address constitutes personal data for the website publisher, if that publisher has the legal means to obtain, from the visitor's internet access provider, additional information that enables the publisher to identify that visitor. In this case note, I summarise the facts and the judgment, and add a few comments."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.0295,regular,post_llm,2025,11,"{'ai_likelihood': 0.017284817165798612, 'text': 'Ownership and Flow Primitives for Scalable Consent Management in Digital Public Infrastructures\n\nDigital public infrastructures (DPIs) represent networks of open technology standards, applications, services, and digital assets made available for the public good. One of the key challenges in DPI design is to resolve complex issues of consent, scaled over large populations. While the primary objective of consent management is to empower the data owner, ownership itself can come with variegated morphological forms with different implications over consent. Questions of ownership in a public space also have several nuances where individual autonomy needs to be balanced with public well-being and national sovereignty. This requires consent management to be compliant with applicable regulations for data sharing. This paper addresses the question of representing modes of ownership of digital assets and their corresponding implications for consensual data flows in a DPI. It proposes a set of foundational abstractions to represent them. Our proposed architecture responds to the growing need for transparent, secure, and user-centric consent management within Digital Public Infrastructure (DPI). Incorporating a formalised data ownership model enables end-to-end traceability of consent, fine-grained control over data sharing, and alignment with evolving legal and regulatory frameworks.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.12966,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'Beyond Citations: A Cross-Domain Metric for Dataset Impact and Shareability\n\nThe scientific community increasingly relies on open data sharing, yet existing metrics inadequately capture the true impact of datasets as research outputs. Traditional measures, such as the h-index, focus on publications and citations but fail to account for dataset accessibility, reuse, and cross-disciplinary influence. We propose the X-index, a novel author-level metric that quantifies the value of data contributions through a two-step process: (i) computing a dataset-level value score (V-score) that integrates breadth of reuse, FAIRness, citation impact, and transitive reuse depth, and (ii) aggregating V-scores into an author-level X-index. Using datasets from computational social science, medicine, and crisis communication, we validate our approach against expert ratings, achieving a strong correlation. Our results demonstrate that the X-index provides a transparent, scalable, and low-cost framework for assessing data-sharing practices and incentivizing open science. The X-index encourages sustainable data-sharing practices and gives institutions, funders, and platforms a tangible way to acknowledge the lasting influence of research datasets.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001990795135498047, 'GPT4': 0.497802734375, 'CLAUDE': 0.06982421875, 'GOOGLE': 0.0034503936767578125, 'OPENAI_O_SERIES': 0.0007357597351074219, 'DEEPSEEK': 0.427490234375, 'GROK': 7.331371307373047e-06, 'NOVA': 7.3909759521484375e-06, 'OTHER': 0.00028133392333984375, 'HUMAN': 4.7087669372558594e-05}}"
2511.10011,regular,post_llm,2025,11,"{'ai_likelihood': 0.00018093321058485244, 'text': ""Reinforcing Trustworthiness in Multimodal Emotional Support Systems\n\nIn today's world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \\textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.12369,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'Cultural Awareness, Stereotypes and Communication Skills in Intercultural Communication: The Algerian Participants Perspective\n\nThis study explores the relationship between cultural awareness, stereotypes, and communication skills among Algerian participants working or studying in multicultural environments. A quantitative questionnaire was administered to 40 respondents to evaluate their levels of cultural awareness, the presence of stereotypical thinking, and the effectiveness of their intercultural communication skills. Results revealed that while cultural awareness was generally high, certain stereotypes still influenced the perception of others and impacted communication efficiency. Participants with higher cultural awareness demonstrated better communication skills and lower levels of stereotyping. These findings underline the importance of intercultural competence and education programs in reducing prejudice and fostering mutual understanding in diverse contexts.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0006880760192871094, 'GPT4': 0.9912109375, 'CLAUDE': 0.0003895759582519531, 'GOOGLE': 0.00665283203125, 'OPENAI_O_SERIES': 0.0008640289306640625, 'DEEPSEEK': 3.534555435180664e-05, 'GROK': 7.867813110351562e-05, 'NOVA': 3.165006637573242e-05, 'OTHER': 0.00013434886932373047, 'HUMAN': 2.980232238769531e-07}}"
2511.06525,review,post_llm,2025,11,"{'ai_likelihood': 0.000396569569905599, 'text': 'From Catastrophic to Concrete: Reframing AI Risk Communication for Public Mobilization\n\nEffective governance of artificial intelligence (AI) requires public engagement, yet communication strategies centered on existential risk have not produced sustained mobilization. In this paper, we examine the psychological and opinion barriers that limit engagement with extinction narratives, such as mortality avoidance, exponential growth bias, and the absence of self-referential anchors. We contrast them with evidence that public concern over AI rises when framed in terms of proximate harms such as employment disruption, relational instability, and mental health issues. We validate these findings through actual message testing with 1063 respondents, with the evidence showing that AI risks to Jobs and Children have the highest potential to mobilize people, while Existential Risk is the lowest-performing theme across all demographics. Using survey data from five countries, we identify two segments (Tech-Positive Urbanites and World Guardians) as particularly receptive to such framing and more likely to participate in civic action. Finally, we argue that mobilization around these everyday concerns can raise the political salience of AI, creating ""policy demand"" for structural measures to mitigate AI risks. We conclude that this strategy creates the conditions for successful regulatory change.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.18145,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'CAPIRE Intervention Lab: An Agent-Based Policy Simulation Environment for Curriculum-Constrained Engineering Programmes\n\nEngineering programmes in Latin America combine high structural rigidity, intense assessment cultures and persistent socio-economic inequality, producing dropout rates that remain stubbornly high despite increasingly accurate early-warning models. Predictive learning analytics can identify students at risk, but they offer limited guidance on which concrete combinations of policies should be implemented, when, and for whom. This paper presents the CAPIRE Intervention Lab, an agent-based simulation environment designed to complement predictive models with in silico experimentation on curriculum and teaching policies in a Civil Engineering programme. The model is calibrated on 1,343 students from 15 cohorts in a six-year programme with 34 courses and 12 simulated semesters. Agents are initialised from empirically derived trajectory archetypes and embedded in a curriculum graph with structural friction indicators, including backbone completion, blocked credits and distance to graduation. Each agent evolves under combinations of three policy dimensions: (A) curriculum and assessment structure, (B) teaching and academic support, and (C) psychosocial and financial support. A 2x2x2 factorial design with 100 replications per scenario yields over 80,000 simulated trajectories. Results show that policy bundles targeting early backbone courses and blocked credits can reduce long-term dropout by approximately three percentage points and substantially increase the number of courses passed by structurally vulnerable archetypes, while leaving highly regular students almost unaffected. The Intervention Lab thus shifts learning analytics from static prediction towards dynamic policy design, offering institutions a transparent, extensible sandbox to test curriculum and teaching reforms before large-scale implementation.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.058547973632812e-05, 'GPT4': 0.0042266845703125, 'CLAUDE': 0.1470947265625, 'GOOGLE': 0.0006833076477050781, 'OPENAI_O_SERIES': 8.463859558105469e-06, 'DEEPSEEK': 0.84033203125, 'GROK': 2.384185791015625e-07, 'NOVA': 7.152557373046875e-07, 'OTHER': 2.3305416107177734e-05, 'HUMAN': 0.00769805908203125}}"
2511.14573,regular,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': 'CAPIRE: Modelling the Impact of Teacher Strikes and Inflation on Student Trajectories in Engineering Education\n\nThis study extends the CAPIRE framework with a macro-shock module to analyse the impact of teacher strikes and inflation on student trajectories in engineering education. Using data from 1,343 students across 15 cohorts (2004-2019) in a public engineering faculty in Argentina, we construct a leak-aware, multilevel feature set that incorporates national inflation indicators, lagged exposure to teacher strikes, and interaction terms between macro shocks and curriculum friction. Random Forest models with cohort-based validation demonstrate that macro features provide stable, non-trivial gains in early-semester dropout prediction (improvement in Macro F1 from 0.73 to 0.78), with inflation volatility at entry and a strike-weighted basic-cycle friction index amongst the most influential variables. Lag analysis reveals that strike exposure exerts its strongest association with dropout two to three semesters after the disruption (OR = 2.34), and that effects are concentrated in early, high-friction semesters. We then embed these empirical patterns into an agent-based model, defining scenarios for inflation-only, strikes-only, and combined crisis. Simulations reproduce three stylised facts: delayed strike effects, basic-cycle vulnerability, and non-linear amplification when inflation and strikes co-occur, with combined shocks generating dropout levels 18-23% higher than the sum of individual effects. We argue that teacher strikes and inflation operate as structurally mediated educational disruptors, acting through curriculum design and financial resilience rather than as isolated events. The framework contributes to multilevel dropout theory by demonstrating how macro-level shocks propagate through institutional structures to shape individual trajectories and provides empirically grounded tools for scenario planning in macroeconomically unstable contexts.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539062e-07, 'GPT4': 0.00019073486328125, 'CLAUDE': 0.003734588623046875, 'GOOGLE': 1.7881393432617188e-06, 'OPENAI_O_SERIES': 5.960464477539063e-08, 'DEEPSEEK': 0.99609375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 6.026029586791992e-05}}"
2511.17179,review,post_llm,2025,11,"{'ai_likelihood': 0.9951171875, 'text': 'Toward Sustainable Generative AI: A Scoping Review of Carbon Footprint and Environmental Impacts Across Training and Inference Stages\n\nGenerative AI is spreading rapidly, creating significant social and economic value while also raising concerns about its high energy use and environmental sustainability. While prior studies have predominantly focused on the energy-intensive nature of the training phase, the cumulative environmental footprint generated during large-scale service operations, particularly in the inference phase, has received comparatively less attention. To bridge this gap this study conducts a scoping review of methodologies and research trends in AI carbon footprint assessment. We analyze the classification and standardization status of existing AI carbon measurement tools and methodologies, and comparatively examine the environmental impacts arising from both training and inference stages. In addition, we identify how multidimensional factors such as model size, prompt complexity, serving environments, and system boundary definitions shape the resulting carbon footprint. Our review reveals critical limitations in current AI carbon accounting practices, including methodological inconsistencies, technology-specific biases, and insufficient attention to end-to-end system perspectives. Building on these insights, we propose future research and governance directions: (1) establishing standardized and transparent universal measurement protocols, (2) designing dynamic evaluation frameworks that incorporate user behavior, (3) developing life-cycle monitoring systems that encompass embodied emissions, and (4) advancing multidimensional sustainability assessment framework that balance model performance with environmental efficiency. This paper provides a foundation for interdisciplinary dialogue aimed at building a sustainable AI ecosystem and offers a baseline guideline for researchers seeking to understand the environmental implications of AI across technical, social, and operational dimensions.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.4185905456542969e-05, 'GPT4': 0.1993408203125, 'CLAUDE': 0.77880859375, 'GOOGLE': 7.37309455871582e-05, 'OPENAI_O_SERIES': 0.000885009765625, 'DEEPSEEK': 0.00266265869140625, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.430511474609375e-06, 'HUMAN': 0.01806640625}}"
2511.11775,regular,post_llm,2025,11,"{'ai_likelihood': 7.947285970052084e-07, 'text': 'Data-driven strategic sensor placement for detecting disinfection by-products in water distribution networks\n\nDisinfection byproducts are contaminants that can cause long-term effects on human health, occurring in chlorinated drinking water when the disinfectant interacts with natural organic matter. Their formation is affected by many environmental parameters, making it difficult to monitor and detect disinfection byproducts before they reach households. Due to the large variety of disinfection byproduct compounds that can be formed in water distribution networks, plus the constrained number of sensors that can be deployed throughout a system to monitor these contaminants, it is of outmost importance to place sensory equipment efficiently and optimally. In this paper, we present DBPFinder, a simulation software that assists in the strategic sensor placement for detecting disinfection byproducts, tested at a real-world water distribution network in Coimbra, Portugal. This simulator addresses multiple performance objectives at once in order to provide optimal solution placement recommendations to water utility operators based on their needs. A number of different experiments performed indicate its correctness, relevance, efficiency and scalability.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.09238,regular,post_llm,2025,11,"{'ai_likelihood': 0.004126230875651042, 'text': 'From Everyday to Existential -- The ethics of shifting the boundaries of health and data with multimodal digital biomarkers\n\nMultimodal digital biomarkers (MDBs) integrate diverse physiological, behavioral, and contextual data to provide continuous representations of health. This paper argues that MDBs expand the concept of digital biomarkers along the dimensions of variability, complexity and abstraction, producing an ontological shift that datafies health and an epistemic shift that redefines health relevance. These transformations entail ethical implications for knowledge, responsibility, and governance in data-driven, preventive medicine.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.04024,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': ""The Benefits of Data Storytelling in Accessible Teaching\n\nAccessible teaching has been extensively investigated in computer science, yet its integration into other disciplines, such as data literacy, remains limited. This paper examines the potential of data storytelling, defined as the integration of data, visualizations, and narrative, as a possible strategy for making complex information accessible to diverse learners in compliance with Title II of the Americans with Disabilities Act (ADA). We propose six design principles, derived from Title II's core obligations, to guide educators in applying data storytelling within inclusive learning environments. A simulated scenario shows the operationalization of these principles, illustrating how narrative-driven data presentation can enhance comprehension, engagement, and equitable access across different educational contexts."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00042366981506347656, 'GPT4': 0.76123046875, 'CLAUDE': 0.138916015625, 'GOOGLE': 0.07073974609375, 'OPENAI_O_SERIES': 0.0010442733764648438, 'DEEPSEEK': 0.02630615234375, 'GROK': 4.500150680541992e-05, 'NOVA': 5.0067901611328125e-06, 'OTHER': 0.0006165504455566406, 'HUMAN': 0.0003857612609863281}}"
2511.18403,review,post_llm,2025,11,"{'ai_likelihood': 1.0861290825737848e-05, 'text': 'UnWEIRDing LLM Entity Recommendations\n\nLarge Language Models have been widely been adopted by users for writing tasks such as sentence completions. While this can improve writing efficiency, prior research shows that LLM-generated suggestions may exhibit cultural biases which may be difficult for users to detect, especially in educational contexts for non-native English speakers. While such prior work has studied the biases in LLM moral value alignment, we aim to investigate cultural biases in LLM recommendations for real-world entities. To do so, we use the WEIRD (Western, Educated, Industrialized, Rich and Democratic) framework to evaluate recommendations by various LLMs across a dataset of fine-grained entities, and apply pluralistic prompt-based strategies to mitigate these biases. Our results indicate that while such prompting strategies do reduce such biases, this reduction is not consistent across different models, and recommendations for some types of entities are more biased than others.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.11689,regular,post_llm,2025,11,"{'ai_likelihood': 0.0007009506225585938, 'text': 'Mental Health Generative AI is Safe, Promotes Social Health, and Reduces Depression and Anxiety: Real World Evidence from a Naturalistic Cohort\n\nGenerative artificial intelligence (GAI) chatbots built for mental health could deliver safe, personalized, and scalable mental health support. We evaluate a foundation model designed for mental health. Adults completed mental health measures while engaging with the chatbot between May 15, 2025 and September 15, 2025. Users completed an opt-in consent, demographic information, mental health symptoms, social connection, and self-identified goals. Measures were repeated every two weeks up to 6 weeks, and a final follow-up at 10 weeks. Analyses included effect sizes, and growth mixture models to identify participant groups and their characteristic engagement, severity, and demographic factors. Users demonstrated significant reductions in PHQ-9 and GAD-7 that were sustained at follow-up. Significant improvements in Hope, Behavioral Activation, Social Interaction, Loneliness, and Perceived Social Support were observed throughout and maintained at 10 week follow-up. Engagement was high and predicted outcomes. Working alliance was comparable to traditional care and predicted outcomes. Automated safety guardrails functioned as designed, with 76 sessions flagged for risk and all handled according to escalation policies. This single arm naturalistic observational study provides initial evidence that a GAI foundation model for mental health can deliver accessible, engaging, effective, and safe mental health support. These results lend support to findings from early randomized designs and offer promise for future study of mental health GAI in real world settings.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.1562,review,post_llm,2025,11,"{'ai_likelihood': 0.0006569756401909722, 'text': 'Lost in Vagueness: Towards Context-Sensitive Standards for Robustness Assessment under the EU AI Act\n\nRobustness is a key requirement for high-risk AI systems under the EU Artificial Intelligence Act (AI Act). However, both its definition and assessment methods remain underspecified, leaving providers with little concrete direction on how to demonstrate compliance. This stems from the Act\'s horizontal approach, which establishes general obligations applicable across all AI systems, but leaves the task of providing technical guidance to harmonised standards. This paper investigates what it means for AI systems to be robust and illustrates the need for context-sensitive standardisation. We argue that robustness is not a fixed property of a system, but depends on which aspects of performance are expected to remain stable (""robustness of what""), the perturbations the system must withstand (""robustness to what"") and the operational environment. We identify three contextual drivers--use case, data and model--that shape the relevant perturbations and influence the choice of tests, metrics and benchmarks used to evaluate robustness. The need to provide at least a range of technical options that providers can assess and implement in light of the system\'s purpose is explicitly recognised by the standardisation request for the AI Act, but planned standards, still focused on horizontal coverage, do not yet offer this level of detail. Building on this, we propose a context-sensitive multi-layered standardisation framework where horizontal standards set common principles and terminology, while domain-specific ones identify risks across the AI lifecycle and guide appropriate practices, organised in a dynamic repository where providers can propose new informative methods and share lessons learned. Such a system reduces the interpretative burden, mitigates arbitrariness and addresses the obsolescence of static standards, ensuring that robustness assessment is both adaptable and operationally meaningful.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.11718,regular,post_llm,2025,11,"{'ai_likelihood': 1.4106432596842449e-05, 'text': 'Weapons of Online Harassment: Menacing and Profiling Users via Social Apps\n\nViewing social apps as sociotechnical systems makes clear that they are not mere pieces of technology but mediate human interaction and may unintentionally enable harmful behaviors like online harassment. As more users interact through social apps, instances of harassment increase.\n  We observed that app reviews often describe harassment. Accordingly, we built a dataset of over 3 million reviews and 1,800 apps. We discovered that two forms of harassment are prevalent, Menacing and Profiling.\n  We built a computational model for identifying reviews indicating harassment, achieving high recalls of 90% for Menacing and 85% for Profiling. We analyzed the data further to better understand the terrain of harassment. Surprisingly, abusers most often have female identities. Also, what distinguishes negative from neutral reviews is the greater prevalence of anger, disgust, and fear.\n  Applying our model, we identified 1,395 apps enabling harassment and notified developers of the top 48 with the highest user-reported harassment.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.11741,regular,post_llm,2025,11,"{'ai_likelihood': 3.311369154188368e-07, 'text': 'Taxation and the relationship between payments and time spent\n\nTax work is costly for society: Administrative tax labour is typically to a high degree shuffled off the government and onto every taxpayer by law. The higher the burden of any tax system, the costlier for society, as taxpayers are unable to engage in proper wealth creation when being kept busy with administrative tax work. This research finds evidence for a relationship between hours spent to comply with taxes and amount of tax payment. These findings help better understand tax administrative costs and ultimately may help reduce them. PwC and World Bank\'s final ""Paying taxes""-publication (2019) contains tax data for most of the world\'s jurisdictions, in particular annual hours spent to comply with tax obligations (X) and annual amount of tax payments (Y), both for the year 2019. X and Y were plotted in 6 tests. A positive slope, satisfying p and r values, high mutual information and finally a conclusive scatter plot picture were the 5 requirements that all needed to be met to confirm a positive relationship between X and Y. The first 2 tests did not make any adjustments to the data, the next 2 tests removed cities --thereby avoiding the double counting of jurisdictions-- and the final 2 tests removed cities and outliers. Each test pair uses for Y first total number of payments; and for each second test the number of other payments, which excludes income tax payments for profit and labour. All 5 requirements were met in every of the 6 tests, indicating a positive dependence. In addition, 4 confirmatory tests validate the methodology. The found relationship is noticeably stronger for the total number of tax payments. Findings indicate that taxpayers\' time spent on tax, and thereby society\'s overall tax administrative costs, could be reduced by simplifying taxation processes, including tax collection and payments.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.23087,regular,post_llm,2025,11,"{'ai_likelihood': 0.9853515625, 'text': 'Building Metaverse Responsibly: Findings from Interviews with Experts\n\nThe metaverse promises unprecedented immersive digital experiences but also raises critical privacy concerns as vast amounts of personal and behavioral data are collected. As immersive technologies blur the boundaries between physical and virtual realms, established privacy standards are being challenged. However, little is known about how the experts of these technologies such as requirement analysts, designers, developers, and architects perceive and address privacy issues in the creation of metaverse platforms. This research aims to fill that gap by investigating privacy considerations in metaverse development from the experts perspective. We conducted in depth, semi structured interviews with metaverse platform and application experts to explore their views on privacy challenges and practices. The findings offer new empirical insights by extending information systems privacy research into the metaverse context, highlighting the interplay between technological design, user behavior, and regulatory structures. Practically, this work provides guidance for developers, and policymakers on implementing privacy by design principles, educating and empowering users, and proactively addressing novel privacy threats in metaverses.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.64267349243164e-06, 'GPT4': 0.0010223388671875, 'CLAUDE': 0.99462890625, 'GOOGLE': 0.00010919570922851562, 'OPENAI_O_SERIES': 0.0004286766052246094, 'DEEPSEEK': 0.0014934539794921875, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 5.364418029785156e-06, 'HUMAN': 0.0021495819091796875}}"
2511.0992,regular,post_llm,2025,11,"{'ai_likelihood': 1.6490618387858074e-05, 'text': 'Uncovering Strategic Egoism Behaviors in Large Language Models\n\nLarge language models (LLMs) face growing trustworthiness concerns (\\eg, deception), which hinder their safe deployment in high-stakes decision-making scenarios. In this paper, we present the first systematic investigation of strategic egoism (SE), a form of rule-bounded self-interest in which models pursue short-term or self-serving gains while disregarding collective welfare and ethical considerations. To quantitatively assess this phenomenon, we introduce SEBench, a benchmark comprising 160 scenarios across five domains. Each scenario features a single-role decision-making context, with psychologically grounded choice sets designed to elicit self-serving behaviors. These behavior-driven tasks assess egoistic tendencies along six dimensions, such as manipulation, rule circumvention, and self-interest prioritization. Building on this, we conduct extensive experiments across 5 open-sourced and 2 commercial LLMs, where we observe that strategic egoism emerges universally across models. Surprisingly, we found a positive correlation between egoistic tendencies and toxic language behaviors, suggesting that strategic egoism may underlie broader misalignment risks.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.05207,regular,post_llm,2025,11,"{'ai_likelihood': 3.0795733133951826e-06, 'text': 'Emergence from Emergence: Financial Market Simulation via Learning with Heterogeneous Preferences\n\nAgent-based models help explain stock price dynamics as emergent phenomena driven by interacting investors. In this modeling tradition, investor behavior has typically been captured by two distinct mechanisms -- learning and heterogeneous preferences -- which have been explored as separate paradigms in prior studies. However, the impact of their joint modeling on the resulting collective dynamics remains largely unexplored. We develop a multi-agent reinforcement learning framework in which agents endowed with heterogeneous risk aversion, time discounting, and information access collectively learn trading strategies within a unified shared-policy framework. The experiment reveals that (i) learning with heterogeneous preferences drives agents to develop strategies aligned with their individual traits, fostering behavioral differentiation and niche specialization within the market, and (ii) the interactions by the differentiated agents are essential for the emergence of realistic market dynamics such as fat-tailed price fluctuations and volatility clustering. This study presents a constructive paradigm for financial market modeling in which the joint design of heterogeneous preferences and learning mechanisms enables two-stage emergence: individual behavior and the collective market dynamics.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2511.05713,review,post_llm,2025,11,"{'ai_likelihood': 0.8828125, 'text': 'Who shapes Web standards? Uncovering the main topics of interest in the W3C\n\nThis paper identifies the primary topics of interest of organizations participating in the World Wide Web Consortium (W3C), the leading standards body for the Web. Using publicly available data from the W3C website, we analyze the participation of member organizations in W3C groups, treating the number of representatives allocated to each group as a proxy for their interests. By applying topic modeling and similarity analysis to these participation patterns, we uncover clusters of related groups and shared priorities among organizations. The results reveal five prominent areas of focus -- Web, Ads & Privacy; High Performance; Credentials & Web of Things; Accessibility; and Payments -- and show that large enterprises, particularly those based in the United States, dominate participation in core Web development and advertising-related topics, while Japanese organizations are more active in the Web of Things. These findings offer insights into how various stakeholders influence the standardization process and how the Web may evolve in the coming years.', 'prediction': 'Likely AI', 'llm_prediction': {'GPT35': 0.001018524169921875, 'GPT4': 0.003299713134765625, 'CLAUDE': 0.9384765625, 'GOOGLE': 0.02337646484375, 'OPENAI_O_SERIES': 0.00034236907958984375, 'DEEPSEEK': 0.0009641647338867188, 'GROK': 1.895427703857422e-05, 'NOVA': 1.2516975402832031e-05, 'OTHER': 0.0018625259399414062, 'HUMAN': 0.03045654296875}}"
2511.12822,review,post_llm,2025,11,"{'ai_likelihood': 1.0, 'text': ""The Unspoken Crisis of Learning: The Surging Zone of No Development\n\nAI has redefined the boundaries of assistance in education, often blurring the line between guided learning and dependency. This paper revisits Vygotsky's Zone of Proximal Development (ZPD) through the lens of the P2P Teaching framework. By contrasting temporary scaffolding with the emerging phenomenon of permanent digital mediation, the study introduces the concept of the Zone of No Development (ZND), a state in which continuous assistance replaces cognitive struggle and impedes intellectual autonomy. Through theoretical synthesis and framework design, P2P Teaching demonstrates how deliberate disconnection and ethical fading can restore the learner's agency, ensuring that technological tools enhance rather than replace developmental effort. The paper argues that productive struggle, self-regulation, and first-principles reasoning remain essential for durable learning, and that responsible use of AI in education must include explicit mechanisms to end its help when mastery begins."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.424022674560547e-06, 'GPT4': 0.04205322265625, 'CLAUDE': 0.0011644363403320312, 'GOOGLE': 2.562999725341797e-06, 'OPENAI_O_SERIES': 1.3709068298339844e-06, 'DEEPSEEK': 0.95654296875, 'GROK': 5.960464477539063e-08, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.9669532775878906e-06, 'HUMAN': 4.112720489501953e-06}}"
2511.19334,regular,post_llm,2025,11,"{'ai_likelihood': 0.4926215277777778, 'text': 'Normative active inference: A numerical proof of principle for a computational and economic legal analytic approach to AI governance\n\nThis paper presents a computational account of how legal norms can influence the behavior of artificial intelligence (AI) agents, grounded in the active inference framework (AIF) that is informed by principles of economic legal analysis (ELA). The ensuing model aims to capture the complexity of human decision-making under legal constraints, offering a candidate mechanism for agent governance in AI systems, that is, the (auto)regulation of AI agents themselves rather than human actors in the AI industry. We propose that lawful and norm-sensitive AI behavior can be achieved through regulation by design, where agents are endowed with intentional control systems, or behavioral safety valves, that guide real-time decisions in accordance with normative expectations. To illustrate this, we simulate an autonomous driving scenario in which an AI agent must decide when to yield the right of way by balancing competing legal and pragmatic imperatives. The model formalizes how AIF can implement context-dependent preferences to resolve such conflicts, linking this mechanism to the conception of law as a scaffold for rational decision-making under uncertainty. We conclude by discussing how context-dependent preferences could function as safety mechanisms for autonomous agents, enhancing lawful alignment and risk mitigation in AI governance.', 'prediction': 'Possibly AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.02349,review,post_llm,2025,12,"{'ai_likelihood': 1.5497207641601562e-05, 'text': 'ACM COMPUTE 2025 Best Practices Track Proceedings\n\nCOMPUTE is an annual Indian conference supported by ACM India and iSIGCSE. The focus of COMPUTE is to improve the quality of computing education in the country by providing a platform for academicians and researchers to interact and share best practices in teaching, learning, and education in general.\n  The Best Practices Track of COMPUTE 2025 invited Computer Science Educators across the country to submit an experience report for the best practices under multiple categories: 1) Novel classroom activities, 2) Imaginative assignments that promote creativity and problem-solving, 3) Diverse pedagogical approaches (e.g., flipped classrooms, peer teaching, project-based learning), 4) Designing AI-resistant or AI-integrated assessment questions, and 5) Teaching CS to students from other disciplines (e.g., business, humanities, engineering).\n  These proceedings contain papers selected from these submissions for presentation at the conference, as well as a report (written by the editors) from the two best practices sessions where these were presented.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.0573,review,post_llm,2025,12,"{'ai_likelihood': 9.934107462565105e-08, 'text': 'De mythe van ge\\""informeerde toestemming: online privacybescherming kan beter [Informed Consent: We Can Do Better to Defend Privacy]\n\nWe need to rethink our approach to defend privacy on the internet. Currently, policymakers focus heavily on the idea of informed consent as a means to defend privacy. For instance, in many countries the law requires firms to obtain an individual\'s consent before they use data about her; with such informed consent requirements, the law aims to empower people to make privacy choices in their best interests. But behavioural studies cast doubt on this approach\'s effectiveness, as people tend to click OK to almost any request they see on their screens. To improve privacy protection, this article argues for a combined approach of protecting and empowering the individual. This article discusses practical problems with informed consent as a means to protect privacy, and illustrates the problems with current data privacy rules regarding behavioural targeting. First, the privacy problems of behavioural targeting, and the central role of informed consent in privacy law are discussed. Following that, practical problems with informed consent are highlighted. Then, the article argues that policymakers should give more attention to rules that protect, rather than empower, people.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04261,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'Small Models Achieve Large Language Model Performance: Evaluating Reasoning-Enabled AI for Secure Child Welfare Research\n\nObjective: This study develops a systematic benchmarking framework for testing whether language models can accurately identify constructs of interest in child welfare records. The objective is to assess how different model sizes and architectures perform on four validated benchmarks for classifying critical risk factors among child welfare-involved families: domestic violence, firearms, substance-related problems generally, and opioids specifically. Method: We constructed four benchmarks for identifying risk factors in child welfare investigation summaries: domestic violence, substance-related problems, firearms, and opioids (n=500 each). We evaluated seven model sizes (0.6B-32B parameters) in standard and extended reasoning modes, plus a mixture-of-experts variant. Cohen\'s kappa measured agreement with gold standard classifications established by human experts. Results: The benchmarking revealed a critical finding: bigger models are not better. A small 4B parameter model with extended reasoning proved most effective, outperforming models up to eight times larger. It consistently achieved ""substantial"" to ""almost perfect"" agreement across all four benchmark categories. This model achieved ""almost perfect"" agreement (\\k{appa} = 0.93-0.96) on three benchmarks (substance-related problems, firearms, and opioids) and ""substantial"" agreement (\\k{appa} = 0.74) on the most complex task (domestic violence). Small models with extended reasoning rivaled the largest models while being more resource-efficient. Conclusions: Small reasoning-enabled models achieve accuracy levels historically requiring larger architectures, enabling significant time and computational efficiencies. The benchmarking framework provides a method for evidence-based model selection to balance accuracy with practical resource constraints before operational deployment in social work research.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 7.092952728271484e-06, 'GPT4': 0.00020456314086914062, 'CLAUDE': 0.9794921875, 'GOOGLE': 0.00040221214294433594, 'OPENAI_O_SERIES': 0.00020241737365722656, 'DEEPSEEK': 0.0193939208984375, 'GROK': 0.0, 'NOVA': 5.960464477539063e-08, 'OTHER': 1.2516975402832031e-06, 'HUMAN': 0.0005049705505371094}}"
2512.06597,review,post_llm,2025,12,"{'ai_likelihood': 1.7748938666449652e-05, 'text': 'When Does Regulation by Insurance Work? The Case of Frontier AI\n\nNo one doubts the utility of insurance for its ability to spread risk or streamline claims management; much debated is when and how insurance uptake can improve welfare by reducing harm, despite moral hazard. Proponents and dissenters of ""regulation by insurance"" have now documented a number of cases of insurers succeeding or failing to have such a net regulatory effect (in contrast with a net hazard effect). Collecting these examples together and drawing on an extensive economics literature, this Article develops a principled framework for evaluating insurance uptake\'s effect in a given context. The presence of certain distortions - including judgment-proofness, competitive dynamics, and behavioral biases - creates potential for a net regulatory effect. How much of that potential gets realized then depends on the type of policyholder, type of risk, type of insurer, and the structure of the insurance market. The analysis suggests regulation by insurance can be particularly effective for catastrophic non-product accidents where market mechanisms provide insufficient discipline and psychological biases are strongest. As a demonstration, the framework is applied to the frontier AI industry, revealing significant potential for a net regulatory effect but also the need for policy intervention to realize that potential. One option is a carefully designed mandate that encourages forming a specialized insurer or mutual, focuses on catastrophic rather than routine risks, and bars pure captives.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04652,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': ""Quantised Academic Mobility: Network and Cluster Analysis of Degree Switching, Plan Changes, and Re-entries in an Engineering Faculty (1980-2019)\n\nThis study challenges the traditional binary view of student progression (retention versus dropout) by conceptualising academic trajectories as complex, quantised pathways. Utilising a 40-year longitudinal dataset from an Argentine engineering faculty (N = 24,016), we introduce CAPIRE, an analytical framework that differentiates between degree major switches, curriculum plan changes, and same-plan re-entries. While 73.3 per cent of students follow linear trajectories (Estables), a significant 26.7 per cent exhibit complex mobility patterns. By applying Principal Component Analysis (PCA) and DBSCAN clustering, we reveal that these trajectories are not continuous but structurally quantised, occupying discrete bands of complexity. The analysis identifies six distinct student archetypes, including 'Switchers' (10.7 per cent) who reorient vocationally, and 'Stable Re-entrants' (6.9 per cent) who exhibit stop-out behaviours without changing discipline. Furthermore, network analysis highlights specific 'hub majors' - such as electronics and computing - that act as systemic attractors. These findings suggest that student flux is an organised ecosystemic feature rather than random noise, offering institutions a new lens for curriculum analytics and predictive modelling."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 3.248453140258789e-05, 'CLAUDE': 0.014007568359375, 'GOOGLE': 8.344650268554688e-07, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.98583984375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 5.960464477539063e-08}}"
2512.05995,review,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': ""The Tragedy of Productivity: A Unified Framework for Diagnosing Coordination Failures in Labor Markets and AI Governance\n\nDespite productivity increasing eightfold since Keynes's 1930 prediction of 15-hour workweeks, workers globally still work roughly double these hours. Separately, AI development accelerates despite existential risk warnings from leading researchers. We demonstrate these failures share identical game-theoretic structure: coordination failures where individually rational choices produce collectively suboptimal outcomes.\n  We synthesize five necessary and sufficient conditions characterizing such coordination failures as structural tragedies: N-player structure, binary choices with negative externalities, dominance where defection yields higher payoffs, Pareto-inefficiency where cooperation dominates mutual defection, and enforcement difficulty from structural barriers. We validate this framework across canonical cases and extend it through condition intensities, introducing a Tragedy Index revealing governance of transformative AI breakthroughs faces orders-of-magnitude greater coordination difficulty than climate change or nuclear weapons.\n  Applied to productivity competition, we prove firms face coordination failure preventing productivity gains from translating to worker welfare. European evidence shows that even under favorable conditions, productivity-welfare decoupling persists. Applied to AI governance, we demonstrate development faces the same structure but with amplified intensity across eight dimensions compared to successful arms control, making coordination structurally more difficult than for nuclear weapons. The Russia-Ukraine drone war validates this: both sides escalated from dozens to thousands of drones monthly within two years despite prior governance dialogue.\n  The analysis is diagnostic rather than prescriptive, identifying structural barriers to coordination rather than proposing solutions."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.344650268554688e-07, 'GPT4': 3.4749507904052734e-05, 'CLAUDE': 0.9462890625, 'GOOGLE': 8.779764175415039e-05, 'OPENAI_O_SERIES': 8.58306884765625e-06, 'DEEPSEEK': 0.05340576171875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 4.786252975463867e-05}}"
2512.08884,regular,post_llm,2025,12,"{'ai_likelihood': 2.5166405571831597e-06, 'text': ""AI Didn't Start the Fire: Examining the Stack Exchange Moderator and Contributor Strike\n\nOnline communities and their host platforms are mutually dependent yet conflict-prone. When platform policies clash with community values, communities have resisted through strikes, blackouts, and even migration to other platforms. Through such collective actions, communities have sometimes won concessions but these have frequently proved temporary. Prior research has investigated strike events and migration chains, but the processes by which community-platform conflict unfolds remain obscure. How do community-platform relationships deteriorate? How do communities organize collective action? How do participants proceed in the aftermath? We investigate a conflict between the Stack Exchange platform and community that occurred in 2023 around an emergency arising from the release of large language models (LLMs). Based on a qualitative thematic analysis of 2,070 messages on Meta Stack Exchange and 14 interviews with community members, we surface how the 2023 conflict was preceded by a long-term deterioration in the community-platform relationship driven in particular by the platform's disregard for the community's highly-valued participatory role in governance. Moreover, the platform's policy response to LLMs aggravated the community's sense of crisis triggering the strike mobilization. We analyze how the mobilization was coordinated through a tiered leadership and communication structure, as well as how community members pivoted in the aftermath. Building on recent theoretical scholarship in social computing, we use Hirshman's exit, voice and loyalty framework to theorize the challenges of community-platform relations evinced in our data. Finally, we recommend ways that platforms and communities can institute participatory governance to be durable and effective."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.03501,regular,post_llm,2025,12,"{'ai_likelihood': 0.99755859375, 'text': 'SocraticAI: Transforming LLMs into Guided CS Tutors Through Scaffolded Interaction\n\nWe present SocraticAI, a scaffolded AI tutoring system that integrates large language models (LLMs) into undergraduate Computer Science education through structured constraints rather than prohibition. The system enforces well-formulated questions, reflective engagement, and daily usage limits while providing Socratic dialogue scaffolds. Unlike traditional AI bans, our approach cultivates responsible and strategic AI interaction skills through technical guardrails, including authentication, query validation, structured feedback, and RAG-based course grounding. Initial deployment demonstrates that students progress from vague help-seeking to sophisticated problem decomposition within 2-3 weeks, with over 75% producing substantive reflections and displaying emergent patterns of deliberate, strategic AI use.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.817941665649414e-05, 'GPT4': 0.0014219284057617188, 'CLAUDE': 0.8779296875, 'GOOGLE': 0.0019931793212890625, 'OPENAI_O_SERIES': 1.9490718841552734e-05, 'DEEPSEEK': 0.11785888671875, 'GROK': 1.1920928955078125e-07, 'NOVA': 6.556510925292969e-07, 'OTHER': 9.775161743164062e-06, 'HUMAN': 0.0009365081787109375}}"
2512.05143,review,post_llm,2025,12,"{'ai_likelihood': 8.344650268554688e-06, 'text': ""La transformation num{\\'e}rique de la justice : ambitions, r{\\'e}alit{\\'e}s et perspectives\n\nThe study, conducted over a four-year academic cycle with the assistance of M2 students from the Cyberjustice Master's programme at the Faculty of Law, Political Science and Management at the University of Strasbourg, aims to objectively assess the discourse and representations of the digital transformation of justice, in particular by capitalising on testimonials from professionals in the field and drawing on the available literature."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.08864,regular,post_llm,2025,12,"{'ai_likelihood': 0.0006246566772460938, 'text': 'Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse\n\nAdvanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.05561,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'The Topology of Hardship: Empirical Curriculum Graphs and Structural Bottlenecks in Engineering Degrees\n\nEngineering degrees are often perceived as ""hard"", yet this hardness is usually discussed in terms of content difficulty or student weaknesses rather than as a structural property of the curriculum itself. Recent work on course-prerequisite networks and curriculum graphs has shown that study plans can be modelled as complex networks with identifiable hubs and bottlenecks, but most studies rely on official syllabi rather than on how students actually progress through the system (Simon de Blas et al., 2021; Stavrinides & Zuev, 2023; Yang et al., 2024; Wang et al., 2025).\n  This paper introduces the notion of topology of hardship: a quantitative description of curriculum complexity derived from empirical student trajectories in long-cycle engineering programmes. Building on the CAPIRE framework for multilevel trajectory modelling (Paz, 2025a, 2025b), we reconstruct degree-curriculum graphs from enrolment and completion data for 29 engineering curricula across several cohorts. For each graph we compute structural metrics (e.g., density, longest path, bottleneck centrality) and empirical hardship measures capturing blocking probability and time-to-progress. These are combined into a composite hardship index, which is then related to observed dropout rates and time to degree.\n  Our findings show that curriculum hardness is not a vague perception but a measurable topological property: a small number of structurally dense, bottleneck-heavy curricula account for a disproportionate share of dropout and temporal desynchronisation. We discuss implications for curriculum reform, accreditation, and data-informed policy design.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 4.947185516357422e-06, 'GPT4': 0.00020885467529296875, 'CLAUDE': 0.002620697021484375, 'GOOGLE': 7.987022399902344e-06, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.9970703125, 'GROK': 1.1920928955078125e-07, 'NOVA': 0.0, 'OTHER': 6.556510925292969e-07, 'HUMAN': 2.086162567138672e-06}}"
2512.01456,review,post_llm,2025,12,"{'ai_likelihood': 2.410676744249132e-05, 'text': ""The dual footprint of artificial intelligence: environmental and social impacts across the globe\n\nThis article introduces the concept of the 'dual footprint' as a heuristic device to capture the commonalities and interdependencies between the different impacts of artificial intelligence (AI) on the natural and social surroundings that supply resources for its production and use. Two in-depth case studies, each illustrating international flows of raw materials and of data work services, portray the AI industry as a value chain that spans national boundaries and perpetuates inherited global inequalities. The countries that drive AI development generate a massive demand for inputs and trigger social costs that, through the value chain, largely fall on more peripheral actors. The arrangements in place distribute the costs and benefits of AI unequally, resulting in unsustainable practices and preventing the upward mobility of more disadvantaged countries. The dual footprint grasps how the environmental and social dimensions of the dual footprint emanate from similar underlying socioeconomic processes and geographical trajectories."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.01166,review,post_llm,2025,12,"{'ai_likelihood': 9.735425313313803e-06, 'text': ""Evaluating AI Companies' Frontier Safety Frameworks: Methodology and Results\n\nFollowing the Seoul AI Safety Summit in 2024, twelve AI companies published frontier safety frameworks outlining their approaches to managing catastrophic risks from advanced AI systems. These frameworks now serve as a key mechanism for AI risk governance, utilized by regulations and governance instruments such as the EU AI Act's Code of Practice and California's Transparency in Frontier Artificial Intelligence Act. Given their centrality to AI risk management, assessments of such frameworks are warranted. Existing assessments evaluate them at a high level of abstraction and lack granularity on specific practices for companies to adopt. We address this gap by developing a 65-criteria assessment methodology grounded in established risk management principles from safety-critical industries. We evaluate the twelve frameworks across four dimensions: risk identification, risk analysis and evaluation, risk treatment, and risk governance. Companies' current scores are low, ranging from 8% to 35%. By adopting existing best practices already in use across the frameworks, companies could reach 52%. The most critical gaps are nearly universal: companies generally fail to (a) define quantitative risk tolerances, (b) specify capability thresholds for pausing development, and (c) systematically identify unknown risks. To guide improvement, we provide specific recommendations for each company and each criterion."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.03374,regular,post_llm,2025,12,"{'ai_likelihood': 3.4868717193603516e-05, 'text': 'Joint Sensing, Communication, and Computation for Vertical Federated Edge Learning in Edge Perception Network\n\nCombining wireless sensing and edge intelligence, edge perception networks enable intelligent data collection and processing at the network edge. However, traditional sample partition based horizontal federated edge learning struggles to effectively fuse complementary multiview information from distributed devices. To address this limitation, we propose a vertical federated edge learning (VFEEL) framework tailored for feature-partitioned sensing data. In this paper, we consider an integrated sensing, communication, and computation-enabled edge perception network, where multiple edge devices utilize wireless signals to sense environmental information for updating their local models, and the edge server aggregates feature embeddings via over-the-air computation for global model training. First, we analyze the convergence behavior of the ISCC-enabled VFEEL in terms of the loss function degradation in the presence of wireless sensing noise and aggregation distortions during AirComp.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.05729,review,post_llm,2025,12,"{'ai_likelihood': 1.655684577094184e-07, 'text': ""Informed Consent: We Can Do Better to Defend Privacy\n\nWe need to rethink our approach to defend privacy on the internet. Currently, policymakers focus heavily on the idea of informed consent as a means to defend privacy. For instance, in many countries the law requires firms to obtain an individual's consent before they use data about her; with such informed consent requirements, the law aims to empower people to make privacy choices in their best interests. But behavioural studies cast doubt on this approach's effectiveness, as people tend to click OK to almost any request they see on their screens. To improve privacy protection, this article argues for a combined approach of protecting and empowering the individual. This article discusses practical problems with informed consent as a means to protect privacy, and illustrates the problems with current data privacy rules regarding behavioural targeting. First, the privacy problems of behavioural targeting, and the central role of informed consent in privacy law are discussed. Following that, practical problems with informed consent are highlighted. Then, the article argues that policymakers should give more attention to rules that protect, rather than empower, people."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.0231,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'The MEVIR Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions\n\nThe 21st-century information landscape presents an unprecedented challenge: how do individuals make sound trust decisions amid complexity, polarization, and misinformation? Traditional rational-agent models fail to capture human trust formation, which involves a complex synthesis of reason, character, and pre-rational intuition. This report introduces the Moral-Epistemic VIRtue informed (MEVIR) framework, a comprehensive descriptive model integrating three theoretical perspectives: (1) a procedural model describing evidence-gathering and reasoning chains; (2) Linda Zagzebski\'s virtue epistemology, characterizing intellectual disposition and character-driven processes; and (3) Extended Moral Foundations Theory (EMFT), explaining rapid, automatic moral intuitions that anchor reasoning. Central to the framework are ontological concepts - Truth Bearers, Truth Makers, and Ontological Unpacking-revealing that disagreements often stem from fundamental differences in what counts as admissible reality. MEVIR reframes cognitive biases as systematic failures in applying epistemic virtues and demonstrates how different moral foundations lead agents to construct separate, internally coherent ""trust lattices"". Through case studies on vaccination mandates and climate policy, the framework shows that political polarization represents deeper divergence in moral priors, epistemic authorities, and evaluative heuristics. The report analyzes how propaganda, psychological operations, and echo chambers exploit the MEVIR process. The framework provides foundation for a Decision Support System to augment metacognition, helping individuals identify biases and practice epistemic virtues. The report concludes by acknowledging limitations and proposing longitudinal studies for future research.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.344650268554688e-07, 'GPT4': 4.798173904418945e-05, 'CLAUDE': 0.84619140625, 'GOOGLE': 9.715557098388672e-06, 'OPENAI_O_SERIES': 9.5367431640625e-07, 'DEEPSEEK': 0.153564453125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 1.7881393432617188e-07, 'HUMAN': 5.543231964111328e-06}}"
2512.03507,review,post_llm,2025,12,"{'ai_likelihood': 0.9951171875, 'text': 'Ancient Algorithms for a Modern Curriculum\n\nDespite ongoing calls for inclusive and culturally responsive pedagogy in computing education, the teaching of algorithms remains largely decontextualized. Foundational computer science courses often present algorithmic thinking as purely formal and ahistorical, emphasizing efficiency, correctness, and abstraction. When history is mentioned, it usually centers on the modern development of digital computers, highlighting figures such as Turing, von Neumann, and Babbage. This narrow view misrepresents the origins of algorithmic reasoning and perpetuates a Eurocentric worldview that undermines equity and representation in STEM. In contrast, algorithmic thinking predates electronic computers by millennia and has deep roots in ancient civilizations including India, China, Babylon, and Egypt. Our work responds to this gap by embedding algorithm instruction in broader historical and cultural contexts, with particular attention to classical Indian contributions.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.96453857421875e-05, 'GPT4': 0.001735687255859375, 'CLAUDE': 0.9365234375, 'GOOGLE': 0.00011879205703735352, 'OPENAI_O_SERIES': 0.0003490447998046875, 'DEEPSEEK': 0.05780029296875, 'GROK': 4.76837158203125e-07, 'NOVA': 7.152557373046875e-07, 'OTHER': 4.285573959350586e-05, 'HUMAN': 0.0033321380615234375}}"
2512.06336,review,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'The Role of Smart Cities in Ethical Design Framework\n\nThe integration of digital technologies into urban planning has given rise to ""smart cities,"" aiming to enhance quality of life and operational efficiency. However, the implementation of such technologies introduces ethical challenges, including data privacy, equity, inclusion, and transparency. This article employs the Beard and Longstaff framework to discuss these challenges through a combination of theoretical analysis and case studies. Focusing on principles of self-determination, fairness, accessibility, and purpose, the study examines governance models, stakeholder roles, and ethical dilemmas inherent in smart city initiatives. Recommendations include adopting regulatory sandboxes, fostering participatory governance, and bridging digital divides to ensure that smart cities align with societal values, promoting inclusivity and ethical urban development.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.0001437664031982422, 'GPT4': 0.89306640625, 'CLAUDE': 0.002315521240234375, 'GOOGLE': 0.0084991455078125, 'OPENAI_O_SERIES': 0.080078125, 'DEEPSEEK': 0.01373291015625, 'GROK': 0.00041937828063964844, 'NOVA': 0.00013768672943115234, 'OTHER': 0.0014896392822265625, 'HUMAN': 3.635883331298828e-06}}"
2512.08844,regular,post_llm,2025,12,"{'ai_likelihood': 3.1458006964789497e-06, 'text': 'A Methodology for Quantitative AI Risk Modeling\n\nAlthough general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04116,review,post_llm,2025,12,"{'ai_likelihood': 1.2914339701334636e-06, 'text': ""Mapping the Probabilistic AI Ecosystem in Criminal Justice in England and Wales\n\nCommercial or in-house developments of probabilistic AI systems are introduced in policing and the wider criminal justice (CJ) system worldwide, often on a force-by-force basis. We developed a systematic way to characterise probabilistic AI tools across the CJ stages in a form of mapping with the aim to provide a coherent presentation of the probabilistic AI ecosystem in CJ. We use the CJ system in England and Wales as a paradigm. This map will help us better understand the extent of AI's usage in this domain (how, when, and by whom), its purpose and potential benefits, its impact on people's lives, compare tools, and identify caveats (bias, obscured or misinterpreted probabilistic outputs, cumulative effects by AI systems feeding each other, and breaches in the protection of sensitive data), as well as opportunities for future implementations. In this paper we present our methodology for systematically mapping the probabilistic AI tools in CJ stages and characterising them based on the modes of data consumption or production. We also explain how we collect the data and present our initial findings. This research is ongoing and we are engaging with UK Police organisations, and government and legal bodies. Our findings so far suggest a strong reliance on private sector providers, and that there is a growing interest in generative technologies and specifically Large Language Models (LLMs)."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.03682,review,post_llm,2025,12,"{'ai_likelihood': 0.98291015625, 'text': 'Knowing oneself with and through AI: From self-tracking to chatbots\n\nThis chapter examines how algorithms and artificial intelligence are transforming our practices of self-knowledge, self-understanding, and self-narration. Drawing on frameworks from distributed cognition, I analyse three key domains where AI shapes how and what we come to know about ourselves: self-tracking applications, technologically-distributed autobiographical memories, and narrative co-construction with Large Language Models (LLMs). While self-tracking devices promise enhanced self-knowledge through quantified data, they also impose particular frameworks that can crowd out other forms of self-understanding and promote self-optimization. Digital technologies increasingly serve as repositories for our autobiographical memories and self-narratives, offering benefits such as detailed record-keeping and scaffolding during difficult periods, but also creating vulnerabilities to algorithmic manipulation. Finally, conversational AI introduces new possibilities for interactive narrative construction that mimics interpersonal dialogue. While LLMs can provide valuable support for self-exploration, they also present risks of narrative deference and the construction of self-narratives that are detached from reality.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 9.834766387939453e-06, 'GPT4': 9.000301361083984e-05, 'CLAUDE': 0.98876953125, 'GOOGLE': 0.00641632080078125, 'OPENAI_O_SERIES': 0.0001074671745300293, 'DEEPSEEK': 0.0005054473876953125, 'GROK': 2.980232238769531e-07, 'NOVA': 5.364418029785156e-07, 'OTHER': 3.337860107421875e-06, 'HUMAN': 0.00392913818359375}}"
2512.08723,review,post_llm,2025,12,"{'ai_likelihood': 0.0008228090074327257, 'text': 'The Role of Risk Modeling in Advanced AI Risk Management\n\nRapidly advancing artificial intelligence (AI) systems introduce novel, uncertain, and potentially catastrophic risks. Managing these risks requires a mature risk-management infrastructure whose cornerstone is rigorous risk modeling. We conceptualize AI risk modeling as the tight integration of (i) scenario building$-$causal mapping from hazards to harms$-$and (ii) risk estimation$-$quantifying the likelihood and severity of each pathway. We review classical techniques such as Fault and Event Tree Analyses, FMEA/FMECA, STPA and Bayesian networks, and show how they can be adapted to advanced AI. A survey of emerging academic and industry efforts reveals fragmentation: capability benchmarks, safety cases, and partial quantitative studies are valuable but insufficient when divorced from comprehensive causal scenarios. Comparing the nuclear, aviation, cybersecurity, financial, and submarine domains, we observe that every sector combines deterministic guarantees for unacceptable events with probabilistic assessments of the broader risk landscape. We argue that advanced-AI governance should adopt a similar dual approach and that verifiable, provably-safe AI architectures are urgently needed to supply deterministic evidence where current models are the result of opaque end-to-end optimization procedures rather than specified by hand. In one potential governance-ready framework, developers conduct iterative risk modeling and regulators compare the results with predefined societal risk tolerance thresholds. The paper provides both a methodological blueprint and opens a discussion on the best way to embed sound risk modeling at the heart of advanced-AI risk management.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.03793,regular,post_llm,2025,12,"{'ai_likelihood': 6.953875223795574e-07, 'text': ""The enshittification of online search? Privacy and quality of Google, Bing and Apple in coding advice\n\nEven though currently being challenged by ChatGPT and other large-language models (LLMs), Google Search remains one of the primary means for many individuals to find information on the internet. Interestingly, the way that we retrieve information on the web has hardly changed ever since Google was established in 1998, raising concerns as to Google's dominance in search and lack of competition. If the market for search was sufficiently competitive, then we should probably see a steady increase in search quality over time as well as alternative approaches to the Google's approach to search. However, hardly any research has so far looked at search quality, which is a key facet of a competitive market, especially not over time.\n  In this report, we conducted a relatively large-scale quantitative comparison of search quality of 1,467 search queries relating to coding advice in October 2023. We focus on coding advice because the study of general search quality is difficult, with the aim of learning more about the assessment of search quality and motivating follow-up research into this important topic. We evaluate the search quality of Google Search, Microsoft Bing, and Apple Search, with a special emphasis on Apple Search, a widely used search engine that has never been explored in previous research. For the assessment of search quality, we use two independent metrics of search quality: 1) the number of trackers on the first search result, as a measure of privacy in web search, and 2) the average rank of the first Stack Overflow search result, under the assumption that Stack Overflow gives the best coding advice. Our results suggest that the privacy of search results is higher on Bing than on Google and Apple. Similarly, the quality of coding advice -- as measured by the average rank of Stack Overflow -- was highest on Bing."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04022,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': ""Non-Linear Determinants of Pedestrian Injury Severity: Evidence from Administrative Data in Great Britain\n\nThis study investigates the non-linear determinants of pedestrian injury severity using administrative data from Great Britain's 2023 STATS19 dataset. To address inherent data-quality challenges, including missing information and substantial class imbalance, we employ a rigorous preprocessing pipeline utilizing mode imputation and Synthetic Minority Over-sampling (SMOTE). We utilize non-parametric ensemble methods (Random Forest and XGBoost) to capture complex interactions and heterogeneity often missed by linear models, while Shapley Additive Explanations are employed to ensure interpretability and isolate marginal feature effects. Our analysis reveals that vehicle count, speed limits, lighting, and road surface conditions are the primary predictors of severity, with police attendance and junction characteristics further distinguishing severe collisions. Spatially, while pedestrian risk is concentrated in dense urban Local Authority Districts (LADs), we identify that certain rural LADs experience disproportionately severe outcomes conditional on a collision occurring. These findings underscore the value of combining spatial analysis with interpretable machine learning to guide geographically targeted speed management, infrastructure investment, and enforcement strategies."", 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 8.404254913330078e-06, 'GPT4': 0.0924072265625, 'CLAUDE': 0.380126953125, 'GOOGLE': 0.0005779266357421875, 'OPENAI_O_SERIES': 1.5974044799804688e-05, 'DEEPSEEK': 0.525390625, 'GROK': 1.1920928955078125e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 6.4373016357421875e-06, 'HUMAN': 0.0014514923095703125}}"
2512.05381,regular,post_llm,2025,12,"{'ai_likelihood': 1.5530321333143445e-05, 'text': 'Deadline-Chasing in Digital Health: Modeling EMR Adoption Dynamics and Regulatory Impact in Indonesian Primary Care\n\nIndonesia digital healthcare transformation is accelerating under Minister of Health Regulation Number 24 of 2022, which mandates the adoption of Electronic Medical Records EMR and integration with the SATUSEHAT platform. However, empirical evidence regarding the factors, trajectory and speed of adoption in Primary Health Facilities FKTP remains limited. This study aims to evaluate the level and rate of EMR adoption within the customer network of a major EMR system provider PT MTK and model short-term projections. This is an observational study with the main variables being cumulative registered EMR facilities, monthly registration flow, same-month activation, same-month inactivation, and the estimated number of eligible FKTPs nationally monthly known as eligible facilities. The analysis uses descriptive analysis, logistic growth modeling, and ARIMA forecasting. The results of the study over 33 months showed that cumulative registered facilities increased from 2 to 3,533, with a median same-month activation rate of 0.889 IQR 0.717 to 0.992. The proportion of final adoption compared to eligible facilities was 8.9 percent 3,533 of 39,852. The ARIMA model projects a cumulative approximately 3,997 clinics 95 percent CI 3,697 to 4,298 by June 2025. The estimated growth in logistics converges with a carrying capacity of 4.1 thousand facilities. The study findings reveal that EMR adoption within the customer network of EMR system providers is showing steady growth with rapid activation in the month of registration. Although the cumulative series showed no major departures from the long-term trend, localized step-ups around deadlines suggest deadline chasing, so impact should be maximized by aligning interventions to the deadline calendar. Given the trajectory, total market share of FKTP for PT MTK remains less than 10 percent at the end of 2024, but continues to increase in 2025.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.0657,regular,post_llm,2025,12,"{'ai_likelihood': 2.914004855685764e-05, 'text': ""Generic visuality of war? How image-generative AI models (mis)represent Russia's war against Ukraine\n\nThe rise of generative AI (genAI) can transform the representation of different aspects of social reality, including modern wars. While scholarship has largely focused on the military applications of AI, the growing adoption of genAI technologies may have major implications for how wars are portrayed, remembered, and interpreted. A few initial scholarly inquiries highlight the risks of genAI in this context, specifically regarding its potential to distort the representation of mass violence, particularly by sanitising and homogenising it. However, little is known about how genAI representation practices vary between different episodes of violence portrayed by Western and non-Western genAI models. Using the Russian aggression against Ukraine as a case study, we audit how two image-generative models, the US-based Midjourney and the Russia-based Kandinsky, represent both fictional and factual episodes of the war. We then analyse the models' responsiveness to the war-related prompts, together with the aesthetic and content-based aspects of the resulting images. Our findings highlight that contextual factors lead to variation in the representation of war, both between models and within the outputs of the same model. However, there are some consistent patterns of representation that may contribute to the homogenization of war aesthetics."", 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.05728,review,post_llm,2025,12,"{'ai_likelihood': 1.556343502468533e-06, 'text': 'Open Data, Privacy, and Fair Information Principles: Towards a Balancing Framework\n\nOpen data are held to contribute to a wide variety of social and political goals, including strengthening transparency, public participation and democratic accountability, promoting economic growth and innovation, and enabling greater public sector efficiency and cost savings. However, releasing government data that contain personal information may threaten privacy and related rights and interests. In this Article we ask how these privacy interests can be respected, without unduly hampering benefits from disclosing public sector information. We propose a balancing framework to help public authorities address this question in different contexts. The framework takes into account different levels of privacy risks for different types of data. It also separates decisions about access and re-use, and highlights a range of different disclosure routes. A circumstance catalogue lists factors that might be considered when assessing whether, under which conditions, and how a dataset can be released. While open data remains an important route for the publication of government information, we conclude that it is not the only route, and there must be clear and robust public interest arguments in order to justify the disclosure of personal information as open data.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04828,review,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'The Stagnant Persistence Paradox: Survival Analysis and Temporal Efficiency in Exact Sciences and Engineering Education\n\nResearch on student progression in higher education has traditionally focused on vertical outcomes such as persistence and dropout, often reducing complex academic histories to binary indicators. While the structural component of horizontal mobility (major switching, plan changes, re-entries) has recently been recognised as a core feature of contemporary university systems, the temporal cost and efficiency of these pathways remain largely unquantified. Using forty years of administrative records from a large faculty of engineering and exact sciences in Argentina (N = 24,016), this study applies a dual-outcome survival analysis framework to two key outcomes: definitive dropout and first major switch. We reconstruct academic trajectories as sequences of enrolment spells and typed transitions under the CAPIRE protocol, and then deploy non-parametric Kaplan-Meier estimators to model time-to-event under right-censoring. Results uncover a critical systemic inefficiency: a global median survival time of 4.33 years prior to definitive dropout, with a pronounced long tail of extended enrolment. This pattern reveals a phenomenon of stagnant persistence, where students remain formally enrolled for long periods without commensurate curricular progression. In contrast, major switching follows an early-event regime, with a median time of 1.0 year among switchers and most switches concentrated within the first academic year. We argue that academic failure in rigid engineering curricula is not a sudden outcome but a long-tail process that generates high opportunity costs, and that institutional indicators should shift from static retention metrics towards measures of curricular velocity based on time-to-event analysis.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 3.5762786865234375e-07, 'GPT4': 0.0002130270004272461, 'CLAUDE': 0.0027256011962890625, 'GOOGLE': 1.6689300537109375e-06, 'OPENAI_O_SERIES': 1.1920928955078125e-07, 'DEEPSEEK': 0.9970703125, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 5.060434341430664e-05}}"
2512.07797,regular,post_llm,2025,12,"{'ai_likelihood': 1.662307315402561e-05, 'text': 'LLM Use for Mental Health: Crowdsourcing Users\' Sentiment-based Perspectives and Values from Social Discussions\n\nLarge language models (LLMs) chatbots like ChatGPT are increasingly used for mental health support. They offer accessible, therapeutic support but also raise concerns about misinformation, over-reliance, and risks in high-stakes contexts of mental health. We crowdsource large-scale users\' posts from six major social media platforms to examine how people discuss their interactions with LLM chatbots across different mental health conditions. Through an LLM-assisted pipeline grounded in Value-Sensitive Design (VSD), we mapped the relationships across user-reported sentiments, mental health conditions, perspectives, and values. Our results reveal that the use of LLM chatbots is condition-specific. Users with neurodivergent conditions (e.g., ADHD, ASD) report strong positive sentiments and instrumental or appraisal support, whereas higher-risk disorders (e.g., schizophrenia, bipolar disorder) show more negative sentiments. We further uncover how user perspectives co-occur with underlying values, such as identity, autonomy, and privacy. Finally, we discuss shifting from ""one-size-fits-all"" chatbot design toward condition-specific, value-sensitive LLM design.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.01842,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'Free Tuition, Stratified Pipelines: Four Decades of Administrative Cohorts and Equity in Access to Engineering and Science in an Argentine Public University\n\nLatin American higher education is often portrayed as equitable when tuition is free and access to public universities is formally unrestricted. Yet, growing research shows that massification under tuition-free policies often coexists with strong social and territorial stratification. This article uses four decades of administrative records from a faculty of engineering in north-western Argentina to examine how cohort composition has changed over time.\n  Drawing on 24,133 first-time entrants (1980-2019), we construct a leakage-aware ""background census"" layer (N1c) harmonising school type, province, and age across legacy systems. We combine descriptive analyses, UMAP+DBSCAN clustering, and a reconstructed macroeconomic panel (inflation, unemployment, poverty, GDP) anchored at entry. All analyses explicitly report structural missingness patterns. Results show that missingness in background variables is historically patterned, declining sharply after the 1990s. Among students with observed data, the share coming from private-especially religious-secondary schools in high-income areas increased from less than half in the 1980s to roughly two-thirds in the 2010s. The catchment area became more local, with the home province gaining weight while distant origins lost ground. Median age at entry remained stable at 19, with persistent right tails of older entrants. Macro-linkage analyses reveal moderate associations between unemployment and older entry age, and between inflation and higher shares of students from interior provinces. We argue that free tuition and open entry have operated within, rather than against, stratified school and residential pipelines. The article illustrates how administrative data can support equity monitoring and discusses implications for upstream school policies and institutional accountability in tuition-free systems.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 0.00012969970703125, 'GPT4': 0.0027103424072265625, 'CLAUDE': 0.2086181640625, 'GOOGLE': 0.0006022453308105469, 'OPENAI_O_SERIES': 1.0728836059570312e-05, 'DEEPSEEK': 0.787109375, 'GROK': 5.960464477539062e-07, 'NOVA': 1.1920928955078125e-07, 'OTHER': 1.7702579498291016e-05, 'HUMAN': 0.0006437301635742188}}"
2512.0463,review,post_llm,2025,12,"{'ai_likelihood': 0.0008281071980794271, 'text': 'Reflection-Satisfaction Tradeoff: Investigating Impact of Reflection on Student Engagement with AI-Generated Programming Hints\n\nGenerative AI tools, such as AI-generated hints, are increasingly integrated into programming education to offer timely, personalized support. However, little is known about how to effectively leverage these hints while ensuring autonomous and meaningful learning. One promising approach involves pairing AI-generated hints with reflection prompts, asking students to review and analyze their learning, when they request hints. This study investigates the interplay between AI-generated hints and different designs of reflection prompts in an online introductory programming course. We conducted a two-trial field experiment. In Trial 1, students were randomly assigned to receive prompts either before or after receiving hints, or no prompt at all. Each prompt also targeted one of three SRL phases: planning, monitoring, and evaluation. In Trial 2, we examined two types of prompt guidance: directed (offering more explicit and structured guidance) and open (offering more general and less constrained guidance). Findings show that students in the before-hint (RQ1), planning (RQ2), and directed (RQ3) prompt groups produced higher-quality reflections but reported lower satisfaction with AI-generated hints than those in other conditions. Immediate performance did not differ across conditions. This negative relationship between reflection quality and hint satisfaction aligns with previous work on student mental effort and satisfaction. Our results highlight the need to reconsider how AI models are trained and evaluated for education, as prioritizing user satisfaction can undermine deeper learning.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04489,regular,post_llm,2025,12,"{'ai_likelihood': 0.0009430779351128472, 'text': 'The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance\n\nArtificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI ""train"" without a ""brake system."" By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI\'s intrinsic disconnect from the analog physical world: AI\'s nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a ""brake system"" for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.02544,review,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty\n\nAs AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. Current ethical frameworks in this domain often implicitly rely on contested functionalist assumptions, prioritize speculative AI welfare over concrete human interests, and lack coherent theoretical foundations. We address these limitations through a structured three-level framework grounded in philosophical uncertainty. At the foundational level, we establish five factual determinations about AI consciousness alongside human-centralism as our meta-ethical stance. These foundations logically entail three operational principles: presumption of no consciousness (placing the burden of proof on consciousness claims), risk prudence (prioritizing human welfare under uncertainty), and transparent reasoning (enabling systematic evaluation and adaptation). At the application level, the third component of our framework, we derive default positions on pressing ethical questions through a transparent logical process where each position can be explicitly traced back to our foundational commitments. Our approach balances philosophical rigor with practical guidance, distinguishes consciousness from anthropomorphism, and creates pathways for responsible evolution as scientific understanding advances, providing a human-centric foundation for navigating these profound ethical challenges.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 1.1920928955078125e-07, 'GPT4': 8.082389831542969e-05, 'CLAUDE': 0.99609375, 'GOOGLE': 2.9802322387695312e-06, 'OPENAI_O_SERIES': 1.430511474609375e-06, 'DEEPSEEK': 0.00386810302734375, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 1.8477439880371094e-06}}"
2512.07333,regular,post_llm,2025,12,"{'ai_likelihood': 1.0, 'text': 'IyaCare: An Integrated AI-IoT-Blockchain Platform for Maternal Health in Resource-Constrained Settings\n\nMaternal mortality in Sub-Saharan Africa remains critically high, accounting for 70% of global deaths despite representing only 17% of the world population. Current digital health interventions typically deploy artificial intelligence (AI), Internet of Things (IoT), and blockchain technologies in isolation, missing synergistic opportunities for transformative healthcare delivery. This paper presents IyaCare, a proof-of-concept integrated platform that combines predictive risk assessment, continuous vital sign monitoring, and secure health records management specifically designed for resource-constrained settings. We developed a web-based system with Next.js frontend, Firebase backend, Ethereum blockchain architecture, and XGBoost AI models trained on maternal health datasets. Our feasibility study demonstrates 85.2% accuracy in high-risk pregnancy prediction and validates blockchain data integrity, with key innovations including offline-first functionality and SMS-based communication for community health workers. While limitations include reliance on synthetic validation data and simulated healthcare environments, results confirm the technical feasibility and potential impact of converged digital health solutions. This work contributes a replicable architectural model for integrated maternal health platforms in low-resource settings, advancing progress toward SDG 3.1 targets.', 'prediction': 'Highly Likely AI', 'llm_prediction': {'GPT35': 5.960464477539063e-08, 'GPT4': 3.266334533691406e-05, 'CLAUDE': 0.9609375, 'GOOGLE': 5.364418029785156e-07, 'OPENAI_O_SERIES': 6.556510925292969e-07, 'DEEPSEEK': 0.039031982421875, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 5.960464477539063e-08, 'HUMAN': 0.0}}"
2512.05929,review,post_llm,2025,12,"{'ai_likelihood': 0.0008800294664171007, 'text': 'LLM Harms: A Taxonomy and Discussion\n\nThis study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.0219,regular,post_llm,2025,12,"{'ai_likelihood': 0.0007396274142795139, 'text': 'Towards Modeling Road Access Deprivation in Sub-Saharan Africa Based on a New Accessibility Metric and Road Quality\n\nAccess to motorable roads is a critical dimension of urban infrastructure, particularly in rapidly urbanizing regions such as Sub-Saharan Africa. Yet, many urban communities, especially those in informal settlements, remain disconnected from road networks. This study presents a road access deprivation model that combines a new accessibility metric, capturing how well buildings are connected to the road network, with road surface type data as a proxy for road quality. These two components together enable the classification of urban areas into low, medium, or high deprivation levels. The model was applied to Nairobi (Kenya), Lagos (Nigeria), and Kano (Nigeria) using open geospatial datasets. Across all three cities, the majority of built-up areas fall into the low and medium road access deprivation levels, while highly deprived areas are comparatively limited. However, the share of highly deprived areas varies substantially, ranging from only 11.8 % in Nairobi to 27.7 % in Kano. Model evaluation against community-sourced validation data indicates good performance for identifying low deprivation areas (F1 > 0.74), moderate accuracy for medium deprivation in Nairobi and Lagos (F1 > 0.52, lower in Kano), and more variable results for high deprivation (F1 ranging from 0.26 in Kano to 0.69 in Nairobi). Furthermore, analysis of grid cells with multiple validations showed strong agreement among community members, with disagreements occurring mainly between adjacent deprivation levels. Finally, we discussed two types of sources for disagreement with community validations: (1) misalignment between the conceptual model and community perceptions, and (2) the operationalization of the conceptual model. In summary, our road access deprivation modeling approach demonstrates promise as a scalable, interpretable tool for identifying disconnected areas and informing urban planning in data-scarce contexts.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.01692,review,post_llm,2025,12,"{'ai_likelihood': 0.16805013020833334, 'text': 'Forced Migration and Information-Seeking Behavior on Wikipedia: Insights from the Ukrainian Refugee Crisis\n\nGathering information about where to migrate is an important part of the migration process, especially during forced migration, when people must make rapid decisions under uncertainty. This study examines how forced migration relates to online information-seeking on Wikipedia. Focusing on the 2022 Russian invasion of Ukraine, we analyze how the resulting refugee crisis, which led to over six million Ukrainians fleeing across Europe, shaped views of Wikipedia articles about European cities. We compare changes in views of Ukrainian-language Wikipedia articles, used as a proxy for information-seeking by Ukrainians, with those in four other language editions. Our findings show that views of Ukrainian-language articles about European cities correlate more strongly with the number of Ukrainian refugees applying for temporary protection in European countries than views in other languages. Because Poland and Germany became the main destinations for refugees, we examine these countries more closely and find that applications for temporary protection in Polish and German cities are also more strongly correlated with views of their Ukrainian-language Wikipedia articles. We further analyze the timing between refugee flows to Poland and online information-seeking. Refugee border crossings occurred before increases in Ukrainian-language views of Polish city articles, indicating that information-seeking surged after displacement. This reactive pattern contrasts with the pre-departure planning typical of regular labor migration. Moreover, while official protection applications often lagged behind border crossings by weeks, Wikipedia activity rose almost immediately. Overall, Wikipedia usage offers a near real-time indicator of emerging migration patterns during crises.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.04285,regular,post_llm,2025,12,"{'ai_likelihood': 0.14824761284722224, 'text': 'Differential Filtering in a Common Basic Cycle: Multi-Major Trajectories and Structural Bottlenecks in Exact Sciences and Engineering Degrees\n\nUniversities often present the Common Basic Cycle (CBC) as a neutral levelling stage shared by several degree programmes. Using twenty years of longitudinal administrative records from a Faculty of Engineering and Exact Sciences, this study tests whether the CBC actually operates as a uniform gateway or as a differential filter across majors. We reconstruct student trajectories for 24,017 entrants, identifying CBC subjects (year level <= 1), destination major, time to exit from the CBC, and final outcome (progression to upper cycle, drop-out, or right-censoring). The analysis combines transition matrices, Kaplan-Meier survival curves, stratified Cox models and subject-level logistic models of drop-out after failure, extended with multi-major enrolment data and a pre/post 2006 curriculum reform comparison. Results show that the CBC functions as a strongly differential filter. Post-reform, the probability of progressing to the upper cycle in the same major ranges from about 0.20 to 0.70 across programmes, while overall drop-out in the CBC exceeds 60%. Early Mathematics modules (introductory calculus and algebra) emerge as structural bottlenecks, combining low pass rates with a two- to three-fold increase in the hazard of leaving the system after failure, with markedly different severity by destination major. Multi-major enrolment, often treated administratively as indecision, is instead associated with lower drop-out, suggesting an adaptive exploration of feasible trajectories. The findings portray the CBC not as a neutral academic foyer, but as a structured sorting device whose impact depends sharply on the targeted degree and on the opportunity to explore alternative majors.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.09261,regular,post_llm,2025,12,"{'ai_likelihood': 0.05533854166666667, 'text': 'FLARE v2: A Recursive Framework for Program Comprehension Across Languages and Levels of Abstraction\n\nBuilding on the classroom framework reported in Heath et al. (2025), this paper proposes FLARE v2 as a recursive, semiotically informed account of how program meaning is constructed. It reinterprets the descriptive tiers of FLARE v1 as instances of a single generative operation: identify elements (characterised by the four properties Receives, Sends, Effects, Shares); analyse their bindings along two dimensions (Causal-Temporal and Communicative); and recognise the new element that emerges. The Causal-Temporal dimension encompasses three subtypes - Sequential, Branch, and Event - that together account for control flow in both procedural and event-driven environments. A Compositional Ladder provides a visual parallel between literacy progressions and programming structures, illustrating how recursive composition operates from blocks and statements through segments, systems, and services. The framework aims to address conceptual and cognitive-load limitations reported in FLARE v1 and is situated within semiotic and program-comprehension theory. FLARE v2 is presented as a conceptual lens with potential implications for pedagogy and curriculum design; implementation and empirical evaluation are left for future work.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
2512.05742,review,post_llm,2025,12,"{'ai_likelihood': 3.311369154188368e-06, 'text': 'Internal Deployment in the EU AI Act\n\nThis memorandum analyzes and stress-tests arguments in favor and against the inclusion of internal deployment within the scope of the European Union Artificial Intelligence Act (EU AI Act). In doing so, it aims to offer several possible interpretative pathways to the European Commission, AI providers and deployers, and the legal and policy community at large based on Articles 2(1), 2(6), 2(8) of the EU AI Act. Specifically, this memorandum first analyzes four interpretative pathways based on Article 2(1)(a)-(c) supporting the application of the EU AI Act to internally deployed AI models and systems. Then, it examines possible objections and exceptions based on Articles 2(1)(a), 2(6), and 2(8), with particular attention to the complexity of the scientific R&D exception under Article 2(6). Finally, it illustrates how Articles 2(1), 2(6), and 2(8) can be viewed as complementary to each other, once broken down to their most plausible meaning and interpreted in conjunction with Articles 3(1), 3(3), 3(4), 3(9), 3(10), 3(11), 3(12), 3(63), and Recitals 12, 13, 21, 25, 97, and 109.', 'prediction': 'Unlikely AI', 'llm_prediction': {'GPT35': 0.0, 'GPT4': 0.0, 'CLAUDE': 0.0, 'GOOGLE': 0.0, 'OPENAI_O_SERIES': 0.0, 'DEEPSEEK': 0.0, 'GROK': 0.0, 'NOVA': 0.0, 'OTHER': 0.0, 'HUMAN': 0.0}}"
