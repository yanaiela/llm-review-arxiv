[
  {
    "arxiv_id":2001.10256,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"WikiHist.html: English Wikipedia's Full Revision History in HTML Format\n\n  Wikipedia is written in the wikitext markup language. When serving content,\nthe MediaWiki software that powers Wikipedia parses wikitext to HTML, thereby\ninserting additional content by expanding macros (templates and mod-ules).\nHence, researchers who intend to analyze Wikipediaas seen by its readers should\nwork with HTML, rather than wikitext. Since Wikipedia's revision history is\npublicly available exclusively in wikitext format, researchers have had to\nproduce HTML themselves, typically by using Wikipedia's REST API for ad-hoc\nwikitext-to-HTML parsing. This approach, however, (1) does not scale to very\nlarge amounts ofdata and (2) does not correctly expand macros in historical\narticle revisions. We solve these problems by developing a parallelized\narchitecture for parsing massive amounts of wikitext using local instances of\nMediaWiki, enhanced with the capacity of correct historical macro expansion. By\ndeploying our system, we produce and release WikiHist.html, English Wikipedia's\nfull revision history in HTML format. We highlight the advantages of\nWikiHist.html over raw wikitext in an empirical analysis of Wikipedia's\nhyperlinks, showing that over half of the wiki links present in HTML are\nmissing from raw wikitext and that the missing links are important for user\nnavigation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09954,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Ten Social Dimensions of Conversations and Relationships\n\n  Decades of social science research identified ten fundamental dimensions that\nprovide the conceptual building blocks to describe the nature of human\nrelationships. Yet, it is not clear to what extent these concepts are expressed\nin everyday language and what role they have in shaping observable dynamics of\nsocial interactions. After annotating conversational text through\ncrowdsourcing, we trained NLP tools to detect the presence of these types of\ninteraction from conversations, and applied them to 160M messages written by\ngeo-referenced Reddit users, 290k emails from the Enron corpus and 300k lines\nof dialogue from movie scripts. We show that social dimensions can be predicted\npurely from conversations with an AUC up to 0.98, and that the combination of\nthe predicted dimensions suggests both the types of relationships people\nentertain (conflict vs. support) and the types of real-world communities\n(wealthy vs. deprived) they shape.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.01511,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Decentralization in Digital Societies -- A Design Paradox\n\n  Digital societies come with a design paradox: On the one hand, technologies,\nsuch as Internet of Things, pervasive and ubiquitous systems, allow a\ndistributed local intelligence in interconnected devices of our everyday life\nsuch as smart phones, smart thermostats, self-driving cars, etc. On the other\nhand, Big Data collection and storage is managed in a highly centralized\nfashion, resulting in privacy-intrusion, surveillance actions, discriminatory\nand segregation social phenomena. What is the difference between a distributed\nand a decentralized system design? How \"decentralized\" is the processing of our\ndata nowadays? Does centralized design undermine autonomy? Can the level of\ndecentralization in the implemented technologies influence ethical and social\ndimensions, such as social justice? Can decentralization convey sustainability?\nAre there parallelisms between the decentralization of digital technology and\nthe decentralization of urban development?\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09955,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"The Effects of Gender Signals and Performance in Online Product Reviews\n\n  This work quantifies the effects of signaling and performing gender on the\nsuccess of reviews written on the popular amazon shopping platform. Highly\nrated reviews play an important role in e-commerce since they are prominently\ndisplayed below products. Differences in how gender-signaling and\ngender-performing review authors are received can lead to important biases in\nwhat content and perspectives are represented among top reviews. To investigate\nthis, we extract signals of author gender from user names, distinguishing\nreviews where the author's likely gender can be inferred. Using reviews\nauthored by these gender-signaling authors, we train a deep-learning classifier\nto quantify the gendered writing style or gendered performance of reviews\nwritten by authors who do not send clear gender signals via their user name. We\ncontrast the effects of gender signaling and performance on review success\nusing matching experiments. While we find no general trend that gendered\nsignals or performances influence overall review success, we find strong\ncontext-specific effects. For example, reviews in product categories such as\nElectronics or Computers are perceived as less helpful when authors signal that\nthey are likely woman, but are received as more helpful in categories such as\nBeauty or Clothing. In addition to these interesting findings, our work\nprovides a general chain of tools for studying gender-specific effects across\nvarious social media platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.07864,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Fairness Metrics: A Comparative Analysis\n\n  Algorithmic fairness is receiving significant attention in the academic and\nbroader literature due to the increasing use of predictive algorithms,\nincluding those based on artificial intelligence. One benefit of this trend is\nthat algorithm designers and users have a growing set of fairness measures to\nchoose from. However, this choice comes with the challenge of identifying how\nthe different fairness measures relate to one another, as well as the extent to\nwhich they are compatible or mutually exclusive. We describe some of the most\nwidely used fairness metrics using a common mathematical framework and present\nnew results on the relationships among them. The results presented herein can\nhelp place both specialists and non-specialists in a better position to\nidentify the metric best suited for their application and goals.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04574,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000123514,
      "text":"Preliminary Study of a Google Home Mini\n\n  Many artificial intelligence (AI) speakers have recently come to market.\nBeginning with Amazon Echo, many companies producing their own speaker\ntechnologies. Due to the limitations of technology, most speakers have similar\nfunctions, but the way of handling the data of each speaker is different. In\nthe case of Amazon echo, the API of the cloud is open for any developers to\ndevelop their API. The Amazon Echo has been around for a while, and much\nresearch has been done on it. However, not much research has been done on\nGoogle Home Mini analysis for digital investigations. In this paper, we will\nconduct some initial research on the data storing and security methods of\nGoogle Home Mini.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09768,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Artificial Intelligence, Values and Alignment\n\n  This paper looks at philosophical questions that arise in the context of AI\nalignment. It defends three propositions. First, normative and technical\naspects of the AI alignment problem are interrelated, creating space for\nproductive engagement between people working in both domains. Second, it is\nimportant to be clear about the goal of alignment. There are significant\ndifferences between AI that aligns with instructions, intentions, revealed\npreferences, ideal preferences, interests and values. A principle-based\napproach to AI alignment, which combines these elements in a systematic way,\nhas considerable advantages in this context. Third, the central challenge for\ntheorists is not to identify 'true' moral principles for AI; rather, it is to\nidentify fair principles for alignment, that receive reflective endorsement\ndespite widespread variation in people's moral beliefs. The final part of the\npaper explores three ways in which fair principles for AI alignment could\npotentially be identified.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.00964,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Saving Face: Investigating the Ethical Concerns of Facial Recognition\n  Auditing\n\n  Although essential to revealing biased performance, well intentioned attempts\nat algorithmic auditing can have effects that may harm the very populations\nthese measures are meant to protect. This concern is even more salient while\nauditing biometric systems such as facial recognition, where the data is\nsensitive and the technology is often used in ethically questionable manners.\nWe demonstrate a set of five ethical concerns in the particular case of\nauditing commercial facial processing technology, highlighting additional\ndesign considerations and ethical tensions the auditor needs to be aware of so\nas not exacerbate or complement the harms propagated by the audited system. We\ngo further to provide tangible illustrations of these concerns, and conclude by\nreflecting on what these concerns mean for the role of the algorithmic audit\nand the fundamental product limitations they reveal.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09758,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Towards organizational guidelines for the responsible use of AI\n\n  In the past few years, several large companies have published ethical\nprinciples of Artificial Intelligence (AI). National governments, the European\nCommission, and inter-governmental organizations have come up with requirements\nto ensure the good use of AI. However, individual organizations that want to\njoin this effort, are faced with many unsolved questions. This paper proposes\nguidelines for organizations committed to the responsible use of AI, but lack\nthe required knowledge and experience. The guidelines consist of two parts: i)\nhelping organizations to decide what principles to adopt, and ii) a methodology\nfor implementing the principles in organizational processes. In case of future\nAI regulation, organizations following this approach will be well-prepared.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04513,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Donald Duck Holiday Game: A numerical analysis of a Game of the Goose\n  role-playing variant\n\n  The 1996 Donald Duck Holiday Game is a role-playing variant of the historical\nGame of the Goose, involving characters with unique attributes, event squares,\nand random event cards. The objective of the game is to reach the camping\nbefore any other player does. We develop a Monte Carlo simulation model that\nautomatically plays the game and enables analyzing its key characteristics. We\nassess the game on various metrics relevant to each playability. Numerical\nanalysis shows that, on average, the game takes between 69 and 123 rounds to\ncomplete, depending on the number of players. However, durations over one hour\n(translated to human play time) occur over 25% of the games, which might reduce\nthe quality of the gaming experience. Furthermore, we show that two characters\nare about 30% likely to win than the other three, primarily due to being\nexposed to fewer random events. We argue that the richer narrative of\nrole-playing games may extend the duration for which the game remains\nenjoyable, such that the metrics cannot directly be compared to those of the\ntraditional Game-of-the-Goose. Based on our analysis, we provide several\nsuggestions to improve the game balance with only slight modifications. In a\nbroader sense, we demonstrate that a basic Monte Carlo simulation suffices to\nanalyze Game-of-the-Goose role-playing variants, verify how they score on\ncriteria that contribute to an enjoyable game, and detect possible anomalies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05961,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"FaceLift: A transparent deep learning framework to beautify urban scenes\n\n  In the area of computer vision, deep learning techniques have recently been\nused to predict whether urban scenes are likely to be considered beautiful: it\nturns out that these techniques are able to make accurate predictions. Yet they\nfall short when it comes to generating actionable insights for urban design. To\nsupport urban interventions, one needs to go beyond predicting beauty, and\ntackle the challenge of recreating beauty. Unfortunately, deep learning\ntechniques have not been designed with that challenge in mind. Given their\n\"black-box nature\", these models cannot be directly used to explain why a\nparticular urban scene is deemed to be beautiful. To partly fix that, we\npropose a deep learning framework called Facelift, that is able to both\nbeautify existing urban scenes (Google Street views) and explain which urban\nelements make those transformed scenes beautiful. To quantitatively evaluate\nour framework, we cannot resort to any existing metric (as the research problem\nat hand has never been tackled before) and need to formulate new ones. These\nnew metrics should ideally capture the presence\/absence of elements that make\nurban spaces great. Upon a review of the urban planning literature, we identify\nfive main metrics: walkability, green spaces, openness, landmarks and visual\ncomplexity. We find that, across all the five metrics, the beautified scenes\nmeet the expectations set by the literature on what great spaces tend to be\nmade of. This result is further confirmed by a 20-participant expert survey in\nwhich FaceLift have been found to be effective in promoting citizen\nparticipation. All this suggests that, in the future, as our framework's\ncomponents are further researched and become better and more sophisticated, it\nis not hard to imagine technologies that will be able to accurately and\nefficiently support architects and planners in the design of spaces we\nintuitively love.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.00367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"Cost of Dietary Data Acquisition with Smart Group Catering\n\n  The need for dietary data management is growing with public awareness of food\nintakes. As a result, there are increasing deployments of smart canteens where\ndietary data is collected through either Radio Frequency Identification (RFID)\nor Computer Vision(CV)-based solutions. As human labor is involved in both\ncases, manpower allocation is critical to data quality. Where manpower\nrequirements are underestimated, data quality is compromised. This paper has\nstudied the relation between the quality of dietary data and the manpower\ninvested, using numerical simulations based on real data collected from\nmultiple smart canteens. We found that in both RFID and CV-based systems, the\nlong-term cost of dietary data acquisition is dominated by manpower. Our study\nprovides a comprehensive understanding of the cost composition for dietary data\nacquisition and useful insights toward future cost effective systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09762,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Bias in Data-driven AI Systems -- An Introductory Survey\n\n  AI-based systems are widely employed nowadays to make decisions that have\nfar-reaching impacts on individuals and society. Their decisions might affect\neveryone, everywhere and anytime, entailing concerns about potential human\nrights issues. Therefore, it is necessary to move beyond traditional AI\nalgorithms optimized for predictive performance and embed ethical and legal\nprinciples in their design, training and deployment to ensure social good while\nstill benefiting from the huge potential of the AI technology. The goal of this\nsurvey is to provide a broad multi-disciplinary overview of the area of bias in\nAI systems, focusing on technical challenges and solutions as well as to\nsuggest new research directions towards approaches well-grounded in a legal\nframe. In this survey, we focus on data-driven AI, as a large part of AI is\npowered nowadays by (big) data and powerful Machine Learning (ML) algorithms.\nIf otherwise not specified, we use the general term bias to describe problems\nrelated to the gathering or processing of data that might result in prejudiced\ndecisions on the bases of demographic features like race, sex, etc.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09755,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000004073,
      "text":"Fairness and Decision-making in Collaborative Shift Scheduling Systems\n\n  The strains associated with shift work decrease healthcare workers'\nwell-being. However, shift schedules adapted to their individual needs can\npartially mitigate these problems. From a computing perspective, shift\nscheduling was so far mainly treated as an optimization problem with little\nattention given to the preferences, thoughts, and feelings of the healthcare\nworkers involved. In the present study, we explore fairness as a central,\nhuman-oriented attribute of shift schedules as well as the scheduling process.\nThree in-depth qualitative interviews and a validating vignette study revealed\nthat while on an abstract level healthcare workers agree on equality as the\nguiding norm for a fair schedule, specific scheduling conflicts should foremost\nbe resolved by negotiating the importance of individual needs. We discuss\nelements of organizational fairness, including transparency and team spirit.\nFinally, we present a sketch for fair scheduling systems, summarizing key\nfindings for designers in a readily usable way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.08293,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"\"How over is it?\" Understanding the Incel Community on YouTube\n\n  YouTube is by far the largest host of user-generated video content worldwide.\nAlas, the platform has also come under fire for hosting inappropriate, toxic,\nand hateful content. One community that has often been linked to sharing and\npublishing hateful and misogynistic content are the Involuntary Celibates\n(Incels), a loosely defined movement ostensibly focusing on men's issues. In\nthis paper, we set out to analyze the Incel community on YouTube by focusing on\nthis community's evolution over the last decade and understanding whether\nYouTube's recommendation algorithm steers users towards Incel-related videos.\nWe collect videos shared on Incel communities within Reddit and perform a\ndata-driven characterization of the content posted on YouTube.\n  Among other things, we find that the Incel community on YouTube is getting\ntraction and that, during the last decade, the number of Incel-related videos\nand comments rose substantially. We also find that users have a 6.3% chance of\nbeing suggested an Incel-related video by YouTube's recommendation algorithm\nwithin five hops when starting from a non Incel-related video. Overall, our\nfindings paint an alarming picture of online radicalization: not only Incel\nactivity is increasing over time, but platforms may also play an active role in\nsteering users towards such extreme content.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.00973,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000110269,
      "text":"Closing the AI Accountability Gap: Defining an End-to-End Framework for\n  Internal Algorithmic Auditing\n\n  Rising concern for the societal implications of artificial intelligence\nsystems has inspired a wave of academic and journalistic literature in which\ndeployed systems are audited for harm by investigators from outside the\norganizations deploying the algorithms. However, it remains challenging for\npractitioners to identify the harmful repercussions of their own systems prior\nto deployment, and, once deployed, emergent issues can become difficult or\nimpossible to trace back to their source. In this paper, we introduce a\nframework for algorithmic auditing that supports artificial intelligence system\ndevelopment end-to-end, to be applied throughout the internal organization\ndevelopment lifecycle. Each stage of the audit yields a set of documents that\ntogether form an overall audit report, drawing on an organization's values or\nprinciples to assess the fit of decisions made throughout the process. The\nproposed auditing framework is intended to contribute to closing the\naccountability gap in the development and deployment of large-scale artificial\nintelligence systems by embedding a robust process to ensure audit integrity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.08777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Whose Tweets are Surveilled for the Police: An Audit of Social-Media\n  Monitoring Tool via Log Files\n\n  Social media monitoring by law enforcement is becoming commonplace, but\nlittle is known about what software packages for it do. Through public records\nrequests, we obtained log files from the Corvallis (Oregon) Police Department's\nuse of social media monitoring software called DigitalStakeout. These log files\ninclude the results of proprietary searches by DigitalStakeout that were\nrunning over a period of 13 months and include 7240 social media posts. In this\npaper, we focus on the Tweets logged in this data and consider the racial and\nethnic identity (through manual coding) of the users that are therein flagged\nby DigitalStakeout. We observe differences in the demographics of the users\nwhose Tweets are flagged by DigitalStakeout compared to the demographics of the\nTwitter users in the region, however, our sample size is too small to determine\nsignificance. Further, the demographics of the Twitter users in the region do\nnot seem to reflect that of the residents of the region, with an apparent\nhigher representation of Black and Hispanic people. We also reconstruct the\nkeywords related to a Narcotics report set up by DigitalStakeout for the\nCorvallis Police Department and find that these keywords flag Tweets unrelated\nto narcotics or flag Tweets related to marijuana, a drug that is legal for\nrecreational use in Oregon. Almost all of the keywords have a common meaning\nunrelated to narcotics (e.g.\\ broken, snow, hop, high) that call into question\nthe utility that such a keyword based search could have to law enforcement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09745,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Constraints and Benefits of the Blockchain Use for Real Estate and\n  Property Rights\n\n  Many recent social media posts and news may create a perception of big\nsuccess in the use of blockchain for the real estate industry, land\nregistration and protection of titles and property rights. A sobering outlook\nis crucial because misleading concepts may bury the whole idea of blockchain\nuse. The paper aims to research the possibilities of blockchain and other\ndistributed ledger technologies (DLT) and applicability of these technologies\nfor different purposes in real estate, property rights and public registries.\nBlockchain, which is distinguished from permissioned systems as the technology\nof the immutable ledger that does not require authorities, is a new word in\ngovernance. However, this technology has some principal features that can\nrestrain its implementation at the state level, and thus require further\nresearch and development. The application of blockchain requires a proper\narchitecture of overlaid technologies to support changes of outdated and\nmistaken data, address issues of digital identity and privacy, legal compliance\nand enforceability of smart contracts and scalability of the ledger. This paper\nshows the constraints of the technology's properties which were not explained\nbefore in the context of title rights and land registration even though\ntechnological limits are known in more specific technical sources. Along with\nthe known benefits this meant to help to avoid misinterpretation of some DLT\nfeatures by non-technical people. A multidisciplinary approach in analysing the\ntechnology and laws helped to better understand what can and cannot be\nbeneficial for public registries and the protection of property rights. The\npresented outcomes can be laid down as requirements for the technical protocols\naimed at addressing the issues of DLT and public policies to put blockchain at\nthe service of society.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09763,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Autonomous Shuttle-as-a-Service (ASaaS): Challenges, Opportunities, and\n  Social Implications\n\n  Modern cities are composed of complex socio-technical systems that exist to\nprovide services effectively to their residents and visitors. In this context,\nsmart mobility systems aim to support the efficient exploitation of the city\ntransport facilities as well as sustainable mobility within the urban\nenvironment. People need to travel quickly and conveniently between locations\nat different scales, ranging from a trip of a few blocks within a city to a\njourney across cities or further. At the same time, goods need to be timely\ndelivered considering the needs of both the users and the businesses. While\nmost of the mobility and delivery solutions can cover significant distances and\nmultiple requests, they suffer when the requests come from the growing\nneighborhoods and hard-to-reach areas such as city centers, corporate\nheadquarters, and hospitals. In the last few years, several cities indicated\ninterest in using Autonomous Vehicles (AV) for the \"last-mile\" mobility\nservices. With them, it seems to be easier to get people and goods around using\nfewer vehicles. In this context, Autonomous Shuttles (AS) are beginning to be\nthought of as a new mobility\/delivery service into the city center where narrow\nstreets are not easily served by traditional buses. They allow them to serve\ncritical areas with minimal new infrastructure and reducing noise and\npollution. The goal of this article is to present an innovative vision on the\nintroduction of the Autonomous Shuttles-as-a service (ASaaS) concept as the key\npillar for the realization of innovative and sustainable proximity mobility.\nThrough a set of real application scenarios, we present our view, and we\ndiscuss a set of challenges, opportunities, and social implications that this\nway to reimage the mobility of the future introduces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.05657,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000121527,
      "text":"Trustworthy AI in the Age of Pervasive Computing and Big Data\n\n  The era of pervasive computing has resulted in countless devices that\ncontinuously monitor users and their environment, generating an abundance of\nuser behavioural data. Such data may support improving the quality of service,\nbut may also lead to adverse usages such as surveillance and advertisement. In\nparallel, Artificial Intelligence (AI) systems are being applied to sensitive\nfields such as healthcare, justice, or human resources, raising multiple\nconcerns on the trustworthiness of such systems. Trust in AI systems is thus\nintrinsically linked to ethics, including the ethics of algorithms, the ethics\nof data, or the ethics of practice. In this paper, we formalise the\nrequirements of trustworthy AI systems through an ethics perspective. We\nspecifically focus on the aspects that can be integrated into the design and\ndevelopment of AI systems. After discussing the state of research and the\nremaining challenges, we show how a concrete use-case in smart cities can\nbenefit from these methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0402,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"Cloudifying the Curriculum with AWS\n\n  The Cloud has become a principal paradigm of computing in the last ten years,\nand Computer Science curricula must be updated to reflect that reality. This\npaper examines simple ways to accomplish curriculum cloudification using Amazon\nWeb Services (AWS), for Computer Science and other disciplines such as\nBusiness, Communication and Mathematics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07768,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Relationship between the visibility of political leaders during campaign\n  and the outcome in general elections. A case study for Spain\n\n  In this article, the authors find the evidence that media coverage consisting\nof 13 online newspapers enhanced the electoral results of right wing party in\nSpain (Vox) during general elections in November 2019. We consider the\npolitical parties and leaders mentions in these media during the electoral\ncampaign from 1st to 10th November 2019, and only visibility or prominence\ndimension is necessary for the evidence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07473,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"The didactic potential of virtual information educational environment as\n  a tool of geography students training\n\n  The article clarifies the concept of \"virtual information educational\nenvironment\" (VIEE) and examines the researchers' views on its meaning exposed\nin the scientific literature. The article determines the didactic potential of\nthe virtual information educational environment for the geography students\ntraining based on the analysis of the authors' experience of blended learning\nby means of the Google Classroom. It also specifies the features (immersion,\ninteractivity, and dynamism, sense of presence, continuity, and causality). The\nauthors highlighted the advantages of virtual information educational\nenvironment implementation, such as: increase of the efficiency of the\neducational process by intensifying the process of cognition and interpersonal\ninteractive communication; continuous access to multimedia content both in\nGoogle Classroom and beyond; saving student time due to the absence of\nnecessity to work out the training material \"manually\"; availability of virtual\npages of the virtual class; individualization of the educational process;\nformation of informational culture of the geography students ; and more\nproductive learning of the educational material at the expense of IT\neducational facilities. Among the disadvantages the article mentions low level\nof computerization, insignificant quantity and low quality of software\nproducts, underestimation of the role of VIEE in the professional training of\ngeography students, and the lack of economic stimuli, etc.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.12616,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Mobile Phone Usage Data for Credit Scoring\n\n  The aim of this study is to demostrate that mobile phone usage data can be\nused to make predictions and find the best classification method for credit\nscoring even if the dataset is small (2,503 customers). We use different\nclassification algorithms to split customers into paying and non-paying ones\nusing mobile data, and then compare the predicted results with actual results.\nThere are several related works publicly accessible in which mobile data has\nbeen used for credit scoring, but they are all based on a large dataset. Small\ncompanies are unable to use datasets as large as those used by these related\npapers, therefore these studies are of little use for them. In this paper we\ntry to argue that there is value in mobile phone usage data for credit scoring\neven if the dataset is small. We found that with a dataset that consists of\nmobile data based only on 2,503 customers, we can predict credit risk. The best\nclassification method gave us the result 0.62 AUC (area under the curve).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11203,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Interactive Summarizing -- Automatic Slide Localization Technology as\n  Generative Learning Tool\n\n  Making a summary is a common learning strategy in lecture learning. It is an\neffective way for learners to engage in both traditional and video lectures.\nVideo summarization is an effective technology applied to enhance learners'\nsummarizing experience in a video lecture. In this article, we propose to apply\ncutting-edge automatic slide localization technology to lecture video learning\nexperience. An interactive summarizing model is designed to explain how\nlearners are engaged in the video lecture learning process supported by\nconvolutional neural network and the possibility of related learning analytics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.06086,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Artificial Intelligence, connected products, virtual reality: potential\n  impacts on consumer safety in terms of their physical and psychological\n  ability or well-being\n\n  With the progressive digitalisation of a majority of services to communities\nand individuals, humankind is facing new challenges. While energy sources are\nrapidly dwindling and rigorous choices have to be made to ensure the\nsustainability of our environment, there is increasing concern in science and\nsociety about the safety of connected products and technology for the\nindividual user. This essay provides a first basis for further inquiry into the\nrisks in terms of potentially negative, short and long-term, effects of\nconnected technologies and massive digitalisation on the psychological and\/or\nphysical abilities and well-being of users or consumers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.1001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Driving with Data in the Motor City: Mining and Modeling Vehicle Fleet\n  Maintenance Data\n\n  The City of Detroit maintains an active fleet of over 2500 vehicles, spending\nan annual average of over \\$5 million on purchases and over \\$7.7 million on\nmaintenance. Modeling patterns and trends in this data is of particular\nimportance to a variety of stakeholders, particularly as Detroit emerges from\nChapter 9 bankruptcy, but the structure in such data is complex, and the city\nlacks dedicated resources for in-depth analysis. The City of Detroit's\nOperations and Infrastructure Group and the University of Michigan initiated a\ncollaboration which seeks to address this unmet need by analyzing data from the\nCity of Detroit's vehicle fleet. This work presents a case study and provides\nthe first data-driven benchmark, demonstrating a suite of methods to aid in\ndata understanding and prediction for large vehicle maintenance datasets. We\npresent analyses to address three key questions raised by the stakeholders,\nrelated to discovering multivariate maintenance patterns over time; predicting\nmaintenance; and predicting vehicle- and fleet-level costs. We present a novel\nalgorithm, PRISM, for automating multivariate sequential data analyses using\ntensor decomposition. This work is a first of its kind that presents both\nmethodologies and insights to guide future civic data research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11836,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"No computation without representation: Avoiding data and algorithm\n  biases through diversity\n\n  The emergence and growth of research on issues of ethics in AI, and in\nparticular algorithmic fairness, has roots in an essential observation that\nstructural inequalities in society are reflected in the data used to train\npredictive models and in the design of objective functions. While research\naiming to mitigate these issues is inherently interdisciplinary, the design of\nunbiased algorithms and fair socio-technical systems are key desired outcomes\nwhich depend on practitioners from the fields of data science and computing.\nHowever, these computing fields broadly also suffer from the same\nunder-representation issues that are found in the datasets we analyze. This\ndisconnect affects the design of both the desired outcomes and metrics by which\nwe measure success. If the ethical AI research community accepts this, we\ntacitly endorse the status quo and contradict the goals of non-discrimination\nand equity which work on algorithmic fairness, accountability, and transparency\nseeks to address. Therefore, we advocate in this work for diversifying\ncomputing as a core priority of the field and our efforts to achieve ethical AI\npractices. We draw connections between the lack of diversity within academic\nand professional computing fields and the type and breadth of the biases\nencountered in datasets, machine learning models, problem formulations, and\ninterpretation of results. Examining the current fairness\/ethics in AI\nliterature, we highlight cases where this lack of diverse perspectives has been\nfoundational to the inequity in treatment of underrepresented and protected\ngroup data. We also look to other professional communities, such as in law and\nhealth, where disparities have been reduced both in the educational diversity\nof trainees and among professional practices. We use these lessons to develop\nrecommendations that provide concrete steps for the computing community to\nincrease diversity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.05652,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Functionally Effective Conscious AI Without Suffering\n\n  Insofar as consciousness has a functional role in facilitating learning and\nbehavioral control, the builders of autonomous AI systems are likely to attempt\nto incorporate it into their designs. The extensive literature on the ethics of\nAI is concerned with ensuring that AI systems, and especially autonomous\nconscious ones, behave ethically. In contrast, our focus here is on the rarely\ndiscussed complementary aspect of engineering conscious AI: how to avoid\ncondemning such systems, for whose creation we would be solely responsible, to\nunavoidable suffering brought about by phenomenal self-consciousness. We\noutline two complementary approaches to this problem, one motivated by a\nphilosophical analysis of the phenomenal self, and the other by certain\ncomputational concepts in reinforcement learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11619,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Election in India: Polling in National Financial Switch\n\n  Indian voters from Kashmir to Kanyakumari select their representatives to\nform their parliament by going to polls. India's election is one of the largest\ndemocratic exercise in the world history. About 850 million eligible voters\ndetermine which political party or alliance will form the government and in\nturn, will serve as prime minister. Given the electoral rules of placing a\npolling place within 2 kilometers of every habitation, it comes as no surprise\nthat is indeed a humongous task for the Election Commission of India (ECI). It\nsends around 11 million election workers through tough terrains to reach the\nlast mile. This exercise also comes as ever growing expenditure for the ECI.\nThis paper proposes the use of Automated Teller Machines (ATM) and Point Of\nSale (POS) machines to be used to cover as much as urban, rural and semi-urban\nplaces possible given the wide network of National Financial Switch (NFS) and\nincrease in connectivity through Digital India initiative. This would add to\nthe use of the existing infrastructure to accommodate a free, fair and\ntransparent election.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.05679,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Ethics of Food Recommender Applications\n\n  The recent unprecedented popularity of food recommender applications has\nraised several issues related to the ethical, societal and legal implications\nof relying on these applications. In this paper, in order to assess the\nrelevant ethical issues, we rely on the emerging principles across the\nAI\\&Ethics community and define them tailored context specifically. Considering\nthe popular Food Recommender Systems (henceforth F-RS) in the European market\ncannot be regarded as personalised F-RS, we show how merely this lack of\nfeature shifts the relevance of the focal ethical concerns. We identify the\nmajor challenges and propose a scheme for how explicit ethical agendas should\nbe explained. We also argue how a multi-stakeholder approach is indispensable\nto ensure producing long-term benefits for all stakeholders. After proposing\neight ethical desiderata points for F-RS, we present a case-study and assess it\nbased on our proposed desiderata points.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0769,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"A Unified Architecture for Data-Driven Metadata Tagging of Building\n  Automation Systems\n\n  This article presents a Unified Architecture for automated point tagging of\nBuilding Automation System data, based on a combination of data-driven\napproaches. Advanced energy analytics applications-including fault detection\nand diagnostics and supervisory control-have emerged as a significant\nopportunity for improving the performance of our built environment. Effective\napplication of these analytics depends on harnessing structured data from the\nvarious building control and monitoring systems, but typical Building\nAutomation System implementations do not employ any standardized metadata\nschema. While standards such as Project Haystack and Brick Schema have been\ndeveloped to address this issue, the process of structuring the data, i.e.,\ntagging the points to apply a standard metadata schema, has, to date, been a\nmanual process. This process is typically costly, labor-intensive, and\nerror-prone. In this work we address this gap by proposing a UA that automates\nthe process of point tagging by leveraging the data accessible through\nconnection to the BAS, including time series data and the raw point names. The\nUA intertwines supervised classification and unsupervised clustering techniques\nfrom machine learning and leverages both their deterministic and probabilistic\noutputs to inform the point tagging process. Furthermore, we extend the UA to\nembed additional input and output data-processing modules that are designed to\naddress the challenges associated with the real-time deployment of this\nautomation solution. We test the UA on two datasets for real-life buildings: 1.\ncommercial retail buildings and 2. office buildings from the National Renewable\nEnergy Laboratory campus. The proposed methodology correctly applied 85-90\npercent and 70-75 percent of the tags in each of these test scenarios,\nrespectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00213,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Machine Ethics: The Creation of a Virtuous Machine\n\n  Artificial intelligence (AI) was initially developed as an implicit moral\nagent to solve simple and clearly defined tasks where all options are\npredictable. However, it is now part of our daily life powering cell phones,\ncameras, watches, thermostats, vacuums, cars, and much more. This has raised\nnumerous concerns and some scholars and practitioners stress the dangers of AI\nand argue against its development as moral agents that can reason about ethics\n(e.g., Bryson 2008; Johnson and Miller 2008; Sharkey 2017; Tonkens 2009; van\nWynsberghe and Robbins 2019). Even though we acknowledge the potential threat,\nin line with most other scholars (e.g., Anderson and Anderson 2010; Moor 2006;\nScheutz 2016; Wallach 2010), we argue that AI advancements cannot be stopped\nand developers need to prepare AI to sustain explicit moral agents and face\nethical dilemmas in complex and morally salient environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0306,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Chinese E-Romance: Analyzing and Visualizing 7.92 Million Alibaba\n  Valentine's Day Purchases\n\n  The days that precede Valentine's Day are characterized by extensive gift\nshopping activities all across the globe. In China, where much shopping takes\nplace online, there has been an explosive growth in e-commerce sales during\nValentine's Day over the recent years. This exploratory study investigates the\nextent to which each product category and each shopper group can exhibit\nromantic love within China's e-market throughout the 2 weeks leading up to 2019\nValentine's Day. Massive data from Alibaba, the biggest e-commerce retailer\nworldwide, are utilized to formulate an innovative romance index (RI) to\nquantitatively measure e-romantic values for products and shoppers. On this\nbasis, millions of shoppers, along with their millions of products purchased\naround Valentine's Day, are analyzed as a case study to demonstrate their love\nconsumption and romantic gift-giving. The results of the analysis are then\nillustrated to help understand Chinese e-romance based on the perspectives of\ndifferent product categories and shopper groups. This empirical information\nvisualization also contributes to improving the segmentation, targeting, and\npositioning of China's e-market for Valentine's Day.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.08035,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000042717,
      "text":"A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous\n  Algorithmic Scores\n\n  The increased use of algorithmic predictions in sensitive domains has been\naccompanied by both enthusiasm and concern. To understand the opportunities and\nrisks of these technologies, it is key to study how experts alter their\ndecisions when using such tools. In this paper, we study the adoption of an\nalgorithmic tool used to assist child maltreatment hotline screening decisions.\nWe focus on the question: Are humans capable of identifying cases in which the\nmachine is wrong, and of overriding those recommendations? We first show that\nhumans do alter their behavior when the tool is deployed. Then, we show that\nhumans are less likely to adhere to the machine's recommendation when the score\ndisplayed is an incorrect estimate of risk, even when overriding the\nrecommendation requires supervisory approval. These results highlight the risks\nof full automation and the importance of designing decision pipelines that\nprovide humans with autonomy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0685,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"The Minimum Hybrid Contract (MHC): Combining legal and blockchain smart\n  contracts\n\n  Corruption is a major global financial problem with billions of dollars\nrendered lost or unaccountable annually. Corruption through contract fraud is\noften conducted by withholding and\/or altering financial information. When such\nscandals are investigated by authorities, financial and legal documents are\nusually altered to conceal the paper trail.\n  Smart contracts have emerged in recent years and appear promising for\napplications such as legal contracts where transparency is critical and of\npublic interest. Transparency and auditability are inherent because smart\ncontracts execute operations on the blockchain, a distributed public ledger.\n  In this paper, we propose the Minimum Hybrid Contract (MHC), with the aim of\nintroducing 1) auditability, 2) transparency, and 3) immutability to the\ncontract's financial transactions. The MHC comprises an online smart contract\nand an offline traditional legal contract. where the two are immutably linked.\n  Secure peer-to-peer financial transactions, transparency, and cost accounting\nare automated by the smart contract, and legal issues or disputes are carried\nout by civil courts. The reliance on established legal processes facilitates an\nappropriate adoption of smart contracts in traditional contracts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.055,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Understanding individual behaviour: from virtual to physical patterns\n\n  As \"Big Data\" has become pervasive, an increasing amount of research has\nconnected the dots between human behaviour in the offline and online worlds.\nConsequently, researchers have exploited these new findings to create models\nthat better predict different aspects of human life and recommend future\nbehaviour. To date, however, we do not yet fully understand the similarities\nand differences of human behaviour in these virtual and physical worlds. Here,\nwe analyse and discuss the mobility and application usage of 400,000\nindividuals over eight months. We find an astonishing similarity between\npeople's mobility in the physical space and how they move from app to app in\nsmartphones. Our data shows that individuals use and visit a finite number of\napps and places, but they keep exploring over time. In particular, two distinct\nprofiles of individuals emerge: those that keep changing places and services,\nand those that are stable over time, named as \"explorers\" and \"keepers\". We see\nthese findings as crucial to enrich a discussion for the potentials and the\nchallenges of building human-centric AI systems, which might leverage recent\nresults in Computational Social Science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.00326,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Unsafe At Any Level: NHTSA's levels of automation are a liability for\n  autonomous vehicle design and regulation\n\n  Walter Huang, a 38-year-old Apple Inc. engineer, died on March 23, 2018,\nafter his Tesla Model X crashed into a highway barrier in Mountain View,\nCalifornia. Tesla immediately disavowed responsibility for the accident. \"The\nfundamental premise of both moral and legal liability is a broken promise, and\nthere was none here: [Mr. Huang] was well aware that the Autopilot was not\nperfect [and the] only way for this accident to have occurred is if Mr. Huang\nwas not paying attention to the road, despite the car providing multiple\nwarnings to do so.\" This is the standard response from Tesla and Uber, the\nmanufacturers of the automated vehicles involved in the six fatal accidents to\ndate: the automated vehicle isn't perfect, the driver knew it wasn't perfect,\nand if only the driver had been paying attention and heeded the vehicle's\nwarnings, the accident would never have occurred.\n  However, as researchers focused on human-automation interaction in aviation\nand military operations, we cannot help but wonder if there really are no\nbroken promises and no legal liabilities. Science has a critical role in\ndetermining legal liability, and courts appropriately rely on scientists and\nengineers to determine whether an accident, or harm, was foreseeable.\nSpecifically, a designer could be found liable if, at the time of the accident,\nscientists knew there was a systematic relationship between the accident and\nthe designer's untaken precaution.\n  Nearly 70 years of research provides an undeniable answer: It is\ninsufficient, inappropriate, and dangerous to automate everything you can and\nleave the rest to the human. There is a systematic relationship between the\ndesign of automated vehicles and the types of accidents that are occurring now\nand will inevitably continue to occur in the future. These accidents were not\nunforeseeable and the drivers were not exclusively to blame.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.11672,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Towards Digital Engineering -- The Advent of Digital Systems Engineering\n\n  Digital Engineering, the digital transformation of engineering to leverage\ndigital technologies, is coming globally. This paper explores digital systems\nengineering, which aims at developing theory, methods, models, and tools to\nsupport the emerging digital engineering. A critical task is to digitalize\nengineering artifacts, thus enabling information sharing across platform,\nacross life cycle, and across domains. We identify significant challenges and\nenabling digital technologies; analyze the transition from traditional\nengineering to digital engineering; define core concepts, including\n\"digitalization\", \"unique identification\", \"digitalized artifacts\", \"digital\naugmentation\", and others; present a big picture of digital systems engineering\nin four levels: vision, strategy, action, and foundation; briefly discuss each\nof main areas of research issues. Digitalization enables fast infusing and\nleveraging novel digital technologies; unique identification enables\ninformation traceability and accountability in engineering lifecycle;\nprovenance enables tracing dependency relations among engineering artifacts;\nsupporting model reproducibility and replicability; helping with\ntrustworthiness evaluation of digital engineering artifacts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07687,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Augmented reality as a tool for open science platform by research\n  collaboration in virtual teams\n\n  The provision of open science is defined as a general policy aimed at\novercoming the barriers that hinder the implementation of the European Research\nArea (ERA). An open science foundation seeks to capture all the elements needed\nfor the functioning of ERA: research data, scientific instruments, ICT services\n(connections, calculations, platforms, and specific studies such as portals).\nManaging shared resources for the community of scholars maximizes the benefits\nto society. In the field of digital infrastructure, this has already\ndemonstrated great benefits. It is expected that applying this principle to an\nopen science process will improve management by funding organizations in\ncollaboration with stakeholders through mechanisms such as public consultation.\nThis will increase the perception of joint ownership of the infrastructure. It\nwill also create clear and non-discriminatory access rules, along with a sense\nof joint ownership that stimulates a higher level of participation,\ncollaboration and social reciprocity. The article deals with the concept of\nopen science. The concept of the European cloud of open science and its\nstructure are presented. According to the study, it has been shown that the\nstructure of the cloud of open science includes an augmented reality as an\nopen-science platform. An example of the practical application of this tool is\nthe general description of MaxWhere, developed by Hungarian scientists, and is\na platform of aggregates of individual 3D spaces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.12347,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Mobile phone data and COVID-19: Missing an opportunity?\n\n  This paper describes how mobile phone data can guide government and public\nhealth authorities in determining the best course of action to control the\nCOVID-19 pandemic and in assessing the effectiveness of control measures such\nas physical distancing. It identifies key gaps and reasons why this kind of\ndata is only scarcely used, although their value in similar epidemics has\nproven in a number of use cases. It presents ways to overcome these gaps and\nkey recommendations for urgent action, most notably the establishment of mixed\nexpert groups on national and regional level, and the inclusion and support of\ngovernments and public authorities early on. It is authored by a group of\nexperienced data scientists, epidemiologists, demographers and representatives\nof mobile network operators who jointly put their work at the service of the\nglobal effort to combat the COVID-19 pandemic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.11906,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Falling into the Echo Chamber: the Italian Vaccination Debate on Twitter\n\n  The reappearance of measles in the US and Europe, a disease considered\neliminated in early 2000s, has been accompanied by a growing debate on the\nmerits of vaccination on social media. In this study we examine the extent to\nwhich the vaccination debate on Twitter is conductive to potential outreach to\nthe vaccination hesitant. We focus on Italy, one of the countries most affected\nby the latest measles outbreaks. We discover that the vaccination skeptics, as\nwell as the advocates, reside in their own distinct \"echo chambers\". The\nstructure of these communities differs as well, with skeptics arranged in a\ntightly connected cluster, and advocates organizing themselves around few\nauthoritative hubs. At the center of these echo chambers we find the ardent\nsupporters, for which we build highly accurate network- and content-based\nclassifiers (attaining 95% cross-validated accuracy). Insights of this study\nprovide several avenues for potential future interventions, including\nnetwork-guided targeting, accounting for the political context, and monitoring\nof alternative sources of information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09501,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000146694,
      "text":"Novel Coronavirus COVID-19 Strike on Arab Countries and Territories: A\n  Situation Report I\n\n  The novel Coronavirus (COVID-19) is an infectious disease caused by a new\nvirus called COVID-19 or 2019-nCoV that first identified in Wuhan, China. The\ndisease causes respiratory illness (such as the flu) with other symptoms such\nas a cough, fever, and in more severe cases, difficulty breathing. This new\nCoronavirus seems to be very infectious and has spread quickly and globally. In\nthis work, information about COVID-19 is provided and the situation in Arab\ncountries and territories regarding the COVID-19 strike is presented. The next\nfew weeks main expectations is also given.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08006,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Forecasting Crime Using ARIMA Model\n\n  Data mining is the process in which we extract the different patterns and\nuseful Information from large dataset. According to London police, crimes are\nimmediately increases from beginning of 2017 in different borough of London. No\nuseful information is available for prevent crime on future basis. We forecasts\ncrime rates in London borough by extracting large dataset of crime in London\nand predicted number of crimes in future. We used time series ARIMA model for\nforecasting crimes in London. By giving 5 years of data to ARIMA model\nforecasting 2 years crime data. Comparatively, with exponential smoothing ARIMA\nmodel has higher fitting values. A real dataset of crimes reported by London\npolice collected from its website and other resources. Our main concept is\ndivided into four parts. Data extraction (DE), data processing (DP) of\nunstructured data, visualizing model in IBM SPSS. DE extracts crime data from\nweb sources during 2012 for the 2016 year. DP integrates and reduces data and\ngive them predefined attributes. Crime prediction is analyzed by applying some\ncalculation, calculated their moving average, difference, and auto-regression.\nForecasted Model gives 80% correct values, which is formed to be an accurate\nmodel. This work helps for London police in decision-making against crime.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05719,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Undergraduate Student Research With Low Faculty Cost\n\n  Undergraduates are unlikely to even consider graduate research in Computer\nScience if they do not know what Computer Science research is. Many programs\naimed at introducing undergraduate to research are structured like graduate\nresearch programs, with a small number of undergraduates working with a faculty\nadvisor. Further, females, under-represented minorities, and first generation\nstudents may be too intimidated or the idea of research may be too amorphous,\nso that they miss out on these programs. As a consequence, we lose out on\nopportunities for greater diversity in CS research. We have started a pilot\nprogram in our department where a larger number of students (close to two\ndozen) work with a single faculty member as part of a research group focused on\nMachine Learning and related areas. The goal of this program is not to convince\nstudents to pursue a research career but rather to enable them to make a more\ninformed decision about what role they would like research to play in their\nfuture. In order to evaluate our approach, we elicited student experience via\ntwo anonymized exit surveys. Students report that they develop a better\nunderstanding of what research in Computer Science is. Their interest in\nresearch was increased as was their reported confidence in their ability to do\nresearch, although not all students wanted to further pursue computer science\nresearch opportunities. Given the reported experience of female students, this\nprogram can offer a starting point for greater diversity in CS research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.10222,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000162588,
      "text":"Proximity: a recipe to break the outbreak\n\n  We present a mobile app solution to help the containment of an epidemic\noutbreak by keeping track of possible infections in the incubation period. We\nconsider the particular case of an infection which primarily spreads among\npeople through proximal contact, via respiratory droplets. This smartphone\napplication will work offline and will be able to detect other devices in close\nproximity and list all the interactions in an anonymous and encrypted way. If\nan app user is tested positive and so is certified as infected, the application\nnotifies immediately the potential contagion to the devices in the list and\nsuggests to start a voluntary quarantine and undergo a medical test. We believe\nthis solution may be useful in particular in the current COVID-19 pandemic and\nmoreover could be used to prevent similar events in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.11746,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Beyond STEM, How Can Women Engage Big Data, Analytics, Robotics and\n  Artificial Intelligence? An Exploratory Analysis of Confidence and\n  Educational Factors in the Emerging Technology Waves Influencing the Role of,\n  and Impact Upon, Women\n\n  In spite of the rapidly advancing global technological environment, the\nprofessional participation of women in technology, big data, analytics,\nartificial intelligence and information systems related domains remains\nproportionately low. Furthermore, it is of no less concern that the number of\nwomen in leadership in these domains are in even lower proportions. In spite of\nnumerous initiatives to improve the participation of women in technological\ndomains, there is an increasing need to gain additional insights into this\nphenomenon especially since it occurs in nations and geographies which have\nseen a sharp rise in overall female education, without such increase\ntranslating into a corresponding spurt in information systems and technological\nroles for women. The present paper presents findings from an exploratory\nanalysis and outlines a framework to gain insights into educational factors in\nthe emerging technology waves influencing the role of, and impact upon, women.\nWe specifically identify ways for learning and self-efficacy as key factors,\nwhich together lead us to the Advancement of Women in Technology (AWT) insights\nframework. Based on the AWT framework, we also proposition principles that can\nbe used to encourage higher professional engagement of women in emerging and\nadvanced technologies. Key Words- Women's Education, Technology, Artificial\nIntelligence, Knowing, Confidence, Self-Efficacy, Learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.04155,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"The Residence History Inference Problem\n\n  The use of online user traces for studies of human mobility has received\nsignificant attention in recent years. This growing body of work, and the more\ngeneral importance of human migration patterns to government and industry,\nmotivates the need for a formalized approach to the computational modeling of\nhuman mobility - in particular how and when individuals change their place of\nresidence - from online traces. Prior work on this topic has skirted the\nunderlying computational modeling of residence inference, focusing on migration\npatterns themselves. As a result, to our knowledge, all prior work has employed\nheuristics to compute something like residence histories. Here, we formalize\nthe residence assignment problem, which seeks, under constraints associated\nwith the minimum length-of-stay at a residence, the most parsimonious sequence\nof residence periods and places that explains the movement history of an\nindividual. Here we provide an exact solution for this problem and establish\nits algorithmic complexity. Because the calculation of optimal residence\nhistories (under the assumptions of the model) is tractable, we believe that\nthis method will be a valuable tool for future work on this topic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08512,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Empirical Characterization of Mobility of Multi-Device Internet Users\n\n  Understanding the mobility of humans and their devices is a fundamental\nproblem in mobile computing. While there has been much work on empirical\nanalysis of human mobility using mobile device data, prior work has largely\nassumed devices to be independent and has not considered the implications of\nmodern Internet users owning multiple mobile devices that exhibit correlated\nmobility patterns. Also, prior work has analyzed mobility at the spatial scale\nof the underlying mobile dataset and has not analyzed mobility characteristics\nat different spatial scales and its implications on system design. In this\npaper, we empirically analyze the mobility of modern Internet users owning\nmultiple devices at multiple spatial scales using a large campus WiFi dataset.\nFirst, our results show that mobility of multiple devices belonging to a user\nneeds to be analyzed and modeled as a group, rather than independently, and\nthat there are substantial differences in the correlations exhibited by device\ntrajectories across users that also need to be considered. Second, our analysis\nshows that the mobility of users shows different characteristics at different\nspatial scales such as within and across buildings. Third, we demonstrate the\nimplications of these results by presenting generative models that highlight\nthe importance of considering the spatial scale of mobility as well as\nmulti-device mobility. More broadly, our empirical results point to the need\nfor new modeling research to fully capture the nuances of mobility of modern\nmulti-device users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07703,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Flexible and Context-Specific AI Explainability: A Multidisciplinary\n  Approach\n\n  The recent enthusiasm for artificial intelligence (AI) is due principally to\nadvances in deep learning. Deep learning methods are remarkably accurate, but\nalso opaque, which limits their potential use in safety-critical applications.\nTo achieve trust and accountability, designers and operators of machine\nlearning algorithms must be able to explain the inner workings, the results and\nthe causes of failures of algorithms to users, regulators, and citizens. The\noriginality of this paper is to combine technical, legal and economic aspects\nof explainability to develop a framework for defining the \"right\" level of\nexplain-ability in a given context. We propose three logical steps: First,\ndefine the main contextual factors, such as who the audience of the explanation\nis, the operational context, the level of harm that the system could cause, and\nthe legal\/regulatory framework. This step will help characterize the\noperational and legal needs for explanation, and the corresponding social\nbenefits. Second, examine the technical tools available, including post hoc\napproaches (input perturbation, saliency maps...) and hybrid AI approaches.\nThird, as function of the first two steps, choose the right levels of global\nand local explanation outputs, taking into the account the costs involved. We\nidentify seven kinds of costs and emphasize that explanations are socially\nuseful only when total social benefits exceed costs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07674,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"Risk Management Practices in Information Security: Exploring the Status\n  Quo in the DACH Region\n\n  Information security management aims at ensuring proper protection of\ninformation values and information processing systems (i.e. assets).\nInformation security risk management techniques are incorporated to deal with\nthreats and vulnerabilities that impose risks to information security\nproperties of these assets. This paper investigates the current state of risk\nmanagement practices being used in information security management in the DACH\nregion (Germany, Austria, Switzerland). We used an anonymous online survey\ntargeting strategic and operative information security and risk managers and\ncollected data from 26 organizations. We analyzed general practices,\ndocumentation artifacts, patterns of stakeholder collaboration as well as tool\ntypes and data sources used by enterprises to conduct information security\nmanagement activities. Our findings show that the state of practice of\ninformation security risk management is in need of improvement. Current\nindustrial practice heavily relies on manual data collection and complex\npotentially subjective decision processes with multiple stakeholders involved.\nDedicated risk management tools and methods are used selectively and neglected\nin favor of general-purpose documentation tools and direct communication\nbetween stakeholders. In light of our results we propose guidelines for the\ndevelopment of risk management practices that are better aligned with the\ncurrent operational situation in information security management.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.11157,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"AI loyalty: A New Paradigm for Aligning Stakeholder Interests\n\n  When we consult with a doctor, lawyer, or financial advisor, we generally\nassume that they are acting in our best interests. But what should we assume\nwhen it is an artificial intelligence (AI) system that is acting on our behalf?\nEarly examples of AI assistants like Alexa, Siri, Google, and Cortana already\nserve as a key interface between consumers and information on the web, and\nusers routinely rely upon AI-driven systems like these to take automated\nactions or provide information. Superficially, such systems may appear to be\nacting according to user interests. However, many AI systems are designed with\nembedded conflicts of interests, acting in ways that subtly benefit their\ncreators (or funders) at the expense of users. To address this problem, in this\npaper we introduce the concept of AI loyalty. AI systems are loyal to the\ndegree that they are designed to minimize, and make transparent, conflicts of\ninterest, and to act in ways that prioritize the interests of users. Properly\ndesigned, such systems could have considerable functional and competitive - not\nto mention ethical - advantages relative to those that do not. Loyal AI\nproducts hold an obvious appeal for the end-user and could serve to promote the\nalignment of the long-term interests of AI developers and customers. To this\nend, we suggest criteria for assessing whether an AI system is sufficiently\ntransparent about conflicts of interest, and acting in a manner that is loyal\nto the user, and argue that AI loyalty should be considered during the\ntechnological design process alongside other important values in AI ethics such\nas fairness, accountability privacy, and equity. We discuss a range of\nmechanisms, from pure market forces to strong regulatory frameworks, that could\nsupport incorporation of AI loyalty into a variety of future AI systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07049,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Evolution of diversity and dominance of companies in online activity\n\n  Ever since the web began, the number of websites has been growing\nexponentially. These websites cover an ever-increasing range of online services\nthat fill a variety of social and economic functions across a growing range of\nindustries. Yet the networked nature of the web, combined with the economics of\npreferential attachment, increasing returns and global trade, suggest that over\nthe long run a small number of competitive giants are likely to dominate each\nfunctional market segment, such as search, retail and social media. Here we\nperform a large scale longitudinal study to quantify the distribution of\nattention given in the online environment to competing organisations. In two\nlarge online social media datasets, containing more than 10 billion posts and\nspanning more than a decade, we tally the volume of external links posted\ntowards the organisations' main domain name as a proxy for the online attention\nthey receive. We also use the Common Crawl dataset -- which contains the\nlinkage patterns between more than a billion different websites -- to study the\npatterns of link concentration over the past three years across the entire web.\nLastly, we showcase the linking between economic, financial and market data by\nexploring the relationships between online attention on social media and the\ngrowth in enterprise value in the electric carmaker Tesla. Our analysis shows\nthat despite the fact that we observe consistent growth in all the macro\nindicators -- the total amount of online attention, in the number of\norganisations with an online presence, and in the functions they perform -- we\nalso observe that a smaller number of organisations account for an\never-increasing proportion of total user attention, usually with one large\nplayer dominating each function. These results highlight how evolution of the\nonline economy involves innovation, diversity, and then competitive dominance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.12143,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Coronavirus Geographic Dissemination at Chicago and its Potential\n  Proximity to Public Commuter Rail\n\n  The community spread of coronavirus at great Chicago area has severely\nthreatened the residents health, family and normal activities. CDC daily\nupdates on infected cases on County level are not satisfying to address publics\nconcern on virus spread. On March 20th, NBC5 published case information of 435\ncoronavirus infections. The data is relative comprehensive and of high value\nfor understanding on the virus spread patterns at Chicago. Data engineering,\nnatural language processing and Google map technology are applied to organize\nthe data and retrieve geographic information of the virus. The analysis shows\ncommunity spread in Chicago areas has a potential proximity relation with\npublic commuter rail. Residents nearby major public commuter rails need limit\noutdoor activities during the outbreak and even the post-peak time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07672,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Urban Traffic Monitoring and Modeling System: An IoT Solution for\n  Enhancing Road Safety\n\n  Qatar expects more than a million visitors during the 2022 World Cup, which\nwill pose significant challenges. The high number of people will likely cause a\nrise in road traffic congestion, vehicle crashes, injuries and deaths. To\ntackle this problem, Naturalistic Driver Behavior can be utilised which will\ncollect and analyze data to estimate the current Qatar traffic system,\nincluding traffic data infrastructure, safety planning, and engineering\npractices and standards. In this paper, an IoT based solution to facilitate\nsuch a study in Qatar is proposed. Different data points from a driver are\ncollected and recorded in an unobtrusive manner, such as trip data, GPS\ncoordinates, compass heading, minimum, average, and maximum speed and his\ndriving behavior, including driver's drowsiness level. Analysis of these data\npoints will help in prediction of crashes and road infrastructure improvements\nto reduce such events. It will also be used for drivers risk assessment and to\ndetect extreme road user behaviors. A framework that will help to visualize and\nmanage this data is also proposed, along with a Deep Learning-based application\nthat detects drowsy driving behavior that netted an 82 percent accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000061923,
      "text":"Transforming Commercial Contracts through Computable Contracting\n\n  Contracts are an essential and fundamental component of commerce and society,\nserving to clarify agreement between multiple parties. While digital\ntechnologies have helped to automate many activities associated with\ncontracting, the contracts themselves continue, in the main, to be in the form\nof unstructured, natural-language text. This limits the scope for improvements\nin productivity and automation, as well as the emergence of new business\nmodels. To this end, this paper examines the concept of computable contracts as\nobjects that are understandable by both humans and computers, and goes on to\npresent a framework that unifies a range of technologies and approaches that\ncollectively will help to make computable contracting a reality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0572,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Cyber Security Incident Handling, Warning and Response System for the\n  European Critical Information Infrastructures (CyberSANE)\n\n  This paper aims to enhance the security and resilience of Critical\nInformation Infrastructures (CIIs) by providing a dynamic collaborative,\nwarning and response system (CyberSANE system) supporting and guiding security\nofficers and operators (e.g. Incident Response professionals) to recognize,\nidentify, dynamically analyse, forecast, treat and respond to their threats and\nrisks and handle their daily cyber incidents. The proposed solution provides a\nfirst of a kind approach for handling cyber security incidents in the digital\nenvironments with highly interconnected, complex and diverse nature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.12132,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"On the Emerging Area of Biocybersecurity and Relevant Considerations\n\n  Biocybersecurity is a novel space for the 21st century that meets our\ninnovations in biotechnology and computing head on. Within this space, many\nconsiderations are open for and demand consideration as groups endeavor to\ndevelop products and policies that adequately ensure asset management and\nprotection. Herein, simplified and brief exploration is given followed by some\nsurface discussion of impacts. These impacts concern the end user, ethical and\nlegal considerations, international proceedings, business, and limitations. It\nis hoped that this will be helpful in future considerations towards\nbiocybersecurity policy developments and implementations.\n  Notice: This article has been queued for publication in the Proceedings of\nthe 2020 Future of Information and Communication Conference (FICC)\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.07671,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Chemotaxis and Quorum Sensing inspired Device Interaction supporting\n  Social Networking\n\n  Conference and social events provides an opportunity for people to interact\nand develop formal contacts with various groups of individuals. In this paper,\nwe propose an efficient interaction mechanism in a pervasive computing\nenvironment that provide recommendation to users of suitable locations within a\nconference or expo hall to meet and interact with individuals of similar\ninterests. The proposed solution is based on evaluation of context information\nto deduce each user's interests as well as bioinspired self-organisation\nmechanism to direct users towards appropriate locations.Simulation results have\nalso been provided to validate our proposed solution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08444,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of\n  Misinformation in News Articles\n\n  In this paper, we present an updated version of the NELA-GT-2018 dataset\n(N{\\o}rregaard, Horne, and Adal{\\i} 2019), entitled NELA-GT-2019. NELA-GT-2019\ncontains 1.12M news articles from 260 sources collected between January 1st\n2019 and December 31st 2019. Just as with NELA-GT-2018, these sources come from\na wide range of mainstream news sources and alternative news sources. Included\nwith the dataset are source-level ground truth labels from 7 different\nassessment sites covering multiple dimensions of veracity. The NELA-GT-2019\ndataset can be found at: https:\/\/doi.org\/10.7910\/DVN\/O7FWPO\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.07213,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable\n  Claims\n\n  With the recent wave of progress in artificial intelligence (AI) has come a\ngrowing awareness of the large-scale impacts of AI systems, and recognition\nthat existing regulations and norms in industry and academia are insufficient\nto ensure responsible AI development. In order for AI developers to earn trust\nfrom system users, customers, civil society, governments, and other\nstakeholders that they are building AI responsibly, they will need to make\nverifiable claims to which they can be held accountable. Those outside of a\ngiven organization also need effective means of scrutinizing such claims. This\nreport suggests various steps that different stakeholders can take to improve\nthe verifiability of claims made about AI systems and their associated\ndevelopment processes, with a focus on providing evidence about the safety,\nsecurity, fairness, and privacy protection of AI systems. We analyze ten\nmechanisms for this purpose--spanning institutions, software, and hardware--and\nmake recommendations aimed at implementing, exploring, or improving those\nmechanisms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14748,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000096361,
      "text":"Observed mobility behavior data reveal social distancing inertia\n\n  The research team has utilized an integrated dataset, consisting of\nanonymized location data, COVID-19 case data, and census population\ninformation, to study the impact of COVID-19 on human mobility. The study\nrevealed that statistics related to social distancing, namely trip rate, miles\ntraveled per person, and percentage of population staying at home have all\nshowed an unexpected trend, which we named social distancing inertia. The\ntrends showed that as soon as COVID-19 cases were observed, the statistics\nstarted improving, regardless of government actions. This suggests that a\nportion of population who could and were willing to practice social distancing\nvoluntarily and naturally reacted to the emergence of COVID-19 cases. However,\nafter about two weeks, the statistics saturated and stopped improving, despite\nthe continuous rise in COVID-19 cases. The study suggests that there is a\nnatural behavior inertia toward social distancing, which puts a limit on the\nextent of improvement in the social-distancing-related statistics. The national\ndata showed that the inertia phenomenon is universal, happening in all the U.S.\nstates and for all the studied statistics. The U.S. states showed a\nsynchronized trend, regardless of the timeline of their statewide COVID-19 case\nspreads or government orders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.09246,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Enabling and Enforcing Social Distancing Measures using Smart City and\n  ITS Infrastructures: A COVID-19 Use Case\n\n  Internet of Things is a revolutionary domain that has the caliber to impact\nour lives and bring significant changes to the world. Several IoT applications\nhave been envisioned to facilitate data driven and smart application for the\nuser. Smart City and Intelligent Transportation System (ITS) offer a futuristic\nvision of smart, secure and safe experience to the end user, and at the same\ntime efficiently manage the sparse resources and optimize the efficiency of\ncity operations. However, outbreaks and pandemics like COVID-19 have revealed\nlimitations of the existing deployments, therefore, architecture, applications\nand technology systems need to be developed for swift and timely enforcement of\nguidelines, rules and government orders to contain such future outbreaks. This\nwork outlines novel architecture, potential use-cases and some future\ndirections in developing such applications using Smart City and ITS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.00491,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Beyond privacy regulations: an ethical approach to data usage in\n  transportation\n\n  With the exponential advancement of business technology in recent years,\ndata-driven decision making has become the core of most industries. With the\nrise of new privacy regulations such as the General Data Protection Regulation\nin the European Union and the California Consumer Privacy Act in the United\nStates, companies dealing with personal data had to conform to these changes\nand adapt their processes accordingly. This obviously included the\ntransportation industry with their use of location data. At the other side of\nthe spectrum, users still expect a form of personalization, without having to\ncompromise on their privacy. For this reason, companies across the industries\nstarted applying privacy-enhancing or preserving technologies at scale in their\nproducts as a competitive advantage. In this paper, we describe how Federated\nMachine Learning can be applied to the transportation sector. We present\nuse-cases for which Federated Learning is beneficial in transportation and the\nnew product lifecycle that is required for using such a technology. We see\nFederated Learning as a method that enables us to process privacy-sensitive\ndata, while respecting customer's privacy and one that guides us beyond\nprivacy-regulations and into the world of ethical data-usage.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.11594,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Predicted by Orwell: A discourse on the gradual shift in electronic\n  surveillance law\n\n  At some point in the history of most nations, one or more events of illegal\nelectronic surveillance by those in power or law enforcement has occurred that\nhas the effect of setting State against Citizen. The media sensationalise these\nincidents for profit, however they more often correctly express the concern\nfelt by the general public. At these times politicians rise, either into fame\nor infamy, by proposing new legislation which the public is told will protect\nthem by from future incidents of illegal and unwarranted invasion by officers\nof the state. Two things have occurred since these protective laws were\nenacted; technological advancement that is claimed has frustrated legitimate\ninvestigation, and changes in the law that are ostensibly presented as\nintending to facilitate the prosecution or prevention of a publicly decried\noffence, like child pornography, but which in context deliver expanded powers\nto the State, effectively weakening the protections previously enacted.\n  This report looks at human rights legislation in three jurisdictions,\nstarting from a position of comparing and contrasting the protections that are\navailable from illegal search and seizure. By identifying legislative changes\nrelated to several forms of electronic surveillance and technology, and the\nsituations that led to them, we can locate the effective peak of protection and\ndiscuss the processes that have led to a gradual yet pervasive weakening of\nthose laws in all three nations.\n  We are regularly diverted by those in power towards disregarding the paranoia\nof the outliers who have been warning us with their purple prose that big\nbrother is watching. But if we focus on the effect of recent legislative\nchanges in the area of electronic surveillance we can clearly see that the\nOrwellian dystopia is already here, and we are living it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.11943,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"From elusive to ubiquitous: understanding smart cities\n\n  Converting the city into a \"smart\" one is the emerging strategy of\nalleviating the problems generated by the rapid population growth in most urban\nareas, i.e., urbanisation. However, as the rate in which the different concepts\nof the smart city architecture are implemented is very high, academic research\npertaining these advancements simply can not keep up. To improve the existing\nknowledge-base regarding the concept of smart cities, this paper reviews the\nexisting definitions, as well as its architectures, based on an in-depth\nliterature review of relevant studies and research fields alike. Additionally,\nwe also provide our own definition of the smart city concept.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.1376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Gender Diversity in Computer Science at a Large Research University\n\n  With the number of Computer Science (CS) jobs on the rise, there is a greater\nneed for Computer Science graduates than ever. At the same time, most CS\ndepartments across the country are only seeing 25 to 30 percent of female\nstudents in their classes, meaning that we are failing to draw interest from a\nlarge portion of the population. In this work, we explore the gender gap in CS\nat a large public research university, using three data sets that span\nthousands of students across 5 and a half academic years. By combining these\ndata sets, we can explore many issues such as retention as students progress\nthrough the CS major. For example, we find that a large percentage of women\ntaking the Introductory CS1 course for majors do not intend to major in CS,\nwhich contributes to a large increase in the gender gap immediately after CS1.\nThis finding implies that a large part of the retention task is attracting\nthese women to further explore the major. We report findings in three areas of\nresearch in the context of the CS department at our university: the CS\nenvironment, the computing background of our students, and the students'\ngrades. These findings may also be applicable to computing programs at other\nlarge public research universities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.01014,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"The Covid19Impact Survey: Assessing the Pulse of the COVID-19 Pandemic\n  in Spain via 24 questions\n\n  In this paper, we describe the results of analyzing a large-scale survey,\ncalled the Covid19Impact survey, to assess citizens feedback on four areas\nrelated to the COVID-19 pandemic in Spain: social contact behavior, financial\nimpact, working situation and health status. A total of 24 questions cover the\nareas of demographics, their home situation, social contact behavior, personal\neconomic impact, their workplace situation and their health. The survey was\nresponded to by 156,614 participants between the evening of March 28th and\nApril 2nd, 2020. Such a large response enables us to gain new insights, as well\nas an unprecedented glimpse at respondents personal experiences and concerns\nduring the current COVID-19 pandemic. From the analysis, we draw several\nimplications for the design of public policies related to the management of the\nCOVID-19 pandemic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14344,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000110269,
      "text":"Advancing computerized cognitive training for early Alzheimer's disease\n  in a Covid-19 pandemic and post-pandemic world\n\n  The COVID-19 pandemic has transformed mobile health applications and\ntelemedicine from nice to have tools into essential healthcare infrastructure.\nThis need is particularly great for the elderly who, due to their greater risk\nfor infection, may avoid medical facilities or be required to self-isolate.\nThese are also the very groups at highest risk for cognitive decline. For\nexample, during the COVID-19 pandemic artificially intelligent conversational\nagents were employed by hospitals and government agencies (such as the CDC) to\nfield queries from patients about symptoms and treatments. Digital health tools\nalso proved invaluable to provide neuropsychiatric and psychological self-help\nto people isolated at home or in retirement centers and nursing homes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.06983,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"The impact of Industry 4.0 technologies on production and supply chains\n\n  This paper sheds light on the current development in major industrialized\ncountries (such as Germany, Japan, and Switzerland): the trend towards\nhighly-integrated and autonomous production systems. The question is how such a\ntransition of a production infrastructure can take place most efficiently. This\nresearch uses the system dynamics method to address this complex transition\nprocess from a legacy production system to a modern and highly integrated\nproduction system (an Industry 4.0 system). The findings mainly relate to the\nidentification of system structures that are relevant for an Industry 4.0\nperspective. Our research is the first in its kind which presents a causal\nmodel that addresses the transition to Industry 4.0.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.10191,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Implementing AI Ethics in Practice: An Empirical Evaluation of the\n  RESOLVEDD Strategy\n\n  As Artificial Intelligence (AI) systems exert a growing influence on society,\nreal-life incidents begin to underline the importance of AI Ethics. Though\ncalls for more ethical AI systems have been voiced by scholars and the general\npublic alike, few empirical studies on the topic exist. Similarly, few tools\nand methods designed for implementing AI ethics into practice currently exist.\nTo provide empirical data into this on-going discussion, we empirically\nevaluate an existing method from the field of business ethics, the RESOLVEDD\nstrategy, in the context of ethical system development. We evaluated RESOLVEDD\nby means of a multiple case study of five student projects where its use was\ngiven as one of the design requirements for the projects. One of our key\nfindings is that, even though the use of the ethical method was forced upon the\nparticipants, its utilization nonetheless facilitated of ethical consideration\nin the projects. Specifically, it resulted in the developers displaying more\nresponsibility, even though the use of the tool did not stem from intrinsic\nmotivation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03765,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Governance of the Internet of Things (IoT)\n\n  Today's increasing rate of technological change results from the rapid growth\nin computer processing speed, when combined with the cost decline of processing\ncapacity, and is of historical import. The daily life of billions of\nindividuals worldwide has been forever changed by technology in just the last\nfew years. Costly data breaches continue at an alarming rate. The challenge\nfacing humans as they attempt to govern the process of artificial intelligence,\nmachine learning, and the impact of billions of sensory devices connected to\nthe Internet is the subject of this Article.\n  We proceed in nine sections. First, we define the Internet of Things (IoT),\ncomment on the explosive growth in sensory devices connected to the Internet,\nprovide examples of IoT devices, and speak to the promise of the IoT. Second,\nwe discuss legal requirements for corporate governance as a foundation for\nconsidering the challenge of governing the IoT. Third, we look at potential IoT\nthreats. Fourth, we discuss the Mirai botnet. Fifth, is a look at the IoT\nthreat vector vulnerabilities during times of crisis. Sixth, we discuss the\nManufactured Usage Description (MUD) methodology. Seventh, is a discussion of\nrecent regulatory developments. Next, we look at a few recommendations. And\nfinally, we conclude. We believe this Article contributes to our understanding\nof the widespread exposure to malware associated with IoT and adds to the\nnascent but emerging literature on governance of enterprise risk, a subject of\nvital societal importance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.07081,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"A Berkeley View of Teaching CS at Scale\n\n  Over the past decade, undergraduate Computer Science (CS) programs across the\nnation have experienced an explosive growth in enrollment as computational\nskills have proven increasingly important across many domains and in the\nworkforce at large. Motivated by this unprecedented student demand, the CS\nprogram at the University of California, Berkeley has tripled the size of its\ngraduating class in five years. The first two introductory courses for majors,\neach taught by one faculty instructor and several hundred student teachers,\ncombine to serve nearly 2,900 students per term. This report presents three\nstrategies that have enabled the effective teaching, delivery, and management\nof large-scale CS courses: (1) the development of autograder infrastructure and\nonline platforms to provide instant feedback with minimal instructor\nintervention and deliver the course at scale; (2) the expansion of academic and\nsocial student support networks resulting from changes in teaching assistant\nresponsibilities and the development of several near-peer mentoring\ncommunities; and (3) the expansion of undergraduate teacher preparation\nprograms to meet the increased demand for qualified student teachers. These\ninterventions have helped both introductory and advanced courses address\ncapacity challenges and expand enrollments while receiving among the highest\nstudent evaluations of teaching in department history. Implications for\ninclusivity and diversity are discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.02784,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"Redesign of web-based exam for knowledge evaluation in Advanced\n  Mathematics for pharmaceutical students based on analysis of the results\n\n  The usage of the information technologies everywhere leads to demands for new\nmanners of education. Modern e-learning environments lead the teaching, the\nlearning and the evaluation of acquired knowledge and skills of the students to\na new era. The students' motivation for e-learning is considered. The course of\nAdvanced Mathematics is part of the curriculum of pharmaceutical students at\nthe Medical University - Plovdiv. For students' knowledge evaluation it is used\na hybrid-type exam in this university discipline, i.e. a problems-solving part\nand a remote web-based test which is created using the free and open-source\ne-educational platform Moodle. This paper presents a detailed analysis of the\nimplemented electronic test for knowledge evaluation of the students, using\nstatistical methods and instruments. The questions included in the test and the\nrespective answers given by the students are estimated and analysed. Thus, it\nis made an improvement of the database of the test questions. The received\nresults are used to enhance the quality of the developed knowledge evaluation\nand the type of its implementation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.11768,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000079804,
      "text":"Cyber Security Behaviour In Organisations\n\n  This review explores the academic and policy literature in the context of\neveryday cyber security in organisations. In so doing, it identifies four\nbehavioural sets that influences how people practice cyber security. These are\ncompliance with security policy, intergroup coordination and communication,\nphishing\/email behaviour, and password behaviour. However, it is important to\nnote that these are not exhaustive and they do not exist in isolation. In\naddition, the review explores the notion of security culture as an overarching\ntheme that overlaps and frames the four behavioural sets. The aim of this\nreview is therefore to provide a summary of the existing literature in the area\nof everyday cyber security within the social sciences, with a particular focus\non organisational contexts. In doing so, it develops a series of suggestions\nfor future research directions based on existing gaps in the literature. The\nreview also includes a theoretical lens that will aid the understanding of\nexisting studies and wider literatures. Where possible, the review makes\nrecommendations for organisations in relation to everyday cyber security.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05185,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Stochastic Multi-Agent-Based Model to Measure Community Resilience-Part\n  2: Simulation Results\n\n  In this paper we investigate the resiliency planning of interdependent\nelectric power systems and emergency services. We investigate the effect of the\nlevel of empathy, cooperation, coordination, flexibility, and experience of\nindividuals on their mental well-being. Furthermore, we explore the impact of\nthe information that is provided by emergency services and the impact of the\navailability of electric energy on the physical, mental, and social well-being\nof individuals. For our simulations, we use a stochastic, multi-agent-based\nnumerical framework that is reported in the companion paper for estimating the\nsocial well-being of a community when facing natural disasters such as\nhurricanes, floods, earthquakes, and tsunamis. The performance of the proposed\nmethod is assessed by measuring community resilience for a multitude of effects\nin the context of two case studies. These effects are analyzed for Gaussian\nsocial random characteristics. Each case study considers nine agents, namely,\nthree areas of three communities each, yielding a total of six communities. The\nresults show that a high level of cooperation can positively change individual\nbehavior. In addition, the relationship among the individuals of a community is\nso vital that the society with less population and more empathy may be more\nresilient than the community with more population and less empathy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.10383,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"A Data-driven Approach for Constructing Multilayer Network-based Service\n  Ecosystem Models\n\n  Services are flourishing drastically both on the Internet and in the real\nworld. Additionally, services have become much more interconnected to\nfacilitate transboundary business collaboration to create and deliver distinct\nnew values to customers. Various service ecosystems have become a focus in both\nresearch and practice. However, due to the lack of widely recognized service\necosystem models and sufficient data for constructing such models, existing\nstudies on service ecosystems are limited to very narrow scope and cannot\neffectively guide the design, optimization, and evolution of service\necosystems. We propose a Multilayer network-based Service Ecosystem Model,\nwhich covers a variety of service-related elements, including stakeholders,\nchannels, functional and nonfunctional features, and domains, and especially,\nstructural and evolutionary relations between them. \"Events\" are introduced to\ndescribe the triggers of service ecosystem evolution. We propose a data-driven\napproach for constructing MSEM from public media news and external data\nsources. Qualitative comparison with state-of-the-art models shows that MSEM\nhas a higher coverage degree of fine-grained elements\/relations in service\necosystems and richer semantics for higher interpretability. Experiments\nconducted on real news corpora show that compared with other approaches, our\napproach can construct large-scale models for real-world service ecosystems\nwith lower cost and higher efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.01084,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Patterns of population displacement during mega-fires in California\n  detected using Facebook Disaster Maps\n\n  Facebook Disaster Maps (FBDM) is the first platform providing analysis-ready\npopulation change products derived from crowdsourced data targeting disaster\nrelief practices. We evaluate the representativeness of FBDM data using the\nMann-Kendall test and emerging hot and cold spots in an anomaly analysis to\nreveal the trend, magnitude, and agglommeration of population displacement\nduring the Mendocino Complex and Woolsey fires in California, USA. Our results\nshow that the distribution of FBDM pre-crisis users fits well with the total\npopulation from different sources. Due to usage habits, the elder population is\nunderrepresented in FBDM data. During the two mega-fires in California, FBDM\ndata effectively captured the temporal change of population arising from the\nplacing and lifting of evacuation orders. Coupled with monotonic trends, the\nfall and rise of cold and hot spots of population revealed the areas with the\ngreatest population drop and potential places to house the displaced residents.\nA comparison between the Mendocino Complex and Woolsey fires indicates that a\ndensely populated region can be evacuated faster than a scarcely populated one,\npossibly due to the better access to transportation. In sparsely populated\nfire-prone areas, resources should be prioritized to move people to shelters as\nthe displaced residents do not have many alternative options, while their\ncounterparts in densely populated areas can utilize their social connections to\nseek temporary stay at nearby locations during an evacuation. Integrated with\nan assessment on underrepresented communities, FBDM data and the derivatives\ncan provide much needed information of near real-time population displacement\nfor crisis response and disaster relief. As applications and data generation\nmature, FBDM will harness crowdsourced data and aid first responder\ndecision-making.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.14484,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000073181,
      "text":"Prevalence of Low-Credibility Information on Twitter During the COVID-19\n  Outbreak\n\n  As the novel coronavirus spreads across the world, concerns regarding the\nspreading of misinformation about it are also growing. Here we estimate the\nprevalence of links to low-credibility information on Twitter during the\noutbreak, and the role of bots in spreading these links. We find that the\ncombined volume of tweets linking to low-credibility information is comparable\nto the volume of New York Times articles and CDC links. Content analysis\nreveals a politicization of the pandemic. The majority of this content spreads\nvia retweets. Social bots are involved in both posting and amplifying\nlow-credibility information, although the majority of volume is generated by\nlikely humans. Some of these accounts appear to amplify low-credibility sources\nin a coordinated fashion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.01392,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"Mobile social media usage and academic performance\n\n  Among the general population, students are especially sensitive to social\nmedia and smartphones because of their pervasiveness. Several studies have\nshown that there is a negative correlation between social media and academic\nperformance since they can lead to behaviors that hurt students' careers, e.g.,\naddictedness. However, these studies either focus on smartphones and social\nmedia addictedness or rely on surveys, which only provide approximate\nestimates. We propose to bridge this gap by i) parametrizing social media usage\nand academic performance, and ii) combining smartphones and time diaries to\nkeep track of users' activities and their smartphone interaction. We apply our\nsolution on the 72 students participating in the SmartUnitn project, which\ninvestigates students' time management and their academic performance. By\nanalyzing the logs of social media apps on students' smartphones and by\ncomparing them to students' credits and grades, we can provide a quantitative\nand qualitative estimate of negative and positive correlations. Our results\nshow the negative impact of social media usage, distinguishing different\ninfluence patterns of social media on academic activities and also underline\nthe need to control the smartphone usage in academic settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04947,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Adherence to Personal Health Devices: A Case Study in Diabetes\n  Management\n\n  Personal health devices can enable continuous monitoring of health\nparameters. However, the benefit of these devices is often directly related to\nthe frequency of use. Therefore, adherence to personal health devices is\ncritical. This paper takes a data mining approach to study continuous glucose\nmonitor use in diabetes management. We evaluate two independent datasets from a\ntotal of 44 subjects for 60 - 270 days. Our results show that: 1) missed target\ngoals (i.e. suboptimal outcomes) is a factor that is associated with wearing\nbehavior of personal health devices, and 2) longer duration of non-adherence,\nidentified through missing data or data gaps, is significantly associated with\npoorer outcomes. More specifically, we found that up to 33% of data gaps\noccurred when users were in abnormal blood glucose categories. The longest data\ngaps occurred in the most severe (i.e. very low \/ very high) glucose\ncategories. Additionally, subjects with poorly-controlled diabetes had longer\naverage data gap duration than subjects with well-controlled diabetes. This\nwork contributes to the literature on the design of context-aware systems that\ncan leverage data-driven approaches to understand factors that influence\nnon-wearing behavior. The results can also support targeted interventions to\nimprove health outcomes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.08669,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Translating the Concept of Goal Setting into Practice -- What 'Else'\n  does it Require than a Goal Setting Tool?\n\n  This conceptual paper reviews the current status of goal setting in the area\nof technology enhanced learning and education. Besides a brief literature\nreview, three current projects on goal setting are discussed. The paper shows\nthat the main barriers for goal setting applications in education are not\nrelated to the technology, the available data or analytical methods, but rather\nthe human factor. The most important bottlenecks are the lack of students goal\nsetting skills and abilities, and the current curriculum design, which,\nespecially in the observed higher education institutions, provides little\nsupport for goal setting interventions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.12409,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000377165,
      "text":"Digitalization of COVID-19 pandemic management and cyber risk from\n  connected systems\n\n  What makes cyber risks arising from connected systems challenging during the\nmanagement of a pandemic? Assuming that a variety of cyber-physical systems are\nalready operational-collecting, analyzing, and acting on data autonomously-what\nrisks might arise in their application to pandemic management? We already have\nthese systems operational, collecting, and analyzing data autonomously, so how\nwould a pandemic monitoring app be different or riskier? In this review\narticle, we discuss the digitalization of COVID-19 pandemic management and\ncyber risk from connected systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Employees Productivity Measurement and Control -- a Case of a National\n  University\n\n  An experimental study, that finds the impact of Internet access control on\nthe employees productivity in the National University. The purpose of the study\nis to boost the employee productivity through proper Internet access control.\nThe main objectives are to find the most used web categories by the staff, find\nif relation exists between productivity and none work related internet usage,\nand choose the best level of Internet access control. Before initiating the\nexperiment, Employees Internet usage was monitored and accordingly classified\nthem into the proper Internet access control groups. Then supervisors were\nasked for a pre-test productivity measures for their staff, after that the\nexperiment was initiated for 45 days. Then, a post-test productivity measure\nwas done. Productivity changes were analyzed with the department nature, its\nInternet usage portfolio and its current Internet access control group; then\nthe best level of restriction was found. The result showed that the\nproductivity of departments with low Internet usage was not affected by\nrestricting and none restricting Internet access. However, for high Internet\nusage departments noticeable productivity improvement was there when the\nInternet restriction policy was not affecting work-related websites; but when\nit was affecting work-related websites the productivity decreased.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.0393,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Encouraging early mastery of computational concepts through play\n\n  Learning to code, and more broadly, learning about computer science is a\ngrowing field of activity and research. Under the label of computational\nthinking, computational concepts are increasingly used as cognitive tools in\nmany subject areas, beyond computer science. Using playful approaches and\ngamification to motivate educational activities, and to encourage exploratory\nlearning is not a new idea since play has been involved in the learning of\ncomputational concepts by children from the very start. There is a tension\nhowever, between learning activities and opportunities that are completely open\nand playful, and learning activities that are structured enough to be easily\nreplicable among contexts, countries and classrooms. This paper describes the\nconception, refinement, design and evaluation of a set of playful computational\nactivities for classrooms or code clubs, that balance the benefits of\nplayfulness with sufficient rigor and structure to enable robust replication.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.02442,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Reliable and Efficient Long-Term Social Media Monitoring\n\n  Social media data is now widely used by many academic researchers. However,\nlong-term social media data collection projects, which most typically involve\ncollecting data from public-use APIs, often encounter issues when relying on\nlocal-area network servers (LANs) to collect high-volume streaming social media\ndata over long periods of time. In this technical report, we present a\ncloud-based data collection, pre-processing, and archiving infrastructure, and\nargue that this system mitigates or resolves the problems most typically\nencountered when running social media data collection projects on LANs at\nminimal cloud-computing costs. We show how this approach works in different\ncloud computing architectures, and how to adapt the method to collect streaming\ndata from other social media platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.14045,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"Scaling Participation -- What Does the Concept of Managed Communities\n  Offer for Participatory Design?\n\n  This paper investigates mechanisms for scaling participation in participatory\ndesign (PD). Specifically, the paper focuses on managed communities, one\nstrategy of generification work. We first give a brief introduction on the\nissue of scaling in PD, followed by exploring the strategy of managed\ncommunities in PD. This exploration is underlined by an ongoing case study in\nthe healthcare sector, and we propose solutions to observed challenges. The\npaper ends with a critical reflection on the possibilities managed communities\noffer for PD. Managed communities have much to offer beyond mere generification\nwork for large-scale information systems, but we need to pay attention to core\nPD values that are in danger of being sidelined in the process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.07465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Labour Market Information Driven, Personalized, OER Recommendation\n  System for Lifelong Learners\n\n  In this paper, we suggest a novel method to aid lifelong learners to access\nrelevant OER based learning content to master skills demanded on the labour\nmarket. Our software prototype 1) applies Text Classification and Text Mining\nmethods on vacancy announcements to decompose jobs into meaningful skills\ncomponents, which lifelong learners should target; and 2) creates a hybrid OER\nRecommender System to suggest personalized learning content for learners to\nprogress towards their skill targets. For the first evaluation of this\nprototype we focused on two job areas: Data Scientist, and Mechanical Engineer.\nWe applied our skill extractor approach and provided OER recommendations for\nlearners targeting these jobs. We conducted in-depth, semi-structured\ninterviews with 12 subject matter experts to learn how our prototype performs\nin terms of its objectives, logic, and contribution to learning. More than 150\nrecommendations were generated, and 76.9% of these recommendations were treated\nas useful by the interviewees. Interviews revealed that a personalized OER\nrecommender system, based on skills demanded by labour market, has the\npotential to improve the learning experience of lifelong learners.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.1214,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000159608,
      "text":"Usage Analysis of Mobile Devices\n\n  Mobile devices have evolved from just communication devices into an\nindispensable part of people's lives in form of smartphones, tablets and smart\nwatches. Devices are now more personal than ever and carry more information\nabout a person than any other. Extracting user behaviour is rather difficult\nand time-consuming as most of the work previously has been manual or requires\nfeature extraction. In this paper, a novel approach of user behavior detection\nis proposed with Deep Learning Network (DNN). Initial approach was to use\nrecurrent neural network (RNN) along with LSTM for completely unsupervised\nanalysis of mobile devices. Next approach is to extract features by using Long\nShort Term Memory (LSTM) to understand the user behaviour, which are then fed\ninto the Convolution Neural Network (CNN). This work mainly concentrates on\ndetection of user behaviour and anomaly detection for usage analysis of mobile\ndevices. Both the approaches are compared against some baseline methods.\nExperiments are conducted on the publicly available dataset to show that these\nmethods can successfully capture the user behaviors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.02489,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"The Pace and Pulse of the Fight against Coronavirus across the US, A\n  Google Trends Approach\n\n  The coronavirus pandemic is impacting our lives at unprecedented speed and\nscale - including how we eat and work, what we worry about, how much we move,\nand our ability to earn. Google Trends can be used as a proxy for what people\nare thinking, needing, and planning. We use it to provide both insights into,\nand potential indicators of, important changes in information-seeking patterns\nduring pandemics like COVID-19. Key questions we address are: (1) What is the\nrelationship between the coronavirus outbreak and internet searches related to\nhealthcare seeking, government support programs, media sources of different\nideologies, planning around social activities, travel, and food, and new\ncoronavirus-specific behaviors and concerns?; (2) How does the popularity of\nsearch terms differ across states and regions and can we explain these\ndifferences?; (3) Can we find distinct, tangible search patterns across states\nsuggestive of policy gaps to inform pandemic response? (4) Does Google Trends\ndata correlate with and potentially precede real-life events? We suggest\nstrategic shifts for policy makers to improve the precision and effectiveness\nof non-pharmaceutical interventions (NPIs) and recommend the development of a\nreal-time dashboard as a decision-making tool. Methods used include trend\nanalysis of US search data; geographic analyses of the differences in search\npopularity across US states during March 1st to April 15th, 2020; and Principal\nComponent Analyses (PCA) to extract search patterns across states.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.00048,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Design of Transformation Initiatives Implementing Organisational Agility\n  -- An Empirical Study\n\n  This study uses 125 responses from companies of all sizes headquartered in\nGermany, Switzerland, France and UK to reveal perceptions of the drivers of\norganisational agility. It further investigates current understanding of\nmanaging principles of multiple organisational dimensions such as culture,\nvalues, leadership, organisational structure, processes and others to achieve\ngreater organisational agility. The data set is disaggregated into four major\nprofiles of agile organisations: laggards, execution specialists,\nexperimenters, and leaders. The approach to agile transformation is analysed by\neach of those profiles. While the positive effect from a more holistic approach\nis confirmed, leaders tend to focus more on processes and products rather than\nproject work. Respondents perceive that IT, product development and research\nare most agile functions within their organisations, while human resources,\nfinance and administration are considered being not agile. Further,\norganisations with higher levels of organisational agility tend use more than\none agile scaling framework. Implications on theories of agile transformations\nand organisational design are discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.14147,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"IMDb data from Two Generations, from 1979 to 2019; Part one, Dataset\n  Introduction and Preliminary Analysis\n\n  \"IMDb\" as a user-regulating and one the most-visited portal has provided an\nopportunity to create an enormous database. Analysis of the information on\nInternet Movie Database - IMDb, either those related to the movie or provided\nby users would help to reveal the determinative factors in the route of success\nfor each movie. As the lack of a comprehensive dataset was felt, we determined\nto do create a compendious dataset for the later analysis using the statistical\nmethods and machine learning models; It comprises of various information\nprovided on IMDb such as rating data, genre, cast and crew, MPAA rating\ncertificate, parental guide details, related movie information, posters, etc,\nfor over 79k titles which is the largest dataset by this date. The present\npaper is the first paper in a series of papers aiming at the mentioned goals,\nby a description of the created dataset and a preliminary analysis including\nsome trend in data, demographic analysis of IMDb scores and their relation of\ngenre MPAA rating certificate has been investigated.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.01224,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Quantifying human mobility behavior changes in response to\n  non-pharmaceutical interventions during the COVID-19 outbreak in the United\n  States\n\n  Ever since the first case of the novel coronavirus disease (COVID-19) was\nconfirmed in Wuhan, China, social distancing has been promoted worldwide,\nincluding the United States. It is one of the major community mitigation\nstrategies, also known as non-pharmaceutical interventions. However, our\nunderstanding is remaining limited in how people practice social distancing. In\nthis study, we construct a Social Distancing Index (SDI) to evaluate people's\nmobility pattern changes along with the spread of COVID-19. We utilize an\nintegrated dataset of mobile device location data for the contiguous United\nStates plus Alaska and Hawaii over a 100-day period from January 1, 2020 to\nApril 9, 2020. The major findings are: 1) the declaration of the national\nemergency concerning the COVID-19 outbreak greatly encouraged social distancing\nand the mandatory stay-at-home orders in most states further strengthened the\npractice; 2) the states with more confirmed cases have taken more active and\ntimely responses in practicing social distancing; 3) people in the states with\nfewer confirmed cases did not pay much attention to maintaining social\ndistancing and some states, e.g., Wyoming, North Dakota, and Montana, already\nbegan to practice less social distancing despite the high increasing speed of\nconfirmed cases; 4) some counties with the highest infection rates are not\nperforming much social distancing, e.g., Randolph County and Dougherty County\nin Georgia, and some counties began to practice less social distancing right\nafter the increasing speed of confirmed cases went down, e.g., in Blaine\nCounty, Idaho, which may be dangerous as well.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06945,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"APPCorp: A Corpus for Android Privacy Policy Document Structure Analysis\n\n  With the increasing popularity of mobile devices and the wide adoption of\nmobile Apps, an increasing concern of privacy issues is raised. Privacy policy\nis identified as a proper medium to indicate the legal terms, such as GDPR, and\nto bind legal agreement between service providers and users. However, privacy\npolicies are usually long and vague for end users to read and understand. It is\nthus important to be able to automatically analyze the document structures of\nprivacy policies to assist user understanding. In this work we create a\nmanually labelled corpus containing $167$ privacy policies (of more than $447$K\nwords and $5,276$ annotated paragraphs). We report the annotation process and\ndetails of the annotated corpus. We also benchmark our data corpus with $4$\ndocument classification models, thoroughly analyze the results and discuss\nchallenges and opportunities for the research committee to use the corpus. We\nrelease our labelled corpus as well as the classification models for public\naccess.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.08618,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Cognitive Analysis of Security Threats on Social Networking Services:\n  Slovakia in need of stronger action\n\n  This short paper examines some of the ongoing research at the UMB Data and\nSociety Lab hosted at the Faculty of Political Science and International\nRelations at Matej Bel University. It begins with an introduction on the\nnecessity of security threat identification on social networking services\n(SNSs), done by states. The paper follows with a general overview of selected\nprojects of the Lab in this field, and afterwards it introduces a use case\nstudy focused on the announcement of the UK snap general election 2017. The\nmain aim of this paper is to demonstrate some of the possibilities of social\nnetworking services analysis in the field of international relations, with an\nemphasis on disinformation and the necessity of identifying novel digital\nactors in Slovakia. We also outline an easy custom system tasked to collect\nsocial media data, and afterwards process it using various cognitive analytic\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.11414,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"Data as Infrastructure for Smart Cities: Linking Data Platforms to\n  Business Strategies\n\n  The systems that operate the infrastructure of cities have evolved in a\nfragmented fashion across several generations of technology, causing city\nutilities and services to operate sub-optimally and limiting the creation of\nnew value-added services and restrict opportunities for cost-saving. The\nintegration of cross-domain city data offers a new wave of opportunities to\nmitigate some of these impacts and enables city systems to draw effectively on\ninteroperable data that will be used to deliver smarter cities. Despite the\nconsiderable potential of city data, current smart cities initiatives have\nmainly addressed the problem of data management from a technology perspective,\nand have disregarded stakeholders and data needs. As a consequence, such\ninitiatives are susceptible to failure from inadequate stakeholder input,\nrequirements neglecting, and information fragmentation and overload. They are\nalso likely to be limited in terms of both scalability and future proofing\nagainst technological, commercial and legislative change. This paper proposes a\nsystematic business-modeldriven framework to guide the design of large and\nhighly interconnected data infrastructures which are provided and supported by\nmultiple stakeholders. The framework is used to model, elicit and reason about\nthe requirements of the service, technology, organization, value, and\ngovernance aspects of smart cities. The requirements serve as an input to a\nclosed-loop supply chain model, which is designed and managed to explicitly\nconsider the activities and processes that enables the stakeholders of smart\ncities to efficiently leverage their collective knowledge. We demonstrate how\nour approach can be used to design data infrastructures by examining a series\nof exemplary scenarios and by demonstrating how our approach handles the\nholistic design of a data infrastructure and informs the decision making\nprocess.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04158,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"IoT and Neural Network-Based Water Pumping Control System For Smart\n  Irrigation\n\n  This article aims at saving the wasted water in the process of irrigation\nusing the Internet of Things (IoT) based on a set of sensors and Multi-Layer\nPerceptron (MLP) neural network. The developed system handles the sensor data\nusing the Arduino board to control the water pump automatically. The sensors\nmeasure the environmental factors; namely temperature, humidity, and soil\nmoisture to estimate the required time for the operation of water irrigation.\nThe water pump control system consists of software and hardware tools such as\nArduino Remote XY interface and electronic sensors in the framework of IoT\ntechnology. The machine learning algorithm such as the MLP neural network plays\nan important role to support the decision of automatic control of IoT-based\nirrigation system, managing the water consumption effectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04928,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Collecting big behavioral data for measuring behavior against obesity\n\n  Obesity is currently affecting very large portions of the global population.\nEffective prevention and treatment starts at the early age and requires\nobjective knowledge of population-level behavior on the region\/neighborhood\nscale. To this end, we present a system for extracting and collecting\nbehavioral information on the individual-level objectively and automatically.\nThe behavioral information is related to physical activity, types of visited\nplaces, and transportation mode used between them. The system employs\nindicator-extraction algorithms from the literature which we evaluate on\npublicly available datasets. The system has been developed and integrated in\nthe context of the EU-funded BigO project that aims at preventing obesity in\nyoung populations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.14248,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Towards an Electronic Health Record System in Vietnam: A Core Readiness\n  Assessment\n\n  Previous studies have shown that health information technologies have a\npositive impact on health systems. Electronic health record (EHR) systems are\none of the most promising applications, demonstrating a positive effect in high\nincome countries. On the other hand, robust evidence for low and middle income\ncountries is still spare. The aim of this study is to initiate a carefully\nplanned nationwide EHR system in Vietnam by assessing the core readiness. The\nassessment structure is mainly based on previous research, which recommends a\nreadiness assessment prior to to an EHR system implementation. To collect data,\nparticipant observation, document analysis and an in-depth interview were used.\nThis study has revealed new insights into the current situation on EHR in\nVietnam. The Ministry of Health is currently working on improving the\nconditions for future implementation of a Vietnamese EHR system. There are\nissues with the current way of handling health records. These issues are\nencouraging the Ministry of Health to work on identifying the next steps for an\nEHR system implementation. The integration of an EHR system with current\nsystems seems to be challenging as most systems are commercial, closed source\nand very likely have no standardised interface. In conclusion, this study\nidentifies points which need to be further investigated prior to an\nimplementation. Generally, health care workers show good awareness of new\ntechnologies. As the Vietnam's health care system is centrally organised, there\nis the possibility for a nation-wide implementation. This could have a positive\nimpact on the health care system, however, besides rigours planning also\nstandards need to be followed and common interfaces implemented. Finally, this\nassessment has focused on only one level of readiness assessment. Further\nresearch is needed to complete the assessment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.0467,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Interoperable Framework to Enhance Citizen Services in the Kingdom of\n  Bahrain\n\n  Citizen records are scattered between different state organizations. It\nwastes time, effort, and resources for both citizen and organization to\ncollect, maintain, and update records to fulfill citizen services.\nInteroperability is a key element that enables seamless collaboration between\ndifferent entities. It requires non-conventional methods to overcome\ninteroperability challenges such as lack of trust, centralization, and policy\nand technology differences. Blockchain is a disruptive technology with the\npotential to overcome these challenges. The technology designed to enable\npeer-to-peer transactions with elimination of intermediary in a trustless\nenvironment through the control of consensus mechanisms. This research aims to\nexplore the status of interoperability in Bahrain, design an interoperable\nframework, and then test the validity of the framework by implementation of a\nprototype using blockchain technology. The research will be divided into four\nphases; I: Information collection, II: Design and modeling the framework, III:\nImplementation of a prototype, and Phase IV: Measuring the performance of the\nprototype. This research is in progress and it is expected, once is it\ncomplete, to enhance the e-government's plan in the Kingdom of Bahrain to\nprovide better services to citizens and help in the transition from\ne-government to seamless government, which will lead to sustainable citizen\nservices. On the other hand, the findings of the study is expected to improve\nthe social, economical, and environmental sustainability by the increase in\nprocess optimization, reduction of cost and complexity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.03296,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"Certain characteristics of financial management strategies of people\n  living in extreme poverty\n\n  This study presents the structure of financial management of incomes,\nexpenses, and borrowing practices of households in extreme poverty, based on a\nsurvey conducted in two disadvantaged regions in Hungary. Additional to that, I\nshed light on financial management practices of households in extreme poverty\nbased on the analysis of in-depth interviews. I draw theoretical conclusions\nand explanations, building upon the experiences of empiric materials about the\nattitudes and behaviors of financial management of households in extreme\npoverty, and the reasons and consequences thereof.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.12358,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000064241,
      "text":"Where Responsible AI meets Reality: Practitioner Perspectives on\n  Enablers for shifting Organizational Practices\n\n  Large and ever-evolving technology companies continue to invest more time and\nresources to incorporate responsible Artificial Intelligence (AI) into\nproduction-ready systems to increase algorithmic accountability. This paper\nexamines and seeks to offer a framework for analyzing how organizational\nculture and structure impact the effectiveness of responsible AI initiatives in\npractice. We present the results of semi-structured qualitative interviews with\npractitioners working in industry, investigating common challenges, ethical\ntensions, and effective enablers for responsible AI initiatives. Focusing on\nmajor companies developing or utilizing AI, we have mapped what organizational\nstructures currently support or hinder responsible AI initiatives, what\naspirational future processes and structures would best enable effective\ninitiatives, and what key elements comprise the transition from current work\npractices to the aspirational future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.05703,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000068876,
      "text":"Exploiting the Solar Energy Surplus for Edge Computing\n\n  In the context of the global energy ecosystem transformation, we introduce a\nnew approach to reduce the carbon emissions of the cloud-computing sector and,\nat the same time, foster the deployment of small-scale private photovoltaic\nplants. We consider the opportunity cost of moving some cloud services to\nprivate, distributed, solar-powered computing facilities. To this end, we\ncompare the potential revenue of leasing computing resources to a cloud pool\nwith the revenue obtained by selling the surplus energy to the grid. We first\nestimate the consumption of virtualized cloud computing instances, establishing\na metric of computational efficiency per nominal photovoltaic power installed.\nBased on this metric and characterizing the site's annual solar production, we\nestimate the total return and payback. The results show that the model is\neconomically viable and technically feasible. We finally depict the still many\nquestions open, such as security, and the fundamental barriers to address,\nmainly related with a cloud model ruled by a few big players.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.07618,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"The Demand Side of Open Government Data: A Case Study of Kingdom of\n  Bahrain\n\n  Governments around the world have realized the importance of Open Government\nData (OGD) as a new paradigm shift in government that focuses on making\ngovernments more service-oriented, transparent, and competent. However, as with\nmany countries, the situation of the OGD initiative in the Kingdom of Bahrain\nis not promising as reflected by a number of assessments that measure the\nimplementation and progress of OGD worldwide. The current research aims at\ninvesting in the local situation regarding consuming and reusing OGD in the\nKingdom of Bahrain. Specifically, this research assesses the level of citizen\nawareness towards OGD, determines citizens requirements of OGD, and identifies\nthe key challenges and obstacles in using\/reusing OGD. A questionnaire was\ndeveloped to investigate the demand side of OGD. The findings show that serious\nand responsible efforts from the publishers of OGD, namely: Government\nOrganizations are believed to be a necessity in order to progress the\nimplementation process of the OGD initiative in the Kingdom of Bahrain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.1696,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Digital Contact Tracing Service: An improved decentralized design for\n  privacy and effectiveness\n\n  We propose a decentralized digital contact tracing service that preserves the\nusers' privacy by design while complying to the highest security standards. Our\napproach is based on Bluetooth and measures actual encounters of people, the\ncontact time period, and estimates the proximity of the contact. We trace the\nusers' contacts and the possible spread of infectious diseases while preventing\nlocation tracking of users, protecting their data and identity. We verify and\nimprove the impact of tracking based on epidemiological models. We compare a\ncentralized and decentralized approach on a legal perspective and find a\ndecentralized approach preferable considering proportionality and data\nminimization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04226,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000043379,
      "text":"Learn-Apply-Reinforce\/Share Learning: Hackathons and CTFs as General\n  Pedagogic Tools in Higher Education, and Their Applicability to Distance\n  Learning\n\n  This paper lays out two teaching\/learning methods that are becoming\nincreasingly prevalent in computer science - hackathons, and Capture the Flag\n(CTF) competitions - and the pedagogic theory that underpins them. A case study\nof each is analysed, and the underpinning similarities extracted. The\nframeworks are then generalised to Learn-Apply-Reinforce\/Share Learning - a\nsocial constructivistic method that can be used subject-independently. The\napplicability of this new method to distance learning is then investigated -\nwith a mind to potential necessity to work from home - both due to increasing\ndemand in the Higher Education sector, but also the devastating impact of\ncrises such as the ongoing COVID-19 pandemic. Finally, a few potential\nextensions and future applications are discussed - including the possibilities\nof pivoting the method to be more research-driven, or indeed, to drive\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.14235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Confidential Computing for Privacy-Preserving Contact Tracing\n\n  Contact tracing is paramount to fighting the pandemic but it comes with\nlegitimate privacy concerns. This paper proposes a system enabling both,\ncontact tracing and data privacy.\n  We propose the use of the Intel SGX trusted execution environment to build a\nprivacy-preserving contact tracing backend. While the concept of a confidential\ncomputing backend proposed in this paper can be combined with any existing\ncontact tracing smartphone application, we describe a full contact tracing\nsystem for demonstration purposes.\n  A prototype of a privacy-preserving contact tracing system based on SGX has\nbeen implemented by the authors in a hackathon.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16879,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000150667,
      "text":"Combating Anti-Blackness in the AI Community\n\n  In response to a national and international awakening on the issues of\nanti-Blackness and systemic discrimination, we have penned this piece to serve\nas a resource for allies in the AI community who are wondering how they can\nmore effectively engage with dismantling racist systems. This work aims to help\nelucidate areas where the AI community actively and passively contributes to\nanti-Blackness and offers actionable items on ways to reduce harm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.10838,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"A Methodology for Assessing the Environmental Effects Induced by ICT\n  Services. Part II: Multiple Services and Companies\n\n  Information and communication technologies (ICT) can make existing products\nand activities more efficient or substitute them altogether and could thus\nbecome crucial for the mitigation of climate change. In this context,\nindividual ICT companies, industry organizations and international initiatives\nhave started to estimate the environmental effects of ICT services. Often such\nassessments rely on crude assumptions and methods, yielding inaccurate or even\nmisleading results. The few existing methodological attempts are too general to\nprovide guidance to practitioners. The starting points of this paper are i) a\nhigh level standard from the European Telecommunication Standardisation\nInstitute (ETSI) and the International Telecommunication Union (ITU), and ii)\nits suggested enhancements for single service assessment outlined in \"A\nMethodology for Assessing the Environmental Effects Induced by ICT Services\nPart I: Single services\" (Part I in short). Building on the assessment of\nsingle services, the current article identifies and addresses shortcomings of\nexisting methodologies and industry practices with regard to multiple services\nassessment. For a collection of services, it addresses the goal and scope\ndefinition, the so far ignored aggregation of effects among several services,\nand the allocation between several companies contributing to one or more\nservices. The article finally brings these considerations together with those\nof Part I into a workflow for performing such assessments in practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.12628,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Paratransit Agency Responses to the Adoption of Sub-contracted Services\n  Using Secure Technologies\n\n  Transportation agencies across the United States have the responsibility of\nproviding transportation services for all travelers. Paratransit services which\nare designed to meet the needs of disabled travelers have been available to a\ncertain extent for decades, but under the Americans with Disabilities Act\nmandate of 1990, uniform requirements were adopted across U.S. agencies. Most\nof these paratransit operators offer services which must be scheduled at least\na day in advance. And, provision of these services by accessible busses is\ngenerally very expensive. Therefore, many agencies are considering\nsub-contracting some services to approved ride-hailing or taxi services. The\npurpose of this work is to examine the opinions of various public agencies with\nrespect to the adoption of sub-contracted services through the use of secure\ntechnologies. Our research provides insight into the future of these\npartnerships. Agencies expressed interest in the use of privacy preserving\nsecure technologies as well as a strong desire for better software solutions\nfor paratransit passengers and operators. The on-line survey received thirty\nresponses for a completion rate of 19.1%. Our primary findings are that a major\nconcern of agencies for this sort of arrangement is the lack of Wheelchair\nAccessible Vehicles offered by taxis and TNCs and about 36% of the surveyed\nagencies have not considered such partnerships.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"AI from concrete to abstract: demystifying artificial intelligence to\n  the general public\n\n  Artificial Intelligence (AI) has been adopted in a wide range of domains.\nThis shows the imperative need to develop means to endow common people with a\nminimum understanding of what AI means. Combining visual programming and WiSARD\nweightless artificial neural networks, this article presents a new methodology,\nAI from concrete to abstract (AIcon2abs), to enable general people (including\nchildren) to achieve this goal. The main strategy adopted by is to promote a\ndemystification of artificial intelligence via practical activities related to\nthe development of learning machines, as well as through the observation of\ntheir learning process. Thus, it is possible to provide subjects with skills\nthat contributes to making them insightful actors in debates and decisions\ninvolving the adoption of artificial intelligence mechanisms. Currently,\nexisting approaches to the teaching of basic AI concepts through programming\ntreat machine intelligence as an external element\/module. After being trained,\nthat external module is coupled to the main application being developed by the\nlearners. In the methodology herein presented, both training and classification\ntasks are blocks that compose the main program, just as the other programming\nconstructs. As a beneficial side effect of AIcon2abs, the difference between a\nprogram capable of learning from data and a conventional computer program\nbecomes more evident. In addition, the simplicity of the WiSARD weightless\nartificial neural network model enables easy visualization and understanding of\ntraining and classification tasks internal realization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.12624,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Effects of Non-Cognitive Factors on Post-Secondary Persistence of Deaf\n  Students: An Agent-Based Modeling Approach\n\n  Post-secondary education persistence is the likelihood of a student remaining\nin post-secondary education. Although statistics show that post-secondary\npersistence for deaf students has increased recently, there are still many\nobstacles obstructing students from completing their post-secondary degree\ngoals. Therefore, increasing the persistence rate is crucial to increase\neducation and work goals for deaf students. In this work, we present an\nagent-based model using NetLogo software for the persistence phenomena of deaf\nstudents. We consider four non-cognitive factors: having clear goals, social\nintegration, social skills, and academic experience, which influence the\ndeparture decision of deaf students. Progress and results of this work suggest\nthat agent-based modeling approaches promise to give better understanding of\nwhat will increase persistence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16964,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Data Science: Nature and Pitfalls\n\n  Data science is creating very exciting trends as well as significant\ncontroversy. A critical matter for the healthy development of data science in\nits early stages is to deeply understand the nature of data and data science,\nand to discuss the various pitfalls. These important issues motivate the\ndiscussions in this article.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.15428,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"A Case Study to Identify the Hindrances to Widespread Adoption of\n  Electric Vehicles in Qatar\n\n  The adoption of electric vehicles (EVs) have proven to be a crucial factor to\ndecreasing the emission of greenhouse gases (GHG) into the atmosphere. However,\nthere are various hurdles that impede people from purchasing EVs. For example,\nlong charging time, short driving range, cost and insufficient charging\ninfrastructures available, etc. This article reports the public perception of\nEV-adoption using statistical analyses and proposes some recommendations for\nimproving EV-adoption in Qatar. User perspectives on EV-adoption barriers in\nQatar were investigated based on survey questionnaires. The survey\nquestionnaires were based on similar studies done in other regions of the\nworld. The study attempted to look at different perspectives of the adoption of\nEV, when asked to a person who is aware of EVs or a person who may or may not\nbe aware of EVs. Cumulative survey responses from the two groups were compared\nand analyzed using a two sample t-test statistical analysis. Detailed analyses\nshowed that among various major hindrances raising of public awareness of such\ngreener modes of transportation, the availability of charging options in more\nplaces and policy incentives towards EVs would play a major role in\nEV-adoption. The authors provide recommendations that along with government\nincentives could help make a gradual shift to a greater number of EVs\nconvenient for people of Qatar. The proposed systematic approach for such a\nstudy and analysis may help in streamlining research on policies,\ninfrastructures and technologies for efficient penetration of EVs in Qatar.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.13831,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Adoption of ICT innovations in the agriculture sector in Africa: A\n  Systematic Literature Review\n\n  According to the latest World Economic Forum report, about 70% of the African\npopulation depends on agriculture for their livelihood. This makes agriculture\na critical sector within the African continent. Nonetheless, agricultural\nproductivity is low and food insecurity is still a challenge. This has in\nrecent years led to several initiatives in using ICT (Information Communication\nTechnology) to improve agriculture productivity. This study aims to explore ICT\ninnovations in the agriculture sector of Africa. To achieve this, we conducted\na SLR (Systematic Literature Review) of the literature published since 2010.\nOur search yielded 779 papers, of which 23 papers were selected for a detailed\nanalysis following a detailed exclusion and quality assessment criteria. The\nanalysis of the selected papers shows that the main ICT technologies adopted\nare text and voice-based services targeting mobile phones. The analysis also\nshows that radios are still widely used in disseminating agriculture\ninformation to rural farmers, while computers are mainly used by researchers.\nThough the mobile-based services aimed at improving access to accurate and\ntimely agriculture information, the literature reviews indicate that the\nadoption of the services is constrained by poor technological infrastructure,\ninappropriate ICT policies and low capacity levels of users, especially\nfarmers, to using the technologies. The findings further indicate that\nliterature on an appropriate theoretical framework for guiding ICT innovations\nis lacking.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.05892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000085433,
      "text":"Privacy by Design in Value-Exchange Systems\n\n  This article addresses some of the most contentious issues related to privacy\nin electronic payment systems, particularly the current zeitgeist of proposed\nsolutions for central bank digital currency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.10831,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"A Methodology for Assessing the Environmental Effects Induced by ICT\n  Services. Part I: Single Services\n\n  Information and communication technologies (ICT) are increasingly seen as key\nenablers for climate change mitigation measures. They can make existing\nproducts and activities more efficient or substitute them altogether.\nConsequently, different initiatives have started to estimate the environmental\neffects of ICT services. Such assessments, however, lack scientific rigor and\noften rely on crude assumptions and methods, leading to inaccurate or even\nmisleading results. The few methodological attempts that exist do not address\nseveral crucial aspects, and are thus insufficient to foster good as-sessment\npractice. Starting from such a high level standard from the European\nTelecommunication Standardisation Institute (ETSI) and the International\nTelecommunication Union (ITU), this article identifies the shortcomings of\nexisting methodologies and proposes solutions. It addresses several aspects for\nthe assessment of single ICT services: the goal and scope definition (analyzing\ndifferences between ICT substitution and optimization, the time perspective of\nthe assessment, the challenge of a hypothetical baseline for the situation\nwithout the ICT solution, and the differences between modelling and case\nstudies) as well as the often ignored influence of rebound effects and the\ndifficult extrapolation from case studies to larger populations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.08365,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"COVID-19 Mobility Data Collection of Seoul, South Korea\n\n  The relationship between pandemic and human mobility has received\nconsiderable attention from scholars, as it can provide an indication of how\nmobility patterns change in response to a public health crisis or whether\nreduced mobility contributes to preventing the spread of an infectious disease.\nWhile several studies attempted to unveil such relationship, no studies have\nfocused on changes in human mobility at a finer scale utilizing comprehensive,\nhigh-resolution data. To address the complex association between pandemic's\nspread and human mobility, this paper presents two categories of mobility\ndatasets - trip mode and trip purpose - that concern nearly 10 million\ncitizens' movements during COVID-19 in the capital city of South Korea, Seoul,\nwhere no lockdowns has been imposed. We curate hourly data of subway ridership,\ntraffic volume and population present count at selected points of interests.\nThe results to be derived from the presented datasets can be used as an\nimportant reference for public health decision making in the post COVID-19 era.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.11129,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"All you can stream: Investigating the role of user behavior for\n  greenhouse gas intensity of video streaming\n\n  The information and communication technology sector reportedly has a relevant\nimpact on the environment. Within this sector, video streaming has been\nidentified as a major driver of CO2-emissions. To make streaming more\nsustainable, environmentally relevant factors must be identified on both the\nuser and the provider side. Hence, environmental assessments, like life cycle\nassessments (LCA), need to broaden their perspective from a mere technological\nto one that includes user decisions and behavior. However, quantitative data on\nuser behavior (e.g. streaming duration, choice of end device and resolution)\nare often lacking or difficult to integrate in LCA. Additionally, identifying\nrelevant determinants of user behavior, such as the design of streaming\nplatforms or user motivations, may help to design streaming services that keep\nenvironmental impact at a passable level. In order to carry out assessments in\nsuch a way, interdisciplinary collaboration is necessary. Therefore, this\nexploratory study combined LCA with an online survey (N= 91, 7 consecutive days\nof assessment). Based on this dataset the use phase of online video streaming\nwas modeled. Additionally, factors such as sociodemographic, motivational and\ncontextual determinants were measured. Results show that CO2-intensity of video\nstreaming depends on several factors. It is shown that for climate intensity\nthere is a factor 10 between choosing a smart TV and smartphone for video\nstreaming. Furthermore, results show that some factors can be tackled from\nprovider side to reduce overall energy demand at the user side; one of which is\nsetting a low resolution as default.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.0315,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Framework for an Integrated Learning Block with CDIO-led Engineering\n  Education\n\n  As a CDIO collaborating member, the School of Mechanical and Electrical\nEngineering of Sheridan maintains a curriculum that is deeply rooted in\nskills-based learning, experiential learning, and engineering design. To ensure\nour graduates are agile and ready for the workforce, we are taking proactive\nmeasures to further improve their learning experiences. An important challenge\nstill impeding our students knowledge acquisition is the perception that\nprogram courses have disjointed learning outcomes. The course map of programs\nis carefully designed in such a way that technical skills acquired in\nparticular courses gradually build on each other. Despite the traditional\nexistence of prerequisites and co-requisites, the inaccurate view that courses\nfunction independently persists among students and, occasionally, among faculty\nmembers. One feasible approach to tackle this pedagogical challenge is to\ncombine various courses into an integrated learning block (ILB) having a\nunified mission and objective. At Sheridan's School of MEET, we are applying an\nILB with three engineering courses offered within the same semester for all our\nB.Eng. degree programs. The ILB deliverables are based on the design of a\nchosen engineering system or subunit in a project-based learning (PBL)\nenvironment. The rationale of this paper is to share our framework for\nimplementing an ILB in engineering programs and to examine the opportunities\nand challenges related to this type of curriculum design. In particular, we\nwill discuss the methodology by which courses are selected to form an ILB while\ntaking into account their appropriateness for an industry-driven PBL. This will\nbe followed up with some of the strategies that are proposed to evaluate the\nperformance of students in an ILB through formative and summative assessments\nbased on CDIO competencies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09416,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000592073,
      "text":"Ethical issues with using Internet of Things devices in citizen science\n  research: A scoping review\n\n  Our chapter presents a scoping review of published scientific studies or case\nstudies of scientific studies that utilise both citizen scientists and Internet\nof Things devices. Specifically, we selected studies where the authors had\nincluded at least a short discussion of the ethical issues encountered during\nthe research process. Having conducted a search of five databases (IEEE Xplore,\nScopus, Web of Science, ProQuest, and PubMed), we identified 631 potential\nresults. Following abstract and title screening, and then full text eligibility\nassessment, we identified 34 published articles that matched our criteria. We\nthen analysed the full text for these articles inductively and deductively,\ncoding ethical issues into three main categories. These categories were\nautonomy and data privacy, data quality, and intellectual property. We also\nanalysed the full text of these articles to see what strategies researchers\ntook to resolve these ethical issues, as well as any legal implications raised.\nFollowing this analysis, our discussion provides recommendations for\nresearchers who wish to integrate citizen scientists and Internet of Things\ndevices into their research. First, all citizen science projects should\nintegrate a data privacy protocol to protect the confidentiality of\nparticipants. Secondly, scientific researchers should consider any potential\nissues of data quality, including whether compromises might be required, before\nestablishing a project. Finally, all intellectual property issues should be\nclarified both at the start of the project and during its lifecycle.\nResearchers should also consider any ethical issues that might flow from the\nuse of commercially available Internet of Things devices for research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.0406,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Study on Computational Thinking as Problem-solving Skill: Comparison\n  Based on Students Mindset in Engineering and Social Science\n\n  One of the capabilities which 21st-century skill compulsory a person is\ncritical thinking and problem-solving skill that becomes top positions rank.\nFocus on problem-solving skills can be taught to a child, especially begun in\nelementary school refer to prior research focus on K-12. Computational thinking\nwas one problem-solving skill that popular to implemented and studied in the\ncurrent decade. This study was conducted to explore students' capability to be\nable solving of the problem based on the possibility use the computational\nthinking way. Participants in this study came from six international students\nthat study in Taiwan and from two deferent sciences disciplines, engineering,\nand social science. A qualitative method was used to analyze data interviews,\ntook example cases from the global issue that is Climate Change. The result\nfounded that survive in a new environment was become evidence of their\nimplementation of problem-solving skills. Problem-solving mindset both students\nof engineering and social science had discrepancy, those are how to use precise\nstructure in the algorithm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.00854,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Random errors are not necessarily politically neutral\n\n  Errors are inevitable in the implementation of any complex process. Here we\nexamine the effect of random errors on Single Transferable Vote (STV)\nelections, a common approach to deciding multi-seat elections. It is usually\nexpected that random errors should have nearly equal effects on all candidates,\nand thus be fair. We find to the contrary that random errors can introduce\nsystematic bias into election results. This is because, even if the errors are\nrandom, votes for different candidates occur in different patterns that are\naffected differently by random errors. In the STV context, the most important\neffect of random errors is to invalidate the ballot. This removes far more\nvotes for those candidates whose supporters tend to list a lot of preferences,\nbecause their ballots are much more likely to be invalidated by random error.\nDifferent validity rules for different voting styles mean that errors are much\nmore likely to penalise some types of votes than others. For close elections\nthis systematic bias can change the result of the election.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.05552,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Tracing Complexity in Food Blogging Entries\n\n  Within this paper, we focus on the concept of complexity and how it is\nrepresented in food blogging entries on Twitter. We turn specific attention to\ncomplexity capture when looking at healthy foods, focusing on food blogging\nentries that entail the notions of health\/healthiness\/healthy. We do so because\nwe consider that complexity manifests hedonism - that is the irrational\ndeterminant of food choice above rational considerations of nutrition and\nhealthiness. Using text as a platform for our analysis, we derive bigrams and\ntopic models that illustrate the frequencies of words and bi-grams, thus,\npointing our attention to current discourse in food blogging entries on\nTwitter. The results show that, contrary to complexity, that the dominating\ncharacteristics in healthy food domain are easiness and speed of preparation,\nhowever, rational and health related considerations may not always take\nprecedence when the choice is determined. Food blogging entries show\nsurprisingly little account of healthy food as being tasty and enjoyable. With\nthis we aim to contribute to the knowledge of how to shape more healthy\nconsumer behaviors. Having discovered the scarcity of hedonic connotations,\nthis work invites for further research in text-based information about food.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.08177,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Elicitation of SME Requirements for Cybersecurity Solutions by Studying\n  Adherence to Recommendations\n\n  Small and medium-sized enterprises (SME) have become the weak spot of our\neconomy for cyber attacks. These companies are large in number and often do not\nhave the controls in place to prevent successful attacks, respectively are not\nprepared to systematically manage their cybersecurity capabilities. One of the\nreasons for why many SME do not adopt cybersecurity is that developers of\ncybersecurity solutions understand little the SME context and the requirements\nfor successful use of these solutions. We elicit requirements by studying how\ncybersecurity experts provide advice to SME. The experts recommendations offer\ninsights into what important capabilities of the solution are and how these\ncapabilities ought to be used for mitigating cybersecurity threats. The\nadoption of a recommendation hints at a correct match of the solution, hence\nsuccessful consideration of requirements. Abandoned recommendations point to a\nmisalignment that can be used as a source to inquire missed requirements.\nRe-occurrence of adoption or abandonment decisions corroborate the presence of\nrequirements. This poster describes the challenges of SME regarding\ncybersecurity and introduces our proposed approach to elicit requirements for\ncybersecurity solutions. The poster describes CYSEC, our tool used to capture\ncybersecurity advice and help to scale cybersecurity requirements elicitation\nto a large number of participating SME. We conclude by outlining the planned\nresearch to develop and validate CYSEC.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.13346,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"Karl Marx and the Blockchain\n\n  Blockchain is often presented as a technological development; however,\nclearly it is not only that: the `Blockchain buzz' exists in the context of\ncurrent social and political developments. In this essay, we analyse blockchain\ntechnology and its social and political context from a perspective of Marxist\neconomic theory.\n  Since arguably the last great inflection point in society and technology was\nanalysed by Marx in terms of labour and capital and since we seem to be\nexperiencing a shift in the balance between these forces today, it makes sense\nto revisit the Marxist ideas and apply them to the current situation, to see\nhow well they still apply and if necessary to update them for current events.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07602,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Automating the Communication of Cybersecurity Knowledge: Multi-Case\n  Study\n\n  Cybersecurity is essential for the protection of companies against cyber\nthreats. Traditionally, cybersecurity experts assess and improve a company's\ncapabilities. However, many small and medium-sized businesses (SMBs) consider\nsuch services not to be affordable. We explore an alternative do-it-yourself\n(DIY) approach to bringing cybersecurity to SMBs. Our method and tool, CYSEC,\nimplements the Self-Determination Theory (SDT) to guide and motivate SMBs to\nadopt good cybersecurity practices. CYSEC uses assessment questions and\nrecommendations to communicate cybersecurity knowledge to the end-user SMBs and\nencourage self-motivated change. In this paper, the operationalisation of SDT\nin CYSEC is presented and the results of a multi-case study shown that offer\ninsight into how SMBs adopted cybersecurity practices with CYSEC. Effective\nautomated cybersecurity communication depended on the SMB's hands-on skills,\ntools adaptedness, and the users' willingness to documenting confidential\ninformation. The SMBs wanted to learn in simple, incremental steps, allowing\nthem to understand what they do. An SMB's motivation to improve security\ndepended on the fitness of assessment questions and recommendations with the\nSMB's business model and IT infrastructure. The results of this study indicate\nthat automated counselling can help many SMBs in security adoption. The final\npublication is available at Springer via\nhttps:\/\/link.springer.com\/chapter\/10.1007%2F978-3-030-59291-2_8\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.06933,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"The ASHRAE Great Energy Predictor III competition: Overview and results\n\n  In late 2019, ASHRAE hosted the Great Energy Predictor III (GEPIII) machine\nlearning competition on the Kaggle platform. This launch marked the third\nenergy prediction competition from ASHRAE and the first since the mid-1990s. In\nthis updated version, the competitors were provided with over 20 million points\nof training data from 2,380 energy meters collected for 1,448 buildings from 16\nsources. This competition's overall objective was to find the most accurate\nmodeling solutions for the prediction of over 41 million private and public\ntest data points. The competition had 4,370 participants, split across 3,614\nteams from 94 countries who submitted 39,403 predictions. In addition to the\ntop five winning workflows, the competitors publicly shared 415 reproducible\nonline machine learning workflow examples (notebooks), including over 40\nadditional, full solutions. This paper gives a high-level overview of the\ncompetition preparation and dataset, competitors and their discussions, machine\nlearning workflows and models generated, winners and their submissions,\ndiscussion of lessons learned, and competition outputs and next steps. The most\npopular and accurate machine learning workflows used large ensembles of mostly\ngradient boosting tree models, such as LightGBM. Similar to the first predictor\ncompetition, preprocessing of the data sets emerged as a key differentiator.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.05075,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Reusable Learning Objects: An Agile Approach\n\n  This paper discusses Reusable Learning Objects (RLOs) and to what extent they\nhave lived up to the promise, particularly of reusability. Reusable Learning\nObjects have actually been discussed in the literature for the last 20 years\nand yet true large scale sharing of learning and teaching materials remains\nrelatively rare and challenging. This paper argues that part of the reason is\nthat the granularity of the learning objects that are in use today is not\nconducive to true reuse. Certainly whole PowerPoint slide decks and word\ndocuments are kept in individual files and folders. It is not an ideal\nsituation. As a result, educators, teachers, course designers, are constantly\nreinventing the wheel, or searching for where that one excellent assignment,\nexplanation, definition was last seen so it can be copied forward. This paper\nargues that to achieve effective reuse of Learning Objects, the following are\nrequired: smaller, more granular (micro) learning objects; means to combine\nthem into larger presentation products; and modern revision and version\ncontrol. The paper proposes applying approaches originating in the software\nengineering community, such as agile methodology, version control and\nmanagement, markup languages, and agile publishing, which together form the\nAgile Approach of the title of the paper. With that foundation laid, the paper\nexamines CourseGen, an open source software platform designed for creating,\nsharing, reusing and publishing reusable course content. CourseGen uses a\nmodified markdown format augmented by CourseGen specific directives, such as\n$link to and $include topic. The CourseGen compiler converts a collection of\nCourseGen files into the final format such as a web site or a PowerPoint.\nCourseGen was designed, used and refined over the last three years in several\nComputer Science Courses at Brandeis University.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09134,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"A Systematic Review of Natural Language Processing for Knowledge\n  Management in Healthcare\n\n  Driven by the visions of Data Science, recent years have seen a paradigm\nshift in Natural Language Processing (NLP). NLP has set the milestone in text\nprocessing and proved to be the preferred choice for researchers in the\nhealthcare domain. The objective of this paper is to identify the potential of\nNLP, especially, how NLP is used to support the knowledge management process in\nthe healthcare domain, making data a critical and trusted component in\nimproving the health outcomes. This paper provides a comprehensive survey of\nthe state-of-the-art NLP research with a particular focus on how knowledge is\ncreated, captured, shared, and applied in the healthcare domain. Our findings\nsuggest, first, the techniques of NLP those supporting knowledge management\nextraction and knowledge capture processes in healthcare. Second, we propose a\nconceptual model for the knowledge extraction process through NLP. Finally, we\ndiscuss a set of issues, challenges, and proposed future research areas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.08331,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Challenges and Prospects of Negawatt Trading in Light of Recent\n  Technological Developments\n\n  With the advancement of the smart grid, the current energy system is moving\ntowards a future where people can buy what they need, sell when they have\nexcess, and can trade the right of buying to other prosumers. While the first\ntwo schemes already exist in the market, selling the right of buying, also\nknown as negawatt trading, is something that is yet to be implemented. Here, we\nreview the challenges and prospects of negawatt trading in light of recent\ntechnological advancements. Through reviewing a number of emerging\ntechnologies, we show that the necessary methodologies that are needed to\nestablish negawatt trading as a feasible energy management scheme in the smart\ngrid are already available. Grid interactive buildings and distributed ledger\ntechnologies for instance can ensure active participation and fair pricing.\nHowever, some additional challenges need to address for fully functional\nnegawatt trading mechanisms in today's energy market.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07096,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"Multi-Utility Market: Framework for a Blockchain Exchange Platform for\n  Sustainable Development\n\n  Water and other resources are becoming scarcer every day, and developing\ncountries are the neediest for an immediate intervention. Water, as a national\nneed, is considered to be one of the main causes for conflicts in the 21st\ncentury. Peer-to-peer trading is one of the most convenient, scalable and\nsustainable solutions but faces organization challenges such as: the absence of\nsuitable business models motivating normal users to sell their generated\nresources, currency and financial settlement complexities, and single utility\nmarkets. We propose a multi-utility trading platform, based on blockchain\ntechnology which can address the challenges faced by peer-to-peer trading. This\nplatform meets the needs of developing countries in particular as well as rural\nareas of developed countries. The open nature of our proposed design makes it\nsuitable for adoption and use by various stakeholders.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07587,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"SARS-CoV-2 Impact on Online Teaching Methodologies and the Ed-Tech\n  Sector: Smile and Learn Platform Case Study\n\n  In the past few years, the use of new technologies and digital, educational\nmaterial has been increasing. Owing to the situation brought about in Spain by\nthe SARS-CoV-2 (COVID-19) pandemic, schools have closed and families were asked\nto confine at home since March 2020. During this period there has been a need\nof resources to be made available to the families and teachers so that they\nwould be able to continue their teaching practice. Consequently, this study\nanalyzes the importance of online methodologies and usage tendency of an\neducational resource example: The Smile and Learn platform. Thereby, the study\npresents the different models implemented to support education and its impact\nin the use of the platform. The analyzed outcomes and their effect on education\nare discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.007,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"Response by the Montreal AI Ethics Institute to the Santa Clara\n  Principles on Transparency and Accountability in Online Content Moderation\n\n  In April 2020, the Electronic Frontier Foundation (EFF) publicly called for\ncomments on expanding and improving the Santa Clara Principles on Transparency\nand Accountability (SCP), originally published in May 2018. The Montreal AI\nEthics Institute (MAIEI) responded to this call by drafting a set of\nrecommendations based on insights and analysis by the MAIEI staff and\nsupplemented by workshop contributions from the AI Ethics community convened\nduring two online public consultation workshops.\n  In its submission, MAIEI provides 12 overarching recommendations for the SCP,\nthese include: 1) ensure there is more diversity in the content moderation\nprocess; 2) increase transparency into how platforms guide content-ranking; 3)\ndisclose anonymized data on the training and\/or cultural background of the\ncontent moderators for a platform; 4) tailor content moderation tools for\nspecific issues; 5) draft specific guidelines for messaging applications with\nregards to data protection in content moderation; 6) take into account cultural\ndifferences relevant to what constitutes acceptable behavior online; 7) ensure\nplatforms are transparent in regards to political advertising; 8) ensure\ngreater transparency into the user-generated flagging\/reporting systems\ndeployed by a platform; 9) clarify if user content is flagged or reported\nthrough an automated system; 10) provide more data on the types of content\nremoved from platforms; 11) provide clear guidelines on the appeal process, as\nwell as data on prior appeals; 12) create a system for periodically revisiting\nthe SCP so it reflects various technological advancements, modifications in law\nand policy, as well as changing trends or movements in content moderation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.15807,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Seating preference analysis for hybrid workplaces\n\n  Due to the increasing nature of flexible work and the recent requirements\nfrom COVID-19 restrictions, workplaces are becoming more hybrid (i.e. allowing\nworkers to work between traditional office spaces and elsewhere including from\nhome). Since workplaces are different in design, layout and available\nfacilities, many workers find it difficult to adjust accordingly. Eventually,\nthis impacts negatively towards work productivity and other related parameters\nincluding concentration, stress, and mood while at work. One of the key factors\nthat causes this negative work experience is directly linked to the available\nseating arrangements. In this paper, we conduct an analysis to understand\nvarious seating preferences of 37 workers with varying demographics, using the\ndata collected pre-COVID-19, and analyse the findings in the context of hybrid\nworkplace settings. We also discuss a list of implications illustrating how our\nfindings can be adapted across wider hybrid work settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.11281,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Big Issues for Big Data: challenges for critical spatial data analytics\n\n  In this paper we consider some of the issues of working with big data and big\nspatial data and highlight the need for an open and critical framework. We\nfocus on a set of challenges underlying the collection and analysis of big\ndata. In particular, we consider 1) the issues related to inference when\nworking with usually biased big data, challenging the assumed inferential\nsuperiority of data with observations, n, approaching N, the population (n->N),\nand the need for data science analysis that answer questions of practical\nsignificance or with greater emphasis n the size of the effect, rather than the\ntruth or falsehood of a statistical statement; 2) the need to accept messiness\nin your data and to document all operations undertaken on the data because of\nthis support of openness and reproducibility paradigms; and 3) the need to\nexplicitly seek to understand the causes of bias, messiness etc in the data and\nthe inferential consequences of using such data in analyses, by adopting\ncritical approaches to spatial data science. In particular we consider the need\nto place individual data science studies in a wider social and economic\ncontexts, along the the role of inferential theory in the presence of big data,\nand issues relating to messiness and complexity in big data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.03447,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Children's health in the digital age\n\n  Environmental studies, metabolic research, and state of the art in\nneurobiology point towards the reduced amount of natural day and sunlight\nexposure of the developing childs organism, as a consequence of increasingly\nlong hours spent indoors online, as the single unifying source of a whole set\nof health risks identified worldwide, as is made clear in this review of the\ncurrent literature. Over exposure to digital environments, from abuse to\naddiction, now concerns even the youngest (ages zero to 2), and triggers, as\nargued on the basis of clear examples herein, a chain of interdependent\nnegative and potentially long-term metabolic changes. This leads to a\nderegulation of the serotonin and dopamine neurotransmitter pathways in the\ndeveloping brain, currently associated with online activity abuse or internet\naddiction, and akin to that found in severe substance abuse syndromes. A\ngeneral functional working model is proposed under the light of evidence\nbrought to the forefront in this review.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.05129,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Senior Living Communities: Made Safer by AI\n\n  There is a historically unprecedented shift in demographics towards seniors,\nwhich will result in significant housing development over the coming decade.\nThis is an enormous opportunity for real-estate operators to innovate and\naddress the demand in this growing market. However, investments in this area\nare fraught with risk. Seniors often have more health issues, and Covid-19 has\nexposed just how vulnerable they are -- especially those living in close\nproximity. Conventionally, most services for seniors are \"high-touch\",\nrequiring close physical contact with trained caregivers. Not only are trained\ncaregivers short in supply, but the pandemic has made it evident that\nconventional high-touch approaches to senior care are high-cost and greater\nrisk. There are not enough caregivers to meet the needs of this emerging\ndemographic, and even fewer who want to undertake the additional training and\nrisk of working in a senior facility, especially given the current pandemic. In\nthis article, we rethink the design of senior living facilities to mitigate the\nrisks and costs using automation. With AI-enabled pervasive automation, we\nclaim there is an opportunity, if not an urgency, to go from high-touch to\nalmost \"no touch\" while dramatically reducing risk and cost. Although our\nvision goes beyond the current reality, we cite measurements from Caspar\nAI-enabled senior properties that show the potential benefit of this approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Explainable AI based Interventions for Pre-season Decision Making in\n  Fashion Retail\n\n  Future of sustainable fashion lies in adoption of AI for a better\nunderstanding of consumer shopping behaviour and using this understanding to\nfurther optimize product design, development and sourcing to finally reduce the\nprobability of overproducing inventory. Explainability and interpretability are\nhighly effective in increasing the adoption of AI based tools in creative\ndomains like fashion. In a fashion house, stakeholders like buyers,\nmerchandisers and financial planners have a more quantitative approach towards\ndecision making with primary goals of high sales and reduced dead inventory.\nWhereas, designers have a more intuitive approach based on observing market\ntrends, social media and runways shows. Our goal is to build an explainable new\nproduct forecasting tool with capabilities of interventional analysis such that\nall the stakeholders (with competing goals) can participate in collaborative\ndecision making process of new product design, development and launch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.14732,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Simulator as a Tool for the Future Maritime Education and Research: A\n  Discussion\n\n  A few studies in the maritime domain utilize co-design in ship design\nworkshops, however, none of them addresses a full picture of how co-design can\nmake changes in simulation-based maritime education. In this paper, we reflect\nhow co-design can help to foresight future skills in the maritime domain,\nespecially on how to use simulators to support increasing competence of\nseafarers and in turn to redesign simulators to support maritime education.\nThus, we address collaborative and innovative research activities, to enable\nall participants (seafarers, trainers, technicians, authorities etc.) to share\ntheir experiences so a joint recognition of needed future skills can be\nreached. Along with the ex-change of experiences, we assert that the supported\nsimulations and simulator techniques could be designed to achieve sustainable\ngrowth for all participants.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.08318,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Organizing Virtual Conferences through Mirrors: The ACM e-Energy 2020\n  Experience\n\n  The emergence of the world-wide COVID-19 pandemic has forced academic\nconferences to be held entirely in a virtual manner. While prior studies have\nadvocated the merits of virtual conferences in terms of energy and cost\nsavings, organizers are increasingly facing the prospect of planning and\nexecuting them systematically, in order to deliver a rich\nconference-attending-experience for all participants. Starting from March 2020,\ntens of conferences have been held virtually. Past conferences have revealed\nnumerous challenges, from budget planning, to selecting the supporting virtual\nplatforms. Among these, two special challenges were identified: 1) how to\ndeliver talks to geo-distributed attendees and 2) how to stimulate social\ninteractions among attendees. These are the two important goals of an academic\nconference. In this paper, we advocate a mirror program approach for academic\nconferences. More specifically, the conference program is executed in multiple\nparallel (mirrored) programs, so that each mirror program can fit a different\ntime zone. This can effectively address the first challenge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11026,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"On Course, But Not There Yet: Enterprise Architecture Conformance and\n  Benefits in Systems Development\n\n  Various claims have been made regarding the benefits that Enterprise\nArchitecture (EA) delivers for both individual systems development projects and\nthe organization as a whole. This paper presents the statistical findings of a\nsurvey study (n=293) carried out to empirically test these claims. First, we\ninvestigated which techniques are used in practice to stimulate conformance to\nEA. Secondly, we studied which benefits are actually gained. Thirdly, we\nverified whether EA creators (e.g. enterprise architects) and EA users (e.g.\nproject members) differ in their perceptions regarding EA. Finally, we\ninvestigated which of the applied techniques most effectively increase project\nconformance to and effectiveness of EA. A multivariate regression analysis\ndemonstrates that three techniques have a major impact on conformance: carrying\nout compliance assessments, management propagation of EA and providing\nassistance to projects. Although project conformance plays a central role in\nreaping various benefits at both the organizational and the project level, it\nis shown that a number of important benefits have not yet been fully achieved.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11808,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"#ISIS vs #ActionCountersTerrorism: A Computational Analysis of Extremist\n  and Counter-extremist Twitter Narratives\n\n  The rapid expansion of cyberspace has greatly facilitated the strategic shift\nof traditional crimes to online platforms. This has included malicious actors,\nsuch as extremist organisations, making use of online networks to disseminate\npropaganda and incite violence through radicalising individuals. In this\narticle, we seek to advance current research by exploring how supporters of\nextremist organisations craft and disseminate their content, and how posts from\ncounter-extremism agencies compare to them. In particular, this study will\napply computational techniques to analyse the narratives of various\npro-extremist and counter-extremist Twitter accounts, and investigate how the\npsychological motivation behind the messages compares between pro-ISIS and\ncounter-extremism narratives. Our findings show that pro-extremist accounts\noften use different strategies to disseminate content (such as the types of\nhashtags used) when compared to counter-extremist accounts across different\ntypes of organisations, including accounts of governments and NGOs. Through\nthis study, we provide unique insights into both extremist and\ncounter-extremist narratives on social media platforms. Furthermore, we define\nseveral avenues for discussion regarding the extent to which counter-messaging\nmay be effective at diminishing the online influence of extremist and other\ncriminal organisations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000057949,
      "text":"A Standardized Radiograph-Agnostic Framework and Platform For Evaluating\n  AI Radiological Systems\n\n  Radiology has been essential to accurately diagnosing diseases and assessing\nresponses to treatment. The challenge however lies in the shortage of\nradiologists globally. As a response to this, a number of Artificial\nIntelligence solutions are being developed. The challenge Artificial\nIntelligence radiological solutions however face is the lack of a benchmarking\nand evaluation standard, and the difficulties of collecting diverse data to\ntruly assess the ability of such systems to generalise and properly handle edge\ncases. We are proposing a radiograph-agnostic platform and framework that would\nallow any Artificial Intelligence radiological solution to be assessed on its\nability to generalise across diverse geographical location, gender and age\ngroups.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.1104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Machine Reasoning to Assess Pandemics Risks: Case of USS Theodore\n  Roosevelt\n\n  Assessment of risks of pandemics to communities and workplaces requires an\nintelligent decision support system (DSS). The core of such DSS must be based\non machine reasoning techniques such as inference and shall be capable of\nestimating risks and biases in decision making. In this paper, we use a causal\nnetwork to make Bayesian inference on COVID-19 data, in particular, assess\nrisks such as infection rate and other precaution indicators. Unlike other\nstatistical models, a Bayesian causal network combines various sources of data\nthrough joint distribution, and better reflects the uncertainty of the\navailable data. We provide an example using the case of the COVID-19 outbreak\nthat happened on board of USS Theodore Roosevelt in early 2020.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11262,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Student Teamwork on Programming Projects: What can GitHub logs show us?\n\n  Teamwork, often mediated by version control systems such as Git and Apache\nSubversion (SVN), is central to professional programming. As a consequence,\nmany colleges are incorporating both collaboration and online development\nenvironments into their curricula even in introductory courses. In this\nresearch, we collected GitHub logs from two programming projects in two\nofferings of a CS2 Java programming course for computer science majors.\nStudents worked in pairs for both projects (one optional, the other mandatory)\nin each year. We used the students' GitHub history to classify the student\nteams into three groups, collaborative, cooperative, or solo-submit, based on\nthe division of labor. We then calculated different metrics for students'\nteamwork including the total number and the average number of commits in\ndifferent parts of the projects and used these metrics to predict the students'\nteamwork style. Our findings show that we can identify the students' teamwork\nstyle automatically from their submission logs. This work helps us to better\nunderstand novices' habits while using version control systems. These habits\ncan identify the harmful working styles among them and might lead to the\ndevelopment of automatic scaffolds for teamwork and peer support in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07326,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Progressing Towards Responsible AI\n\n  The field of Artificial Intelligence (AI) and, in particular, the Machine\nLearning area, counts on a wide range of performance metrics and benchmark data\nsets to assess the problem-solving effectiveness of its solutions. However, the\nappearance of research centres, projects or institutions addressing AI\nsolutions from a multidisciplinary and multi-stakeholder perspective suggests a\nnew approach to assessment comprising ethical guidelines, reports or tools and\nframeworks to help both academia and business to move towards a responsible\nconceptualisation of AI. They all highlight the relevance of three key aspects:\n(i) enhancing cooperation among the different stakeholders involved in the\ndesign, deployment and use of AI; (ii) promoting multidisciplinary dialogue,\nincluding different domains of expertise in this process; and (iii) fostering\npublic engagement to maximise a trusted relation with new technologies and\npractitioners. In this paper, we introduce the Observatory on Society and\nArtificial Intelligence (OSAI), an initiative grew out of the project AI4EU\naimed at stimulating reflection on a broad spectrum of issues of AI (ethical,\nlegal, social, economic and cultural). In particular, we describe our work in\nprogress around OSAI and suggest how this and similar initiatives can promote a\nwider appraisal of progress in AI. This will give us the opportunity to present\nour vision and our modus operandi to enhance the implementation of these three\nfundamental dimensions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11084,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Historical Context and Key Features of Digital Money Tokens\n\n  Digital money tokens have attracted the attention of financial institutions,\ncentral banks, regulators, international associations and fintechs. Their\nresearch and experimentation with digital money tokens has included creating\ninnovative technical and operational frameworks. In this paper, we present a\n'money tree' which places this recent concept of digital money tokens into a\nhistorical context by illustrating their evolution from more traditional forms\nof money. We then identify key features of digital money tokens with options\nand examples. We hope this paper will be of interest to the financial services\nindustry and we look forward to feedback.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02851,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"A no-phone\/no-app contact tracing hardware token\n\n  We report the development of an open-source, hardware-based contact tracer,\nmade from readily available parts, costing less than $20USD. This work was\nmotivated by the need for a technology-assisted contact tracer that avoids\nprivacy issues found with those involving a mobile phone. Contact tracing is\ndone here without the use of a mobile phone or an app at all. Instead, contact\ntracing is implemented using Bluetooth Low Energy on an ESP32 micro-controller.\nThe ESP32 is used to both advertise and receive health information to others in\nclose proximity, forming a strictly peer-to-peer contact tracer. The contact\ntracer can be assembled by an individual and configured use within minutes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.10274,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Understanding the online behavior and risks of children: results of a\n  large-scale national survey on 10-18 year olds\n\n  The Internet has opened up new horizons of knowledge, communication and\nentertainment in our lives. Through this, young people are presented with a\nwealth of opportunities and activities that can enhance their skills and\nempower their knowledge and creativity. However, the online engagement of young\npeople often comes with significant risks, encountered by children accidentally\nor deliberately. The emergence of new online services at an unprecedented speed\nand innovation brings the need, internationally, for a constant monitoring and\ninvestigation of the rapidly changing landscape and the associated emerging\nrisk factors that could potentially jeopardize children's development,\nopportunities and lives. The Greek Safer Internet Center conducted two\nlarge-scale surveys to understand children's internet engagement, aiming to\ncontribute towards improved child protection policies that could guide the\nefforts of key stakeholders towards a safer cyberspace. The first survey took\nplace at the end of 2018, with the approval of the Greek Ministry of Education\nand Religious Affairs, and was conducted online among 14,000 pupils aged 10-18\nyears from 400 schools spread in five different urban areas of Greece. A follow\nup survey was realized the following year, among 13,000 students of the same\nage group from 500 school units in six different prefectures of Greece. To our\nknowledge, it is the first tie national surveys of such scale are conducted in\nGreece. The paper presents the analysis of the collected data, and describe the\nunderlined methodology based on which the survey was formulated and conducted\naccording to international standards, around specific thematic areas, namely\ninternet use and online behavior, parental engagement, confidence level of\nchildren, digital literacy, social media, and online risks. The results were\nmainly analysed based on educational level and gender.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00229,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Standardized Green View Index and Quantification of Different Metrics of\n  Urban Green Vegetation\n\n  Urban greenery is considered an important factor in relation to sustainable\ndevelopment and people's quality of life in the city. Although ways to measure\nurban greenery have been proposed, the characteristics of each metric have not\nbeen fully established, rendering previous researches vulnerable to changes in\ngreenery metrics. To make estimation more robust, this study aims to (1)\npropose an improved indicator of greenery visibility for analytical use\n(standardized GVI; sGVI), and (2) quantify the relation between sGVI and other\ngreenery metrics. Analyzing a data set for Yokohama city, Japan, it is shown\nthat the sGVI, a weighted form of GVI aggregated to an area, mitigates the bias\nof densely located measurement sites. Also, by comparing sGVI and NDVI at city\nblock level, we found that sGVI captures the presence of vegetation better in\nthe city center, whereas NDVI is better in capturing vegetation in parks and\nforests. These tools provide a foundation for accessing the effect of\nvegetation in urban landscapes in a more robust matter, enabling comparison on\nany arbitrary geographical scale.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02359,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"Risk, Trust, and Bias: Causal Regulators of Biometric-Enabled Decision\n  Support\n\n  Biometrics and biometric-enabled decision support systems (DSS) have become a\nmandatory part of complex dynamic systems such as security checkpoints,\npersonal health monitoring systems, autonomous robots, and epidemiological\nsurveillance. Risk, trust, and bias (R-T-B) are emerging measures of\nperformance of such systems. The existing studies on the R-T-B impact on system\nperformance mostly ignore the complementary nature of R-T-B and their causal\nrelationships, for instance, risk of trust, risk of bias, and risk of trust\nover biases. This paper offers a complete taxonomy of the R-T-B causal\nperformance regulators for the biometric-enabled DSS. The proposed novel\ntaxonomy links the R-T-B assessment to the causal inference mechanism for\nreasoning in decision making. Practical details of the R-T-B assessment in the\nDSS are demonstrated using the experiments of assessing the trust in synthetic\nbiometric and the risk of bias in face biometrics. The paper also outlines the\nemerging applications of the proposed approach beyond biometrics, including\ndecision support for epidemiological surveillance such as for COVID-19\npandemics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00371,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"An\\'alisis jur\\'idico de la discriminaci\\'on algor\\'itmica en los\n  procesos de selecci\\'on laboral\n\n  The use of machine learning systems in processing job applications has made\nthe process agile and efficient, but at the same time it has created problems\nin terms of equality, reliability and transparency. In this paper we explain\nsome of the uses of ML in job selection processes in the United States, and we\npresent some the racial and sexual biases that have been detected. There are\nboth practical and legal obstacles that impede the detection and analysis of\nthese biases. It is also unclear how to approach algorithmic discrimination\nfrom a legal point of view. A possible analytical tool is provided by the\nAmerican doctrine of Disparate Impact, but we show some of its limitations and\nproblems when adapted to other legal systems, such as Colombian law. To\nconclude, we offer some desiderata that any legal analysis of algorithmic\ndiscrimination should provide.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02653,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"OpenStreetMap data use cases during the early months of the COVID-19\n  pandemic\n\n  Created by volunteers since 2004, OpenStreetMap (OSM) is a global geographic\ndatabase available under an open access license and currently used by a\nmultitude of actors worldwide. This chapter describes the role played by OSM\nduring the early months (from January to July 2020) of the ongoing COVID-19\npandemic, which - in contrast to past disasters and epidemics - is a global\nevent impacting both developed and developing countries. A large number of\nCOVID-19-related OSM use cases were collected and grouped into a number of\nresearch frameworks which are analyzed separately: dashboards and services\nsimply using OSM as a basemap, applications using raw OSM data, initiatives to\ncollect new OSM data, imports of authoritative data into OSM, and traditional\nacademic research on OSM in the COVID-19 response. The wealth of examples\nprovided in the chapter, including an analysis of OSM tile usage in two\ncountries (Italy and China) deeply affected in the earliest months of 2020,\nprove that OSM has been and still is heavily used to address the COVID-19\ncrisis, although with types and mechanisms that are often different depending\non the affected area or country and the related communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09803,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"COVID-19 Pandemic Outbreak in the Subcontinent: A data-driven analysis\n\n  Human civilization is experiencing a critical situation that presents itself\nfor a new coronavirus disease 2019 (COVID-19). This virus emerged in late\nDecember 2019 in Wuhan city, Hubei, China. The grim fact of COVID-19 is, it is\nhighly contagious in nature, therefore, spreads rapidly all over the world and\ncauses severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Responding\nto the severity of COVID-19 research community directs the attention to the\nanalysis of COVID-19, to diminish its antagonistic impact towards society.\nNumerous studies claim that the subcontinent, i.e., Bangladesh, India, and\nPakistan, could remain in the worst affected region by the COVID-19. In order\nto prevent the spread of COVID-19, it is important to predict the trend of\nCOVID-19 beforehand the planning of effective control strategies.\nFundamentally, the idea is to dependably estimate the reproduction number to\njudge the spread rate of COVID-19 in a particular region. Consequently, this\npaper uses publicly available epidemiological data of Bangladesh, India, and\nPakistan to estimate the reproduction numbers. More specifically, we use\nvarious models (for example, susceptible infection recovery (SIR), exponential\ngrowth (EG), sequential Bayesian (SB), maximum likelihood (ML) and time\ndependent (TD)) to estimate the reproduction numbers and observe the model\nfitness in the corresponding data set. Experimental results show that the\nreproduction numbers produced by these models are greater than 1.2\n(approximately) indicates that COVID-19 is gradually spreading in the\nsubcontinent.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11344,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Data Mining Approach to Analyze Covid19 Dataset of Brazilian Patients\n\n  The pandemic originated by coronavirus(covid-19), name coined by World Health\nOrganization during the first month in 2020. Actually, almost all the countries\npresented covid19 positive cases and governments are choosing different health\npolicies to stop the infection and many research groups are working on patients\ndata to understand the virus, at the same time scientists are looking for a\nvacuum to enhance imnulogy system to tack covid19 virus. One of top countries\nwith more infections is Brazil, until August 11 had a total of 3,112,393 cases.\nResearch Foundation of Sao Paulo State(Fapesp) released a dataset, it was an\ninnovative in collaboration with hospitals(Einstein, Sirio-Libanes),\nlaboratory(Fleury) and Sao Paulo University to foster reseach on this trend\ntopic. The present paper presents an exploratory analysis of the datasets,\nusing a Data Mining Approach, and some inconsistencies are found, i.e. NaN\nvalues, null references values for analytes, outliers on results of analytes,\nencoding issues. The results were cleaned datasets for future studies, but at\nleast a 20\\% of data were discarded because of non numerical, null values and\nnumbers out of reference range.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.07383,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000082784,
      "text":"On Digital Currency and the Transfer of World Wealth and Technology\n  Centers\n\n  The emergence and transfer of wealth promote the evolution of civilizations.\nThrough the pursuit of the form of wealth valued by the members of society, the\nself-assertiveness demands of a society can be met and thus stimulate\ncreativity. As means of overdrawing the future, sovereign currency and bonds\nhave gradually become modern forms of wealth and have strongly promoted\nscientific and technological progress and social development. However, due to\nthe unequal distribution of wealth, the sustainability of sovereign currency\nand bonds is not certain. The world has been changing rapidly since the\noutbreak of COVID-19, and new forms of wealth need to be constructed as an\nextension of the Self of the masses, among which digital currency may be an\neffective carrier of value. China is on an upward trajectory, and the complex\nand volatile global environment can provide an opportunity for China to focus\non developing aspects of its science and technology, optimize its system of\ngovernance and strengthen its internal driving force.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.01611,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Machine Learning in Ethnobotany -- a first experiment\n\n  We describe new opportunities afforded by bring A.I to the field of\nethnobotany. In particular we describe a novel approach to ethnobotany\ndocumentation that harnesses machine learning opportunities, specifically for\nthe documentation of traditional ecological knowledge with mobile phones in\nemerging economies. Using a case study on the island of Bali as a departure\npoint, the project maps out machine learning approaches to documentation and\nresponds to technology and capital gradients between research contexts in the\nglobal north and south in an attempt to capture knowledge that might otherwise\nnot be represented.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12099,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Penerapan Metode SVM-Based Machine Learning Untuk Menganalisa Pengguna\n  Data Trafik Internet (Studi Kasus Jaringan Internet Wlan Mahasiswa Bina\n  Darma)\n\n  Internet usage is an important requirement that supports the performance and\nactivities on campus. To control internet usage, it is necessary to know the\ndistribution of internet usage. By utilizing a number of machine learning\nalgorithms and WEKA software, the research is carried out by observation and\ntaking data from wifi hotspots on campus. The classification method using\nSVM-Based utilizes the classification method owned by Support Vector Machine\n(SVM). This study aims to classify data on internet usage so that from this\nclassification can be known destination network, protocol, and bandwidth that\nare widely accessed at certain times. Internet traffic data is retrieved\nthrough Wireshark software. Whereas data processing and data processing of\ninternet traffic is processed by WEKA. The results showed: 1) UBD internet\nusage in the week I 133,196 users, week II 304,042 users,2) Use of Destination\nNetwork 24,150 and Use of Protocol 37,321,3) Destination networks that are\noften addressed are 172.21.206.143 (the week I) and 172.21.172.234 (week II),\nprotocols that are often used by TCP, and4) SVM method is a good data mining\nmethod for classifying network packet patterns so as to produce network traffic\nclassification according to destination network and protocol.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.03418,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000073844,
      "text":"Service Ecosystem: A Lens of Smart Society\n\n  Intelligence services are playing an increasingly important role in the\noperation of our society. Exploring the evolution mechanism, boundaries and\nchallenges of service ecosystem is essential to our ability to realize smart\nsociety, reap its benefits and prevent potential risks. We argue that this\nnecessitates a broad scientific research agenda to study service ecosystem that\nincorporates and expands upon the disciplines of computer science and includes\ninsights from across the sciences. We firstly outline a set of research issues\nthat are fundamental to this emerging field, and then explores the technical,\nsocial, legal and institutional challenges on the study of service ecosystem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09065,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000100003,
      "text":"A Distributed Framework to Orchestrate Video Analytics Applications\n\n  The concept of the Internet of Things (IoT) is a reality now. This paradigm\nshift has caught everyones attention in a large class of applications,\nincluding IoT-based video analytics using smart doorbells. Due to its growing\napplication segments, various efforts exist in scientific literature and many\nvideo-based doorbell solutions are commercially available in the market.\nHowever, contemporary offerings are bespoke, offering limited composability and\nreusability of a smart doorbell framework. Second, they are monolithic and\nproprietary, which means that the implementation details remain hidden from the\nusers. We believe that a transparent design can greatly aid in the development\nof a smart doorbell, enabling its use in multiple application domains.\n  To address the above-mentioned challenges, we propose a distributed framework\nto orchestrate video analytics across Edge and Cloud resources. We investigate\ntrade-offs in the distribution of different software components over a\nbespoke\/full system, where components over Edge and Cloud are treated\ngenerically. This paper evaluates the proposed framework as well as the\nstate-of-the-art models and presents comparative analysis of them on various\nmetrics (such as overall model accuracy, latency, memory, and CPU usage). The\nevaluation result demonstrates our intuition very well, showcasing that the\nAWS-based approach exhibits reasonably high object-detection accuracy, low\nmemory, and CPU usage when compared to the state-of-the-art approaches, but\nhigh latency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.03997,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"The development and implementation of a PhD Program in ICT for the\n  Kosovo Education System\n\n  Despite ever accelerating workplace changes, including rapidly expanding\ntechnological access and fast improving information and communication systems,\nthe education system in Kosovo is not fully developed enough to provide a\nhigh-quality research-based education in Information and Communication\nTechnology. Coping simultaneously with varied national priorities, Kosovo, a\nsmall country with 2 million inhabitants and a national budget of only 2.3\nbillion, lacks the needed investments to fundamentally transform the quality of\nthe education system.\n  A funded ICT doctoral program would address todays workforce priorities and\nrequirements. The design and delivery of a national PhD program in ICT is\ncrucial for Kosovo in order to ensure competitive readiness within the regional\neducation systems and national economies of the West Balkans, and beyond. This\npaper argues the need for PhD programs and offers insights into a proposed\nproject, the aim of which is to put Kosovo on the map by offering a PhD in the\nICT field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01713,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Unique Exams: Designing assessments for integrity and fairness\n\n  Educators have faced new challenges in effective course assessment during the\nrecent, unprecedented shift to remote online learning during the COVID-19\npandemic. In place of typical proctored, timed exams, instructors must now\nrethink their methodology for assessing course-level learning goals. Are exams\nappropriate---or even feasible---in this new online, open-internet learning\nenvironment? In this experience paper, we discuss the unique exams framework:\nour framework for upholding exam integrity and student privacy. In our\nProbability for Computer Scientists Course at an R1 University, we developed\nautogenerated, unique exams where each student had the same four problem\nskeletons with unique numeric variations per problem. Without changing the\nprocess of the traditional exam, unique exams provide a layer of security for\nboth students and instructors about exam reliability for any classroom\nenvironment---in-person or online. In addition to sharing our experience\ndesigning unique exams, we also present a simple end-to-end tool and example\nquestion templates for different CS subjects that other instructors can adapt\nto their own courses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09073,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Running the COVID-19 marathon: the behavioral adaptations in mobility\n  and facemask over 27 weeks of pandemic in Seoul, South Korea\n\n  Battle with COVID-19 turned out to be a marathon, not a sprint, and\nbehavioral adjustments have been unavoidable to stay viable. In this paper, we\nemploy a data-centric approach to investigate individual mobility adaptations\nand mask-wearing in Seoul, South Korea. We first identify six epidemic phases\nand two waves based on COVID-19 case count and its geospatial dispersion. The\nphase-specific linear models reveal the strong, self-driven mobility reductions\nin the first escalation and peak with a common focus on public transit use and\nless-essential weekend\/afternoon trips. However, comparable reduction was not\npresent in the second wave, as the shifted focus from mobility to mask-wearing\nwas evident. Although no lockdowns and gentle nudge to wear mask seemed\ncounter-intuitive, simple and persistent communication on personal safety has\nbeen effective and sustainable to induce cooperative behavioral adaptations.\nOur phase-specific analyses and interpretation highlight the importance of\ntargeted response consistent with the fluctuating epidemic risk.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01334,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Gender Stereotype Reinforcement: Measuring the Gender Bias Conveyed by\n  Ranking Algorithms\n\n  Search Engines (SE) have been shown to perpetuate well-known gender\nstereotypes identified in psychology literature and to influence users\naccordingly. Similar biases were found encoded in Word Embeddings (WEs) learned\nfrom large online corpora. In this context, we propose the Gender Stereotype\nReinforcement (GSR) measure, which quantifies the tendency of a SE to support\ngender stereotypes, leveraging gender-related information encoded in WEs.\nThrough the critical lens of construct validity, we validate the proposed\nmeasure on synthetic and real collections. Subsequently, we use GSR to compare\nwidely-used Information Retrieval ranking algorithms, including lexical,\nsemantic, and neural models. We check if and how ranking algorithms based on\nWEs inherit the biases of the underlying embeddings. We also consider the most\ncommon debiasing approaches for WEs proposed in the literature and test their\nimpact in terms of GSR and common performance measures. To the best of our\nknowledge, GSR is the first specifically tailored measure for IR, capable of\nquantifying representational harms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.1247,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000102652,
      "text":"Effective Voice: Beyond Exit and Affect in Online Communities\n\n  Online communities provide ample opportunities for user self-expression but\ngenerally lack the means for average users to exercise direct control over\ncommunity policies. This paper sets out to identify a set of strategies and\ntechniques through which the voices of participants might be better heard\nthrough defined mechanisms for institutional governance. Drawing on Albert O.\nHirschman's distinction between \"exit\" and \"voice\" in institutional life, it\nintroduces a further distinction between two kinds of participation: effective\nvoice, as opposed to the far more widespread practices of affective voice.\nEffective voice is a form of individual or collective speech that brings about\na binding effect according to transparent processes. Platform developers and\nresearchers might explore this neglected form of voice by introducing\nmechanisms for authority and accountability, collective action, and community\nevolution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.04885,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"\"Is it a Qoincidence?\": An Exploratory Study of QAnon on Voat\n\n  Online fringe communities offer fertile grounds for users seeking and sharing\nideas fueling suspicion of mainstream news and conspiracy theories. Among\nthese, the QAnon conspiracy theory emerged in 2017 on 4chan, broadly supporting\nthe idea that powerful politicians, aristocrats, and celebrities are closely\nengaged in a global pedophile ring. Simultaneously, governments are thought to\nbe controlled by \"puppet masters,\" as democratically elected officials serve as\na fake showroom of democracy.\n  This paper provides an empirical exploratory analysis of the QAnon community\non Voat.co, a Reddit-esque news aggregator, which has captured the interest of\nthe press for its toxicity and for providing a platform to QAnon followers.\nMore precisely, we analyze a large dataset from \/v\/GreatAwakening, the most\npopular QAnon-related subverse (the Voat equivalent of a subreddit), to\ncharacterize activity and user engagement. To further understand the discourse\naround QAnon, we study the most popular named entities mentioned in the posts,\nalong with the most prominent topics of discussion, which focus on US politics,\nDonald Trump, and world events. We also use word embeddings to identify\nnarratives around QAnon-specific keywords. Our graph visualization shows that\nsome of the QAnon-related ones are closely related to those from the Pizzagate\nconspiracy theory and so-called drops by \"Q.\" Finally, we analyze content\ntoxicity, finding that discussions on \/v\/GreatAwakening are less toxic than in\nthe broad Voat community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00549,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"Bubble Storytelling with Automated Animation: A Brexit Hashtag Activism\n  Case Study\n\n  Hashtag data are common and easy to acquire. Thus, they are widely used in\nstudies and visual data storytelling. For example, a recent story by China\nCentral Television Europe (CCTV Europe) depicts Brexit as a hashtag movement\ndisplayed on an animated bubble chart. However, creating such a story is\nusually laborious and tedious, because narrators have to switch between\ndifferent tools and discuss with different collaborators. To reduce the burden,\nwe develop a prototype system to help explore the bubbles' movement by\nautomatically inserting animations connected to the storytelling of the video\ncreators and the interaction of viewers to those videos. We demonstrate the\nusability of our method through both use cases and a semi-structured user\nstudy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Technological Platform for the Prevention and Management of Healthcare\n  Associated Infections and Outbreaks\n\n  Hospital acquired infections are infections that occur in patients during\nhospitalization, which were not present at the time of admission. They are\namong the most common adverse events in healthcare around the world, leading to\nincreased mortality and morbidity rates, prolonged hospitalization periods and\nconsiderable financial burden on both hospitals and patients. Preventive\nguidelines and regulations have been devised, however compliance to these is\nfrequently poor and there is much room for improvement. This paper presents the\nprototype of an extensible, configurable cyber-physical system, developed under\nEuropean Union funding, that will assist in the prevention of hospital\ninfections and outbreaks. Integrating a wireless sensor network for the\nsurveillance of clinical processes with configurable monitoring software built\naround a workflow engine as key component, our solution detects deviations from\nestablished hygiene practices and provides real-time information and alerts\nwhenever an infection risk is discovered. The platform is described from both\nhardware and software perspective, with emphasis on the wireless network's\nelements as well as the most important software components. Furthermore, two\nclinical workflows of different complexity, which are included in the system\nprototype are detailed. The finalized system is expected to facilitate the\ncreation and automated monitoring of clinical workflows that are associated\nwith over 90% of hospital infections.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.02093,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Cyber-Physical Platform for Preeclampsia Detection\n\n  Hypertension-related conditions are the most prevalent complications of\npregnancy worldwide. They manifest in up to 8% of cases and if left untreated,\ncan lead to serious detrimental effects. Early detection of their sudden onset\ncan help physicians alleviate the condition and improve outcomes for both\nwould-be mother and baby. Today's prevalence of smartphones and cost-effective\nwearable technology provide new opportunities for individualized medicine.\nExisting devices promote heart health, they monitor and encourage physical\nactivity and measure sleep quality. This builds interest and encourages users\nto require more advanced features. We believe these aspects form suitable\nconditions to create and market specialized wearable devices. The present paper\ndetails a cyber-physical system built around an intelligent bracelet for\nmonitoring hypertension-related conditions tailored to pregnant women. The\nbracelet uses a microfluidic layer that is compressed by the blood pressing\nagainst the arterial wall. Integrated sensors register the waveform and send it\nto the user's smartphone, where the systolic and diastolic values are\ndetermined. The system is currently developed under European Union research\nfunding, and includes a software server where data is stored and further\nprocessing is carried out through machine learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.10617,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"An Enhanced Geo Location Technique for Social Network Communication\n  System\n\n  Social networks have become very popular in recent years because of the\nincreasing large number and affordability of internet enabled gadgets such as\npersonal computers, mobile devices and internet tablets. It has been observed\nthat the tempo of fraud in social media these days is over alarming most\nespecially in Nigeria. As a result of this, there is need to fortify the social\nnetwork services in order to secure e-mail communication and reinforce data\nsecurity. This paper advocates for an advanced and secured approach for\nimproving communication in a social Network with the use of geo-location\ntechnique. The system was designed using an Object-Oriented software\ndevelopment methodology and implemented using the server-based scripting\nlanguage - PHP, Cascading Style Sheets (CSS) and back-end with MySQL. The\nproposed system will help the government and security agencies fight recent\nsecurity challenges in the country.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.11669,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Forecasting for Social Good\n\n  Forecasting plays a critical role in the development of organisational\nbusiness strategies. Despite a considerable body of research in the area of\nforecasting, the focus has largely been on the financial and economic outcomes\nof the forecasting process as opposed to societal benefits. Our motivation in\nthis study is to promote the latter, with a view to using the forecasting\nprocess to advance social and environmental objectives such as equality, social\njustice and sustainability. We refer to such forecasting practices as\nForecasting for Social Good (FSG) where the benefits to society and the\nenvironment take precedence over economic and financial outcomes. We\nconceptualise FSG and discuss its scope and boundaries in the context of the\n\"Doughnut theory\". We present some key attributes that qualify a forecasting\nprocess as FSG: it is concerned with a real problem, it is focused on advancing\nsocial and environmental goals and prioritises these over conventional measures\nof economic success, and it has a broad societal impact. We also position FSG\nin the wider literature on forecasting and social good practices. We propose an\nFSG maturity framework as the means to engage academics and practitioners with\nresearch in this area. Finally, we highlight that FSG: (i) cannot be distilled\nto a prescriptive set of guidelines, (ii) is scalable, and (iii) has the\npotential to make significant contributions to advancing social objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.12979,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Moral Framing and Ideological Bias of News\n\n  News outlets are a primary source for many people to learn what is going on\nin the world. However, outlets with different political slants, when talking\nabout the same news story, usually emphasize various aspects and choose their\nlanguage framing differently. This framing implicitly shows their biases and\nalso affects the reader's opinion and understanding. Therefore, understanding\nthe framing in the news stories is fundamental for realizing what kind of view\nthe writer is conveying with each news story. In this paper, we describe\nmethods for characterizing moral frames in the news. We capture the frames\nbased on the Moral Foundation Theory. This theory is a psychological concept\nwhich explains how every kind of morality and opinion can be summarized and\npresented with five main dimensions. We propose an unsupervised method that\nextracts the framing Bias and the framing Intensity without any external\nframing annotations provided. We validate the performance on an annotated\ntwitter dataset and then use it to quantify the framing bias and partisanship\nof news.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09987,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Synthetic Control, Synthetic Interventions, and COVID-19 spread:\n  Exploring the impact of lockdown measures and herd immunity\n\n  The synthetic control method is an empirical methodology forcausal inference\nusing observational data. By observing thespread of COVID-19 throughout the\nworld, we analyze the dataon the number of deaths and cases in different\nregions usingthe power of prediction, counterfactual analysis, and\nsyntheticinterventions of the synthetic control and its extensions. Weobserve\nthat the number of deaths and cases in different re-gions would have been much\nsmaller had the lockdowns beenimposed earlier and had the re-openings been done\nlater, es-pecially among indoor bars and restaurants. We also analyzethe\nspeculated impact of herd immunity on the spread giventhe population of each\nregion and show that lockdown policieshave a very strong impact on the spread\nregardless of the levelof prior infections.\n  Our most up-to-date code, model, and data can be foundon github:\nhttps:\/\/github.com\/niloofarbayat\/COVID19-synthetic-control-analysis\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.05718,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Recent advances in Blockchain Technology: A survey on Applications and\n  Challenges\n\n  The rise of blockchain technology within a few years has attracted\nresearchers across the world. The prime reason for worldwide attention is\nundoubtedly due to its feature of immutability along with the decentralized\napproach of data protection. As this technology is progressing, lots of\ndevelopments in terms of identifying new applications, blockchain-based\nplatforms, consensus mechanisms, etc are taking place. Hence, in this article,\nan attempt has been made to review the recent advancements in blockchain\ntechnology. Furthermore, we have also explored the available blockchain\nplatforms, highlighted and explored future research directions and challenges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.13626,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Monitoring My Dehydration: A Non-Invasive Dehydration Alert System Using\n  Electrodermal Activity\n\n  Staying hydrated and drinking fluids is extremely crucial to stay healthy and\nmaintaining even basic bodily functions. Studies have shown that dehydration\nleads to loss of productivity, cognitive impairment and mood in both men and\nwomen. However, there are no such an existing tool that can monitor dehydration\ncontinuously and provide alert to users before it affects on their health. In\nthis paper, we propose to utilize wearable Electrodermal Activity (EDA) sensors\nin conjunction with signal processing and machine learning techniques to\ndevelop first time ever a dehydration self-monitoring tool, \\emph{Monitoring My\nDehydration} (MMD), that can instantly detect the hydration level of human\nskin. Moreover, we develop an Android application over Bluetooth to connect\nwith wearable EDA sensor integrated wristband to track hydration levels of the\nusers real-time and instantly alert to the users when the hydration level goes\nbeyond the danger level. To validate our developed tool's performance, we\nrecruit 5 users, carefully designed the water intake routines to annotate the\ndehydration ground truth and trained state-of-art machine learning models to\npredict instant hydration level i.e., well-hydrated, hydrated, dehydrated and\nvery dehydrated. Our system provides an accuracy of 84.5% in estimating\ndehydration level with an sensitivity of 87.5% and a specificity of 90.3% which\nprovides us confidence of moving forward with our method for larger\nlongitudinal study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00597,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Return to Bali\n\n  This paper gives an overview of the project Return to Bali that seeks to\ncreate a living dataset of ethnobotanically significant flora on the island of\nBali and new methods through which underrepresented forms of knowledge can be\ndocumented, shared and made compatible within the logics of machine learning\nwhile considering practical approaches to benefit multiple stakeholders and\npreventing unintended harm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09071,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000344051,
      "text":"Measurement in AI Policy: Opportunities and Challenges\n\n  As artificial intelligence increasingly influences our world, it becomes\ncrucial to assess its technical progress and societal impact. This paper\nsurveys problems and opportunities in the measurement of AI systems and their\nimpact, based on a workshop held at Stanford University in the fall of 2019. We\nidentify six summary challenges inherent to measuring the progress and impact\nof AI, and summarize over 40 presentations and associated discussions from the\nworkshop. We hope this can inspire research agendas in this crucial area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.06989,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000102652,
      "text":"Review the Enterprise Resource Planning in Moroccan Healthcare\n  Organizations\n\n  The Hospital Information Systems (HIS) in Morocco take a central place in the\nprocess of patient care. An approach is made to analyze the current situation\nof the HIS within the institutions in order to bring an integral and generic\nvision, allowing the judicious articulation of the business and IT layers.\nCurrently, the Enterprise Resource Planning (ERP) implemented remains a system\nconsisting of several applications dedicated to specific areas. These systems\nhave become an indispensable element within any hospital. The goal of our study\nis to discover how the ERP has been used in Moroccan healthcare sector and how\nthese software should be implemented and used to improve healthcare services.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09102,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Amazon Fake Reviews\n\n  Often, there are suspicious Amazon reviews that seem to be excessively\npositive or have been created through a repeating algorithm. I moved to detect\nfake reviews on Amazon through semantic analysis in conjunction with meta data\nsuch as time, word choice, and the user who posted. I first came up with\nseveral instances that may indicate a review isn't genuine and constructed what\nthe algorithm would look like. Then I coded the algorithm and tested the\naccuracy of it using statistical analysis and analyzed it based on the six\nqualities of code.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.00251,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000171529,
      "text":"Fragmented digital connectivity and security at sea\n\n  This paper explores how uneven and often unreliable digital connections shape\nthe patterns and routines of everyday life, work and rest for seafarers, during\nlong periods at sea. Such fragmented connections, which surface when the ship\nmoves in and out of connectivity or when onboard data allowances run out,\ncreate a series of uncertainties that might unsettle individual and collective\nnotions of security. Ethnographic in nature, the study engaged 43 seafarers on\nboard two container ships in European waters, during two two-week voyages\nbetween February and April 2018. This provided an empirically grounded\nexploration of how digitally facilitated connections, relations and networks,\nenabled through increasingly connected ships, shape and reshape seafarer lives.\nFindings from this study demonstrate the creative ways in which seafarers\nnavigate and negotiate digitally facilitated connections to maintain relational\nties with family and friends. The paper concludes by setting out future\nresearch directions and practical implications that speak to connectivity and\nsecurity at sea.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07848,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000130468,
      "text":"Towards a Flexible Framework for Algorithmic Fairness\n\n  Increasingly, scholars seek to integrate legal and technological insights to\ncombat bias in AI systems. In recent years, many different definitions for\nensuring non-discrimination in algorithmic decision systems have been put\nforward. In this paper, we first briefly describe the EU law framework covering\ncases of algorithmic discrimination. Second, we present an algorithm that\nharnesses optimal transport to provide a flexible framework to interpolate\nbetween different fairness definitions. Third, we show that important normative\nand legal challenges remain for the implementation of algorithmic fairness\ninterventions in real-world scenarios. Overall, the paper seeks to contribute\nto the quest for flexible technical frameworks that can be adapted to varying\nlegal and normative fairness constraints.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07018,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000064241,
      "text":"Singularity and Coordination Problems: Pandemic Lessons from 2020\n\n  Are there any indications that a Technological Singularity may be on the\nhorizon? In trying to answer these questions, the authors made a small\nintroduction to the area of safety research in artificial intelligence. The\nauthors review some of the current paradigms in the development of autonomous\nintelligent systems, searching for evidence that may indicate the coming of a\npossible Technological Singularity. Finally, the authors present a reflection\nusing the COVID-19 pandemic, something that showed that global society biggest\nproblem in managing existential risks is its lack of coordination skills as a\nglobal society.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07636,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Enhancement of the e-Invoicing Systems by Increasing the Efficiency of\n  Workflows via Disruptive Technologies\n\n  E-invoicing is a rapidly growing e-service in Europe as well as in the world.\nIt is identified as a substantially significant element in progressing towards\nthe goals of Digital Economy in the European Union.\n  This thesis focuses on identifying inefficiencies in e-invoicing systems\ncurrently in use and the opportunities to apply emerging technologies such as\nartificial intelligence and robotic process automation, in order to increase\nefficiency and level of automatization. The study incorporates expert opinions\nand users perceptions in e-invoicing systems on the status quo and the\nnecessities for higher automation. We focus on e-invoicing systems in the\nBaltic region consisting of the countries Estonia, Latvia and Lithuania. Based\non the conducted research, the drawbacks in e-invoicing systems were identified\nrelated to operational, technological and information security related.\nFurthermore, the automation opportunities and general requirements for\nautomation were identified. The functionalities that can be improved are\ndiscovered as well discussed in this thesis and the advantages of using\nemerging technologies in the context are explained. Based on research outcomes\nwe propose a conceptual e-invoicing ecosystem and present recommenda-tions for\nits application along the future work needed in that field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.15607,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"CRICTRS: Embeddings based Statistical and Semi Supervised Cricket Team\n  Recommendation System\n\n  Team Recommendation has always been a challenging aspect in team sports. Such\nsystems aim to recommend a player combination best suited against the\nopposition players, resulting in an optimal outcome. In this paper, we propose\na semi-supervised statistical approach to build a team recommendation system\nfor cricket by modelling players into embeddings. To build these embeddings, we\ndesign a qualitative and quantitative rating system which considers the\nstrength of opposition also for evaluating player performance. The embeddings\nobtained, describes the strengths and weaknesses of the players based on past\nperformances of the player. We also embark on a critical aspect of team\ncomposition, which includes the number of batsmen and bowlers in the team. The\nteam composition changes over time, depending on different factors which are\ntough to predict, so we take this input from the user and use the player\nembeddings to decide the best possible team combination with the given team\ncomposition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.12006,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000058611,
      "text":"Micromobility Trip Origin and Destination Inference Using General\n  Bikeshare Feed Specification (GBFS) Data\n\n  Emerging micromobility services (e.g., e-scooters) have a great potential to\nenhance urban mobility but more knowledge on their usage patterns is needed.\nThe General Bikeshare Feed Specification (GBFS) data are a possible source for\nexamining micromobility trip patterns, but efforts are needed to infer trips\nfrom the GBFS data. Existing trip inference methods are usually based upon the\nassumption that the vehicle ID of a micromobility option (e-scooter or e-bike)\ndoes not change, and so they cannot deal with data with vehicle IDs that change\nover time. In this study, we propose a comprehensive package of algorithms to\ninfer trip origins and destinations from GBFS data with different types of\nvehicle ID. We implement the algorithms in Washington DC by analyzing one-week\n(last week of February 2020) of GBFS data published by six vendors, and we\nevaluate the inference accuracy of the proposed algorithms by R-squared, mean\nabsolute error, and sum absolute error. We find that the R-squared measure is\nlarger than 0.9 and the MAE measure is smaller than 2 when the algorithms are\nevaluated with a 400m*400m grid, and the absolute errors are relatively larger\nin the downtown area. The accuracy of the trip-inference algorithms is\nsufficiently high for most practical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.00358,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"A Survey of H-index, Stress, Tenure & Reference Management software use\n  in Academia\n\n  We describe the findings of a survey that covered the topics of stress,\ncitation tool use habits, subjective happiness, h-index, research topic and\ntenure among a sample of 2286 authors of arxiv.org. Ph.D. students report the\nlowest subjective happiness score among all faculty roles, while tenured\nfaculty report the highest. Tenured faculty report the lowest levels of stress.\nUndergraduate and graduate students report the highest levels of stress.\nNon-tenured faculty report stress similar to postdocs. No association between\ncitation management tool usage and h-index was found. The average age at tenure\nstart is 34.9 years. In addition, no significant association between stress\nlevels and the research topic was found\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.04496,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Young Adult Unemployment Through the Lens of Social Media: Italy as a\n  case study\n\n  Youth unemployment rates are still in alerting levels for many countries,\namong which Italy. Direct consequences include poverty, social exclusion, and\ncriminal behaviours, while negative impact on the future employability and wage\ncannot be obscured. In this study, we employ survey data together with social\nmedia data, and in particular likes on Facebook Pages, to analyse personality,\nmoral values, but also cultural elements of the young unemployed population in\nItaly. Our findings show that there are small but significant differences in\npersonality and moral values, with the unemployed males to be less agreeable\nwhile females more open to new experiences. At the same time, unemployed have a\nmore collectivist point of view, valuing more in-group loyalty, authority, and\npurity foundations. Interestingly, topic modelling analysis did not reveal\nmajor differences in interests and cultural elements of the unemployed.\nUtilisation patterns emerged though; the employed seem to use Facebook to\nconnect with local activities, while the unemployed use it mostly as for\nentertainment purposes and as a source of news, making them susceptible to\nmis\/disinformation. We believe these findings can help policymakers get a\ndeeper understanding of this population and initiatives that improve both the\nhard and the soft skills of this fragile population.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07025,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"A window view quality assessment framework\n\n  The views that windows provide from inside a building affect human health and\nwell-being. Although window view is an important element of architecture, there\nis no established framework to guide its design. The literature is widely\ndispersed across different disciplinary fields, and there is a need to coalesce\nthis information into a framework that can be applied into the building design.\nBased on the literature, we present a framework for what constitutes 'view\nquality.' At the basis of our framework, we propose three primary variables:\nView Content (the assessment of visual features seen in the window view), View\nAccess (the measure of how much of the view can be seen through the window from\nthe occupant's position), and View Clarity (the assessment of how clear the\nview content appears in the window view when seen by an occupant). Each\nvariable was thematically derived from different sources including daylighting\nstandards, green certification systems, and scientific research studies. We\ndescribe the most important characteristics of each variable, and from our\nreview of the literature, we propose a conceptual index that can evaluate the\nquality of a window view. While discussing the index, we summarize design\nrecommendations for integrating these three variables into the building process\nand identify knowledge gaps for future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.10015,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Algodynamics: Teaching Algorithms using Interactive Transition Systems\n\n  The importance of algorithms and data structures in computer science\ncurricula has been amply recognized. For many students, however, gaining a good\nunderstanding of algorithms remains a challenge.\n  Because of the automated nature of sequential algorithms. there is an\ninherent tension in directly applying the `learning by doing' approach. This\npartly explains the limitations of efforts like algorithm animation and code\ntracing.\n  Algodynamics, the approach we propose and advocate, situates algorithms\nwithin the framework of transition systems and their dynamics and offers an\nattractive approach for teaching algorithms. Algodynamics starts with the\npremise that the key ideas underlying an algorithm can be identified and\npackaged into interactive transition systems. The algorithm when `opened up',\nreveals a transition system, shorn of most control aspects, enriched instead\nwith interaction. The design of an algorithm can be carried out by constructing\na series of interactive systems, progressively trading interactivity with\nautomation. These transition systems constitute a family of notional machines.\n  We illustrate the algodynamics approach by considering Bubblesort. A sequence\nof five interactive transition systems culminate in the classic Bubblesort\nalgorithm. The exercise of constructing the individual systems also pays off\nwhen coding Bubblesort: a highly modular implementation whose primitives are\nborrowed from the transition systems. The transition systems used for\nBubblesort have been implemented as interactive experiments. These web based\nimplementations are easy to build. The simplicity and flexibility afforded by\nthe algodynamics framework makes it an attractive option to teach algorithms in\nan interactive way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.02359,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000083778,
      "text":"Modeling Traffic Congestion in Developing Countries using Google Maps\n  Data\n\n  Traffic congestion research is on the rise, thanks to urbanization, economic\ngrowth, and industrialization. Developed countries invest a lot of research\nmoney in collecting traffic data using Radio Frequency Identification (RFID),\nloop detectors, speed sensors, high-end traffic light, and GPS. However, these\nprocesses are expensive, infeasible, and non-scalable for developing countries\nwith numerous non-motorized vehicles, proliferated ride-sharing services, and\nfrequent pedestrians. This paper proposes a novel approach to collect traffic\ndata from Google Map's traffic layer with minimal cost. We have implemented\nwidely used models such as Historical Averages (HA), Support Vector Regression\n(SVR), Support Vector Regression with Graph (SVR-Graph), Auto-Regressive\nIntegrated Moving Average (ARIMA) to show the efficacy of the collected traffic\ndata in forecasting future congestion. We show that even with these simple\nmodels, we could predict the traffic congestion ahead of time. We also\ndemonstrate that the traffic patterns are significantly different between\nweekdays and weekends.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.05084,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Reflexive Design for Fairness and Other Human Values in Formal Models\n\n  Algorithms and other formal models purportedly incorporating human values\nlike fairness have grown increasingly popular in computer science. In response\nto sociotechnical challenges in the use of these models, designers and\nresearchers have taken widely divergent positions on how formal models\nincorporating aspects of human values should be used: encouraging their use,\nmoving away from them, or ignoring the normative consequences altogether. In\nthis paper, we seek to resolve these divergent positions by identifying the\nmain conceptual limits of formal modeling, and develop four reflexive\nvalues--value fidelity, appropriate accuracy, value legibility, and value\ncontestation--vital for incorporating human values adequately into formal\nmodels. We then provide a brief methodology for reflexively designing formal\nmodels incorporating human values.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.04929,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Defining Computer Art: Methods, Themes, and the Aesthetic Problematic\n\n  The application of computer technology in the field of art has given rise to\nnovel modes of artistic practice, including media art, and it is a necessity to\nfind a commensurable conceptual fundament. Therefore, computer art starting\nfrom the 1950s reenters the view. To clarify the definition, major methods for\ndefining are reviewed, and it is argued that the thematic definition guided by\nsituational logic provides a feasible approach. There is a triad of themes: the\nrelationship between art and technology, the problem of machine creation, and\nthe ontology of art. Consisted of primitive and mutually supportive questions,\na logical space of questioning, i.e. a problematic, is formed as the basis for\nthe identity of computer art. Among them, the problem of the ontology of art is\nlocated at the logical starting point of questioning, and this problematic is\ntherefore an aesthetic one. The anticipation that computer art presents and\nresponds to the above-mentioned aesthetic problematic suggests the plausibility\nof computer art being a legitimate category of art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.0483,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"Students Readiness for E-learning in the Universities in Yemen\n\n  The e-learning is an advanced version of traditional education. It is defined\nas a way of learning by using the communication mechanisms of modern computer\nnetworks and multimedia, including voice, image, and graphics and mechanisms to\nsearch electronic libraries, as well as web portals, whether in the context of\ndistance learning or in the classroom. The people who engage in the transition\nto web-supported education are the administrative staff, the faculty, and the\nstudents. They all have their needs and they all should meet specific\nrequirements in order to facilitate the transition. The article presents the\nresults of questionnaire research of the students readiness for e-learning in\nYemeni universities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.16309,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Ethical Decision Making During Automated Vehicle Crashes\n\n  Automated vehicles have received much attention recently, particularly the\nDARPA Urban Challenge vehicles, Google's self-driving cars, and various others\nfrom auto manufacturers. These vehicles have the potential to significantly\nreduce crashes and improve roadway efficiency by automating the\nresponsibilities of the driver. Still, automated vehicles are expected to crash\noccasionally, even when all sensors, vehicle control components, and algorithms\nfunction perfectly. If a human driver is unable to take control in time, a\ncomputer will be responsible for pre-crash behavior. Unlike other automated\nvehicles--such as aircraft, where every collision is catastrophic, and guided\ntrack systems, which can only avoid collisions in one dimension--automated\nroadway vehicles can predict various crash trajectory alternatives and select a\npath with the lowest damage or likelihood of collision. In some situations, the\npreferred path may be ambiguous. This study investigates automated vehicle\ncrashing and concludes the following: (1) automated vehicles will almost\ncertainly crash, (2) an automated vehicle's decisions preceding certain crashes\nwill have a moral component, and (3) there is no obvious way to effectively\nencode complex human morals in software. A three-phase approach to developing\nethical crashing algorithms is presented, consisting of a rational approach, an\nartificial intelligence approach, and a natural language requirement. The\nphases are theoretical and should be implemented as the technology becomes\navailable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.00303,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000300672,
      "text":"Optimizing Waste Management Collection Routes in Urban Haiti: A\n  Collaboration between DataKind and SOIL\n\n  Sustainable Organic Integrated Livelihoods (SOIL) is a research and\ndevelopment organization that aims to increase access to cost-effective\nhousehold sanitation services in urban communities in Haiti. Each week, SOIL\nprovides over 1,000 households with ecological sanitation toilets, then\ntransports the waste to be transformed into rich compost. However, SOIL faces\nseveral challenges regarding the route optimization of their mixed fleet\nvehicle routing. This paper builds upon the authors' submission to Bloomberg's\n2019 Data for Good Exchange (D4GX), presenting preliminary findings from a\njoint collaboration between DataKind, a data science nonprofit, and SOIL. This\nresearch showcases how optimization algorithms and open source tools (i.e.,\nOpenStreetMap and Google OR-Tools) can help improve and reduce the costs of\nmixed-fleet routing problems, particularly in the context of developing\ncountries. As a result of this work, SOIL is able to make improvement to their\ncollection routes, which account for different road conditions and vehicle\ntypes. These improvements reduce operational costs and fuel use, which are\nessential to the service's expansion in the coming years.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07298,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"Enabling older citizens safe mobility. The ACTIVAGE approach\n\n  We live in an ever-aging world. The percentage of older citizens increases in\nmodern societies as older citizens represent the 19.20% of the general\npopulation. In Greece, an increase of almost 7% of older citizens has been\nobserved in the last twenty years. As old age never comes alone, age-related\nimpairments should be considered in the effort to provide safe transport\nconditions for them. It is of importance that transportation services have to\nmeet the special requirements and needs of older citizens. European Union,\nthrough the ACTIVAGE project, aims at using the Internet of Things (IoT)\nsolutions in favor of older citizens. In the framework of this project, the\nACTIVAGE Safe Mobility Platform (ASMP) has been designed. Therefore, older\ncitizens and their relatives have access to information regarding their\ntravels. In the context of this study, an extensive description of the ASMP and\nthe services offered through it is provided.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11677,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000067552,
      "text":"Second layer data governance for permissioned blockchains: the privacy\n  management challenge\n\n  Data privacy is a trending topic in the internet era. Given such importance,\nmany challenges emerged in order to collect, manage, process, and publish data.\nIn this sense, personal data have got attention, and many regulations emerged,\nsuch as GDPR in the European Union and LGPD in Brazil. This regulation model\naims to protect users' data from misusage and leakage and allow users to\nrequest an explanation from companies when needed. In pandemic situations, such\nas the COVID-19 and Ebola outbreak, the action related to sharing health data\nbetween different organizations is\/ was crucial to develop a significant\nmovement to avoid the massive infection and decrease the number of deaths.\nHowever, the data subject, i.e., the users, should have the right to request\nthe purpose of data use, anonymization, and data deletion. In this sense,\npermissioned blockchain technology emerges to empower users to get their rights\nproviding data ownership, transparency, and security through an immutable,\nunified, and distributed database ruled by smart contracts. The governance\nmodel discussed in blockchain applications is usually regarding the first layer\ngovernance, i.e., public and permissioned models. However, this discussion is\ntoo superficial, and they do not cover compliance with the data regulations.\nTherefore, in order to organize the relationship between data owners and the\nstakeholders, i.e., companies and governmental entities, we developed a second\nlayer data governance model for permissioned blockchains based on the\nGovernance Analytical Framework principles applied in pandemic situations\npreserving the users' privacy and their duties. From the law perspective, we\nbased our model on the UE GDPR in regard to data privacy concerns.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13276,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000188086,
      "text":"Towards Empowering Diabetic Patients: A perspective on self-management\n  in the context of a group-based education program\n\n  This paper provides a novel framework for maximizing the effectiveness of the\nDiabetes Group Education Program, which could be generalized in any similar\nproblem context.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.07297,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Assessing the Readiness of Greece for Autonomous Vehicle Technologies\n\n  Despite the debate regarding the timeframe and rate of penetration of\nAutonomous Vehicles, their potential benefits and implications have been widely\nrecognized. Therefore, assessing the readiness of individual countries to adopt\nsuch technologies and adapt to their introduction is of particular importance.\nThis paper aims to enrich our understanding of EU readiness regarding the\nintroduction of autonomous vehicle technologies by assessing the case of\nGreece. Thus, through a literature review, the criteria upon which such an\nassessment should be based are established and analyzed. Subsequently, the case\nof Greece is assessed based on those criteria by finding relevant sources that\nsupport and justify any assessment. Regardless of the outcome concerning the\nreadiness of Greece, such an assessment should help identify areas in which\nfocus should be given in order to ensure a smoother transition to such\ntechnologies. This contribution is expected to assist policy makers in Greece.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11525,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"An Interactive Foreign Language Trainer Using Assessment and Feedback\n  Modalities\n\n  English has long been set as the universal language. Basically most, if not\nall countries in the world know how to speak English or at least try to use it\nin their everyday communications for the purpose of globalizing. This study is\ndesigned to help the students learn from one or all of the four most commonly\nused foreign languages in the field of Information Technology namely Korean,\nMandarin Chinese, Japanese, and Spanish. Composed of a set of words, phrases,\nand sentences, the program is intended to quickly teach the students in the\nform of basic, intermediate, and advanced levels. This study has used the Agile\nmodel in system development. Functionality, reliability, usability, efficiency,\nand portability were also considered in determining the level of the\nacceptability of the system in terms of ISO 25010:2011. This interactive\nforeign language trainer is built to associate fun with learning, to remedy the\nlack of perseverance by some in learning a new language, and to make learning\nthe users' favorite playtime activity. The study allows the user to interact\nwith the program which provides support for their learning. Moreover, this\nstudy reveals that integrating feedback modalities in the training and\nassessment modules of the software strengthens and enhances the memory in\nlearning the language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13416,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Overcoming Failures of Imagination in AI Infused System Development and\n  Deployment\n\n  NeurIPS 2020 requested that research paper submissions include impact\nstatements on \"potential nefarious uses and the consequences of failure.\"\nHowever, as researchers, practitioners and system designers, a key challenge to\nanticipating risks is overcoming what Clarke (1962) called 'failures of\nimagination.' The growing research on bias, fairness, and transparency in\ncomputational systems aims to illuminate and mitigate harms, and could thus\nhelp inform reflections on possible negative impacts of particular pieces of\ntechnical work. The prevalent notion of computational harms -- narrowly\nconstrued as either allocational or representational harms -- does not fully\ncapture the open, context dependent, and unobservable nature of harms across\nthe wide range of AI infused systems.The current literature focuses on a small\nrange of examples of harms to motivate algorithmic fixes, overlooking the wider\nscope of probable harms and the way these harms might affect different\nstakeholders. The system affordances may also exacerbate harms in unpredictable\nways, as they determine stakeholders' control(including of non-users) over how\nthey use and interact with a system output. To effectively assist in\nanticipating harmful uses, we argue that frameworks of harms must be\ncontext-aware and consider a wider range of potential stakeholders, system\naffordances, as well as viable proxies for assessing harms in the widest sense.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14059,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Discrete Math with Programming: A Principled Approach\n\n  Discrete mathematics is the foundation of computer science. It focuses on\nconcepts and reasoning methods that are studied using math notations. It has\nlong been argued that discrete math is better taught with programming, which\ntakes concepts and computing methods and turns them into executable programs.\nWhat has been lacking is a principled approach that supports all central\nconcepts of discrete math -- especially predicate logic -- and that directly\nand precisely connects math notations with executable programs. This paper\nintroduces such an approach. It is based on the use of a powerful language that\nextends the Python programming language with proper logic quantification (\"for\nall\" and \"exists some\"), as well as declarative set comprehension (also known\nas set builder) and aggregation (e.g., sum and product). Math and logical\nstatements can be expressed precisely at a high level and be executed directly\non a computer, encouraging declarative programming together with algorithmic\nprogramming. We describe the approach, detailed examples, experience in using\nit, and the lessons learned.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.00843,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Analyzing the Structure of Mondrian's 1920-1940 Compositions\n\n  Mondrian was one of the most significant painters of the 20th century. He was\na prominent member of DeStijl, the movement which revolutionized art by setting\nit free of the obligation to make images of existing objects, persons, or\nsituations. DeStijl was one of the interrelated movements in early 20th century\nEurope including Cubism, Constructivism, the Futurists, and Dada. Their\ndisruptive ideas changed the meaning of Western art. It was Mondrian, more than\nanyone else, who worked restlessly to find expression for the purest possible\nkind of beauty and truth, based on a theory called Neoplasticism. He was\nalready famous during his lifetime and still now, his name is almost synonym\nfor modern art. We analyze the structure of the system of black lines in his\npaintings and put the hypothesis to the test that the paintings could be\nobtained by recursive (binary) splitting. We used a novel tailor-made\ninteractive analysis tool and apply it to as many Mondrian paintings as\npossible (in total 147). The results will be explained in a visual manner, but\nwe also present statistical findings from the analysis of the 147 paintings.\nOur main conclusion is that the hypothesis of splitting decomposition is in\ngeneral not true. It is possible to make Mondrian-like compositions by\nsplitting, yet one misses out on a great deal of Neoplastic beauty if one would\nwork by splitting only. It is possible to consider all crossings as pairs of\nTees, but that is clumsy, and it leaves out essential information. Moreover\nthere are other important design decisions of Mondrian, such as the\nkeeping-distance to the canvas-edge which are not well-described by splitting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.00464,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"What is the T-Algorithm? A case study to evaluate a new University\n\n  We evaluate Scott Galloway's T-algorithm as a thinking framework via a\nsimulated case. We explain how we applied it to analyze the investment\nstrategies when starting a new university. We note that the algorithm can be\nused to describe but also prescribe the strategy design space. We also point\nweak and strong points of the said algorithm and compare it to existing tools\nsuch as the Business canvas model and SWOT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.07212,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Do Cyber Capabilities and Cyber Power Incentivize International\n  Cooperation?\n\n  This paper explores a research question about whether defensive and offensive\ncyber security power and the capabilities to exercise the power influence the\nincentives of nation-states to participate in bilateral and multilateral\ncooperation (BMC) through formal and informal agreements, alliances, and norms.\nDrawing from international relations in general and structural realism in\nparticular, three hypotheses are presented for assessing the research question\nempirically: (i) increasing cyber capability lessens the incentives for BMC;\n(ii) actively demonstrating and exerting cyber power decreases the willingness\nfor BMC; and (iii) small states prefer BMC for cyber security and politics\nthereto. According to a cross-country dataset of 29 countries, all three\nhypotheses are rejected. Although presenting a \"negative result\" with respect\nto the research question, the accompanying discussion contributes to the\nstate-centric cyber security research in international relations and political\nscience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.03117,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"IFC models for (semi)automating common planning checks for building\n  permits\n\n  To support building permit issuing with automatic digital tools, the reuse of\nmodels produced by designers would make the process quicker and more objective.\nHowever, current studies and pilots often leave a gap with respect to the\nmodels as actually provided by architects, having varying quality and content.\nIn this study, rather than taking a top down approach, we started from the\navailable data and made the necessary inferences, which gave the opportunity to\ntackle basic and common issues often preventing smooth automatic processing.\nSpecific characteristics of the IFC models were outlined and a tool was\ndeveloped to extract the necessary information from them to check\nrepresentative regulations. While the case study is specific in location,\nregulations and input models, the type of issues encountered are a generally\napplicable example for automated code compliance checking. This represents a\nsolid base for future works towards the automation of building permits issuing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.0118,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Usability Dimensions and Behavioral Intention to Use Markdown to Moodle\n  in Test Construction\n\n  Creating test with numerous items in Moodle can be tedious and less intuitive\ncompared to conventional method. This study aims to determine the Markdown to\nMoodle performance in easing the test construction process and explain the\nunderlying factors of the behavioral intention to use the application. Markdown\nto Moodle is an application that allows users to type the bulk of test items\ndirectly to the browser and generates .doc, .md and .xml files stored in the\nlocal drive. The .xml can be imported to Moodle test bank. This lessens the\ntime of creating test items one at a time in the Moodle. A training and a\nsurvey were conducted among teachers with Moodle usage experience. Results from\nthis study allowed the researchers to determine the usability of the\napplication and the users behavioral intention. This highlights the workflow\ncontinuity in test construction as a key factor in the usage and performance of\nthe application.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09861,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"The human quest for discovering mathematical beauty in the arts\n\n  In the words of the twentieth-century British mathematician G. H. Hardy, \"the\nhuman function is to 'discover or observe' mathematics\" (1). For centuries,\nstarting from the ancient Greeks, mankind has hunted for beauty and order in\narts and in nature. This quest for mathematical beauty has led to the discovery\nof recurrent mathematical structures, such as the golden ratio, Fibonacci, and\nLucas numbers, whose ubiquitous presences have been tantalizing the minds of\nartists and scientists alike. The captivation for this quest comes with high\nstakes. In fact, art is the definitive expression of human creativity, and its\nmathematical understanding would deliver us the keys for decoding human culture\nand its evolution (2). However, it was not until fairly recently that the scope\nand the scale of the human quest for mathematical beauty was radically expanded\nby the simultaneous confluence of three separate innovations. The mass\ndigitization of large art archives, the surge in computational power, and the\ndevelopment of robust statistical methods to capture hidden patterns in vast\namounts of data have made it possible to reveal the---otherwise unnoticeable to\nthe human eye---mathematics concealed in large artistic corpora. Starting from\nits inception, marked by the foundational work by Birkhoff (3), progress in the\nbroad field of computational aesthetics has reached a scale that would have\nbeen unimaginable just a decade ago. The recent expansion is not limited to the\nvisual arts (2) but includes music (4), stories (5), language phonology (6),\nhumor in jokes (7), and even equations (8); for a comprehensive review, see\nref. 9.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.12923,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"Assessing the Quality of Gridded Population Data for Quantifying the\n  Population Living in Deprived Communities\n\n  Over a billion people live in slums in settlements that are often located in\necologically sensitive areas and hence highly vulnerable. This is a problem in\nmany parts of the world, but it is more prominent in low-income countries,\nwhere in 2014 on average 65% of the urban population lived in slums. As a\nresult, building resilient communities requires quantifying the population\nliving in these deprived areas and improving their living conditions. However,\nmost of the data about slums comes from census data, which is only available at\naggregate levels and often excludes these settlements. Consequently,\nresearchers have looked at alternative approaches. These approaches, however,\ncommonly rely on expensive high-resolution satellite imagery and field-surveys,\nwhich hinders their large-scale applicability. In this paper, we investigate a\ncost-effective methodology to estimate the slum population by assessing the\nquality of gridded population data. We evaluate the accuracy of the WorldPOP\nand LandScan population layers against ground-truth data composed of 1,703\ngeoreferenced polygons that were mapped as deprived areas and which had their\npopulation surveyed during the 2010 Brazilian census. While the LandScan data\ndid not produce satisfactory results for most polygons, the WorldPOP estimates\nwere less than 20% off for 67% of the polygons and the overall error for the\ntotality of the studied area was only -5.9%. This small error margin\ndemonstrates that population layers with a resolution of at least a 100m, such\nas WorldPOP's, can be useful tools to estimate the population living in slums.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01168,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Use-cases of Blockchain Technology for Humanitarian Engineering\n\n  Humanitarian Engineers need innovative methods to make technological\ninterventions for solving societal problems. The emerging blockchain technology\nhas the enormous potential to provide effective interventions in various\ndevelopmental sectors, including Agriculture, Education, Health, and\nTransportation. In these sectors, mediators have been considered as one of the\nimpediments for developmental work. Blockchain technology facilitates\npeer-to-peer business transactions, thus eliminating the role of mediators.\nHence, the blockchain technology is emerging as an alternative to conventional\nmediator-centred solutions adopting client-server based Internet technologies.\nA combination of blockchain technology with other technologies can be used to\naddress domain-specific challenges. For example, the combination of blockchain\ntechnology and Internet-of-Thing (IoT) has the potential to monitor the usage\nof scarce resources such as the level of ground-water and amount of energy\nconsumption. The aims of this chapter are twofold. Firstly, it describes the\nprimary building blocks of blockchain technology. Secondly, it illustrates\nvarious use-case scenarios of blockchain technology in the fields of\nAgriculture, Energy Health and others.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01167,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"Seminar and Training Programs Recommender System for Faculty Members of\n  Higher Education Institution\n\n  This study aims to develop a personalized Recommender System that helps to\naddress the problems encountered by the faculty members of Higher Education\nInstitutions in the selection of Seminar and Training Programs (STP). The\nresearcher used the Descriptive Developmental Method of research to gather\ninformation relevant to the current problems and challenges encountered and\nused these to develop software that addresses the identified challenges. For\nthe development of the software, the researcher adopted a step-wise approach\ndefined in the Incremental Developmental Model. The level of acceptance of the\ndeveloped system was evaluated by 24 faculty respondents. The level of\nacceptance of the developed system was classified into functionality,\nreliability, and usability and the study garnered an evaluation score of 4.65,\n4.67, and 4.67 respectively. The overall interpretation of the results of the\nevaluation is Highly Acceptable. The study created a system that provides\nseminars and training program recommendations. The developed recommender system\nwas rated Highly Acceptable, respondents were very satisfied with the features\nof the system and agreed that it was functional, reliable, and usable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11384,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000118547,
      "text":"Influence of Murder Incident of Ride-hailing Drivers on Ride-hailing\n  User's Consuming Willingness in Nanchang\n\n  Due to the frequent murder incidents of ride-hailing drivers in China in\n2018, ride-hailing companies took a series of measures to prevent such\nincidents and ensure ride-hailing passengers' safety. This study investigated\nusers' willingness to use ride-hailing apps after murder incidents and users'\nattitudes toward Safety Rectification. We found that murder incidents of\nride-hailing drivers had a significant adverse impact on people's usage of\nride-hailing apps. Female users' consuming willingness was 0.633 times that of\nmale users, such as\" psychological harm\" was more evident among females, and\nSafety Rectification had a calming effect for some users. Finally, we found\nthat people were satisfied with ride-hailing apps' efficiency, but were not\nsatisfied with safety and reliability, considered them important; female users\nwere more concerned about the security than male users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11033,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000122852,
      "text":"Who's Who in the Information Technology Research in the Philippines A\n  Social Network Analysis\n\n  This study reported the conference papers presented conducted by the two\ncomputing societies in the Philippines. Toward this goal, all published\nconference proceedings from the National Conference of IT Education and\nPhilippine Computing Society Conference were gathered and analyzed using social\nnetwork analysis. The findings of the study disclosed that there are 733 papers\npresented in the conference for the span of 18 years. On the average, both\nconferences had 27 papers presented annually. Private higher education\ninstitutions dominated the list of research productive schools where De La\nSalle University tops the list. A researcher in the University of the\nPhilippines-Diliman is the most prolific researcher with 39 publications and\n\"algorithm\" was the most researched topic. Researchers tend to work in small\nteam consisting of 2 to 3 members. Implications and limitations of the study\nare also presented.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01268,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Measuring Bangladeshi Female Farmers' Values for Agriculture Mobile\n  Applications Development\n\n  The ubiquity of mobile applications (apps) in daily life raises the\nimperative that the apps should reflect users' values. However, users' values\nare not usually taken into account in app development. Thus there is\nsignificant potential for user dissatisfaction and negative socio-economic\nconsequences. To be cognizant of values in apps, the first step is to find out\nwhat those values are, and that was the objective of this study conducted in\nBangladesh. Our focus was on rural women, specifically female farmers. The\nbasis for our study was Schwartz's universal human values theory, and we used\nan associated survey instrument, the Portrait Values Questionnaire (PVQ). Our\nsurvey of 193 Bangladeshi female farmers showed that Conformity and Security\nwere regarded as the most important values, while Power, Hedonism, and\nStimulation were the least important. This finding would be helpful for\ndevelopers to take into account when developing agriculture apps for this\nmarket. In addition, the methodology we used provides a model to follow to\nelicit the values of apps' users in other communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11095,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"How will AI and automation transform society and cities?\n\n  Against the backdrop of rising anxiety and discussions on the impact of AI on\nsociety, I explore in this article the structural possibilities of AI and\nautomation triggering a new social conflict between the current capitalist\nelites and the emerging \"creative class\" (R&D scientists, engineers, business\ndevelopers, etc.), and how this conflict can produce social tensions and\ntransform urban space. By drawing insights from a structurally similar conflict\nin 17-18th century Europe between the aristocracy and the emerging bourgeoisie,\nthe impact of this conflict on the social, spatial, and power landscapes in\ncities of that time, as well as current trends in urban geography, this article\noutlines the prospects of urban transformations under changing production and\nconsumption economies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.10265,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"How do you feel: Emotions exhibited while Playing Computer Games and\n  their Relationship with Gaming Behaviors\n\n  This descriptive study utilized a validated questionnaire to determine the\nemotions exhibited by computer gamers in cyber caf\\'es. It was revealed that\nmost of the gamers were young, male, single, as well as high school and\nvocational students who belonged to middle-income families. Most of them had\ncomputer access at home but only a few had Internet access at home. Gamers\ntended to play games in cyber caf\\'es at least three times a week, usually in\nthe evening, for at least two hours per visit. They also reported that they\nplayed games frequently. Majority of the gamers were fond of playing DOTA,\nLeague of Legends, and CABAL and they had been playing games for at least two\nyears. It was disclosed that they exhibited both positive and negative emotions\nwhile playing games. It was shown that gamers were inclined to be more anxious\nto be defeated in a game as gaming became frequent and length of years in\nplaying games increased. They also had the tendency to become more stressed\nwhen length of years of playing games increased. On the other hand, other\ngaming behaviors were not significantly related to other emotions. Thus, the\nnull hypothesis stating that gaming behaviors of the respondents are not\nsignificantly related to the emotions exhibited while playing the computer\ngames is partially rejected. Therefore, not all emotions exhibited while\nplaying computer games could be attributed to their gaming behaviors. It is\nrecommended that other emotions such as anger, frustration, boredom, amusement,\netc. be included in future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11694,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Alone or With Others? Understanding Eating Episodes of College Students\n  with Mobile Sensing\n\n  Understanding food consumption patterns and contexts using mobile sensing is\nfundamental to build mobile health applications that require minimal user\ninteraction to generate mobile food diaries. Many available mobile food\ndiaries, both commercial and in research, heavily rely on self-reports, and\nthis dependency limits the long term adoption of these apps by people. The\nsocial context of eating (alone, with friends, with family, with a partner,\netc.) is an important self-reported feature that influences aspects such as\nfood type, psychological state while eating, and the amount of food, according\nto prior research in nutrition and behavioral sciences. In this work, we use\ntwo datasets regarding the everyday eating behavior of college students in two\ncountries, namely Switzerland (N_ch=122) and Mexico (N_mx=84), to examine the\nrelation between the social context of eating and passive sensing data from\nwearables and smartphones. Moreover, we design a classification task, namely\ninferring eating-alone vs. eating-with-others episodes using passive sensing\ndata and time of eating, obtaining accuracies between 77% and 81%. We believe\nthat this is a first step towards understanding more complex social contexts\nrelated to food consumption using mobile sensing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13153,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Nose to Glass: Looking In to Get Beyond\n\n  Brought into the public discourse through investigative work by journalists\nand scholars, awareness of algorithmic harms is at an all-time high. An\nincreasing amount of research has been conducted under the banner of enhancing\nresponsible artificial intelligence (AI), with the goal of addressing,\nalleviating, and eventually mitigating the harms brought on by the roll out of\nalgorithmic systems. Nonetheless, implementation of such tools remains low.\nGiven this gap, this paper offers a modest proposal: that the field,\nparticularly researchers concerned with responsible research and innovation,\nmay stand to gain from supporting and prioritising more ethnographic work. This\nembedded work can flesh out implementation frictions and reveal organisational\nand institutional norms that existing work on responsible artificial\nintelligence AI has not yet been able to offer. In turn, this can contribute to\nmore insights about the anticipation of risks and mitigation of harm. This\npaper reviews similar empirical work typically found elsewhere, commonly in\nscience and technology studies and safety science research, and lays out\nchallenges of this form of inquiry.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01188,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"Investigating the Influence of Computer Anxiety on the Academic\n  Performance of Junior Secondary School Students in Computer Studies in\n  Nigeria\n\n  This study examined the influence of computer anxiety on the academic\nperformance of junior secondary school students in Computer Studies in Nigeria.\nThe research instrument that was used in the study was computer anxiety scale\nwhich was validated by the Guidance and Counseling lecturers and Educational\nMeasurement and Evaluation experts. The result of the study showed that most of\nthe students used in the study were mildly anxious when dealing with computer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.00874,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"\"A cold, technical decision-maker\": Can AI provide explainability,\n  negotiability, and humanity?\n\n  Algorithmic systems are increasingly deployed to make decisions in many areas\nof people's lives. The shift from human to algorithmic decision-making has been\naccompanied by concern about potentially opaque decisions that are not aligned\nwith social values, as well as proposed remedies such as explainability. We\npresent results of a qualitative study of algorithmic decision-making,\ncomprised of five workshops conducted with a total of 60 participants in\nFinland, Germany, the United Kingdom, and the United States. We invited\nparticipants to reason about decision-making qualities such as explainability\nand accuracy in a variety of domains. Participants viewed AI as a\ndecision-maker that follows rigid criteria and performs mechanical tasks well,\nbut is largely incapable of subjective or morally complex judgments. We discuss\nparticipants' consideration of humanity in decision-making, and introduce the\nconcept of 'negotiability,' the ability to go beyond formal criteria and work\nflexibly around the system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14419,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Urban volumetrics: spatial complexity and wayfinding, extending space\n  syntax to three dimensional space\n\n  Wayfinding behavior and pedestrian movement pattern research relies on\nobjective spatial configuration representation and analysis, such as space\nsyntax, to quantify and control for the difficulty of wayfinding in multi-level\nbuildings and urban built environments. However, the space syntax's\nrepresentation oversimplifies multi-level vertical connections. The more recent\nsegment and angular approaches to space syntax remain un-operationalizable in\nthree dimensional space. The two dimensional axial-map and segment map line\nrepresentations are reviewed to determine their extension to a novel three\ndimensional space line representation. Using an extreme case study research\nstrategy, four representations of a large scale complex multi-level outdoor and\nindoor built environment are tested against observed pedestrian movement\npatterns N = 17,307. Association with the movement pattern increases steadily\nas the representation increases toward high three-dimensional space level of\ndefinition and completeness. A novel hybrid angular-Euclidean analysis was used\nfor the objective description of three dimensional built environment\ncomplexity. The results suggest that pedestrian wayfinding and movement pattern\nresearch in a multi-level built environment should include interdependent\noutdoor and indoor, and use full three-dimensioanal line representation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07748,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"Investigation of the Impacts of COVID-19 on the Electricity Consumption\n  of a University Dormitory Using Weather Normalization\n\n  This study investigated the impacts of the COVID-19 pandemic on the\nelectricity consumption of a university dormitory building in the southern U.S.\nThe historical electricity consumption data of this university dormitory\nbuilding and weather data of an on-campus weather station, which were collected\nfrom January 1st, 2017 to July 31st, 2020, were used for analysis. Four inverse\ndata-driven prediction models, i.e., Artificial Neural Network, Long Short-Term\nMemory Recurrent Neural Network, eXtreme Gradient Boosting, and Light Gradient\nBoosting Machine, were exploited to account for the influence of the weather\nconditions. The results suggested that the total electricity consumption of the\nobjective building decreased by nearly 41% (about 276,000 kWh (942 MMBtu))\ncompared with the prediction value during the campus shutdown due to the\nCOVID-19. Besides, the daily load ratio (DLR) varied significantly as well. In\ngeneral, the DLR decreased gradually from 80% to nearly 40% in the second half\nof March 2020, maintained on a relatively stable level between 30% to 60% in\nApril, May, and June 2020, and then slowly recovered to 80% of the normal\ncapacity in July 2020.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.05859,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Evolution of Digital Advertising Strategies during the 2020 US\n  Presidential Primary\n\n  Political advertising on digital platforms has grown dramatically in recent\nyears as campaigns embrace new ways of targeting supporters and potential\nvoters. Previous scholarship shows that digital advertising has both positive\neffects on democratic politics through increased voter knowledge and\nparticipation, and negative effects through user manipulation, opinion\necho-chambers, and diminished privacy. However, research on election campaign\nstrategies has focused primarily on traditional media, such as television.\nHere, we examine how political campaign dynamics have evolved in response to\nthe growth of digital media by analyzing the advertising strategies of US\npresidential election campaigns during the 2020 primary cycle. To identify\ngeographic and temporal trends, we employ regression analyses of campaign\nspending across nearly 600,000 advertisements published on Facebook. We show\nthat campaigns heavily target voters in candidates' home states during the\n\"invisible primary\" stage before shifting to states with early primaries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.00515,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Civic Technologies: Research, Practice and Open Challenges\n\n  Over the last years, civic technology projects have emerged around the world\nto advance open government and community action. Although Computer-Supported\nCooperative Work (CSCW) and Human-Computer Interaction (HCI) communities have\nshown a growing interest in researching issues around civic technologies, yet\nmost research still focuses on projects from the Global North. The goal of this\nworkshop is, therefore, to advance CSCW research by raising awareness for the\nongoing challenges and open questions around civic technology by bridging the\ngap between researchers and practitioners from different regions.\n  The workshop will be organized around three central topics: (1) discuss how\nthe local context and infrastructure affect the design, implementation,\nadoption, and maintenance of civic technology; (2) identify key elements of the\nconfiguration of trust among government, citizenry, and local organizations and\nhow these elements change depending on the sociopolitical context where\ncommunity engagement takes place; (3) discover what methods and strategies are\nbest suited for conducting research on civic technologies in different\ncontexts. These core topics will be covered across sessions that will initiate\nin-depth discussions and, thereby, stimulate collaboration between the CSCW\nresearch community and practitioners of civic technologies from both Global\nNorth and South.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01546,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Effective Feedback for Introductory CS Theory: A JFLAP Extension and\n  Student Persistence\n\n  Computing theory analyzes abstract computational models to rigorously study\nthe computational difficulty of various problems. Introductory computing theory\ncan be challenging for undergraduate students, and the main goal of our\nresearch is to help students learn these computational models. The most common\npedagogical tool for interacting with these models is the Java Formal Languages\nand Automata Package (JFLAP). We developed a JFLAP server extension, which\naccepts homework submissions from students, evaluates the submission as correct\nor incorrect, and provides a witness string when the submission is incorrect.\nOur extension currently provides witness feedback for deterministic finite\nautomata, nondeterministic finite automata, regular expressions, context-free\ngrammars, and pushdown automata.\n  In Fall 2019, we ran a preliminary investigation on two sections (Control and\nStudy) of the required undergraduate course Introduction to Computer Science\nTheory. The Study section used our extension for five targeted homework\nquestions, and the Control section solved and submitted these problems using\ntraditional means. Our results show that on these five questions, the Study\nsection performed better on average than the Control section. Moreover, the\nStudy section persisted in submitting attempts until correct, and from this\nfinding, our preliminary conclusion is that minimal (not detailed or\ngrade-based) witness feedback helps students to truly learn the concepts. We\ndescribe the results that support this conclusion as well as a related\nhypothesis conjecturing that with witness feedback and unlimited number of\nsubmissions, partial credit is both unnecessary and ineffective.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Using Data Science to monitor the pandemic with a single number: the\n  Synthetic COVID Index\n\n  Rapid and affordable methods of summarizing the multitude of data relating to\nthe pandemic can be useful to health authorities and policy makers who are\ndealing with the COVID-19 pandemic at various levels in the territories\naffected by SARSCoV-2. This is the goal of the Synthetic COVID Index, an index\nbased on an ensemble of Unsupervised Machine Learning techniques which focuses\non the identification of a latent variable present in data that contains\nmeasurement errors. This estimated latent variable can be interpreted as \"the\nstrength of the pandemic\". An application to the Italian case shows how the\nindex is able to provide a concise representation of the situation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01286,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"IBM Employee Attrition Analysis\n\n  In this paper, we analyzed the dataset IBM Employee Attrition to find the\nmain reasons why employees choose to resign. Firstly, we utilized the\ncorrelation matrix to see some features that were not significantly correlated\nwith other attributes and removed them from our dataset. Secondly, we selected\nimportant features by exploiting Random Forest, finding monthlyincome, age, and\nthe number of companies worked significantly impacted employee attrition. Next,\nwe also classified people into two clusters by using K-means Clustering.\nFinally, We performed binary logistic regression quantitative analysis: the\nattrition of people who traveled frequently was 2.4 times higher than that of\npeople who rarely traveled. And we also found that employees who work in Human\nResource have a higher tendency to leave.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000057949,
      "text":"A Serious Game Approach for the Electro-Mobility Sector\n\n  Serious Games (SGs) represent a new approach to improve learning processes\nmore effectively and economically than traditional methods. This paper aims to\npresent a SG approach for the electro-mobility context, in order to encourage\nthe use of electric light vehicles. The design of the SG is based on the\ntypical elements of the classic \"game\" with a real gameplay with different\npurposes. In this work, the proposed SG aims to raise awareness on\nenvironmental issues caused by mobility and actively involve users, on\nimproving livability in the city and on real savings using alternative means to\ntraditional vehicles. The objective of the designed tool is to propose elements\nof fun and entertainment for tourists or users of electric vehicles in the\ncities, while giving useful information about the benefits of using such\nvehicles, discovering touristic and interesting places in the city to discover.\nIn this way, the user is stimulated to explore the artistic and historical\naspects of the city through an effective learning process: he\/she is encouraged\nto search the origins and the peculiarities of the monuments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.0497,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"Internet of Things-based innovations in Saudi healthcare sector: A\n  methodological approach for investigating adoption issues\n\n  Using today's Internet network capacities, this technology has extended\nvarious benefits in healthcare sectors. For instance, existing studies already\nindicated that information technology applications with IoT-based innovations\nmay revolutionize the healthcare industry and subsequently help to improve the\nreal-time reporting of patients' health data. It should be noted that the\nadoption of IoT and its relevant interventions in the health sector has not\nbeen as fast as the uptake been observed in other industries. To tackle this\nissue, we develop a qualitative phenomenological approach for investigating\nfactors that affect IoT adoption and its integration into healthcare service\ndelivery in Saudi Arabia.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02724,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000190735,
      "text":"The Treachery of Images in the Digital Sovereignty Debate\n\n  This short theoretical and argumentative essay contributes to the ongoing\ndeliberation about the so-called digital sovereignty, as pursued particularly\nin the European Union (EU). Drawing from classical political science\nliterature, the essay approaches the debate through paradoxes that arise from\napplying classical notions of sovereignty to the digital domain. With these\nparadoxes and a focus on the Peace of Westphalia in 1648, the essay develops a\nviewpoint distinct from the conventional territorial notion of sovereignty.\nAccordingly, the lesson from Westphalia has more to do with the capacity of a\nstate to govern. It is also this capacity that is argued to enable the\nsovereignty of individuals within the digital realm. With this viewpoint, the\nessay further advances another, broader, and more pressing debate on politics\nand democracy in the digital era.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14308,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"MOLAM: A Mobile Multimodal Learning Analytics Conceptual Framework to\n  Support Student Self-Regulated Learning\n\n  Online distance learning is highly learner-centred, requiring different\nskills and competences from learners, as well as alternative approaches for\ninstructional design, student support, and provision of resources. Learner\nautonomy and self-regulated learning (SRL) in online learning settings are\nconsidered key success factors that predict student performance. SRL comprises\nprocesses of planning, monitoring, action and reflection according to\nZimmerman. And typically focuses on three key features of learners: (1) use of\nSRL strategies, (2) responsiveness to self-oriented feedback about learning\neffectiveness, and (3) motivational processes. SRL has been identified as\nhaving a direct correlation with students success, including improvements in\ngrades and the development of relevant skills and strategies. Such skills and\nstrategies are needed to become a successful lifelong learner. This chapter\nintroduces a Mobile Multimodal Learning Analytics approach (MOLAM). I argue\nthat the development of student Self-Regulated Learning would benefit from the\nadoption of this approach, and that its use would allow continuous measurement\nand provision of in-time support of student SRL in online learning contexts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07744,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000104308,
      "text":"Green IT as a tool for design cloud-oriented sustainable learning\n  environment of a higher education institution\n\n  The paper proposes the use of green IT as a tool for designing a\ncloud-oriented sustainable learning environment for a higher education\ninstitution. The article substantiates the expediency of designing such an\nenvironment as a prerequisite for the sustainable development of Ukraine. It is\nestablished that one of the goals of Ukraine's sustainable development for 2030\nis to provide fair quality education and to promote lifelong learning\nopportunities for all. Green IT is a set of approaches related to sustainable\ncomputing and information technology. The work of foreign scientists was\nanalyzed, which considered the issues of designing the learning environment\nusing green computing. As a result, Cloud LMS has been established that cloud\nLMS is a type of green IT and can serve as a tool for designing a\ncloud-oriented sustainable learning environment of a higher education\ninstitution. A model of a cloud-oriented sustainable learning environment of a\nhigher education institution using cloud LMS is proposed. The application of a\ncloud-oriented sustainable learning environment will provide such capabilities:\nkeep electronic journals; use on-line services; conduct correspondence,\nassessment of knowledge on-line; and more. And all of the above is the key to a\nsustainable development of the learning environment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.14349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Roofpedia: Automatic mapping of green and solar roofs for an open\n  roofscape registry and evaluation of urban sustainability\n\n  Sustainable roofs, such as those with greenery and photovoltaic panels,\ncontribute to the roadmap for reducing the carbon footprint of cities. However,\nresearch on sustainable urban roofscapes is rather focused on their potential\nand it is hindered by the scarcity of data, limiting our understanding of their\ncurrent content, spatial distribution, and temporal evolution. To tackle this\nissue, we introduce Roofpedia, a set of three contributions: (i) automatic\nmapping of relevant urban roof typology from satellite imagery; (ii) an open\nroof registry mapping the spatial distribution and area of solar and green\nroofs of more than one million buildings across 17 cities; and (iii) the\nRoofpedia Index, a derivative of the registry, to benchmark the cities by the\nextent of sustainable roofscape in term of solar and green roof penetration.\nThis project, partly inspired by its street greenery counterpart `Treepedia',\nis made possible by a multi-step pipeline that combines deep learning and\ngeospatial techniques, demonstrating the feasibility of an automated\nmethodology that generalises successfully across cities with an accuracy of\ndetecting sustainable roofs of up to 100% in some cities. We offer our results\nas an interactive map and open dataset so that our work could aid researchers,\nlocal governments, and the public to uncover the pattern of sustainable\nrooftops across cities, track and monitor the current use of rooftops,\ncomplement studies on their potential, evaluate the effectiveness of existing\nincentives, verify the use of subsidies and fulfilment of climate pledges,\nestimate carbon offset capacities of cities, and ultimately support better\npolicies and strategies to increase the adoption of instruments contributing to\nthe sustainable development of cities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.05484,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Preference-Based Privacy Trading\n\n  The question we raise through this paper is: Is it economically feasible to\ntrade consumer personal information with their formal consent (permission) and\nin return provide them incentives (monetary or otherwise)?. In view of (a) the\nbehavioral assumption that humans are `compromising' beings and have privacy\npreferences, (b) privacy as a good not having strict boundaries, and (c) the\npractical inevitability of inappropriate data leakage by data holders\ndownstream in the data-release supply-chain, we propose a design of regulated\nefficient\/bounded inefficient economic mechanisms for oligopoly data trading\nmarkets using a novel preference function bidding approach on a simplified\nsellers-broker market. Our methodology preserves the heterogeneous privacy\npreservation constraints (at a grouped consumer, i.e., app, level) upto certain\ncompromise levels, and at the same time satisfies information demand (via the\nbroker) of agencies (e.g., advertising organizations) that collect client data\nfor the purpose of targeted behavioral advertising.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01772,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000126494,
      "text":"Digital Landscape of COVID-19 Testing: Challenges and Opportunities\n\n  The COVID-19 Pandemic has left a devastating trail all over the world, in\nterms of loss of lives, economic decline, travel restrictions, trade deficit,\nand collapsing economy including real-estate, job loss, loss of health\nbenefits, the decline in quality of access to care and services and overall\nquality of life. Immunization from the anticipated vaccines will not be the\nstand-alone guideline that will help surpass the pandemic and return to\nnormalcy. Four pillars of effective public health intervention include\ndiagnostic testing for both asymptomatic and symptomatic individuals, contact\ntracing, quarantine of individuals with symptoms or who are exposed to\nCOVID-19, and maintaining strict hygiene standards at the individual and\ncommunity level. Digital technology, currently being used for COVID-19 testing\ninclude certain mobile apps, web dashboards, and online self-assessment tools.\nHerein, we look into various digital solutions adapted by communities across\nuniversities, businesses, and other organizations. We summarize the challenges\nexperienced using these tools in terms of quality of information, privacy, and\nuser-centric issues. Despite numerous digital solutions available and being\ndeveloped, many vary in terms of information being shared in terms of both\nquality and quantity, which can be overwhelming to the users. Understanding the\ntesting landscape through a digital lens will give a clear insight into the\nmultiple challenges that we face including data privacy, cost, and\nmiscommunication. It is the destiny of digitalization to navigate testing for\nCOVID-19. Block-chain based systems can be used for privacy preservation and\nensuring ownership of the data to remain with the user. Another solution\ninvolves having digital health passports with relevant and correct information.\nIn this early draft, we summarize the challenges and propose possible solutions\nto address the same.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03055,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Conceptualizing and Realizing A Smart City Model for Bangladesh\n\n  The outbreak of the novel Corona Virus in 2019, named COVID-19, causes the\nongoing global pandemic. This pandemic has a devastating socio-economic impact\nacross the globe. On the other hand, due to the pre-pandemic aggressive\nurbanization with the steep population growth, modern cities also facing\nsubstantial challenges. Hence, usage of technology is anticipated to be the\nprecondition for adaptive, resilient, and sustainable development, where, a\nsmart city is defined as the accumulated advanced ideas of information and\ntechnology aiming to ensure a decent quality of life for the inhabitants.\nConsidering the current growth rate of 1.9 percent, the projected population of\nBangladesh will exceed 180 million in 2026. It is also speculated that Dhaka\nbeing the capital will be populated with 14 million inhabitants. Moreover,\nDhaka has already been labeled as the most densely populated city in the world.\nThus, concerned authorities are facing enormous challenges to provide and\nensure fundamental services to the in-habitants. Therefore, this has become the\nneed of the hour to conceptualize an information and communication\ntechnology-driven smart city for Bangladesh. A smart city may contain numerous\ncomponents; however, the proposed framework identifies seven components and\nservices for smart cities in Bangladesh. These are healthcare, education,\ntransportation, public safety, real estate, utilities, and city administration.\nDiscussions on these components are carried out in this paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Teaching Creativity Using a Realistic Multi-User Operation: Packet\n  Tracer\n\n  Multi-user capabilities in Cisco Packet Tracer provide an incentive for\nimmersive realistic learning to increase the efficiency of teaching and\nlearning in the networking class. It is difficult to evaluate the configuration\nskills of the students in the classroom, particularly during laboratory class,\nwhile teaching networking course, the most problems for lecturers. The\nmulti-user functionality that uses the one-to-many remote relay server concept\nallows lecturers to remotely control and test many students at one time. The\nkey purpose of this paper is to incorporate creativity in teaching by using\nrealistic multi-user practises in the networking class. In this study, the\nmultiuser operation was developed and used via an existing LAN link in the\nclassroom. The focus of the operation is the simple configuration of network\nequipment such as routers and switches. This practise was used during the\nlaboratory session, encouraging lecturers to establish relationships based on a\nlarge audience. The professor is able to evaluate the success of the students\nduring the execution of the activity and to ensure that each student engages in\nthe activity. As a result of introducing this approach, the curiosity of\nstudents in technical subjects can be improved and student success can be\nconveniently tracked on the part of the lecturer. Lecturers are able to monitor\nthe configuration capabilities of students closely to ensure that each student\nengages in the laboratory class. The lecturer will train students not only to\ndevelop their configuration and troubleshooting abilities by using this\nteaching form, but also to accelerate the pace at which students need to fix\nnetwork problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01325,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Green Internet of Things: The Next Generation Energy Efficient Internet\n  of Things\n\n  The Internet of Things (IoT) is seen as a novel technical paradigm aimed at\nenabling connectivity between billions of interconnected devices all around the\nworld. This IoT is being served in various domains, such as smart healthcare,\ntraffic surveillance, smart homes, smart cities, and various industries. IoT's\nmain functionality includes sensing the surrounding environment, collecting\ndata from the surrounding, and transmitting those data to the remote data\ncenters or the cloud. This sharing of vast volumes of data between billions of\nIoT devices generates a large energy demand and increases energy wastage in the\nform of heat. The Green IoT envisages reducing the energy consumption of IoT\ndevices and keeping the environment safe and clean. Inspired by achieving a\nsustainable next-generation IoT ecosystem and guiding us toward making a\nhealthy green planet, we first offer an overview of Green IoT (GIoT), and then\nthe challenges and the future directions regarding the GIoT are presented in\nour study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07475,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000060929,
      "text":"A Canine Census to Influence Public Policy\n\n  The potential threat that domestic animals pose to the health of human\npopulations tends to be overlooked. We posit that positive steps forward can be\nmade in this area, via suitable state-wide public policy. In this paper, we\ndescribe the data collection process that took place in Casilda (a city in\nArgentina), in the context of a canine census. We outline preliminary findings\nemerging from the data, based on a number of perspectives, along with\nimplications of these findings in terms of informing public policy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10078,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Mechanical TA 2: A System for Peer Grading with TA Support\n\n  Mechanical TA 2 (MTA2) is an open source web-based peer grading application\nthat leverages trusted TA graders to incentivize high-quality peer review. A\nprevious, prototype implementation of MTA proved the value of the concept, but\nwas neither suitable for use at scale nor easily extensible; MTA2 is a complete\nreimplementation of the system that overcomes these hurdles. MTA2 serves two,\ninterconnected purposes: facilitating practical peer grading and serving as a\ntestbed for experimentation with different peer grading mechanisms. The system\nis characterized by a modular design that makes customization easy; support for\ndividing students into different pools based on their peer-grading prowess;\nmechanisms for automated calibration and spot checking; and the ability for\nstudents to appeal grades and to give feedback about individual reviews.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09257,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"Brazilian Favela Women: How Your Standard Solutions for Technology Abuse\n  Might Actually Harm Them\n\n  Brazil is home to over 200M people, the majority of which have access to the\nInternet. Over 11M Brazilians live in favelas, or informal settlements with no\noutside government regulation, often ruled by narcos or militias. Victims of\nintimate partner violence (IPV) in these communities are made extra vulnerable\nnot only by lack of access to resources, but by the added layer of violence\ncaused by criminal activity and police confrontations. In this paper, we use an\nunintended harms framework to analyze the unique online privacy needs of favela\nwomen and present research questions that we urge tech abuse researchers to\nconsider.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09346,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000159277,
      "text":"User Requirements and Analysis of Preeclampsia Detection done through a\n  Smart Bracelet\n\n  Medical students along with the medical staff have to monitor the state of\nthe patients by using modern devices which have to offer precise results in a\nshort amount of time, so that the intervention to be done as soon as possible.\nE-learning systems for blood pressure monitoring are used and new methods of\npatient observation, evaluation and treatment are applied compared to classical\nintervention. Based on this, medical students can improve their knowledge for\nthe practical training.\n  In the medical activities specialized devices occupy an important place. A\ndevice that can monitor the blood pressure is a smart bracelet that\nincorporates a pressure sensor along the wrist for continuous recording of\nblood pressure values. This enables the prediction of the emergency disorders\nusing a decision support system. It facilitates the learning of new\nintervention approaches and boosts the responsiveness among learners. According\nto the World Health Organization, hypertensive disorders affect about 10% of\npregnant women worldwide and are an important cause of disability and long-term\ndeath among mothers and children. This paper is based on a survey completed by\npersons of different ages and having various specialization domains regarding\nthe use of smart bracelets for detecting preeclampsia. The aim is to decide\nupon its popularity among people and to determine the user requirements. The\npregnant women will be constantly monitored, doctors can update the diagnosis\nof the patient. The medical students can learn from the critical situations and\nbenefit from these cases while learning. The results of the survey showed that\nmost of the interviewed persons consider the existence of such a device to be\nvery useful, mostly the female individuals would feel more comfortable to have\ntheir blood pressure monitored during pregnancy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.00407,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"Emotion and color in paintings: a novel temporal and spatial\n  quantitative perspective\n\n  As subjective artistic creations, artistic paintings carry emotion of their\ncreators. Emotions expressed in paintings and emotion aroused in spectators by\npaintings are two kinds of emotions that scholars have paid attention to.\nTraditional studies on emotions expressed by paintings are mainly conducted\nfrom qualitative perspectives, with neither quantitative output on the\nemotional values of a painting, nor exploration of trends in the expression of\nemotion in art history. In this research we threat facial expressions in\npaintings as an artistic characteristics of art history and employ cognitive\ncomputation technology to identify the facial emotions in paintings and to\ninvestigate the quantitative measures of paintings from three emotion-related\naspects: the spatial and temporal patterns of painting emotions in art history,\nthe gender difference on the emotion of paintings and the color preference\nassociated with emotions. We discovered that the emotion of happiness has a\ngrowing trend from ancient to modern times in paintings history, and men and\nwomen have different facial expressions patterns along time. As for color\npreference, artists with different culture backgrounds had similar association\npreferences between colors and emotions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.01392,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Modeling National Trends on Health in the Philippines Using ARIMA\n\n  Health is a very important prerequisite in peoples well-being and happiness.\nSeveral studies were more focused on presenting the occurrence on specific\ndisease like forecasting the number of dengue and malaria cases. This paper\nutilized the time series data for trend analysis and data forecasting using\nARIMA model to visualize the trends of health data on the ten leading causes of\ndeaths, leading cause of morbidity and leading cause of infants deaths\nparticularly in the Philippines presented in a tabular data. Figures for each\ndisease trend are presented individually with the use of the GRETL software.\nForecasting results of the leading causes of death showed that Diseases of the\nheart, vascular system, accidents, Chronic lower respiratory diseases and\nChronic Tuberculosis (all forms) showed a slight changed of the forecasted\ndata, Malignant neoplasms showed unstable behavior of the forecasted data, and\nPneumonia, diabetes mellitus, Nephritis, nephrotic syndrome and nephrosis and\ncertain conditions originating in perinatal showed a decreasing patterns based\non the forecasted data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04212,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Understanding health and behavioral trends of successful students\n  through machine learning models\n\n  This study analyzes patterns of physical, mental, lifestyle, and personality\nfactors in college students in different periods over the course of a semester\nand models their relationships with students' academic performance. The data\nanalyzed was collected through smartphones and Fitbit. The use of machine\nlearning models derived from the gathered data was employed to observe the\nextent of students' behavior associated with their GPA, lifestyle, physical\nhealth, mental health, and personality attributes. A mutual agreement method\nwas used in which rather than looking at the accuracy of results, the model\nparameters and weights of features were used to find common behavioral trends.\nFrom the results of the model creation, it was determined that the most\nsignificant indicator of academic success defined as a higher GPA, was the\nplaces a student spent their time. Lifestyle and personality factors were\ndeemed more significant than mental and physical factors. This study will\nprovide insight into the impact of different factors and the timing of those\nfactors on students' academic performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.03827,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Representativeness in Statistics, Politics, and Machine Learning\n\n  Representativeness is a foundational yet slippery concept. Though familiar at\nfirst blush, it lacks a single precise meaning. Instead, meanings range from\ntypical or characteristic, to a proportionate match between sample and\npopulation, to a more general sense of accuracy, generalizability, coverage, or\ninclusiveness. Moreover, the concept has long been contested. In statistics,\ndebates about the merits and methods of selecting a representative sample date\nback to the late 19th century; in politics, debates about the value of likeness\nas a logic of political representation are older still. Today, as the concept\ncrops up in the study of fairness and accountability in machine learning, we\nneed to carefully consider the term's meanings in order to communicate clearly\nand account for their normative implications. In this paper, we ask what\nrepresentativeness means, how it is mobilized socially, and what values and\nideals it communicates or confronts. We trace the concept's history in\nstatistics and discuss normative tensions concerning its relationship to\nlikeness, exclusion, authority, and aspiration. We draw on these analyses to\nthink through how representativeness is used in FAccT debates, with emphasis on\ndata, shift, participation, and power.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.08812,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"The Internet of Things in Ports: Six Key Security and Governance\n  Challenges for the UK (Policy Brief)\n\n  In January 2019, the UK Government published its Maritime 2050 on Navigating\nthe Future strategy. In the strategy, the government highlighted the importance\nof digitalization (with well-designed regulatory support) to achieve its goal\nof ensuring that the UK plays a global leadership role in the maritime sector.\nPorts, the gateways for 95% of UK trade movements, were identified as key sites\nfor investment in technological innovation. The government identified the\npotential of the Internet of Things (IoT), in conjunction with other\ninformation-sharing technologies, such as shared data platforms, and Artificial\nIntelligence applications (AI), to synchronize processes within the port\necosystem leading to improved efficiency, safety, and environmental benefits,\nincluding improved air quality and lower greenhouse gas emissions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12718,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"Comparative Performance of Machine Learning Algorithms in Cyberbullying\n  Detection: Using Turkish Language Preprocessing Techniques\n\n  With the increasing use of the internet and social media, it is obvious that\ncyberbullying has become a major problem. The most basic way for protection\nagainst the dangerous consequences of cyberbullying is to actively detect and\ncontrol the contents containing cyberbullying. When we look at today's internet\nand social media statistics, it is impossible to detect cyberbullying contents\nonly by human power. Effective cyberbullying detection methods are necessary in\norder to make social media a safe communication space. Current research efforts\nfocus on using machine learning for detecting and eliminating cyberbullying.\nAlthough most of the studies have been conducted on English texts for the\ndetection of cyberbullying, there are few studies in Turkish. Limited methods\nand algorithms were also used in studies conducted on the Turkish language. In\naddition, the scope and performance of the algorithms used to classify the\ntexts containing cyberbullying is different, and this reveals the importance of\nusing an appropriate algorithm. The aim of this study is to compare the\nperformance of different machine learning algorithms in detecting Turkish\nmessages containing cyberbullying. In this study, nineteen different\nclassification algorithms were used to identify texts containing cyberbullying\nusing Turkish natural language processing techniques. Precision, recall,\naccuracy and F1 score values were used to evaluate the performance of\nclassifiers. It was determined that the Light Gradient Boosting Model (LGBM)\nalgorithm showed the best performance with 90.788% accuracy and 90.949% F1\nScore value.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.12089,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Eye: Program Visualizer for CS2\n\n  In recent years, programming has witnessed a shift towards using standard\nlibraries as a black box. However, there has not been a synchronous development\nof tools that can help demonstrate the working of such libraries in general\nprograms, which poses an impediment to improved learning outcomes and makes\ndebugging exasperating. We introduce Eye, an interactive pedagogical tool that\nvisualizes a program's execution as it runs. It demonstrates properties and\nusage of data structures in a general environment, thereby helping in learning,\nlogical debugging, and code comprehension. Eye provides a comprehensive\noverview at each stage during run time including the execution stack and the\nstate of data structures. The modular implementation allows for extension to\nother languages and modification of the graphics as desired.\n  Eye opens up a gateway for CS2 students to more easily understand myriads of\nprograms that are available on online programming websites, lowering the\nbarrier towards self-learning of coding. It expands the scope of visualizing\ndata structures from standard algorithms to general cases, benefiting both\nteachers as well as programmers who face issues in debugging. Line by line\ninterpreting allows Eye to describe the execution and not only the current\nstate. We also conduct experiments to evaluate the efficacy of Eye for\ndebugging and comprehending a new piece of code. Our findings show that it\nbecomes faster and less frustrating to debug certain problems using this tool,\nand also makes understanding new code a much more pleasant experience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.01264,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"A Research Ecosystem for Secure Computing\n\n  Computing devices are vital to all areas of modern life and permeate every\naspect of our society. The ubiquity of computing and our reliance on it has\nbeen accelerated and amplified by the COVID-19 pandemic. From education to work\nenvironments to healthcare to defense to entertainment - it is hard to imagine\na segment of modern life that is not touched by computing. The security of\ncomputers, systems, and applications has been an active area of research in\ncomputer science for decades. However, with the confluence of both the scale of\ninterconnected systems and increased adoption of artificial intelligence, there\nare many research challenges the community must face so that our society can\ncontinue to benefit and risks are minimized, not multiplied. Those challenges\nrange from security and trust of the information ecosystem to adversarial\nartificial intelligence and machine learning.\n  Along with basic research challenges, more often than not, securing a system\nhappens after the design or even deployment, meaning the security community is\nroutinely playing catch-up and attempting to patch vulnerabilities that could\nbe exploited any minute. While security measures such as encryption and\nauthentication have been widely adopted, questions of security tend to be\nsecondary to application capability. There needs to be a sea-change in the way\nwe approach this critically important aspect of the problem: new incentives and\neducation are at the core of this change. Now is the time to refocus research\ncommunity efforts on developing interconnected technologies with security\n\"baked in by design\" and creating an ecosystem that ensures adoption of\npromising research developments. To realize this vision, two additional\nelements of the ecosystem are necessary - proper incentive structures for\nadoption and an educated citizenry that is well versed in vulnerabilities and\nrisks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04221,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"The Sanction of Authority: Promoting Public Trust in AI\n\n  Trusted AI literature to date has focused on the trust needs of users who\nknowingly interact with discrete AIs. Conspicuously absent from the literature\nis a rigorous treatment of public trust in AI. We argue that public distrust of\nAI originates from the under-development of a regulatory ecosystem that would\nguarantee the trustworthiness of the AIs that pervade society. Drawing from\nstructuration theory and literature on institutional trust, we offer a model of\npublic trust in AI that differs starkly from models driving Trusted AI efforts.\nThis model provides a theoretical scaffolding for Trusted AI research which\nunderscores the need to develop nothing less than a comprehensive and visibly\nfunctioning regulatory ecosystem. We elaborate the pivotal role of externally\nauditable AI documentation within this model and the work to be done to ensure\nit is effective, and outline a number of actions that would promote public\ntrust in AI. We discuss how existing efforts to develop AI documentation within\norganizations -- both to inform potential adopters of AI components and support\nthe deliberations of risk and ethics review boards -- is necessary but\ninsufficient assurance of the trustworthiness of AI. We argue that being\naccountable to the public in ways that earn their trust, through elaborating\nrules for AI and developing resources for enforcing these rules, is what will\nultimately make AI trustworthy enough to be woven into the fabric of our\nsociety.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.02968,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000223849,
      "text":"Group Fairness: Independence Revisited\n\n  This paper critically examines arguments against independence, a measure of\ngroup fairness also known as statistical parity and as demographic parity. In\nrecent discussions of fairness in computer science, some have maintained that\nindependence is not a suitable measure of group fairness. This position is at\nleast partially based on two influential papers (Dwork et al., 2012, Hardt et\nal., 2016) that provide arguments against independence. We revisit these\narguments, and we find that the case against independence is rather weak. We\nalso give arguments in favor of independence, showing that it plays a\ndistinctive role in considerations of fairness. Finally, we discuss how to\nbalance different fairness considerations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.01671,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"A National Research Agenda for Intelligent Infrastructure: 2021 Update\n\n  Strategic, sustained Federal investments in intelligent infrastructure will\nincrease safety and resilience, improve efficiencies and civic services, and\nbroaden employment opportunities and job growth nationwide. The technologies\nthat comprise intelligent infrastructure can also provide keys to solving some\nof the most vexing challenges we face today, including confronting future\npandemics and natural disasters, achieving sustainability and energy efficiency\ngoals, and advancing social justice. Enabling those technologies effectively\nwill require investment in the associated computing research as well, beyond\nand in concert with the basic building projects. In 2017, the Computing\nCommunity Consortium (CCC) produced a series of intelligent infrastructure\nwhitepapers, and in 2020 CCC issued a set of companion whitepapers on closely\nrelated topics. Here we briefly survey those earlier works, and then highlight\nfour themes of rising national prominence where intelligent infrastructure can\nalso play an enabling role, driven by experiences with the COVID-19 pandemic\nand the social justice movement. We conclude with recommendations for the\nnecessary research investments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04217,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0004461077,
      "text":"Understanding Underground Incentivized Review Services\n\n  While human factors in fraud have been studied by the HCI and security\ncommunities, most research has been directed to understanding either the\nvictims' perspectives or prevention strategies, and not on fraudsters, their\nmotivations and operation techniques. Additionally, the focus has been on a\nnarrow set of problems: phishing, spam and bullying. In this work, we seek to\nunderstand review fraud on e-commerce platforms through an HCI lens. Through\nsurveys with real fraudsters (N=36 agents and N=38 reviewers), we uncover\nsophisticated recruitment, execution, and reporting mechanisms fraudsters use\nto scale their operation while resisting takedown attempts, including the use\nof AI tools like ChatGPT. We find that countermeasures that crack down on\ncommunication channels through which these services operate are effective in\ncombating incentivized reviews. This research sheds light on the complex\nlandscape of incentivized reviews, providing insights into the mechanics of\nunderground services and their resilience to removal efforts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.01693,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000698037,
      "text":"COVID-19 Tests Gone Rogue: Privacy, Efficacy, Mismanagement and\n  Misunderstandings\n\n  COVID-19 testing, the cornerstone for effective screening and identification\nof COVID-19 cases, remains paramount as an intervention tool to curb the spread\nof COVID-19 both at local and national levels. However, the speed at which the\npandemic struck and the response was rolled out, the widespread impact on\nhealthcare infrastructure, the lack of sufficient preparation within the public\nhealth system, and the complexity of the crisis led to utter confusion among\ntest-takers. Invasion of privacy remains a crucial concern. The user experience\nof test takers remains low. User friction affects user behavior and discourages\nparticipation in testing programs. Test efficacy has been overstated. Test\nresults are poorly understood resulting in inappropriate follow-up\nrecommendations. Herein, we review the current landscape of COVID-19 testing,\nidentify four key challenges, and discuss the consequences of the failure to\naddress these challenges. The current infrastructure around testing and\ninformation propagation is highly privacy-invasive and does not leverage\nscalable digital components. In this work, we discuss challenges complicating\nthe existing covid-19 testing ecosystem and highlight the need to improve the\ntesting experience for the user and reduce privacy invasions. Digital tools\nwill play a critical role in resolving these challenges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.051,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000041392,
      "text":"Demystifying Removed Apps in iOS App Store\n\n  With the popularity of mobile devices, mobile applications have become an\nessential part of people's lives. To provide secure mobile application download\nchannels for users, various modern app markets are maintained by different\ncompanies. For example, Google maintains Google Play for Android users, while\nApple maintains App Store for iOS, iPadOS, and MacOS users. Though app markets\nhave come up with strict policies which impose restrictions on developers to\navoid the potential harmful applications, we still have quite limited knowledge\non the process of app vetting and the status of potential harmful apps. To fill\nthis gap, this paper takes the initiative to conduct a large-scale and\nlongitudinal study of removed apps in the iOS app store. Our analysis reveals\nthat although most of the removed apps are low-quality apps, a number of them\nare quite popular. Furthermore, the mis-behaviors of these apps are reflected\non app metadata, which makes it possible to distinguish potential harmful apps.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06105,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0001758999,
      "text":"Scientific Relevance and Future of Digital Immortality and Virtual\n  Humans\n\n  We are on the threshold of a significant change in the way we view digital\nlife, which will have a major effect on the physical world. Computers have\nincreasingly emulated deceased human beings through growing awareness in the\nfields of artificial intelligence, big data, and machine learning, and have\nsymbolically managed to overcome death with the help of technology. One thing\nis clear, though: now that there are proper and legitimate discussions\nhappening about human immortality, we can be certain that the future is upon\nus. This article attempts to explain and challenge the ways in which digital\nimmortality, in particular, has manifested itself. This paper summarizes the\ntechnological solutions, research findings and technical challenges of major\nresearchers by reviewing the key technologies and general technical schemes in\nthe field of digital human beings. The prospects of digital human beings are\nbeing investigated.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.1013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Insights into the Impact of COVID-19 on Bicycle Usage in Colorado\n  Counties\n\n  Coronavirus, which emerged in China towards the end of 2019 and subsequently\ninfluenced the whole world, has changed the daily lives of people to a great\nextent. In many parts of the world, in both cities and rural areas, people have\nbeen forced to stay home weeks. They have only been allowed to leave home for\nfundamental needs such as food and health needs, and most started to work from\nhome. In this period, very few people, including essential workers, had to\nleave their homes. Avoiding social contact is proven to be the best method to\nreduce the spread of the novel Coronavirus. Because of the COVID-19 pandemic,\npeople are adapting their behavior to this new reality, and it may change the\ntype of public events people perform and how people go to these activities.\nConsumer behaviors have been altered during the pandemic. While people try to\navoid gatherings, they also stayed away from mass transport modes and turned to\nprivate modes of transportation more -- private cars, private taxis and\nbike-sharing systems; even walking became more popular. In this study, we\nattempt to analyze how the use of bicycling has changed -- pre- and\npost-pandemic -- using open data sources and investigating how socio-economics\ncharacteristics affect this change. The results showed that average income,\naverage education level, and total population are the most crucial variables\nfor the Pandemic to Transition period and the Transition to the Normalization\nperiod.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000041392,
      "text":"The Challenges of Assessing and Evaluating the Students at Distance\n\n  The COVID-19 pandemic has caused a strong effect on higher education\ninstitutions with the closure of classroom teaching activities. In this\nunprecedented crisis, of global proportion, educators and families had to deal\nwith unpredictability and learn new ways of teaching. This short essay aims to\nexplore the challenges posed to Portuguese higher education institutions and to\nanalyze the challenges posed to evaluation models. To this end, the relevance\nof formative and summative assessment models in distance education is explored\nand the perception of teachers and students about the practices adopted in\nremote assessment is discussed. On the teachers' side, there is a high concern\nabout adopting fraud-free models, and an excessive focus on the summative\nassessment component that in the distance learning model has less preponderance\nwhen compared to the gradual monitoring and assessment processes of the\nstudents, while on the students' side, problems arise regarding equipment to\nfollow the teaching sessions and concerns about their privacy, particularly\nwhen intrusive IT solutions request the access to their cameras, audio, and\ndesktop.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11625,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Assessing the Readability of Policy Documents on the Digital Single\n  Market of the European Union\n\n  Today, literature skills are necessary. Engineering and other technical\nprofessions are not an exception from this requirement. Traditionally,\ntechnical reading and writing have been framed with a limited scope, containing\ndocumentation, specifications, standards, and related text types. Nowadays,\nhowever, the scope covers also other text types, including legal, policy, and\nrelated documents. Given this motivation, this paper evaluates the readability\nof 201 legislations and related policy documents in the European Union (EU).\nThe digital single market (DSM) provides the context. Five classical\nreadability indices provide the methods; these are quantitative measures of a\ntext's readability. The empirical results indicate that (i) generally a Ph.D.\nlevel education is required to comprehend the DSM laws and policy documents.\nAlthough (ii) the results vary across the five indices used, (iii) readability\nhas slightly improved over time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.02279,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Insiders and Outsiders in Research on Machine Learning and Society\n\n  A subset of machine learning research intersects with societal issues,\nincluding fairness, accountability and transparency, as well as the use of\nmachine learning for social good. In this work, we analyze the scholars\ncontributing to this research at the intersection of machine learning and\nsociety through the lens of the sociology of science. By analyzing the\nauthorship of all machine learning papers posted to arXiv, we show that\ncompared to researchers from overrepresented backgrounds (defined by gender and\nrace\/ethnicity), researchers from underrepresented backgrounds are more likely\nto conduct research at this intersection than other kinds of machine learning\nresearch. This state of affairs leads to contention between two perspectives on\ninsiders and outsiders in the scientific enterprise: outsiders being those\noutside the group being studied, and outsiders being those who have not\nparticipated as researchers in an area historically. This contention manifests\nas an epistemic question on the validity of knowledge derived from lived\nexperience in machine learning research, and predicts boundary work that we see\nin a real-world example.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.08537,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Political Bias and Factualness in News Sharing across more than 100,000\n  Online Communities\n\n  As civil discourse increasingly takes place online, misinformation and the\npolarization of news shared in online communities have become ever more\nrelevant concerns with real world harms across our society. Studying online\nnews sharing at scale is challenging due to the massive volume of content which\nis shared by millions of users across thousands of communities. Therefore,\nexisting research has largely focused on specific communities or specific\ninterventions, such as bans. However, understanding the prevalence and spread\nof misinformation and polarization more broadly, across thousands of online\ncommunities, is critical for the development of governance strategies,\ninterventions, and community design. Here, we conduct the largest study of news\nsharing on reddit to date, analyzing more than 550 million links spanning 4\nyears. We use non-partisan news source ratings from Media Bias\/Fact Check to\nannotate links to news sources with their political bias and factualness. We\nfind that, compared to left-leaning communities, right-leaning communities have\n105% more variance in the political bias of their news sources, and more links\nto relatively-more biased sources, on average. We observe that reddit users'\nvoting and re-sharing behaviors generally decrease the visibility of extremely\nbiased and low factual content, which receives 20% fewer upvotes and 30% fewer\nexposures from crossposts than more neutral or more factual content. This\nsuggests that reddit is more resilient to low factual content than Twitter. We\nshow that extremely biased and low factual content is very concentrated, with\n99% of such content being shared in only 0.5% of communities, giving credence\nto the recent strategy of community-wide bans and quarantines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00541,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"Dados Abertos Governamentais no contexto de Pol\\'iticas P\\'ublicas de\n  Sa\\'ude e Sistemas Prisionais: Realidade ou Utopia?\n\n  There are many initiatives of transparency reported in the access and use of\ngovernment open data for different purposes. This practice reveals an important\nrequirement to accomplish the participatory governance. The literature has\nreported a minimal set of criteria to categorize a specific data repository as\nopen. This paper discusses to which extent specific national and international\ndata repositories address these criteria. The analyzed repositories focus on\npublic health and prison systems. The results show different levels of\nalignment to the criteria and provide evidence that the adoption of government\nopen data practices are a reality. On the other hand, there is still a long way\nto achieve full alignment to the stated criteria.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01648,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"The Privatization of AI Research(-ers): Causes and Potential\n  Consequences -- From university-industry interaction to public research\n  brain-drain?\n\n  The private sector is playing an increasingly important role in basic\nArtificial Intelligence (AI) R&D. This phenomenon, which is reflected in the\nperception of a brain drain of researchers from academia to industry, is\nraising concerns about a privatisation of AI research which could constrain its\nsocietal benefits. We contribute to the evidence base by quantifying transition\nflows between industry and academia and studying its drivers and potential\nconsequences. We find a growing net flow of researchers from academia to\nindustry, particularly from elite institutions into technology companies such\nas Google, Microsoft and Facebook. Our survival regression analysis reveals\nthat researchers working in the field of deep learning as well as those with\nhigher average impact are more likely to transition into industry. A\ndifference-in-differences analysis of the effect of switching into industry on\na researcher's influence proxied by citations indicates that an initial\nincrease in impact declines as researchers spend more time in industry. This\npoints at a privatisation of AI knowledge compared to a counterfactual where\nthose high-impact researchers had remained in academia. Our findings highlight\nthe importance of strengthening the public AI research sphere in order to\nensure that the future of this powerful technology is not dominated by private\ninterests.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09366,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Software startup education: gamifying growth hacking\n\n  Startups seek to create highly scalable business models. For startups, growth\nis thus vital. Growth hacking is a marketing strategy advocated by various\nstartup practitioner experts. It focuses on using low cost practices while\nutilizing existing platforms in creative ways to gain more users for the\nservice. Though topics related to growth hacking such as marketing on a general\nlevel have been extensively studied in the past, growth hacking as a\npractitioner-born topic has not seen much interesting among the academia. To\nboth spark interest in growth hacking, and to facilitate teaching growth\nhacking in the academia, we present two board games intended to serve as an\nengaging introduction to growth hacking for students.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12406,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Actionable Principles for Artificial Intelligence Policy: Three Pathways\n\n  In the development of governmental policy for artificial intelligence (AI)\nthat is informed by ethics, one avenue currently pursued is that of drawing on\nAI Ethics Principles. However, these AI Ethics Principles often fail to be\nactioned in governmental policy. This paper proposes a novel framework for the\ndevelopment of Actionable Principles for AI. The approach acknowledges the\nrelevance of AI Ethics Principles and homes in on methodological elements to\nincrease their practical implementability in policy processes. As a case study,\nelements are extracted from the development process of the Ethics Guidelines\nfor Trustworthy AI of the European Commissions High Level Expert Group on AI.\nSubsequently, these elements are expanded on and evaluated in light of their\nability to contribute to a prototype framework for the development of\nActionable Principles for AI. The paper proposes the following three\npropositions for the formation of such a prototype framework: (1) preliminary\nlandscape assessments; (2) multi-stakeholder participation and cross-sectoral\nfeedback; and, (3) mechanisms to support implementation and\noperationalizability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12837,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Are Anti-Feminist Communities Gateways to the Far Right? Evidence from\n  Reddit and YouTube\n\n  Researchers have suggested that \"the Manosphere,\" a conglomerate of\nmen-centered online communities, may serve as a gateway to far right movements.\nIn that context, this paper quantitatively studies the migratory patterns\nbetween a variety of groups within the Manosphere and the Alt-right, a loosely\nconnected far right movement that has been particularly active in mainstream\nsocial networks. Our analysis leverages over 300 million comments spread\nthrough Reddit (in 115 subreddits) and YouTube (in 526 channels) to investigate\nwhether the audiences of channels and subreddits associated with these\ncommunities have converged between 2006 and 2018. In addition to subreddits\nrelated to the communities of interest, we also collect data on counterparts:\nother groups of users which we use for comparison (e.g., for YouTube we use a\nset of media channels). Besides measuring the similarity in the commenting user\nbases of these communities, we perform a migration study, calculating to which\nextent users in the Manosphere gradually engage with Alt-right content. Our\nresults suggest that there is a large overlap between the user bases of the\nAlt-right and of the Manosphere and that members of the Manosphere have a\nbigger chance to engage with far right content than carefully chosen\ncounterparts. However, our analysis also shows that migration and user base\noverlap varies substantially across different platforms and within the\nManosphere. Members of some communities (e.g., Men's Rights Activists)\ngradually engage with the Alt-right significantly more than counterparts on\nboth Reddit and YouTube, whereas for other communities, this engagement happens\nmostly on Reddit (e.g., Pick Up Artists). Overall, our work paints a nuanced\npicture of the pipeline between the Manosphere and the Alt-right, which may\ninform platforms' policies and moderation decisions regarding these\ncommunities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10531,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Exploring the dynamics of protest against National Register of Citizens\n  & Citizenship Amendment Act through online social media: the Indian\n  experience\n\n  The generic fluidity observed in the nature of political protest movements\nacross the world during the last decade weigh heavily with the presence of\nsocial media. As such, there is a possibility to study the contemporary\nmovements with an interdisciplinary approach combining computational analytics\nwith social science perspectives. The present study has put efforts to\nunderstand such dynamics in the context of the ongoing nationwide movement in\nIndia opposing the NRC-CAA enactment. The transformative nature of individual\ndiscontent into collective mobilization, especially with a reflective\nintervention in social media across a sensitive region of the nation state, is\npresented here with a combination of qualitative (fieldwork) and quantitative\n(computing) techniques. The study is augmented further by the primary data\ngeneration coupled with real-time application of analytical approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09364,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Ethics as a service: a pragmatic operationalisation of AI Ethics\n\n  As the range of potential uses for Artificial Intelligence (AI), in\nparticular machine learning (ML), has increased, so has awareness of the\nassociated ethical issues. This increased awareness has led to the realisation\nthat existing legislation and regulation provides insufficient protection to\nindividuals, groups, society, and the environment from AI harms. In response to\nthis realisation, there has been a proliferation of principle-based ethics\ncodes, guidelines and frameworks. However, it has become increasingly clear\nthat a significant gap exists between the theory of AI ethics principles and\nthe practical design of AI systems. In previous work, we analysed whether it is\npossible to close this gap between the what and the how of AI ethics through\nthe use of tools and methods designed to help AI developers, engineers, and\ndesigners translate principles into practice. We concluded that this method of\nclosure is currently ineffective as almost all existing translational tools and\nmethods are either too flexible (and thus vulnerable to ethics washing) or too\nstrict (unresponsive to context). This raised the question: if, even with\ntechnical guidance, AI ethics is challenging to embed in the process of\nalgorithmic design, is the entire pro-ethical design endeavour rendered futile?\nAnd, if no, then how can AI ethics be made useful for AI practitioners? This is\nthe question we seek to address here by exploring why principles and technical\ntranslational tools are still needed even if they are limited, and how these\nlimitations can be potentially overcome by providing theoretical grounding of a\nconcept that has been termed Ethics as a Service.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11147,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000074506,
      "text":"Improving Concept Learning Through Specialized Digital Fanzines\n\n  Specialized digital fanzines were successfully used to facilitate learning\nproblematic concepts in an undergraduate programming course, dynamically\nadapting to student needs. The design of these fanzines favors creating and\nreading them quickly by establishing a common graphical layout, rules, and\nfocusing in the most problematic parts of the concepts. This paper details the\nagile fanzine creation procedure, the way problematic concepts were identified\nand quickly handled, and how this approach was implemented in an actual course,\nso it could be applied to other courses with similar needs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09391,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000330144,
      "text":"Interleaving Computational and Inferential Thinking: Data Science for\n  Undergraduates at Berkeley\n\n  The undergraduate data science curriculum at the University of California,\nBerkeley is anchored in five new courses that emphasize computational thinking,\ninferential thinking, and working on real-world problems. We believe that\ninterleaving these elements within our core courses is essential to preparing\nstudents to engage in data-driven inquiry at the scale that contemporary\nscientific and industrial applications demand. This new curriculum is already\nreshaping the undergraduate experience at Berkeley, where these courses have\nbecome some of the most popular on campus and have led to a surging interest in\na new undergraduate major and minor program in data science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09401,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Blockchain in agriculture\n\n  Blockchain is an emerging digital technology allowing ubiquitous financial\ntransactions among distributed untrusted parties, without the need of\nintermediaries such as banks. This chapter examines the impact of blockchain\ntechnology in agriculture and food supply chain, presents existing ongoing\nprojects and initiatives, and discusses overall implications, challenges and\npotential, with a critical view over the maturity of these projects. Our\nfindings indicate that blockchain is a promising technology towards a\ntransparent supply chain of food, with many ongoing initiatives in various food\nproducts and food-related issues, but many barriers and challenges still exist,\nwhich hinder its wider popularity among farmers and systems. These challenges\ninvolve technical aspects, education, policies and regulatory frameworks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000038412,
      "text":"Measuring Asset Composability as a Proxy for DeFi Integration\n\n  Decentralized financial (DeFi) applications on the Ethereum blockchain are\nhighly interoperable because they share a single state in a deterministic\ncomputational environment. Stakeholders can deposit claims on assets, referred\nto as 'liquidity shares', across applications producing effects equivalent to\nrehypothecation in traditional financial systems. We seek to understand the\ndegree to which this practice may contribute to financial integration on\nEthereum by examining transactions in 'composed' derivatives for the assets\nDAI, USDC, USDT, ETH and tokenized BTC for the full set of 344.8 million\nEthereum transactions computed in 2020. We identify a salient trend for\n'composing' assets in multiple sequential generations of derivatives and\ncomment on potential systemic implications for the Ethereum network.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09103,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Gender Bias, Social Bias and Representation: 70 Years of B$^H$ollywood\n\n  With an outreach in more than 90 countries, a market share of 2.1 billion\ndollars and a target audience base of at least 1.2 billion people, Bollywood,\naka the Mumbai film industry, is a formidable entertainment force. While the\nnumber of lives Bollywood can potentially touch is massive, no comprehensive\nNLP study on the evolution of social and gender biases in Bollywood dialogues\nexists. Via a substantial corpus of movie dialogues spanning a time horizon of\n70 years, we seek to understand the portrayal of women, in a broader context\nstudying subtle social signals, and analyze the evolving trends in geographic\nand religious representation in India. Our argument is simple -- popular movie\ncontent reflects social norms and beliefs in some form or shape. In this\nproject, we propose to analyze such trends over 70 years of Bollywood movies\ncontrasting them with their Hollywood counterpart and critically acclaimed\nworld movies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03656,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Under the Spotlight: Web Tracking in Indian Partisan News Websites\n\n  India is experiencing intense political partisanship and sectarian divisions.\nThe paper performs, to the best of our knowledge, the first comprehensive\nanalysis on the Indian online news media with respect to tracking and\npartisanship. We build a dataset of 103 online, mostly mainstream news\nwebsites. With the help of two experts, alongside data from the Media Ownership\nMonitor of the Reporters without Borders, we label these websites according to\ntheir partisanship (Left, Right, or Centre). We study and compare user tracking\non these sites with different metrics: numbers of cookies, cookie\nsynchronizations, device fingerprinting, and invisible pixel-based tracking. We\nfind that Left and Centre websites serve more cookies than Right-leaning\nwebsites. However, through cookie synchronization, more user IDs are\nsynchronized in Left websites than Right or Centre. Canvas fingerprinting is\nused similarly by Left and Right, and less by Centre. Invisible pixel-based\ntracking is 50% more intense in Centre-leaning websites than Right, and 25%\nmore than Left. Desktop versions of news websites deliver more cookies than\ntheir mobile counterparts. A handful of third-parties are tracking users in\nmost websites in this study. This paper, by demonstrating intense web tracking,\nhas implications for research on overall privacy of users visiting partisan\nnews websites in India.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.07119,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Sociotechnical Challenges of eHealth Technology for Patient\n  Self-Management: A Systematic Review\n\n  Ageing of society and increase of time spent with chronic conditions\nchallenge the traditional long-term care model. Assistive technology and\neHealth are seen to play an important role when addressing these challenges.\nOne prominent example are patient self-management systems. These systems not\nonly transform the way patients with chronic conditions interact with the\nhealthcare system, but also change work practices of care providers. This\nliterature review addresses sociotechnical challenges of eHealth technologies\nwith a strong collaborative component. As a result, four themes are identified\nand discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04567,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000356966,
      "text":"NELA-GT-2020: A Large Multi-Labelled News Dataset for The Study of\n  Misinformation in News Articles\n\n  In this paper, we present an updated version of the NELA-GT-2019 dataset,\nentitled NELA-GT-2020. NELA-GT-2020 contains nearly 1.8M news articles from 519\nsources collected between January 1st, 2020 and December 31st, 2020. Just as\nwith NELA-GT-2018 and NELA-GT-2019, these sources come from a wide range of\nmainstream news sources and alternative news sources. Included in the dataset\nare source-level ground truth labels from Media Bias\/Fact Check (MBFC) covering\nmultiple dimensions of veracity. Additionally, new in the 2020 dataset are the\nTweets embedded in the collected news articles, adding an extra layer of\ninformation to the data. The NELA-GT-2020 dataset can be found at\nhttps:\/\/doi.org\/10.7910\/DVN\/CHMUYZ.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01761,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000109275,
      "text":"Urban Building Energy Modeling (UBEM) Tools: A State-of-the-Art Review\n  of bottom-up physics-based approaches\n\n  Regulations corroborate the importance of retrofitting existing building\nstocks or constructing new energy efficient district. There is, thus, a need\nfor modeling tools to evaluate energy scenarios to better manage and design\ncities, and numerous methodologies and tools have been developed. Among them,\nUrban Building Energy Modeling (UBEM) tools allow the energy simulation of\nbuildings at large scales. Choosing an appropriate UBEM tool, balancing the\nlevel of complexity, accuracy, usability, and computing needs, remains a\nchallenge for users. The review focuses on the main bottom-up physics-based\nUBEM tools, comparing them from a user-oriented perspective. Five categories\nare used: (i) the required inputs, (ii) the reported outputs, (iii) the\nexploited workflow, (iv) the applicability of each tool, and (v) the potential\nusers. Moreover, a critical discussion is proposed focusing on interests and\ntrends in research and development. The results highlighted major differences\nbetween UBEM tools that must be considered to choose the proper one for an\napplication. Barriers of adoption of UBEM tools include the needs of a\nstandardized ontology, a common three dimensional city model, a standard\nprocedure to collect data, and a standard set of test cases. This feeds into\nfuture development of UBEM tools to support cities' sustainability goals.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10522,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Effect of Cloud Based Learning Management System on The Learning\n  Management System Implementation Process: Faculty and Student Perspectives\n\n  The concept of E-learning in Universities has grown rapidly over the years to\ninclude not just only a learning management system but also tools initially not\ndesigned for learning such as Facebook and advanced learning tools, for example\ngames, simulations and virtualization. As a result, Cloud-based LMS is being\ntouted as the next evolution of the traditional LMS. It is hoped that Cloud\nbased LMS will resolve some of the challenges associated with the traditional\nLMS implementation process. In a previous study, we reported that lack of\ninvolvement of faculty and students in the LMS implementation process results\nin the limited use of the LMS by faculty and students. The question then is,\nWill the cloud-based LMS resolve these issues? We conducted a review of\nliterature and presented an overview of the traditional LMS, cloud computing\nand the cloudbased LMS and we described how the cloud computing LMS resolve\nissues raised by faculty and students. we find that even though, cloud-based\nLMS resolve most of the technical issues associated with the traditional LMS,\nsome of the human issues were not resolved. We hope that this study draws\nattention to non-technical issues associated with the LMS implementation\nprocess.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.07215,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Development of An Assessment Benchmark for Synchronous Online Learning\n  for Nigerian Universities\n\n  In recent times, as a result of COVID-19 pandemic, higher institutions in\nNigeria have been shutdown and the leadership of Academic Staff Union of\nUniversity (ASUU) said that Nigerian universities cannot afford to mount Online\nlearning platforms let alone conduct such learning system in Nigeria due to\nlack of infrastructure, capacity and skill sets in the face of COVID-19\npandemic. In the light of this, this research undertook an online survey using\nUniversity of Nigeria, Nsukka (UNN) as a case study to know which type of\nonline learning system ASUU leadership is talking about - Asynchronous or\nSynchronous? How did ASUU come about their facts? Did ASUU base their assertion\non facts, if YES, what are the benchmarks? Therefore, this research project is\nfocused on providing benchmarks to assess if a Nigerian University has what it\ntakes to run a synchronous Online Learning. It includes Infrastructure needed\n(Hardware, Software, Network connectivity), Skill sets from staff (Computer\nliteracy level). In a bid to do this, an online survey was administered to the\nstaff of Centre for Distance and E-learning of UNN and out of the 40 members of\nthat section of the University, we had 32 respondents. The survey seeks to find\nwhether UNN has the requisite infrastructure and the skill sets to mount\nsynchronous online learning. The available results of the study reveal that UNN\nis deficit in both the requisite infrastructure and Skills sets to mount\nsynchronous online learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12539,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000006689,
      "text":"Exploratory Learning Environments for Responsible Management Education\n  Using Lego Serious Play\n\n  Research into responsible management education has largely focused on the\nmerits, attributes, and transformation opportunities to enhance responsible\nbusiness school education aims. As such, a prominent part of the literature has\noccupied itself with examining if responsible management modules are inherently\nconsidered a non-crucial element of the curriculum and determining the extent\nto which business schools have introduced such learning content into their\ncurriculum. However, there has been scant research into how to apply novel\nteaching approaches to engage students and promote responsible management\neducation endeavours. As such, this paper seeks to address this gap through the\ndevelopment of a teaching framework to support educators in designing effective\nlearning environments focused on responsible management education. We will draw\non constructivist learning theories and Lego Serious Play (LSP) as a learning\nenhancement approach to develop a pedagogical framework. LSP is selected due to\nits increasing application in learning environments to help promote critical\ndiscourse, and engage with highly complex problems, whether these are social,\neconomic, environmental, or organisational.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.09049,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000067552,
      "text":"Implementation of Technology Acceptance Model (TAM) and Importance\n  Performance Analysis (IPA) in Testing the Ease and Usability of E-wallet\n  Applications\n\n  Digital payment innovation is currently increasingly needed by the community,\nespecially in making non-cash payment transactions. The purpose of this\nresearch is to know and measure the ease and usefulness of e-wallet digital\nwallet services, especially in the GoPay application. The population in this\nstudy are users of the Go-Pay service on the GO-JEK platform. The sample of\nthis study consisted of 124 respondents from distributing questionnaires in\nDepok, West Java using a modified Technology Acceptance Model (TAM) based on\nexisting references. The data processing in this research uses Importance\nPerformance Analysis (IPA) analysis. The results show that based on the gap\nanalysis, it is found that in general Go-Pay users are not satisfied with the\ncurrent service quality. Based on the IPA analysis, the priority scale of\nE-Wallet Go-Pay quality improvement can be mapped, where quadrant I is the\nhighest priority scale according to the user's perspective: [1], [4], [5], and\n[6]. These three items must be upgraded immediately by the manager to meet user\nexpectations. Areas that become the achievements or advantages of the GoPay\nE-Wallet that must be maintained are in quadrant II, namely: [2] and [3]. From\nthis explanation, it can be concluded that in general the E-Wallet GoPay\nService must be improved to improve its service performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.16455,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Privacidade digital como direito do cidadao: o caso dos grupos indigenas\n  do Brasil\n\n  The article presents a brief review of the Brazilian legislation that impacts\nindigenous communities' digital privacy rights through governmental data\ncollection. Furthermore, the article aims to discuss the need to collect\nsensitive data from indigenous communities without the participation of the\nsame communities in developing the features for the data collection. The\narticle argues that the digitalization of indigenous communities' information\nfollows a colonial paradigm, harming entire indigenous communities and\nworsening their already precarious situation.\n  --\n  O artigo apresenta uma breve revis\\~ao da legisla\\c{c}\\~ao brasileira que\nimpacta os direitos de privacidade digital das comunidades ind\\'igenas por meio\nda coleta de dados governamentais. Al\\'em disso, o artigo visa discutir a\nnecessidade de coletar dados sens\\'iveis de comunidades ind\\'igenas sem a\nparticipa\\c{c}\\~ao das mesmas comunidades no desenvolvimento dos recursos para\na coleta de dados. O artigo argumenta que a digitaliza\\c{c}\\~ao das\ninforma\\c{c}\\~oes das comunidades ind\\'igenas segue um paradigma colonial,\nprejudicando comunidades ind\\'igenas inteiras e agravando sua situa\\c{c}\\~ao\nj\\'a prec\\'aria.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0407,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"A Strategy for Advancing Research and Impact in New Computing Paradigms\n\n  In the world of Information Technology, new computing paradigms, driven by\nrequirements of different classes of problems and applications, emerge rapidly.\nThese new computing paradigms pose many new research challenges. Researchers\nfrom different disciplines are working together to develop innovative solutions\naddressing them. In newer research areas with many unknowns, creating roadmaps,\nenabling tools, inspiring technological and application demonstrators offer\nconfidence and prove feasibility and effectiveness of new paradigm. Drawing on\nour experience, we share strategy for advancing the field and community\nbuilding in new and emerging computing research areas. We discuss how the\ndevelopment simulators can be cost-effective in accelerating design of real\nsystems. We highlight strategic role played by different types of publications,\nconferences, and educational programs. We illustrate effectiveness of elements\nof our strategy with a case study on progression of cloud computing paradigm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.04811,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000139409,
      "text":"A Framework for Enabling Safe and Resilient Food Factories for the\n  Public Feeding Programs\n\n  Public feeding programs continue to be a major source of nutrition to a large\npart of the population across the world. Any disruption to these activities,\nlike the one during the Covid-19 pandemic, can lead to adverse health outcomes,\nespecially among children. Policymakers and other stakeholders must balance the\nneed for continuing the feeding programs while ensuring the health and safety\nof workers engaged in the operations. This has led to several innovations that\nleverage advanced technologies like AI and IOT to monitor the health and safety\nof workers and ensure hygienic operations. However, there are practical\nchallenges in its implementation on a large scale. This paper presents an\nimplementation framework to build resilient public feeding programs using a\ncombination of intelligent technologies. The framework is a result of piloting\nthe technology solution at a facility run as part of a large mid-day meal\nfeeding program in India. Using existing resources like CCTV cameras and new\ntechnologies like AI and IOT, hygiene and safety compliance anomalies can be\ndetected and reported in a resource-efficient manner. It will guide\nstakeholders running public feeding programs as they seek to restart suspended\noperations and build systems that better adapt to future crises.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05252,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"Implementation of Departmental and Periodical Examination Analyzer\n  System\n\n  Administering examinations both in public and private academic institutions\ncan be tedious and unmanageable. The multiplicity of problems affecting the\nconduct of departmental and periodical examination can be greatly reduced by\nautomating the examination process. The purpose of this action research is to\nprovide an alternative technical solution in administering test through the use\nof Examination System. This software application can facilitate a plenitude of\nexaminees for different subjects that implements a random questioning technique\nand can generate item analysis and test results. The Departmental and\nPeriodical Examination System was developed using Visual Basic language. The\nsoftware modules were tested using the functional testing method. Using the\ncriteria and metrics of ISO 9126 software quality model, the system was\nevaluated by a group of students, teachers, school administrators and\ninformation technology professionals and has received an overall weighted mean\nof 4.56585 with an excellent descriptive rating. Therefore, the performance of\nthe application software provides solution that can surmount the gargantuan\nproblems of test administration and post-examination issues and performs all\nthe operations specified in the objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.12411,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"CubeFlow: Money Laundering Detection with Coupled Tensors\n\n  Money laundering (ML) is the behavior to conceal the source of money achieved\nby illegitimate activities, and always be a fast process involving frequent and\nchained transactions. How can we detect ML and fraudulent activity in large\nscale attributed transaction data (i.e.~tensors)? Most existing methods detect\ndense blocks in a graph or a tensor, which do not consider the fact that money\nare frequently transferred through middle accounts. CubeFlow proposed in this\npaper is a scalable, flow-based approach to spot fraud from a mass of\ntransactions by modeling them as two coupled tensors and applying a novel\nmulti-attribute metric which can reveal the transfer chains accurately.\nExtensive experiments show CubeFlow outperforms state-of-the-art baselines in\nML behavior detection in both synthetic and real data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06412,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Designing and Implementing e-justice Systems: An Information Systems\n  Approach to Regional Trial Court Case Docket Management in Northern Mindanao,\n  Philippines\n\n  Computer-based information systems for case management are still at an early\nstage of adoption in many trial courts in the Philippines. In most cases,\ninformation system implemented is the case docket using the official record\nbook on which cases are written and inventory of cases and reports are\ngenerated. This is a standalone system that often face data processing, data\nsecurity and case management challenges. However, there are examples of\nInformation systems in overcoming these pitfalls and producing innovative\nsolutions that surpass data management practices in in many trial courts in the\ncountry. One such case is the Regional Trial Court Branch 23 of Cagayan de Oro\nCity in Northern Mindanao, Philippines. A project named Web-based Case Docket\nInformation System (WCDIS) has been designed and developed for the court\nbranch. This system uses a framework known as System Development Life Cycle\n(SDLC) which is a guide for the design and development. This paper also\ndiscusses the key system functionalities and the implementation methodology,\nincluding both the benefits and shortcomings of this approach, with the goal of\napplying lessons learned for future installations. Foremost among the successes\nof this project is its ability to increase efficiency and reliability in\ncompleting court transactions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00822,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Understanding the Complexity of Detecting Political Ads\n\n  Online political advertising has grown significantly over the last few years.\nTo monitor online sponsored political discourse, companies such as Facebook,\nGoogle, and Twitter have created public Ad Libraries collecting the political\nads that run on their platforms. Currently, both policymakers and platforms are\ndebating further restrictions on political advertising to deter misuses.\n  This paper investigates whether we can reliably distinguish political ads\nfrom non-political ads. We take an empirical approach to analyze what kind of\nads are deemed political by ordinary people and what kind of ads lead to\ndisagreement. Our results show a significant disagreement between what ad\nplatforms, ordinary people, and advertisers consider political and suggest that\nthis disagreement mainly comes from diverging opinions on which ads address\nsocial issues. Overall our results imply that it is important to consider\nsocial issue ads as political, but they also complicate political advertising\nregulations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01149,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000004669,
      "text":"Reflections on the Clinical Acceptance of Artificial Intelligence\n\n  In this chapter, we reflect on the use of Artificial Intelligence (AI) and\nits acceptance in clinical environments. We develop a general view of\nhindrances for clinical acceptance in the form of a pipeline model combining AI\nand clinical practise. We then link each challenge to the relevant stage in the\npipeline and discuss the necessary requirements in order to overcome each\nchallenge. We complement this discussion with an overview of opportunities for\nAI, which we currently see at the periphery of clinical workflows.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04065,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000143051,
      "text":"Quantitative Assessment of Solution Innovation in Engineering Education\n\n  The article discusses the quantitative assessment approach to the innovation\nof engineering system components. The validity of the approach is based on the\nexpert appraisal of the university's electronic information educational\nenvironment components and the measurement of engineering solution innovation\nin engineering education. The implementation of batch processing of object\ninnovation assessments is justified and described.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.16613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Tracking Knowledge Propagation Across Wikipedia Languages\n\n  In this paper, we present a dataset of inter-language knowledge propagation\nin Wikipedia. Covering the entire 309 language editions and 33M articles, the\ndataset aims to track the full propagation history of Wikipedia concepts, and\nallow follow up research on building predictive models of them. For this\npurpose, we align all the Wikipedia articles in a language-agnostic manner\naccording to the concept they cover, which results in 13M propagation\ninstances. To the best of our knowledge, this dataset is the first to explore\nthe full inter-language propagation at a large scale. Together with the\ndataset, a holistic overview of the propagation and key insights about the\nunderlying structural factors are provided to aid future research. For example,\nwe find that although long cascades are unusual, the propagation tends to\ncontinue further once it reaches more than four language editions. We also find\nthat the size of language editions is associated with the speed of propagation.\nWe believe the dataset not only contributes to the prior literature on\nWikipedia growth but also enables new use cases such as edit recommendation for\naddressing knowledge gaps, detection of disinformation, and cultural\nrelationship analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.06621,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Towards Determining the Effect of Age and Educational Level on\n  Cyber-Hygiene\n\n  As internet related challenges increase such as cyber-attacks, the need for\nsafe practises among users to maintain computer system's health and online\nsecurity have become imperative, and this is known as cyber-hygiene. Poor\ncyber-hygiene among internet users is a very critical issue undermining the\ngeneral acceptance and adoption of internet technology. It has become a global\nissue and concern in this digital era when virtually all business transactions,\nlearning, communication and many other activities are performed online. Virus\nattack, poor authentication technique, improper file backups and the use of\ndifferent social engineering approaches by cyber-attackers to deceive internet\nusers into divulging their confidential information with the intention to\nattack them have serious negative implications on the industries and\norganisations, including educational institutions. Moreover, risks associated\nwith these ugly phenomena are likely to be more in developing countries such as\nNigeria. Thus, authors of this paper undertook an online pilot study among\nstudents and employees of University of Nigeria, Nsukka and a total of 145\nresponses were received and used for the study. The survey seeks to find out\nthe effect of age and level of education on the cyber hygiene knowledge and\nbehaviour of the respondents, and in addition, the type of devices used and\nactivities they engage in while on the internet. Our findings show wide\nadoption of internet in institution of higher learning, whereas, significant\nnumber of the internet users do not have good cyber hygiene knowledge and\nbehaviour. Hence, our findings can instigate an organised training for students\nand employees of higher institutions in Nigeria.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.03896,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"The transformation process from in-campus classes into online classes\n  due to the COVID-19 situation -- the case of higher education institutions in\n  Kosovo\n\n  The COVID-19 pandemic has caused changes in terms of traditional teaching\nglobally. In Kosova context, the Universities have found the transition from\nteaching in class to online classes quite challenging. This study investigates\nthe transformation process from in-campus classes to online classes from the\ntechnical perspective within five Higher Education Institutions (HEI) in\nKosovo. The data was collected using the qualitative methods and its analysis\nfollowed the 3C Lichtman approach. The results show that each of the\nUniversities followed a different approach, by using either their limited\npremises infrastructure or using additional cloud infrastructure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Analysing the Correlation of Geriatric Assessment Scores and Activity in\n  Smart Homes\n\n  A continuous monitoring of the physical strength and mobility of elderly\npeople is important for maintaining their health and treating diseases at an\nearly stage. However, frequent screenings by physicians are exceeding the\nlogistic capacities. An alternate approach is the automatic and unobtrusive\ncollection of functional measures by ambient sensors. In the current\npublication, we show the correlation among data of ambient motion sensors and\nthe well-established mobility assessments Short-Physical-Performance-Battery,\nTinetti and Timed Up & Go. We use the average number of motion sensor events as\nactivity measure for correlation with the assessment scores. The evaluation on\na real-world dataset shows a moderate to strong correlation with the scores of\nstandardised geriatrics physical assessments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.0906,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"Do e-scooters fill mobility gaps and promote equity before and during\n  COVID-19? A spatiotemporal analysis using open big data\n\n  The growing popularity of e-scooters and their rapid expansion across urban\nstreets has attracted widespread attention. A major policy question is whether\ne-scooters substitute existing mobility options or fill the service gaps left\nby them. This study addresses this question by analyzing the spatiotemporal\npatterns of e-scooter service availability and use in Washington DC, focusing\non their spatial relationships with public transit and bikesharing. Results\nfrom an analysis of three open big datasets suggest that e-scooters have both\ncompeting and complementary effects on transit and bikesharing services. The\nsupply of e-scooters significantly overlaps with the service areas of transit\nand bikesharing, and we classify a majority of e-scooter trips as substitutes\nto transit and bikesharing uses. A travel-time-based analysis further reveals\nthat when choosing e-scooters over transit, travelers pay a price premium and\nsave some travel time. The price premium is greater during the COVID-19\npandemic but the associated travel-time savings are smaller. This implies that\npublic health considerations rather than time-cost tradeoffs are the main\ndriver for many to choose e-scooters over transit during COVID. In addition, we\nfind that e-scooters complement bikesharing and transit by providing services\nto underserved neighborhoods. A sizeable proportion (about 10 percent) of\ne-scooter trips are taken to connect with the rail services. Future research\nmay combine the big-data-based analysis presented here with traditional methods\nto further shed light on the interactions between e-scooter services,\nbikesharing, and public transit.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12486,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Fostering learners' self-regulation and collaboration skills and\n  strategies for mobile language learning beyond the classroom\n\n  Many language learners need to be supported in acquiring a second or foreign\nlanguage quickly and effectively across learning environments beyond the\nclassroom. The chapter argues that support should focus on the development of\ntwo vital learning skills, namely being able to self-regulate and to\ncollaborate effectively in the learning process. We base our argumentation on\nthe theoretical lenses of self-regulated learning (SRL) and collaborative\nlearning in the context of mobile situated learning that can take place in a\nvariety of settings. The chapter examines a sample of selected empirical\nstudies within the field of mobile-assisted language learning with a twofold\naim. Firstly, the studies are analyzed in order to understand the role of\nlearner self-regulation and collaboration while acquiring a new language beyond\nthe classroom. Secondly, we aim to provide a deeper understanding of any\nmechanisms provided to develop or support language learners' self-regulated and\ncollaborative learning skills. Finally, we propose that fostering SRL and\ncollaborative learning skills and strategies will benefit from recent advances\nin the fields of learning analytics and artificial intelligence, coupled with\nthe use of mobile technologies and self-monitoring mechanisms. The ultimate aim\nis to enable the provision of individual adaptive learning paths to facilitate\nlanguage learning beyond the classroom.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04061,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Transition from physical to online shopping alternatives due to the\n  COVID-19 pandemic\n\n  By using 530 responses from an online questionnaire, this study aims to\ninvestigate the transition from physical to online shopping alternatives during\nthe first wave of the COVID-19 pandemic period at the individual level. The\nfocus areas of the study are Sweden and Italy, two European countries that\nimplemented contrasting prevention measures. This study analyses the impacts of\nthe pandemic to the shopping behaviour and identifies, among the respondents,\nwho have changed the behaviour the most, how respondents have adopted different\nshopping strategies, what the main differences between Italian and Swedish\nresponses are and the influence of population density on the behavioural\nchange. Multivariate statistical analyses, including linear and binary logistic\nregressions and multinomial logit models were used to analyse the dataset. The\nresults confirm the differences between Italy and Sweden in terms of social\ndistancing measures, social structures and technology readiness. Moreover, the\nsocio-demographic and household structures of the respondents were found\ninstrumental in influencing the amount and the direction of change in shopping\nbehaviour during the first wave of the pandemic period. The output of this\nstudy highlights the impact that contrasting policies have on citizens, and\nalso the importance of having policies that are adaptable to different\nsituations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.11138,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Let's Ask Students About Their Programs, Automatically\n\n  Students sometimes produce code that works but that its author does not\ncomprehend. For example, a student may apply a poorly-understood code template,\nstumble upon a working solution through trial and error, or plagiarize.\nSimilarly, passing an automated functional assessment does not guarantee that\nthe student understands their code. One way to tackle these issues is to probe\nstudents' comprehension by asking them questions about their own programs. We\npropose an approach to automatically generate questions about student-written\nprogram code. We moreover propose a use case for such questions in the context\nof automatic assessment systems: after a student's program passes unit tests,\nthe system poses questions to the student about the code. We suggest that these\nquestions can enhance assessment systems, deepen student learning by acting as\nself-explanation prompts, and provide a window into students' program\ncomprehension. This discussion paper sets an agenda for future technical\ndevelopment and empirical research on the topic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04389,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"A Few Observations About State-Centric Online Propaganda\n\n  This paper presents a few observations about pro-Kremlin propaganda between\n2015 and early 2021 with a dataset from the East Stratcom Task Force (ESTF),\nwhich is affiliated with the European Union (EU) but working independently from\nit. Instead of focusing on misinformation and disinformation, the observations\nare motivated by classical propaganda research and the ongoing transformation\nof media systems. According to the tentative results, (i) the propaganda can be\nassumed to target both domestic and foreign audiences. Of the countries and\nregions discussed, (ii) Russia, Ukraine, the United States, and within Europe,\nGermany, Poland, and the EU have been the most frequently discussed. Also other\nconflict regions such as Syria have often appeared in the propaganda. In terms\nof longitudinal trends, however, (iii) most of these discussions have decreased\nin volume after the digital tsunami in 2016, although the conflict in Ukraine\nseems to have again increased the intensity of pro-Kremlin propaganda. Finally,\n(iv) the themes discussed align with state-centric war propaganda and conflict\nzones, although also post-truth themes frequently appear; from conspiracy\ntheories via COVID-19 to fascism -- anything goes, as is typical to propaganda.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11757,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000057287,
      "text":"Becoming Good at AI for Good\n\n  AI for good (AI4G) projects involve developing and applying artificial\nintelligence (AI) based solutions to further goals in areas such as\nsustainability, health, humanitarian aid, and social justice. Developing and\ndeploying such solutions must be done in collaboration with partners who are\nexperts in the domain in question and who already have experience in making\nprogress towards such goals. Based on our experiences, we detail the different\naspects of this type of collaboration broken down into four high-level\ncategories: communication, data, modeling, and impact, and distill eleven\ntakeaways to guide such projects in the future. We briefly describe two case\nstudies to illustrate how some of these takeaways were applied in practice\nduring our past collaborations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06388,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Online Hackathons as an Engaging Tool to Promote Group Work in Emergency\n  Remote Learning\n\n  In 2020, due to the COVID-19 pandemic, educational activities had to be done\nremotely as a way to avoid the spread of the disease. What happened was not\nexactly a shift to an online learning model but a transition to a new approach\ncalled Emergency Remote Teaching. It is a temporary strategy to keep activities\ngoing on until it is safe again to return to the physical facilities of\nuniversities. This new setting became a challenge to both teachers and\nstudents. The lack of interaction and classroom socialization became obstacles\nfor students to continue engaged. Before the pandemic, hackathons --\nshort-lived events (1 to 3 days) where participants intensively collaboration\nto develop software prototypes -- were starting to be explored as an\nalternative venue to engage students in acquiring and practicing technical\nskills. In this paper, we present an experience report on the usage of an\nonline hackathon as a resource to engage students in the development of their\nsemester project in a distributed applications course during this emergency\nremote teaching period. We describe details of the intervention and present an\nanalysis of the students' perspective of the approach. One of the important\nfindings was the efficient usage of the Discord communication tool -- already\nused by all students while playing games -- which helped them socialize and\nkeep them continuously engaged in synchronous group work, \"virtually\ncollocated\".\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.13139,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Mobsimilarity: Vector Graph Optimization for Mobility Tableau Comparison\n\n  Human mobility similarity comparison plays a critical role in mobility\nestimation\/prediction model evaluation, mobility clustering and mobility\nmatching, which exerts an enormous impact on improving urban mobility,\naccessibility, and reliability. By expanding origin-destination matrix, we\npropose a concept named mobility tableau, which is an aggregated tableau\nrepresentation to the population flow distributed between different location\npairs of a study site and can be represented by a vector graph. Compared with\ntraditional OD matrix-based mobility comparison, mobility tableau comparison\nprovides high-dimensional similarity information, including volume similarity,\nspatial similarity, mass inclusiveness and structure similarity. A novel\nmobility tableaus similarity measurement method is proposed by optimizing the\nleast spatial cost of transforming the vector graph for one mobility tableau\ninto the other and is optimized to be efficient. The robustness of the measure\nis supported through several sensitive analysis on GPS based mobility tableau.\nThe better performance of the approach compared with traditional mobility\ncomparison methods in two case studies demonstrate the practicality and\nsuperiority, while one study is estimated mobility tableaus validation and the\nother is different cities' mobility tableaus comparison.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04079,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000175171,
      "text":"The Internet of People: A Survey and Tutorial\n\n  Along with the rapid development of Internet and cyber techniques, the\nInternet of People (IoP) has been emerging as a novel paradigm which\nestablishes ubiquitous connections between social space and cyberspace. Unlike\nthe Internet of Things (IoT), human nodes in IoP are not only terminal users,\nbut also significant participants bonded with tighter relationships. Note that\nIoP has deeply influenced our lives. For example, the prosperity of social\nmedia enables us to build and maintain new relationships without considering\nphysical boundaries. In this paper, we give a comprehensive overview of IoP by\ncomparing it with IoT, introduce its enabling techniques from aspects of\nsensing, communication and application. In addition, we summarize and compare\npopular IoP-enabled platforms according to different functions, and finally\nenvision open issues needed to be further discussed in IoP. It provides\nconstructive suggestions, lays solid foundations, and makes a basic blueprint\nfor the future development of IoP paradigm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12557,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Flexible Educational Software Architecture\n\n  EAs.LiT is an e-assessment management and analysis software for which\ncontextual requirements and usage scenarios changed over time. Based on these\nfactors and further development activities, the decision was made to adopt a\nmicroservice architecture for EAs.LiT version 2 in order to increase its\nflexibility to adapt to new and changed circumstances. This architectural style\nand a few adopted technologies, like RDF as a data format, enabled an eased\nimplementation of various use cases. Thus we consider the microservice\narchitecture productive and recommend it for usage in other educational\nprojects. The specific architecture of EAs.LiT version 2 is presented within\nthis article, targeting to enable other educational projects to adopt it by\nusing our work as a foundation or template.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11968,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Effective Metagraph-based Life Pattern Clustering with Big Human\n  Mobility Data\n\n  Life pattern clustering is essential for abstracting the groups'\ncharacteristics of daily mobility patterns and activity regularity. Based on\nmillions of GPS records, this paper proposed a framework on the life pattern\nclustering which can efficiently identify the groups have similar life pattern.\nThe proposed method can retain original features of individual life pattern\ndata without aggregation. Metagraph-based data structure is proposed for\npresenting the diverse life pattern. Spatial-temporal similarity includes\nsignificant places semantics, time sequential properties and frequency are\nintegrated into this data structure, which captures the uncertainty of an\nindividual and the diversities between individuals.\nNon-negative-factorization-based method was utilized for reducing the\ndimension. The results show that our proposed method can effectively identify\nthe groups have similar life pattern and takes advantages in computation\nefficiency and robustness comparing with the traditional method. We revealed\nthe representative life pattern groups and analyzed the group characteristics\nof human life patterns during different periods and different regions. We\nbelieve our work will help in future infrastructure planning, services\nimprovement and policies making related to urban and transportation, thus\npromoting a humanized and sustainable city.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06646,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Influenza Surveillance using Search Engine, SNS, On-line Shopping, Q&A\n  Service and Past Flu Patients\n\n  Influenza, an infectious disease, causes many deaths worldwide. Predicting\ninfluenza victims during epidemics is an important task for clinical, hospital,\nand community outbreak preparation. On-line user-generated contents (UGC),\nprimarily in the form of social media posts or search query logs, are generally\nused for prediction for reaction to sudden and unusual outbreaks. However, most\nstudies rely only on the UGC as their resource and do not use various UGCs. Our\nstudy aims to solve these questions about Influenza prediction: Which model is\nthe best? What combination of multiple UGCs works well? What is the nature of\neach UGC? We adapt some models, LASSO Regression, Huber Regression, Support\nVector Machine regression with Linear kernel (SVR) and Random Forest, to test\nthe influenza volume prediction in Japan during 2015 - 2018. For that, we use\non-line five data resources: (1) past flu patients, (2) SNS (Twitter), (3)\nsearch engines (Yahoo! Japan), (4) shopping services (Yahoo! Shopping), and (5)\nQ&A services (Yahoo! Chiebukuro) as resources of each model. We then validate\nrespective resources contributions using the best model, Huber Regression, with\nall resources except one resource. Finally, we use Bayesian change point method\nfor ascertaining whether the trend of time series on any resources is reflected\nin the trend of flu patient count or not. Our experiments show Huber Regression\nmodel based on various data resources produces the most accurate results. Then,\nfrom the change point analysis, we get the result that search query logs and\nsocial media posts for three years represents these resources as a good\npredictor. Conclusions: We show that Huber Regression based on various data\nresources is strong for outliers and is suitable for the flu prediction.\nAdditionally, we indicate the characteristics of each resource for the flu\nprediction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.13175,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"C-Ports: a proposal for a comprehensive standardization and\n  implementation plan of digital services offered by the \"Port of the Future\"\n\n  In this paper we address the topic of a possible path to standardize the ICT\nservices expected to be delivered by the so-called \"Port of the Future\". How\nthe most relevant technologies and Information Systems are used by the Port\nCommunities for their businesses is discussed together with a detailed analysis\nof the on-going actions carried on by Standard Setting Organizations.\nConsidering the examples given by the C-ITS Platform and the C-Roads programme\nat EU level, a proposal of contents to be considered in a comprehensive\nstandardization action is given. The innovation services are therefore grouped\ninto four bundles: (i) Vessel \\& Marine Navigation, (ii) e-Freight \\&\n(Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental\nsustainability. The standardized version of these applications will be finally\nlabeled as C-Port services. Alongside the standardization plan, a proposal for\nranking the ports on the basis of a specially-defined C- Port vector is\ndiscussed with the purpose of addressing the well-known lack of consensus\naround the mathematical definition of the Smart Port Index. Considering the\ngood practice and the background offered by the Port of Livorno in terms of\ninnovation actions, the prospected final user applications are then labeled as\nDay 1, Day 1.5, and Day 2 services in consideration of the technical and\ncommercial gaps to be filled. As a case study about the evolution in the C-Port\nvector experienced by the Port of Livorno in the last years will also be\ndiscussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.01028,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000094705,
      "text":"Limiting Tags Fosters Efficiency\n\n  Tagging facilitates information retrieval in social media and other online\ncommunities by allowing users to organize and describe online content.\nResearchers found that the efficiency of tagging systems steadily decreases\nover time, because tags become less precise in identifying specific documents,\ni.e., they lose their descriptiveness. However, previous works did not answer\nhow or even whether community managers can improve the efficiency of tags. In\nthis work, we use information-theoretic measures to track the descriptive and\nretrieval efficiency of tags on Stack Overflow, a question-answering system\nthat strictly limits the number of tags users can specify per question. We\nobserve that tagging efficiency stabilizes over time, while tag content and\ndescriptiveness both increase. To explain this observation, we hypothesize that\nlimiting the number of tags fosters novelty and diversity in tag usage, two\nproperties which are both beneficial for tagging efficiency. To provide\nqualitative evidence supporting our hypothesis, we present a statistical model\nof tagging that demonstrates how novelty and diversity lead to greater tag\nefficiency in the long run. Our work offers insights into policies to improve\ninformation organization and retrieval in online communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04486,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Investigating sentence severity with judicial open data -- A case study\n  on sentencing high-tech crime in the Dutch criminal justice system\n\n  Open data promotes transparency and accountability as everyone can analyse\nit. Law enforcement and the judiciary are increasingly making data available,\nto increase trust and confidence in the criminal justice system. Due to privacy\nlegislation, judicial open data -- like court judgments -- in Europe is usually\nanonymised. Because this removes part of the information on for instance\noffenders, the question arises to what extent criminological research into\nsentencing can make use of anonymised open data. We answer this question based\non a case study in which we use the open data of the Dutch criminal justice\nsystem that rechtspraak.nl\/Uitspraken makes available. Over the period\n2015-2020, we analysed sentencing in 25,366 court judgments and, in particular,\ninvestigated the relationship between sentence severity and the offender's use\nof advanced ICT -- as this is information that is readily available in open\ndata.\n  The most important results are, firstly, that offenders who use advanced ICT\nare sentenced to longer custodial sentences compared to other offenders.\nSecond, our results show that the quality of sentencing research with open data\nis comparable to the quality of sentencing research with judicial databases,\nwhich are not anonymised.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.05287,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Consideration of resilience for digital farming systems\n\n  Latest and current innovations of agricultural tech industry are increasingly\ndriven by digital technologies. These digital farming solutions provide\nattractive advantages for farmers. The trend is going to devices and sensors,\nwhich send the acquired data directly to the cloud. Also the number of\nscientific publications on cloud based solutions follows this development.\nConsidering on the other hand the necessity of continuous agricultural\nproduction in any kind of crises, new cloud-based digital systems and\napplications need to be reliable, independent of internet supply. In this\nconceptual study the necessary resilience is defined, which is marginally taken\ninto account by agtech industry innovations. Problems of development using\nweb-based farming systems are identified and discussed. For digital farming\nsystems the farmers individual needs of resilience are classified into five\nlevels. Consequently, suggestions for soft- and hardware equipment are made.\nThis includes the installation of a farm server, a local farm network, offline\napplications and consideration of edge computing, which can ensure a high level\nof resilience of new digital farming components.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06263,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"A Report on the Cost of Data Privacy\n\n  As our lives migrate to the digital realm, our online identity has evolved to\nbecome an increasingly robust collection of data about every aspect of our\nonline and offline lives. This data is extremely appealing to companies who\nwish to use it for a variety of analytics. In this report, we create awareness\nfor the consumers around the world who have only a vague understanding of how\nmuch of their data is being tracked, where, when, and by which companies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.1411,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Requirements Contracts: Definition, Design, and Analysis\n\n  What are the necessary and sufficient conditions for a proposition to be\ncalled a requirement? In Requirements Engineering research, a proposition is a\nrequirement if and only if specific grammatical and\/or communication conditions\nhold. I offer an alternative, that a proposition is a requirement if and only\nif specific contractual, economic, and engineering relationships hold. I\nintroduce and define the concept of \"Requirements Contract\" which defines these\nconditions. I argue that seeing requirements as propositions governed by\nspecific types of contracts leads to new and interesting questions for the\nfield, and relates requirements engineering to such topics as economic\nincentives, interest alignment, principal agent problem, and decision-making\nwith incomplete information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04502,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000050333,
      "text":"Auditing for Discrimination in Algorithms Delivering Job Ads\n\n  Ad platforms such as Facebook, Google and LinkedIn promise value for\nadvertisers through their targeted advertising. However, multiple studies have\nshown that ad delivery on such platforms can be skewed by gender or race due to\nhidden algorithmic optimization by the platforms, even when not requested by\nthe advertisers. Building on prior work measuring skew in ad delivery, we\ndevelop a new methodology for black-box auditing of algorithms for\ndiscrimination in the delivery of job advertisements. Our first contribution is\nto identify the distinction between skew in ad delivery due to protected\ncategories such as gender or race, from skew due to differences in\nqualification among people in the targeted audience. This distinction is\nimportant in U.S. law, where ads may be targeted based on qualifications, but\nnot on protected categories. Second, we develop an auditing methodology that\ndistinguishes between skew explainable by differences in qualifications from\nother factors, such as the ad platform's optimization for engagement or\ntraining its algorithms on biased data. Our method controls for job\nqualification by comparing ad delivery of two concurrent ads for similar jobs,\nbut for a pair of companies with different de facto gender distributions of\nemployees. We describe the careful statistical tests that establish evidence of\nnon-qualification skew in the results. Third, we apply our proposed methodology\nto two prominent targeted advertising platforms for job ads: Facebook and\nLinkedIn. We confirm skew by gender in ad delivery on Facebook, and show that\nit cannot be justified by differences in qualifications. We fail to find skew\nin ad delivery on LinkedIn. Finally, we suggest improvements to ad platform\npractices that could make external auditing of their algorithms in the public\ninterest more feasible and accurate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04633,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Automated Meta-Analysis: A Causal Learning Perspective\n\n  Meta-analysis is a systematic approach for understanding a phenomenon by\nanalyzing the results of many previously published experimental studies. It is\ncentral to deriving conclusions about the summary effect of treatments and\ninterventions in medicine, poverty alleviation, and other applications with\nsocial impact. Unfortunately, meta-analysis involves great human effort,\nrendering a process that is extremely inefficient and vulnerable to human bias.\nTo overcome these issues, we work toward automating meta-analysis with a focus\non controlling for risks of bias. In particular, we first extract information\nfrom scientific publications written in natural language. From a novel causal\nlearning perspective, we then propose to frame automated meta-analysis -- based\non the input of the first step -- as a multiple-causal-inference problem where\nthe summary effect is obtained through intervention. Built upon existing\nefforts for automating the initial steps of meta-analysis, the proposed\napproach achieves the goal of automated meta-analysis and largely reduces the\nhuman effort involved. Evaluations on synthetic and semi-synthetic datasets\nshow that this approach can yield promising results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08522,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Quantifying the Need for Attorney Pro Bono Services in Connection with\n  the Social Determinants of Health\n\n  The paper estimates the need for additional attorney hours annually to\naddress the legal needs of indigent clients throughout the United States in\nmatters that comprise the so-called social determinants of health (SDoH). The\nresult will inform stakeholders such as policy makers and private donors so\nthey can allocate resources appropriately and design programs to close the\ndo-called justice gap. As a pilot study, the scope of the project covers only a\nfew major justice problems related to the social determinants of health\n(standard housing, evictions and foreclosures, guardianships for the\nincapacitated, and victims of domestic violence) because they significantly\nimpact health outcomes and there are data available. Based on our calculations,\nwe estimate that the total number of attorney hours to address only these five\nlegal issues is over 34 million per year. To put that in perspective, the\nAmerican Bar Association estimated that in 2018, the year from which much of\nour data comes, the total number of practicing attorneys in the United States\nwas 1,338,678 (Weiss). Thus, to provide the needed hours, every single\npracticing attorney in the United States would need to contribute about 26\nhours a year. While many lawyers do in fact contribute pro bono hours, they\naddress the full range of legal needs that go well beyond just the five we\nstudied.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.01765,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Comparative analysis of the government plans of the Peruvian\n  presidential candidates, SDO(UN) and State Policies of the National Agreement\n  based on NLP\n\n  The analysis of government proposal during elections from political parties\nis vital to choose the next authorities in any city or country. In this paper,\nwe use a text mining approach to analyze the documents and provide an easy\nvisualization to support an easy analysis. Besides, a comparison with a\nnational plan based on sustainable development objectives of UN(United Nations)\nfrom 2030 Agenda is perfomed using Natural Language techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06264,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Do We Expect More from Radiology AI than from Radiologists?\n\n  What we expect from radiology AI algorithms will shape the selection and\nimplementation of AI in the radiologic practice. In this paper I consider\nprevailing expectations of AI and compare them to expectations that we have of\nhuman readers. I observe that the expectations from AI and radiologists are\nfundamentally different. The expectations of AI are based on a strong and\njustified mistrust about the way that AI makes decisions. Because AI decisions\nare not well understood, it is difficult to know how the algorithms will behave\nin new, unexpected situations. However, this mistrust is not mirrored in our\nexpectations of human readers. Despite well-proven idiosyncrasies and biases in\nhuman decision making, we take comfort from the assumption that others make\ndecisions in a way as we do, and we trust our own decision making. Despite poor\nability to explain decision making processes in humans, we accept explanations\nof decisions given by other humans. Because the goal of radiology is the most\naccurate radiologic interpretation, our expectations of radiologists and AI\nshould be similar, and both should reflect a healthy mistrust of complicated\nand partially opaque decision processes undergoing in computer algorithms and\nhuman brains. This is generally not the case now.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06658,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"Small World Model for scaling up prediction result based on SEIR model\n\n  Data-driven epidemic simulation helps better policymaking. Compared with\nmacro-scale simulations driven by statistical data, individual-level GPS data\ncan afford finer and spatialized results. However, the big GPS data, usually\ncollected from mobile phone users, cannot cover all populations. Therefore,\nthis study proposes a Small World Model, to map the results from the \"small\nworld\" (simulation with partially sampled data) to the real world. Based on the\nbasic principles of disease transmission, this study derives two parameters: a\ntime scaling factor to map the simulated period to the real period, and an\namount scaling factor to map the simulated infected number to the real infected\nnumber. It is believed that this model could convert the simulation of the\n\"small world\" into the state of the real world, and analyze the effectiveness\nof different mobility restriction policies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.02117,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"Ethics and Governance of Artificial Intelligence: Evidence from a Survey\n  of Machine Learning Researchers\n\n  Machine learning (ML) and artificial intelligence (AI) researchers play an\nimportant role in the ethics and governance of AI, including taking action\nagainst what they perceive to be unethical uses of AI (Belfield, 2020; Van\nNoorden, 2020). Nevertheless, this influential group's attitudes are not well\nunderstood, which undermines our ability to discern consensuses or\ndisagreements between AI\/ML researchers. To examine these researchers' views,\nwe conducted a survey of those who published in the top AI\/ML conferences (N =\n524). We compare these results with those from a 2016 survey of AI\/ML\nresearchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey\nof the US public (Zhang & Dafoe, 2020). We find that AI\/ML researchers place\nhigh levels of trust in international organizations and scientific\norganizations to shape the development and use of AI in the public interest;\nmoderate trust in most Western tech companies; and low trust in national\nmilitaries, Chinese tech companies, and Facebook. While the respondents were\noverwhelmingly opposed to AI\/ML researchers working on lethal autonomous\nweapons, they are less opposed to researchers working on other military\napplications of AI, particularly logistics algorithms. A strong majority of\nrespondents think that AI safety research should be prioritized and that ML\ninstitutions should conduct pre-publication review to assess potential harms.\nBeing closer to the technology itself, AI\/ML re-searchers are well placed to\nhighlight new risks and develop technical solutions, so this novel attempt to\nmeasure their attitudes has broad relevance. The findings should help to\nimprove how researchers, private sector executives, and policymakers think\nabout regulations, governance frameworks, guiding principles, and national and\ninternational governance strategies for AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.1104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000083447,
      "text":"An exploratory study of skill requirements for social media positions: A\n  content analysis of job advertisements\n\n  There has been considerable debate about the comparative advantages of\nmarketing education emphasizing theoretical knowledge and applied skills. The\ncurrent study investigated the skills necessary for entry-level marketing\npositions, specifically that of Social Media Manager (SMMgr) and Social Media\nMarketer (SMMkt). Data was collected from Indeed.com using a web crawler to\nextract job postings for SMMgr and SMMkt. A total of 766 and 654 entry-level\njobs for SMMgr and SMMkt, respectively, across the entire United States, was\ncollected. Independent raters separately analyzed the data for keywords and\ncategories. Findings suggest that the most desired skills are occupational\ndigital marketing skills. Other relevant skill categories included\ncommunication, employee attributes, problem-solving, and information technology\nskills. This study extends the current literature by highlighting the desired\nskills prevalent across the social media industry. The findings also have\nrelevance in designing the marketing education curriculum, specifically in\nisolating core skills that could be integrated into the marketing courses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0931,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Informatiekunde -- Visie 2003\n\n  This document discusses (in Dutch) the vision underlying the business\ninformatics (Informatiekunde) curriculum and research programme at the Nijmegen\nInstitute for Informatics and Information Science (NIII). The ultimate aim of\nthis document is to provide a 'repository' with regard to these visions, and a\nbasis for the specific structure of the program's curriculum and research\nplans.\n  Since business informatics is a relatively new area for teaching and research\nat NIII, the current (2003) version of this document primarily focuses on the\neducational perspective. It is to be expected that in the coming years, updates\nto this document will also pay more attention to business informatics research.\nThe fact that this document can be updated annually does not mean, however,\nthat we expect an annual change of course. The ambition with regard to the\nstability of what is described in this document is 5 to 6 years. In the current\nversion, this applies specifically to the vision of the information science\nstudy program. The research part of this document will have to be fleshed out\neven more specifically in the coming years.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07523,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Analyzing the \"Sleeping Giants\" Activism Model in Brazil\n\n  In 2020, amidst the COVID pandemic and a polarized political climate, the\nSleeping Giants online activist movement gained traction in Brazil. Its\nrationale was simple: to curb the spread of misinformation by harming the\nadvertising revenue of sources that produce this type of content. Like its\ninternational counterparts, Sleeping Giants Brasil (SGB) campaigned against\nmedia outlets using Twitter to ask companies to remove ads from the targeted\noutlets. This work presents a thorough quantitative characterization of this\nactivism model, analyzing the three campaigns carried out by SGB between May\nand September 2020. To do so, we use digital traces from both Twitter and\nGoogle Trends, toxicity and sentiment classifiers trained for the Portuguese\nlanguage, and an annotated corpus of SGB's tweets. Our key findings were\nthreefold. First, we found that SGB's requests to companies were largely\nsuccessful (with 83.85\\% of all 192 targeted companies responding positively)\nand that user pressure was correlated to the speed of companies' responses.\nSecond, there were no significant changes in the online attention and the user\nengagement going towards the targeted media outlets in the six months that\nfollowed SGB's campaign (as measured by Google Trends and Twitter engagement).\nThird, we observed that user interactions with companies changed only\ntransiently, even if the companies did not respond to SGB's request. Overall,\nour results paint a nuanced portrait of internet activism. On the one hand,\nthey suggest that SGB was successful in getting companies to boycott specific\nmedia outlets, which may have harmed their advertisement revenue stream. On the\nother hand, they also suggest that the activist movement did not impact the\nonline attention these media outlets received nor the online image of companies\nthat did not respond positively to their requests.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05138,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"The Smoothed Likelihood of Doctrinal Paradox\n\n  When aggregating logically interconnected judgments from $n$ agents, the\nresult might be inconsistent with the logical connection. This inconsistency is\nknown as the doctrinal paradox, which plays a central role in the field of\njudgment aggregation. Despite a large body of literature on the worst-case\nanalysis of the doctrinal paradox, little is known about its likelihood under\nnatural statistical models, except for a few i.i.d. distributions [List, 2005].\n  In this paper, we characterize the likelihood of the doctrinal paradox under\na much more general and realistic model called the smoothed social choice\nframework [Xia, 2020b], where agents' ground truth judgments are arbitrarily\ncorrelated while the noises are independent. Our main theorem states that under\nmild conditions, the smoothed likelihood of the doctrinal paradox is either\n$0$, $\\exp(-\\Theta(n))$, $\\Theta(n^{-1\/2})$ or $\\Theta(1)$. This not only\nanswers open questions by List [2005] for i.i.d. distributions but also draws\nclear lines between situations with frequent and with vanishing paradoxes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0476,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000153316,
      "text":"Unpacking the Expressed Consequences of AI Research in Broader Impact\n  Statements\n\n  The computer science research community and the broader public have become\nincreasingly aware of negative consequences of algorithmic systems. In\nresponse, the top-tier Neural Information Processing Systems (NeurIPS)\nconference for machine learning and artificial intelligence research required\nthat authors include a statement of broader impact to reflect on potential\npositive and negative consequences of their work. We present the results of a\nqualitative thematic analysis of a sample of statements written for the 2020\nconference. The themes we identify broadly fall into categories related to how\nconsequences are expressed (e.g., valence, specificity, uncertainty), areas of\nimpacts expressed (e.g., bias, the environment, labor, privacy), and\nresearchers' recommendations for mitigating negative consequences in the\nfuture. In light of our results, we offer perspectives on how the broader\nimpact statement can be implemented in future iterations to better align with\npotential goals.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14637,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Organizational Artifacts of Code Development\n\n  Software is the outcome of active and effective communication between members\nof an organization. This has been noted with Conway's law, which states that\n``organizations design systems that mirror their own communication structure.''\nHowever, software developers are often members of multiple organizational\ngroups (e.g., corporate, regional,) and it is unclear how association with\ngroups beyond one's company influence the development process. In this paper,\nwe study social effects of country by measuring differences in software\nrepositories associated with different countries. Using a novel dataset we\nobtain from GitHub, we identify key properties that differentiate software\nrepositories based upon the country of the developers. We propose a novel\napproach of modeling repositories based on their sequence of development\nactivities as a sequence embedding task and coupled with repo profile features\nwe achieve 79.2% accuracy in identifying the country of a repository. Finally,\nwe conduct a case study on repos from well-known corporations and find that\ncountry can describe the differences in development better than the company\naffiliation itself. These results have larger implications for software\ndevelopment and indicate the importance of considering the multiple groups\ndevelopers are associated with when considering the formation and structure of\nteams.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.09252,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Measuring the technological pedagogical content knowledge (TPACK) of\n  in-service teachers of computer science who teach algorithms and programming\n  in upper secondary education\n\n  Based on the Technological Pedagogical and Content Knowledge (TPACK)\nframework (Mishra & Koehler, 2006) and the Schmidt et al. (2009) instrument\nwhich explore TPACK, this study examines a national sample of 1032 secondary\nteachers of computer science and measures their knowledge with respect to\ntechnology, pedagogy, content knowledge and the combination of each of these\nareas. Findings indicate that content knowledge and technology knowledge rating\nare high (average 4.38 and 4.16 respectively) and it seems that secondary\nteachers are less confident with their pedagogical content knowledge and their\ntechnological content knowledge (average 3.51 and 3.68 respectively).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.15133,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"An Assessment of the AI Regulation Proposed by the European Commission\n\n  In April 2021, the European Commission published a proposed regulation on AI.\nIt intends to create a uniform legal framework for AI within the European Union\n(EU). In this chapter, we analyze and assess the proposal. We show that the\nproposed regulation is actually not needed due to existing regulations. We also\nargue that the proposal clearly poses the risk of overregulation. As a\nconsequence, this would make the use or development of AI applications in\nsafety-critical application areas, such as in healthcare, almost impossible in\nthe EU. This would also likely further strengthen Chinese and US corporations\nin their technology leadership. Our assessment is based on the oral evidence we\ngave in May 2021 to the joint session of the European Union affairs committees\nof the German federal parliament and the French National Assembly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07858,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Optimal Group Formulation Using Machine Learning\n\n  Group formation itself a perplexing process. Over the decade of time\neducation and others disciple has improved imminently but optimal group\nformation in educational system is still struggling. Our research focus on to\ncreate optimal group in a class of any institute. In this research we use\nSimulated Annealing (SA) for best group formation based on the previous\nacademic record. We generally create an arbitrary cluster first then optimise\nusing SA. Our model has significant success rate over a large number of\ndatasets. This research will play a pioneer role in group formations in the\nacademic and related researches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.12429,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"The Evaluation Case Study of Online Course During Pandemic Period in\n  Mongolia\n\n  This paper describes a test and case study of self-evaluation of online\ncourses during the pandemic time. Due to the Covid-19, the whole world needs to\nsit on lockdown in different periods. Many things need to be done in all kinds\nof business including the education sector of countries. To sustain the\neducation development teaching methods had to switch from traditional\nface-to-face teaching to online courses. The government made decisions in a\nshort time and educational institutions had no time to prepare the materials\nfor the online teaching. All courses of the Mongolian University of\nPharmaceutical Sciences switched to online lessons. Challenges were raised\nbefore professors and tutors during online teaching. Our university did not\nhave a specific learning management system for online teaching and e-learning.\nTherefore professors used different platforms for their online teaching such as\nZoom, Microsoft teams for instance. Moreover, different social networking\nplatforms played an active role in communication between students and\nprofessors. The situation is very difficult for professors and students. To\nmeasure the quality of online courses and to figure out the positive and weak\npoints of online teaching we need an evaluation of e-learning. The focus of\nthis paper is to share the evaluation process of e-learning based on a\nstructure-oriented evaluation model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0964,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Enabling News Consumers to View and Understand Biased News Coverage: A\n  Study on the Perception and Visualization of Media Bias\n\n  Traditional media outlets are known to report political news in a biased way,\npotentially affecting the political beliefs of the audience and even altering\ntheir voting behaviors. Many researchers focus on automatically detecting and\nidentifying media bias in the news, but only very few studies exist that\nsystematically analyze how theses biases can be best visualized and\ncommunicated. We create three manually annotated datasets and test varying\nvisualization strategies. The results show no strong effects of becoming aware\nof the bias of the treatment groups compared to the control group, although a\nvisualization of hand-annotated bias communicated bias instances more\neffectively than a framing visualization. Showing participants an overview\npage, which opposes different viewpoints on the same topic, does not yield\ndifferences in respondents' bias perception. Using a multilevel model, we find\nthat perceived journalist bias is significantly related to perceived political\nextremeness and impartiality of the article.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.12504,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Blockchain-Based Approach to Foster Student Engagement on Campus\n\n  On-campus activities like positions of responsibility in campus amenities and\nparticipation in research, benefit the students as well as the university,\nwhile also making students financially self-sufficient to a certain extent.\nHowever, this student participation is stymied by lack of awareness and\nmotivation. Significant impetus to innovation and student participation can be\nprovided by incentivization of these activities. In this paper, we propose a\nsystem to create a blockchain-based economy, to incentivize students with\nempirical benefits or monetary awards calculated using objective algorithms.\nThe incentivization algorithms have been designed for three promising use\ncases: research work, positions of responsibility in universities, and\ncrowdfunding. The demonstrated implementation of this system utilises VJTI\nChain, an already established Proof of Authority blockchain in VJTI Mumbai,\nIndia. This creates a circular economy within the university which encourages\nstudents to earn more rewards by reinforcing positive feedback.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.15125,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Strengthening e-Education in India using Machine Learning\n\n  e-Education has developed as one of the most encouraging territories. The\nIndian Government is investing all amounts of energy to improve education among\nthe residents of the nation. School and graduate understudies are focused on,\nhowever the stage is being created for all the residents seeking to learn.\nWithout a doubt, the objective is to build the quantity of literates with\nadvanced education. To accomplish the equivalent, propels in Data and\nCorrespondence innovation are being utilized in the education division, which\nhas cleared route for e-Training in India as well. To help educators in\nconcentrating more on more current viewpoints, their excess work can be\ndisposed of utilizing Machine Learning (ML). Difference to programming, ML\ndeals with information and answers to create rules. In the event that Machine\nLearning is tackled effectively, it can setup the training division and\ncontribute essentially to the development of the country. Hence, the work\npresented in this paper fortifies e-Education in India utilizing Machine\nLearning. For the most part, three concerns are focused to be tended to:\nPersonalized recommendation of course and Customized teaching methodology. The\nwork proposes utilizing developmental methodology of hereditary calculations\nfor improving conventional procedures. Implementation and experiments presented\nin the paper verify the viability of proposed calculations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.15114,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"The use of e-portfolios in teaching and assessment\n\n  In this paper, we will initially go through the results of assessment in\nmathematics according to the international assessment programs PISA, TIMSS\n(2003), with respect to students' portfolios. Furthermore, we will present the\nforms and the ways of assessment and will focus on that assessment which refers\nto the use of eportfolios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14154,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"How the University Portal Inspired Changes in the Academic Assessment\n  Culture\n\n  Information retrieval (IR) is known facilitator of changes ongoing in human\nsociety and vice versa. This is due to the fact that IR is a key component of\nthe digital ecosystems, where both information providers and information\nconsumers collaboratively address their problems with the use of technologies.\nOrganization and design of such ecosystems drives particular social impact for\nall the players involved. In this paper, we study the impact made by a\nparticular IR ecosystem (semantic portal) used for management of academic\ninformation resources and processes within the Ukrainian higher education. We\nshow how this portal is changing a collective mindset of the academic community\nof its users. We argue that such impact becomes possible due to specific\norganization of the ecosystem, where all the information resources, IR services\nand related analytics (search, assessment, ranking, etc.) and IR users inhabit\nthe same semantic space under umbrella of the corresponding ontologies.\nPersonal values and preferences of the users configure on-the-fly the\ncorresponding IR analytics and enable personalized value-driven IR services,\nmaking everyone feel involved into the organizational decision-making\nprocesses. Four years of active use of this portal in university environment\nhas been reported and related impact is evaluated in this study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.0851,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000312593,
      "text":"Study of a Hybrid Photovoltaic-Wind Smart Microgrid using Data Science\n  Approach\n\n  In this paper, a smart microgrid implemented in Paracas, Ica, Peru, composed\nof 6kWp PV + 6kW Wind and that provides electricity to a rural community of 40\nfamilies, was studied using a data science approach. Real data of solar\nirradiance, wind speed, energy demand, and voltage of the battery bank from 2\nperiods of operation were studied to find patterns, seasonality, and existing\ncorrelations between the analyzed data. Among the main results are the\nperiodicity of renewable resources and demand, the weekly behavior of\nelectricity demand and how it has progressively increased from an average of\n0.7kW in 2019 to 1.2kW in 2021, and how power outages are repeated at certain\nhours in the morning when resources are low or there is a failure in the\nbattery bank. These analyzed data will be used to improve sizing techniques and\nprovide recommendations for energy management to optimize the performance of\nsmart microgrids.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.15122,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"The Social Responsibility of Game AI\n\n  Over the last decade we have watched as artificial intelligence has been\ntransformed into one of the most important issues of our time, and games have\ngrown into the biggest entertainment industry. As a result, game AI research as\na field has enjoyed increased access to funding, exposure in the press, and\ninfluence with governments and some of the largest technology firms in the\nworld. At this pivotal moment in the history of our field, this paper argues\nthat this privileged position brings with it an important set of\nresponsibilities which we have largely failed to meet. We show to whom we are\nresponsible, identify some of these responsibilities, and suggest actions we\ncan take as a community to leverage this power for good.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.11035,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"A Meta-model for Process Failure Mode and Effects Analysis (PFMEA)\n\n  Short product lifecycles and a high variety of products force industrial\nmanufacturing processes to change frequently. Due to the manual approach of\nmany quality analysis techniques, they can significantly slow down adaption\nprocesses of production systems or make production unprofitable. Therefore,\nautomating them can be a key technology for keeping pace with market demand of\nthe future. The methodology presented here aims at a meta-model supporting\nautomation for PFMEA. The method differentiates product requirements,\nproduction steps and quality measures in such a way, that complex quality\nrequirements can be addressed in any instance of a factory using a common\nmeta-modeling language.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14956,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Leveraging Mobile Phone Data for Migration Flows\n\n  Statistics on migration flows are often derived from census data, which\nsuffer from intrinsic limitations, including costs and infrequent sampling.\nWhen censuses are used, there is typically a time gap - up to a few years -\nbetween the data collection process and the computation and publication of\nrelevant statistics. This gap is a significant drawback for the analysis of a\nphenomenon that is continuously and rapidly changing. Alternative data sources,\nsuch as surveys and field observations, also suffer from reliability, costs,\nand scale limitations. The ubiquity of mobile phones enables an accurate and\nefficient collection of up-to-date data related to migration. Indeed, passively\ncollected data by the mobile network infrastructure via aggregated,\npseudonymized Call Detail Records (CDRs) is of great value to understand human\nmigrations. Through the analysis of mobile phone data, we can shed light on the\nmobility patterns of migrants, detect spontaneous settlements and understand\nthe daily habits, levels of integration, and human connections of such\nvulnerable social groups. This Chapter discusses the importance of leveraging\nmobile phone data as an alternative data source to gather precious and\npreviously unavailable insights on various aspects of migration. Also, we\nhighlight pending challenges that would need to be addressed before we can\neffectively benefit from the availability of mobile phone data to help make\nbetter decisions that would ultimately improve millions of people's lives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.05498,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"It's COMPASlicated: The Messy Relationship between RAI Datasets and\n  Algorithmic Fairness Benchmarks\n\n  Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS\ndataset, are commonly used in algorithmic fairness papers due to benchmarking\npractices of comparing algorithms on datasets used in prior work. In many\ncases, this data is used as a benchmark to demonstrate good performance without\naccounting for the complexities of criminal justice (CJ) processes. However, we\nshow that pretrial RAI datasets can contain numerous measurement biases and\nerrors, and due to disparities in discretion and deployment, algorithmic\nfairness applied to RAI datasets is limited in making claims about real-world\noutcomes. These reasons make the datasets a poor fit for benchmarking under\nassumptions of ground truth and real-world impact. Furthermore, conventional\npractices of simply replicating previous data experiments may implicitly\ninherit or edify normative positions without explicitly interrogating\nvalue-laden assumptions. Without context of how interdisciplinary fields have\nengaged in CJ research and context of how RAIs operate upstream and downstream,\nalgorithmic fairness practices are misaligned for meaningful contribution in\nthe context of CJ, and would benefit from transparent engagement with normative\nconsiderations and values related to fairness, justice, and equality. These\nfactors prompt questions about whether benchmarks for intrinsically\nsocio-technical systems like the CJ system can exist in a beneficial and\nethical way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.10427,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Reassessing Measures for Press Freedom\n\n  There has been an increasing interest in press freedom in the face of various\nglobal scandals, transformation of media, technological change, obstacles to\ndeliberative democracy, and other factors. Press freedom is frequently used\nalso as an explanatory factor in comparative empirical research. However,\nvalidations of existing measurement instruments on press freedom have been far\nand few between. Given these points, this paper evaluates eight cross-country\ninstruments on press freedom in 146 countries between 2001 and 2020,\nreplicating an earlier study with a comparable research setup. The methodology\nis based on principal component analysis and multi-level regression modeling.\nAccording to the results, the construct (convergence) validity of the\ninstruments is good; they all measure the same underlying semi-narrow\ndefinition for press freedom elaborated in the paper. In addition, any of the\nindices seems suitable to be used interchangeability in empirical research.\nLimitations and future research directions are further discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.1103,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"BIM, Digital Twin and Cyber-Physical Systems: crossing and blurring\n  boundaries\n\n  Digital Twin in construction and the built environment have started to\nattract the attention of researchers and practitioners in recent times. Its\nanticipated value proposition is focussed on its capability of generating new\nunderstanding and insights into an asset at all stages of its life cycle,\nexploiting diverse data sets from a multitude of sources and professions, in\nreal or near real-time. However, there is still a significant debate about the\ndelineation (i.e. communalities and differences) between digital twin and other\nrelated concepts, particularly Building Information Modelling (BIM) and\nCyber-Physical Systems (CPS). To date, this debate has been confined to social\nmedia discussions, insights blogs and position papers. This paper addresses\nthis challenge using a systematic review. The aim is to investigate\ncommunalities and differences between the three concepts, Digital Twin, BIM and\nCPS. The results of this paper are expected to foster the discussion around\nthis theme within construction and the built environment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04003,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Human Resource Development and the Internet of Things\n\n  The Internet of Things (IoT) is affecting national innovation ecosystems and\nthe approach of organizations to innovation and how they create and capture\nvalue in everyday business activities. The Internet of Things (IoT), is\ndisruptive, and it will change the manner in which human resources are\ndeveloped and managed, calling for a new and adaptive human resource\ndevelopment approach. The Classical Internet communication form is human-human.\nThe prospect of IoT is that every object will have a unique way of\nidentification and can be addressed so that every object can be connected. The\ncommunication forms will expand from human-human to human-human, human-thing,\nand thing-thing. This will bring a new challenge to how Human Resource\nDevelopment (HRD) is practiced. This paper provides an overview of the Internet\nof Things and conceptualizes the role of HRD in the age of the Internet of\nThings. Keywords:\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12793,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Trust Me If You Can: Trusted Transformation Between (JSON) Schemas to\n  Support Global Authentication of Education Credentials\n\n  Recruiters and institutions around the world struggle with the verification\nof diplomas issued in a diverse and global education setting. Firstly, it is a\nnontrivial problem to identify bogus institutions selling education\ncredentials. While institutions are often accredited by qualified authorities\non a regional level, there is no global authority fulfilling this task.\nSecondly, many different data schemas are used to encode education credentials,\nwhich represents a considerable challenge to automated processing.\nConsequently, significant manual effort is required to verify credentials.\n  In this paper, we tackle these challenges by introducing a decentralized and\nopen system to automatically verify the legitimacy of issuers and interpret\ncredentials in unknown schemas. We do so by enabling participants to publish\ntransformation information, which enables verifiers to transform credentials\ninto their preferred schema. Due to the lack of a global root of trust, we\nutilize a distributed ledger to build a decentralized web of trust, which\nverifiers can query to gather information on the trustworthiness of issuing\ninstitutions and to establish trust in transformation information. Going beyond\ndiploma fraud, our system can be generalized to tackle the generalized problem\nfor other domains lacking a root of trust and agreements on data schemas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08737,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Benefits, Challenges and Contributors to Success for National eHealth\n  Systems Implementation: A Scoping Review\n\n  Our scoping review aims to assess what legal, ethical, and socio-technical\nfactors contribute or inhibit the success of national eHealth system\nimplementations. In addition, our review seeks to describe the characteristics\nand benefits of eHealth systems. We conducted a scoping review of literature\npublished in English between January 2000 and 2020 using a keyword search on\nfive databases; PubMed, Scopus, Web of Science, IEEEXplore, and ProQuest. After\nremoval of duplicates, abstract screening and full-text filtering, 86 articles\nwere included from 8276 search results. We identified 17 stakeholder groups, 6\neHealth Systems areas, and 15 types of legal regimes and standards. In-depth\ntextual analysis revealed challenges mainly in implementation, followed by\nethico-legal and data related aspects. Key factors influencing success include\npromoting trust of the system, ensuring wider acceptance amongst users,\nreconciling the system with legal requirements and ensuring an adaptable\ntechnical platform. Results revealed support for decentralised implementations\nbecause they carry less implementation and engagement challenges than\ncentralised ones. Simultaneously, due to decentralised systems interoperability\nissues, federated implementations (with a set of national standards) might be\npreferable. This study identifies the primary socio-technical, legal and\nethical factors that challenge and contribute to the success of eHealth system\nimplementations. This study also describes the complexities and characteristics\nof existing eHealth implementation programs, and surmises suggested guidance\nfor resolving the identified challenges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03673,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"Algorithms and Decision-Making in the Public Sector\n\n  This article surveys the use of algorithmic systems to support\ndecision-making in the public sector. Governments adopt, procure, and use\nalgorithmic systems to support their functions within several contexts --\nincluding criminal justice, education, and benefits provision -- with important\nconsequences for accountability, privacy, social inequity, and public\nparticipation in decision-making. We explore the social implications of\nmunicipal algorithmic systems across a variety of stages, including problem\nformulation, technology acquisition, deployment, and evaluation. We highlight\nseveral open questions that require further empirical research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.01589,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000219875,
      "text":"The Emotion coding and Propagation based on improved Genetic algorithm\n\n  Computational communication research on information has been prevalent in\nrecent years, as people are progressively inquisitive in social behavior and\npublic opinion. Nevertheless, it is of great significance to analyze the\ndirection of predominant sentiment from the sentiment communication\nperspective. In this paper, the information emotion propagation model is\nestablished by introducing revamp genetic algorithms into information emotion.\nIn the process of information dissemination, both the information emotions and\nthe network emotions are dynamic. For this model, the information emotions and\nthe network nodes emotions are quantified as binary codes. The convergence\neffects, crossover and mutation algorithms are introduced. These factors all\nact on the transmission process via dynamic propagation rate, and the improved\ngenetic algorithm also acts on the emotion transmission. In particular, the\nlatter two algorithms are different from the existing biological domain. Based\non the existing research results in other manuscripts, we perform simulation\ndescribed above on the hybrid network. The simulation results demonstrate that\nthe trend approximate to the actual data. As a result, our work can prove that\nour proposed model is essentially consistent with the actual emotion\ntransmission phenomenon.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04024,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Staying Ahead in the MOOC-Era by Teaching Innovative AI Courses\n\n  As a result of the rapidly advancing digital transformation of teaching,\nuniversities have started to face major competition from Massive Open Online\nCourses (MOOCs). Universities thus have to set themselves apart from MOOCs in\norder to justify the added value of three to five-year degree programs to\nprospective students. In this paper, we show how we address this challenge at\nDeggendorf Institute of Technology in ML and AI. We first share our best\npractices and present two concrete courses including their unique selling\npropositions: Computer Vision and Innovation Management for AI. We then\ndemonstrate how these courses contribute to Deggendorf Institute of\nTechnology's ability to differentiate itself from MOOCs (and other\nuniversities).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.06844,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Amplifying Privacy: Scaling Up Transparency Research Through Delegated\n  Access Requests\n\n  In recent years, numerous studies have used 'data subject access requests' in\na collective manner, to tackle information asymmetries and shed light on data\ncollection and privacy practices of organizations. While successful at\nincreasing transparency, such studies are quite hard to conduct for the simple\nfact that right of access is an individual right. This means that researchers\nhave to recruit participants and guide them through the often-cumbersome\nprocess of access. In this paper, we present an alternative method: to ask\nparticipants to delegate their right of access to the researchers. We discuss\nthe legal grounds for doing this, the advantages it can bring to both\nresearchers and data subjects, and present a procedural and technical design to\nexecute it in a manner that ensures data subjects stay informed and in charge\nduring the process. We tested our method in a pilot study in the Netherlands,\nand found that it creates a win-win for both the researchers and the\nparticipants. We also noted differences in how data controllers from various\nsectors react to such requests and discuss some remaining challenges.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.13663,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"The Tale of Two Localization Technologies: Enabling Accurate\n  Low-Overhead WiFi-based Localization for Low-end Phones\n\n  WiFi fingerprinting is one of the mainstream technologies for indoor\nlocalization. However, it requires an initial calibration phase during which\nthe fingerprint database is built manually. This process is labour intensive\nand needs to be repeated with any change in the environment. While a number of\nsystems have been introduced to reduce the calibration effort through RF\npropagation models or crowdsourcing, these still have some limitations. Other\napproaches use the recently developed iBeacon technology as an alternative to\nWiFi for indoor localization. However, these beacon-based solutions are limited\nto a small subset of high-end phones. In this paper, we present HybridLoc: an\naccurate low-overhead indoor localization system. The basic idea HybridLoc\nbuilds on is to leverage the sensors of high-end phones to enable localization\nof lower-end phones. Specifically, the WiFi fingerprint is crowdsourced by\nopportunistically collecting WiFi-scans labeled with location data obtained\nfrom BLE-enabled high-end smart phones. These scans are used to automatically\nconstruct the WiFi-fingerprint, that is used later to localize any lower-end\ncell phone with the ubiquitous WiFi technology. HybridLoc also has provisions\nfor handling the inherent error in the estimated BLE locations used in\nconstructing the fingerprint as well as to handle practical deployment issues\nincluding the noisy wireless environment, heterogeneous devices, among others.\nEvaluation of HybridLoc using Android phones shows that it can provide accurate\nlocalization in the same range as manual fingerprinting techniques under the\nsame conditions. Moreover, the localization accuracy on low-end phones\nsupporting only WiFi is comparable to that achieved with high-end phones\nsupporting BLE. This accuracy is achieved with no training overhead, is robust\nto the different user devices, and is consistent under environment changes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03896,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000035763,
      "text":"AI and the future of pharmaceutical research\n\n  This paper examines how pharmaceutical Artificial Intelligence advancements\nmay affect the development of new drugs in the coming years. The question was\nanswered by reviewing a rich body of source material, including industry\nliterature, research journals, AI studies, market reports, market projections,\ndiscussion papers, press releases, and organizations' websites. The paper\nargues that continued innovation in pharmaceutical AI will enable rapid\ndevelopment of safe and effective therapies for previously untreatable\ndiseases. A series of major points support this conclusion: The pharmaceutical\nindustry is in a significant productivity crisis today, and AI-enabled research\nmethods can be directly applied to reduce the time and cost of drug discovery\nprojects. The industry already reported results such as a 10-fold reduction in\ndrug molecule discovery times. Numerous AI alliances between industry,\ngovernments, and academia enabled utilizing proprietary data and led to\noutcomes such as the largest molecule toxicity database to date or more than\n200 drug safety predictive models. The momentum was recently increased by the\ninvolvement of tech giants combined with record rounds of funding. The\nlong-term effects will range from safer and more effective therapies, through\nthe diminished role of pharmaceutical patents, to large-scale collaboration and\nnew business strategies oriented around currently untreatable diseases. The\npaper notes that while many reviewed resources seem to have overly optimistic\nfuture expectations, even a fraction of these developments would alleviate the\nproductivity crisis. Finally, the paper concludes that the focus on\npharmaceutical AI put the industry on a trajectory towards another significant\ndisruption: open data sharing and collaboration.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00236,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"\"Why wouldn't someone think of democracy as a target?\": Security\n  practices & challenges of people involved with U.S. political campaigns\n\n  People who are involved with political campaigns face increased digital\nsecurity threats from well-funded, sophisticated attackers, especially\nnation-states. Improving political campaign security is a vital part of\nprotecting democracy. To identify campaign security issues, we conducted\nqualitative research with 28 participants across the U.S. political spectrum to\nunderstand the digital security practices, challenges, and perceptions of\npeople involved in campaigns. A main, overarching finding is that a unique\ncombination of threats, constraints, and work culture lead people involved with\npolitical campaigns to use technologies from across platforms and domains in\nways that leave them--and democracy--vulnerable to security attacks. Sensitive\ndata was kept in a plethora of personal and work accounts, with ad hoc adoption\nof strong passwords, two-factor authentication, encryption, and access\ncontrols. No individual company, committee, organization, campaign, or academic\ninstitution can solve the identified problems on their own. To this end, we\nprovide an initial understanding of this complex problem space and\nrecommendations for how a diverse group of experts can begin working together\nto improve security for political campaigns.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12223,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000055962,
      "text":"Reporting Revenge Porn: a Preliminary Expert Analysis\n\n  In our research, we focus on the response to the non-consensual distribution\nof intimate or sexually explicit digital images of adults, also referred as\nrevenge porn, from the point of view of the victims. In this paper, we present\na preliminary expert analysis of the process for reporting revenge porn abuses\nin selected content sharing platforms. Among these, we included social\nnetworks, image hosting websites, video hosting platforms, forums, and\npornographic sites. We looked at the way to report abuse, concerning both the\nnon-consensual online distribution of private sexual image or video (revenge\npornography), as well as the use of deepfake techniques, where the face of a\nperson can be replaced on original visual content with the aim of portraying\nthe victim in the context of sexual behaviours. This preliminary analysis is\ndirected to understand the current practices and potential issues in the\nprocedures designed by the providers for reporting these abuses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.09933,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000089076,
      "text":"Data Enforced: An Exploratory Impact Analysis of Automated Speed\n  Enforcement in the District of Columbia\n\n  In 2015, the District of Columbia framed a Vision Zero mission and action\nplan, with a target of achieving zero traffic fatalities by 2024. This study\nexamines the impacts of Automated Speed Enforcement (ASE) and its role in\nachieving the goals of Vision Zero. Independent datasets containing detailed\ninformation about traffic crashes, ASE camera locations, and citation records,\nand driving speeds across the District's streets were collected, combined, and\nanalyzed to identify patterns and trends in crashes, speed limit violations,\nand speeding behavior before and after the ASE camera installation. The results\nof this exploratory analysis confirm the safety benefits of ASE systems in\nWashington, D.C. The study also provides a blueprint for the different means of\nevaluating the short-term impact of ASE systems using different data sources\nwhich can aid practitioners in better evaluating existing systems and support\nthe decision-making process regarding future installations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03925,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Analysis of geospatial behaviour of visitors of urban gardens: is\n  positioning via smartphones a valid solution?\n\n  Tracking locations is practical and speditive with smartphones, as they are\nomnipresent devices, relatively cheap, and have the necessary sensors for\npositioning and networking integrated in the same box. Nowadays recent models\nhave GNSS antennas capable of receiving multiple constellations. In the\nproposed work we test the hypothesis that GNSS positions directly recorded by\nsmartphones can be a valid solution for spatial analysis of people's behaviour\nin an urban garden. Particular behaviours can be linked to therapeutic spots\nthat promote health and well-being of visitors. Three parts are reported: (i)\nassessment of the accuracy of the positions relative to a reference track, (ii)\nimplementation of a framework for automating transmission and processing of the\nlocation information, (iii) analysis of preferred spots via spatial analytics.\nDifferent devices were used to survey at different times and with different\nmethods, i.e. in the pocket of the owner or on a rigid frame. Accuracy was\nestimated using distance of each located point to the reference track, and\nprecision was estimated with static multiple measures. A chat-bot through the\nTelegram application was implemented to allow users to send their data to a\ncentralized computing environment thus automating the spatial analysis. Results\nreport a horizontal accuracy below ~2.3 m at 95% confidence level, without\nsignificant difference between surveys, and very little differences between\ndevices. GNSS-only and assisted navigation with telephone cells also did not\nshow significant difference. Autocorrelation of the residuals over time and\nspace showed strong consistency of the residuals, thus proving a valid solution\nfor spatial analysis of walking behaviour.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12403,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000068545,
      "text":"A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and\n  the Quest for Auditability\n\n  In this paper, we suggest a systematic approach for developing\nsocio-technical assessment for hiring ADS. We suggest using a matrix to expose\nunderlying assumptions rooted in pseudoscientific essentialized understandings\nof human nature and capability, and to critically investigate emerging auditing\nstandards and practices that fail to address these assumptions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.11021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000161264,
      "text":"Defending IEEE Software Standards in Federal Criminal Court\n\n  IEEE's 1012 Standard for independent software and hardware verification and\nvalidation (IV&V) is under attack in U.S. federal criminal court. As software\nspreads through the criminal legal system, scientists, engineers, and IEEE have\nan essential role in ensuring courts understand and respect IEEE 1012 and IV&V.\nIf scientists, engineers, and IEEE do not engage, courts will continue to allow\nunreliable scientific evidence to deprive people of their life and liberty.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08258,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Identifying Roles, Requirements and Responsibilities in Trustworthy AI\n  Systems\n\n  Artificial Intelligence (AI) systems are being deployed around the globe in\ncritical fields such as healthcare and education. In some cases, expert\npractitioners in these domains are being tasked with introducing or using such\nsystems, but have little or no insight into what data these complex systems are\nbased on, or how they are put together. In this paper, we consider an AI system\nfrom the domain practitioner's perspective and identify key roles that are\ninvolved in system deployment. We consider the differing requirements and\nresponsibilities of each role, and identify a tension between transparency and\nprivacy that needs to be addressed so that domain practitioners are able to\nintelligently assess whether a particular AI system is appropriate for use in\ntheir domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.11714,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Self-sovereign identity as a tool for digital democracy\n\n  The importance of digital identity as a foundation for digital public\nservices is considered. As the classical, centralised model digital identity\nhas proven to be subject to several limitations, self-sovereign identities are\nproposed as replacement, especially in the context of e-government platforms\nand direct participation to policymaking (e.g. through e-voting tools).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.13986,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"User-Centric Health Data Using Self-sovereign Identities\n\n  This article presents the potential use of the Self-Sovereign Identities\n(SSI), combining with Distributed Ledger Technologies (DLT), to improve the\nprivacy and control of health data. The paper presents the SSI technology,\nlists the prominent use cases of decentralized identities in the health area,\nand discusses an effective blockchain-based architecture. The main\ncontributions of the article are: (i) mapping SSI general and abstract\nconcepts, e.g., issuers and holders, to the health domain concepts, e.g.,\nphysicians and patients; (ii) creating a correspondence between the SSI\ninteractions, e.g., issue and verify a credential, and the US standardized set\nof health use cases; (iii) presenting and instantiating an architecture to deal\nwith the use cases mentioned, effectively organizing the data in a user-centric\nway, that uses well-known SSI and Blockchain technologies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14095,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Exploring the Scope and Potential of Local Newspaper-based Dengue\n  Surveillance in Bangladesh\n\n  Dengue fever has been considered to be one of the global public health\nproblems of the twenty-first century, especially in tropical and subtropical\ncountries of the global south. The high morbidity and mortality rates of Dengue\nfever impose a huge economic and health burden for middle and low-income\ncountries. It is so prevalent in such regions that enforcing a granular level\nof surveillance is quite impossible. Therefore, it is crucial to explore an\nalternative cost-effective solution that can provide updates of the ongoing\nsituation in a timely manner. In this paper, we explore the scope and potential\nof a local newspaper-based dengue surveillance system, using well-known\ndata-mining techniques, in Bangladesh from the analysis of the news contents\nwritten in the native language. In addition, we explain the working procedure\nof developing a novel database, using human-in-the-loop technique, for further\nanalysis, and classification of dengue and its intervention-related news. Our\nclassification method has an f-score of 91.45%, and matches the ground truth of\nreported cases quite closely. Based on the dengue and intervention-related\nnews, we identified the regions where more intervention efforts are needed to\nreduce the rate of dengue infection. A demo of this project can be accessed at:\nhttp:\/\/erdos.dsm.fordham.edu:3009\/\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14099,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000088745,
      "text":"The ghost of AI governance past, present and future: AI governance in\n  the European Union\n\n  The received wisdom is that artificial intelligence (AI) is a competition\nbetween the US and China. In this chapter, the author will examine how the\nEuropean Union (EU) fits into that mix and what it can offer as a third way to\ngovern AI. The chapter presents this by exploring the past, present and future\nof AI governance in the EU. Section 1 serves to explore and evidence the EUs\ncoherent and comprehensive approach to AI governance. In short, the EU ensures\nand encourages ethical, trustworthy and reliable technological development.\nThis will cover a range of key documents and policy tools that lead to the most\ncrucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive\ntowards digital sovereignty through the lens of regulation and infrastructure.\nThis covers topics such as the trustworthiness of AI systems, cloud, compute\nand foreign direct investment. In Section 3, the chapter concludes by offering\nseveral considerations to achieve good AI governance in the EU.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.01662,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Security implications of digitalization: The dangers of data colonialism\n  and the way towards sustainable and sovereign management of environmental\n  data\n\n  Digitalization opens up new opportunities in the collection, analysis, and\npresentation of data which can contribute to the achievement of the 2030 Agenda\nand its Sustainable Development Goals (SDGs). In particular, the access to and\ncontrol of environmental and geospatial data is fundamental to identify and\nunderstand global issues and trends. Also immediate crises such as the COVID-19\npandemic demonstrate the importance of accurate health data such as infection\nstatistics and the relevance of digital tools like video conferencing\nplatforms. However, today much of the data is collected and processed by\nprivate actors. Thus, governments and researchers depend on data platforms and\nproprietary systems of big tech companies such as Google or Microsoft. The\nmarket capitalization of the seven largest US and Chinese big tech companies\nhas grown to 8.7tn USD in recent years, about twice the size of Germany's gross\ndomestic product (GDP). Therefore, their market power is enormous, allowing\nthem to dictate many rules of the digital space and even interfere with\nlegislations. Based on a literature review and nine expert interviews this\nstudy presents a framework that identifies the risks and consequences along the\nworkflow of collecting, processing, storing, using of data. It also includes\nsolutions that governmental and multilateral actors can strive for to alleviate\nthe risks. Fundamental to this framework is the novel concept of \"data\ncolonialism\" which describes today's trend of private companies appropriating\nthe digital sphere. Historically, colonial nations used to grab indigenous land\nand exploit the cheap labor of slave workers. In a similar way, today's big\ntech corporations use cheap data of their users to produce valuable services\nand thus create enormous market power.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14043,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"The Characteristics of Enjoyable Online Learning for Culinary Arts\n  Student\n\n  The Covid-19 pandemic outbreak has forced all courses to be carried out\nonline, but only a few truly fulfill student expectations. This study aims to\nexplain the characteristics of enjoyable online learning based on the platform,\nthe content, and the learning model. Data collected using closed and open\nquestionnaires which were responded to by 110 students in 2019\/2020. The closed\nquestionnaire revealed the most preferred learning elements, and the open\nquestionnaire was to clarify their reasons. The data were arranged sequentially\nfrom the quantitative data to the qualitative data. The results of the study\nshowed that (1) the preferred online platforms were Moodle, Google Meet, and\nWhatsApp. They like Moodle because the content is well structured, Google Meet\nis easily accessible, and WhatsApp is their daily routine application; (2) The\nlearning content consists of 2 to 3 resources i.e.: 6-10 pages papers, 11-15\npages PowerPoint and 6-10 minute videos. Too much content causes a heavy\nlearning burden; (3) Most students preferred the blended learning strategy. The\nsynchronous lectures for 60-75 minutes can motivate them because they can\ninteract with lecturers and other students. Asynchronous lectures are more\nflexible that can be done anytime and anywhere so that the students become more\nindependent in their learning\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.01674,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"PyLUSAT: An open-source Python toolkit for GIS-based land use\n  suitability analysis\n\n  Desktop GIS applications, such as ArcGIS and QGIS, provide tools essential\nfor conducting suitability analysis, an activity that is central in formulating\na land-use plan. But, when it comes to building complicated land-use\nsuitability models, these applications have several limitations, including\noperating system-dependence, lack of dedicated modules, insufficient\nreproducibility, and difficult, if not impossible, deployment on a computing\ncluster. To address the challenges, this paper introduces PyLUSAT: Python for\nLand Use Suitability Analysis Tools. PyLUSAT is an open-source software package\nthat provides a series of tools (functions) to conduct various tasks in a\nsuitability modeling workflow. These tools were evaluated against comparable\ntools in ArcMap 10.4 with respect to both accuracy and computational\nefficiency. Results showed that PyLUSAT functions were two to ten times more\nefficient depending on the job's complexity, while generating outputs with\nsimilar accuracy compared to the ArcMap tools. PyLUSAT also features\nextensibility and cross-platform compatibility. It has been used to develop\nfourteen QGIS Processing Algorithms and implemented on a high-performance\ncomputational cluster (HiPerGator at the University of Florida) to expedite the\nprocess of suitability analysis. All these properties make PyLUSAT a\ncompetitive alternative solution for urban planners\/researchers to customize\nand automate suitability analysis as well as integrate the technique into a\nlarger analytical framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14312,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Decentralized Basic Income: Creating Wealth with On-Chain Staking and\n  Fixed-Rate Protocols\n\n  In this review, we evaluate the mechanisms behind the decentralized finance\nprotocols for generating stable, passive income. Currently, such savings\ninterest rates can be as high as 20% annually, payable in traditional currency\nvalues such as US dollars. Therefore, one can benefit from the growth of the\ncryptocurrency markets, with minimal exposure to their volatility risks. We aim\nto explain the rationale behind these savings products in simple terms. The key\nto this puzzle is that asset deposits in cryptocurrency ecosystems are of\nintrinsic economic value, as they facilitate network consensus mechanisms and\nautomated marketplaces (e.g. for lending). These functions create wealth for\nthe participants, and they provide unique advantages unavailable in traditional\nfinancial systems. Our review speaks to the notion of decentralized basic\nincome - analogous to universal basic income but guaranteed by financial\nproducts on blockchains instead of public policies. We will go through their\nimplementations of how savings can be channeled into the staking deposits in\nProof-of-Stake (PoS) protocols, through fixed-rate lending protocols and\nstaking derivative tokens, thereby exposing savers with minimal risks. We will\ndiscuss potential pitfalls, assess how these protocols may behave in market\ncycles, as well as suggest areas for further research and development.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04124,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000055631,
      "text":"PAC: Partial Area Cluster for adjusting the distribution of\n  transportation platforms in modern cities\n\n  In the modern city, the utilization rate of public transportation attached\nimportance to the efficiency of public traffic. However, the unreasonable\ndistribution of transportation platforms results in a low utilization rate. In\nthis paper, we researched and evaluated the distribution of platforms -- bus\nand subway -- and proposed a method, called \"partial area cluster\" (PAC), to\nimprove the utilization by changing and renewing the original distribution. The\nnovel method was based on the K-means algorithm in the field of machine\nlearning. PAC worked to search the suitable bus platforms as the center and\nmodified the original one to the subway. Experience has shown that the use of\npublic transport resources has increased by 20%. The study uses a similar\ncluster algorithm to solve transport networks' problems in a novel but\npractical term. As a result, the PAC is expected to be used extensively in the\ntransportation system construction process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00441,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000057287,
      "text":"When Curation Becomes Creation: Algorithms, Microcontent, and the\n  Vanishing Distinction between Platforms and Creators\n\n  Ever since social activity on the Internet began migrating from the wilds of\nthe open web to the walled gardens erected by so-called platforms, debates have\nraged about the responsibilities that these platforms ought to bear. And yet,\ndespite intense scrutiny from the news media and grassroots movements of\noutraged users, platforms continue to operate, from a legal standpoint, on the\nfriendliest terms. Under the current regulatory framework, platforms\nsimultaneously benefit from: (1) broad discretion to organize (and censor)\ncontent however they choose; (2) powerful algorithms for curating a practically\nlimitless supply of user-posted microcontent according to whatever ends they\nwish; and (3) absolution from the sorts of liability born by creators of the\nunderlying content. In this paper, we contest the very validity of the\nplatform-creator distinction, arguing that it is ill-adapted to the modern\nsocial media landscape where, in a real sense, platforms are creating\nderivative media products. We argue that any coherent regulatory framework must\nadapt to this reality, recognizing the subtle continuum of activities that span\nthe curation-creation spectrum, providing a finer system of categorization and\nclearer guidance for precisely when platforms assume the responsibilities\nassociated with content creation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00746,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Case study of Innovative Teaching Practices and their Impact for\n  Electrical Engineering Courses during COVID-19 Pandemic\n\n  Due to the COVID-19 pandemic, there was an urgent need to move to online\nteaching and develop innovations to guarantee the Student Learning Outcomes\n(SLOs) are being fulfilled. The contributions of this paper are two-fold: the\neffects of an experimented teaching strategy, i.e. multi-course project-based\nlearning (MPL) approach, are presented followed with online assessment\ntechniques investigation for senior level electrical engineering (EE) courses\nat Qatar University. The course project of the senior course was designed in\nsuch a way that it helps in simultaneously attaining the objectives of the\nsenior and capstone courses, that the students were taking at the same time. It\nis known that the MPL approach enhances the critical thinking capacity of\nstudents which is also a major outcome of Education for Sustainable Development\n(ESD). The developed project ensures the fulfillment of a series of SLOs, that\nare concentrated on soft engineering and project management skills. The\ndifficulties of adopting the MPL method for the senior level courses are in\naligning the project towards fulfilling the learning outcomes of every\nindividual course. The study also provides the students feedback on online\nassessment techniques incorporated with the MPL, due to online teaching during\nCOVID-19 pandemic. In order to provide a benchmark and to highlight the\nobtained results, the innovative teaching approaches were compared to\nconventional methods taught on the same senior course in a previous semester.\nBased on the feedback from teachers and students from previously conducted case\nstudy it was believed that the MPL approach would support the students. With\nthe statistical analysis (Chi-square, two-tailed T statistics and hypothesis\ntesting using z-test) it can be concluded that the MPL and online assessment\nactually help to achieve better attainment of the SLOs, even during a pandemic\nsituation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14107,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Analysis of the Visitor Data of a Higher Education Institution Website\n\n  In todays world, the internet affects every aspect of human life; it has\ncaused changes in corporate websites as well as in many other areas. Corporate\nwebsites should be more dynamic, more interactive, and more compatible with new\ntechnologies. The interaction of the website with users, search engines, and\nother devices has to be examined by experts, and improvements and changes\nshould be made for this interaction. In this study, a higher education\ninstitution website was examined. Visitor data collected between 2013 and 2019\nwere used for the analysis. In the study, which includes a wide range of\nexaminations and data, important findings from traffic analysis to development\nsuggestions were included. In particular, useful information has been obtained\nthrough the compatibility of the site with mobile devices, optimization of\npictures and videos, geographical features of users, language options, and\ndensity analysis of the content accessed over time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.1405,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Preventing Spoliation of Evidence with Blockchain: A Perspective from\n  South Asia\n\n  Evidence destruction and tempering is a time-tested tactic to protect the\npowerful perpetrators, criminals, and corrupt officials. Countries where law\nenforcing institutions and judicial system can be comprised, and evidence\ndestroyed or tampered, ordinary citizens feel disengaged with the investigation\nor prosecution process, and in some instances, intimidated due to the\nvulnerability to exposure and retribution. Using Distributed Ledger\nTechnologies (DLT), such as blockchain, as the underpinning technology, here we\npropose a conceptual model - 'EvidenceChain', through which citizens can\nanonymously upload digital evidence, having assurance that the integrity of the\nevidence will be preserved in an immutable and indestructible manner. Person\nuploading the evidence can anonymously share it with investigating authorities\nor openly with public, if coerced by the perpetrators or authorities.\nTransferring the ownership of evidence from authority to ordinary citizen, and\ncustodianship of evidence from susceptible centralized repository to an\nimmutable and indestructible distributed repository, can cause a paradigm shift\nof power that not only can minimize spoliation of evidence but human rights\nabuse too. Here the conceptual model was theoretically tested against some\nhigh-profile spoliation of evidence cases from four South Asian developing\ncountries that often rank high in global corruption index and low in human\nrights index.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.11929,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000060863,
      "text":"Collaborative Problem Solving on a Data Platform Kaggle\n\n  Data exchange across different domains has gained much attention as a way of\ncreating new businesses and improving the value of existing services. Data\nexchange ecosystem is developed by platform services that facilitate data and\nknowledge exchange and offer co-creation environments for organizations to\npromote their problem-solving. In this study, we investigate Kaggle, a data\nanalysis competition platform, and discuss the characteristics of data and the\necosystem that contributes to collaborative problem-solving by analyzing the\ndatasets, users, and their relationships.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14047,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"Measuring Domain Knowledge for Early Prediction of Student Performance:\n  A Semantic Approach\n\n  The growing popularity of data mining catalyses the researchers to explore\nvarious exciting aspects of education. Early prediction of student performance\nis an emerging area among them. The researchers have used various predictors in\nperformance modelling studies. Although prior cognition can affect student\nperformance, establishing their relationship is still an open research\nchallenge. Quantifying the knowledge from readily available data is the major\nchallenge here. We have proposed a semantic approach for this purpose.\nAssociation mining on nearly 0.35 million observations establishes that prior\ncognition impacts the student performance. The proposed approach of measuring\ndomain knowledge can help the early performance modelling studies to use it as\na predictor.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Developing and delivering a remote experiment based on the experiential\n  learning framework during COVID-19 pandemic\n\n  The students following Engineering disciplines should not only acquire the\nconceptual understanding of the concepts but also the processors and attitudes.\nThere are two recognizable learning environments for students, namely,\nclassroom environment and laboratory environment. With the COVID-19 pandemic,\nboth environments merged to online environments, impacting students'\ndevelopment of processes and characteristic attitudes. This paper introduces a\ntheoretical framework based on experiential learning to plan and deliver\nprocesses through an online environment. A case study based on the power factor\ncorrection experiment was presented. The traditional experiment that runs for 3\nhours was broken into smaller tasks such as a pre-lab activity, a simulation\nexercise, a PowerPoint presentation, a remote laboratory activity, and a final\nreport based on the experiential learning approach. A questionnaire that\ncarries close and open-ended questions were administered to obtain students'\nreflections about developing the processes through an online-friendly\nexperiential learning approach. The majority of the students like the approach\nfollowed and praise for providing them with an opportunity to perform the\nexperiment in a novel way during the COVID-19 situation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.10556,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Codeathon Activity: A Design Prototype for Real World Problems\n\n  Activity-based learning helps students to learn through participation. A\nvirtual codeathon activity, as part of this learning scheme, was conducted for\n180 undergraduate students to focus on analysis and design of solutions to\ncrucial real-world problems in the existing Covid-19 pandemic situation. In\nthis paper, an analysis is made to know the problem solving skills of students\ngiven a single problem statement. Evaluators can further collate these multiple\nsolutions into one optimal solution. This Codeathon activity impacts their\npractical approach towards the analysis and design.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02846,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000088745,
      "text":"Visions in Theoretical Computer Science: A Report on the TCS Visioning\n  Workshop 2020\n\n  Theoretical computer science (TCS) is a subdiscipline of computer science\nthat studies the mathematical foundations of computational and algorithmic\nprocesses and interactions. Work in this field is often recognized by its\nemphasis on mathematical technique and rigor. At the heart of the field are\nquestions surrounding the nature of computation: What does it mean to compute?\nWhat is computable? And how efficiently?\n  Every ten years or so the TCS community attends visioning workshops to\ndiscuss the challenges and recent accomplishments in the TCS field. The\nworkshops and the outputs they produce are meant both as a reflection for the\nTCS community and as guiding principles for interested investment partners.\nConcretely, the workshop output consists of a number of nuggets, each\nsummarizing a particular point, that are synthesized in the form of a white\npaper and illustrated with graphics\/slides produced by a professional graphic\ndesigner. The second TCS Visioning Workshop was organized by the SIGACT\nCommittee for the Advancement of Theoretical Computer Science and took place\nduring the week of July 20, 2020. Despite the conference being virtual, there\nwere over 76 participants, mostly from the United States, but also a few from\nEurope and Asia who were able to attend due to the online format. Workshop\nparticipants were divided into categories as reflected in the sections of this\nreport: (1) models of computation; (2) foundations of data science; (3)\ncryptography; and (4) using theoretical computer science for other domains.\nEach group participated in a series of discussions that produced the nuggets\nbelow.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0769,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000044372,
      "text":"A Machine Learning Based DSS in Predicting Undergraduate Freshmen\n  Enrolment in a Philippine University\n\n  The sudden change in the landscape of Philippine education, including the\nimplementation of K to 12 program, Higher Education institutions, have been\nstruggling in attracting freshmen applicants coupled with difficulties in\nprojecting incoming enrollees. Private HEIs Enrolment target directly impacts\nsuccess factors of Higher Education Institutions. A review of the various\ncharacteristics of freshman applicants influencing their admission status at a\nPhilippine university were included in this study. The dataset used was\nobtained from the Admissions Office of the University via an online form which\nwas circulated to all prospective applicants. Using Logistic Regression, a\npredictive model was developed to determine the likelihood that an enrolled\nstudent would seek enrolment in the institution or not based on both students\nand institution's characteristics. The LR Model was used as the algorithm in\nthe development of the Decision Support System. Weka was utilized on selection\nof features and building the LR model. The DSS was coded and designed using R\nStudio and R Shiny which includes data visualization and individual prediction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.141,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Smart Band: An Integrated Device for Emergency Management\n\n  In the event of a kidnapping or a medical emergency, a person is often\nincapacitated to be able to call for help. And it's usually too late before the\nfirst responders arrive on-scene. Currently, a vast array of 'safety' devices\navailable in the market are often far too rudimentary such as double tapping\nthe power button that isn't very practical, or an app on a smart watch that is\na huge investment in the first place. The Smart Band aims to eliminate the need\nfor a physical trigger by the person who finds himself in dangerous situations\nlike kidnapping, front-facing some wild animal, heart attack, etc., and rather\nsenses the heartbeat. The Smart Band is designed as a personalized wearable\ndevice, wherein the user heart beat rate is collected and trained using machine\nlearning algorithm, which triggers the alert system automatically when the\nevent is identified. Hence the accuracy of assessing emergency situation will\nbe high and false rate will be reduced. As soon as the event is detected, the\nband relays GPS coordinates to first responders and emergency contacts, which\nwill be sent via the Network Carrier (SIM card module) directly from the band,\nnot relying on a mobile phone, which is usually out of reach during such\nemergency situations. In essence, the Smart Band consists of a GPS tracker, a\nheartbeat sensor, a Network module, and a Bluetooth module, all existing\ntechnologies which have been mass produced to an extent that the end product\ncan be made affordable, and in huge quantities as well. On further development\nthe smart band can be customized to a finely wearable jewel which can serve the\npurpose autonomously.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03895,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Social Media Marketing (SMM) A Strategic Tool for Developing Business\n  for Tourism Companies\n\n  Social media marketing is an emerging marketing technique worldwide. This\nresearch concentrates on how effectively social media can be used to promote a\nproduct in tourism industry. The efficient use of social media develops a\ntourism company in terms of sales, branding, reach and relationship management.\nThe study aims to find the best social media platform to promote and develop a\ntourism company and the customer opinion towards planning a trip through\nonline. It also concentrates on customer response for online offers and\ndiscounts in those social media platforms. The study attempts to understand and\ncreate suitable model for social media marketing for tourism companies with a\nsample size of 400. The sampling technique used in this study is purposive\nsampling method. The purposive sample can also be called as judgemental sample.\nNormally the sample will be selected based on the knowledge possessed by the\nrespondents on a particular phenomenon. Here, the study is been conducted among\nthe people who use social media. The sampling technique helped the researcher\nto identify the target sample i.e., the social media users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09939,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Transcending Old Boundaries: Digital Afterlife in the Age of COVID-19\n\n  The primary objective of our exploratory research is to contribute to the\nongoing conversation on Digital Afterlife from the lenses of Global South\nduring the COVID-19 period. Digital Afterlife is fast becoming a challenge for\nour increasingly connected society. Moreover, the situation got worse with the\nCOVID-19 pandemic. The on-going research is to address the disparity in the\nGlobal South, specifically in countries like Indonesia, India and The\nPhilippines compared to the Global North for Digital Afterlife services such as\npolicies and digital mourning services. By addressing the research question,\n'What services and policy frameworks are available for Digital Afterlife in the\nGlobal South during COVID-19?', we aim to find the multitude of ways people in\nthe Global South are managing their digital footprints. Our preliminary\nfindings show that some considerable research and death related digital\nservices and innovation have taken place during the pandemic. However,\noverwhelming majority of these works are western-centric and mainly dealing\nwith post-mortem personal asset management. Cultural nuances, socio-economic\nperspectives, religion, political climate, regional infrastructures are mostly\nsidelined. We found significant disparity in Digital Afterlife product and\nservice designs, which got worse during the global pandemic. Our goal is to\ncollect further in-depth data within the three big ICT powerhouses of global\nsouth (Indonesia, India and The Philippines), identify the challenges as well\nas the innovations around Digital Afterlife.We envision proposing a set of\nrecommendations, based on our findings, for developing a more inclusive and\nequitable digital space in this pandemic-stricken world.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05576,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"Common Investigation Process Model for Internet of Things Forensics\n\n  Internet of Things Forensics (IoTFs) is a new discipline in digital forensics\nscience used in the detection, acquisition, preservation, rebuilding,\nanalyzing, and the presentation of evidence from IoT environments. IoTFs\ndiscipline still suffers from several issues and challenges that have in the\nrecent past been documented. For example, heterogeneity of IoT infrastructures\nhas mainly been a key challenge. The heterogeneity of the IoT infrastructures\nmakes the IoTFs very complex, and ambiguous among various forensic domain. This\npaper aims to propose a common investigation processes for IoTFs using the\nmetamodeling method called Common Investigation Process Model (CIPM) for IoTFs.\nThe proposed CIPM consists of four common investigation processes: i)\npreparation process, ii) collection process, iii) analysis process and iv)\nfinal report process. The proposed CIPM can assist IoTFs users to facilitate,\nmanage, and organize the investigation tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.13068,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000091725,
      "text":"COVID-19 Datathon Based on Deidentified Governmental Data as an Approach\n  for Solving Policy Challenges, Increasing Trust, and Building a Community:\n  Case Study\n\n  Triggered by the COVID-19 crisis, Israel's Ministry of Health (MoH) held a\nvirtual Datathon based on deidentified governmental data. Organized by a\nmultidisciplinary committee, Israel's research community was invited to offer\ninsights to COVID-19 policy challenges. The Datathon was designed to (1)\ndevelop operationalizable data-driven models to address COVID-19 health-policy\nchallenges and (2) build a community of researchers from academia, industry,\nand government and rebuild their trust in the government. Three specific\nchallenges were defined based on their relevance (significance, data\navailability, and potential to anonymize the data): immunization policies,\nspecial needs of the young population, and populations whose rate of compliance\nwith COVID-19 testing is low. The MoH team extracted diverse, reliable,\nup-to-date, and deidentified governmental datasets for each challenge. Secure\nremote-access research environments with relevant data science tools were set\non Amazon Web. The MoH screened the applicants and accepted around 80\nparticipants, teaming them to balance areas of expertise as well as represent\nall sectors of the community. One week following the event, anonymous surveys\nfor participants and mentors were distributed to assess overall usefulness and\npoints for improvement. The 48-hour Datathon and pre-event sessions included 18\nmultidisciplinary teams, mentored by 20 data scientists, 6 epidemiologists, 5\npresentation mentors, and 12 judges. The insights developed by the 3 winning\nteams are currently considered by the MoH as potential data science methods\nrelevant for national policies. The most positive results were increased trust\nin the MoH and greater readiness to work with the government on these or future\nprojects. Detailed feedback offered concrete lessons for improving the\nstructure and organization of future government-led datathons.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07721,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"From social netizens to data citizens: variations of GDPR awareness in\n  28 European countries\n\n  We studied variability in General Data Protection Regulation (GDPR) awareness\nin relation to digital experience in the 28 European countries of EU27-UK,\nthrough secondary analysis of the Eurobarometer 91.2 survey conducted in March\n2019 (N = 27,524). Education, occupation, and age were the strongest\nsociodemographic predictors of GDPR awareness, with little influence of gender,\nsubjective economic well-being, or locality size. Digital experience was\nsignificantly and positively correlated with GDPR awareness in a linear model,\nbut this relationship proved to be more complex when we examined it through a\ntypological analysis. Using an exploratory k-means cluster analysis we\nidentified four clusters of digital citizenship, across both dimensions of\ndigital experience and GDPR awareness: the off-line citizens (22%), the social\nnetizens (32%), the web citizens (17%), and the data citizens (29%). The\noff-line citizens ranked lowest in internet use and GDPR awareness; the web\ncitizens ranked at about average values, while the data citizens ranked highest\nin both digital experience and GDPR knowledge and use. The fourth identified\ncluster, the social netizens, had a discordant profile, with remarkably high\nsocial network use, below average online shopping experiences, and low GDPR\nawareness. Digitalization in human capital and general internet use is a strong\ncountry-level correlate of the national frequency of the data citizen type. Our\nresults confirm previous studies of the low privacy awareness and skills\nassociated with intense social media consumption, but we found that young\ngenerations are evenly divided between the rather carefree social netizens and\nthe strongly invested data citizens.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09938,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000046359,
      "text":"The Commodification of Open Educational Resources for Teaching and\n  Learning by Academics in an Open Distance e-Learning Institution\n\n  The use of open educational resources (OER) is gaining momentum in higher\neducation institutions. This study sought to establish academics' perceptions\nand knowledge of OER for teaching and learning in an open distance e-learning\n(ODeL) university. The study also sought to establish how perceptions are\nformed. The inductive approach followed the lens of commodification to answer\nthe research questions. The commodification phase allowed for a better\nunderstanding of the academics' prior knowledge, informers, academics behaviour\nabout OER, and how they perceived OER to be useful for teaching and learning.\nThe study employed a qualitative method, with semi-structured interviews to\ncollect data. The study found that academics with prior experience and\nknowledge of OER are more successful in the use of these resources for\nteaching, learning, and research. OER is also perceived as a useful tool to\npromote African knowledge, showcase the contributions of African academics,\nimprove academic research capabilities, improve student's success rate,\nparticularly for financially vulnerable students. Based on the acquired\nperceptions, the study able to propose a new guideline to formulate user\nperceptions. However, this can only be achieved through a solid OER policy with\nthe support of government and tertiary institution top management. The findings\nmay inform higher education institutions when they consider the development of\nOER strategies and policies, especially in response to the Covid-19 emergency\nonline learning transition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07711,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Explainability Auditing for Intelligent Systems: A Rationale for\n  Multi-Disciplinary Perspectives\n\n  National and international guidelines for trustworthy artificial intelligence\n(AI) consider explainability to be a central facet of trustworthy systems. This\npaper outlines a multi-disciplinary rationale for explainability auditing.\nSpecifically, we propose that explainability auditing can ensure the quality of\nexplainability of systems in applied contexts and can be the basis for\ncertification as a means to communicate whether systems meet certain\nexplainability standards and requirements. Moreover, we emphasize that\nexplainability auditing needs to take a multi-disciplinary perspective, and we\nprovide an overview of four perspectives (technical, psychological, ethical,\nlegal) and their respective benefits with respect to explainability auditing.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09724,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Deconstructing the Dichotomous Relationship Between \"IT analysts and\n  End-users\": A Case of Implementing Standard Indicators in Cameroon\n\n  Diff\\'erance and suppl\\'ement are post-structuralist concepts for analyzing\nlanguage in text and are most often associated with the work of Jacque Derrida.\nThe findings after the implementation of standard health indicators in Cameroon\nshow that staff at the peripheral level encounter multiple challenges,\nincluding lack of participation during the implementation process, and tension\nbetween staff at the peripheral level and IT staff at the central level, which\nresult in non-use of the system. We use deconstruction to understand the root\ncause and the findings reveal that IT professionals and end-users are embedded\nin a relation of domination. That is, IT professionals are diff\\'erance from\nend-users and end-users are suppl\\'ement of IT professionals. Although\nend-users are portrayed as supplementary, they are supposed to manage the\nsystem, which is contradictory. This led to IT professionals having more\nprivilege and authority over end-users. This dichotomous relation is a\nderivative of the organizational structure. The notion of portraying IT\nprofessionals in charge and having more authority over end-users is an avenue\nfor conflict. The paper concludes that a HIS organizational structure where\ndecision-making is centralized is a ground for conflict and a major roadblock\nof building local capacity and providing infrastructural support at the\nperipheral.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09712,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Resilient ICT4D: Building and Sustaining our Community in Pandemic Times\n\n  The impacts of the COVID-19 pandemic, disproportionally affecting vulnerable\npeople and deepening pre-existing inequalities (Dreze, 2020; Qureshi, 2021),\nhave interested the very same \"development\" processes that the IFIP Working\nGroup 9.4 on the Implications of Information and Digital Technologies for\nDevelopment has dealt with over time. A global development paradigm (Oldekop et\nal., 2020) has emerged in response to the global nature of the crisis, infusing\nnew meaning in the spirit of \"making a better world\" with ICTs (Walsham, 2012)\nthat always have characterised ICT4D research. Such a new meaning\ncontextualises our research in the landscape of the first pandemic of the\ndatafied society (Milan & Trere, 2020), coming to terms with the silencing of\nnarratives from the margins within the pandemic (Milan et al., 2021) - in\nQureshi's (2021) words, a \"pandemics within the pandemic\" producing new\nsocio-economic inequities in a state of global emergency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09958,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000088414,
      "text":"Earth Observation and the New African Rural Datascapes: Defining an\n  Agenda for Critical Research\n\n  The increasing availability of Earth Observation data could transform the use\nand governance of African rural landscapes, with major implications for the\nlivelihoods and wellbeing of people living in those landscapes. Recent years\nhave seen a rapid increase in the development of EO data applications targeted\nat stakeholders in African agricultural systems. But there is still relatively\nlittle critical scholarship questioning how EO data are accessed, presented,\ndisseminated and used in different socio-political contexts, or of whether this\nincreases or decreases the wellbeing of poorer and marginalized peoples. We\nhighlight three neglected areas in existing EO-for-development research: (i)\nthe imaginaries of 'ideal' future landscapes informing deployments of EO data;\n(ii) how power relationships in larger EO-for-development networks shape the\ndistribution of costs and benefits; and (iii) how these larger-scale political\ndynamics interact with local-scale inequalities to influence the resilience of\nmarginalised peoples. We then propose a framework for critical\nEO-for-development research drawing on recent thinking in critical data\nstudies, ICT4D and political ecology.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09731,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Reflections, Learnings and Proposed Interventions on Data Validation and\n  Data Use for Action in Health: A Case of Mozambique\n\n  The ideal of a country's health information system (HIS) is to develop\nprocesses that ensure easy collection of relevant data and enable their\nconversion to useful health indicators, which guide decision making and support\nhealth interventions. In many Low- and Middle-Income Countries (LMICs),\nactively engaged in health reform efforts, the role of HIS is crucial,\nparticularly in terms of quality of data and its ability to inspire trust in\ndecision makers to actively use routine HIS data. Recognizing digital platforms\npotential to support those efforts, several interventions have been implemented\nin many LMICs. In turn, while the transition from paper registers to digital\nplatforms carries the promise of improving data quality processes, this promise\nhas been notoriously complex to materialize in practice. The authors draw upon\nmore than 15 years of experience implementing HIS in Mozambique to understand\nhow the potential of digital platforms have been realized with respect to data\nquality, what are the gaps and required remedial steps.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.12262,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Information Disorders, Moral Values and the Dispute of Narratives\n\n  In this paper we propose a framework characterizing information disorders as\ndisputes of narratives. Such narratives present claims to readers, who must\ndecide whether to accept the statements in the claims as facts. We point out\nthat this process requires establishing connections to moral values, since it\nhas been shown that human decision making is heavily dependent on them. A\nsimple example illustrating how this could be done is given, related to claims\nabout fraud in the US 2020 Presidential elections.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.098,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Big Data Analytics in Humanitarian and Disaster Operations: A Systematic\n  Review\n\n  By the outset of this review, 168 million people needed humanitarian aid, and\nthe number grew to 235 million by the end of the completion of this review.\nThere is no time to lose, definitely no data to lose. Humanitarian relief is\ncrucial not just to contend with a pandemic once a century but also to provide\nhelp during civil conflicts, ever-increasing natural disasters, and other forms\nof crisis. Reliance on technology has never been so relevant and critical than\nnow. The creation of more data and advancements in data analytics provides an\nopportunity to the humanitarian field. This review aimed at providing a\nholistic understanding of big data analytics in a humanitarian and disaster\nsetting. A systematic literature review method is used to examine the field and\nthe results of this review explain research gaps, and opportunities available\nfor future research. This study has shown a significant research imbalance in\nthe disaster phase, highlighting how the emphasis is on responsive measures\nthan preventive measures. Such reactionary measures would only exacerbate the\ndisaster, as is the case in many nations with COVID-19. Overall this research\ndetails the current state of big data analytics in a humanitarian and disaster\nsetting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0995,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Digital Resilience for What? Case Study of South Korea\n\n  Resilience has become an emerging topic in various fields of academic\nresearch. In spite of its widespread use, there remains conceptual confusion\nover what resilience means particularly in multi-disciplinary studies including\nthe field of ICT and Development. With the potential of digital technology,\nresearch is needed to critically question what key socio-institutional values\nrelated to resilience are being strengthened, for what and for whom through the\ndifferent conceptualizations of resilience. In this study, we conduct an\ninterpretive case study on South Korea's response to the pandemic and construct\na chronological narrative to identify key aspects of digital resilience. We\nidentify agility, diversity, and plurality - enabled by active roles of various\nstakeholders, including citizens, research communities, and private sector - as\nkeys to digital resilience to the pandemic. Findings from the case of South\nKorea provide implications to ICT4D research while discussing how developing\ncountries, where a national single window platform is typically implemented\nwith greater level of homogeneity, achieve digital resilience with inclusive\ninnovation with plurality of diverse platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09758,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Smart Cities: Potentialities and Challenges in a Context of Sharing\n  Economy\n\n  The purpose of the present paper is to show how blockchain and IoT\ntechnologies can benefit smart city projects, which tend to spread in the\ncontext of the sharing economy. The article also aims to describe the\nchallenges and potentialities of smart city projects. It was found that\ntechnology platforms can serve as a strategy to build the basis for product\ndevelopment (goods and services) and technology-based innovation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02324,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Shifting Mobility Behaviors in Unprecedented Times: Intentions to Use\n  On-demand Ride Services During the COVID-19 Pandemic\n\n  The spread of COVID-19 has been a major disruptive force in people's everyday\nlives and mobility behavior. The demand for on-demand ride services, such as\ntaxis and ridehailing has been specifically impacted, given both restrictions\nin service operations and user's concerns about virus transmission in shared\nvehicles. During the pandemic, demand for these modes have decreased by as much\nas 80%. This study examines intentions to use on-demand ride services in a\nperiod of drastic changes in lifestyles and daily routines coupled with\nunprecedented mobility reductions. Specifically, we examine the determinants\nfor the shift of intentions to use these on-demand modes of travel in the early\nstages of the pandemic. Using data from a survey disseminated in June 2020 to\n700 respondents from contiguous United States, ordinal regression modeling is\napplied to analyze the shift in consideration. The results indicate that\npolitical orientation and health-related experiences during the pandemic are\nsignificant sources of variation for individual changes in intentions to use\nridehailing. Additionally, characteristics such as age and income result in\nconsideration shifts that contradict the typical ridership profiles found in\nthe ridehailing literature. Specifically, on-demand ride consideration\ndecreases as a function of age and income. Moreover, transit-users are more\nwilling to consider on-demand rides than private vehicle users, suggesting that\nshared vehicle modes have a similar risk-profile. We discuss the role of\non-demand ride services in the pandemic era, and the need to investigate\npolitical orientation and evolving pandemic experiences to pinpoint their role\nin future mobility systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09789,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"An Exploration of Factors Influencing the Adoption of ICT Enabled\n  Entrepreneurship Applications in Namibian Rural Communities\n\n  Digital services have the potential to improve rural entrepreneurs' access to\nwider markets and increase their competitiveness among other benefits.\nMoreover, during the ongoing Covid-19 pandemic in which movement and physical\ncontacts have been limited, businesses relied much on digital services.\nHowever, many Namibian rural entrepreneurs have not been able to use digital\nservices to maintain their livelihood. Therefore, this study investigated the\nfactors affecting the adoption of ICT enabled services by rural entrepreneurs.\nThe study applied a cross-sectional survey of 77 respondents comprising 14\nrural entrepreneurs and 63 rural community members from four sites. It was\nfound that the five main factors affecting the adoption of digital services by\nrural entrepreneurs are a lack of awareness of digital services, electricity,\nskills to navigate smart devices, high cost of both devices and mobile internet\nand cybercrime. We recommend a tailor-made training program for rural\nentrepreneurs which includes raising awareness of digital services and\nassociated benefits, capacity building on digital skills and best practice for\ncybersecurity. In addition, we propose that the Namibian Government should\nenhance digital inclusion through a policy initiative to reduce the cost to\nmake both data and smart devices affordable for the poor and rural communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10079,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Cloud Computing Adoption: Opportunities and Challenges for Small, Medium\n  and Micro Enterprises in South Africa\n\n  The purpose of the paper is to determine the opportunities and challenges\nthat lead to cloud computing adoption by SMMEs in South Africa by looking at\nthe factors that influence adoption. The TOE framework is used to contextualize\nthe factors that influence cloud computing adoption and evaluate the\nopportunities and challenges that are presented by cloud computing to SMMEs in\nSouth Africa. An online survey questionnaire was used to collect data from\nleaders of SMMEs from all geographical regions and business industries in South\nAfrica. A quantitative research approach was adopted to investigate the\nobjectives, and descriptive analysis was used to evaluate the relationships and\npresent the results. The findings of the study show that relative advantage is\nan important factor in the consideration of cloud computing adoption by SMMEs,\nwhile government and regulatory support is perceived as a barrier. Top\nmanagement support, which has been previously found by other studies to be a\nsignificant factor has been found to be insignificant in this study. The study\nhas revealed that cloud computing presents opportunities to SMMEs and improves\ntheir competitiveness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0996,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"The Moderating Effect of Gender on Adopting Digital Government\n  Innovations in Ethiopia\n\n  Digital government innovation is being recognised as a solution to many\nproblems faced by governments in providing services to their citizens. It is\nespecially important for low-income countries where there are resource\nconstraints. This research was aimed at exploring the moderating effect of\ngender on the adoption of a digital government innovation in Ethiopia based on\nthe UTAUT model (n=270) and using structural equation modeling (SEM). The\nresults reveal that gender only moderates the relationship between facilitating\nconditions and usage behavior of government employees to adopt the digital\ngovernment innovation which is inconsistent with other findings. Another key\nfinding was that even though the innovation was regarded as not being easy to\nuse, women identified that they would still use it because of the social\ninfluence from the peers and the bosses. This finding suggests that women\ngovernment employees who obtain external support are more likely to use digital\ngovernment innovations compared with men who are unlikely to use it even if\nthey were facilitated. The paper recommends that governments of low-income\ncountries like Ethiopia should design appropriate policies that encourage women\nin digital government.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09953,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Impact of Culture on the Adoption of Diabetes Self-Management\n  Applications: Cape Flats, South Africa\n\n  Diabetes is a global health problem with a high mortality rate. The research\nindicates low levels of technology use amongst diabetic patients in low\nsocioeconomic environments and minority groups. We posit that the culture of\npatients is a potential reason for the low adoption and use of technology.\nHowever, research on the proliferation of culture at an individual level is\nlimited. Therefore, this paper assessed the influence of culture on mobile\napplication adoption and use amongst diabetic patients in the Cape Flats, South\nAfrica. This study used key constructs from the Theory of Planned Behaviour\n(TPB) and Hofstede's cultural dimensions. It was analysed using survey data\nfrom 439 respondents using purposive sampling. It was found that the dimensions\nof Hofstede and the Theory of Planned Behaviour can identify how culture\ninfluences mobile application adoption of diabetic patients in the geographical\nCape Flats area. However, this research indicates a stronger relationship\nbetween culture and diabetes self-management activities than culture and the\nadoption of mobile applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.09721,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Improving Data Use and Participatory Action and Design to Support Data\n  Use: The Case of DHIS2 in Rwanda\n\n  This article reports from an ongoing 'evaluation for improvement' action\nresearch and participatory design project in Rwanda, where the aim is to\nimprove data use practices and the capabilities of the District Health\nInformation Software 2 (DHIS2), an open source health information management\nplatform, to support data use. The study of data use at health facility and\ndistrict level showed that while data was used routinely at, for example,\nmonthly coordination meetings, the DHIS2 dashboards and other analytical tools\nwere in limited use because users preferred to use Microsoft Excel for data\nanalysis and use. Given such findings, a major focus of the project has been\ndirected towards identifying shortcomings in data use practices and in the\nsoftware platform and to suggest, design and eventually implement changes.\nWhile the practical work on implementing improvements have been slow due to the\nCOVID-19 pandemic, the suggested design improvements involve many levels of\nsystem design and participation, from the global core DHIS2 software team, the\ncountry DHIS2 team and local app development, the Rwanda Ministry of Health,\nand health workers at local level.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09255,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Towards responsible research in digital technology for health care\n\n  Digital technology is everywhere for the benefit of our daily and\nprofessional life. It strongly impacts our life and was crucial to maintain\nprofessional and social activities during the COVID19 crisis. Similarly,\ndigital technologies are key within biomedical engineering research topics.\nInnovations have been generated and introduced over the last 40 years,\ndemonstrating how computing and digital technologies have impacted health care.\nAlthough the benefits of digital technology are obvious now, we are at the\nconvergence of several issues which makes us aware about social, societal and\nenvironmental challenges associated with this technology. In the social domain,\ndigital technologies raise concern about exclusion (financial, geographical,\neducational, demographical, racial, gender, language, and disabled related\nexclusion) and physical and mental health. In the societal dimension, digital\ntechnologies raise concern about politics and democracy (sovereignty and\ngovernance, cognitive filters and citizen's engagement), privacy and security\n(data acquisition and usage transparency, level of personal approval, and level\nof anonymization), and economics. In the environmental dimension, digital\ntechnologies raise concern about energy consumption and hardware production.\nThis paper introduces and defines these challenges for digital technology in\ngeneral, as well as when applied to health care. The objective of this paper is\nto make the research community more aware about the challenges of digital\ntechnology and to promote more transparency for innovative and responsible\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07745,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000053644,
      "text":"Estimating Wildfire Evacuation Decision and Departure Timing Using\n  Large-Scale GPS Data\n\n  With increased frequency and intensity due to climate change, wildfires have\nbecome a growing global concern. This creates severe challenges for fire and\nemergency services as well as communities in the wildland-urban interface\n(WUI). To reduce wildfire risk and enhance the safety of WUI communities,\nimproving our understanding of wildfire evacuation is a pressing need. To this\nend, this study proposes a new methodology to analyze human behavior during\nwildfires by leveraging a large-scale GPS dataset. This methodology includes a\nhome-location inference algorithm and an evacuation-behavior inference\nalgorithm, to systematically identify different groups of wildfire evacuees\n(i.e., self-evacuee, shadow evacuee, evacuee under warning, and ordered\nevacuee). We applied the methodology to the 2019 Kincade Fire in Sonoma County,\nCA. We found that among all groups of evacuees, self-evacuees and shadow\nevacuees accounted for more than half of the evacuees during the Kincade Fire.\nThe results also show that inside of the evacuation warning\/order zones, the\ntotal evacuation compliance rate was around 46% among all the categorized\npeople. The findings of this study can be used by emergency managers and\nplanners to better target public outreach campaigns, training protocols, and\nemergency communication strategies to prepare WUI households for future\nwildfire events.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11779,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000059605,
      "text":"Factors influencing Drug Consumption and Prediction Methods\n\n  Estimating the needs of healthcare products and inventory management are\nstill challenging issues in hospitals nowadays. Centers are supposed to cope\nwith tight budgets and patient satisfaction at the same time. Some issues can\nbe tackled in advance, especially regarding the prediction of drug consumption\nneeds. This work delves into the literature in order to highlight existing\nmethods of quantifying and estimating the needs for drugs in health facilities.\nA second objective is to draw up a list of factors that impact drug consumption\nin particular, factors that are used in these prediction methods. Following\nthis literature review, it appears that six sustainable methods are being used\nby practitioners around the world, taking into account certain prerequisites\nand types of data. Thirty-four factors are identified as well and grouped into\nthree categories. These results should participate in setting up new tools for\npredicting the need of drugs, to facilitate the upstream dimensioning of new\npharmaceutical warehouses and to solve some hospital logistics issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09251,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Predicting Indian Supreme Court Judgments, Decisions, Or Appeals\n\n  Legal predictive models are of enormous interest and value to legal\ncommunity. The stakeholders, specially, the judges and attorneys can take the\nbest advantages of these models to predict the case outcomes to further augment\ntheir future course of actions, for example speeding up the decision making,\nsupport the arguments, strengthening the defense, etc. However, accurately\npredicting the legal decisions and case outcomes is an arduous process, which\ninvolves several complex steps -- finding suitable bulk case documents, data\nextracting, cleansing and engineering, etc. Additionally, the legal complexity\nfurther adds to its intricacies. In this paper, we introduce our newly\ndeveloped ML-enabled legal prediction model and its operational prototype,\neLegPredict; which successfully predicts the Indian supreme court decisions.\nThe eLegPredict is trained and tested over 3072 supreme court cases and has\nachieved 76% accuracy (F1-score). The eLegPredict is equipped with a mechanism\nto aid end users, where as soon as a document with new case description is\ndropped into a designated directory, the system quickly reads through its\ncontent and generates prediction. To our best understanding, eLegPredict is the\nfirst legal prediction model to predict Indian supreme court decisions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.13197,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Using Comics to Introduce and Reinforce Programming Concepts in CS1\n\n  Recent work investigated the potential of comics to support the teaching and\nlearning of programming concepts and suggested several ways $coding$ $strips$,\na form of comic strip with its corresponding code, can be used. Building on\nthis work, we tested the recommended use cases of $coding$ $strip$ in an\nundergraduate introductory computer science course at a large comprehensive\nuniversity. At the end of the course, we surveyed students to assess their\nexperience and found they benefited in various ways. Our work contributes a\ndemonstration of the various ways comics can be used in introductory CS courses\nand an initial understanding of benefits and challenges with using comics in\ncomputing education gleaned from an analysis of students' survey responses and\ncode submissions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07907,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"How diverse is the ACII community? Analysing gender, geographical and\n  business diversity of Affective Computing research\n\n  ACII is the premier international forum for presenting the latest research on\naffective computing. In this work, we monitor, quantify and reflect on the\ndiversity in ACII conference across time by computing a set of indexes. We\nmeasure diversity in terms of gender, geographic location and academia vs\nresearch centres vs industry, and consider three different actors: authors,\nkeynote speakers and organizers. Results raise awareness on the limited\ndiversity in the field, in all studied facets, and compared to other AI\nconferences. While gender diversity is relatively high, equality is far from\nbeing reached. The community is dominated by European, Asian and North American\nresearchers, leading the rest of continents under-represented. There is also a\nstrong absence of companies and research centres focusing on applied research\nand products. This study fosters discussion in the community on the need for\ndiversity and related challenges in terms of minimizing potential biases of the\ndeveloped systems to the represented groups. We intend our paper to contribute\nwith a first analysis to consider as a monitoring tool when implementing\ndiversity initiatives. The data collected for this study are publicly released\nthrough the European divinAI initiative.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.02359,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"Examining the tech stacks of Czech and Slovak untrustworthy websites\n\n  The burgeoning of misleading or false information spread by untrustworthy\nwebsites has, without doubt, created a dangerous concoction. Thus, it is not a\nsurprise that the threat posed by untrustworthy websites has emerged as a\ncentral concern on the public agenda in many countries, including Czechia and\nSlovakia. However, combating this harmful phenomenon has proven to be\ndifficult, with approaches primarily focusing on tackling consequences instead\nof prevention, as websites are routinely seen as quasi-sovereign organisms.\nWebsites, however, rely upon a host of service providers, which, in a way, hold\nsubstantial power over them. Notwithstanding the apparent power hold by such\ntech stack layers, scholarship on this topic remains largely limited. This\narticle contributes to this small body of knowledge by providing a\nfirst-of-its-kind systematic mapping of the back-end infrastructural support\nthat makes up the tech stacks of Czech and Slovak untrustworthy websites. Our\napproach is based on collecting and analyzing data on top-level domain\noperators, domain name Registrars, email providers, web hosting providers, and\nutilized website tracking technologies of 150 Czech and Slovak untrustworthy\nwebsites. Our findings show that the Czech and Slovak untrustworthy website\nlandscape relies on a vast number of back-end services spread across multiple\ncountries, but in key tech stack layers is nevertheless still heavily dominated\nby locally based companies. Finally, given our findings, we discuss various\npossible avenues of utilizing the numeral tech stack layers in combating online\ndisinformation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07012,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Searching for Representation: A sociotechnical audit of googling for\n  members of U.S. Congress\n\n  High-quality online civic infrastructure is increasingly critical for the\nsuccess of democratic processes. There is a pervasive reliance on search\nengines to find facts and information necessary for political participation and\noversight. We find that approximately 10\\% of the top Google search results are\nlikely to mislead California information seekers who use search to identify\ntheir congressional representatives. 70\\% of the misleading results appear in\nfeatured snippets above the organic search results. We use both qualitative and\nquantitative methods to understand what aspects of the information ecosystem\nlead to this sociotechnical breakdown. Factors identified include Google's\nheavy reliance on Wikipedia, the lack of authoritative, machine parsable, high\naccuracy data about the identity of elected officials based on geographic\nlocation, and the search engine's treatment of under-specified queries. We\nrecommend steps that Google can take to meet its stated commitment to providing\nhigh quality civic information, and steps that information providers can take\nto improve the legibility and quality of information about congressional\nrepresentatives available to search algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.13816,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000073512,
      "text":"Fighting the Fog: Evaluating the Clarity of Privacy Disclosures in the\n  Age of CCPA\n\n  Vagueness and ambiguity in privacy policies threaten the ability of consumers\nto make informed choices about how businesses collect, use, and share their\npersonal information. The California Consumer Privacy Act (CCPA) of 2018 was\nintended to provide Californian consumers with more control by mandating that\nbusinesses (1) clearly disclose their data practices and (2) provide choices\nfor consumers to opt out of specific data practices. In this work, we explore\nto what extent CCPA's disclosure requirements, as implemented in actual privacy\npolicies, can help consumers to answer questions about the data practices of\nbusinesses. First, we analyzed 95 privacy policies from popular websites; our\nfindings showed that there is considerable variance in how businesses interpret\nCCPA's definitions. Then, our user survey of 364 Californian consumers showed\nthat this variance affects the ability of users to understand the data\npractices of businesses. Our results suggest that CCPA's mandates for privacy\ndisclosures, as currently implemented, have not yet yielded the level of\nclarity they were designed to deliver, due to both vagueness and ambiguity in\nCCPA itself as well as potential non-compliance by businesses in their privacy\npolicies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07022,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"How Does Counterfactually Augmented Data Impact Models for Social\n  Computing Constructs?\n\n  As NLP models are increasingly deployed in socially situated settings such as\nonline abusive content detection, it is crucial to ensure that these models are\nrobust. One way of improving model robustness is to generate counterfactually\naugmented data (CAD) for training models that can better learn to distinguish\nbetween core features and data artifacts. While models trained on this type of\ndata have shown promising out-of-domain generalizability, it is still unclear\nwhat the sources of such improvements are. We investigate the benefits of CAD\nfor social NLP models by focusing on three social computing constructs --\nsentiment, sexism, and hate speech. Assessing the performance of models trained\nwith and without CAD across different types of datasets, we find that while\nmodels trained on CAD show lower in-domain performance, they generalize better\nout-of-domain. We unpack this apparent discrepancy using machine explanations\nand find that CAD reduces model reliance on spurious features. Leveraging a\nnovel typology of CAD to analyze their relationship with model performance, we\nfind that CAD which acts on the construct directly or a diverse set of CAD\nleads to higher performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09252,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Exploring Individual and Collaborative Storytelling in an Introductory\n  Creative Coding Class\n\n  Teaching programming through storytelling is a popular pedagogical approach\nand an active area of research. However, most previous work in this area\nfocused on K-12 students using block-based programming. Little, if any, work\nhas examined the approach with university students using text-based\nprogramming. This experience report fills this gap. Specifically, we report our\nexperience administering three storytelling assignments -- two individual and\none collaborative -- in an introductory computer science class with 49\nundergraduate students using $\\textit{p5.js}$, a text-based programming library\nfor creative coding. Our work contributes an understanding of students'\nexperiences with the three authoring processes and a set of recommendations to\nimprove the administration of and experience with individual and collaborative\nstorytelling with text-based programming.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07905,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000094043,
      "text":"Risk Management of AI\/ML Software as a Medical Device (SaMD): On ISO\n  14971 and Related Standards and Guidances\n\n  Safety and efficacy are the paramount objectives of medical device\nregulation. And in line with the medical ethos of non-maleficence, first do no\nharm, safety is the primary goal of regulation also. As such, risk management\nis the underlying principle that governs the regulation of medical devices,\nwhether traditional devices or Software as a Medical Device (SaMD). In this\narticle, I review how Risk Management Standard ISO 14971:2019 both connects\nwith and serves as a foundation for the other parts of the Artificial\nIntelligence (AI)\/Machine Learning (ML) SaMD regulatory framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07902,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Token-based Insurance Solutions on Blockchain\n\n  With the rising demand for protection against new risks such as loss of\ndigital assets, novel insurance services and products emerge. In particular,\ntoken-based insurance solutions on blockchain transform the insurance business\nby providing cover for new risks and streamlined, (semi-)automated underwriting\nand claim processes. In the chapter, we present a general framework of\ntoken-based insurance solutions, delegating their fundamental building blocks\nthat include core roles, main tokens and assets, as well as key processes and\noperations. We describe three major token-based insurance solutions in the\nmarket and compare them in terms of native token functionality, tokenized cover\ntypes, claim assessment process and capital model. Based on the discussion on\nthe general framework and concrete examples of token-based insurance solutions,\nwe summarize their advantages and point out their drawbacks. We conclude that\ndespite being at a nascent stage, the token-based insurance space bears the\npromise to unseat the incumbent players with increasingly more action taking\nplace and more use cases being explored.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14138,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000035101,
      "text":"A simulation sandbox to compare fixed-route, semi-flexible-transit, and\n  on-demand microtransit system designs\n\n  With advances in emerging technologies, options for operating public transit\nservices have broadened from conventional fixed-route service through\nsemi-flexible service to on-demand microtransit. Nevertheless, guidelines for\ndeciding between these services remain limited in the real implementation. An\nopen-source simulation sandbox is developed that can compare\nstate-of-the-practice methods for evaluating between the different types of\npublic transit operations. For the case of the semi-flexible service, the\nMobility Allowance Shuttle Transit (MAST) system is extended to include\npassenger deviations. A case study demonstrates the sandbox to evaluate and\nexisting B63 bus route in Brooklyn, NY and compares its performance with the\nfour other system designs spanning across the three service types for three\ndifferent demand scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00198,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000059605,
      "text":"Designing and Implementing e-School Systems: An Information Systems\n  Approach to School Management of a Community College in Northern Mindanao,\n  Philippines\n\n  Colleges and Universities have been established to provide educational\nservices to the people. Like any other organization, the school has processes\nand procedures similar to business or industry that involve admissions,\nprocessing of data, and generation of reports. Those processes are made\npossible through a centralized system in storing, processing, and retrieval of\ndata and information. The absence of a computer system and the complexity of\nthe transactions of the college which makes the personnel be loaded with paper\nworks in storing and keeping student records and information is the motivating\nfactor why the School Management Information System has been designed and\ndeveloped for a community college in the northern part of Mindanao. This paper\ndiscusses the Major Functionalities and Modules of the system through its\nimplementation methodology which is the Agile Model and its impact on the\ndelivery of services and procedures in the overall operation of the college.\nThe project has been evaluated based on ISO 25010, a quality model used for\nproduct\/software quality evaluation systems. Based on the results of the\nevaluation, SMIS has been Functional, Usable, and Reliable with an average for\nevery criterion above 4.04 indicating very good performance based on a Likert\nscale descriptive interpretation. Based on the preceding findings of the study,\nthe respondents agreed that the developed e-school system was functional and\nlifted the transaction process of the school. The overall quality and\nperformance of the system was very good in terms of functionality, usability,\nand reliability. It is recommended that future development such as the\nsmartphone and tablet-based attendance monitoring should be integrated, a kiosk\nfor grades and schedule viewing should also be placed inside the campus that is\nconnected to the database server.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.06974,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Algorithmic Auditing and Social Justice: Lessons from the History of\n  Audit Studies\n\n  Algorithmic audits have been embraced as tools to investigate the functioning\nand consequences of sociotechnical systems. Though the term is used somewhat\nloosely in the algorithmic context and encompasses a variety of methods, it\nmaintains a close connection to audit studies in the social sciences--which\nhave, for decades, used experimental methods to measure the prevalence of\ndiscrimination across domains like housing and employment. In the social\nsciences, audit studies originated in a strong tradition of social justice and\nparticipatory action, often involving collaboration between researchers and\ncommunities; but scholars have argued that, over time, social science audits\nhave become somewhat distanced from these original goals and priorities. We\ndraw from this history in order to highlight difficult tensions that have\nshaped the development of social science audits, and to assess their\nimplications in the context of algorithmic auditing. In doing so, we put forth\nconsiderations to assist in the development of robust and engaged assessments\nof sociotechnical systems that draw from auditing's roots in racial equity and\nsocial justice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07911,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"AI, orthogonality and the M\\\"uller-Cannon instrumental vs general\n  intelligence distinction\n\n  The by now standard argument put forth by Yudkowsky, Bostrom and others for\nwhy the possibility of a carelessly handled AI breakthrough poses an\nexistential threat to humanity is shown through careful conceptual analysis to\nbe very much alive and kicking, despite the suggestion in a recent paper by\nM\\\"uller and Cannon that the argument contains a flaw.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.12955,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000035763,
      "text":"The Forgotten Preconditions for a Well-Functioning Internet\n\n  For decades, proponents of the Internet have promised that it would one day\nprovide a seamless way for everyone in the world to communicate with each\nother, without introducing new boundaries, gatekeepers, or power structures.\nWhat happened? This article explores the system-level characteristics of a key\ndesign feature of the Internet that helped it to achieve widespread adoption,\nas well as the system-level implications of certain patterns of use that have\nemerged over the years as a result of that feature. Such patterns include the\nsystem-level acceptance of particular authorities, mechanisms that promote and\nenforce the concentration of power, and network effects that implicitly\npenalise those who do not comply with decisions taken by privileged actors. We\nprovide examples of these patterns and offer some key observations, toward the\ndevelopment of a general theory of why they emerged despite our best efforts,\nand we conclude with some suggestions on how we might mitigate the worst\noutcomes and avoid similar experiences in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.06309,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000033776,
      "text":"Fairness and Data Protection Impact Assessments\n\n  In this paper, we critically examine the effectiveness of the requirement to\nconduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General\nData Protection Regulation (GDPR) in light of fairness metrics. Through this\nanalysis, we explore the role of the fairness principle as introduced in\nArticle 5(1)(a) and its multifaceted interpretation in the obligation to\nconduct a DPIA. Our paper argues that although there is a significant\ntheoretical role for the considerations of fairness in the DPIA process, an\nanalysis of the various guidance documents issued by data protection\nauthorities on the obligation to conduct a DPIA reveals that they rarely\nmention the fairness principle in practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00835,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"WikiCheck: An end-to-end open source Automatic Fact-Checking API based\n  on Wikipedia\n\n  With the growth of fake news and disinformation, the NLP community has been\nworking to assist humans in fact-checking. However, most academic research has\nfocused on model accuracy without paying attention to resource efficiency,\nwhich is crucial in real-life scenarios. In this work, we review the\nState-of-the-Art datasets and solutions for Automatic Fact-checking and test\ntheir applicability in production environments. We discover overfitting issues\nin those models, and we propose a data filtering method that improves the\nmodel's performance and generalization. Then, we design an unsupervised\nfine-tuning of the Masked Language models to improve its accuracy working with\nWikipedia. We also propose a novel query enhancing method to improve evidence\ndiscovery using the Wikipedia Search API. Finally, we present a new\nfact-checking system, the \\textit{WikiCheck} API that automatically performs a\nfacts validation process based on the Wikipedia knowledge base. It is\ncomparable to SOTA solutions in terms of accuracy and can be used on low-memory\nCPU instances.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.08567,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000241068,
      "text":"Detecting directional forces in the evolution of grammar: A case study\n  of the English perfect with intransitives across EEBO, COHA, and Google Books\n\n  Languages have diverse characteristics that have emerged through evolution.\nIn modern English grammar, the perfect is formed with \\textit{have}+PP (past\nparticiple), but in earlier English the \\textit{be}+PP form also existed. It is\nwidely recognised that the auxiliary verb BE was replaced by HAVE throughout\nevolution, except for some special cases. However, whether this evolution was\ncaused by natural selection or random drift is still unclear. Here we examined\ndirectional forces in the evolution of the English perfect with intransitives\nby combining three large-scale data sources: EEBO (Early English Books Online),\nCOHA (Corpus of Historical American English), and Google Books. We found that\nmost intransitive verbs exhibited an apparent transition from \\textit{be}+PP to\n\\textit{have}+PP, most of which were classified as `selection' by a deep neural\nnetwork-based model. These results suggest that the English perfect could have\nevolved through natural selection rather than random drift, and provide\ninsights into the cultural evolution of grammar.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09238,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Foundations for the Future: Institution building for the purpose of\n  Artificial Intelligence governance\n\n  Governance efforts for artificial intelligence (AI) are taking on\nincreasingly more concrete forms, drawing on a variety of approaches and\ninstruments from hard regulation to standardisation efforts, aimed at\nmitigating challenges from high-risk AI systems. To implement these and other\nefforts, new institutions will need to be established on a national and\ninternational level. This paper sketches a blueprint of such institutions, and\nconducts in-depth investigations of three key components of any future AI\ngovernance institutions, exploring benefits and associated drawbacks: (1)\npurpose, relating to the institution's overall goals and scope of work or\nmandate; (2) geography, relating to questions of participation and the reach of\njurisdiction; and (3) capacity, the infrastructural and human make-up of the\ninstitution. Subsequently, the paper highlights noteworthy aspects of various\ninstitutional roles specifically around questions of institutional purpose, and\nframes what these could look like in practice, by placing these debates in a\nEuropean context and proposing different iterations of a European AI Agency.\nFinally, conclusions and future research directions are proposed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01226,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Role of Artificial Intelligence, Clinicians & Policymakers in Clinical\n  Decision Making: A Systems Viewpoint\n\n  What is a system? Is one of those questions that is yet not clear to most\nindividuals in this world. A system is an assemblage of interacting,\ninterrelated and interdependent components forming a complex and integrated\nwhole with an unambiguous and common goal. This paper emphasizes on the fact\nthat all components of a complex system are inter-related and interdependent in\nsome way and the behavior of that system depends on these independences. A\nhealth care system as portrayed in this article is widespread and complex. This\nencompasses not only hospitals but also governing bodies like the FDA,\ntechnologies such as AI, biomedical devices, Cloud computing and many more. The\ninteractions between all these components govern the behavior and existence of\nthe overall healthcare system. In this paper, we focus on the interaction of\nartificial intelligence, care providers and policymakers and analyze using\nsystems thinking approach, their impact on clinical decision making\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10566,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Exploring the Relationship Between \"Positive Risk Balance\" and \"Absence\n  of Unreasonable Risk\"\n\n  International discussions on the overarching topic of how to define and\nquantify what a \"safe enough\" Automated Driving System (ADS) is are currently\nhinged on the question of determining the relationship between \"positive risk\nbalance\" (PRB) and \"absence of unreasonable risk\" (AUR). In order to advance\nthe conversation on these important safety topics at the international level,\nit is first important to start from a shared common understanding, grounded in\nclear definitions and terminology. To that end, this paper will start with an\noverview of the notions of PRB and AUR; it will then summarize different\npositions of the present debate; finally, it will conclude that two possible\ninterpretations exist for PRB, and that failure to distinguish them can lead to\nmisunderstanding different parties' positions. The argumentation in this paper\nis aimed at showing that the two interpretations for PRB can actually\ncomplement each other, but can be considered independently, and can both be\nsubsumed within non-prescriptive guidelines toward ADS safety assurance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10836,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Application of E-Commerce Technologies in accelerating the Success of\n  SME Operation\n\n  Electronic commerce (e-Commerce) technologies have been increased over the\npast two decades in different business sectors. In particular, the technologies\nof B2C operations have significantly improved the productivity of online small\nbusinesses such as SMEs. Systematic literature review in this domain\ncategorized different benefits but a limited number of studies on SME success\nfrom a view of an information systems (IS) research exists, which needs to be\ntaken for further attention. Informing through a comprehensive analysis this\nstudy introduces a conceptual framework of the application of e-Commerce\ntechnologies in accelerating the SME operation. Content analysis methodology\nwas adopted for generating the outcome associated with the success of the\ntechnologies in SMEs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11037,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000097023,
      "text":"\"Computer Says No\": Algorithmic Decision Support and Organisational\n  Responsibility\n\n  Algorithmic decision support is increasingly used in a whole array of\ndifferent contexts and structures in various areas of society, influencing many\npeople's lives. Its use raises questions, among others, about accountability,\ntransparency and responsibility. While there is substantial research on the\nissue of algorithmic systems and responsibility in general, there is little to\nno prior research on organisational responsibility and its attribution. Our\narticle aims to fill that gap; we give a brief overview of the central issues\nconnected to ADS, responsibility and decision-making in organisational contexts\nand identify open questions and research gaps. Furthermore, we describe a set\nof guidelines and a complementary digital tool to assist practitioners in\nmapping responsibility when introducing ADS within their organisational\ncontext.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.06487,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Recent trends in Social Engineering Scams and Case study of Gift Card\n  Scam\n\n  Social engineering scams (SES) has been existed since the adoption of the\ntelecommunications by humankind. An earlier version of the scams include\nleveraging premium phone service to charge the consumers and service providers\nbut not limited to. There are variety of techniques being considered to scam\nthe people due to the advancements in digital data access capabilities and\nInternet technology. A lot of research has been done to identify the scammer\nmethodologies and characteristics of the scams. However, the scammers finding\nnew ways to lure the consumers and stealing their financial assets. An example\nwould be a recent circumstance of Covid-19 unemployment, which was used as a\nweapon to scam the US citizens. These scams will not be stopping here, and will\nkeep appearing with new social engineering strategies in the near future. So,\nto better prepare these kind of scams in ever-changing world, we describe the\nrecent trends of various social engineering scams targeting the innocent people\nall over the world, who oversight the consequences of scams,and also give\ndetailed description of recent social engineering scams including Covid scams.\nThe social engineering scan threat model architecture is also proposed to map\nvarious scams. In addition, we discuss the case study of real-time gift card\nscam targeting various enterprise organization customers to steal their money\nand put the organization reputation in stake. We also provide recommendations\nto internet users for not falling a victim of social engineering scams. In the\nend, we provide insights on how to prepare\/respond to the social engineering\nscams by following the security incident detection and response life cycle in\nenterprises\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.06094,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Increasing Gender Balance Across Academic Staffing in Computer Science\n  -- case study\n\n  As at 2019, Technological University Dublin* Computer Science is the top\nuniversity in Ireland in terms of gender balance of female academic staff in\ncomputer science schools. In an academic team of approximately 55 full-time\nequivalents, 36% of our academic staff are female, 50% of our senior academic\nleadership team (2 of 4) are female and 75% of our School Executive are female\n(3 of 4), including a female Head of School. This is as a result of our seven\nyear SUCCESS programme which had a four strand approach: Source, Career,\nEnvironment and Support. The Source strand explicitly encouraged females to\napply for each recruitment drive; Career focused on female career and skills\ndevelopment initiatives; Environment created a female-friendly culture and\nreputation, both within the School, across our organisation and across the\nthird level sector in Ireland and Support addressed practical supports for the\nspecific difficulties experienced by female staff. As a result we have had 0%\nturnover in female staff in the past five years (in contrast to 10% male staff\nturnover). We will continue to work across these four strands to preserve our\npipeline of female staff and ensure their success over the coming years in an\nacademic and ICT sector that remains challenging for females.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.12777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"An approach to optimize study programs using Discrete Event Simulation\n\n  Creating a study program for optimal academic completion is a complex\nassignment. Especially programs in the science, technology, engineering, and\nmathematics field are known for extended completion time as well as high\ndrop-out rates throughout the years. Drop-outs are caused by various reasons\nand can not be directly generalized. This leads to unnecessary costs for the\nstudents and the university. Reasons for dropping out of university could be\nstudents failing classes too often or poorly designed study programs causing a\nloss of interest in a subject. Besides dropping out of university, students are\noften having trouble with specific classes. This results in postponing certain\nclasses, which causes a bottleneck in the overall progression and delays\ngraduation. To achieve a better understanding of a general student's\nprogression as well as finding mentioned bottlenecks in an average academic\nprogression a discrete event simulation was created. The insights gained by the\nsimulation shall furthermore be used by Technische Hochschule K\\\"oln to analyze\nmentioned problems, find solutions and create a healthier environment for\noptimal academic progression.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.0454,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000078479,
      "text":"Emergent Insight of the Cyber Security Management for Saudi Arabian\n  Universities: A Content Analysis\n\n  While cyber security has become a prominent concept of emerging information\ngovernance, the Kingdom of Saudi Arabia has been dealing with severe threats to\nindividual and organizational IT systems for a long time. These risks have\nrecently permeated into educational institutions, thereby undermining the\nconfidentiality of information as well as the delivery of education. Recent\nresearch has identified various causes and possible solutions to the problem.\nHowever, most scholars have considered a reductionist approach, in which the\nability of computer configurations to prevent unwanted intrusions is evaluated\nby breaking them down to their constituent parts. This method is inadequate at\nstudying complex adaptive systems. Therefore, the proposed project is designed\nto utilize a holistic stance to assess the cybersecurity management and\npolicies in Saudi Arabian universities. Qualitative research, entailing a\nthorough critical review of ten public universities, will be utilized to\ninvestigate the subject matter. The subsequent recommendations can be adopted\nto enhance the security of IT systems, not only in institutional settings but\nalso in any other environment in which such structures are used.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.04539,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"A New Innovation Concept on End user Contextual and Behavioural\n  Perspectives\n\n  The phenomenon of innovation has been shifting away from focusing on tangible\nto intangible modernization with its vitalizing context. This shift appears\nvitally in innovation developed by individual end-users in organizations and\nsocieties, including the exploration of the intangible end-user innovation\nexistence and impact in the household sector on a national scale. Some examples\nof intangible end-user innovation include technique, service, and user\nbehavior. Although, there is a variety of intangible end-user innovation\ndiscussed in the literature, limited understanding is existed for constructing\nan efficient and comprehensive typology, which encompasses the nature of this\ninnovation phenomenon. This research study explores this original phenomenon\nfor proposing a new concept that will act as an overarching descriptor of\ninnovation types: idea, object, and behavior. This proposed concept, relating\nto intangible innovation, will explain the sequence within one or many\nconnected intangible activities that provide novelty to its end-user relative\nto previous activities and practices. Using a design science research approach,\nthe study comprises two goals: a) identifying opportunities and issues to\nmeasure intangible inputs to the innovation and b) proposing a framework for\nextending the existing innovation theories that to better capture intangible\nend-user innovation and its diffusion insights in their online environment\nacross nations\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00308,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Mobile Technologies in Education\n\n  The growth of smartphone users globally is a factor that educational\ntechnologists should not ignore. This ever-growing market will eventually lead\nto ubiquitous learning (u-learning). The development of specific content should\nbe replaced by the design of content in languages that are compatible with\nscalable technologies and that will reach the hands of students in the future\nin an attractive way.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04453,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"An awareness-based model to minimize the environmental damage of the\n  internet usage: A Longitudinal Study\n\n  The record-breaking increase of internet usage in 2020 with the spread of the\nCOVID-19 pandemic has made us think about the alarming consequences of it in\nthe aspect of climate change. As countries go into lockdown the use of the\ninternet to perform tasks remotely has increased in record numbers. As per the\ntrend and at times addictive nature of its usage, it is unlikely that the usage\nof the internet will decrease in the future reducing its current contribution\nto climate change. Considering the sustainability perspective, this study\ninvestigates whether the pervasive nature of internet usage could be reduced by\nsimply inducing awareness. A population-based survey experiment comprising of\n326 respondents was employed to investigate if awareness alone could reduce\nindividual internet usage.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.0737,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Ethics lines and Machine learning: a design and simulation of an\n  Association Rules Algorithm for exploiting the data\n\n  Data mining techniques offer great opportunities for developing ethics lines,\ntools for communication, participation and innovation whose main aim is to\nensure improvements and compliance with the values, conduct and commitments\nmaking up the code of ethics. The aim of this study is to suggest a process for\nexploiting the data generated by the data generated and collected from an\nethics line by extracting rules of association and applying the Apriori\nalgorithm. This makes it possible to identify anomalies and behaviour patterns\nrequiring action to review, correct, promote or expand them, as appropriate.\nFinally, I offer a simulated application of the Apriori algorithm, supplying it\nwith synthetic data to find out its potential, strengths and limitations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.14419,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Toward a Theory of Justice for Artificial Intelligence\n\n  This paper explores the relationship between artificial intelligence and\nprinciples of distributive justice. Drawing upon the political philosophy of\nJohn Rawls, it holds that the basic structure of society should be understood\nas a composite of socio-technical systems, and that the operation of these\nsystems is increasingly shaped and influenced by AI. As a consequence,\negalitarian norms of justice apply to the technology when it is deployed in\nthese contexts. These norms entail that the relevant AI systems must meet a\ncertain standard of public justification, support citizens rights, and promote\nsubstantively fair outcomes -- something that requires specific attention be\npaid to the impact they have on the worst-off members of society.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09313,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Impact of review valence and perceived uncertainty on purchase of\n  time-constrained and discounted search goods\n\n  Increasing online shoppers have generated enormous amount of data in form of\nreviews (text) and sales data. Aggregate reviews in form of rating (stars) have\nbecome noticeable indicators of product quality and vendor performance to\nprospective consumers at first sight. Consumers subjected to product discount\ndeadlines search for ways in which they could evaluate product and vendor\nservice using a comprehensible benchmark. Considering the effect of time\npressure on consumers, aggregate reviews, known as review valence, become a\nviable indicator of product quality. This study investigates how purchase\ndecisions for new products are affected by past customer aggregate ratings when\na soon-to-expire discount is being offered. We examine the role that a\nconsumer's attitude towards review valence (RV) plays as an antecedent to that\nconsumer's reliance on RV in a purchase decision for time-discounted search\ngoods. Considering review credibility, diagnosticity, and effectiveness as\ndeterminants of consumer attitude in a time-constrained search and purchase\nenvironment, we follow the approach-avoidance conflict theory to examine the\nrole of review valence and perceived uncertainty in a time-constrained\nenvironment. The data was collected through an online survey and analyzed using\nstructural equation modelling. This study provides significant implications for\npractitioners as they can better understand how review valence can influence a\npurchase decision. Empirical analysis includes two contributions: 1. It helps\nto understand how consumer attitude toward review valence, when positively\ninfluenced by the determinants, can lead to reliance on review valence, further\ninfluencing purchase decision; 2. Time constrained purchase-related perceived\nuncertainty negatively moderates the relationship between consumer attitude and\nreliance on review valence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04443,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000077155,
      "text":"A strategy to identify event specific hospitalizations in large health\n  claims database\n\n  Health insurance claims data offer a unique opportunity to study disease\ndistribution on a large scale. Challenges arise in the process of accurately\nanalyzing these raw data. One important challenge to overcome is the accurate\nclassification of study outcomes. For example, using claims data, there is no\nclear way of classifying hospitalizations due to a specific event. This is\nbecause of the inherent disjointedness and lack of context that typically come\nwith raw claims data. In this paper, we propose a framework for classifying\nhospitalizations due to a specific event. We then test this framework in a\nhealth insurance claims database with approximately 4 million US adults who\ntested positive with COVID-19 between March and December 2020. Our claims\nspecific COVID-19 related hospitalizations proportion is then compared to\nnationally reported rates from the Centers for Disease Control by age and sex.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04446,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"An Investigative Model of Adult Cyberbullying: A Court Case Analysis\n\n  Cyberbullying is a major social issue that is on the rise with a substantial\npotential to impact a large number of Internet users globally. The growth and\nrapid proliferation of the Internet and other ubiquitous technologies like\nsocial media and smart mobile devices have increased the propensity of\ncyberbullying, providing it with a wider audience and rapid access. This\nresearch developed an investigative model for cyberbullying, specifically\ndeveloped for adults. Therein, the model considers the cyberbullying journey\nfrom conception of the bullying idea, identification of the target to the\nbullying as an action. The a-priori model is motivated by the General Theory of\nCrime and the Routine Activity Theory. The a-priori model is then validated\nusing 20 cyberbullying court cases from Australia, Canada, the United States\nand Scotland.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04452,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000011358,
      "text":"A Critical Assessment of Online Vs Traditional Review Characteristics\n\n  With the expansion of internet-based platforms, social media and sharing\neconomy, most individuals are tempted to review products or services that they\nconsume.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.09284,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Towards a Systematic Survey for Carbon Neutral Data Centers\n\n  Data centers are carbon-intensive enterprises due to their massive energy\nconsumption, and it is estimated that data center industry will account for 8\\%\nof global carbon emissions by 2030. However, both technological and policy\ninstruments for reducing or even neutralizing data center carbon emissions have\nnot been thoroughly investigated. To bridge this gap, this survey paper\nproposes a roadmap towards carbon-neutral data centers that takes into account\nboth policy instruments and technological methodologies. We begin by presenting\nthe carbon footprint of data centers, as well as some insights into the major\nsources of carbon emissions. Following that, carbon neutrality plans for major\nglobal cloud providers are discussed to summarize current industrial efforts in\nthis direction. In what follows, we introduce the carbon market as a policy\ninstrument to explain how to offset data center carbon emissions in a\ncost-efficient manner. On the technological front, we propose achieving\ncarbon-neutral data centers by increasing renewable energy penetration,\nimproving energy efficiency, and boosting energy circulation simultaneously. A\ncomprehensive review of existing technologies on these three topics is\nelaborated subsequently. Based on this, a multi-pronged approach towards carbon\nneutrality is envisioned and a digital twin-powered industrial artificial\nintelligence (AI) framework is proposed to make this solution a reality.\nFurthermore, three key scientific challenges for putting such a framework in\nplace are discussed. Finally, several applications for this framework are\npresented to demonstrate its enormous potential.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05142,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"The Second-Level Smartphone Divide: A Typology of Smartphone Usage Based\n  on Frequency of Use, Skills, and Types of Activities\n\n  This paper examines inequalities in the usage of smartphone technology based\non five samples of smartphone owners collected in Germany and Austria between\n2016 and 2020. We identify six distinct types of smartphone users by conducting\nlatent class analyses that classify individuals based on their frequency of\nsmartphone use, self-rated smartphone skills, and activities carried out on\ntheir smartphone. The results show that the smartphone usage types differ\nsignificantly by sociodemographic and smartphone-related characteristics: The\ntypes reflecting more frequent and diverse smartphone use are younger, have\nhigher levels of educational attainment, and are more likely to use an iPhone.\nOverall, the composition of the latent classes and their characteristics are\nrobust across samples and time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04887,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Programming for All: Understanding the Nature of Programs\n\n  Computer programs are part of our daily life, we use them, we provide them\nwith data, they support our decisions, they help us remember, they control\nmachines, etc. Programs are made by people, but in most cases we are not their\nauthors, so we have to decide if we can trust them. Programs enable computers\nand computer-controlled machines to behave in a large variety of ways. They\nbring the intrinsic power of computers to life. Programs have a variety of\nproperties that all citizens must be aware of. Due to the intangible nature of\nprograms, most of these properties are very unusual, but important to\nunderstand the digital world. In this position paper, we describe the Nature of\nPrograms in the form of knowledge statements, accompanied by examples from\neveryday life to clarify their meaning. Everything is formulated in an easily\nunderstandable manner and avoids obscure technical language. We suggest that\nthese knowledge statements must be imparted to all teachers and school\nstudents.\n  A great way to learn and experience the nature of programs is to develop\nprograms yourself.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01251,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Health Detection on Cattle Compressed Images in Precision Livestock\n  Farming\n\n  The constant population growth brings the needing to make up for food also\ngrows at the same rate. The livestock provides one-third of humans protein base\nas meat and milk. To improve cattles health and welfare the pastoral farming\nemploys Precision Livestock farming (PLF). This technique implementation brings\na challenge to minimize energy consumption due to farmers not having enough\nenergy or devices to transmit large volumes of information at the size are\nreceived from their farms monitors. Therefore, in this project, we will design\nan algorithm to compress and decompress images reducing energy consumption with\nthe less information lost. Initially, the related problems have been read and\nanalyzed to learn about the techniques used in the past and to be updated with\nthe current works. We implemented Seam Carving and LZW algorithms. The\ncompression of all images, around 1000 takes a time of 5 hours 10 min. We got a\ncompression rate of 1.82:1 with 13.75s average time for each file and a\ndecompression rate of 1.64:1 and 7.5 s average time for each file. The memory\nconsumption we obtained was between 146MB and 504 MB and time consumption was\nbetween 30,5s for 90MB to 12192s for 24410 MB, it was all files.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01281,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Expose Uncertainty, Instill Distrust, Avoid Explanations: Towards\n  Ethical Guidelines for AI\n\n  In this position paper, I argue that the best way to help and protect humans\nusing AI technology is to make them aware of the intrinsic limitations and\nproblems of AI algorithms. To accomplish this, I suggest three ethical\nguidelines to be used in the presentation of results, mandating AI systems to\nexpose uncertainty, to instill distrust, and, contrary to traditional views, to\navoid explanations. The paper does a preliminary discussion of the guidelines\nand provides some arguments for their adoption, aiming to start a debate in the\ncommunity about AI ethics in practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01282,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Achieving a Data-driven Risk Assessment Methodology for Ethical AI\n\n  The AI landscape demands a broad set of legal, ethical, and societal\nconsiderations to be accounted for in order to develop ethical AI (eAI)\nsolutions which sustain human values and rights. Currently, a variety of\nguidelines and a handful of niche tools exist to account for and tackle\nindividual challenges. However, it is also well established that many\norganizations face practical challenges in navigating these considerations from\na risk management perspective. Therefore, new methodologies are needed to\nprovide a well-vetted and real-world applicable structure and path through the\nchecks and balances needed for ethically assessing and guiding the development\nof AI. In this paper we show that a multidisciplinary research approach,\nspanning cross-sectional viewpoints, is the foundation of a pragmatic\ndefinition of ethical and societal risks faced by organizations using AI.\nEqually important is the findings of cross-structural governance for\nimplementing eAI successfully. Based on evidence acquired from our\nmultidisciplinary research investigation, we propose a novel data-driven risk\nassessment methodology, entitled DRESS-eAI. In addition, through the evaluation\nof our methodological implementation, we demonstrate its state-of-the-art\nrelevance as a tool for sustaining human values in the data-driven AI era.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.06116,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Implementation of Ethically Aligned Design with Ethical User stories in\n  SMART terminal Digitalization project: Use case Passenger Flow\n\n  Digitalization and Smart systems are part of our everyday lives today. So far\nthe development has been rapid and all the implications that comes after the\ndeployment has not been able to foresee or even assess during the development,\nespecially when ethics or trustworthiness is concerned. Artificial Intelligence\n(AI) and Autonomous Systems (AS) are the direction that software systems are\ntaking today. It is witnessed in banks, stores, internet and it is proceeding\nto transportation as well as on traveling. Autonomous maritime industry has\nalso taking this direction when taking under development in digitalization on\nfairway and port terminals. AI ethics has advanced profoundly since the machine\nlearning develop during the last decade and is now being implemented in AI\ndevelopment and workflow of software engineers. It is not an easy task and\ntools are needed to make the ethical assessment easier. This paper will review\na research in an industrial setting, where Ethically Aligned Design practice,\nEthical User Stories are used to transfer ethical requirements to ethical user\nstories to form practical solutions for project use. This project is in the\nfield of maritime industry and concentrates on digitalization of port terminals\nand this particular paper focuses on the passenger flow. Results are positive\ntowards the practice of Ethical User Stories, drawn from a large empirical data\nset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"(SARS-CoV-2) COVID 19: Genomic surveillance and evaluation of the impact\n  on the population speaker of indigenous language in Mexico\n\n  The importance of the working document is that it allows the analysis of\ninformation and cases associated with (SARS-CoV-2) COVID-19, based on the daily\ninformation generated by the Government of Mexico through the Secretariat of\nHealth, responsible for the Epidemiological Surveillance System for Viral\nRespiratory Diseases (SVEERV). The information in the SVEERV is disseminated as\nopen data, and the level of information is displayed at the municipal, state\nand national levels. On the other hand, the monitoring of the genomic\nsurveillance of (SARS-CoV-2) COVID-19, through the identification of variants\nand mutations, is registered in the database of the Information System of the\nGlobal Initiative on Sharing All Influenza Data (GISAID) based in Germany.\nThese two sources of information SVEERV and GISAID provide the information for\nthe analysis of the impact of (SARS-CoV-2) COVID-19 on the population in\nMexico. The first data source identifies information, at the national level, on\npatients according to age, sex, comorbidities and COVID-19 presence\n(SARS-CoV-2), among other characteristics. The data analysis is carried out by\nmeans of the design of an algorithm applying data mining techniques and\nmethodology, to estimate the case fatality rate, positivity index and identify\na typology according to the severity of the infection identified in patients\nwho present a positive result. for (SARS-CoV-2) COVID-19. From the second data\nsource, information is obtained worldwide on the new variants and mutations of\nCOVID-19 (SARS-CoV-2), providing valuable information for timely genomic\nsurveillance. This study analyzes the impact of (SARS-CoV-2) COVID-19 on the\nindigenous language-speaking population, it allows us to provide information,\nquickly and in a timely manner, to support the design of public policy on\nhealth.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13272,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Intelligent edge-based recommender system for internet of energy\n  applications\n\n  Preserving energy in households and office buildings is a significant\nchallenge, mainly due to the recent shortage of energy resources, the uprising\nof the current environmental problems, and the global lack of utilizing\nenergy-saving technologies. Not to mention, within some regions, COVID-19\nsocial distancing measures have led to a temporary transfer of energy demand\nfrom commercial and urban centers to residential areas, causing an increased\nuse and higher charges, and in turn, creating economic impacts on customers.\nTherefore, the marketplace could benefit from developing an internet of things\n(IoT) ecosystem that monitors energy consumption habits and promptly recommends\naction to facilitate energy efficiency. This paper aims to present the full\nintegration of a proposed energy efficiency framework into the Home-Assistant\nplatform using an edge-based architecture. End-users can visualize their\nconsumption patterns as well as ambient environmental data using the\nHome-Assistant user interface. More notably, explainable energy-saving\nrecommendations are delivered to end-users in the form of notifications via the\nmobile application to facilitate habit change. In this context, to the best of\nthe authors' knowledge, this is the first attempt to develop and implement an\nenergy-saving recommender system on edge devices. Thus, ensuring better privacy\npreservation since data are processed locally on the edge, without the need to\ntransmit them to remote servers, as is the case with cloudlet platforms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05237,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000292063,
      "text":"Psycho-linguistic differences among competing vaccination communities on\n  social media\n\n  Currently, the significance of social media in disseminating noteworthy\ninformation on topics such as health, politics, and the economy is\nindisputable. During the COVID-19 pandemic, anti-vaxxers use social media to\ndistribute fake news and anxiety-provoking information about the vaccine, which\nmay harm the public. Here, we characterize the psycho-linguistic features of\nanti-vaxxers on the online social network Twitter. For this, we collected\nCOVID-19 related tweets from February 2020 to June 2021 to analyse vaccination\nstance, linguistic features, and social network characteristics. Our results\ndemonstrated that, compared to pro-vaxxers, anti-vaxxers tend to have more\nnegative emotions, narrative thinking, and worse moral tendencies. This study\ncan advance our understanding of the online anti-vaccination movement, and\nbecome critical for social media management and policy action during and after\nthe pandemic.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.06562,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Satellite Images and Deep Learning to Identify Discrepancy in Mailing\n  Addresses with Applications to Census 2020 in Houston\n\n  The accuracy and completeness of population estimation would significantly\nimpact the allocation of public resources. However, the current census paradigm\nexperiences a non-negligible level of under-counting. Existing solutions to\nthis problem by the Census Bureau is to increase canvassing efforts, which\nleads to expensive and inefficient usage of human resources. In this work, we\nargue that the existence of hidden multi-family households is a significant\ncause of under-counting. Accordingly, we introduce a low-cost but high-accuracy\nmethod that combines satellite imagery and deep learning technologies to\nidentify hidden multi-family (HMF) households. With comprehensive knowledge of\nthe HMF households, the efficiency and effectiveness of the decennial census\ncould be vastly improved. An extensive experiment demonstrates that our\napproach can discover over 1800 undetected HMF in a single zipcode of the\nHouston area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09673,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0002576245,
      "text":"A Survey on Metaverse: the State-of-the-art, Technologies, Applications,\n  and Challenges\n\n  Metaverse is a new type of Internet application and social form that\nintegrates a variety of new technologies. It has the characteristics of\nmulti-technology, sociality, and hyper spatiotemporality. This paper introduces\nthe development status of Metaverse, from the five perspectives of network\ninfrastructure, management technology, basic common technology, virtual reality\nobject connection, and virtual reality convergence, it introduces the technical\nframework of Metaverse. This paper also introduces the nature of Metaverse's\nsocial and hyper spatiotemporality, and discusses the first application areas\nof Metaverse and some of the problems and challenges it may face.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08791,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000063909,
      "text":"PROVENANCE: An Intermediary-Free Solution for Digital Content\n  Verification\n\n  The threat posed by misinformation and disinformation is one of the defining\nchallenges of the 21st century. Provenance is designed to help combat this\nthreat by warning users when the content they are looking at may be\nmisinformation or disinformation. It is also designed to improve media literacy\namong its users and ultimately reduce susceptibility to the threat among\nvulnerable groups within society. The Provenance browser plugin checks the\ncontent that users see on the Internet and social media and provides warnings\nin their browser or social media feed. Unlike similar plugins, which require\nhuman experts to provide evaluations and can only provide simple binary\nwarnings, Provenance's state of the art technology does not require human input\nand it analyses seven aspects of the content users see and provides warnings\nwhere necessary.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0438,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Ethics-Based Auditing of Automated Decision-Making Systems: Intervention\n  Points and Policy Implications\n\n  Organisations increasingly use automated decision-making systems (ADMS) to\ninform decisions that affect humans and their environment. While the use of\nADMS can improve the accuracy and efficiency of decision-making processes, it\nis also coupled with ethical challenges. Unfortunately, the governance\nmechanisms currently used to oversee human decision-making often fail when\napplied to ADMS. In previous work, we proposed that ethics-based auditing\n(EBA), i.e. a structured process by which ADMS are assessed for consistency\nwith relevant principles or norms, can (a) help organisations verify claims\nabout their ADMS and (b) provide decision-subjects with justifications for the\noutputs produced by ADMS. In this article, we outline the conditions under\nwhich EBA procedures can be feasible and effective in practice. First, we argue\nthat EBA is best understood as a 'soft' yet 'formal' governance mechanism. This\nimplies that the main responsibility of auditors should be to spark ethical\ndeliberation at key intervention points throughout the software development\nprocess and ensure that there is sufficient documentation to respond to\npotential inquiries. Second, we frame ADMS as parts of larger socio-technical\nsystems to demonstrate that to be feasible and effective, EBA procedures must\nlink to intervention points that span all levels of organisational governance\nand all phases of the software lifecycle. The main function of EBA should\ntherefore be to inform, formalise, assess, and interlink existing governance\nstructures. Finally, we discuss the policy implications of our findings. To\nsupport the emergence of feasible and effective EBA procedures, policymakers\nand regulators could provide standardised reporting formats, facilitate\nknowledge exchange, provide guidance on how to resolve normative tensions, and\ncreate an independent body to oversee EBA of ADMS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.1359,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Academic Lobification: Low-performance Control Strategy for Long-planed\n  Academic Purpose\n\n  Academic lobification refers to a collection of academic performance control\nstrategies, methods, and means that a student deliberately hides academic\nbehaviors, or deliberately lowers academic performance, or deliberately delays\nacademic returns for a certain long-term purpose, but does not produce academic\nrisks. Understanding academic lobification is essential to our ability to\ncompensate for inherent deviations in the evaluation of students' academic\nperformance, discover gifted student, reap benefits and minimize harms. It\noutlines a set of questions that are fundamental to this emerging\ninterdisciplinary research field, including research object, research question,\nresearch scope, research method, and explores the technical, legal and other\nconstraints on the study of academic lobification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.11881,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"TecCoBot: Technology-aided support for self-regulated learning\n\n  In addition to formal learning at universities, like in lecture halls and\nseminar rooms, students are regularly confronted with self-study activities.\nInstead of being left to their own devices, students might benefit from a\nproper design of such activities, including pedagogical interventions. Such\ndesigns can increase the degree of activity and the contribution of self-study\nactivities to the achievement of learning outcomes.\n  Especially in times of a global pandemic, self-study activities are\nincreasingly executed at home, where students already use technology-enhanced\nmaterials, processes, and digital platforms. Thus we pick up these building\nblocks and introduce TecCoBot within this paper. TecCoBot is not only a\nchatbot, supporting students in reading texts by offering writing assignments\nand providing automated feedback on these, but also implements a design for\nself-study activities, typically only offered to a few students as face-to-face\nmentoring.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.15661,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Open Data and Quantitative Techniques for Anthropology of Road Traffic\n\n  What kind of questions about human mobility can computational analysis help\nanswer? How to translate the findings into anthropology? We analyzed a publicly\navailable data set of road traffic counters in Slovenia to answer these\nquestions. The data reveals interesting information on how a nation drives, how\nit travels for tourism, which locations it prefers, what it does during the\nweek and the weekend, and how its habits change during the year. We conducted\nthe empirical analysis in two parts. First, we defined interesting traffic\nspots and designed computational methods to find them in a large data set. As\nshown in the paper, traffic counters hint at potential causes and effects in\ndriving practices that we can interpret anthropologically. Second, we used\nclustering to find groups of similar traffic counters as described by their\ndaily profiles. Clustering revealed the main features of road traffic in\nSlovenia. Using the two quantitative approaches, we outline the general\nproperties of road traffic in the country and identify and explain interesting\noutliers. We show that quantitative data analysis only partially answers\nanthropological questions, but it can be a valuable tool for preliminary\nresearch. We conclude that open data are a useful component in an\nanthropological analysis and that quantitative discovery of small local events\ncan help us pinpoint future fieldwork sites.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08889,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Measuring Geometric Similarity Across Possible Plans for Automated\n  Redistricting\n\n  Algorithmic and statistical approaches to congressional redistricting are\nbecoming increasingly valuable tools in courts and redistricting commissions\nfor quantifying gerrymandering in the United States. While there is existing\nliterature covering how various Markov chain Monte Carlo distributions differ\nin terms of projected electoral outcomes and geometric quantifiers of\ncompactness, there is still work to be done on measuring similarities between\ndifferent congressional redistricting plans. This paper briefly introduces an\nintuitive and interpretive measure of similarity, and a corresponding\nassignment matrix, that corresponds to the percentage of a state's area or\npopulation that stays in the same congressional district between two plans. We\nthen show how to calculate this measure in polynomial time and briefly\ndemonstrate some potential use-cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.06477,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Accounting for carbon emissions caused by cryptocurrency and token\n  systems\n\n  The energy consumption and related carbon emissions of cryptocurrencies such\nas Bitcoin are subject to extensive discussion in public, academia, and\nindustry. As cryptocurrencies continue their journey into mainstream finance,\nincentives to participate in the networks and consume energy to do so remain\nsignificant. First guidance on how to allocate the carbon footprint of the\nBitcoin network to single investors exist, however a holistic framework\ncapturing a wider range of cryptocurrencies and tokens remains absent. This\nwhite paper explores different approaches of how to allocate emissions caused\nby cryptocurrencies and tokens. Based on our analysis of the strengths and\nlimitations of potential approaches, we propose a framework that combines key\ndrivers of emissions in Proof of Work and Proof of Stake networks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01275,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000097023,
      "text":"Advancing Artificial Intelligence and Machine Learning in the U.S.\n  Government Through Improved Public Competitions\n\n  In the last two years, the U.S. government has emphasized the importance of\naccelerating artificial intelligence (AI) and machine learning (ML) within the\ngovernment and across the nation. In particular, the National Artificial\nIntelligence Initiative Act of 2020, which became law on January 1, 2021,\nprovides for a coordinated program across the entire federal government to\naccelerate AI research and application. The U.S. government can benefit from\npublic artificial intelligence and machine learning challenges through the\ndevelopment of novel algorithms and participation in experiential training.\nAlthough the public, private, and non-profit sectors have a history of\nleveraging crowdsourcing initiatives to generate novel solutions to difficult\nproblems and engage stakeholders, interest in public competitions has waned in\nrecent years as a result of at least three major factors: (1) a lack of\nhigh-quality, high-impact data; (2) a narrow engagement focus on specialized\ngroups; and (3) insufficient operationalization of challenge results. Herein we\nidentify common issues and recommend approaches to increase the effectiveness\nof challenges. To address these barriers, enabling the use of public\ncompetitions for accelerating AI and ML practice, the U.S. government must\nleverage methods that protect sensitive data while enabling modelling, enable\neasier participation, empower deployment of validated models, and incentivize\nengagement from broad sections of the population.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13041,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Non-Asimov Explanations Regulating AI through Transparency\n\n  An important part of law and regulation is demanding explanations for actual\nand potential failures. We ask questions like: What happened (or might happen)\nto cause this failure? And why did (or might) it happen? These are disguised\nnormative questions - they really ask what ought to have happened, and how the\nhumans involved ought to have behaved. To answer the normative questions, law\nand regulation seeks a narrative explanation, a story. At present, we seek\nthese kinds of narrative explanation from AI technology, because as humans we\nseek to understand technology's working through constructing a story to explain\nit. Our cultural history makes this inevitable - authors like Asimov, writing\nnarratives about future AI technologies like intelligent robots, have told us\nthat they act in ways explainable by the narrative logic which we use to\nexplain human actions and so they can also be explained to us in those terms.\nThis is, at least currently, not true. This work argues that we can only solve\nthis problem by working from both sides. Technologists will need to find ways\nto tell us stories which law and regulation can use. But law and regulation\nwill also need to accept different kinds of narratives, which tell stories\nabout fundamental legal and regulatory concepts like fairness and\nreasonableness that are different from those we are used to.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.121,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Hybrid Human-AI Curriculum Development for Personalised Informal\n  Learning Environments\n\n  Informal learning procedures have been changing extremely fast over the\nrecent decades not only due to the advent of online learning, but also due to\nchanges in what humans need to learn to meet their various life and career\ngoals. Consequently, online, educational platforms are expected to provide\npersonalized, up-to-date curricula to assist learners. Therefore, in this\npaper, we propose an Artificial Intelligence (AI) and Crowdsourcing based\napproach to create and update curricula for individual learners. We show the\ndesign of this curriculum development system prototype, in which contributors\nreceive AI-based recommendations to be able to define and update high-level\nlearning goals, skills, and learning topics together with associated learning\ncontent. This curriculum development system was also integrated into our\npersonalized online learning platform. To evaluate our prototype we compared\nexperts' opinion with our system's recommendations, and resulted in 89%, 79%,\nand 93% F1-scores when recommending skills, learning topics, and educational\nmaterials respectively. Also, we interviewed eight senior level experts from\neducational institutions and career consulting organizations. Interviewees\nagreed that our curriculum development method has high potential to support\nauthoring activities in dynamic, personalized learning environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11198,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Best Privacy Practice Recommendations for Global Audio Streaming\n  Platforms\n\n  Spoon Radio is a rapidly growing global audio streaming platform which\ncurrently operates in South Korea, the United States, Japan as well as the\nMiddle East and North Africa. The platform believes that its commitment to user\nprivacy is an important competitive factor. As such, it aims to not just comply\nwith existing privacy regulations in regions where it operates today but to\nalso ensure that it anticipates likely evolution to these regulations and of\nuser expectations. In doing so, Spoon Radio wants to ensure it is well prepared\nto continue its expansion into new markets. As part of an effort to inform the\nevolution of its data practices, Spoon Radio reached out to the Privacy\nEngineering Program at CMU and sponsored a capstone project in which two\nmaster's students in the Program worked with Spoon Radio personnel over the\ncourse of the 2021 Fall Semester. The present report summarizes best practice\nrecommendations that have emerged from this collaboration. These best practices\nare a combination of practices that are already implemented or in the process\nof being implemented by Spoon Radio today as well as more aspirational\nrecommendations, which are expected to help inform Spoon Radio's practices in\nthe future. In this report, best practice recommendations are organized around\nfour stages of the data life cycle: data collection, data storage, data usage,\nand finally data destruction. A separate section is devoted to content\nmoderation, an area where platforms such as Spoon Radio need to reconcile\nconsiderations such as promoting freedom of expression with the need to create\na safe and respectful environment that complies with applicable laws and\nrespects relevant cultural values.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11117,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"Before and after GDPR: tracking in mobile apps\n\n  Third-party tracking, the collection and sharing of behavioural data about\nindividuals, is a significant and ubiquitous privacy threat in mobile apps. The\nEU General Data Protection Regulation (GDPR) was introduced in 2018 to protect\npersonal data better, but there exists, thus far, limited empirical evidence\nabout its efficacy. This paper studies tracking in nearly two million Android\napps from before and after the introduction of the GDPR. Our analysis suggests\nthat there has been limited change in the presence of third-party tracking in\napps, and that the concentration of tracking capabilities among a few large\ngatekeeper companies persists. However, change might be imminent.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.00301,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000065234,
      "text":"Uncertainty in Criminal Justice Algorithms: simulation studies of the\n  Pennsylvania Additive Classification Tool\n\n  Much attention has been paid to algorithms related to sentencing, the setting\nof bail, parole decisions and recidivism while less attention has been paid to\ncarceral algorithms, those algorithms used to determine an incarcerated\nindividual's lived experience. In this paper we study one such algorithm, the\nPennsylvania Additive Classification Tool (PACT) that assigns custody levels to\nincarcerated individuals. We analyze the PACT in ways that criminal justice\nalgorithms are often analyzed: namely, we train an accurate machine learning\nmodel for the PACT; we study its fairness across sex, age and race; and we\ndetermine which features are most important. In addition to these conventional\ncomputations, we propose and carry out some new ways to study such algorithms.\nInstead of focusing on the outcomes themselves, we propose shifting our\nattention to the variability in the outcomes, especially because many carceral\nalgorithms are used repeatedly and there can be a propagation of uncertainty.\nBy carrying out several simulations of assigning custody levels, we shine light\non problematic aspects of tools like the PACT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.07047,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"SoK: A Framework for Unifying At-Risk User Research\n\nAt-risk users are people who experience elevated digital security, privacy, and safety threats because of what they do, who they are, where they are, or who they are with. In this systematization work, we present a framework for reasoning about at-risk users based on a wide-ranging meta-analysis of 85 papers. Across the varied populations that we examined (e.g., children, activists, women in developing regions), we identified 10 unifying contextual risk factors--such as oppression or stigmatization and access to a sensitive resource--which augment or amplify digital-safety threats and their resulting harms. We also identified technical and non-technical practices that at-risk users adopt to attempt to protect themselves from digital-safety threats. We use this framework to discuss barriers that limit at-risk users' ability or willingness to take protective actions. We believe that the security, privacy, and human-computer interaction research and practitioner communities can use our framework to identify and shape research investments to benefit at-risk users, and to guide technology design to better support at-risk users.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08579,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"The Credibility Cryptocurrency Valuation: Statistical Learning Analysis\n  for Influencer Tweets\n\n  Cryptocurrency has attracted significant attention. Considering the number of\nindividuals investing in bitcoin, their motivations are comparatively less\nclear than traditional investment decisions. As of December 2020, the market\nhas continuously increased in cryptocurrency. Especially, the spike of joke\nDogecoin shows the weirdness of the modern meme economy with the support of\nElon Musk, whom himself appointed as \"Dogefather\". In this paper, we analysis\nthe impact of tweets by Elon musk and present some statistical analyze with\nevent study.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06119,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000083447,
      "text":"Visualizing Environmental Justice Issues in Urban Areas with a\n  Community-based Approach\n\n  According to environmental justice, environmental degradation and benefits\nshould not be disproportionately shared between communities. Identifying\ndisparities in the spatial distribution of environmental degradation is\ntherefore a prerequisite for validating the state of environmental justice in a\ngeographic region. Under ideal circumstances, environmental risk assessment is\na preferred metric, but only when exposure levels have been quantified reliably\nafter estimating the risk. In this study, we adopt a proximity burden metric\ncaused by adjacent hazardous sources, allowing us to evaluate the environmental\nburden distribution and vulnerability to pollution sources. In close\ncollaboration with a predominantly Latinx community in Chicago, we highlight\nthe usefulness of our approach through a case study that shows how certain\ncommunity areas in the city are likely to bear a disproportionate burden of\nenvironmental pollution caused by industrial roads.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01868,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000004371,
      "text":"A Large Scale Study of Reader Interactions with Images on Wikipedia\n\n  Wikipedia is the largest source of free encyclopedic knowledge and one of the\nmost visited sites on the Web. To increase reader understanding of the article,\nWikipedia editors add images within the text of the article's body. However,\ndespite their widespread usage on web platforms and the huge volume of visual\ncontent on Wikipedia, little is known about the importance of images in the\ncontext of free knowledge environments. To bridge this gap, we collect data\nabout English Wikipedia reader interactions with images during one month and\nperform the first large-scale analysis of how interactions with images happen\non Wikipedia. First, we quantify the overall engagement with images, finding\nthat one in 29 pageviews results in a click on at least one image, one order of\nmagnitude higher than interactions with other types of article content. Second,\nwe study what factors associate with image engagement and observe that clicks\non images occur more often in shorter articles and articles about visual arts\nor transports and biographies of less well-known people. Third, we look at\ninteractions with Wikipedia article previews and find that images help support\nreader information need when navigating through the site, especially for more\npopular pages. The findings in this study deepen our understanding of the role\nof images for free knowledge and provide a guide for Wikipedia editors and web\nuser communities to enrich the world's largest source of encyclopedic\nknowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11192,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"The Industry Relevance of an IT Transition Programme\n\n  There is a shortage of qualified people in the IT industry in the world. To\naddress this shortage, transition programmes are being created that help people\nchange to careers in IT. To provide useful programmes, we need to know if the\ncurrent curriculum provides value to its graduates. Moreover, as the IT\nindustry undergoes continuous change, we need to regularly review what the\nindustry needs and update any existing programmes as appropriate. In this paper\nwe present the results of a survey of graduates of one such programme, the\nPGCertInfoTech at University of Auckland, with the view to evaluating the\ncurrency of the existing programme and to gather data on which to base\ndecisions on updating it. Our conclusion is that our programme is largely\nuseful to graduates, but could be improved with the addition of material on\ncontinuous integration, and some adjustment to the time spent on testing,\nconcurrency, and project management. Our results will be useful to any other\ninstitutions having, or considering to have, IT transition programmes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.09793,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000045035,
      "text":"Assessing m-Learning Adoption in Higher Education\n\n  New mobile platforms, connected seamlessly to the Internet via wireless\naccess have become increasingly more powerful and have found usage in a diverse\nset of application areas, including the education sector. The educational\ninstitutions are becoming more open to embracing new learning platforms, which\nin turn has sparked the interest in developing new assessment frameworks. This\npaper has used the framework of the Capability Maturity Model (CMM) to design a\nmodel for M-learning within educational institutions. The framework has been\nvalidated with studies cases from higher education institutions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.08279,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000004073,
      "text":"Crowdsourcing County-Level Data on Early COVID-19 Policy Interventions\n  in the United States: Technical Report\n\n  Beginning in April 2020, we gathered partial county-level data on\nnon-pharmaceutical interventions (NPIs) implemented in response to the COVID-19\npandemic in the United States, using both volunteer and paid crowdsourcing. In\nthis report, we document the data collection process and summarize our results,\nto increase the utility of our open data and inform the design of future rapid\ncrowdsourcing data collection efforts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.09544,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"It's Time to Do Something: Mitigating the Negative Impacts of Computing\n  Through a Change to the Peer Review Process\n\n  The computing research community needs to work much harder to address the\ndownsides of our innovations. Between the erosion of privacy, threats to\ndemocracy, and automation's effect on employment (among many other issues), we\ncan no longer simply assume that our research will have a net positive impact\non the world. While bending the arc of computing innovation towards societal\nbenefit may at first seem intractable, we believe we can achieve substantial\nprogress with a straightforward step: making a small change to the peer review\nprocess. As we explain below, we hypothesize that our recommended change will\nforce computing researchers to more deeply consider the negative impacts of\ntheir work. We also expect that this change will incentivize research and\npolicy that alleviates computing's negative impacts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.14073,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"The Impacts of Low\/No-Code Development on Digital Transformation and\n  Software Development\n\n  Low\/No-code development is a software development method that provides users\nwith a platform for visually creating applications with little or no coding.\nCompanies and organizations need software applications and information systems\nfor various business purposes like management in the technology era.\nLow\/No-code development gives non-IT professionals a convenient tool for\nrapidly building simple business applications they need without or with little\ncoding. In this paper, we explored the benefits & limitations of Low\/No-Code\ndevelopment and modern Low\/No-Code development platforms in the industry. In\naddition, we analyzed how it can be improved and prospected the impacts of\nLow\/No-Code development on society and related industries in the future. In\nconclusion, we find that Low\/No-code development is a promising trend that can\nsignificantly impact future software development and digital transformation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.09674,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000107951,
      "text":"Relativistic Conceptions of Trustworthiness: Implications for the\n  Trustworthy Status of National Identification Systems\n\n  Trustworthiness is typically regarded as a desirable feature of national\nidentification systems (NISs); but the variegated nature of the trustor\ncommunities associated with such systems makes it difficult to see how a single\nsystem could be equally trustworthy to all actual and potential trustors. This\nworry is accentuated by common theoretical accounts of trustworthiness.\nAccording to such accounts, trustworthiness is relativized to particular\nindividuals and particular areas of activity, such that one can be trustworthy\nwith regard to some individuals in respect of certain matters, but not\ntrustworthy with regard to all trustors in respect of every matter. The present\narticle challenges this relativistic approach to trustworthiness by outlining a\nnew account of trustworthiness, dubbed the expectation-oriented account. This\naccount allows for the possibility of an absolutist (or one-place) approach to\ntrustworthiness. Such an account, we suggest, is the approach that best\nsupports the effort to develop NISs. To be trustworthy, we suggest, is to\nminimize the error associated with trustor expectations in situations of social\ndependency (commonly referred to as trust situations), and to be trustworthy in\nan absolute sense is to assign equal value to all expectation-related errors in\nall trust situations. In addition to outlining the features of the\nexpectation-oriented account, we describe some of the implications of this\naccount for the design, development, and management of trustworthy NISs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.04465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"Designing a Dashboard for Student Teamwork Analysis\n\n  Classroom dashboards are designed to help instructors effectively orchestrate\nclassrooms by providing summary statistics, activity tracking, and other\ninformation. Existing dashboards are generally specific to an LMS or platform\nand they generally summarize individual work, not group behaviors. However, CS\ncourses typically involve constellations of tools and mix on- and offline\ncollaboration. Thus, cross-platform monitoring of individuals and teams is\nimportant to develop a full picture of the class. In this work, we describe our\nwork on Concert, a data integration platform that collects data about student\nactivities from several sources such as Piazza, My Digital Hand, and GitHub and\nuses it to support classroom monitoring through analysis and visualizations. We\ndiscuss team visualizations that we have developed to support effective group\nmanagement and to help instructors identify teams in need of intervention.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.1146,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000102984,
      "text":"The Techno-politics of Crowdsourced Disaster Data in the Smart City\n\n  This article interrogates the techno-politics of crowdsourced data in the\nstudy of environmental hazards such as floods, storms, wildfires, and cyclones.\nWe highlight some of the main debates around the use of citizen-generated data\nfor assessing, monitoring, and responding to disasters. We then argue that,\ncompared to the number of articles discussing the quality of citizen-generated\ndata, little attention has been dedicated to discussing the social and\npolitical implications of this kind of practice. While this article does not\nintend to present definitive answers, it outlines inevitable challenges and\nindicates potential directions for future studies on the techno-politics of\ndisaster data collection. Within a techno-politics approach, we argue for a\nmodel of political participation that recognizes citizens providing data to\nshape cities as equal experts in the production of knowledge and\ndecision-making, rather than external contributors collecting data for formal\nauthorities. This political participation approach, we believe, would increase\nthe dependence of formal scientific knowledge on citizens' daily-lived\nexperiences, create horizontal collaborations among diverse stakeholders, in\nterms of respect and recognition, and increase the humanization of marginalized\ncommunities, particularly from the Global South.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11215,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000087089,
      "text":"Examining Older Adults' Information Exposure, Wellbeing, and Adherence\n  to Protective Measures During the COVID-19 Pandemic\n\n  Older adults are at greater risk of experiencing negative physical and\npsychological impacts of the novel coronavirus 2019 (COVID-19) pandemic. Our\nongoing study is assessing COVID-19 information exposure in adults aged 55 and\nabove compared to other age groups living in Massachusetts and Georgia. This\nwork investigates the potential association between information exposure and\nwellbeing as well as adherence to COVID-19 protective measures. Our initial\nresults show that older adults received information related to COVID-19 less\nfrequently than the middle-aged group, yet they feel more content and less\nstressed than the other age groups. Further analysis to identify other\npotential confounding variables is addressed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Observations on Transitioning to Teaching Computer Science Online\n\n  The hit of the COVID-19 pandemic has hugely affected higher education in the\nworld, and as a result, most of the physical classes have been (partially)\nreplaced by online teaching platforms. This transition is challenging even for\nexperienced software engineering instructors, as they were pushed to break\ntheir habits and tricks developed over the years. This paper is an experience\nreport in teaching an undergraduate course (revolving theoretical computer\nscience topics) for the first time in an online format, and some observations\nand ideas of how to engage students during online lectures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.0324,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"Analyzing a Carceral Algorithm used by the Pennsylvania Department of\n  Corrections\n\n  Scholars have focused on algorithms used during sentencing, bail, and parole,\nbut little work explores what we call carceral algorithms that are used during\nincarceration. This paper is focused on the Pennsylvania Additive\nClassification Tool (PACT) used to classify prisoners' custody levels while\nthey are incarcerated. Algorithms that are used during incarceration warrant\ndeeper attention by scholars because they have the power to enact the lived\nreality of the prisoner. The algorithm in this case determines the likelihood a\nperson would endure additional disciplinary actions, can complete required\nprogramming, and gain experiences that, among other things, are distilled into\nvariables feeding into the parole algorithm. Given such power, examining\nalgorithms used on people currently incarcerated offers a unique analytic view\nto think about the dialectic relationship between data and algorithms. Our\nexamination of the PACT is two-fold and complementary. First, our qualitative\noverview of the historical context surrounding PACT reveals that it is designed\nto prioritize incapacitation and control over rehabilitation. While it closely\ninforms prisoner rehabilitation plans and parole considerations, it is rooted\nin population management for prison securitization. Second, on analyzing data\nfor 146,793 incarcerated people in PA, along with associated metadata related\nto the PACT, we find it is replete with racial bias as well as errors,\nomissions, and inaccuracies. Our findings to date further caution against\ndata-driven criminal justice reforms that rely on pre-existing data\ninfrastructures and expansive, uncritical, data-collection routines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.05751,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"The Co-Designed Post-Pandemic University: A Participatory and Continual\n  Learning Approach for the Future of Work\n\n  The pandemic has shattered the traditional enclosures of learning. The\npost-pandemic university (PPU) will no longer be contained within the 4 walls\nof a lecture theatre, and finish once students have left the premises. The use\nof online services has now blended home and university life, and the PPU needs\nto reflect this. Our proposal of a continuous learning model will take\nadvantage of the newfound omnipresence of learning, while being dynamic enough\nto continually adapt to the ever-evolving virus situation. Universities\nrestricting themselves to fixed subject themes that are then forgotten once\ncompleted, will miss out on the \"fresh start\" presented by the virus.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12071,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"What is Legitimate Decision Support?\n\n  Decision support is the science and associated practice that consist in\nproviding recommendations to decision makers facing problems, based on\navailable theoretical knowledge and empirical data. Although this activity is\noften seen as being concerned with solving mathematical problems and conceiving\nalgorithms, it is essentially an empirical and socially framed activity, where\ninteractions between clients and analysts, and between them and concerned third\nparties, play a crucial role. Since the 80s, two concepts have structured the\nliterature devoted to analysing this aspect of decision support: validity and\nlegitimacy. Whereas validity is focused on the interactions between the client\nand the analyst, legitimacy refers to the broader picture: the organisational\ncontext, the overall problem situation, the environment, culture, history.\nDespite its importance, this concept has not received the attention it deserves\nin the literature in decision support. The present paper aims at filling this\ngap. For that purpose, we review the literature in other disciplines relevant\nto elaborate a concept of legitimacy useful in decision support contexts. Based\non this review, we propose a general theory of legitimacy, adapted to decision\nsupport contexts, encompassing the relevant contributions we found in the\nliterature. According to this general theory, a legitimate decision support\nintervention is one for which the decision support provider produces a\njustification that satisfies two conditions: (i) it effectively convinces the\ndecision support provider's interlocutors (effectiveness condition) and (ii) it\nis organised around the active elicitation of as many and as diverse\ncounterarguments as possible (truthfulness condition). Despite its conceptual\nsimplicity, legitimacy, understood in this sense, is a very exacting\nrequirement, opening ambitious research avenues that we delineate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13402,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"Privacy Limitations Of Interest-based Advertising On The Web: A\n  Post-mortem Empirical Analysis Of Google's FLoC\n\n  In 2020, Google announced it would disable third-party cookies in the Chrome\nbrowser to improve user privacy. In order to continue to enable interest-based\nadvertising while mitigating risks of individualized user tracking, Google\nproposed FLoC. The FLoC algorithm assigns users to \"cohorts\" that represent\ngroups of users with similar browsing behaviors so that ads can be served to\nusers based on their cohort. In 2022, after testing FLoC in a real world trial,\nGoogle canceled the proposal with little explanation. In this work, we provide\na post-mortem analysis of two critical privacy risks for FloC by applying an\nimplementation of FLoC to a browsing dataset collected from over 90,000 U.S.\ndevices over a one year period. First, we show how, contrary to its privacy\ngoals, FLoC would have enabled cross-site user tracking by providing a unique\nidentifier for users available across sites, similar to the third-party cookies\nFLoC was meant to be an improvement over. We show how FLoC cohort ID sequences\nobserved over time can provide this identifier to trackers, even with\nthird-party cookies disabled. We estimate the number of users in our dataset\nthat could be uniquely identified by FLoC IDs is more than 50% after 3 weeks\nand more than 95% after 4 weeks. We also show how these risks increase when\ncohort data are combined with browser fingerprinting, and how our results\nunderestimate the true risks FLoC would have posed in a real-world deployment.\nSecond, we examine the risk of FLoC leaking sensitive demographic information.\nAlthough we find statistically significant differences in browsing behaviors\nbetween demographic groups, we do not find that FLoC significantly risks\nexposing race or income information about users in our dataset. Our\ncontributions provide insights and example analyses for future approaches that\nseek to protect user privacy while monetizing the web.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12637,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000033776,
      "text":"A visualization tool for data analysis on higher education dropout: a\n  case study at UFES\n\n  Through the analysis of cultural, socioeconomic and academic performance\naspects it is possible to map the profile of the students and their motivations\nto drop out. This article aims to create a computational tool for data\nvisualization that allows drawing the profile of students to support\neducational institutions managers in the definition of dropout avoidance\npolicies. We present a method to treat data collected by higher education\ninstitutions over the years, analyze them to understand the dropout and provide\nthat information to the university and the general public. Eight questions were\nproposed to clarify the dropout from the Federal University of Esp\\'irito\nSanto, Brazil. The questions were answered through the dashboard that helps to\nunderstand the causes of dropout. It is expected that this tool can be used by\nothers educational institutions to draw student profiles contributing to\npossible resolution of the problem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13284,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000137753,
      "text":"Exploring Preferences for Transportation Modes in the City of Munich\n  after the Recent Incorporation of Ride-Hailing Companies\n\n  The growth of ridehailing (RH) companies over the past few years has affected\nurban mobility in numerous ways. Despite widespread claims about the benefits\nof such services, limited research has been conducted on the topic. This paper\nassesses the willingness of Munich transportation users to pay for RH services.\nRealizing the difficulty of obtaining data directly from RH companies, a stated\npreference survey was designed. The dataset includes responses from 500\ncommuters. Sociodemographic attributes, current travel behavior and\ntransportation mode preference in an 8 km trip scenario using RH service and\nits similar modes (auto and transit), were collected. A multinomial logit model\nwas used to estimate the time and cost coefficients for using RH services\nacross income groups, which was then used to estimate the value of time (VOT)\nfor RH. The model results indicate RH services popularity among those aged 18\nto 39, larger households and households with fewer autos. Higher income groups\nare also willing to pay more for using RH services. To examine the impact of RH\nservices on modal split in the city of Munich, we incorporated RH as a new mode\ninto an existing nested logit mode choice model using an incremental logit.\nTravel time, travel cost and VOT were used as measures for the choice commuters\nmake when choosing between RH and its closest mode, metro. A total of 20\nscenarios were evaluated at four different congestion levels and four price\nlevels to reflect the demand in response to acceptable costs and time\ntradeoffs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.10677,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"From Data Leverage to Data Co-Ops: An Institutional Model for User\n  Control over Information Access\n\n  Internet companies derive value from users by recording and influencing their\nbehavior. Users can pressure companies to refrain from certain invasive and\nmanipulative practices by selectively withdrawing their attention, an exercise\nof data leverage as formulated by Vincent et al. Ligett and Nissim's proposal\nfor an institution representing the interests of users, the data co-op, offers\na means of coordinating this action. We present one possible instantiation of\nthe data co-op, including the Platform for Untrusted Resource Evaluation\n(PURE), a system for assigning labels provided by untrusted and semi-trusted\nparties to Internet resources. We also describe PURESearch, a client program\nthat re-ranks search results according to labels provided by data co-ops and\nother sources.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03508,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"On the interplay of data and cognitive bias in crisis information\n  management -- An exploratory study on epidemic response\n\n  Humanitarian crises, such as the 2014 West Africa Ebola epidemic, challenge\ninformation management and thereby threaten the digital resilience of the\nresponding organizations. Crisis information management (CIM) is characterised\nby the urgency to respond despite the uncertainty of the situation. Coupled\nwith high stakes, limited resources and a high cognitive load, crises are prone\nto induce biases in the data and the cognitive processes of analysts and\ndecision-makers. When biases remain undetected and untreated in CIM, they may\nlead to decisions based on biased information, increasing the risk of an\ninefficient response. Literature suggests that crisis response needs to address\nthe initial uncertainty and possible biases by adapting to new and better\ninformation as it becomes available. However, we know little about whether\nadaptive approaches mitigate the interplay of data and cognitive biases.\n  We investigated this question in an exploratory, three-stage experiment on\nepidemic response. Our participants were experienced practitioners in the\nfields of crisis decision-making and information analysis. We found that\nanalysts fail to successfully debias data, even when biases are detected, and\nthat this failure can be attributed to undervaluing debiasing efforts in favor\nof rapid results. This failure leads to the development of biased information\nproducts that are conveyed to decision-makers, who consequently make decisions\nbased on biased information. Confirmation bias reinforces the reliance on\nconclusions reached with biased data, leading to a vicious cycle, in which\nbiased assumptions remain uncorrected. We suggest mindful debiasing as a\npossible counter-strategy against these bias effects in CIM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07413,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"On Heuristic Models, Assumptions, and Parameters\n\nInsightful interdisciplinary collaboration is essential to the principled governance of technology. When such efforts address the interaction between computation and society, they often focus on modeling, the process by which computer scientists formally define problems in order to enable algorithmic solutions. But modeling is a multifaceted and inherently imperfect process. Especially in interdisciplinary work, it often receives uneven scrutiny because of the practical challenges of communicating complex technical details to non-experts. We argue that there is an underappreciated if loose family of obscure and opaque technical caveats, choices, and qualifiers that the social effects of computing can depend just as much on as far more heavily scrutinized modeling choices. These artifacts are often used by researchers to paper over the incomplete theoretical foundations of computing or to burden shift responsibility for the impact of normative design decisions. Further, their nuanced technical nature often complicates thorough sociotechnical scrutiny of the discretionary decisions made to manage them. We describe three specific classes of such objects: heuristic models, assumptions, and parameters. We raise six reasons these objects may be hazardous to comprehensive analysis of computing and argue they deserve deliberate consideration as researchers explain scientific work.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.06969,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"A conceptual framework of Intelligent Management Control System for\n  Higher Education\n\n  The utilization of management control systems in university management poses\na considerable challenge because university's strategic goals are not identical\nto those applied in profit-oriented management. A university's management\ncontrol system should take into account the processing of management\ninformation for management purposes, allowing for the relationships between\ndifferent groups of stakeholders. The specificity of the university operation\nassumes conducting long-term scientific research and educational programmes.\nTherefore, the controlling approach to university management should considerat\nlong-term performance measurement as well as management in key areas such as\nresearch, provision of education to students, and interaction with the tertiary\ninstitution's socioeconomic environment.This paper aims to develop a conceptual\nframework of the Intelligent Management Control System for Higher Education\n(IMCSHE) based on cognitive agents. The main findings are related to developing\nthe assumption, model, and technological basis including the artificial\nintelligence method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07455,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Transforming agrifood production systems and supply chains with digital\n  twins\n\n  Digital twins can transform agricultural production systems and supply\nchains, curbing greenhouse gas emissions, food waste and malnutrition. However,\nthe potential of these advanced virtualization technologies is yet to be\nrealized. Here, we consider the promise of digital twins across five typical\nagrifood supply chain steps and emphasize key implementation barriers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07458,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"IMPACT: Integrated Bottom-Up Greenhouse Gas Emission Pathways for Cities\n\n  Increasing urbanization puts pressure on cities to prioritize sustainable\ngrowth and avoid carbon lock-in. Available modeling frameworks fall acutely of\nguiding such pivotal decision-making at the local level. Financial incentives,\nbehavioral interventions, and mandates drive sustainable technology adoption,\nwhile land-use zoning plays a critical role in carbon emissions from the built\nenvironment. Researchers typically evaluate impacts of policies top down, on a\nnational scale, or else post-hoc on developments vis-\\`a-vis different polices\nin the past. Such analyses cannot forecast emission pathways for specific\ncities, and hence cannot serve as input to local policymakers. Here, we present\nIMPACT pathways, from a bottom-up model with residence level granularity, that\nintegrate technology adoption policies with zoning policies, climate change,\nand grid decarbonization scenarios. With the city at the heart of our analysis,\nwe identify an emission premium for sprawling and show that adverse policy\ncombinations exist that can exhibit rebounding emissions over time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.10553,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Online discussion forums for monitoring the need for targeted\n  psychological health support: an observational case study of\n  r\/COVID19_support\n\n  The COVID-19 pandemic has placed a severe mental strain on people in general,\nand on young people in particular. Online support forums offer opportunities\nfor peer-to-peer health support, which can ease pressure on professional and\nestablished volunteer services when demand is high. Such forums can also be\nused to monitor at-risk communities to identify concerns and causes of\npsychological stress. We created and monitored r\/COVID19_support, an online\nforum for people seeking support during the COVID-19 pandemic, on the platform\nReddit. We identify posts made by users self-identifying as students or posting\nabout college\/university life, then coded these posts to identify emerging\nthemes that related to triggers of psychological anxiety and distress. 147\nposts were made to the forum by 111 unique users during the study period. A\nnumber of themes were identified by manual coding, included: feelings of grief\nassociated with the loss of college-related life experiences, such as\ngraduation ceremonies or proms; difficulties with focussing on online and\nself-guided learning; and fears for the future, in particular of graduating\ninto a constrained job market. The identification of specific issues enabled\nusers to be signposted to information to help them cope with address those\nparticular concerns. Monitoring peer-to-peer forums can help to identify\nspecific issues with which vulnerable groups may require additional support,\nenabling users to be signposted on to high-quality information to address\nspecific issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.03407,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"How to Do It Right: A Framework for Biometrics Supported Border Control\n\n  Complying with the European Union (EU) perspective on human rights goes or\nshould go together with handling ethical, social and legal challenges arising\ndue to the use of biometrics technology as border control technology. While\nthere is no doubt that the biometrics technology at European borders is a\nvaluable element of border control systems, these technologies lead to issues\nof fundamental rights and personal privacy, among others. This paper discusses\nvarious ethical, social and legal challenges arising due to the use of\nbiometrics technology in border control. First, a set of specific challenges\nand values affected were identified and then, generic considerations related to\nmitigation of these issues within a framework is provided. The framework is\nexpected to meet the emergent need for supplying interoperability among\nmultiple information systems used for border control.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05993,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000073181,
      "text":"Computer Simulation-Based Learning: Student Self-Efficacy During\n  COVID-19 Outbreak\n\n  Due to the COVID-19 as a pandemic, the government has forced the nationwide\nshutdown of several activities, including educational activities. It has\nresulted in gigantic migration of universities with education over the internet\nserving as the educational platform. Hand-on-based learning becomes a new\nchallenge. This paper aims to investigate the effect of computer\nsimulation-based learning on student self-efficacy in an electric circuit\nanalysis course. For the 17 participants included in this study, the students\nhave overcome their existing achievements indicated by a long-term average\nscore. Computer simulation-based learning provides positive results on student\nself-efficacy. Students also perceived a valuable learning experience.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.1108,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"A Comparative User Study of Human Predictions in Algorithm-Supported\n  Recidivism Risk Assessment\n\n  In this paper, we study the effects of using an algorithm-based risk\nassessment instrument to support the prediction of risk of criminalrecidivism.\nThe instrument we use in our experiments is a machine learning version\nofRiskEval(name changed for double-blindreview), which is the main risk\nassessment instrument used by the Justice Department ofCountry(omitted for\ndouble-blind review).The task is to predict whether a person who has been\nreleased from prison will commit a new crime, leading to\nre-incarceration,within the next two years. We measure, among other variables,\nthe accuracy of human predictions with and without algorithmicsupport. This\nuser study is done with (1)generalparticipants from diverse backgrounds\nrecruited through a crowdsourcing platform,(2)targetedparticipants who are\nstudents and practitioners of data science, criminology, or social work and\nprofessionals who workwithRiskEval. Among other findings, we observe that\nalgorithmic support systematically leads to more accurate predictions fromall\nparticipants, but that statistically significant gains are only seen in the\nperformance of targeted participants with respect to thatof crowdsourced\nparticipants. We also run focus groups with participants of the targeted study\nto interpret the quantitative results,including people who useRiskEvalin a\nprofessional capacity. Among other comments, professional participants indicate\nthat theywould not foresee using a fully-automated system in criminal risk\nassessment, but do consider it valuable for training, standardization,and to\nfine-tune or double-check their predictions on particularly difficult cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02344,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"The Study of Peer Assessment Impact on Group Learning Activities\n\n  Comparing with lecturer marked assessments, peer assessment is a more\ncomprehensive learning process and many of the associated problems have\noccurred. In this research work, we study the peer-assessment impact on group\nlearning activities in order to provide a complete and systematic review,\nincrease the practice and quality of the peer assessment process. Pilot studies\nwere conducted and took the form of surveys, focus group interviews, and\nquestionnaires. Prelimi-nary surveys were conducted with 582 students and 276\nresponses were received, giving a response rate of 47.4%. The results show 37%\nstudent will choose individual work over group work if given the choice. In the\ncase study, 82.1% of the total of 28 students have en-joyed working in a group\nusing Facebook as communication tools. 89.3% of the students can demonstrate\ntheir skills through group-working and most importantly, 82.1% of them agree\nthat peer assess-ment is an impartial method of assessment with the help of\nFacebook as proof of self-contribution. Our suggestions to make group work a\npleasant experience are by identifying and taking action against the\nfreeloader, giving credit to the deserving students, educating students on how\nto give constructive feedback and making the assessment pro-cess transparent to\nall.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.13268,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"PreDefense: Defending Underserved AI Students and Researchers from\n  Predatory Conferences\n\n  Mentorship in the AI community is crucial to maintaining and increasing\ndiversity, especially with respect to fostering the academic growth of\nunderserved students. While the research process itself is important, there is\nnot sufficient emphasis on the submission, presentation, and publication\nprocess, which is a cause for concern given the meteoric rise of predatory\nscientific conferences, which are based on profit only and have little to no\npeer review. These conferences are a direct threat to integrity in science by\npromoting work with little to no scientific merit. However, they also threaten\ndiversity in the AI community by marginalizing underrepresented groups away\nfrom legitimate conferences due to convenience and targeting mechanisms like\ne-mail invitations. Due to the importance of conference presentation in AI\nresearch, this very specific problem must be addressed through direct\nmentorship. In this work, we propose PreDefense, a mentorship program that\nseeks to guide underrepresented students through the scientific conference and\nworkshop process, with an emphasis on choosing legitimate venues that align\nwith the specific work that the students are focused in and preparing students\nof all backgrounds for future successful, integrous AI research careers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04754,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Everything You wanted to Know about Smart Agriculture\n\n  The world population is anticipated to increase by close to 2 billion by 2050\ncausing a rapid escalation of food demand. A recent projection shows that the\nworld is lagging behind accomplishing the \"Zero Hunger\" goal, in spite of some\nadvancements. Socio-economic and well being fallout will affect the food\nsecurity. Vulnerable groups of people will suffer malnutrition. To cater to the\nneeds of the increasing population, the agricultural industry needs to be\nmodernized, become smart, and automated. Traditional agriculture can be remade\nto efficient, sustainable, eco-friendly smart agriculture by adopting existing\ntechnologies. In this survey paper the authors present the applications,\ntechnological trends, available datasets, networking options, and challenges in\nsmart agriculture. How Agro Cyber Physical Systems are built upon the\nInternet-of-Agro-Things is discussed through various application fields.\nAgriculture 4.0 is also discussed as a whole. We focus on the technologies,\nsuch as Artificial Intelligence (AI) and Machine Learning (ML) which support\nthe automation, along with the Distributed Ledger Technology (DLT) which\nprovides data integrity and security. After an in-depth study of different\narchitectures, we also present a smart agriculture framework which relies on\nthe location of data processing. We have divided open research problems of\nsmart agriculture as future research work in two groups - from a technological\nperspective and from a networking perspective. AI, ML, the blockchain as a DLT,\nand Physical Unclonable Functions (PUF) based hardware security fall under the\ntechnology group, whereas any network related attacks, fake data injection and\nsimilar threats fall under the network research problem group.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07003,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000106295,
      "text":"Use of augmented and virtual reality tools in a general secondary\n  education institution in the context of blended learning\n\n  The study examines the problem of using augmented and virtual reality in the\nprocess of blended learning in general secondary education. The study analyzes\nthe meaning of the concept of \"blended learning\". The conceptual principles of\nblended learning are considered. The definition of augmented and virtual\nreality is given. The mixed reality is considered as a separate kind of notion.\nSeparate applications of virtual and augmented reality that can be used in the\nprocess of blended learning are considered. As a result of the study, the\nauthors propose possible ways to use augmented reality in the educational\nprocess. The model of using augmented and virtual reality in blended learning\nin general secondary education institutions was designed. It consists of the\nfollowing blocks: goal; teacher's activity; forms of education; teaching\nmethods; teaching aids; organizational forms of education; pupil activity and\nresults. Based on the model, the methodology of using augmented and virtual\nreality in blended learning in general secondary education was developed. The\nmethodology contains the following components: target component, content\ncomponent, technological component and resultant component. The methodology is\nquite universal and can be used for any subject in general secondary education.\nThe types of lessons in which it is expedient to use augmented (AR) and virtual\nreality(VR) are determined. Recommendations are given at which stage of the\nlesson it is better to use AR and VR tools (depending on the type of lesson).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000052982,
      "text":"Teacher and Student Experiences in Online Classes During COVID-19\n  Pandemic in Serbia, Bosnia and Herzegovina and Croatia\n\n  In March 2020, the World Health Organization declared the COVID pandemic,\nwhich caused interruptions and delays in many activities, but most importantly,\nit led to some huge changes in education. Online teaching will prove to be the\nmost commonly used method that should compensate for the inability to work in\nthe classroom and allow the educational process to continue. Of course, this\nteaching method was not created in 2020, but it was only presented and\nimplemented in Serbia, Bosnia and Herzegovina and Croatia with the beginning of\nthe pandemic. In this paper, we see how these countries have faced abrupt\nchanges in teaching, and how this change has affected students. Online teaching\ncannot be a mere transfer of analog content to digital; a different approach is\nneeded in the implementation of teaching as required and offered by the digital\nmedium, but at the same time it is necessary to preserve the basic principles\nof the lecturer and the curriculum. It is a call, both for teachers and\nstudents. Since this is a current and universal problem, we hope that the\nconclusions presented are useful.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.05368,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Towards a Fairer Digital Marketing Model\n\n  Surfing on the internet boom, the digital marketing industry has seen an\nexponential growth in the recent years and is often at the origin of the\nfinancial success of the biggest tech firms. In this paper we study the current\nlandscape of this industry and comment on the monopoly that Google has managed\nto gain over the years through technical innovations and intelligent\nacquisitions. We then propose potential avenues to explore in an effort to help\nmoving the digital marketing industry towards a fairer model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.10786,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"A Hybrid Cloud ERP Framework For Processing Purchasing Data\n\n  Cloud-based enterprise resource planning (cloud ERP) systems have existed in\nthe business market for around ten years. Cloud ERP supports enterprises' daily\nactivities by integrating organizational back-end systems in the cloud\nenvironment. One of the critical functions that cloud ERP offers is the\npurchasing application. The purchasing function of cloud ERP enables\nenterprises to streamline all the online purchasing transactions in real-time\nautomatically. Even cloud ERP is deployed quite often these days, organizations\nsomehow still lack the knowledge of it; to be specific, there are many issues\nattached to cloud ERP implementation yet to be solved. Hence, this paper\ncompares four leading cloud ERP platforms in Australia and proposes a hybrid\ncloud ERP framework to process online purchasing transactions. By adopting a\ncase study approach, a purchasing web-based application is designed and\npresented in this paper. In general, the proposed hybrid cloud ERP framework\nand the integrated web-based purchasing application allow user companies to\nprocess online purchasing transactions with short operation time and increased\nbusiness efficiency; in the meantime, the proposed framework also reduces\nsecurity risks attached to the public cloud.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13105,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Python for Smarter Cities: Comparison of Python libraries for static and\n  interactive visualisations of large vector data\n\n  Local governments, as part of 'smart city' initiatives and to promote\ninteroperability, are increasingly incorporating open-source software into\ntheir data management, analysis, and visualisation workflows. Python, with its\nconcise and natural syntax, presents a low barrier to entry for municipal staff\nwithout computer science backgrounds. However, with regard to geospatial\nvisualisations in particular, the range of available Python libraries has\ndiversified to such an extent that identifying candidate libraries for specific\nuse cases is a challenging undertaking. This study therefore assesses\nprominent, actively-developed visualisation libraries in the Python ecosystem\nwith respect to their suitability for producing visualisations of large vector\ndatasets. A simple visualisation task common in urban development is used to\nproduce near-identical thematic maps across static and an interactive 'tracks'\nof comparison. All short-listed libraries were able to generate the sample map\nproducts for both a small and larger dataset. Code complexity differed more\nstrongly for interactive visualisations. Formal and informal documentation\nchannels are highlighted to outline available resources for flattening learning\ncurves. CPU runtimes for the Python-based portion of the process chain differed\nstarkly for both tracks, pointing to avenues for further research. These\nresults demonstrate that the Python ecosystem offers local governments powerful\ntools, free of vendor lock-in and licensing fees, to produce performant and\nconsistently formatted visualisations for both internal and public\ndistribution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.05079,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Who Funds Misinformation? A Systematic Analysis of the Ad-related Profit\n  Routines of Fake News sites\n\n  Fake news is an age-old phenomenon, widely assumed to be associated with\npolitical propaganda published to sway public opinion. Yet, with the growth of\nsocial media, it has become a lucrative business for Web publishers. Despite\nmany studies performed and countermeasures proposed, unreliable news sites have\nincreased in the last years their share of engagement among the top performing\nnews sources. Stifling fake news impact depends on our efforts in limiting the\n(economic) incentives of fake news producers.\n  In this paper, we aim at enhancing the transparency around these exact\nincentives, and explore: Who supports the existence of fake news websites via\npaid ads, either as an advertiser or an ad seller? Who owns these websites and\nwhat other Web business are they into? We are the first to systematize the\nauditing process of fake news revenue flows. We identify the companies that\nadvertise in fake news websites and the intermediary companies responsible for\nfacilitating those ad revenues. We study more than 2,400 popular news websites\nand show that well-known ad networks, such as Google and IndexExchange, have a\ndirect advertising relation with more than 40% of fake news websites. Using a\ngraph clustering approach on 114.5K sites, we show that entities who own fake\nnews sites, also operate other types of websites pointing to the fact that\nowning a fake news website is part of a broader business operation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07424,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000068876,
      "text":"The potential of artificial intelligence for achieving healthy and\n  sustainable societies\n\n  In this chapter we extend earlier work (Vinuesa et al., Nature Communications\n11, 2020) on the potential of artificial intelligence (AI) to achieve the 17\nSustainable Development Goals (SDGs) proposed by the United Nations (UN) for\nthe 2030 Agenda. The present contribution focuses on three SDGs related to\nhealthy and sustainable societies, i.e. SDG 3 (on good health), SDG 11 (on\nsustainable cities) and SDG 13 (on climate action). This chapter extends the\nprevious study within those three goals, and goes beyond the 2030 targets.\nThese SDGs are selected because they are closely related to the coronavirus\ndisease 19 (COVID-19) pandemic, and also to crises like climate change, which\nconstitute important challenges to our society.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.08239,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"\"Way back then\": A Data-driven View of 25+ years of Web Evolution\n\n  Since the inception of the first web page three decades back, the Web has\nevolved considerably, from static HTML pages in the beginning to the dynamic\nweb pages of today, from mainly the text-based pages of the 1990s to today's\nmultimedia rich pages, etc. Although much of this is known anecdotally, to our\nknowledge, there is no quantitative documentation of the extent and timing of\nthese changes. This paper attempts to address this gap in the literature by\nlooking at the top 100 Alexa websites for over 25 years from the Internet\nArchive or the \"Wayback Machine\", archive.org. We study the changes in\npopularity, from Geocities and Yahoo! in the mid-to-late 1990s to the likes of\nGoogle, Facebook, and Tiktok of today. We also look at different categories of\nwebsites and their popularity over the years and find evidence for the decline\nin popularity of news and education-related websites, which have been replaced\nby streaming media and social networking sites. We explore the emergence and\nrelative prevalence of different MIME-types (text vs. image vs. video vs.\njavascript and json) and study whether the use of text on the Internet is\ndeclining.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09527,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000038412,
      "text":"Ethics and Efficacy of Unsolicited Anti-Trafficking SMS Outreach\n\n  The sex industry exists on a continuum based on the degree of work autonomy\npresent in labor conditions: a high degree exists on one side of the continuum\nwhere independent sex workers have a great deal of agency, while much less\nautonomy exists on the other side, where sex is traded under conditions of\nhuman trafficking. Organizations across North America perform outreach to sex\nindustry workers to offer assistance in the form of services (e.g., healthcare,\nfinancial assistance, housing), prayer, and intervention. Increasingly,\ntechnology is used to look for trafficking victims or facilitate the provision\nof assistance or services, for example through scraping and parsing sex\nindustry workers' advertisements into a database of contact information that\ncan be used by outreach organizations. However, little is known about the\nefficacy of anti-trafficking outreach technology, nor the potential risks of\nusing it to identify and contact the highly stigmatized and marginalized\npopulation of those working in the sex industry.\n  In this work, we investigate the use, context, benefits, and harms of an\nanti-trafficking technology platform via qualitative interviews with multiple\nstakeholders: the technology developers (n=6), organizations that use the\ntechnology (n=17), and sex industry workers who have been contacted or wish to\nbe contacted (n=24). Our findings illustrate misalignment between developers,\nusers of the platform, and sex industry workers they are attempting to assist.\nIn their current state, anti-trafficking outreach tools such as the one we\ninvestigate are ineffective and, at best, serve as a mechanism for spam and, at\nworst, scale and exacerbate harm against the population they aim to serve. We\nconclude with a discussion of best practices for technology-facilitated\noutreach efforts to minimize risk or harm to sex industry workers while\nefficiently providing needed services.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03714,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000011358,
      "text":"Cloud Computing-based Higher Education Platforms during the COVID-19\n  Pandemic\n\n  Cloud computing has become the infrastructure that supports people's daily\nactivities, business operations, and education delivery around the world. Cloud\ncomputing-based education platforms have been widely applied to assist online\nteaching during the COVID-19 pandemic. This paper examines the impact and\nimportance of cloud computing in remote learning and education. This study\nconducted multiple-case analyses of 22 online platforms of higher education in\nChinese universities during the epidemic. A comparative analysis of the 22\nplatforms revealed that they applied different cloud computing models and tools\nbased on their unique requirements and needs. The study results provide\nstrategic insights to higher education institutions regarding effective\napproaches to applying cloud computing-based platforms for remote education,\nespecially during crisis situations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09152,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000154972,
      "text":"The application of geographic information systems in schools around the\n  world: a retrospective analysis\n\n  The article is devoted to the problem of incorporation geographic information\nsystems (GIS) in world school practice. The authors single out the stages of\nGIS application in school geographical education based on the retrospective\nanalysis of the scientific literature. The first stage (late 70s - early 90s of\nthe 20th century) is the beginning of the first educational GIS programs and\npartnership agreements between schools and universities. The second stage (mid\n90s of the 20th century - the beginning of the 21st century) comprises the\ndistribution of GIS-educational programs in European and Australian schools\nwith the involvement of leading developers of GIS-packages (ESRI, Intergraph,\nMapInfo Corp., etc.). The third stage (2005-2012) marks the spread of the GIS\nschool education in Eastern Europe, Asia, Africa and Latin America; on the\nfourth stage (from 2012 to the present) geographic information systems emerge\nin school curricula in most countries. The haracteristics of the\nGIS-technologies development stages are given considering the GIS didactic\npossibilities for the study of school geography, as well as highlighting their\nadvantages and disadvantages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13215,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Unique Device Identification Based Linkage of Hierarchically Accessible\n  Data Domains in Prospective Hospital Data Ecosystems\n\n  The electronic health record (EHR) targets the systematized collection of\npatient-specific electronically-stored health data. Currently the EHR is an\nevolving concept driven by ongoing technical developments and open or unclear\nlegal issues concerning used medical technologies, data integration from other\ndomains and unclear access roles. This paper addresses cross-domain data\nintegration, data fusion and access control using the specific example of a\nUnique Device Identification (UDI) expanded hip implant. In fact, the\nintegration of technical focus data into the hospital information system (HIS)\nis discussed and presented based on surgically relevant information. Moreover,\nthe acquisition of social focus databased on mHealth is approached, which also\ncovers data integration and networking with therapeutic intervention or acute\ndiagnostics data. Data integration from heterogeneous domains is covered while\nusing a data ecosystem with hierarchical access based on a shell embedded role\nmodel, which includes staggered access scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03717,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"An Analysis of Energy Consumption and Carbon Footprints of\n  Cryptocurrencies and Possible Solutions\n\n  There is an urgent need to control global warming caused by humans to achieve\na sustainable future. $CO_2$ levels are rising steadily and while countries\nworldwide are actively moving toward the sustainability goals proposed during\nthe Paris Agreement in 2015, we are still a long way to go from achieving a\nsustainable mode of global operation. The increased popularity of\ncryptocurrencies since the introduction of Bitcoin in 2009 has been accompanied\nby an increasing trend in greenhouse gas emissions and high electrical energy\nconsumption. Popular energy tracking studies (e.g., Digiconomist and the\nCambridge Bitcoin Energy Consumption Index (CBECI)) have estimated energy\nconsumption ranges of 29.96 TWh to 135.12 TWh and 26.41 TWh to 176.98 TWh\nrespectively for Bitcoin as of July 2021, which are equivalent to the energy\nconsumption of countries such as Sweden and Thailand. The latest estimate by\nDigiconomist on carbon footprints shows a 64.18 Mt$CO_2$ emission by Bitcoin as\nof July 2021, close to the emissions by Greece and Oman. This review compiles\nestimates made by various studies from 2018 to 2021. We compare with the energy\nconsumption and carbon footprints of these cryptocurrencies with countries\naround the world, and centralized transaction methods such as Visa. We identify\nthe problems associated with cryptocurrencies, and propose solutions that can\nhelp reduce their energy usage and carbon footprints. Finally, we present case\nstudies on cryptocurrency networks namely, Ethereum 2.0 and Pi Network, with a\ndiscussion on how they solve some of the challenges we have identified.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"How to De-Escalate a Cyber Conflict\n\n  De-escalation of a cyber conflict can be substantially more difficult than\nde-escalation of a conventional military conflict. This paper will first\nexplain the reasons why de-escalation of a cyber conflict can be so difficult,\nand then present a list of suggestions about how to overcome these specific\ndifficulties.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09861,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"How might Driver Licensing and Vehicle Registration evolve if we adopt\n  Autonomous Cars and Digital Identification?\n\n  It has been said that fewer or greater numbers of driver's licenses may be\nissued in future. However, when we extrapolate based on comparisons to other\ntransport domains that have high levels of automation like aviation, the future\nfor driver's licenses may not be so up in the air. Introduction of autonomous\nsystems in aviation frequently leads to pilot retraining, and while modern\ncommercial jets are generally sufficiently capable to take off, fly to their\ndestination and land with little in the way of human intervention, pilots are\nstill required and every pilot must hold a current license with ratings for the\ntypes of plane they command. Contemporary in-service training for commercial\npilots has shifted focus from extended practicing of general flight skills like\nnavigation and radio use. Pilots now spend many hours each year in simulators\ntraining to building knowledge, experience and thus muscle memory for how to\nrespond when the airplane's extensive interconnected collection of semi- and\nfully-autonomous fly-by-wire systems go awry. While the plane itself is\nactually flying for much of the time during most flights today, the pilot's\nlicense still attests to their ability to assume full control and complete the\nflight where it becomes necessary. We contend a similar future may exist for\ndriver training and licensure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.10619,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"An Architecture for Web 3.0 and the Emergence of Spontaneous Time Order\n\n  In this study, we proposed an architecture for Web 3.0, which is based on the\nhashed interactions among user nodes that can transform bilateral trusts into\ncollective time order, which is the major achievement of blockchain technology,\nwithout the expensive Proof of Work or the questionable Proof of Stake.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.1205,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Experiments as Code: A Concept for Reproducible, Auditable, Debuggable,\n  Reusable, & Scalable Experiments\n\n  A common concern in experimental research is the auditability and\nreproducibility of experiments. Experiments are usually designed, provisioned,\nmanaged, and analyzed by diverse teams of specialists (e.g., researchers,\ntechnicians and engineers) and may require many resources (e.g. cloud\ninfrastructure, specialized equipment). Even though researchers strive to\ndocument experiments accurately, this process is often lacking, making it hard\nto reproduce them. Moreover, when it is necessary to create a similar\nexperiment, very often we end up \"reinventing the wheel\" as it is easier to\nstart from scratch than trying to reuse existing work, thus losing valuable\nembedded best practices and previous experiences. In behavioral studies this\nhas contributed to the reproducibility crisis. To tackle this challenge, we\npropose the \"Experiments as Code\" paradigm, where the whole experiment is not\nonly documented but additionally the automation code to provision, deploy,\nmanage, and analyze it is provided. To this end we define the Experiments as\nCode concept, provide a taxonomy for the components of a practical\nimplementation, and provide a proof of concept with a simple desktop VR\nexperiment that showcases the benefits of its \"as code\" representation, i.e.,\nreproducibility, auditability, debuggability, reusability, and scalability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.04085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Urban Vehicle Mobility Characteristic Mining and Trip Generation Based\n  on Knowledge Graph\n\n  The operation of urban transportation produces massive traffic data, which\ncontains abundant information and is of great significance for the study of\nintelligent transportation systems. In particular, with the improvement of\nperception technology, it has become possible to obtain trip data in\nindividual-level of vehicles. It has finer granularity and greater research\npotential, but at the same time requires higher requirements in terms of data\norganization and analysis. More importantly it cannot be made public due to\nprivacy issues. To handle individual-level urban vehicle trip big data better,\nwe introduce the knowledge graph for the study. For organization of individual\nlevel trip data, we designed and constructed an individual-level trip knowledge\ngraph which greatly improves the efficiency of obtaining data. Then we used the\ntrip knowledge graph as the data engine and designed logical rules to mine the\ntrip characteristics of vehicles by combining the transportation domain\nknowledge. Finally, we further propose an individual-level trip synthesis\nmethod based on knowledge graph generation to address the privacy issue of\nindividual-level traffic data. The experiment shows that the final generated\ntrip data are similar to the historical one in mobility patterns and vehicle\nassociations, and have high spatial continuity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03615,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000004073,
      "text":"Academic mobility as an organizational mechanism of intercultural\n  interaction\n\n  The trends of academic mobility of students in the context of\ninternationalization are presented. The results of a study de-voted to the\nstudents' attitudes towards international education, cultural and educational\nexchange and academic mobility programs are presented. It is concluded that the\norganization of academic mobility, as a part of intercultural environment of\nthe university, is a necessary tool of motivation for the educational process\nand organizational mechanism for intercultural communication.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07439,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Inclusive Study Group Formation At Scale\n\n  Underrepresented students face many significant challenges in their\neducation. In particular, they often have a harder time than their peers from\nmajority groups in building long-term high-quality study groups. This challenge\nis exacerbated in remote-learning scenarios, where students are unable to meet\nface-to-face and must rely on pre-existing networks for social support.\n  We present a scalable system that removes structural obstacles faced by\nunderrepresented students and supports all students in building inclusive and\nflexible study groups. One of our main goals is to make the traditionally\ninformal and unstructured process of finding study groups for homework more\nequitable by providing a uniform but lightweight structure. We aim to provide\nstudents from underrepresented groups an experience that is similar in quality\nto that of students from majority groups. Our process is unique in that it\nallows students the opportunity to request group reassignments during the\nsemester if they wish. Unlike other collaboration tools our system is not\nmandatory and does not use peer-evaluation.\n  We trialed our approach in a 1000+ student introductory Engineering and\nComputer Science course that was conducted entirely online during the COVID-19\npandemic. We find that students from underrepresented backgrounds were more\nlikely to ask for group-matching support compared to students from majority\ngroups. At the same time, underrepresented students that we matched into study\ngroups had group experiences that were comparable to students we matched from\nmajority groups. B-range students in high-comfort and high-quality groups had\nimproved learning outcomes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07445,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000057287,
      "text":"A Global Survey of Technological Resources and Datasets on COVID-19\n\n  The application and successful utilization of technological resources in\ndeveloping solutions to health, safety, and economic issues caused by COVID-19\nindicate the importance of technology in curbing COVID-19. Also, the medical\nfield has had to race against tie to develop and distribute the COVID-19\nvaccine. This endeavour became successful with the vaccines created and\napproved in less than a year, a feat in medical history. Currently, much work\nis being done on data collection, where all significant factors impacting the\ndisease are recorded. These factors include confirmed cases, death rates,\nvaccine rates, hospitalization data, and geographic regions affected by the\npandemic. Continued research and use of technological resources are highly\nrecommendable-the paper surveys list of packages, applications and datasets\nused to analyse COVID-19.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07467,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000096361,
      "text":"The Perception of Filipinos on the Advent of Cryptocurrency and\n  Non-Fungible Token (NFT) Games\n\n  This study aims to shed light on the rise of play-to-earn games in the\nPhilippines alongside cryptocurrency. The lack of research and public\nunderstanding of its benefits and drawbacks prompted the researchers to\ninvestigate its market. As such, the study tried to look into the risks and\nbenefits of crypto gaming if it would be regulated by the government, and how\nmarket volatility influences the churn rate of crypto games. The research used\na descriptive study to determine the perception of people who are engaged in\nplaying a crypto-game named as Axie Infinity. The results showed that most\nplayers spend their time playing Axie Infinity for about 1 to 4 hours a day.\nPredominantly, the return of investments for playing the game will take about 1\nto 3 months. It also showed that these players agreed that there is a possible\nfinancial instability in a volatile market. With this, they have a high trust\nissue in terms of price manipulation, privacy and security, and its design and\nusability. Understanding the cryptocurrency market requires comprehending the\nperspective of the people who are engaged in a play-to-earn game, and their\nconcerns are critical for any government actions aimed at regulating\nself-employed income earners playing (Non-fungible Tokens) NFT games in the\nPhilippines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.01415,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Synthesis of Modeling, Visualization, and Programming in GeoGebra as an\n  Effective Approach for Teaching and Learning STEM Topics\n\n  GeoGebra is an interactive geometry, algebra, statistics, and calculus\napplication designed for teaching and learn-ing math, science, and engineering.\nIts dynamic interface allows its users to accurately and interactively\nvisualize their work, models, and results. GeoGebra employs the synthesis of\nthree key features: modeling, visualization, and programming (MVP). Many\nstudies have shown the positive effects of GeoGebra on the efficiency and\neffectiveness of learning and teaching topics related to science, technology,\nengineering, and mathematics. In this study, we dis-cuss how GeoGebra provides\nan environment for learning that is very interactive and collaborative between\nthe learner and the instructor. We also show how integrating GeoGebra into the\nlearning scheme can help improve the skills and knowledge of school and\nuniversity students in numerous advanced mathematical courses, such as\ncalcu-lus, mathematical statistics, linear algebra, linear programming,\ncomputer-aided design, computer-aided geomet-ric design, analytic and\nprojective geometry, and graphical representation. Therefore, this study shows\nthe effec-tiveness of GeoGebra and its MVP key features in science and\nengineering, particularly in topics related to mathe-matics. Each key feature\nof GeoGebra is thoroughly analyzed, and further analyses, along with how\nGeoGebra can be helpful in different topics, are discussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.10666,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"YouTube, The Great Radicalizer? Auditing and Mitigating Ideological\n  Biases in YouTube Recommendations\n\n  Recommendations algorithms of social media platforms are often criticized for\nplacing users in \"rabbit holes\" of (increasingly) ideologically biased content.\nDespite these concerns, prior evidence on this algorithmic radicalization is\ninconsistent. Furthermore, prior work lacks systematic interventions that\nreduce the potential ideological bias in recommendation algorithms. We conduct\na systematic audit of YouTube's recommendation system using a hundred thousand\nsock puppets to determine the presence of ideological bias (i.e., are\nrecommendations aligned with users' ideology), its magnitude (i.e., are users\nrecommended an increasing number of videos aligned with their ideology), and\nradicalization (i.e., are the recommendations progressively more extreme).\nFurthermore, we design and evaluate a bottom-up intervention to minimize\nideological bias in recommendations without relying on cooperation from\nYouTube. We find that YouTube's recommendations do direct users -- especially\nright-leaning users -- to ideologically biased and increasingly radical content\non both homepages and in up-next recommendations. Our intervention effectively\nmitigates the observed bias, leading to more recommendations to ideologically\nneutral, diverse, and dissimilar content, yet debiasing is especially\nchallenging for right-leaning users. Our systematic assessment shows that while\nYouTube recommendations lead to ideological bias, such bias can be mitigated\nthrough our intervention.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05256,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000078148,
      "text":"Cyber security and the Leviathan\n\n  Dedicated cyber-security functions are common in commercial businesses, who\nare confronted by evolving and pervasive threats of data breaches and other\nperilous security events. Such businesses are enmeshed with the wider societies\nin which they operate. Using data gathered from in-depth, semi-structured\ninterviews with 15 Chief Information Security Officers, as well as six senior\norganisational leaders, we show that the work of political philosopher Thomas\nHobbes, particularly Leviathan, offers a useful lens through which to\nunderstand the context of these functions and of cyber security in Western\nsociety. Our findings indicate that cyber security within these businesses\ndemonstrates a number of Hobbesian features that are further implicated in, and\nprovide significant benefits to, the wider Leviathan-esque state. These include\nthe normalisation of intrusive controls, such as surveillance, and the\nstimulation of consumption. We conclude by suggesting implications for\ncyber-security practitioners, in particular, the reflexivity that these\nperspectives offer, as well as for businesses and other researchers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.09378,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Revealing the determinants of gender inequality in urban cycling with\n  large-scale data\n\n  Cycling is an outdoor activity with massive health benefits, and an effective\nsolution towards sustainable urban transport. Despite these benefits and the\nrecent rising popularity of cycling, most countries still have a negligible\nuptake. This uptake is especially low for women: there is a largely\nunexplained, persistent gender gap in cycling. To understand the determinants\nof this gender gap in cycling at scale, here we use massive,\nautomatically-collected data from the tracking application Strava on outdoor\ncycling for 61 cities across the United States, the United Kingdom, Italy and\nthe Benelux area. Leveraging the associated gender and usage information, we\nfirst quantify the emerging gender gap in recreational cycling at city-level. A\ncomparison of cycling rates of women across cities within similar geographical\nareas unveils a broad range of gender gaps. On a macroscopic level, we link\nthis heterogeneity to a variety of urban indicators and provide evidence for\ntraditional hypotheses on the determinants of the gender-cycling-gap. We find a\npositive association between female cycling rate and urban road safety. On a\nmicroscopic level, we identify female preferences for street-specific features\nin the city of New York. Enhancing the quality of the dedicated cycling\ninfrastructure may be a way to make urban environments more accessible for\nwomen, thereby making urban transport more sustainable for everyone.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.06567,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000043048,
      "text":"Location Intelligence Reveals the Extent, Timing, and Spatial Variation\n  of Hurricane Preparedness\n\n  Improving hurricane preparedness is essential to reduce hurricane impacts.\nInherent in traditional methods for quantifying and monitoring hurricane\npreparedness are significant lags. This study establishes a methodological\nframework to quantify the extent, timing, and spatial variation of hurricane\npreparedness at the CBG level using high-resolution location intelligence data.\nAnonymized cell phone data on visits to POIs for each CBG before 2017 Hurricane\nHarvey were used to examine hurricane preparedness. Four categories of POI,\ngrocery stores, gas stations, pharmacies and home improvement stores, were\nidentified as having close relationship with hurricane preparedness, and the\ndaily number of visits from each CBG to these four categories of POIs were\ncalculated during preparation period. Two metrics, extent of preparedness and\nproactivity, were calculated based on the daily visit percentage change\ncompared to the baseline period. The results show that peak visits to\npharmacies often occurred in the early stage, whereas the peak of visits to gas\nstations happened closer to landfall. The spatial and temporal patterns of\nvisits to grocery stores and home improvement stores were quite similar.\nHowever, correlation analysis demonstrates that extent of preparedness and\nproactivity are independent of each other. Combined with synchronous evacuation\ndata, CBGs were divided into four clusters in terms of extent of preparedness\nand evacuation rate. The clusters with low preparedness and low evacuation rate\nwere identified as hotspots of vulnerability for shelter-in-place households\nthat would need urgent attention during response. The study advances\ndata-driven understanding of human protective actions and provide emergency\nresponse managers with novel insights to proactively monitor disaster\npreparedness, facilitating identifying under-prepared areas and better\nallocating resources timely.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.0598,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000184112,
      "text":"The competent Computational Thinking test (cCTt): Development and\n  validation of an unplugged Computational Thinking test for upper primary\n  school\n\n  With the increasing importance of Computational Thinking (CT) at all levels\nof education, it is essential to have valid and reliable assessments.\nCurrently, there is a lack of such assessments in upper primary school. That is\nwhy we present the development and validation of the competent CT test (cCTt),\nan unplugged CT test targeting 7-9 year-old students. In the first phase, 37\nexperts evaluated the validity of the cCTt through a survey and focus group. In\nthe second phase, the test was administered to 1519 students. We employed\nClassical Test Theory, Item Response Theory, and Confirmatory Factor Analysis\nto assess the instruments' psychometric properties. The expert evaluation\nindicates that the cCTt shows good face, construct, and content validity.\nFurthermore, the psychometric analysis of the student data demonstrates\nadequate reliability, difficulty, and discriminability for the target age\ngroups. Finally, shortened variants of the test are established through\nConfirmatory Factor Analysis. To conclude, the proposed cCTt is a valid and\nreliable instrument, for use by researchers and educators alike, which expands\nthe portfolio of validated CT assessments across compulsory education. Future\nassessments looking at capturing CT in a more exhaustive manner might consider\ncombining the cCTt with other forms of assessments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03854,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"Metaverse: Security and Privacy Concerns\n\n  The term \"metaverse\", a three-dimensional virtual universe similar to the\nreal realm, has always been full of imagination since it was put forward in the\n1990s. Recently, it is possible to realize the metaverse with the continuous\nemergence and progress of various technologies, and thus it has attracted\nextensive attention again. It may bring a lot of benefits to human society such\nas reducing discrimination, eliminating individual differences, and\nsocializing. However, everything has security and privacy concerns, which is no\nexception for the metaverse. In this article, we firstly analyze the concept of\nthe metaverse and propose that it is a super virtual-reality (VR) ecosystem\ncompared with other VR technologies. Then, we carefully analyze and elaborate\non possible security and privacy concerns from four perspectives: user\ninformation, communication, scenario, and goods, and immediately, the potential\nsolutions are correspondingly put forward. Meanwhile, we propose the need to\ntake advantage of the new buckets effect to comprehensively address security\nand privacy concerns from a philosophical perspective, which hopefully will\nbring some progress to the metaverse community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05303,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000122521,
      "text":"Use of Digital Technologies in Public Health Responses to Tackle\n  Covid-19: the Bangladesh Perspective\n\n  This paper aims to study the fight against COVID-19 in Bangladesh and digital\nintervention initiatives. To achieve the purpose of our research, we conducted\na methodical review of online content. We have reviewed the first digital\nintervention that COVID-19 has been used to fight against worldwide. Then we\nreviewed the initiatives that have been taken in Bangladesh. Our paper has\nshown that while Bangladesh can take advantage of the digital intervention\napproach, it will require rigorous collaboration between government\norganizations and universities to get the most out of it. Public health can\nbecome increasingly digital in the future, and we are reviewing international\nalignment requirements. This exploration also focused on the strategies for\ncontrolling, evaluating, and using digital technology to strengthen epidemic\nmanagement and future preparations for COVID-19.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05085,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Census TopDown: The Impacts of Differential Privacy on Redistricting\n\n  The 2020 Decennial Census will be released with a new disclosure avoidance\nsystem in place, putting differential privacy in the spotlight for a wide range\nof data users. We consider several key applications of Census data in\nredistricting, developing tools and demonstrations for practitioners who are\nconcerned about the impacts of this new noising algorithm called TopDown. Based\non a close look at reconstructed Texas data, we find reassuring evidence that\nTopDown will not threaten the ability to produce districts with tolerable\npopulation balance or to detect signals of racial polarization for Voting\nRights Act enforcement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16743,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000054969,
      "text":"A Peek into the Political Biases in Email Spam Filtering Algorithms\n  During US Election 2020\n\n  Email services use spam filtering algorithms (SFAs) to filter emails that are\nunwanted by the user. However, at times, the emails perceived by an SFA as\nunwanted may be important to the user. Such incorrect decisions can have\nsignificant implications if SFAs treat emails of user interest as spam on a\nlarge scale. This is particularly important during national elections. To study\nwhether the SFAs of popular email services have any biases in treating the\ncampaign emails, we conducted a large-scale study of the campaign emails of the\nUS elections 2020 by subscribing to a large number of Presidential, Senate, and\nHouse candidates using over a hundred email accounts on Gmail, Outlook, and\nYahoo. We analyzed the biases in the SFAs towards the left and the right\ncandidates and further studied the impact of the interactions (such as reading\nor marking emails as spam) of email recipients on these biases. We observed\nthat the SFAs of different email services indeed exhibit biases towards\ndifferent political affiliations. We present this and several other important\nobservations in this paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.0148,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000131461,
      "text":"Life, the Metaverse and Everything: An Overview of Privacy, Ethics, and\n  Governance in Metaverse\n\n  The metaverse is expected to be the next major evolution phase of the\ninternet. The metaverse will have an impact on human society, production, and\nlife. In this work, we analyze the current trends and challenges that building\nsuch a virtual environment will face. We focus on three major pillars to guide\nthe development of the metaverse: privacy, governance, and ethical design, to\nguide the development of the metaverse. Finally, we propose a preliminary\nmodular-based framework for an ethical design of the metaverse.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.07028,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000234776,
      "text":"Trustable Mobile Crowd Sourcing for Acquiring Information from a Flooded\n  Smart Area\n\n  Flood is a natural phenomenon that causes severe environmental damage and\ndestruction in smart cities. After a flood, topographic, geological, and living\nconditions change. As a result, the previous information regarding the\nenvironment is no more valid. Rescue and relief organizations that intend to\nhelp the affected people need to obtain new and accurate information about the\nconditions of the flooded environment. Acquiring this required information in\nthe shortest time is a challenge for realizing smart cities. Due to the\nadvances in the Internet of Things technology and the prevalence of smartphones\nwith several sensors and functionalities, it is possible to obtain the required\ninformation by leveraging the Crowdsourcing model. In this paper, the\ninformation required from a flooded area is classified into four categories:\nvictim, Facility and Livelihood, medical, and transfer. Next, a crowdsourcing\nscheme for acquiring information is proposed, including malicious user\ndetection to ensure the accuracy of information received. Finally, simulation\nresults indicate that the proposed scheme correctly detects malicious users and\nensures the quality of obtained information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.06245,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Predatory Medicine: Exploring and Measuring the Vulnerability of Medical\n  AI to Predatory Science\n\n  Medical Artificial Intelligence (MedAI) for diagnosis, treatment options, and\ndrug development represents the new age of healthcare. The security, integrity,\nand credibility of MedAI tools are paramount issues because human lives are at\nstake. MedAI solutions are often heavily dependent on scientific medical\nresearch literature as a primary data source that draws the attacker's\nattention as a potential target. We present a first study of how the output of\nMedAI can be polluted with Predatory Publications Presence (PPP). We study two\nMedAI systems: mediKanren (disease independent) and CancerMine\n(Disease-specific), which use research literature as primary data input from\nthe research repository PubMed, PubMed derived database SemMedDB, and NIH\ntranslational Knowledge Graphs (KGs). Our study has a three-pronged focus: (1)\nidentifying the PPP in PubMed; (2) verifying the PPP in SemMedDB and the KGs;\n(3) demonstrating the existing vulnerability of PPP traversing to the MedAI\noutput. Our contribution lies in identifying the existing PPP in the MedAI\ninputs and demonstrating how predatory science can jeopardize the credibility\nof MedAI solutions, making their real-life deployment questionable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.0061,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000212259,
      "text":"The Transfer Student Experience: It's A Lot Like Buying a Used Car\n\n  The experience transfer students encounter as they navigate their journeys\nfrom community college to university is similar to that of buying a used car.\nWe demonstrate this by showing how the information asymmetry in the market for\nused cars also occurs in the market for transfer students, producing\ninefficient markets in both cases, thereby increasing the chances of adverse\nselection. We diagnose the underlying conditions that produce transfer\ninefficiencies, identifying them as a structural inequity within the system of\nhigher education. Finally, recommendations for alleviating information\nasymmetry in transfer processes, that would lead to better outcomes for\ntransfer students, are provided.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.07201,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Evaluation of websites of state public health agencies during the\n  COVID-19 pandemic demonstrating the degree of effort to design for\n  accessibility\n\n  Since the beginning of the pandemic, every state public health agency in the\nUnited States has created and maintained a website dedicated to COVID 19. Our\ngoal was to evaluate these websites for conformity to accessibility guidelines.\nSpecifically, we assessed, on a scale of increasing levels of accessibility\ncompliance requirements, the results of the efforts made by website developers\nto incorporate and meet accessibility compliance criteria. We focused on\nhomepages and vaccine pages in 49 states. For this study, we used the automated\nAChecker tool to assess conformance to the WCAG 2.0 guidelines at A, AA and AAA\nlevels of conformance, and conformance with the Section 508c standard. We also\nmanually rated, on a scale 0 (none) to 3 (highest), the specific accessibility\nfeatures, if any, that web developers had included on the pages. We found that\naccessibility violations were prevalent across states but to varying degrees\nfor a specific accessibility criterion. Although violations were detected in\nall 4 POUR accessibility principles, the most number of known violations\noccurred in meeting the perceivability and operability principles. Most\nviolations in 508c guidelines occurred in not providing functional text in\nscripting languages and in not providing text equivalents for nontext. The\ndegree of effort and conformance significantly varied between states; a\nmajority of states exhibited a lower degree of effort, while a few attempted\ninnovative ways to enhance accessibility on their websites. The efforts seemed\nto focus on meeting the minimum threshold. It is not clear if websites were\ndesigned proactively for accessibility.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.06932,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000081791,
      "text":"Going Down the Rabbit Hole: Characterizing the Long Tail of Wikipedia\n  Reading Sessions\n\n  \"Wiki rabbit holes\" are informally defined as navigation paths followed by\nWikipedia readers that lead them to long explorations, sometimes involving\nunexpected articles. Although wiki rabbit holes are a popular concept in\nInternet culture, our current understanding of their dynamics is based on\nanecdotal reports only. To bridge this gap, this paper provides a large-scale\nquantitative characterization of the navigation traces of readers who fell into\na wiki rabbit hole. First, we represent user sessions as navigation trees and\noperationalize the concept of wiki rabbit holes based on the depth of these\ntrees. Then, we characterize rabbit hole sessions in terms of structural\npatterns, time properties, and topical exploration.\n  We find that article layout influences the structure of rabbit hole sessions\nand that the fraction of rabbit hole sessions is higher during the night.\nMoreover, readers are more likely to fall into a rabbit hole starting from\narticles about entertainment, sports, politics, and history. Finally, we\nobserve that, on average, readers tend to stay focused on one topic by\nremaining in the semantic neighborhood of the first articles even during rabbit\nhole sessions.\n  These findings contribute to our understanding of Wikipedia readers'\ninformation needs and user behavior on the Web.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.01455,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"A relationship and not a thing: A relational approach to algorithmic\n  accountability and assessment documentation\n\n  Central to a number of scholarly, regulatory, and public conversations about\nalgorithmic accountability is the question of who should have access to\ndocumentation that reveals the inner workings, intended function, and\nanticipated consequences of algorithmic systems, potentially establishing new\nroutes for impacted publics to contest the operations of these systems.\nCurrently, developers largely have a monopoly on information about how their\nsystems actually work and are incentivized to maintain their own ignorance\nabout aspects of how their systems affect the world. Increasingly, legislators,\nregulators and advocates have turned to assessment documentation in order to\naddress the gap between the public's experience of algorithmic harms and the\nobligations of developers to document and justify their design decisions.\nHowever, issues of standing and expertise currently prevent publics from\ncohering around shared interests in preventing and redressing algorithmic\nharms; as we demonstrate with multiple cases, courts often find computational\nharms non-cognizable and rarely require developers to address material claims\nof harm. Constructed with a triadic accountability relationship, algorithmic\nimpact assessment regimes could alter this situation by establishing procedural\nrights around public access to reporting and documentation. Developing a\nrelational approach to accountability, we argue that robust accountability\nregimes must establish opportunities for publics to cohere around shared\nexperiences and interests, and to contest the outcomes of algorithmic systems\nthat affect their lives. Furthermore, algorithmic accountability policies\ncurrently under consideration in many jurisdictions must provide the public\nwith adequate standing and opportunities to access and contest the\ndocumentation provided by the actors and the judgments passed by the forum.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.07789,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Engineering data-driven solutions for future mobility: perspectives and\n  challenges\n\n  The automotive industry is currently undergoing major changes. These include\na general shift towards decarbonised mode of transportation, the implementation\nof mobility as an end-to-end service, and the transition to vehicles that\nincreasingly rely on software and digital tools to function. Digitalisation is\nexpected to play a key role in shaping the future of mobility ecosystems by\nfostering the integration of traditionally independent system domains in the\nenergy, transportation and information sectors. This report discusses\nopportunities and challenges for engineering data-driven solutions that support\nthe requirements of future digitalised mobility systems based on three use\ncases for electric vehicle public charging infrastructures, services and\nsecurity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.02799,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Accelerated carrier invoice factoring using predictive freight transport\n  events\n\n  Invoice factoring is an invoice financing process where business\norganizations sell their invoices to banks or financial institutions at a\ndiscount to gain faster access to the invoice amount. In global trade, ocean\nand land carriers exercise invoice factoring to gain quick access to the money\nthey get paid for the shipment of consignments by shippers. Shippers typically\nclear the invoice payment within 60-90 days of goods getting delivered. In\norder to get early access to capital, carriers initiate invoice factoring after\nthe completion of goods delivery to the shipper. In this work, we provide an\napproach to enable accelerated carrier invoice factoring even before the\ndelivery of goods. Carrier invoice value for a given shipment depends on the\nactual values of shipment tracking events. We predict the carrier invoice value\nat different freight transport milestone events as the goods transportation\nprogresses from supplier to shipper using smart contracts on a blockchain\nnetwork that is operated by the global trade logistics participants. The\nprediction at a given stage is based on past shipment tracking events and\npredicted future shipment tracking events. Accurate prediction of invoice value\nfor ongoing shipment enables the carrier organization to initiate invoice\nfactoring on the trade finance network before the completion of goods delivery\nto the shipper. Further, based on the past accuracy of prediction models, the\nfinancial institutions may choose to release the invoice amount in installments\nat different freight transportation milestone events.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16432,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Long-term Dynamics of Fairness Intervention in Connection Recommender\n  Systems\n\n  Recommender system fairness has been studied from the perspectives of a\nvariety of stakeholders including content producers, the content itself and\nrecipients of recommendations. Regardless of which type of stakeholders are\nconsidered, most works in this area assess the efficacy of fairness\nintervention by evaluating a single fixed fairness criterion through the lens\nof a one-shot, static setting. Yet recommender systems constitute dynamical\nsystems with feedback loops from the recommendations to the underlying\npopulation distributions which could lead to unforeseen and adverse\nconsequences if not taken into account. In this paper, we study a connection\nrecommender system patterned after the systems employed by web-scale social\nnetworks and analyze the long-term effects of intervening on fairness in the\nrecommendations. We find that, although seemingly fair in aggregate, common\nexposure and utility parity interventions fail to mitigate amplification of\nbiases in the long term. We theoretically characterize how certain fairness\ninterventions impact the bias amplification dynamics in a stylized P\\'{o}lya\nurn model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.06243,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000042386,
      "text":"QSDsan: An Integrated Platform for Quantitative Sustainable Design of\n  Sanitation and Resource Recovery Systems\n\n  Sustainable sanitation and resource recovery technologies are needed to\naddress rapid environmental and socioeconomic changes. Research prioritization\nis critical to expedite the development and deployment of such technologies\nacross their vast system space (e.g., technology choices, design and operating\ndecisions). In this study, we introduce QSDsan - an open-source tool written in\nPython (under the object-oriented programming paradigm) and developed for the\nquantitative sustainable design (QSD) of sanitation and resource recovery\nsystems. As an integrated platform for system design, process modeling and\nsimulation, techno-economic analysis (TEA), and life cycle assessment (LCA),\nQSDsan can be used to enumerate and investigate the opportunity space for\nemerging technologies under uncertainty, while considering contextual\nparameters that are critical to technology deployment. We illustrate the core\ncapabilities of QSDsan through two distinct examples: (i) evaluation of a\ncomplete sanitation value chain that compares three alternative systems; and\n(ii) dynamic simulation of the wastewater treatment plant described in the\nbenchmark simulation model no. 1 (BSM1). Through these examples, we show the\nutility of QSDsan to automate design, enable flexible process modeling, achieve\nrapid and reproducible simulations, and to perform advanced statistical\nanalyses with integrated visualization. We strive to make QSDsan a\ncommunity-led platform with online documentation, tutorials (explanatory notes,\nexecutable scripts, and video demonstrations), and a growing ecosystem of\nsupporting packages (e.g., DMsan for decision-making). This platform can be\nfreely accessed, used, and expanded by researchers, practitioners, and the\npublic alike, ultimately contributing to the advancement of safe and affordable\nsanitation technologies around the globe.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03405,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Recommended Guidelines for Effective MOOCs based on a Multiple-Case\n  Study\n\n  Massive Open Online Courseware (MOOCs) appeared in 2008 and grew considerably\nin the past decade, now reaching millions of students and professionals all\nover the world. MOOCs do not replace other educational forms. Instead, they\ncomplement them by offering a powerful educational tool that can reach students\nthat, otherwise, would not have access to that information. Nevertheless,\ndesigning and implementing a successful MOOC is not straightforward. Simply\nrecording traditional classes is an approach that does not work, since the\nconditions in which a MOOC student learns are very different from the\nconventional classroom. In particular, dropout rates in MOOCs are, normally, at\nleast an order of magnitude higher than in conventional courses. In this paper,\nwe analyze data from 7 successful MOOCs that have attracted over 150,000\nstudents in the past years. The analysis led to the proposal of a set of\nguidelines to help instructors in designing more effective MOOCs. These results\ncontribute to the existing body of knowledge in the field, bring new insights,\nand pose new questions for future research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.05884,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000070863,
      "text":"A Decentralized Resource Management System Proposal For Disasters:\n  NGO-RMSD (STK-AKYS)\n\n  Disaster and emergency management are under the responsibility of many\norganizations and there are serious coordination problems in post-disaster\ncrisis management. This paper proposes a decentralized non-governmental\norganization resource management system for disasters (NGO-RMSD \/ STK-AKYS).\nThis system is based on blockchain technology and it will enable the\nnon-governmental organizations (NGO) and public institutions to manage and\ncoordinate the resources in a trusted environment in the case of disasters. A\nproof of concept implementation is developed by using the Quorum blockchain\nframework which is more energy-efficient than crypto currency-based blockchain\nsolutions. Smart contracts are developed for the autonomous working of the\nsystem. These smart contacts are used for the verification of the needs of the\none who is in need, delivering resources to the right people, and identifying\nthe urgent needs. The system aims to reach more disaster victims in a more\ntimely manner. NGO-RMSD is designed according to the needs of the NGOs in the\nfield. The application is shared with the free software license and further\ndevelopment with the community is aimed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07073,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Longitudinal Complex Dynamics of Labour Markets Reveal Increasing\n  Polarisation\n\n  In this paper we conduct a longitudinal analysis of the structure of labour\nmarkets in the US over 7 decades of technological, economic and policy change.\nWe make use of network science, natural language processing and machine\nlearning to uncover structural changes in the labour market over time. We find\na steady rate of both disappearance of jobs and a shift in the required work\ntasks, despite much technological and economic change over this time period.\nMachine learning is used to classify jobs as being predominantly cognitive or\nphysical based on the textual description of the workplace tasks. We also\nmeasure increasing polarisation between these two classes of jobs, linked by\nthe similarity of tasks, over time that could constrain workers wishing to move\nto different jobs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12564,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Defining the role of open source software in research reproducibility\n\n  Reproducibility is inseparable from transparency, as sharing data, code and\ncomputational environment is a pre-requisite for being able to retrace the\nsteps of producing the research results. Others have made the case that this\nartifact sharing should adopt appropriate licensing schemes that permit reuse,\nmodification and redistribution. I make a new proposal for the role of open\nsource software, stemming from the lessons it teaches about distributed\ncollaboration and a commitment-based culture. Reviewing the defining features\nof open source software (licensing, development, communities), I look for\nexplanation of its success from the perspectives of connectivism -- a learning\ntheory for the digital age -- and the language-action framework of Winograd and\nFlores. I contend that reproducibility engenders trust, which we routinely\nbuild in community via conversations, and the practices of open source software\nhelp us to learn how to be more effective learning (discovering) together,\ncontributing to the same goal.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.1154,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000133779,
      "text":"Research Status of Deep Learning Methods for Rumor Detection\n\n  To manage the rumors in social media to reduce the harm of rumors in society.\nMany studies used methods of deep learning to detect rumors in open networks.\nTo comprehensively sort out the research status of rumor detection from\nmultiple perspectives, this paper analyzes the highly focused work from three\nperspectives: Feature Selection, Model Structure, and Research Methods. From\nthe perspective of feature selection, we divide methods into content feature,\nsocial feature, and propagation structure feature of the rumors. Then, this\nwork divides deep learning models of rumor detection into CNN, RNN, GNN,\nTransformer based on the model structure, which is convenient for comparison.\nBesides, this work summarizes 30 works into 7 rumor detection methods such as\npropagation trees, adversarial learning, cross-domain methods, multi-task\nlearning, unsupervised and semi-supervised methods, based knowledge graph, and\nother methods for the first time. And compare the advantages of different\nmethods to detect rumors. In addition, this review enumerate datasets available\nand discusses the potential issues and future work to help researchers advance\nthe development of field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.14108,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000076161,
      "text":"Motivating Data Science Students to Participate and Learn\n\n  Data science education is increasingly involving human subjects and societal\nissues such as privacy, ethics, and fairness. Data scientists need to be\nequipped with skills to tackle the complexities of the societal context\nsurrounding their data science work. In this paper, we offer insights into how\nto structure our data science classes so that they motivate students to deeply\nengage with material about societal context and lean into the types of\nconversations that will produce long lasting growth in critical thinking\nskills. In particular, we describe a novel assessment tool called participation\nportfolios, which is motivated by a framework that promotes student autonomy,\nself reflection, and the building of a learning community. We compare student\nparticipation before and after implementing this assessment tool, and our\nresults suggest that this tool increased student participation and helped them\nmove towards course learning objectives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.10305,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000073512,
      "text":"People are not coins. Morally distinct types of predictions necessitate\n  different fairness constraints\n\n  A recent paper (Hedden 2021) has argued that most of the group fairness\nconstraints discussed in the machine learning literature are not necessary\nconditions for the fairness of predictions, and hence that there are no genuine\nfairness metrics. This is proven by discussing a special case of a fair\nprediction. In our paper, we show that Hedden 's argument does not hold for the\nmost common kind of predictions used in data science, which are about people\nand based on data from similar people; we call these human-group-based\npractices. We argue that there is a morally salient distinction between\nhuman-group-based practices and those that are based on data of only one\nperson, which we call human-individual-based practices. Thus, what may be a\nnecessary condition for the fairness of human-group-based practices may not be\na necessary condition for the fairness of human-individual-based practices, on\nwhich Hedden 's argument is based. Accordingly, the group fairness metrics\ndiscussed in the machine learning literature may still be relevant for most\napplications of prediction-based decision making.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11615,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Gerrymandering Individual Fairness\n\n  Individual fairness, proposed by Dwork et al., is a fairness measure that is\nsupposed to prevent the unfair treatment of individuals on the subgroup level,\nand to overcome the problem that group fairness measures are susceptible to\nmanipulation, or gerrymandering. The goal of the present paper is to explore\nthe extent to which it is possible to gerrymander individual fairness itself.\nIt will be proved that gerrymandering individual fairness in the context of\npredicting scores is possible. It will also be argued that individual fairness\nprovides a very weak notion of fairness for some choices of feature space and\nmetric. Finally, it will be discussed how the general idea of individual\nfairness may be preserved by formulating a notion of fairness that allows us to\novercome some of the problems with individual fairness identified here and\nelsewhere.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11076,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000589755,
      "text":"Turning the Hunted into the Hunter via Threat Hunting: Life Cycle,\n  Ecosystem, Challenges and the Great Promise of AI\n\n  The threat hunting lifecycle is a complex atmosphere that requires special\nattention from professionals to maintain security. This paper is a collection\nof recent work that gives a holistic view of the threat hunting ecosystem,\nidentifies challenges, and discusses the future with the integration of\nartificial intelligence (AI). We specifically establish a life cycle and\necosystem for privacy-threat hunting in addition to identifying the related\nchallenges. We also discovered how critical the use of AI is in threat hunting.\nThis work paves the way for future work in this area as it provides the\nfoundational knowledge to make meaningful advancements for threat hunting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03241,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Three Laws of Technology Rise or Fall\n\n  Newton's laws of motion perfectly explain or approximate physical phenomena\nin our everyday life. Are there any laws that explain or approximate\ntechnology's rise or fall? After reviewing thirteen information technologies\nthat succeeded, this article concludes three laws of technology and derives\nfive corollaries to explain or approximate the rise or fall of technology.\nThree laws are the laws of technology inertia, technology change force, and\ntechnology action and reaction. Five corollaries are the corollaries of\nmeasurement of technology change force, technology breakthrough, technology\nmonopoly, technology openness, and technology business opportunity. I present\nhow to use the laws and the corollaries to analyze an emerging technology --\nthe open-source RISC-V processor. Also, I elaborate on benchmarks' role in\napplying those laws.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.01041,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"The Chaotic State of UK Drone Regulation\n\n  In December 2020 the law for drone pilots and unmanned aerial vehicle (UAV)\nuse went into a transition phase in preparation for new EU international UAV\nregulation. That EU regulation comes into full effect as the transition periods\ndefined in the United Kingdom's Civil Aviation Authority Air Policy CAP722\nexpire during December 2022 (CAA, 2020). However, international homologation\nregulation will not address the patchwork of inconsistent drone use regulations\nthat exist in the United Kingdom from the layering of local and subordinate\nauthority byelaws over UK aviation law. We provide an extensive review of local\nauthority regulation of drone use on public open and green spaces, finding that\nmany local authorities are unaware of the issues being created through: (i)\ninappropriately couched or poorly framed byelaws; (ii) multiple byelaws\ncovering the same area by virtue of overlapping jurisdictions; or (iii) the\nlack readily identifiable policies for drone use on public land.\nOverregulation, inconsistent regulation and regulatory disharmony are causing\nconfusion for recreational drone enthusiasts such that it is never clear which\npublic or crown-owned open and green spaces they are allowed to, or prohibited\nfrom, flying. While the government and local authorities might like them to,\ndrones are not going away. Therefore, we conclude, the easiest way to ensure\ncitizens stay within the bounds of drone law that is intended to ensure public\nsafety, is to make that law comprehensible, consistent and easy to comply with.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.04098,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Identifying Experts in Question & Answer Portals: A Case Study on Data\n  Science Competencies in Reddit\n\n  The irreplaceable key to the triumph of Question & Answer (Q&A) platforms is\ntheir users providing high-quality answers to the challenging questions posted\nacross various topics of interest. From more than a decade, the expert finding\nproblem attracted much attention in information retrieval research. Based on\nthe encountered gaps in the expert identification across several Q&A portals,\nwe inspect the feasibility of identifying data science experts in Reddit. Our\nmethod is based on the manual coding results where two data science experts\nlabelled not only expert and non-expert comments, but also out-of-scope\ncomments, which is a novel contribution to the literature, enabling the\nidentification of more groups of comments across web portals. We present a\nsemi-supervised approach which combines 1,113 labelled comments with 100,226\nunlabelled comments during training. The proposed model uses the activity\nbehaviour of every user, including Natural Language Processing (NLP),\ncrowdsourced and user feature sets. We conclude that the NLP and user feature\nsets contribute the most to the better identification of these three classes.\nIt means that this method can generalise well within the domain. Finally, we\nmake a novel contribution by presenting different types of users in Reddit,\nwhich opens many future research directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12872,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Rationality in current era -- A recent survey\n\n  Rationality has been an intriguing topic for several decades. Even the scope\nof definition of rationality across different subjects varies. Several theories\n(e.g., game theory) initially evolved on the basis that agents (e.g., humans)\nare perfectly rational. One interpretation of perfect rationality is that\nagents always make the optimal decision which maximizes their expected\nutilities. However, subsequently this assumption was relaxed to include bounded\nrationality where agents have limitations in terms of computing resources and\nbiases which prevents them to take the optimal decision. However, with recent\nadvances in (quantum) computing, artificial intelligence (AI), science and\ntechnology etc., has led to the thought that perhaps the concept of rationality\nwould be augmented with machine intelligence which will enable agents to take\ndecision optimally with higher regularity. However, there are divergent views\non this topic. The paper attempts to put forward a recent survey (last five\nyears) of research on these divergent views. These viewsmay be grouped into\nthree schools of thoughts. The first school is the one which is sceptical of\nprogress of AI and believes that human intelligencewill always supersede\nmachine intelligence. The second school of thought thinks that advent of AI and\nadvances in computing will help in better understanding of bounded rationality.\nThird school of thought believes that bounds of bounded rationality will be\nextended by advances in AI and various other fields. This survey hopes to\nprovide a starting point for further research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00823,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0009187063,
      "text":"Recordism: A social-scientific prospect of blockchain from social,\n  legal, financial, and technological perspectives\n\n  Blockchain technology has the potential to revolutionize the architecture of\ncyberspace by transforming the way information is stored, circulated, and\nexchanged in cyberspace through decentralization, transparency, and\nde-identification. This means that ordinary participants can simultaneously\nbecome traders, miners, retailers, and customers, thus breaking down barriers,\nreducing the information gap between participants in the community, and\ncontributing to the futuristic metaverse with an open, progressive, and equal\nideology. The impact of this information transformation empowered by blockchain\nextends to our understanding of methodology, legal governance in cyberspace,\nand financial and technological development. This study asks: what are the\nimplications of the blockchain-driven information revolution for society and\nsocial sciences? In order to answer this main question, the paper focuses on\nfour key perspectives: methodological, legal, financial, and technical. Through\nthe analysis of these four perspectives, the paper provides a comprehensive\nunderstanding of the impact of blockchain on society, the social sciences, and\ntechnology, making a contribution to current scholarship. It finds that\nblockchain is not only an innovative cognition method, but also a community\nrepresentative, serving as a source of trust, a governance watchdog, an\nenforcer of cyber laws, and an incubator for future technologies. Despite some\nchallenges in integrating blockchain with existing social structures, this\npaper concludes that blockchain has the potential to play a significant role in\nshaping the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12046,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000076824,
      "text":"Experience Report: Standards-Based Grading at Scale in Algorithms\n\n  We report our experiences implementing standards-based grading at scale in an\nAlgorithms course, which serves as the terminal required CS Theory course in\nour department's undergraduate curriculum. The course had 200-400 students,\ntaught by two instructors, eight graduate teaching assistants, and supported by\ntwo additional graders and several undergraduate course assistants. We\nhighlight the role of standards-based grading in supporting our students during\nthe COVID-19 pandemic. We conclude by detailing the successes and adjustments\nwe would make to the course structure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06453,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Exploiting the Right: Inferring Ideological Alignment in Online\n  Influence Campaigns Using Shared Images\n\n  This work advances investigations into the visual media shared by agents in\ndisinformation campaigns by characterizing the images shared by accounts\nidentified by Twitter as being part of such campaigns. Using images shared by\nUS politicians' Twitter accounts as a baseline and training set, we build\nmodels for inferring the ideological presentation of accounts using the images\nthey share. Results show that, while our models recover the expected bimodal\nideological distribution of US politicians, we find that, on average, four\nseparate influence campaigns -- attributed to Iran, Russia, China, and\nVenezuela -- all present conservative ideological presentations in the images\nthey share. Given that prior work has shown Twitter accounts used by Russian\ndisinformation agents are ideologically diverse in the text and news they\nshare, these image-oriented findings provide new insights into potential axes\nof coordination and suggest these accounts may not present consistent\nideological positions across modalities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06611,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000182125,
      "text":"Information and Communication Technology in Migration: A Framework for\n  Applications, Customization, and Research\n\n  This paper addresses the role of Information and Communication Technology\n(ICT) in migration governance, support, and experience with particular\nattention to emerging technologies such as artificial intelligence, social\nmedia, and virtual reality. We propose a framework for technology use based on\nuser groups and process types. We provide examples of using emerging\ntechnologies for migration-related tasks within the context of this framework.\nWe then identify how such technologies can be applied to migration-related\ntasks, developed for customized use, and improved through research to add new\nfeatures that can help different migration stakeholders. We suggest a series of\npossible directions for future research and development to take advantage of\nspecific affordances of those emerging technologies more effectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.04244,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"How does online teamwork change student communication patterns in\n  programming courses?\n\n  Online teaching has become a new reality due to the COVID-19 pandemic raising\na lot of questions about its learning outcomes. Recent studies have shown that\npeer communication positively affects learning outcomes of online teaching.\nHowever, it is not clear how collaborative programming tasks change peer\ncommunication patterns in the learning process. In this study, we compare\ncommunication patterns in MOOCs where peer communication is limited with those\nof a blended course in which students are involved in online peer instruction.\nWe used a mixed-method approach comprising automated text analysis and\ncommunity extraction with further qualitative analysis. The results show that\nstudents prefer to seek help in programming from peers and not the teacher.\nTeam assignment helped to support this habit. Students communicated more\npositively and intensively with each other, while only team leaders\ncommunicated with the instructor reducing teacher overload. This shift could\nexplain how peer communication improves learning outcomes, as has been shown in\nprevious studies on MOOCs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.09482,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Feel Old Yet? Updating Mode of Transportation Distributions from Travel\n  Surveys using Data Fusion with Mobile Phone Data\n\n  Up-to-date information on different modes of travel to monitor transport\ntraffic and evaluate rapid urban transport planning interventions is often\nlacking. Transport systems typically rely on traditional data sources providing\noutdated mode-of-travel data due to their data latency, infrequent data\ncollection and high cost. To address this issue, we propose a method that\nleverages mobile phone data as a cost-effective and rich source of geospatial\ninformation to capture current human mobility patterns at unprecedented\nspatiotemporal resolution. Our approach employs mobile phone application usage\ntraces to infer modes of transportation that are challenging to identify (bikes\nand ride-hailing\/taxi services) based on mobile phone location data. Using data\nfusion and matrix factorization techniques, we integrate official data sources\n(household surveys and census data) with mobile phone application usage data.\nThis integration enables us to reconstruct the official data and create an\nupdated dataset that incorporates insights from digital footprint data from\napplication usage. We illustrate our method using a case study focused on\nSantiago, Chile successfully inferring four modes of transportation:\nmass-transit, motorised, active, and taxi. Our analysis revealed significant\nchanges in transportation patterns between 2012 and 2020. We quantify a\nreduction in mass-transit usage across municipalities in Santiago, except where\nmetro\/rail lines have been more recently introduced, highlighting added\nresilience to the public transport network of these infrastructure\nenhancements. Additionally, we evidence an overall increase in motorised\ntransport throughout Santiago, revealing persistent challenges in promoting\nurban sustainable transportation. We validate our findings comparing our\nupdated estimates with official smart card transaction data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03789,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Broadening AI Ethics Narratives: An Indic Art View\n\n  Incorporating interdisciplinary perspectives is seen as an essential step\ntowards enhancing artificial intelligence (AI) ethics. In this regard, the\nfield of arts is perceived to play a key role in elucidating diverse historical\nand cultural narratives, serving as a bridge across research communities. Most\nof the works that examine the interplay between the field of arts and AI ethics\nconcern digital artworks, largely exploring the potential of computational\ntools in being able to surface biases in AI systems. In this paper, we\ninvestigate a complementary direction--that of uncovering the unique\nsocio-cultural perspectives embedded in human-made art, which in turn, can be\nvaluable in expanding the horizon of AI ethics. Through semi-structured\ninterviews across sixteen artists, art scholars, and researchers of diverse\nIndian art forms like music, sculpture, painting, floor drawings, dance, etc.,\nwe explore how {\\it non-Western} ethical abstractions, methods of learning, and\nparticipatory practices observed in Indian arts, one of the most ancient yet\nperpetual and influential art traditions, can shed light on aspects related to\nethical AI systems. Through a case study concerning the Indian dance system\n(i.e. the {\\it `Natyashastra'}), we analyze potential pathways towards\nenhancing ethics in AI systems. Insights from our study outline the need for\n(1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal\ndata formats for ethical AI system design and development, (3) viewing AI\nethics as a dynamic, diverse, cumulative, and shared process rather than as a\nstatic, self-contained framework to facilitate adaptability without\nannihilation of values (4) consistent life-long learning to enhance AI\naccountability\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03259,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Future Computer Systems and Networking Research in the Netherlands: A\n  Manifesto\n\n  Our modern society and competitive economy depend on a strong digital\nfoundation and, in turn, on sustained research and innovation in computer\nsystems and networks (CompSys). With this manifesto, we draw attention to\nCompSys as a vital part of ICT. Among ICT technologies, CompSys covers all the\nhardware and all the operational software layers that enable applications; only\napplication-specific details, and often only application-specific algorithms,\nare not part of CompSys. Each of the Top Sectors of the Dutch Economy, each\nroute in the National Research Agenda, and each of the UN Sustainable\nDevelopment Goals pose challenges that cannot be addressed without\ngroundbreaking CompSys advances. Looking at the 2030-2035 horizon, important\nnew applications will emerge only when enabled by CompSys developments.\nTriggered by the COVID-19 pandemic, millions moved abruptly online, raising\ninfrastructure scalability and data sovereignty issues; but governments\nprocessing social data and responsible social networks still require a paradigm\nshift in data sovereignty and sharing. AI already requires massive computer\nsystems which can cost millions per training task, but the current technology\nleaves an unsustainable energy footprint including large carbon emissions.\nComputational sciences such as bioinformatics, and \"Humanities for all\" and\n\"citizen data science\", cannot become affordable and efficient until computer\nsystems take a generational leap. Similarly, the emerging quantum internet\ndepends on (traditional) CompSys to bootstrap operation for the foreseeable\nfuture. Large commercial sectors, including finance and manufacturing, require\nspecialized computing and networking or risk becoming uncompetitive. And, at\nthe core of Dutch innovation, promising technology hubs, deltas, ports, and\nsmart cities, could see their promise stagger due to critical dependency on\nnon-European technology.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15406,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"From Explanation to Recommendation: Ethical Standards for Algorithmic\n  Recourse\n\n  People are increasingly subject to algorithmic decisions, and it is generally\nagreed that end-users should be provided an explanation or rationale for these\ndecisions. There are different purposes that explanations can have, such as\nincreasing user trust in the system or allowing users to contest the decision.\nOne specific purpose that is gaining more traction is algorithmic recourse. We\nfirst propose that recourse should be viewed as a recommendation problem, not\nan explanation problem. Then, we argue that the capability approach provides\nplausible and fruitful ethical standards for recourse. We illustrate by\nconsidering the case of diversity constraints on algorithmic recourse. Finally,\nwe discuss the significance and implications of adopting the capability\napproach for algorithmic recourse research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.00462,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"The use of Semantic Technologies in Computer Science Curriculum: A\n  Systematic Review\n\n  Semantic technologies are evolving and being applied in several research\nareas, including the education domain. This paper presents the outcomes of a\nsystematic review carried out to provide an overview of the application of\nsemantic technologies in the context of the Computer Science curriculum and\ndiscuss the limitations in this field whilst offering insights for future\nresearch. A total of 4,510 studies were reviewed, and 37 were analysed and\nreported. As a result, while semantic technologies have been increasingly used\nto develop Computer Science curricula, the alignment of ontologies and accurate\ncurricula assessment appears to be the most significant limitations to the\nwidespread adoption of such technologies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.09897,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"An Empirical Evaluation of the Implementation of the California Consumer\n  Privacy Act (CCPA)\n\n  On January 1, 2020, California passed the California Consumer Privacy Act\n(CCPA) by more than 56% of voters intended to enhance privacy rights and\nconsumer protection for residents of California, United States. Since then,\nmore conditions have been added to the Act to support consumers' privacy. In\naddition, two years after the first effective day of CCPA, consumers have seen\nCalifornia organizations apply approaches to adapt to CCPA. Many organizations\nquickly upgrade their policy to comply with the legislation and create\neffective platforms such as data portals that allow consumers to exercise their\nprivacy rights. However, on the other hand, we still noticed aspects of CCPA\nbeing absent on some websites. Additionally, we found no prior evaluation of\nthe CCPA implementation in organizations. Therefore, the convergence of the\nregulatory landscape and the organization's privacy policy needs to be studied.\nThis paper was about an empirical evaluation of the implementation of the\nCalifornia Consumer Privacy Act. The report includes the evaluations of the\nfollowing industries: social media, financial institutions, mortgages,\nhealthcare providers, and academic institutions. Our approach was to set up a\ncriteria table constructed from the CCPA Act and then use that table as a\nchecklist while reviewing a company's privacy notice. Finally, we concluded\nthis paper with an online tool application design that verifies the CCPA\nimplementation. Upon completion, the application would be free to use so\nconsumers can quickly inspect a website for CCPA compliance. Additionally, it\nis an advising tool that a website admin can utilize to enhance CCPA compliance\nfor their website. The conjunction of this empirical report and a practical\napplication function as a stimulus to promote CCPA implementation in\norganizations and deliver awareness to consumers about privacy rights they can\ndemand.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.04221,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000006689,
      "text":"The Forgotten Margins of AI Ethics\n\n  How has recent AI Ethics literature addressed topics such as fairness and\njustice in the context of continued social and structural power asymmetries? We\ntrace both the historical roots and current landmark work that have been\nshaping the field and categorize these works under three broad umbrellas: (i)\nthose grounded in Western canonical philosophy, (ii) mathematical and\nstatistical methods, and (iii) those emerging from critical\ndata\/algorithm\/information studies. We also survey the field and explore\nemerging trends by examining the rapidly growing body of literature that falls\nunder the broad umbrella of AI Ethics. To that end, we read and annotated\npeer-reviewed papers published over the past four years in two premier\nconferences: FAccT and AIES. We organize the literature based on an annotation\nscheme we developed according to three main dimensions: whether the paper deals\nwith concrete applications, use-cases, and\/or people's lived experience; to\nwhat extent it addresses harmed, threatened, or otherwise marginalized groups;\nand if so, whether it explicitly names such groups. We note that although the\ngoals of the majority of FAccT and AIES papers were often commendable, their\nconsideration of the negative impacts of AI on traditionally marginalized\ngroups remained shallow. Taken together, our conceptual analysis and the data\nfrom annotated papers indicate that the field would benefit from an increased\nfocus on ethical analysis grounded in concrete use-cases, people's experiences,\nand applications as well as from approaches that are sensitive to structural\nand historical power asymmetries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.1235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Telechain: Bridging Telecom Policy and Blockchain Practice\n\n  The use of blockchain in regulatory ecosystems is a promising approach to\naddress challenges of compliance among mutually untrusted entities. In this\nwork, we consider applications of blockchain technologies in telecom\nregulations. In particular, we address growing concerns around Unsolicited\nCommercial Communication (UCC aka. spam) sent through text messages (SMS) and\nphone calls in India. Despite several regulatory measures taken to curb the\nmenace of spam it continues to be a nuisance to subscribers while posing\nchallenges to telecom operators and regulators alike.\n  In this paper, we present a consortium blockchain based architecture to\naddress the problem of UCC in India. Our solution improves subscriber\nexperiences, improves the efficiency of regulatory processes while also\npositively impacting all stakeholders in the telecom ecosystem. Unlike previous\napproaches to the problem of UCC, which are all ex-post, our approach to\nadherence to the regulations is ex-ante. The proposal described in this paper\nis a primary contributor to the revision of regulations concerning UCC and spam\nby the Telecom Regulatory Authority of India (TRAI). The new regulations\npublished in July 2018 were first of a kind in the world and amended the 2010\nTelecom Commercial Communication Customer Preference Regulation (TCCCPR),\nthrough mandating the use of a blockchain\/distributed ledgers in addressing the\nUCC problem. In this paper, we provide a holistic account of of the projects'\nevolution from (1) its design and strategy, to (2) regulatory and policy\naction, (3) country wide implementation and deployment, and (4) evaluation and\nimpact of the work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.05521,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Comparison of Brick and Project Haystack to Support Smart Building\n  Applications\n\n  Enabling buildings with Smart Building applications will help to achieve the\nongoing efficient commissioning of buildings, ultimately attaining peak\nperformance in energy use and improved occupant health and comfort, at minimum\ncost. For these technologies to be scalable data ontology must be adopted to\nsemantically represent data generated by building mechanical systems, acting as\nconduit for connection to Smart Building applications. The viability of Brick\nand Project Haystack ontologies, as found by industry and academia, prompted a\nquantitative comparison of completeness and expressiveness using a case study\nwith an industry ontology as the baseline. Additionally, a qualitative\ncomparison was completed using key ontology qualities outlined in literature. A\nrecommendation of Brick is made based on results. Brick achieved higher\nassessment values in completeness and expressiveness achieving 59% and 100%\nrespectively, as compared to Haystacks 43% and 96%. Additionally, Brick\nexhibited five of six desirable qualities, where Haystack exhibited only three.\nThe recommendation of the appropriate ontology forms the basis for longer-term\nSmart Building application development, which will support innovative\napproaches to sustainability in building operations across scale, as well as\nnext-generation building controls and automation strategies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.1474,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Investigating Participation Mechanisms in EU Code Week\n\n  Digital competence (DC) is a broad set of skills, attitudes, and knowledge\nfor confident, critical and responsible use of digital technologies in every\naspect of life. DC is fundamental to all people in conducting a productive and\nfulfilling life in an increasingly digital world. However, prejudices,\nmisconceptions, and lack of awareness reduce the diffusion of DC, hindering\ndigital transformation and preventing countries and people from realising their\nfull potential. Teaching Informatics in the curriculum is increasingly\nsupported by the institutions but faces serious challenges, such as teacher\nupskilling and support, and will require several years to observe sizeable\noutcomes. In response, grassroots movements promoting computing literacy in an\ninformal setting have grown, including EU Code Week, whose vision is to develop\ncomputing skills while promoting diversity and raising awareness of the\nimportance of digital skills. Code Week participation is a form of public\nengagement that could be affected by socio-economic and demographic factors, as\nany other form of participation. The aim of the manuscript is twofold: first,\nto offer a detailed and comprehensive statistical description of Code Week's\nparticipation in the EU Member States in terms of penetration, retention,\ndemographic composition, and spatial distribution in order to inform more\neffective awareness-raising campaigns; second, to investigate the impact of\nsocio-economic factors on Code Week involvement. The study identifies a strong\nnegative correlation between participation and income at different geographical\nscales. It also suggests underlying mechanisms driving participation that are\ncoherent with the \"psychosocial\" and the \"resource\" views, i.e. the two most\nwidely accepted explanations of the effect of income on public engagement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03268,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Ubiquitous knowledge empowers the Smart Factory: The impacts of a\n  Service-oriented Digital Twin on enterprises' performance\n\n  While the Industry 4.0 is idolizing the potential of an artificial\nintelligence embedded into \"things\", it is neglecting the role of the human\ncomponent, which is still indispensable in different manufacturing activities,\nsuch as a machine setup or maintenance operations. The present research study\nfirst proposes an Industrial Internet pyramid as emergent human-centric\nmanufacturing paradigm within Industry 4.0 in which central is the role of a\nUbiquitous Knowledge about the manufacturing system intuitively accessed and\nused by the manufacturing employees. Second, the prototype of a\nService-oriented Digital Twin, which leverage on a flexible ontology-oriented\nknowledge structure and on augmented reality combined to a vocal interaction\nsystem for an intuitive knowledge retrieval and fruition, has been designed and\ndeveloped to deliver this manufacturing knowledge. Two test-beds, complimentary\nfor the problems in practice (the former on the maintenance-production\ninterface in a large enterprise, the latter majorly focused in production and\nsetups in a small and medium enterprise), show the significant benefits in\nterms of time, costs and process quality, thus validating the approach\nproposed. This research shows that a human-centric and knowledge-driven\napproach can drive the performance of Industry 4.0 initiatives and lead a Smart\nFactory towards its full potential.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.00771,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Local Differential Privacy Meets Computational Social Choice --\n  Resilience under Voter Deletion\n\n  The resilience of a voting system has been a central topic in computational\nsocial choice. Many voting rules, like plurality, are shown to be vulnerable as\nthe attacker can target specific voters to manipulate the result. What if a\nlocal differential privacy (LDP) mechanism is adopted such that the true\npreference of a voter is never revealed in pre-election polls? In this case,\nthe attacker can only infer stochastic information about a voter's true\npreference, and this may cause the manipulation of the electoral result\nsignificantly harder. The goal of this paper is to provide a quantitative study\non the effect of adopting LDP mechanisms on a voting system. We introduce the\nmetric PoLDP (power of LDP) that quantitatively measures the difference between\nthe attacker's manipulation cost under LDP mechanisms and that without LDP\nmechanisms. The larger PoLDP is, the more robustness LDP mechanisms can add to\na voting system. We give a full characterization of PoLDP for the voting system\nwith plurality rule and provide general guidance towards the application of LDP\nmechanisms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03215,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Prote\\c{c}\\~ao intelectual de obras produzidas por sistemas baseados em\n  intelig\\^encia artificial: uma vis\\~ao tecnicista sobre o tema\n\n  The pervasiveness of Artificial Intelligence (AI) is unquestionable in our\nsociety. Even in the arts, AI is present. A notorious case is the song \"Hey\nYa!\" of the OutKast group, successful in the 2000s. At this time, the music\nindustry began to make decisions based on data to strategize based on\npredictions of listeners' habits. This case is just one of the countless\nexamples of AI applications in the arts. The advent of deep learning made it\npossible to build systems capable of accurately recognizing artistic style in\npaintings. Content generation is also possible; for example, Deepart customizes\nimages from two \\textit{inputs}: 1) an image to be customized; 2) a style of\npainting. The generation of songs according to specific styles from AI-based\nsystems is also possible. Such possibilities raise questions about the\nintellectual property of such works. On this occasion, who owns the copyright\nof a work produced from a system based on Artificial Intelligence? To the\ncreator of the AI? The company\/corporation that subsidized the development of\nthis system? Or AI itself as a creator? This essay aims to contribute with a\ntechnicist view on the discussion of copyright applicability from works\nproduced by AI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03946,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"What does it mean to be a responsible AI practitioner: An ontology of\n  roles and skills\n\n  With the growing need to regulate AI systems across a wide variety of\napplication domains, a new set of occupations has emerged in the industry. The\nso-called responsible AI practitioners or AI ethicists are generally tasked\nwith interpreting and operationalizing best practices for ethical and safe\ndesign of AI systems. Due to the nascent nature of these roles, however, it is\nunclear to future employers and aspiring AI ethicists what specific function\nthese roles serve and what skills are necessary to serve the functions. Without\nclarity on these, we cannot train future AI ethicists with meaningful learning\nobjectives.\n  In this work, we examine what responsible AI practitioners do in the industry\nand what skills they employ on the job. We propose an ontology of existing\nroles alongside skills and competencies that serve each role. We created this\nontology by examining the job postings for such roles over a two-year period\n(2020-2022) and conducting expert interviews with fourteen individuals who\ncurrently hold such a role in the industry. Our ontology contributes to\nbusiness leaders looking to build responsible AI teams and provides educators\nwith a set of competencies that an AI ethics curriculum can prioritize.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.01553,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Why The Trans Programmer?\n\n  Through online anecdotal evidence and online communities, there is an\nin-group idea of trans people (specifically trans-feminine individuals)\ndisproportionately entering computer science education & fields. Existing data\nsuggests this is a plausible trend, yet no research has been done into exactly\nwhy. As computer science education (traditional schooling or self-taught\nmethods) is integral to working in computer science fields, a simple research\nsurvey was conducted to gather data on 138 trans people's experiences with\ncomputer science & computer science education. This article's purpose is to\nshed insight on the motivations for trans individuals choosing computer science\npaths, while acting as a basis and call to action for further research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10913,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Improved Healthcare Access in Low-resource Regions: A Review of\n  Technological Solutions\n\n  Technological advancements have led to significant improvements in healthcare\nfor prevention, diagnosis, treatments, and care. While resourceful regions can\ncapitalize on state-of-the-art healthcare technologies, there might be barriers\nand delays in technology-enabled healthcare availability for a low-resource\nregion. Unique innovations guided by the constraints of low-resource regions\nare required to truly make healthcare technologies ubiquitous and achieve the\ngoal of \"healthcare for all\". In this review, we identified several research\nand development works that have investigated technology-based healthcare\ninnovations targeted at low-resource regions. We found three main pillars of\nwork towards this end: low-cost hardware for the affordability of medical\ndevices, use of information and communication technology (ICT) tools for\nscalability and operational efficiencies in healthcare services, and mobile\nhealth solutions. Several emerging technologies are also promising for\nhealthcare in low-resource regions, such as artificial intelligence, the\nInternet of Things (IoT), and blockchain technology. We discuss these emerging\ntechnologies too in this review.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03263,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000079142,
      "text":"Embedded Systems Education in the 2020s: Challenges, Reflections, and\n  Future Directions\n\n  Embedded computing systems are pervasive in our everyday lives, imparting\ndigital intelligence to a variety of electronic platforms used in our vehicles,\nsmart appliances, wearables, mobile devices, and computers. The need to train\nthe next generation of embedded systems designers and engineers with relevant\nskills across hardware, software, and their co-design remains pressing today.\nThis paper describes the evolution of embedded systems education over the past\ntwo decades and challenges facing the designers and instructors of embedded\nsystems curricula in the 2020s. Reflections from over a decade of teaching the\ndesign of embedded computing systems are presented, with insights on strategies\nthat show promise to address these challenges. Lastly, some important future\ndirections in embedded systems education are highlighted.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.12098,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000019537,
      "text":"COVID-19: An exploration of consecutive systemic barriers to\n  pathogen-related data sharing during a pandemic\n\n  In 2020, the COVID-19 pandemic resulted in a rapid response from governments\nand researchers worldwide. As of late 2023, over millions have died as a result\nof COVID-19, with many COVID-19 survivors going on to experience long-term\neffects weeks, months, or years after their illness. Despite this staggering\ntoll, those who work with pandemic-relevant data often face significant\nsystemic barriers to accessing, sharing or re-using this data. In this paper we\nreport results of a study, where we interviewed data professionals working with\nCOVID-19-relevant data types including social media, mobility, viral genome,\ntesting, infection, hospital admission, and deaths. These data types are\nvariously used for pandemic spread modelling, healthcare system strain\nawareness, and devising therapeutic treatments for COVID-19. Barriers to data\naccess, sharing and re-use include the cost of access to data (primarily\ncertain healthcare sources and mobility data from mobile phone carriers), human\nthroughput bottlenecks, unclear pathways to request access to data,\nunnecessarily strict access controls and data re-use policies, unclear data\nprovenance, inability to link separate data sources that could collectively\ncreate a more complete picture, poor adherence to metadata standards, and a\nlack of computer-suitable data formats.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03212,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Dependency, Data and Decolonisation: A Framework for Decolonial Thinking\n  in Collaborative AI Research\n\n  This essay seeks to tie together thoughts on the political economy of\nacademia, the inequities in access to the academic means of production and\ndecolonial practice in data empowerment. To demonstrate this I will provide a\nbrief analysis of the neo-colonial, extractive practices of the Western\nAcademy, introduce concepts around decolonial AI practice and then use these to\nform an investigative framework. Using this framework, I present a brief case\nstudy of the AirQo project in Kampala, Uganda. The project aims to deploy a\nlow-cost air pollution sensor network across the city, using machine learning\nmethods to calibrate these sensors against reference instruments, providing\nhigh-quality air pollution data at a far lower cost.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.1429,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000344714,
      "text":"Building net-native agreement systems\n\n  Agreements and contracts are everywhere, but they are built on layers and\nlayers of legal and social institutions. Software is slowly entering into this\nstack. In this article, we introduce agreement paths, a general model for\nunderstanding and decomposing digital agreement systems, and Agreement Engine,\nan open-source software service for building net-native agreement systems. We\ndemonstrate Agreement Engine by building two example agreement systems: Scarce\nKnowledge, an app for crowdfunding essays, and Twitter Social Capital, a bot\nthat allows users to form and enforce Twitter agreements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.13885,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"YouTubers Not madeForKids: Detecting Channels Sharing Inappropriate\n  Videos Targeting Children\n\n  In the last years, hundreds of new Youtube channels have been creating and\nsharing videos targeting children, with themes related to animation, superhero\nmovies, comics, etc. Unfortunately, many of these videos are inappropriate for\nconsumption by their target audience, due to disturbing, violent, or sexual\nscenes. In this paper, we study YouTube channels found to post suitable or\ndisturbing videos targeting kids in the past. We identify a clear discrepancy\nbetween what YouTube assumes and flags as inappropriate content and channel,\nvs. what is found to be disturbing content and still available on the platform,\ntargeting kids. In particular, we find that almost 60\\% of videos that were\nmanually annotated and classified as disturbing by an older study in 2019 (a\ncollection bootstrapped with Elsa and other keywords related to children\nvideos), are still available on YouTube in mid 2021. In the meantime, 44% of\nchannels that uploaded such disturbing videos, have yet to be suspended and\ntheir videos to be removed. For the first time in literature, we also study the\n\"madeForKids\" flag, a new feature that YouTube introduced in the end of 2019,\nand compare its application to the channels that shared disturbing videos, as\nflagged from the previous study. Apparently, these channels are less likely to\nbe set as \"madeForKids\" than those sharing suitable content. In addition,\nchannels posting disturbing videos utilize their channel features such as\nkeywords, description, topics, posts, etc., to appeal to kids (e.g., using\ngame-related keywords). Finally, we use a collection of such channel and\ncontent features to train ML classifiers able to detect, at channel creation\ntime, when a channel will be related to disturbing content uploads. These\nclassifiers can help YouTube moderators reduce such incidences, pointing to\npotentially suspicious accounts without analyzing actual videos.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03739,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000111924,
      "text":"Airport Digital Twins for Resilient Disaster Management Response\n\n  Airports are constantly facing a variety of hazards and threats from natural\ndisasters to cybersecurity attacks and airport stakeholders are confronted with\nmaking operational decisions under irregular conditions. We introduce the\nconcept of the foundational twin, which can serve as a resilient data platform,\nincorporating multiple data sources and enabling the interaction between an\numbrella of twins. We then focus on providing data sources and metrics for each\nfoundational twin, with an emphasis on the environmental airport twin for major\nUS airports.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01513,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Mitigating Sovereign Data Exchange Challenges: A Mapping to Apply\n  Privacy- and Authenticity-Enhancing Technologies\n\n  Harmful repercussions from sharing sensitive or personal data can hamper\ninstitutions' willingness to engage in data exchange. Thus, institutions\nconsider Authenticity Enhancing Technologies (AETs) and Privacy-Enhancing\nTechnologies (PETs) to engage in Sovereign Data Exchange (SDE), i.e., sharing\ndata with third parties without compromising their own or their users' data\nsovereignty. However, these technologies are often technically complex, which\nimpedes their adoption. To support practitioners select PETs and AETs for SDE\nuse cases and highlight SDE challenges researchers and practitioners should\naddress, this study empirically constructs a challenge-oriented technology\nmapping. First, we compile challenges of SDE by conducting a systematic\nliterature review and expert interviews. Second, we map PETs and AETs to the\nSDE challenges and identify which technologies can mitigate which challenges.\nWe validate the mapping through investigator triangulation. Although the most\ncritical challenge concerns data usage and access control, we find that the\nmajority of PETs and AETs focus on data processing issues.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01493,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"AI Ethics: An Empirical Study on the Views of Practitioners and\n  Lawmakers\n\n  Artificial Intelligence (AI) solutions and technologies are being\nincreasingly adopted in smart systems context, however, such technologies are\ncontinuously concerned with ethical uncertainties. Various guidelines,\nprinciples, and regulatory frameworks are designed to ensure that AI\ntechnologies bring ethical well-being. However, the implications of AI ethics\nprinciples and guidelines are still being debated. To further explore the\nsignificance of AI ethics principles and relevant challenges, we conducted a\nsurvey of 99 representative AI practitioners and lawmakers (e.g., AI engineers,\nlawyers) from twenty countries across five continents. To the best of our\nknowledge, this is the first empirical study that encapsulates the perceptions\nof two different types of population (AI practitioners and lawmakers) and the\nstudy findings confirm that transparency, accountability, and privacy are the\nmost critical AI ethics principles. On the other hand, lack of ethical\nknowledge, no legal frameworks, and lacking monitoring bodies are found the\nmost common AI ethics challenges. The impact analysis of the challenges across\nAI ethics principles reveals that conflict in practice is a highly severe\nchallenge. Moreover, the perceptions of practitioners and lawmakers are\nstatistically correlated with significant differences for particular principles\n(e.g. fairness, freedom) and challenges (e.g. lacking monitoring bodies,\nmachine distortion). Our findings stimulate further research, especially\nempowering existing capability maturity models to support the development and\nquality assessment of ethics-aware AI systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000177821,
      "text":"Oxford-style Debates in Telecommunication and Computer Science Education\n\n  Oxford-style debating is a well-known tool in social sciences. Such formal\ndiscussions on particular topics are widely used by historians and\nsociologists. However, when we try to go beyond standard thinking, it turns out\nthat Oxford-style debating can be a great educational tool in telecommunication\nand computer science. This article presents this unusual method of education at\ntechnical universities and in the IT industry, and describes its features and\nchallenges. Best practices and examples of debating are provided, taking into\naccount emerging topics in telecommunications and computer science, such as\ncybersecurity. The article also contains feedback from IT engineers who\nparticipated in Oxford-style debates. All this aims to encourage this form of\neducation in telecommunication and computer science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.06149,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Tackling Algorithmic Disability Discrimination in the Hiring Process: An\n  Ethical, Legal and Technical Analysis\n\n  Tackling algorithmic discrimination against persons with disabilities (PWDs)\ndemands a distinctive approach that is fundamentally different to that applied\nto other protected characteristics, due to particular ethical, legal, and\ntechnical challenges. We address these challenges specifically in the context\nof artificial intelligence (AI) systems used in hiring processes (or automated\nhiring systems, AHSs), in which automated assessment procedures are subject to\nunique ethical and legal considerations and have an undeniable adverse impact\non PWDs. In this paper, we discuss concerns and opportunities raised by\nAI-driven hiring in relation to disability discrimination. Ultimately, we aim\nto encourage further research into this topic. Hence, we establish some\nstarting points and design a roadmap for ethicists, lawmakers, advocates as\nwell as AI practitioners alike.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08697,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Reviewer Preferences and Gender Disparities in Aesthetic Judgments\n\n  Aesthetic preferences are considered highly subjective resulting in\ninherently noisy judgements of aesthetic objects, yet certain aspects of\naesthetic judgement display convergent trends over time. This paper present a\nstudy that uses literary reviews as a proxy for aesthetic judgement in order to\nidentify systematic components that can be attributed to bias. Specifically we\nfind that judgement of literary quality in newspapers displays a gender bias in\npreference of male writers. Male reviewers have a same gender preference while\nfemale reviewer show an opposite gender preference. While alternative accounts\nexist of this apparent gender disparity, we argue that it reflects a cultural\ngender antagonism.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0123,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Formalizing Human Ingenuity: A Quantitative Framework for Copyright\n  Law's Substantial Similarity\n\n  A central notion in U.S. copyright law is judging the substantial similarity\nbetween an original and an (allegedly) derived work. Capturing this notion has\nproven elusive, and the many approaches offered by case law and legal\nscholarship are often ill-defined, contradictory, or internally-inconsistent.\n  This work suggests that key parts of the substantial-similarity puzzle are\namendable to modeling inspired by theoretical computer science. Our proposed\nframework quantitatively evaluates how much \"novelty\" is needed to produce the\nderived work with access to the original work, versus reproducing it without\naccess to the copyrighted elements of the original work. \"Novelty\" is captured\nby a computational notion of description length, in the spirit of\nKolmogorov-Levin complexity, which is robust to mechanical transformations and\navailability of contextual information.\n  This results in an actionable framework that could be used by courts as an\naid for deciding substantial similarity. We evaluate it on several pivotal\ncases in copyright law and observe that the results are consistent with the\nrulings, and are philosophically aligned with the\nabstraction-filtration-comparison test of Altai.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01502,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Is there an Ethical Operational Research?\n\n  The ethical dimension of Operational Research and Decision Aiding, although\nnot a new subject, turns to be a concern, both for the large public and the OR\ncommunity, because of the wide spread of autonomous artefacts endowed with\ndecision capacity thanks to the use of models, methods and tools developed\nwithin our field. The paper addresses the question of whether there exists an\n\"Ethical Operational Research\", identifies the ethical questions which are\nspecific to our professional community and suggests research topics which,\nalthough independently developed, are relevant for handling such questions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02492,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"Terms-we-Serve-with: a feminist-inspired social imaginary for improved\n  transparency and engagement in AI\n\n  Power and information asymmetries between people and digital technology\ncompanies have predominantly been legitimized through contractual agreements\nthat have failed to provide diverse people with meaningful consent and\ncontestability. We offer an interdisciplinary multidimensional perspective on\nthe future of regulatory frameworks - the Terms-we-Serve-with (TwSw) social,\ncomputational, and legal contract for restructuring power asymmetries and\ncenter-periphery dynamics to enable improved human agency in individual and\ncollective experiences of algorithmic harms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.03273,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000058611,
      "text":"City-scale synthetic individual-level vehicle trip data\n\n  Trip data that records each vehicle's trip activity on the road network\ndescribes the operation of urban traffic from the individual perspective, and\nit is extremely valuable for transportation research. However, restricted by\ndata privacy, the trip data of individual-level cannot be opened for all\nresearchers, while the need for it is very urgent. In this paper, we produce a\ncity-scale synthetic individual-level vehicle trip dataset by generating for\neach individual based on the historical trip data, where the availability and\ntrip data privacy protection are balanced. Privacy protection inevitably\naffects the availability of data. Therefore, we have conducted numerous\nexperiments to demonstrate the performance and reliability of the synthetic\ndata in different dimensions and at different granularities to help users\nproperly judge the tasks it can perform. The result shows that the synthetic\ndata is consistent with the real data (i.e., historical data) on the aggregated\nlevel and reasonable from the individual perspective.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.13281,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Extracting Large Scale Spatio-Temporal Descriptions from Social Media\n\n  The ability to track large-scale events as they happen is essential for\nunderstanding them and coordinating reactions in an appropriate and timely\nmanner. This is true, for example, in emergency management and decision-making\nsupport, where the constraints on both quality and latency of the extracted\ninformation can be stringent. In some contexts, real-time and large-scale\nsensor data and forecasts may be available. We are exploring the hypothesis\nthat this kind of data can be augmented with the ingestion of semi-structured\ndata sources, like social media. Social media can diffuse valuable knowledge,\nsuch as direct witness or expert opinions, while their noisy nature makes them\nnot trivial to manage. This knowledge can be used to complement and confirm\nother spatio-temporal descriptions of events, highlighting previously unseen or\nundervalued aspects. The critical aspects of this investigation, such as event\nsensing, multilingualism, selection of visual evidence, and geolocation, are\ncurrently being studied as a foundation for a unified spatio-temporal\nrepresentation of multi-modal descriptions. The paper presents, together with\nan introduction on the topics, the work done so far on this line of research,\nalso presenting case studies relevant to the posed challenges, focusing on\nemergencies caused by natural disasters.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01512,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Advancing Education Through Extended Reality and Internet of Everything\n  Enabled Metaverses: Applications, Challenges, and Open Issues\n\n  Metaverse has evolved as one of the popular research agendas that let the\nusers learn, socialize, and collaborate in a networked 3D immersive virtual\nworld. Due to the rich multimedia streaming capability and immersive user\nexperience with high-speed communication, the metaverse is an ideal model for\neducation, training, and skill development tasks. To facilitate research in\nthis area, we provide a comprehensive review of the various educational use\ncases and explore how enabling technologies such as Extended reality (XR) and\nInternet of Everything (IoE) will play a major role in educational services in\nfuture metaverses. Secondly, we provide an overview of metaverse-based\neducational applications focusing on education, training, and skill development\nand analyze the technologies they are built upon. We identify common research\nproblems and future research directions in the domain. The paper also\nidentifies core ethical considerations of metaverse for education and potential\npitfalls. We believe this survey can fully demonstrate the versatility of\nmetaverse-driven education, which could serve as a potential guideline for the\nresearchers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02848,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Plagiarism deterrence for introductory programming\n\n  Plagiarism in introductory programming courses is an enormous challenge for\nboth students and institutions. For students, relying on the work of others too\nearly in their academic development can make it impossible to acquire necessary\nskills for independent success in the future. For institutions, widespread\nstudent cheating can dilute the quality of the educational experience being\noffered. Currently available solutions consider only pairwise comparisons\nbetween student submissions and focus on punitive deterrence. Our approach\ninstead relies on a class-wide statistical characterization that can be clearly\nand securely shared with students via an intuitive new p-value representing\nindependence of student effort. A pairwise, compression-based similarity\ndetection algorithm captures relationships between assignments more accurately.\nAn automated deterrence system is used to warn students that their behavior is\nbeing closely monitored. High-confidence instances are made directly available\nfor instructor review using our open-source toolkit. An unbiased scoring system\naids students and the instructor in understanding true independence of effort.\nPreliminary results indicate that the system can provide meaningful\nmeasurements of independence from week one, improving the efficacy of technical\neducation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01489,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000050002,
      "text":"Integration of Remote Patient Monitoring Systems into Physicians Work in\n  Underserved Communities: Survey of Healthcare Provider Perspectives\n\n  Remote patient monitoring (RPM) technologies have been identified as a viable\nalternative to improve access to care in underserved communities. Successful\nRPM platforms are designed and implemented for seamless integration into\nhealthcare providers work to increase adoption and availability for offering\nremote care. A quantitative survey was designed and administered to elicit\nperspectives from a wide range of stakeholders, including healthcare providers\nand healthcare administrators, about barriers and facilitators in the adoption\nand integration of RPM into clinical workflows in underserved areas. Ease of\nadoption, workflow disruption, changes in the patient-physician relationship,\nand costs and financial benefits are identified as relevant factors that\ninfluence the widespread use of RPM by healthcare providers; significant\ncommunication and other implementation preferences also emerged. Further\nresearch is needed to identify methods to address such concerns and use\ninformation collected in this study to develop protocols for RPM integration\ninto clinical workflow.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.11822,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"DeepSafety:Multi-level Audio-Text Feature Extraction and Fusion Approach\n  for Violence Detection in Conversations\n\n  Natural Language Processing has recently made understanding human interaction\neasier, leading to improved sentimental analysis and behaviour prediction.\nHowever, the choice of words and vocal cues in conversations presents an\nunderexplored rich source of natural language data for personal safety and\ncrime prevention. When accompanied by audio analysis, it makes it possible to\nunderstand the context of a conversation, including the level of tension or\nrift between people. Building on existing work, we introduce a new information\nfusion approach that extracts and fuses multi-level features including verbal,\nvocal, and text as heterogeneous sources of information to detect the extent of\nviolent behaviours in conversations. Our multilevel multimodel fusion framework\nintegrates four types of information from raw audio signals including\nembeddings generated from both BERT and Bi-long short-term memory (LSTM) models\nalong with the output of 2D CNN applied to Mel-frequency Cepstrum (MFCC) as\nwell as the output of audio Time-Domain dense layer. The embeddings are then\npassed to three-layer FC networks, which serve as a concatenated step. Our\nexperimental setup revealed that the combination of the multi-level features\nfrom different modalities achieves better performance than using a single one\nwith F1 Score=0.85. We expect that the findings derived from our method\nprovides new approaches for violence detection in conversations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08202,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000081129,
      "text":"Token Spammers, Rug Pulls, and SniperBots: An Analysis of the Ecosystem\n  of Tokens in Ethereum and in the Binance Smart Chain (BNB)\n\n  In this work, we perform a longitudinal analysis of the BNB Smart Chain and\nEthereum blockchain from their inception to March 2022. We study the ecosystem\nof the tokens and liquidity pools, highlighting analogies and differences\nbetween the two blockchains. We discover that about 60% of tokens are active\nfor less than one day. Moreover, we find that 1% of addresses create an\nanomalous number of tokens (between 20% and 25%). We discover that these tokens\nare used as disposable tokens to perform a particular type of rug pull, which\nwe call 1-day rug pull. We quantify the presence of this operation on both\nblockchains discovering its prevalence on the BNB Smart Chain. We estimate that\n1-day rug pulls generated $240 million in profits. Finally, we present sniper\nbots, a new kind of trader bot involved in these activities, and we detect\ntheir presence and quantify their activity in the rug pull operations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08971,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Capture the Flag for Team Construction in Cybersecurity\n\n  Team collaboration among individuals with diverse sets of expertise and\nskills is essential for solving complex problems. As part of an\ninterdisciplinary effort, we studied the effects of Capture the Flag (CTF)\ngame, a popular and engaging education\/training tool in cybersecurity and\nengineering, in enhancing team construction and collaboration. We developed a\nframework to incorporate CTF as part of a computer-human process for expertise\nrecognition and role assignment and evaluated and tested its effectiveness\nthrough a study with cybersecurity students enrolled in a Virtual Teams course.\nIn our computer-human process framework, the post-CTF algorithm using the CTF\noutcomes assembles the team (assigning individuals to teams) and provides the\ninitial role assignments, which then gets updated by human-based team\ndiscussions. This paper shares our insights, design choices\/rationales, and\nanalyses of our CTF-incorporated computer-human process framework. The\nstudents' evaluations revealed that the computer-human process framework was\nhelpful in learning about their team members' backgrounds and expertise and\nassigning roles accordingly made a positive impact on the learning outcomes for\nthe team collaboration skills in the course. This experience report showcases\nthe utility of CTF as a tool for expertise recognition and role assignments in\nteams and highlights the complementary roles of CTF-based and discussion-based\nprocesses for an effective team collaboration among engineering students.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.07992,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Deconstructing written rules and hierarchy in peer produced software\n  communities\n\n  We employ recent advances in computational institutional analysis and NLP to\ninvestigate the systems of authority that are reflected in the written policy\ndocuments of the ASF. Our study to decipher the effective similarities or\ndepartures of the ASF model from conventional software companies reveals\nevidence of both flat and bureaucratic governance in a peer production set up,\nsuggesting a complicated relationship between business-based theories of\nadministrative hierarchy and foundational principles of the OSS movement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01498,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"(Meta) Competences for Digital Practice: Educational Scenarios for the\n  Workplace of the Future Exemplified by Building Information Modeling Work\n  Processes\n\n  Workplaces of the future require advanced competence profiles from employees,\nnot least due to new options for teleworking and new complex digital tools. The\nacquisition of advanced competence profiles is to be addressed by formal\neducation. For example, the method of Building Information Modeling (BIM) aims\nat digitizing the design, construction, and operation of structures and as such\nrequires advanced competence profiles. In this study, two educational scenarios\nbased on teleworking and complex digital tools are compared, each with one\ncohort and consisting of two learning activities. The first cohort initially\ncompletes as first learning activity a semester-long course that aims at BIM\ndomain competences. The semester-long course of the second cohort fosters meta\ncompetences, such as communication, collaboration, and digital literacy. At the\nend of the semester, both cohorts solve in a second learning activity a BIM\npractice task. Research questions are: (1) Do the two educational scenarios\npromote the competences to be addressed? And related: (2) What is the impact of\nthe initial course that fosters domain competences or meta competences?\nMethodologically, the learning outcomes are assessed by measuring the domain\ncompetences three times during the educational scenario using online tests in\nthe two cohorts (N=11). Further, students' perceptions are surveyed in parallel\nusing online questionnaires. In addition, semi-structured interviews are\nconducted at the end of the educational scenarios. The quantitative and\nqualitative results of the study designating the training of meta competencies\npartly as a substitute for imparting domain competences are presented. Further,\nthe influence of both educational scenarios on competence development for the\nworkplace of the future characterized by telework and digital tools is\ndiscussed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.11068,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"AI Challenges for Society and Ethics\n\n  Artificial intelligence is already being applied in and impacting many\nimportant sectors in society, including healthcare, finance, and policing.\nThese applications will increase as AI capabilities continue to progress, which\nhas the potential to be highly beneficial for society, or to cause serious\nharm. The role of AI governance is ultimately to take practical steps to\nmitigate this risk of harm while enabling the benefits of innovation in AI.\nThis requires answering challenging empirical questions about current and\npotential risks and benefits of AI: assessing impacts that are often widely\ndistributed and indirect, and making predictions about a highly uncertain\nfuture. It also requires thinking through the normative question of what\nbeneficial use of AI in society looks like, which is equally challenging.\nThough different groups may agree on high-level principles that uses of AI\nshould respect (e.g., privacy, fairness, and autonomy), challenges arise when\nputting these principles into practice. For example, it is straightforward to\nsay that AI systems must protect individual privacy, but there is presumably\nsome amount or type of privacy that most people would be willing to give up to\ndevelop life-saving medical treatments. Despite these challenges, research can\nand has made progress on these questions. The aim of this chapter will be to\ngive readers an understanding of this progress, and of the challenges that\nremain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.04132,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Forecasting AI Progress: Evidence from a Survey of Machine Learning\n  Researchers\n\n  Advances in artificial intelligence (AI) are shaping modern life, from\ntransportation, health care, science, finance, to national defense. Forecasts\nof AI development could help improve policy- and decision-making. We report the\nresults from a large survey of AI and machine learning (ML) researchers on\ntheir beliefs about progress in AI. The survey, fielded in late 2019, elicited\nforecasts for near-term AI development milestones and high- or human-level\nmachine intelligence, defined as when machines are able to accomplish every or\nalmost every task humans are able to do currently. As part of this study, we\nre-contacted respondents from a highly-cited study by Grace et al. (2018), in\nwhich AI\/ML researchers gave forecasts about high-level machine intelligence\nand near-term milestones in AI development. Results from our 2019 survey show\nthat, in aggregate, AI\/ML researchers surveyed placed a 50% likelihood of\nhuman-level machine intelligence being achieved by 2060. The results show\nresearchers newly contacted in 2019 expressed similar beliefs about the\nprogress of advanced AI as respondents in the Grace et al. (2018) survey. For\nthe recontacted participants from the Grace et al. (2018) study, the aggregate\nforecast for a 50% likelihood of high-level machine intelligence shifted from\n2062 to 2076, although this change is not statistically significant, likely due\nto the small size of our panel sample. Forecasts of several near-term AI\nmilestones have reduced in time, suggesting more optimism about AI progress.\nFinally, AI\/ML researchers also exhibited significant optimism about how\nhuman-level machine intelligence will impact society.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.14086,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Ever heard of ethical AI? Investigating the salience of ethical AI\n  issues among the German population\n\n  Building and implementing ethical AI systems that benefit the whole society\nis cost-intensive and a multi-faceted task fraught with potential problems.\nWhile computer science focuses mostly on the technical questions to mitigate\nsocial issues, social science addresses citizens' perceptions to elucidate\nsocial and political demands that influence the societal implementation of AI\nsystems. Thus, in this study, we explore the salience of AI issues in the\npublic with an emphasis on ethical criteria to investigate whether it is likely\nthat ethical AI is actively requested by the population. Between May 2020 and\nApril 2021, we conducted 15 surveys asking the German population about the most\nimportant AI-related issues (total of N=14,988 respondents). Our results show\nthat the majority of respondents were not concerned with AI at all. However, it\ncan be seen that general interest in AI and a higher educational level are\npredictive of some engagement with AI. Among those, who reported having thought\nabout AI, specific applications (e.g., autonomous driving) were by far the most\nmentioned topics. Ethical issues are voiced only by a small subset of citizens\nwith fairness, accountability, and transparency being the least mentioned ones.\nThese have been identified in several ethical guidelines (including the EU\nCommission's proposal) as key elements for the development of ethical AI. The\nsalience of ethical issues affects the behavioral intentions of citizens in the\nway that they 1) tend to avoid AI technology and 2) engage in public\ndiscussions about AI. We conclude that the low level of ethical implications\nmay pose a serious problem for the actual implementation of ethical AI for the\nCommon Good and emphasize that those who are presumably most affected by\nethical issues of AI are especially unaware of ethical risks. Yet, once ethical\nAI is top of the mind, there is some potential for activism.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.04312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"At the Intersection of Deep Learning and Conceptual Art: The End of\n  Signature\n\n  MIT wanted to commission a large scale artwork that would serve to\n'illuminate a new campus gateway, inaugurate a space of exchange between MIT\nand Cambridge, and inspire our students, faculty, visitors, and the surrounding\ncommunity to engage with art in new ways and to have art be part of their daily\nlives.' Among other things, the art was to reflect the fact that scientific\ndiscovery is often the result of many individual contributions, both\nacknowledged and unacknowledged. In this work, a group of computer scientists\ncollaborated with a conceptual artist to produce a collective signature, or a\nsignature learned from contributions of an entire community. After collecting\nsignatures from two communities -- the university, and the surrounding city --\nthe computer scientists developed generative models and a human-in-the-loop\nfeedback process to work with the artist create an original signature-like\nstructure representative of each community. These signatures are now\nlarge-scale steel, LED and neon light sculptures that appear to sign two new\nbuildings in Cambridge, MA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08169,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000080466,
      "text":"Ethnic Representation Analysis of Commercial Movie Posters\n\n  In the last decades, global awareness towards the importance of diverse\nrepresentation has been increasing. Lack of diversity and discrimination toward\nminorities did not skip the film industry. Here, we examine ethnic bias in the\nfilm industry through commercial posters, the industry's primary advertisement\nmedium for decades. Movie posters are designed to establish the viewer's\ninitial impression. We developed a novel approach for evaluating ethnic bias in\nthe film industry by analyzing nearly 125,000 posters using state-of-the-art\ndeep learning models. Our analysis shows that while ethnic biases still exist,\nthere is a trend of reduction of bias, as seen by several parameters.\nParticularly in English-speaking movies, the ethnic distribution of characters\non posters from the last couple of years is reaching numbers that are\napproaching the actual ethnic composition of US population. An automatic\napproach to monitor ethnic diversity in the film industry, potentially\nintegrated with financial value, may be of significant use for producers and\npolicymakers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.09592,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"Inclusive Privacy Design for Older Adults Living in Ambient Assisted\n  Living\n\n  Ambient assisted living (AAL) environments support independence and quality\nof life of older adults However, in an AAL environment, privacy-related issues\n(e.g., unawareness, information disclosure, and lack of support) directly\nimpact older adults and bystanders (e.g., caregivers, service providers, etc.).\nWe explore the privacy challenges that both older adults and bystanders face in\nAAL. We call for inclusive privacy design and recommend following areas of\nimprovement: consent, notification, and consideration for cultural differences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.0259,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000071857,
      "text":"MetroGAN: Simulating Urban Morphology with Generative Adversarial\n  Network\n\n  Simulating urban morphology with location attributes is a challenging task in\nurban science. Recent studies have shown that Generative Adversarial Networks\n(GANs) have the potential to shed light on this task. However, existing\nGAN-based models are limited by the sparsity of urban data and instability in\nmodel training, hampering their applications. Here, we propose a GAN framework\nwith geographical knowledge, namely Metropolitan GAN (MetroGAN), for urban\nmorphology simulation. We incorporate a progressive growing structure to learn\nhierarchical features and design a geographical loss to impose the constraints\nof water areas. Besides, we propose a comprehensive evaluation framework for\nthe complex structure of urban systems. Results show that MetroGAN outperforms\nthe state-of-the-art urban simulation methods by over 20% in all metrics.\nInspiringly, using physical geography features singly, MetroGAN can still\ngenerate shapes of the cities. These results demonstrate that MetroGAN solves\nthe instability problem of previous urban simulation GANs and is generalizable\nto deal with various urban attributes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.09593,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Privacy Threats on the Internet of Medical Things\n\n  The Internet of Medical Things (IoMT) is a frequent target of attacks --\ncompromising both patient data and healthcare infra-structure. While\nprivacy-enhanced technologies and services (PETS) are developed to mitigate\ntraditional privacy concerns, they cannot be applied without identifying\nspecific threat models. Therefore, our position is that the new threat\nland-scape created by the relatively new and underexplored IoMT domain must be\nstudied. We briefly discuss specific privacy threats and threat actors in IoMT.\nFurthermore, we argue that the privacy policy gap needs to be identified for\nthe IoMT threat landscape.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.09525,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Crowdsourcing Impacts: Exploring the Utility of Crowds for Anticipating\n  Societal Impacts of Algorithmic Decision Making\n\n  With the increasing pervasiveness of algorithms across industry and\ngovernment, a growing body of work has grappled with how to understand their\nsocietal impact and ethical implications. Various methods have been used at\ndifferent stages of algorithm development to encourage researchers and\ndesigners to consider the potential societal impact of their research. An\nunderstudied yet promising area in this realm is using participatory foresight\nto anticipate these different societal impacts. We employ crowdsourcing as a\nmeans of participatory foresight to uncover four different types of impact\nareas based on a set of governmental algorithmic decision making tools: (1)\nperceived valence, (2) societal domains, (3) specific abstract impact types,\nand (4) ethical algorithm concerns. Our findings suggest that this method is\neffective at leveraging the cognitive diversity of the crowd to uncover a range\nof issues. We further analyze the complexities within the interaction of the\nimpact areas identified to demonstrate how crowdsourcing can illuminate\npatterns around the connections between impacts. Ultimately this work\nestablishes crowdsourcing as an effective means of anticipating algorithmic\nimpact which complements other approaches towards assessing algorithms in\nsociety by leveraging participatory foresight and cognitive diversity.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06313,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Social media sharing by political elites: An asymmetric American\n  exceptionalism\n\n  Increased sharing of untrustworthy information on social media platforms is\none of the main challenges of our modern information society. Because\ninformation disseminated by political elites is known to shape citizen and\nmedia discourse, it is particularly important to examine the quality of\ninformation shared by politicians. Here we show that from 2016 onward, members\nof the Republican party in the U.S. Congress have been increasingly sharing\nlinks to untrustworthy sources. The proportion of untrustworthy information\nposted by Republicans versus Democrats is diverging at an accelerating rate,\nand this divergence has worsened since president Biden was elected. This\ndivergence between parties seems to be unique to the U.S. as it cannot be\nobserved in other western democracies such as Germany and the United Kingdom,\nwhere left-right disparities are smaller and have remained largely constant.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12943,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Unique in what sense? Heterogeneous relationships between multiple types\n  of uniqueness and popularity in music\n\n  How does our society appreciate the uniqueness of cultural products? This\nfundamental puzzle has intrigued scholars in many fields, including psychology,\nsociology, anthropology, and marketing. It has been theorized that cultural\nproducts that balance familiarity and novelty are more likely to become\npopular. However, a cultural product's novelty is typically multifaceted. This\npaper uses songs as a case study to study the multiple facets of uniqueness and\ntheir relationship with success. We first unpack the multiple facets of a\nsong's novelty or uniqueness and, next, measure its impact on a song's\npopularity. We employ a series of statistical models to study the relationship\nbetween a song's popularity and novelty associated with its lyrics, chord\nprogressions, or audio properties. Our analyses performed on a dataset of over\nfifty thousand songs find a consistently negative association between all types\nof song novelty and popularity. Overall we found a song's lyrics uniqueness to\nhave the most significant association with its popularity. However, audio\nuniqueness was the strongest predictor of a song's popularity, conditional on\nthe song's genre. We further found the theme and repetitiveness of a song's\nlyrics to mediate the relationship between the song's popularity and novelty.\nBroadly, our results contradict the \"optimal distinctiveness theory\" (balance\nbetween novelty and familiarity) and call for an investigation into the\nmultiple dimensions along which a cultural product's uniqueness could manifest.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01934,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"How sustainable is \"common\" data science in terms of power consumption?\n\n  Continuous developments in data science have brought forth an exponential\nincrease in complexity of machine learning models. Additionally, data\nscientists have become ubiquitous in the private market, academic environments\nand even as a hobby. All of these trends are on a steady rise, and are\nassociated with an increase in power consumption and associated carbon\nfootprint. The increasing carbon footprint of large-scale advanced data science\nhas already received attention, but the latter trend has not. This work aims to\nestimate the contribution of the increasingly popular \"common\" data science to\nthe global carbon footprint. To this end, the power consumption of several\ntypical tasks in the aforementioned common data science tasks will be measured\nand compared to: large-scale \"advanced\" data science, common computer-related\ntasks, and everyday non-computer related tasks. This is done by converting the\nmeasurements to the equivalent unit of \"km driven by car\". Our main findings\nare: \"common\" data science consumes $2.57$ more power than regular computer\nusage, but less than some common everyday power-consuming tasks such as\nlighting or heating; large-scale data science consumes substantially more power\nthan common data science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08287,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Spatial Distribution of Solar PV Deployment: An Application of the\n  Region-Based Convolutional Neural Network\n\n  This paper presents a comprehensive analysis of the social and environmental\ndeterminants of solar photovoltaic (PV) deployment rates in Colorado, USA.\nUsing 652,795 satellite imagery and computer vision frameworks based on a\nconvolutional neural network, we estimated the proportion of households with\nsolar PV systems and the roof areas covered by solar panels. At the census\nblock group level, 7% of Coloradan households have a rooftop PV system, and\n2.5% of roof areas in Colorado are covered by solar panels as of 2021. Our\nmachine learning models predict solar PV deployment based on 43 natural and\nsocial characteristics of neighborhoods. Using four algorithms (Random Forest,\nCATBoost, LightGBM, XGBoost), we find that the share of Democratic party votes,\nhail risks, strong wind risks, median home value, and solar PV permitting\ntimelines are the most important predictors of solar PV count per household. In\naddition to the size of the houses, PV-to-roof area ratio is highly dependent\non solar PV permitting timelines, proportion of renters and multifamily\nhousing, and winter weather risks. We also find racial and ethnic disparities\nin rooftop solar deployment. The average marginal effects of median household\nincome on solar deployment are lower in communities with a greater proportion\nof African American and Hispanic residents and are higher in communities with a\ngreater proportion of White and Asian residents. In the ongoing energy\ntransition, knowing the key predictors of solar deployment can better inform\nbusiness and policy decision making for more efficient and equitable grid\ninfrastructure investment and distributed energy resource management.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.03856,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000042717,
      "text":"Participatory Action for Citizens' Engagement to Develop a\n  Pro-Environmental Research Application\n\n  To understand and begin to address the challenge of air pollution in Europe\nwe conducted participatory research, art and design activities with the\nresidents of one of the areas most affected by smog in Poland. The\nparticipatory research events, described in detail in this article, centered\naround the theme of ecology and served to design an application that would\nallow us to conduct field research on pro-environmental behaviours at a larger\nscale. As a result we developed a research application, rooted in local culture\nand history and place attachment, which makes use of gamification techniques.\nThe application gathers air quality data from the densest network of air\npollution sensors in Europe, thereby aligning the visible signs of pollution in\nthe app with the local sensor data. At the same time it reinforces the users'\npro-environmental habits and exposes them to educational messages about air\nquality and the environment. The data gathered with this application will\nvalidate the efficacy of this kind of an intervention in addressing residents'\nsmog-causing behaviours.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.02423,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000188417,
      "text":"WE model: A Machine Learning Model Based on Data-Driven Movie\n  Derivatives Market Prediction\n\n  The mature development and the extension of the industry chain make the\nincome structure of the film industry. The income of the traditional film\nindustry depends on the box office and also includes movie merchandising,\nadvertisement, home entertainment, book sales etc. Movie merchandising can even\nbecome more profitable than the box office. Therefore, market analysis and\nforecasting methods for multi-feature merchandising of multi-type films are\nparticularly important. Traditional market research is time-consuming and\nlabour-intensive, and its practical value is restricted. Due to the limited\nresearch method, more effective predictive analysis technology needs to be\nformed. With the rapid development of machine learning and big data, a large\nnumber of machine learning algorithms for predictive regression and\nclassification recognition have been proposed and widely used in product design\nand industry analysis. This paper proposes a high-precision movie merchandising\nprediction model based on machine learning technology: WE model. This model\nintegrates three machine learning algorithms to accurately predict the movie\nmerchandising market. The WE model learns the relationship between the movie\nmerchandising market and movie features by analyzing the main feature\ninformation of movies. After testing, the accuracy rate of prediction and\nevaluation in the merchandising market reaches 72.5%, and it has achieved a\nstrong market control effect.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07369,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000057949,
      "text":"A Systematic Literature Review of Game-based Assessment Studies: Trends\n  and Challenges\n\n  Technology has become an essential part of our everyday life, and its use in\neducational environments keeps growing. In addition, games are one of the most\npopular activities across cultures and ages, and there is ample evidence that\nsupports the benefits of using games for assessment. This field is commonly\nknown as game-based assessment (GBA), which refers to the use of games to\nassess learners' competencies, skills, or knowledge. This paper analyzes the\ncurrent status of the GBA field by performing the first systematic literature\nreview on empirical GBA studies. It is based on 65 research papers that used\ndigital GBAs to determine: (1) the context where the study has been applied;\n(2) the primary purpose; (3) the domain of the game used; (4) game\/tool\navailability; (5) the size of the data sample; (6) the computational methods\nand algorithms applied; (7) the targeted stakeholders of the study; and (8)\nwhat limitations and challenges are reported by authors. Based on the\ncategories established and our analysis, the findings suggest that GBAs are\nmainly used in K-16 education and for assessment purposes, and that most GBAs\nfocus on assessing STEM content, and cognitive and soft skills. Furthermore,\nthe current limitations indicate that future GBA research would benefit from\nthe use of bigger data samples and more specialized algorithms. Based on our\nresults, we discuss current trends in the field and open challenges (including\nreplication and validation problems), providing recommendations for the future\nresearch agenda of the GBA field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.11602,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Challenges Faced by Teaching Assistants in Computer Science Education\n  Across Europe\n\n  Teaching assistants (TAs) are heavily used in computer science courses as a\nway to handle high enrollment and still being able to offer students individual\ntutoring and detailed assessments. TAs are themselves students who take on this\nadditional role in parallel with their own studies at the same institution.\nPrevious research has shown that being a TA can be challenging but has mainly\nbeen conducted on TAs from a single institution or within a single course. This\npaper offers a multi-institutional, multi-national perspective of challenges\nthat TAs in computer science face. This has been done by conducting a thematic\nanalysis of 180 reflective essays written by TAs from three institutions across\nEurope. The thematic analysis resulted in five main challenges: becoming a\nprofessional TA, student focused challenges, assessment, defining and using\nbest practice, and threats to best practice. In addition, these challenges were\nall identified within the essays from all three institutions, indicating that\nthe identified challenges are not particularly context-dependent. Based on\nthese findings, we also outline implications for educators involved in TA\ntraining and coordinators of computer science courses with TAs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.047,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000235107,
      "text":"Decolonisation, Global Data Law, and Indigenous Data Sovereignty\n\n  This research examines the impact of digital neo-colonialism on the Global\nSouth and encourages the development of legal and economic incentives to\nprotect Indigenous cultures globally. Data governance is discussed in an\nevolutionary context while focusing on data sharing and data mining. Case\nstudies that exemplify the need to steer global data law towards protecting the\nearth, while addressing issues of data access, privacy, rights, and colonialism\nin the global South are explored. The case studies highlight connections to\nindigenous people's rights, in regard to the protection of environmental\necosystems, thus establishing how data law can serve the earth from an\nautochthonous lens. This framework examines histories shaped by colonialism and\nsuggests how data governance could be used to create healthier balances of\npower.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04706,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0109269884,
      "text":"Knowledge Enhancement and Mobile Technology: Improving Effectiveness and\n  Efficiency\n\n  Mobile technology creates value on three fundamental pillars: productivity,\ncoordination, and transformation. Mobile apps are becoming an increasingly\nimportant aspect of teaching and learning in many countries. The use of mobile\napplications in education is not only beneficial, but also provides students\nwith an enjoyable and interactive experience. For a mobile product launch to be\nas successful as it can be, it is imperative that a systematic, precise,\ncontrolled, and well-established process is in place, which is controlled,\nefficient, and well-established. Many educational organisation's find\nthemselves in situations where they have to get all departments working\neffectively and together in order to meet a specific deadline, including\nmarketing, production, and operations, after the organization's product\nclearance board approves the new product. In many ways, the situation is\nsimilar to the software crisis that took place in the middle of the 1970's. As\na result of globalisation and communication, oftentimes the effects of\nglobalisation are amplified because of the vast amount of information that must\nbe shared among project team members. Every educational organization has a\nunique style and way of doing things, and the project management team is no\nexception. As a consequence, it is commonplace to see that every education\nentity has its own particular style and way of doing things. In regards to the\ncreation of a mobile application that is going to function efficiently, it is\nimportant to remember that it is extremely important to stick to the strategies\nand requirements that will yield the best results for the education sector.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.05326,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"A Multicriteria Evaluation for Data-Driven Programming Feedback Systems:\n  Accuracy, Effectiveness, Fallibility, and Students' Response\n\n  Data-driven programming feedback systems can help novices to program in the\nabsence of a human tutor. Prior evaluations showed that these systems improve\nlearning in terms of test scores, or task completion efficiency. However,\ncrucial aspects which can impact learning or reveal insights important for\nfuture improvement of such systems are ignored in these evaluations. These\naspects include inherent fallibility of current state-of-the-art, students'\nprogramming behavior in response to correct\/incorrect feedback, and\neffective\/ineffective system components. Consequently, a great deal of\nknowledge is yet to be discovered about such systems. In this paper, we apply a\nmulti-criteria evaluation with 5 criteria on a data-driven feedback system\nintegrated within a block-based novice programming environment. Each criterion\nin the evaluation reveals a unique pivotal aspect of the system: 1) How\naccurate the feedback system is; 2) How it guides students throughout\nprogramming tasks; 3) How it helps students in task completion; 4) What happens\nwhen it goes wrong; and 5) How students respond generally to the system. Our\nevaluation results showed that the system was helpful to students due to its\neffective design and feedback representation despite being fallible. However,\nnovices can be negatively impacted by this fallibility due to high reliance and\nlack of self-evaluation. The negative impacts include increased working time,\nimplementation, or submission of incorrect\/partially correct solutions. The\nevaluation results reinforced the necessity of multi-criteria system\nevaluations while revealing important insights helpful to ensuring proper usage\nof data-driven feedback systems, designing fallibility mitigation steps, and\ndriving research for future improvement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.05179,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000139409,
      "text":"Digital Therapeutics for Mental Health: Is Attrition the Achilles Heel?\n\n  Digit therapeutics are novel software devices that clinicians may utilize in\ndelivering quality mental health care and ensuring positive outcomes. However,\nuptake of digital therapeutics and clinically tested software-based programs\nremains low. This article presents possible reasons for attrition and low\nengagement in clinical studies investigating digital therapeutics, analyses of\nstudies in which engagement was high, and design constructs that may encourage\nuser engagement. The aim is to shed light on the importance of real-world\nattrition data of digital therapeutics, and important characteristics of\nmedical devices that have positively influenced user engagement. The findings\npresented in this article will be useful to relevant stakeholders and medical\ndevice experts tasked with addressing the gap between software medical design\nand user engagement present in digital therapeutic clinical trials.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04709,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Blockchain-enabled tokenization for sustainable and inclusive\n  infrastructure investment\n\n  Infrastructure is critical for enabling society to function and the economy\nto thrive, but there is an increasing mismatch between the need for\ninfrastructure investments and available capital, which is in consequence of\nconstraints on public resources and limited capacity to leverage the private\nsector co-financing under the current system. With the emergence of distributed\nledger technology, such as blockchain-enabled tokenization, there is a\nsignificant potential to improve investment liquidity, transparency, efficiency\nand create new economic models to integrate non-financial values to promote\nsustainability and inclusiveness. This research analyzed 21 projects to\ninvestigate how tokenization is implemented in energy infrastructure projects.\nExploratory case study analyses were conducted, which shows the diversity of\ntokenization arrangements. The state of the art, potential benefits,\nimplications, and obstacles associated with the application of tokenization in\ninfrastructure investment and development are discussed. The purpose of this\nresearch is to understand tokenization within the context of the energy sector\nbut also to forecast its application in a broad spectrum of infrastructure\nprojects (e.g., transportation, telecommunication, healthcare, education).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.13235,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"How Segregation Patterns Affect the Availability of Fair District Plans\n\n  We create 4200 synthetic cities which vary in percent minority population and\ntheir residential segregation patterns. Of these, 1200 are modeled on existing\ncities, and 3000 are rectangular grid cities. In each city, we consider\nsingle-member voting district plans for a hypothetical city council election. A\nfair district plan is defined as one where the number of minority-majority\ndistricts is proportional to the city-wide minority population. Thus each city\nis summarized by three traits: minority percent, a measure of segregation, and\navailability of a fair district plan. We find that when the minority population\nis around 25%-33%, there is a positive correlation between the degree of\nsegregation and the availability of proportional district plan. Consistently,\nwhen the minority population lives in a more diffuse residential pattern, there\nare fewer available proportional district plans. Finally, we develop a new\nmethod to validate runtime and sample size of an ensemble of district plans\ncreated by the GerryChain software program.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.00681,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"The Many Facets of Trust in AI: Formalizing the Relation Between Trust\n  and Fairness, Accountability, and Transparency\n\n  Efforts to promote fairness, accountability, and transparency are assumed to\nbe critical in fostering Trust in AI (TAI), but extant literature is\nfrustratingly vague regarding this 'trust'. The lack of exposition on trust\nitself suggests that trust is commonly understood, uncomplicated, or even\nuninteresting. But is it? Our analysis of TAI publications reveals numerous\norientations which differ in terms of who is doing the trusting (agent), in\nwhat (object), on the basis of what (basis), in order to what (objective), and\nwhy (impact). We develop an ontology that encapsulates these key axes of\ndifference to a) illuminate seeming inconsistencies across the literature and\nb) more effectively manage a dizzying number of TAI considerations. We then\nreflect this ontology through a corpus of publications exploring fairness,\naccountability, and transparency to examine the variety of ways that TAI is\nconsidered within and between these approaches to promoting trust.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05471,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"PATE: Property, Amenities, Traffic and Emotions Coming Together for Real\n  Estate Price Prediction\n\n  Real estate prices have a significant impact on individuals, families,\nbusinesses, and governments. The general objective of real estate price\nprediction is to identify and exploit socioeconomic patterns arising from real\nestate transactions over multiple aspects, ranging from the property itself to\nother contributing factors. However, price prediction is a challenging\nmultidimensional problem that involves estimating many characteristics beyond\nthe property itself. In this paper, we use multiple sources of data to evaluate\nthe economic contribution of different socioeconomic characteristics such as\nsurrounding amenities, traffic conditions and social emotions. Our experiments\nwere conducted on 28,550 houses in Beijing, China and we rank each\ncharacteristic by its importance. Since the use of multi-source information\nimproves the accuracy of predictions, the aforementioned characteristics can be\nan invaluable resource to assess the economic and social value of real estate.\nCode and data are available at: https:\/\/github.com\/IndigoPurple\/PATE\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.14451,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Foreseeing the Impact of the Proposed AI Act on the Sustainability and\n  Safety of Critical Infrastructures\n\n  The AI Act has been recently proposed by the European Commission to regulate\nthe use of AI in the EU, especially on high-risk applications, i.e. systems\nintended to be used as safety components in the management and operation of\nroad traffic and the supply of water, gas, heating and electricity. On the\nother hand, IEC 61508, one of the most adopted international standards for\nsafety-critical electronic components, seem to mostly forbid the use of AI in\nsuch systems. Given this conflict between IEC 61508 and the proposed AI Act,\nalso stressed by the fact that IEC 61508 is not an harmonised European\nstandard, with the present paper we study and analyse what is going to happen\nto industry after the entry into force of the AI Act. In particular, we focus\non how the proposed AI Act might positively impact on the sustainability of\ncritical infrastructures by allowing the use of AI on an industry where it was\npreviously forbidden. To do so, we provide several examples of AI-based\nsolutions falling under the umbrella of IEC 61508 that might have a positive\nimpact on sustainability in alignment with the current long-term goals of the\nEU and the Sustainable Development Goals of the United Nations, i.e.,\naffordable and clean energy, sustainable cities and communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.10814,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"From alternative conceptions of honesty to alternative facts in\n  communications by U.S. politicians\n\n  The spread of online misinformation on social media is increasingly perceived\nas a problem for societal cohesion and democracy. The role of political leaders\nin this process has attracted less research attention, even though politicians\nwho \"speak their mind\" are perceived by segments of the public as authentic and\nhonest even if their statements are unsupported by evidence. Analyzing\ncommunications by members of the U.S. Congress on Twitter between 2011 and\n2022, we show that politicians' conception of honesty has undergone a distinct\nshift, with authentic belief-speaking that may be decoupled from evidence\nbecoming more prominent and more differentiated from explicitly evidence-based\ntruth seeking. We show that for Republicans - but not Democrats - an increase\nof belief-speaking of 10% is associated with a decrease of 12.8 points of\nquality (NewsGuard scoring system) in the sources shared in a tweet.\nConversely, an increase in truth-seeking language is associated with an\nincrease in quality of sources for both parties. The results support the\nhypothesis that the current dissemination of misinformation in political\ndiscourse is in part driven by an alternative understanding of truth and\nhonesty that emphasizes invocation of subjective belief at the expense of\nreliance on evidence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02852,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000090069,
      "text":"A 35-Year Longitudinal Analysis of Dermatology Patient Behavior across\n  Economic & Cultural Manifestations in Tunisia, and the Impact of Digital\n  Tools\n\n  The evolution of behavior of dermatology patients has seen significantly\naccelerated change over the past decade, driven by surging availability and\nadoption of digital tools and platforms. Through our longitudinal analysis of\nthis behavior within Tunisia over a 35-year time frame, we identify behavioral\npatterns across economic and cultural dimensions and how digital tools have\nimpacted those patterns in preceding years. Throughout this work, we highlight\nthe witnessed effects of available digital tools as experienced by patients,\nand conclude by presenting a vision for how future tools can help address the\nissues identified across economic and cultural manifestations. Our analysis is\nfurther framed around three types of digital tools: \"Dr. Google\", social media,\nand artificial intelligence (AI) tools, and across three stages of clinical\ncare: pre-visit, in-visit, and post-visit.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01802,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Mutual Information Scoring: Increasing Interpretability in Categorical\n  Clustering Tasks with Applications to Child Welfare Data\n\n  Youth in the American foster care system are significantly more likely than\ntheir peers to face a number of negative life outcomes, from homelessness to\nincarceration. Administrative data on these youth have the potential to provide\ninsights that can help identify ways to improve their path towards a better\nlife. However, such data also suffer from a variety of biases, from missing\ndata to reflections of systemic inequality. The present work proposes a novel,\nprescriptive approach to using these data to provide insights about both data\nbiases and the systems and youth they track. Specifically, we develop a novel\ncategorical clustering and cluster summarization methodology that allows us to\ngain insights into subtle biases in existing data on foster youth, and to\nprovide insight into where further (often qualitative) research is needed to\nidentify potential ways of assisting youth.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05459,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"How Do AI Timelines Affect Existential Risk?\n\n  Superhuman artificial general intelligence could be created this century and\nwould likely be a significant source of existential risk. Delaying the creation\nof superintelligent AI (ASI) could decrease total existential risk by\nincreasing the amount of time humanity has to work on the AI alignment problem.\n  However, since ASI could reduce most risks, delaying the creation of ASI\ncould also increase other existential risks, especially from advanced future\ntechnologies such as synthetic biology and molecular nanotechnology.\n  If AI existential risk is high relative to the sum of other existential risk,\ndelaying the creation of ASI will tend to decrease total existential risk and\nvice-versa.\n  Other factors such as war and a hardware overhang could increase AI risk and\ncognitive enhancement could decrease AI risk. To reduce total existential risk,\nhumanity should take robustly positive actions such as working on existential\nrisk analysis, AI governance and safety, and reducing all sources of\nexistential risk by promoting differential technological development.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12609,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Untangling the Dueling Expert Witnesses: Comparing Ensemble Methods in\n  Pennsylvania's Redistricting Plans\n\n  Ensembles of random legislative districts are a valuable tool for assessing\nwhether a proposed district plan is an outlier or gerrymander. Expert witnesses\nhave presented these in litigation using various methods, and unsurprisingly,\nthey often disagree.\n  Recent open source methods now permit independent validation of expert\nwitness testimony. Here, we compare ensembles for the Pennsylvania House and\nCongressional districts calculated using \"Redist\" and \"Gerrychain\" further\nincorporating constraints restricting county and municipal boundary splitting,\nas required by Pennsylvania for legal plans.\n  We compare results to expert witness testimony submitted by Republican and\nDemocratic parties. We confirm some of the testimony but could not reproduce\nall of it, struggling with metrics based on a heuristic \"sum of votes index\"\nrathern than a straightforward average of metrics across multiple elections. We\nrecommend against relying on analytics based on summing votes from multiple\nelections to create vote incides and derivative metrics as these are inherently\npoorly behaved. To promote transparency, we recommend that where possible,\nexpert witness testimony be based solely on publicly available election data as\nopposed to proprietary data closely held by political parties.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02638,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000043379,
      "text":"Age Appropriate Design: Assessment of TikTok, Twitch, and YouTube Kids\n\n  The presence of children in the online world is increasing at a rapid pace.\nAs children interact with services such as video sharing, live streaming, and\ngaming, a number of concerns arise regarding their security and privacy as well\nas their safety. To address such concerns, the UK's Information Commissioner's\nOffice (ICO) sets out 15 criteria alongside a risk management process for\ndevelopers of online services for children. We present an analysis of 15 ICO\ncriteria for age appropriate design. More precisely, we investigate whether\nthose criteria provide actionable requirements for developers and whether video\nsharing and live streaming platforms that are used by children of different age\nranges (i.e., TikTok, Twitch and YouTube Kids) comply with them. Our findings\nregarding the ICO criteria suggest that some criteria such as age verification\nand transparency provide adequate guidance for assessment whereas other\ncriteria such as parental controls, reporting of inappropriate content, and\nhandling of sensitive data need further clarification. Our findings regarding\nthe platforms themselves suggest that they choose to implement the simplest\nform of self-declared age verification with limited parental controls and\nplenty of opportunities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.04741,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000088414,
      "text":"Lisbon Hotspots: Wi-Fi access point dataset for time-bound location\n  proofs\n\n  Wi-Fi hotspots are a valuable resource for people on the go, especially\ntourists, as they provide a means to connect personal devices to the Internet.\nThis extra connectivity can be helpful in many situations, e.g., to enable map\nand chat applications to operate outdoors when cellular connectivity is\nunavailable or is expensive. Retail stores and many public services have\nrecognized that hotspots have potential to attract and retain customers, so\nmany of them offer free and open Wi-Fi. In busy cities, with many locals and\nvisitors, the number of hotspots is very significant. Some of these hotspots\nare available for long periods of time, while others are short-lived. When we\nhave many users with devices collecting hotspot observations, they can be used\nto detect the location -- using the long-lived hotspots -- and to prove the\ntime when the location was visited -- using the short-lived hotspots observed\nby others users at the location.\n  In this article, we present a dataset of collected Wi-Fi data from the most\nimportant tourist locations in the city of Lisbon, Portugal, over a period of\nmonths, that was used to show the feasibility of using hotspot data for\nlocation detection and proof. The obtained data and algorithms were assessed\nfor a specific use case: smart tourism. We also present the data model used to\nstore the observations and the algorithms developed to detect and prove\nlocation of a user device at a specific time. The Lisbon Hotspots dataset,\nLXspots, is made publicly available to the scientific community so that other\nresearchers can also make use of it to develop new and innovative mobile and\nInternet of Things applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.14275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Long-term variation of population exposure to PM2.5 in Eastern China: A\n  perspective from SDG 11.6.2\n\n  Air pollution (e.g., PM2.5) has a negative effect on human health. Recently,\nthe population-weighted annual mean PM2.5 concentration (PWAM) has been\nselected as an indicator 11.6.2 in Sustainable Development Goals (SDGs), for\nvarious countries to perfrom a long-term monitoring of population exposure to\nPM2.5 in cities. However, few studies have employed this indicator for a\ncity-level analysis and also in a long-time series (e.g., for decades). To fill\nthis research gap, this study investigates the long-term (2000-2020) variation\nof population exposure to PM2.5 in Eastern China (including 318\nprefecture-level cities). Three categories of open geospatial data (including\nhigh-resolution and long-term PM2.5 and population data, and administrative\nboundary data of cities) are involved for analysis. We found that: 1) A\nconsiderable decrease has been observed for the PWAM during 2014-2020. 2) In\n2020, the PWAM is for the first time lower than the interim target-1 (35\n{\\mu}g\/m3) defined by the World Health Organization for 214 prefecture-level\ncities in Eastern China, which accounts for 67% of the total population. The\nresults indicates a considerable improvement of air quality in Eastern China.\nMore important, this study illustrates the feasibility of using open geospatial\ndata to monitor the SDG indicator 11.6.2.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.0135,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000114242,
      "text":"Application of Blockchain Smart Contracts in E-Commerce and Government\n\n  With technological advances and the establishment of e-commerce models,\nbusiness challenges have shifted to online platforms. The promise of embedding\nself-executing and autonomous programs into blockchain technologies has\nattracted increased interest and its use in niche solutions. Using qualitative\ninterviews, this paper sought the opinions of the eleven industry leaders\nregarding smart contracts. Findings reveal that the technology is gaining\nmomentum in e-commerce, particularly in financial transfer, record-keeping,\nreal estate, and property management, insurance, mortgage, supply chain\nmanagement, data storage, authorization of credit, denaturalized intelligence,\naviation sector, shipping of products, invoice financing and other domains. The\nsignificant benefits of widespread adoption and deployment of smart contracts\ninclude their capability to deliver decentralization, efficacy,\ncost-effectiveness, transparency, speed, autonomy, transparency, privacy, and\nsecurity, encouraging the emergence of novel business models. Albeit these\nbenefits that revolutionize online transactions, the technology faced\nmultifaceted challenges. Smart technologies are only a decade old and are not\nadvanced in security, transparency, cost-effectiveness, and regulatory\nframework. Furthermore, organizational, and technical challenges limit their\ndeployment: incompatibility with legacy systems, scalability, bugs, speed, and\nlack of talent and understanding regarding smart contracts. Consequently,\npolicymakers, developers, researchers, practitioners, and other stakeholders\nneed to invest effort and time to foster the technologies and address pertinent\nissues to enable the global adoption of smart contracts by small and big\nbusinesses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12641,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Smart Healthcare System Implementation Challenges: A stakeholder\n  perspective\n\n  The smart healthcare system has gained significant attention for the\nimprovement of the customary healthcare system. The system is comprised of\nseveral key stakeholders that make the whole ecosystem successful. However,\nthese stakeholders offer considerable challenges that need much research to\naddress for making the system acceptable and reliable. Furthermore, very few\nstudies examine the key challenges from the perspective of stakeholders of the\nsmart healthcare system. The objective of this research study is to identify\nthe key challenges associated with each stakeholder of the smart healthcare\nsystem. We have identified 27 challenges associated with eight key stakeholders\nof smart healthcare reported in the state-of-the-art literature. Further, a\nquantitative survey was conducted and the data from 85 respondents were\ncollected in order to assess the significance of challenges in the real-world\nsmart healthcare system. The collected data from the respondents were further\nanalyzed using the smart-PSL (3.0). The results indicated that all the\nidentified challenges associated with each stakeholder negatively influence the\nsmart healthcare system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05457,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Reliable and Resilient AI and IoT-based Personalised Healthcare\n  Services: A Survey\n\n  Recent technological and economic developments have transformed the\nhealthcare sector towards more personalized and IoT-based healthcare services.\nThese services are realized through control and monitoring applications that\nare typically developed using artificial intelligence\/machine learning-based\nalgorithms, which play a significant role in highlighting the efficiency of\ntraditional healthcare systems. Current personalized healthcare services are\ndedicated to a specific environment to support technological personalization.\nHowever, they are unable to consider different interrelated health conditions,\nleading to inappropriate diagnoses and affecting sustainability and the\nlong-term health of patients. To this end, current Healthcare 5.0 technology\nhas evolved that supersede previous healthcare technologies. The goal of\nhealthcare 5.0 is to achieve an autonomous healthcare service, that takes into\naccount the interdependent effect of different health conditions of a patient.\nThis paper conducts a comprehensive survey on personalized healthcare services.\nIn particular, we first present an overview of key requirements of\ncomprehensive personalized healthcare services in modern healthcare Internet of\nThings (HIoT), including the definition of personalization and an example use\ncase scenario as a representative for modern HIoT. Second, we explored a\nfundamental three-layer architecture for IoT-based healthcare systems using AI\nand non-AI-based approaches, considering key requirements for CPHS followed by\ntheir strengths and weaknesses in the frame of personalized healthcare\nservices. Third, we highlighted different security threats against each layer\nof IoT architecture along with the possible AI and non-AI-based solutions.\nFinally, we propose a methodology to develop reliable, resilient, and\npersonalized healthcare services that address the identified weaknesses of\nexisting approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09298,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000184774,
      "text":"Applying Back Propagation Algorithm and Analytic Hierarchy Process to\n  Environment Assessment\n\n  This paper designs a new and scientific environmental quality assessment\nmethod, and takes Saihan dam as an example to explore the environmental\nimprovement degree to the local and Beijing areas. AHP method is used to assign\nvalues to each weight 7 primary indicators and 21 secondary indicators were\nused to establish an environmental quality assessment model. The conclusion\nshows that after the establishment of Saihan dam, the local environmental\nquality has been improved by 7 times, and the environmental quality in Beijing\nhas been improved by 13%. Then the future environmental index is predicted.\nFinally the Spearson correlation coefficient is analyzed, and it is proved that\ncorrelation is 99% when the back-propagation algorithm is used to test and\nprove that the error is little.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12759,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Remote Data Auditing and How it May Affect the Chain of Custody in a\n  Cloud Environment\n\n  As big data collection continues to grow, more and more organizations are\nrelying on outsourcing their data to cloud-based environments. This includes\nthe federal government and several agencies that depend on maintaining our\ncitizens' secure data. Law enforcement agencies from the national level down to\nlarge city police departments are also using the cloud environment to store\ndata. These agencies see this as a method of securing data while saving money\nby not maintaining the large data centers required to house this information.\nThis data solution presents in own set of problems in that the outsourced data\ncan become untrustworthy due to the lack of control of the data owners. Cloud\ncomputing is facing many difficulties, with security being the primary issue.\nThis is because the cloud computing service provider is a separate entity; any\ndata stored in the cloud can be interpreted as giving up control of the data by\nthe primary data owner. [1] Remote data auditing (RDA) is increasingly\nimportant when managing data in a cloud environment, especially when\norganizations have to store their data in a multi-cloud environment. The\nchallenging security threats posed by attempting to maintain the integrity of\nthe data for proper auditing is a trial that was never addressed in the past.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0544,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000054969,
      "text":"Bias Impact Analysis of AI in Consumer Mobile Health Technologies:\n  Legal, Technical, and Policy\n\n  Today's large-scale algorithmic and automated deployment of decision-making\nsystems threatens to exclude marginalized communities. Thus, the emergent\ndanger comes from the effectiveness and the propensity of such systems to\nreplicate, reinforce, or amplify harmful existing discriminatory acts.\nAlgorithmic bias exposes a deeply entrenched encoding of a range of unwanted\nbiases that can have profound real-world effects that manifest in domains from\nemployment, to housing, to healthcare. The last decade of research and examples\non these effects further underscores the need to examine any claim of a\nvalue-neutral technology. This work examines the intersection of algorithmic\nbias in consumer mobile health technologies (mHealth). We include mHealth, a\nterm used to describe mobile technology and associated sensors to provide\nhealthcare solutions through patient journeys. We also include mental and\nbehavioral health (mental and physiological) as part of our study. Furthermore,\nwe explore to what extent current mechanisms - legal, technical, and or\nnormative - help mitigate potential risks associated with unwanted bias in\nintelligent systems that make up the mHealth domain. We provide additional\nguidance on the role and responsibilities technologists and policymakers have\nto ensure that such systems empower patients equitably.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05458,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"The digital harms of smart home devices: A systematic literature review\n\n  The connection of home electronic devices to the internet allows remote\ncontrol of physical devices and involves the collection of large volumes of\ndata. With the increase in the uptake of Internet-of-Things home devices, it\nbecomes critical to understand the digital harms of smart homes. We present a\nsystematic literature review on the security and privacy harms of smart homes.\nPRISMA methodology is used to systematically review 63 studies published\nbetween January 2011 and October 2021; and a review of known cases is\nundertaken to illustrate the literature review findings with real-world\nscenarios. Published literature identifies that smart homes may pose threats to\nconfidentiality (unwanted release of information), authentication (sensing\ninformation being falsified) and unauthorised access to system controls. Most\nexisting studies focus on privacy intrusions as a prevalent form of harm\nagainst smart homes. Other types of harms that are less common in the\nliterature include hacking, malware and DoS attacks. Digital harms, and data\nassociated with these harms, may vary extensively across smart devices. Most\nstudies propose technical measures to mitigate digital harms, while fewer\nconsider social prevention mechanisms. We also identify salient gaps in\nresearch, and argue that these should be addressed in future cross-disciplinary\nresearch initiatives.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.10087,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"A Trust Framework for Government Use of Artificial Intelligence and\n  Automated Decision Making\n\n  This paper identifies the current challenges of the mechanisation,\ndigitisation and automation of public sector systems and processes, and\nproposes a modern and practical framework to ensure and assure ethical and high\nveracity Artificial Intelligence (AI) or Automated Decision Making (ADM)\nsystems in public institutions. This framework is designed for the specific\ncontext of the public sector, in the jurisdictional and constitutional context\nof Australia, but is extendable to other jurisdictions and private sectors. The\ngoals of the framework are to: 1) earn public trust and grow public confidence\nin government systems; 2) to ensure the unique responsibilities and\naccountabilities (including to the public) of public institutions under\nAdministrative Law are met effectively; and 3) to assure a positive human,\nsocietal and ethical impact from the adoption of such systems. The framework\ncould be extended to assure positive environmental or other impacts, but this\npaper focuses on human\/societal outcomes and public trust. This paper is meant\nto complement principles-based frameworks like Australia's Artificial\nIntelligence Ethics Framework and the EU Assessment List for Trustworthy AI. In\nmany countries, COVID created a bubble of improved trust, a bubble which has\narguably already popped, and in an era of unprecedented mistrust of public\ninstitutions (but even in times of high trust) it is not enough that a service\nis faster, or more cost-effective. This paper proposes recommendations for\ngovernment systems (technology platforms, operations, culture, governance,\nengagement, etc.) that would help to improve public confidence and trust in\npublic institutions, policies and services, whilst meeting the special\nobligations and responsibilities of the public sector.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.04383,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000018643,
      "text":"Impacts and Integration of Remote-First Working Environments\n\n  Due to the Covid-19 pandemic in 2020 or other business decisions, remote work\nis becoming increasingly popular. \"Remote first\" working environments exist\nwithin companies where most employees work remotely. This paper takes a deep\ndive into the remote-first mentality. It investigates its effects on employees\nat varying stages in their careers, day-to-day productivity, and working\nrelationships with team members. We found that the remote-first mentality most\nimpacts seasoned employees and managers, potentially due to trouble adjusting\nto a new way of working compared to the rest of their careers and the \"always\non\" mentality associated with working from home. Regarding productivity, we\nfound that while software development productivity appears unimpacted, the\neffectiveness of communication and employee wellbeing saw declines which are\ngenerally associated with lowered productivity. Finally, we looked closer at\nthe communication side of things and how remote work impacts relationship\nbuilding. We found that the most significant impacts on relationship building\ncentered around \"trust\" and \"credibility\" being harder to build due to a lack\nof non-verbal cues during social interactions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08983,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Temporal Analysis and Gender Bias in Computing\n\n  Recent studies of gender bias in computing use large datasets involving\nautomatic predictions of gender to analyze computing publications, conferences,\nand other key populations. Gender bias is partly defined by software-driven\nalgorithmic analysis, but widely used gender prediction tools can result in\nunacknowledged gender bias when used for historical research. Many names change\nascribed gender over decades: the \"Leslie problem.\" Systematic analysis of the\nSocial Security Administration dataset -- each year, all given names,\nidentified by ascribed gender and frequency of use -- in 1900, 1925, 1950,\n1975, and 2000 permits a rigorous assessment of the \"Leslie problem.\" This\narticle identifies 300 given names with measurable \"gender shifts\" across\n1925-1975, spotlighting the 50 given names with the largest such shifts. This\narticle demonstrates, quantitatively, there is net \"female shift\" that likely\nresults in the overcounting of women (and undercounting of men) in earlier\ndecades, just as computer science was professionalizing. Some aspects of the\nwidely accepted 'making programming masculine' perspective may need revision.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.08312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Base rate neglect in computer science education\n\n  Machine learning (ML) algorithms are gaining increased importance in many\nacademic and industrial applications, and such algorithms are, accordingly,\nbecoming common components in computer science curricula. Learning ML is\nchallenging not only due to its complex mathematical and algorithmic aspects,\nbut also due to a) the complexity of using correctly these algorithms in the\ncontext of real-life situations and b) the understanding of related social and\nethical issues. Cognitive biases are phenomena of the human brain that may\ncause erroneous perceptions and irrational decision-making processes. As such,\nthey have been researched thoroughly in the context of cognitive psychology and\ndecision making; they do, however, have important implications for computer\nscience education as well. One well-known cognitive bias, first described by\nKahneman and Tversky, is the base rate neglect bias, according to which humans\nfail to consider the base rate of the underlying phenomena when evaluating\nconditional probabilities. In this paper, we explore the expression of the base\nrate neglect bias in ML education. Specifically, we show that about one third\nof students in an Introduction to ML course, from varied backgrounds (computer\nscience students and teachers, data science, engineering, social science and\ndigital humanities), fail to correctly evaluate ML algorithm performance due to\nthe base rate neglect bias. This failure rate should alert educators and\npromote the development of new pedagogical methods for teaching ML algorithm\nperformance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05462,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Why Are Some Online Educational Programs Successful? Student Cognition\n  and Success\n\n  Massive Open Online Courses (MOOCs) once offered the promise of accessibility\nand affordability. However, MOOCs typically lack expert feedback and social\ninteraction, and have low student engagement and retention. Thus, alternative\nprograms for online education have emerged including an online graduate program\nin computer science at a major public university in USA. This program is\nconsidered a success with over 9000 students now enrolled in the program. We\nadopt the perspective of cognitive science to answer the question why do only\nsome online educational courses succeed? We measure learner motivation and\nself-regulation in one course in the program, specifically a course on\nartificial intelligence (AI). Surveys of students indicate that students\nself-reported assessments of self-efficacy, cognitive strategy use, and\nintrinsic value of the course are not only fairly high, but also generally\nincrease over the course of learning. This data suggests that the online AI\ncourse might be a success because the students have high self-efficacy and the\nclass fosters self-regulated learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02866,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Algorithmic Learning Foundations for Common Law\n\n  This paper looks at a common law legal system as a learning algorithm, models\nspecific features of legal proceedings, and asks whether this system learns\nefficiently. A particular feature of our model is explicitly viewing various\naspects of court proceedings as learning algorithms. This viewpoint enables\ndirectly pointing out that when the costs of going to court are not\ncommensurate with the benefits of going to court, there is a failure of\nlearning and inaccurate outcomes will persist in cases that settle.\nSpecifically, cases are brought to court at an insufficient rate. On the other\nhand, when individuals can be compelled or incentivized to bring their cases to\ncourt, the system can learn and inaccuracy vanishes over time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.13391,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"Waste Management Hackathon Providing New Ideas to Increase Citizen\n  Awareness, Motivation and Engagement\n\n  This paper describes the International Disruptive Information Solutions\nhackathon and one the winning solutions. The purpose of the hackathon was to\npromote the use of disruptive ICT technologies (e.g. IoT, Big data, AI,\nblockchain) in urban infrastructures to create innovative waste management\nsolutions in a smart city context. 29 students enrolled into this hackathon and\nin the end 4 teams submitted their solutions to the challenges. The winning\nproposal EcoQ, an approach for plogging collecting trashes while jogging,\nanswered more than well to the presented challenge on waste management and\nengagement. The original idea was extended and partly refocused during an\ninternship. As the outcome of the internship a mobile application for\norganizing and holding waste collection events was developed. This mobile\napplication was shortly tested in a real environment and it provides a working\ncitizen-centric platform, which enables anyone to arrange waste management\nevents, and motivates other residents to participate in these activities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.01871,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000016888,
      "text":"Impact of COVID-19 on human mobility and retail sales in the US\n\n  Due to the COVID-19 pandemic, governments had to rapidly implement lockdown\npolicies that restricted human mobility to suppress the spread of the disease\nand reduce mortality. Because of the movement restrictions resulting from\ngovernment responses to the pandemic, US retail sales declined by -22% in April\n2020 compared to the previous year. This study looks at the stringency of\ngovernment policies, mobility patterns, and implied compliance levels. The\nrelationships between these variables and the influence on retail sales serve\nto understand past human behavior and prepare for future pandemics. Retail\nlosses varied dramatically across the US states, from -1.6% in Mississippi to\n-38.9% in Hawaii. States in the west and northeast were most affected, while\nthose in the south were relatively resilient. Regression was used to identify\nstatistically significant state-level characteristics. The greatest losses\noccurred in states with a high percentage of Democrat voters in the 2020\nPresidential Election and those with large populations. A 10% increase in the\nDemocrat vote is associated with a 2.4% increase in retail sales loss. States\nwith a high percentage of adults with less than a high school diploma were most\nresilient. The number of trips of less than one-mile per capita is defined as\nthe mobility index as it has the greatest influence on retail sales, on\naverage, across the US states. An increase of 10% in this mobility index is\nassociated with a 4.6% increase in retail sales. All states were generally\ncompliant and exhibited reduced mobility with increasing stringency. A rise of\n1% in the stringency index is associated with a decline of 1% in the mobility\nindex. States with a high percentage of Democrat voters, large populations, and\nlocated in the west tend to be most compliant. A 10% rise in the proportion of\npeople voting Democrat is associated with a 5% increase in compliance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08993,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000026226,
      "text":"When Digital Economy Meets Web3.0: Applications and Challenges\n\n  With the continuous development of web technology, Web3.0 has attracted a\nconsiderable amount of attention due to its unique decentralized\ncharacteristics. The digital economy is an important driver of high-quality\neconomic development and is currently in a rapid development stage. In the\ndigital economy scenario, the centralized nature of the Internet and other\ncharacteristics usually bring about security issues such as infringement and\nprivacy leakage. Therefore, it is necessary to investigate how to use Web3.0\ntechnologies to solve the pain points encountered in the development of the\ndigital economy by fully exploring the critical technologies of digital economy\nand Web3.0. In this paper, we discuss the aspects of Web3.0 that should be\nintegrated with the digital economy to better find the entry point to solve the\nproblems by examining the latest advances of Web3.0 in machine learning,\nfinance, and data management. We hope this research will inspire those who are\ninvolved in both academia and industry, and finally help to build a favourable\necology for the digital economy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08961,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Determinants Influencing Intention to Use Social Commerce for Shopping\n  in developing countries: A Case Study of Oman\n\n  Social media has had a significant impact on our individual lives, including\nour behavior regarding the purchasing of daily products. This study\ninvestigates the factors influencing Omani nationals' intentions to obtain\nproducts via social commerce. The researcher surveyed 202 participants and\nutilized the Technology Acceptance Model to develop the theoretical framework.\nThe data collection was analyzed statistically using an appropriate testing\nmechanism. Statistical methods, including Cronbach's alpha and multiple linear\nregression, were utilized for reliability and hypotheses testing. After\nanalyzing the collected data and testing the hypotheses, the findings indicated\nthat perceived usefulness, enjoyment, and ease of use of social commerce affect\npositively on Omani nationals' intentions to utilize social commerce for\nshopping. The independent variables had a statistically significant impact on\nthe intention to use social commerce shopping for products; these explain 69.9%\nof the variation on customers intention to utilize social commerce for\nshopping.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.04206,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000406967,
      "text":"Managing a blockchain-based platform ecosystem for industry-wide\n  adoption: The case of TradeLens\n\n  The proliferation of blockchain-based platform ecosystems in recent years has\nprompted scholars across various disciplines to explore the conditions leading\nto their successful deployment. However, developing a blockchain-based platform\necosystem creates various challenges for the platform sponsor that may\ninfluence industry-wide adoption and, ultimately, the platform's success. This\nstudy follows the development of TradeLens, a leading global shipping platform\necosystem underpinned by blockchain technology. We examine the factors\naffecting industry-wide adoption among global supply chain actors by unpacking\nplatform value drivers and platform governance mechanisms identified at\nTradeLens. While the platform value hinges on the digitalization of workflows\nand the ecosystem leverage, the platform governance includes strategic\n(off-chain), technology (on-chain), and interoperability (on- and off-chain)\ngovernance - as mechanisms for effectively managing a blockchain-based platform\necosystem. This paper contributes to the literature on blockchain-based\nplatform ecosystems and the platform literature.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08966,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Artificial Intelligence for Scientific Research: Authentic Research\n  Education Framework\n\n  We report a framework that enables the wide adoption of authentic research\neducational methodology at various schools by addressing common barriers. The\nguiding principles we present were applied to implement a program in which\nteams of students with complementary skills develop useful artificial\nintelligence (AI) solutions for researchers in natural sciences. To accomplish\nthis, we work with research laboratories that reveal\/specify their needs, and\nthen our student teams work on the discovery, design, and development of an AI\nsolution for unique problems using a consulting-like arrangement. To date, our\ngroup has been operating at New York University (NYU) for seven consecutive\nsemesters, has engaged more than a hundred students, ranging from first-year\ncollege students to master's candidates, and has worked with more than twenty\nprojects and collaborators. While creating education benefits for students, our\napproach also directly benefits scientists, who get an opportunity to evaluate\nthe usefulness of machine learning for their specific needs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.09961,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"Generating Synthetic Population\n\n  In this paper, we provide a method to generate synthetic population at\nvarious administrative levels for a country like India. This synthetic\npopulation is created using machine learning and statistical methods applied to\nsurvey data such as Census of India 2011, IHDS-II, NSS-68th round, GPW etc. The\nsynthetic population defines individuals in the population with characteristics\nsuch as age, gender, height, weight, home and work location, household\nstructure, preexisting health conditions, socio-economical status, and\nemployment. We used the proposed method to generate the synthetic population\nfor various districts of India. We also compare this synthetic population with\nsource data using various metrics. The experiment results show that the\nsynthetic data can realistically simulate the population for various districts\nof India.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.10418,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"The Objective Function: Science and Society in the Age of Machine\n  Intelligence\n\n  Machine intelligence, or the use of complex computational and statistical\npractices to make predictions and classifications based on data representations\nof phenomena, has been applied to domains as disparate as criminal justice,\ncommerce, medicine, media and the arts, mechanical engineering, among others.\nHow has machine intelligence become able to glide so freely across, and to make\nsuch waves for, these domains? In this dissertation, I take up that question by\nethnographically engaging with how the authority of machine learning has been\nconstructed such that it can influence so many domains, and I investigate what\nthe consequences are of it being able to do so. By examining the workplace\npractices of the applied machine learning researchers who produce machine\nintelligence, those they work with, and the artifacts they produce. The\ndissertation begins by arguing that machine intelligence proceeds from a naive\nform of empiricism with ties to positivist intellectual traditions of the 17th\nand 18th centuries. This naive empiricism eschews other forms of knowledge and\ntheory formation in order for applied machine learning researchers to enact\ndata performances that bring objects of analysis into existence as entities\ncapable of being subjected to machine intelligence. By data performances, I\nmean generative enactments which bring into existence that which machine\nintelligence purports to analyze or describe. The enactment of data\nperformances is analyzed as an agential cut into a representational field that\nproduces both stable claims about the world and the interpretive frame in which\nthose claims can hold true. The dissertation also examines how machine\nintelligence depends upon a range of accommodations from other institutions and\norganizations, from data collection and processing to organizational\ncommitments to support the work of applied machine learning researchers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12076,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Digital currency hardware wallets and the essence of money\n\nMany proposals for the design and implementation of digital wallets assume that the purpose of the wallet is to enable offline payments via custodial accounts, ignoring the real problems faced by individuals and businesses that engage in retail payments, such as the anticompetitive behaviour of payment platforms and the decline of cash. More importantly, the proposals ignore the raison d'\\^etre of digital currency as a kind of digital money that can be held independently of custodians. Finally, the proposals demonstrate a profound lack of imagination about the nature of digital money and the devices that could be used to hold, manage, and exchange it. From these presumptions flows a set of architectural requirements that stifle the promise of digital currency to deliver novel and efficient ways to exchange value in the digital economy. In this article, we critically assess the essential problems that digital currency solutions are being proposed to solve, particularly with respect to the future of payments and the future of cash. We assess the validity of common justifications for account-based payments and certified hardware in the context of alternative designs, limitations, and trade-offs. We conclude that the interests of consumers would be better served by design approaches to digital currency that anticipate that digital assets would be held outside accounts, stored offline, but transacted online, without requiring the use of trusted hardware.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02755,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Play&Go Corporate: An End-to-End Solution for Facilitating Urban\n  Cyclability\n\n  Mobility plays a fundamental role in modern cities. How citizens experience\nthe urban environment, access city core services, and participate in city life,\nstrongly depends on its mobility organization and efficiency. The challenges\nthat municipalities face are very ambitious: on the one hand, administrators\nmust guarantee their citizens the right to mobility and to easily access local\nservices; on the other hand, they need to minimize the economic, social, and\nenvironmental costs of the mobility system. Municipalities are increasingly\nfacing problems of traffic congestion, road safety, energy dependency and air\npollution, and therefore encouraging a shift towards sustainable mobility\nhabits based on active mobility is of central importance. Active modes, such as\ncycling, should be particularly encouraged, especially for local recurrent\njourneys (e.g., home--to--school, home--to--work). In this context, addressing\nand mitigating commuter-generated traffic requires engaging public and private\nstakeholders through innovative and collaborative approaches that focus not\nonly on supply (e.g., roads and vehicles) but also on transportation demand\nmanagement. In this paper, we present an end-to-end solution, called Play&Go\nCorporate, for enabling urban cyclability and its concrete exploitation in the\nrealization of a home-to-work sustainable mobility campaign (i.e., Bike2Work)\ntargeting employees of public and private companies. To evaluate the\neffectiveness of the proposed solution we developed two analyses: the first to\ncarefully analyze the user experience and any behaviour change related to the\nBike2Work mobility campaign, and the second to demonstrate how exploiting the\ncollected data we can potentially inform and guide the involved municipality\n(i.e., Ferrara, a city in Northern Italy) in improving urban cyclability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.05465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Towards a Recommender System for Profiling Users in a Renewable\n  Energetic Community\n\n  Current Energy systems located in almost all nations are going through a\nradical transformation motivated by technological, environmental and\ninstitutional needs. The introduction of novel technologies for energy\nproduction and storing, the insurgence of climate change and the attention for\nthe introduction of low impact technologies in some countries are main factors\nleading this transformation. Here we focus in particular on the introduction of\nrelatively small community energy systems based on solar energy that aim to\nre-organize local energy systems to integrate distributed energy resources and\nengage local communities. In each community, there is a set of producers and a\nset of consumers (and a set of producers\/consumers called prosumers). One of\nthe key aspects of the energetic communities is to maximise the energy that is\nshared within the user. Thus, it is crucial to select the best\nconsumers\/prosumers on the basis of their profile of consumption, in order to\nminimize subsequent management of the energy once the community is built. Here\nwe describe the design of a recommender sysstem that is able to profile users\non the basis of their past profile for subsequent admission into the energetic\ncommunity. Experiments supporting this publication have been carried out under\nthe BDTI (Big Data Test Infrastructure) of the European Union. The contents of\nthis publication are the sole responsibility of authors and do not necessarily\nreflect the opinion of the European Union.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0546,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Data Innovation in Demography, Migration and Human Mobility\n\n  With the consolidation of the culture of evidence-based policymaking, the\navailability of data has become central to policymakers. Nowadays, innovative\ndata sources offer an opportunity to describe demographic, mobility, and\nmigratory phenomena more accurately by making available large volumes of\nreal-time and spatially detailed data. At the same time, however, data\ninnovation has led to new challenges (ethics, privacy, data governance models,\ndata quality) for citizens, statistical offices, policymakers and the private\nsector. Focusing on the fields of demography, mobility, and migration studies,\nthe aim of this report is to assess the current state of data innovation in the\nscientific literature as well as to identify areas in which data innovation has\nthe most concrete potential for policymaking. Consequently, this study has\nreviewed more than 300 articles and scientific reports, as well as numerous\ntools, that employed non-traditional data sources to measure vital population\nevents (mortality, fertility), migration and human mobility, and the population\nchange and population distribution. The specific findings of our report form\nthe basis of a discussion on a) how innovative data is used compared to\ntraditional data sources; b) domains in which innovative data have the greatest\npotential to contribute to policymaking; c) the prospects of innovative data\ntransition towards systematically contributing to official statistics and\npolicymaking.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12523,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"From Silicon Shield to Carbon Lock-in ? The Environmental Footprint of Electronic Components Manufacturing in Taiwan (2015-2020)\n\nTaiwan plans to rapidly increase its industrial production capacity of electronic components while concurrently setting policies for its ecological transition. Given that the island is responsible for the manufacturing of a significant part of worldwide electronics components, the sustainability of the Taiwanese electronics industry is therefore of critical interest. In this paper, we survey the environmental footprint of 16 Taiwanese electronic components manufacturers (ECM) using corporate sustainability responsibility reports (CSR). Based on data from 2015 to 2020, this study finds out that our sample of 16 manufacturers increased its greenhouse gases (GHG) emissions by 7.5\\% per year, its final energy and electricity consumption by 8.8\\% and 8.9\\%, and the water usage by 6.1\\%. We show that the volume of manufactured electronic components and the environmental footprints compiled in this study are strongly correlated, which suggests that relative efficiency gains are not sufficient to curb the environmental footprint at the national scale. Given the critical nature of electronics industry for Taiwan's geopolitics and economics, the observed increase of energy consumption and the slow renewable energy roll-out, these industrial activities could create a carbon lock-in, blocking the Taiwanese government from achieving its carbon reduction goals and its sustainability policies. Besides, the European Union, the USA or even China aim at developing an industrial ecosystem targeting sub-10nm CMOS technology nodes similar to Taiwan. This study thus provides important insights regarding the environmental implications associated with such a technology roadmap. All data and calculation models used in this study are provided as supplementary material.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Towards Responsible Medical Diagnostics Recommendation Systems\n\n  The early development and deployment of hospital and healthcare information\nsystems have encouraged the ongoing digitization of processes in hospitals.\nMany of these processes, which previously required paperwork and telephone\narrangements, are now integrated into IT solutions and require physicians and\nmedical staff to interact with appropriate interfaces and tools. Although this\nshift to digital data management and process support has benefited patient care\nin many ways, it requires physicians to accurately capture all relevant\ninformation digitally for billing and documentation purposes, which takes a lot\nof time away from actual patient care work. However, systematic collection of\nhealthcare data over a long period of time offers opportunities to improve this\nprocess and support medical staff by introducing recommender systems. Based on\na practical working example, in this position paper, we will outline the design\nof a responsible recommender system in the medical context from a technical,\napplication driven perspective and discuss potential design choices and\ncriteria with a specific focus on accountability, safety, and fairness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.13613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Building a National Smart Campus to support sustainable business\n  development: An ecosystem approach\n\n  Universities are racing towards making their campuses and cities smart in\nresponse to the global digitalization trend. However, the sustainability impact\nof Smart Campus research, development, and innovation services on other\nrelevant stakeholders such as the small and medium-sized businesses, remain\nunder-investigated. The Finnish National Smart Campus project seeks to bridge\nthis gap by orchestrating a SC ecosystem where eight SC collaborate to bring\ntrailblazing services to businesses and society. To maximize the sustainability\nimpact of the SC ecosystem, this study used a participatory workshop to\nidentify the challenges of SC, provide a step-by-step guide on how to identify\nother relevant stakeholders, and ascertain the perceived sustainability impact\nusing one of the SC ecosystems RDIs as a case study. The preliminary results\nrevealed that barriers to university-industry ecosystem development include\n(i), the lack of clarity in the shared goals (i.e., value proposition) between\nactors and (ii), weak stakeholder involvement in university RDI processes.\nFinally, this paper proposed a SC ecosystem model which offers a mindset shift\nfor higher educational institutions in promoting the convergence of SC services\nand sustainability to support the sustainable development of Finnish-based\nSMEs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04764,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"End-to-end verifiable voting for developing countries -- what's hard in\n  Lausanne is harder still in Lahore\n\n  In recent years end-to-end verifiable voting (E2EVV) has emerged as a\npromising new paradigm to conduct evidence-based elections. However, E2EVV\nsystems thus far have primarily been designed for the developed world and the\nfundamental assumptions underlying the design of these systems do not readily\ntranslate to the developing world, and may even act as potential barriers to\nadoption of these systems. This is unfortunate because developing countries\naccount for 80\\% of the global population, and given their economic and\nsocio-political dilemmas and their track record of contentious elections, these\ncountries arguably stand to benefit most from this exciting new paradigm. In\nthis paper, we highlight various limitations and challenges in adapting E2EVV\nsystems to these environments, broadly classed across social, political,\ntechnical, operational, and human dimensions. We articulate corresponding\nresearch questions and identify significant literature gaps in these\ncategories. We also suggest relevant strategies to aid researchers,\npractitioners, and policymakers in visualizing and exploring solutions that\nalign with the context and unique ground realities in these environments. Our\ngoal is to outline a broader research agenda for the community to successfully\nadapt E2EVV voting systems to developing countries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.0235,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000022186,
      "text":"Crowdsourcing and Sidewalk Data: A Preliminary Study on the\n  Trustworthiness of OpenStreetMap Data in the US\n\n  Sidewalks play a pivotal role in urban mobility of everyday life. Ideally,\nsidewalks provide a safe walkway for pedestrians, link public transportation\nfacilities, and equip people with routing and navigation services. However,\nthere is a scarcity of open sidewalk data, which not only impacts the\naccessibility and walkability of cities but also limits policymakers in\ngenerating insightful measures to improve the current state of pedestrian\nfacilities. As one of the most famous crowdsourced data repositories,\nOpenStreetMap (OSM) could aid the lack of open sidewalk data to a large extent.\nHowever, completeness and quality of OSM data have long been a major issue. In\nthis paper, we offer a preliminary study on the availability and\ntrustworthiness of OSM sidewalk data. First, we compare OSM sidewalk data\ncoverage in over 50 major cities in the United States. Then, we select three\nmajor cities (Seattle, Chicago, and New York City) to further analyze the\ncompleteness of sidewalk data and its features, and to compute a\ntrustworthiness index leveraging historical OSM sidewalk data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03901,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000039736,
      "text":"A fairness assessment of mobility-based COVID-19 case prediction models\n\n  In light of the outbreak of COVID-19, analyzing and measuring human mobility\nhas become increasingly important. A wide range of studies have explored\nspatiotemporal trends over time, examined associations with other variables,\nevaluated non-pharmacologic interventions (NPIs), and predicted or simulated\nCOVID-19 spread using mobility data. Despite the benefits of publicly available\nmobility data, a key question remains unanswered: are models using mobility\ndata performing equitably across demographic groups? We hypothesize that bias\nin the mobility data used to train the predictive models might lead to unfairly\nless accurate predictions for certain demographic groups. To test our\nhypothesis, we applied two mobility-based COVID infection prediction models at\nthe county level in the United States using SafeGraph data, and correlated\nmodel performance with sociodemographic traits. Findings revealed that there is\na systematic bias in models performance toward certain demographic\ncharacteristics. Specifically, the models tend to favor large, highly educated,\nwealthy, young, urban, and non-black-dominated counties. We hypothesize that\nthe mobility data currently used by many predictive models tends to capture\nless information about older, poorer, non-white, and less educated regions,\nwhich in turn negatively impacts the accuracy of the COVID-19 prediction in\nthese regions. Ultimately, this study points to the need of improved data\ncollection and sampling approaches that allow for an accurate representation of\nthe mobility patterns across demographic groups.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00067,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"COVID-19 Infection Exposure to Customers Shopping during Black Friday\n\n  The outbreak of COVID-19 within the last two years has resulted in much\nfurther investigation into the safety of large events that involve a gathering\nof people. This study aims to investigate how COVID-19 can spread through a\nlarge crowd of people shopping in a store with no safety precautions taken. The\nevent being investigated is Black Friday, where hundreds or thousands of\ncustomers flood stores to hopefully receive the best deals on popular items. A\nmock store was created, separated into several different shopping sections, and\nrepresented using a 2-D grid where each square on the grid represented a 5 feet\nby 5 feet area of the mock store. Customers were simulated to enter the store,\nshop for certain items, check out, and then leave the store. A percentage of\ncustomers were chosen to be infective when they entered the store, which means\nthat they could spread infection quantum to other customers. Four hours of time\nwas simulated with around 6,000 customers being included. The maximum distance\nexposure could be spread (2 feet-10 feet), the minimum time of exposure needed\nto become infected (2 - 15 minutes), and the total percentage of customers who\nstarted as infective (1% - 5%) were all changed and their effects on the number\nof newly infected customers were measured. It was found that increasing the\nmaximum exposure distance by 2 feet resulted in between a 20% to 250% increase\nin newly infected customers, depending on the distances being used. It was also\nfound that increasing the percentage of customers who started as infective from\n1% to 2% and then to 5% resulted in a 200% to 300% increase in newly infected\ncustomers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.08978,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Decentralized nation, solving the web identity crisis\n\n  The web of today whether you prefer to call it web 2.0, web 3.0, web 5.0 or\neven the metaverse is at a critical stage of evolution and challenge, largely\ncentered around its crisis of identity. Like teenagers who cannot assess\nproperly their reason for being and do not seem ready to take responsibility\nfor their actions, we are constantly blaming the very system we are trying to\nget away from. To truly realize the benefits from innovation and technology,\nthis crisis has to be resolved, not just through tactical solutions but through\ndevelopments that enhance the sustainability of the web and its benefits.\nSignificant strides are being made in the evolution of digital services enabled\nby technology, regulation, and the sheer pace of societal change. The journey\nto the decentralized web is mirroring the convergence of the physical and\ndigital worlds across all economies and is increasingly embracing the digital\nnative world. Technology has provided the foundational platform for individuals\nand entities to create and manage wealth, potentially without the need for big\ninstitutions. Ironically, despite all of the advancements, we are still facing\nan unprecedented and increasing wealth gap. Clearly, the system is broken, not\njust around the edges but at the very core of the democratic underpinning of\nour society. In this whitepaper, we propose how artificial intelligence on\nblockchain can be used to generate a new class of identity through direct human\ncomputer interaction. We demonstrate how this, combined with new perspectives\nfor sustaining community and governance embedded within the use of blockchain\ntechnology, will underpin a sustainable solution to protect identity,\nauthorship and privacy at the same time while contributing to restore trust\namongst members of a future decentralized nation and hence contribute to\nsolving the web most significant identity crisis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00074,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"IoT-based Efficient Streetlight Controlling, Monitoring and Real-time\n  Error Detection System in Major Bangladeshi Cities\n\n  A huge wastage of electricity can be seen in Bangladesh due to improper\nstreet light management which leads to an enormous financial loss every year.\nMany noteworthy works have been done by researchers from different parts of the\nworld in tackling this issue by using the Internet of Things yet very few in\nBangladeshi perspective. In this work, we propose an efficient Internet of\nThings-based integrated streetlight framework that offers cloud-powered\nmonitoring, controlling through light dimming as per external lighting\nconditions and traffic detection, as well as a fault-detecting system to ensure\nlow power and electricity consumption. We analyzed data from Dhaka North and\nSouth City Corporation, Narayanganj City Corporation, and Chattogram City\nCorporation where our proposed model demonstrates a reduction in energy cost of\nup to approximately 60 percent more than that of the existing system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.15476,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Quotatives Indicate Decline in Objectivity in U.S. Political News\n\n  According to journalistic standards, direct quotes should be attributed to\nsources with objective quotatives such as \"said\" and \"told\", as nonobjective\nquotatives, like \"argued\" and \"insisted\" would influence the readers'\nperception of the quote and the quoted person. In this paper, we analyze the\nadherence to this journalistic norm to study trends in objectivity in political\nnews across U.S. outlets of different ideological leanings. We ask: 1) How has\nthe usage of nonobjective quotatives evolved? and 2) How do news outlets use\nnonobjective quotatives when covering politicians of different parties? To\nanswer these questions, we developed a dependency-parsing-based method to\nextract quotatives and applied it to Quotebank, a web-scale corpus of\nattributed quotes, obtaining nearly 7 million quotes, each enriched with the\nquoted speaker's political party and the ideological leaning of the outlet that\npublished the quote. We find that while partisan outlets are the ones that most\noften use nonobjective quotatives, between 2013 and 2020, the outlets that\nincreased their usage of nonobjective quotatives the most were \"moderate\"\ncentrist news outlets (around 0.6 percentage points, or 20% in relative\npercentage over 7 years). Further, we find that outlets use nonobjective\nquotatives more often when quoting politicians of the opposing ideology (e.g.,\nleft-leaning outlets quoting Republicans), and that this \"quotative bias\" is\nrising at a swift pace, increasing up to 0.5 percentage points, or 25% in\nrelative percentage, per year. These findings suggest an overall decline in\njournalistic objectivity in U.S. political news.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.09059,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000070863,
      "text":"Space Trusted Autonomy Readiness Levels\n\n  Technology Readiness Levels are a mainstay for organizations that fund,\ndevelop, test, acquire, or use technologies. Technology Readiness Levels\nprovide a standardized assessment of a technology's maturity and enable\nconsistent comparison among technologies. They inform decisions throughout a\ntechnology's development life cycle, from concept, through development, to use.\nA variety of alternative Readiness Levels have been developed, including\nAlgorithm Readiness Levels, Manufacturing Readiness Levels, Human Readiness\nLevels, Commercialization Readiness Levels, Machine Learning Readiness Levels,\nand Technology Commitment Levels. However, while Technology Readiness Levels\nhave been increasingly applied to emerging disciplines, there are unique\nchallenges to assessing the rapidly developing capabilities of autonomy. This\npaper adopts the moniker of Space Trusted Autonomy Readiness Levels to identify\na two-dimensional scale of readiness and trust appropriate for the special\nchallenges of assessing autonomy technologies that seek space use. It draws\ninspiration from other readiness levels' definitions, and from the rich field\nof trust and trustworthiness. The Space Trusted Autonomy Readiness Levels were\ndeveloped by a collaborative Space Trusted Autonomy subgroup, which was created\nfrom The Space Science and Technology Partnership Forum between the United\nStates Space Force, the National Aeronautics and Space Administration, and the\nNational Reconnaissance Office.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.10568,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000154972,
      "text":"What matters in the new field of machine learning and satellite\n  imagery-based poverty predictions? A review with relevance for potential\n  downstream applications and development research\n\n  This paper reviews the state of the art in satellite and machine learning\nbased poverty estimates and finds some interesting results. The most important\nfactors correlated to the predictive power of welfare in the reviewed studies\nare the number of pre-processing steps employed, the number of datasets used,\nthe type of welfare indicator targeted, and the choice of AI model. As\nexpected, studies that used hard indicators as targets achieved better\nperformance in predicting welfare than those that targeted soft ones. Also\nexpected was the number of pre-processing steps and datasets used having a\npositive and statistically significant relationship with welfare estimation\nperformance. Even more important, we find that the combination of ML and DL\nsignificantly increases predictive power by as much as 15 percentage points\ncompared to using either alone. Surprisingly, we find that the spatial\nresolution of the satellite imagery used is important but not critical to the\nperformance as the relationship is positive but not statistically significant.\nThe finding of no evidence indicating that predictive performance of a\nstatistically significant effect occurs over time was also unexpected. These\nfindings have important implications for future research in this domain. For\nexample, the level of effort and resources devoted to acquiring more expensive,\nhigher resolution SI will have to be reconsidered given that medium resolutions\nones seem to achieve similar results. The increasingly popular approach of\ncombining ML, DL, and TL, either in a concurrent or iterative manner, might\nbecome a standard approach to achieving better results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.12181,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Urban Socio-Technical Systems: An Autonomy and Mobility Perspective\n\n  The future of the human race is urban. The world's population is projected to\ngrow an additional 2.5 billion by 2050, with all expected to live in urban\nareas. This will increase the percentage of urban population from 55% today to\n70% within three decades and further strengthen the role of cities as the hub\nfor information, transportation, and overall socio-economic development. Unlike\nany other time in human history, the increasing levels of autonomy and machine\nintelligence are transforming cities to be no longer just human agglomerations\nbut a fusion of humans, machines, and algorithms making collective decisions,\nthus complex socio-technical systems. This manuscript summarizes and discusses\nmy efforts from the urban autonomy and mobility perspective to develop the\nurban socio-technical system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.06132,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Integrating Accessibility in a Mobile App Development Course\n\n  The growing interest in accessible software reflects in computing educators'\nand education researchers' efforts to include accessibility in core computing\neducation. We integrated accessibility in a junior\/senior-level Android app\ndevelopment course at a large private university in India. The course\nintroduced three accessibility-related topics using various interventions:\nAccessibility Awareness (a guest lecture by a legal expert), Technical\nKnowledge (lectures on Android accessibility guidelines and testing practices\nand graded components for implementing accessibility in programming\nassignments), and Empathy (an activity that required students to blindfold\nthemselves and interact with their phones using a screen-reader). We evaluated\ntheir impact on student learning using three instruments: (A) A pre\/post-course\nquestionnaire, (B) Reflective questions on each of the four programming\nassignments, and (C) Midterm and Final exam questions. Our findings demonstrate\nthat: (A) significantly more ($p<.05$) students considered disabilities when\ndesigning an app after taking this course, (B) many students developed empathy\ntowards the challenges persons with disabilities face while using inaccessible\napps, and (C) all students could correctly identify at least one accessibility\nissue in the user interface of a real-world app given its screenshot, and 90%\nof them could provide a correct solution to fix it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06326,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Bad, mad, and cooked: Moral responsibility for civilian harms in\n  human-AI military teams\n\n  This chapter explores moral responsibility for civilian harms by\nhuman-artificial intelligence (AI) teams. Although militaries may have some bad\napples responsible for war crimes and some mad apples unable to be responsible\nfor their actions during a conflict, increasingly militaries may 'cook' their\ngood apples by putting them in untenable decision-making environments through\nthe processes of replacing human decision-making with AI determinations in war\nmaking. Responsibility for civilian harm in human-AI military teams may be\ncontested, risking operators becoming detached, being extreme moral witnesses,\nbecoming moral crumple zones or suffering moral injury from being part of\nlarger human-AI systems authorised by the state. Acknowledging military ethics,\nhuman factors and AI work to date as well as critical case studies, this\nchapter offers new mechanisms to map out conditions for moral responsibility in\nhuman-AI teams. These include: 1) new decision responsibility prompts for\ncritical decision method in a cognitive task analysis, and 2) applying an AI\nworkplace health and safety framework for identifying cognitive and\npsychological risks relevant to attributions of moral responsibility in\ntargeting decisions. Mechanisms such as these enable militaries to design\nhuman-centred AI systems for responsible deployment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00062,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0001666281,
      "text":"Technology and COVID-19: How Reliant is Society on Technology?\n\n  Social media and messaging platforms have become a support system for those\nin fear of COVID-19 while, at the same time, becoming the root cause of\nspreading hate, inaccurate representations, and false realities. As technology\nhas morphed into a commodity for daily tasks and actions, this article may be\nuseful for people of all ages and backgrounds who are interested in\nunderstanding the impact of technology on society.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00058,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000084109,
      "text":"Semantic Web in Healthcare: A Systematic Literature Review of\n  Application, Research Gap, and Future Research Avenues\n\n  Today, healthcare has become one of the largest and most fast-paced\nindustries due to the rapid development of digital healthcare technologies. The\nfundamental thing to enhance healthcare services is communicating and linking\nmassive volumes of available healthcare data. However, the key challenge in\nreaching this ambitious goal is letting the information exchange across\nheterogeneous sources and methods as well as establishing efficient tools and\ntechniques. Semantic Web (SW) technology can help to tackle these problems.\nThey can enhance knowledge exchange, information management, data\ninteroperability, and decision support in healthcare systems. They can also be\nutilized to create various e-healthcare systems that aid medical practitioners\nin making decisions and provide patients with crucial medical information and\nautomated hospital services. This systematic literature review (SLR) on SW in\nhealthcare systems aims to assess and critique previous findings while adhering\nto appropriate research procedures. We looked at 65 papers and came up with\nfive themes: e-service, disease, information management, frontier technology,\nand regulatory conditions. In each thematic research area, we presented the\ncontributions of previous literature. We emphasized the topic by responding to\nfive specific research questions. We have finished the SLR study by identifying\nresearch gaps and establishing future research goals that will help to minimize\nthe difficulty of adopting SW in healthcare systems and provide new approaches\nfor SW-based medical systems progress.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.0897,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000078479,
      "text":"Digital Twins for Industry 4.0 in the 6G Era\n\n  Having the Fifth Generation (5G) mobile communication system recently rolled\nout in many countries, the wireless community is now setting its eyes on the\nnext era of Sixth Generation (6G). Inheriting from 5G its focus on industrial\nuse cases, 6G is envisaged to become the infrastructural backbone of future\nintelligent industry. Especially, a combination of 6G and the emerging\ntechnologies of Digital Twins (DT) will give impetus to the next evolution of\nIndustry 4.0 (I4.0) systems. This article provides a survey in the research\narea of 6G-empowered industrial DT system. With a novel vision of 6G industrial\nDT ecosystem, this survey discusses the ambitions and potential applications of\nindustrial DT in the 6G era, identifying the emerging challenges as well as the\nkey enabling technologies. The introduced ecosystem is supposed to bridge the\ngaps between humans, machines, and the data infrastructure, and therewith\nenable numerous novel application scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.0404,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Reliability of fault-tolerant system architectures for automated driving\n  systems\n\n  Automated driving functions at high levels of autonomy operate without driver\nsupervision. The system itself must provide suitable responses in case of\nhardware element failures. This requires fault-tolerant approaches using domain\nECUs and multicore processors operating in lockstep mode. The selection of a\nsuitable architecture for fault-tolerant vehicle systems is currently\nchallenging. Lockstep CPUs enable the implementation of majority redundancy or\nM-out-of-N ($M$oo$N$) architectures. In addition to structural redundancy,\ndiversity redundancy in the ECU architecture is also relevant to fault\ntolerance. Two fault-tolerant ECU architecture groups exist: architectures with\none ECU (system on a chip) and architectures consisting of multiple\ncommunicating ECUs. The single-ECU systems achieve higher reliability, whereas\nthe multi-ECU systems are more robust against dependent failures, such as\ncommon-cause or cascading failures, due to their increased potential for\ndiversity redundancy. Yet, it remains not fully understood how different types\nof architectures influence the system reliability. The work aims to design\narchitectures with respect to CPU and sensor number, $M$oo$N$ expression, and\nhardware element reliability. The results enable a direct comparison of\ndifferent architecture types. We calculate their reliability and quantify the\neffort to achieve high safety requirements. Markov processes allow comparing\nsensor and CPU architectures by varying the number of components and failure\nrates. The objective is to evaluate systems' survival probability and fault\ntolerance and design suitable sensor-CPU architectures. The results show that\nthe system architecture strongly influences the reliability. However, a\nsuitable system architecture must have a trade-off between reliability and\nself-diagnostics that parallel systems without majority redundancies do not\nprovide.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.10454,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"Automated Content Moderation Increases Adherence to Community Guidelines\n\n  Online social media platforms use automated moderation systems to remove or\nreduce the visibility of rule-breaking content. While previous work has\ndocumented the importance of manual content moderation, the effects of\nautomated content moderation remain largely unknown. Here, in a large study of\nFacebook comments (n=412M), we used a fuzzy regression discontinuity design to\nmeasure the impact of automated content moderation on subsequent rule-breaking\nbehavior (number of comments hidden\/deleted) and engagement (number of\nadditional comments posted). We found that comment deletion decreased\nsubsequent rule-breaking behavior in shorter threads (20 or fewer comments),\neven among other participants, suggesting that the intervention prevented\nconversations from derailing. Further, the effect of deletion on the affected\nuser's subsequent rule-breaking behavior was longer-lived than its effect on\nreducing commenting in general, suggesting that users were deterred from\nrule-breaking but not from commenting. In contrast, hiding (rather than\ndeleting) content had small and statistically insignificant effects. Our\nresults suggest that automated content moderation increases adherence to\ncommunity guidelines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.10373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000056293,
      "text":"Virtual learning environment is pleasure or pressure\n\n  The primary aim of this study intends the perception of students towards\nonline learning in the covid-19 pandemic period. The pandemic has changed the\ntraditional concepts of the education system and broken the functions of the\neducational institutions. But, they give it an opportunity to change pedagogy.\nThe research paper discussed the students opinions on online learning and\nvirtual classroom learning. This study applied a qualitative approach and\nprepared a systematic questionnaire for data collection. The researcher\ncollected the data from 258 students from different places in India and also,\nthe disproportionate sampling used for data collection. The research mainly\nfocused on the students perception, the comfort and discomfort of e-learning,\nusing electronic devices for communication, the virtual learning is a pleasure\nor pressure to the students, the digital skills of the students and their\nactive performance. The study revealed that over 50 percent of the students are\nhaving excellent knowledge of digital skills. The students are attending online\nclasses through their personal computers or laptops and phones. The teachers\nare allowing the students to ask questions and clear the doubt of the students.\nThe study found that the students are losing social interaction with teachers,\nfriends and cannot access the library because of online classes. Finally, the\nstudents felt that online learning is a pressure instead of pleasure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.03908,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000098348,
      "text":"Variability Analysis of Isolated Intersections Through Case Study\n\n  Population and economic growth of urban areas have led to intensive use of\nprivate vehicles, thereby increasing traffic volume and congestion on roads.\nThe traffic management in the city is a challenge for concerned authorities,\nand the signalized intersections are the primary interest of traffic\nmanagement. Interpreting traffic patterns and current traffic signal operations\ncan provide thorough insights to take appropriate actions. In this view, a\ncomprehensive study is conducted at selected intersections from Tumakuru\n(tier-2 city), Karnataka, India. Data estimates traffic parameters such as\nsaturation flow, composition, volume, and volume-to-capacity ratio. The\nstatistical results currently confirm the stable traffic condition but do not\nensure sustainability. The volume-to-capacity ratio is greater than 0.73 along\nthree major arterial roads of study intersections, indicating congestion in the\nfuture as the traffic volume is increasing gradually, as per the Directorate of\nUrban Land Use and Transportation, Government of Karnataka. The statistical\nresults obtained through the current study uphold the report. The empirical\nresults showed 40% of green time wastage at one of the study intersections,\nwhich results in additional waiting delays, thereby increasing fuel consumption\nand emissions. The overall service level of the study intersections is of class\nC based on computed delay and volume-to-capacity ratio. The study suggests\npossible treatments for improving the service level at the intersection\noperations and sustaining the city's stable traffic condition. The study\nsupports city traffic management authorities in identifying suitable treatment\nand implementing accordingly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.07796,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Nobody Wants to Work Anymore: An Analysis of r\/antiwork and the\n  Interplay between Social and Mainstream Media during the Great Resignation\n\n  r\/antiwork is a Reddit community that focuses on the discussion of worker\nexploitation, labour rights and related left-wing political ideas (e.g.\nuniversal basic income). In late 2021, r\/antiwork became the fastest growing\ncommunity on Reddit, coinciding with what the mainstream media began referring\nto as the Great Resignation. This same media coverage was attributed with\npopularising the subreddit and, therefore, accelerating its growth. In this\narticle, we explore how the r\/antiwork community was affected by the\nexponential increase in subscribers and the media coverage that chronicled its\nrise. We investigate how subreddit activity changed over time, the behaviour of\nheavy and light users, and how the topical nature of the discourse evolved with\nthe influx of new subscribers. We report that, despite the continuing rise of\nsubscribers well into 2022, activity on the subreddit collapsed after January\n25th 2022, when a moderator's Fox news interview was widely criticised. While\nmany users never commented again, longer running trends of users' posting and\ncommenting behaviour did not change. Finally, while many users expressed their\ndiscontent at the changing nature of the subreddit as it became more popular,\nwe found no evidence of major shifts in the topical content of discussion over\nthe period studied, with the exception of the introduction of topics related to\nseasonal events (e.g. holidays, such as Thanksgiving) and ongoing developments\nin the news (e.g. working from home and the curtailing of reproductive rights\nin the United States).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.10615,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Exploring the Confounding Factors of Academic Career Success: An\n  Empirical Study with Deep Predictive Modeling\n\n  Understanding determinants of success in academic careers is critically\nimportant to both scholars and their employing organizations. While\nconsiderable research efforts have been made in this direction, there is still\na lack of a quantitative approach to modeling the academic careers of scholars\ndue to the massive confounding factors. To this end, in this paper, we propose\nto explore the determinants of academic career success through an empirical and\npredictive modeling perspective, with a focus on two typical academic honors,\ni.e., IEEE Fellow and ACM Fellow. We analyze the importance of different\nfactors quantitatively, and obtain some insightful findings. Specifically, we\nanalyze the co-author network and find that potential scholars work closely\nwith influential scholars early on and more closely as they grow. Then we\ncompare the academic performance of male and female Fellows. After comparison,\nwe find that to be elected, females need to put in more effort than males. In\naddition, we also find that being a Fellow could not bring the improvements of\ncitations and productivity growth. We hope these derived factors and findings\ncan help scholars to improve their competitiveness and develop well in their\nacademic careers.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.14452,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000208616,
      "text":"Web-based Management Information System of Cases Filed with the National\n  Labor Relations Commission\n\n  This study was developed to describe the daily operations and encountered\nproblems of the National Labor Relations Commission Regional Arbitration Branch\nNo. IV (NLRC RAB IV) through conducted observations and interviews. These\nproblems were addressed and analyzed to be the features of the developed\nweb-based management information system (MIS) for cases. The research\nmethodology utilized in this project was the descriptive developmental\napproach. The Agile Software Development methodology was followed to develop\nthe system. It was used to quickly produce the desired output while allowing\nthe user to go back through phases without finishing the whole cycle. The\nsystem covered managing filed complaints, Single-Entry Approach (SEnA), labor\ncases, and report generation. The findings, through the interview, of handling\nrecords were inconsistent and inaccurate. This study also focused on ensuring\nthe Data Privacy Act of 2012, protecting the database's information using the\nXOR Cipher Algorithm. This study was evaluated using standard web evaluation\ncriteria. Using the criteria, the study's overall mean was 4.27 and 4.43, with\nthe descriptive meaning of Very Good, which showed that the system was accepted\nas perceived by experts and end-users, respectively. Management of filed cases\nis a vital process for the Commission. With that said, developing a web-based\nmanagement information system could ease the internal operations of handling\nand managing filed labor cases. Moreover, respondents and complainants can\neasily determine their filed cases' status using the case status tracking\nsystem. For further improvements to the system, additional printable documents\nmay be added that could be found needed by the Commission. Lastly, further\nresearch about the effectiveness of the web-based system may be conducted for\nfurther enhancements of the system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05369,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Decomposing the Fundamentals of Creepy Stories\n\n  Fear is a universal concept; people crave it in urban legends, scary movies,\nand modern stories. Open questions remain, however, about why these stories are\nscary and more generally what scares people. In this study, we explore these\nquestions by analyzing tens of thousands of scary stories on forums (known as\nsubreddits) in a social media website, Reddit. We first explore how writing\nstyles have evolved to keep these stories fresh before we analyze the stable\ncore techniques writers use to make stories scary. We find that writers have\nchanged the themes of their stories over years from haunted houses to\nschool-related themes, body horror, and diseases. Yet some features remain\nstable; words associated with pseudo-human nouns, such as clown or devil are\nmore common in scary stories than baselines. In addition, we collect a range of\ndatasets that annotate sentences containing fear. We use these data to develop\na high-accuracy fear detection neural network model, which is used to quantify\nwhere people express fear in scary stories. We find that sentences describing\nfear, and words most often seen in scary stories, spike at particular points in\na story, possibly as a way to keep the readers on the edge of their seats until\nthe story's conclusion. These results provide a new understanding of how\nauthors cater to their readers, and how fear may manifest in stories.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.1457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"The Role of In-House Procurement According to Finnish Municipalities'\n  Purchase Invoice Data\n\n  Public sector is a large consumer of ICT systems and services, used for\nvarious public services. Tendering for such systems is governed by laws aimed\nat eliminating unfair advantages and offering all possible parties equal\nopportunities to participate in the tendering process. In this article, we\nstudy in-house rpocurement, where the acquiring organization is an owner of the\nsubcontractor that delivers the system or the service. Municipalities' purchase\ninvoice data is used to determine how much municipalities in Finland depend on\nin-house procurement. In conclusion, the understanding if included\nmunicipalities have ICT service and development units within the organizations\nneeds closer examination, as in-house companies may offer municipalities with\nlimited resources divided costs in the public procurement process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.03905,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Dynamics of Gender Bias in Computing\n\n  Gender bias in computing is a hard problem that has resisted decades of\nresearch. One obstacle has been the absence of systematic data that might\nindicate when gender bias emerged in computing and how it has changed. This\narticle presents a new dataset (N=50,000) focusing on formative years of\ncomputing as a profession (1950-1980) when U.S. government workforce statistics\nare thin or non-existent. This longitudinal dataset, based on archival records\nfrom six computer user groups (SHARE, USE, and others) and ACM conference\nattendees and membership rosters, revises commonly held conjectures that gender\nbias in computing emerged during professionalization of computer science in the\n1960s or 1970s and that there was a 'linear' one-time onset of gender bias to\nthe present. Such a linear view also lent support to the \"pipeline\" model of\ncomputing's \"losing\" women at successive career stages. Instead, this dataset\nreveals three distinct periods of gender bias in computing and so invites\ntemporally distinct explanations for these changing dynamics. It significantly\nrevises both scholarly assessment and popular understanding about gender bias\nin computing. It also draws attention to diversity within computing. One\nconsequence of this research for CS reform efforts today is data-driven\nrecognition that legacies of gender bias beginning in the mid-1980s (not in\nearlier decades) is the problem. A second consequence is correcting the public\nimage of computer science: this research shows that gender bias is a contingent\naspect of professional computing, not an intrinsic or permanent one.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06313,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000006689,
      "text":"BioJam Camp: toward justice through bioengineering and biodesign\n  co-learning with youth\n\n  BioJam is a political, artistic, and educational project in which Bay Area\nartists, scientists, and educators collaborate with youth and communities of\ncolor to address historical exclusion of their communities in STEM fields and\nreframe what science can be. As an intergenerational collective, we co-learn on\ntopics of culture (social and biological), community (cultural and ecological),\nand creativity. We reject the notion that increasing the number of scientists\nof color requires inculcation in the ways of the dominant culture. Instead, we\ncenter cultural practices, traditional ways of knowing, storytelling, art,\nexperiential learning, and community engagement to break down the framing that\npositions these practices as distinct from science. The goal of this work is to\nrealize a future in which the practice of science is relatable, accessible, and\nliberatory.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13183,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000080797,
      "text":"Precision Medicine for the Population-The Hope and Hype of Public Health\n  Genomics\n\n  Public health is the most recent of the biomedical sciences to be seduced by\nthe trendy moniker \"precision.\" Advocates for \"precision public health\" (PPH)\ncall for a data-driven, computational approach to public health, leveraging\nswaths of genomic \"big data\" to inform public health decision-making. Yet, like\nprecision medicine, PPH oversells the value of genomic data to determine health\noutcomes, but on a population-level. A large historical literature has shown\nthat over-emphasizing heredity tends to disproportionately harm underserved\nminorities and disadvantaged communities. By comparing and contrasting PPH with\nan earlier attempt at using big data and genetics, in the Progressive era\n(1890-1920), we highlight some potential risks of a genotype-driven preventive\npublic health. We conclude by suggesting that such risks may be avoided by\nprioritizing data integration across many levels of analysis, from the\nmolecular to the social.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.0752,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000070201,
      "text":"Wikigender: A Machine Learning Model to Detect Gender Bias in Wikipedia\n\n  The way Wikipedia's contributors think can influence how they describe\nindividuals resulting in a bias based on gender. We use a machine learning\nmodel to prove that there is a difference in how women and men are portrayed on\nWikipedia. Additionally, we use the results of the model to obtain which words\ncreate bias in the overview of the biographies of the English Wikipedia. Using\nonly adjectives as input to the model, we show that the adjectives used to\nportray women have a higher subjectivity than the ones used to describe men.\nExtracting topics from the overview using nouns and adjectives as input to the\nmodel, we obtain that women are related to family while men are related to\nbusiness and sports.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.01104,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"A digital business ecosystem maturity model for personal service firms\n\n  Personal services can be found in sectors such as education, retail,\nhospitality, and craftsmanship. As of today, personal service firms lack the\nknow-how and experience on how to implement processes and practices to\neffectively build digital business ecosystems. This becomes an obstacle for\nthese kinds of firms to overcome the challenges of todays digital age. Based on\nthe guidelines of Design Science Research (DSR), we address this gap by\nproposing a maturity model, which offers specific guidance for this sector to\nbe able to achieve the transition from analog to digital. The design of the\nmodel is grounded in a systematic literature review, semi-structured\ninterviews, and a validation test involving company representatives from the\nfield of personal services, business ecosystems, and digitalization. Results\nrevealed a series of dimensions, capabilities, and maturity stages indicating\nan evolutionary path towards digital maturity for personal service firms. Thus,\nleading them to achieve a digital business ecosystem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.1628,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Distance Teaching Experience of Campus-based Teachers at Times of\n  Pandemic Confinement\n\n  Amidst the outbreak of the coronavirus (COVID 19) pandemic, distance\neducation, where the learning process is conducted online, has become the norm.\nCampus-based programs and courses have been redesigned in a timely manner which\nwas a challenge for teachers not used to distance teaching. Students engagement\nand active participation become an issue; add to that new emerging effects\nassociating with this set-up, such as the so called 'Zoom fatigue', which was\ncoined recently by some authors. In realising this problem, solutions were\nsuggested in the literature to help trigger students engagement and enhance\nteachers experience in online teaching. This study analyses these effects along\nwith our teachers experience in the new learning environment and concludes by\ndevising some recommendations. To attain the above objectives, we conducted\nonline interviews with six of our teachers, transcribed the content of the\nvideos and then applied the inductive research approach to assess the results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.01998,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Driving innovation through project based learning: A pre-university\n  STEAM for Social Good initiative\n\n  The Covid pandemic is a clarion call for increased sensitivity to the\ninterconnected nature of social problems facing our world today. A\nfuture-oriented education on critical issues, such as those outlined in the\nUnited Nations Sustainable Development Goals (UN SDGs) and designing potential\nsolutions for such problems is an imperative skill that must be imparted to\nchildren to help them navigate their future in today's unpredictable world.\nTowards this goal, we have been conducting 3.5 month-long mentoring programs\nfor pre-university students in India to participate in a STEAM for Social Good\ninnovation challenge conducted annually by the Government of India. Using\ndigital and physical computing skills, we helped children explore creative\nsolutions for social problems through a constructionist approach to learning,\nwherein they ideated and reflected upon the problems in their communities. The\nchildren learnt the Engineering Design Thinking process and worked in online\ngroups of two or three, from concept to completion. Despite the constraints\nposed by the pandemic, they explored creative ways to think about design and\ninnovation. They completed a variety of tasks by making, tinkering,\nengineering, assembling, and programming to grasp the intricate relationship\nbetween software and hardware. Subsequently, the children showcased their\ncreative abilities through video storytelling to a panel of domain experts. In\nthis paper, we present the children's perspective of their experiences through\nthis journey, the evaluation metrics based on IEEE design principles, and our\nlearnings from conducting this initiative as a university-school partnership\nmodel for 84 middle and high school students. The aspirational intent of this\ninitiative is to make the children better social problem solvers and help them\nperceive social problems as opportunities to enhance life for themselves and\ntheir communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00498,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Should I disclose my dataset? Caveats between reproducibility and\n  individual data rights\n\n  Natural language processing techniques have helped domain experts solve legal\nproblems. Digital availability of court documents increases possibilities for\nresearchers, who can access them as a source for building datasets -- whose\ndisclosure is aligned with good reproducibility practices in computational\nresearch. Large and digitized court systems, such as the Brazilian one, are\nprone to be explored in that sense. However, personal data protection laws\nimpose restrictions on data exposure and state principles about which\nresearchers should be mindful. Special caution must be taken in cases with\nhuman rights violations, such as gender discrimination, over which we elaborate\nas an example of interest. We present legal and ethical considerations on the\nissue, as well as guidelines for researchers dealing with this kind of data and\ndeciding whether to disclose it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05933,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Teaching Blockchain in K9-12: Instruction materials and their assessment\n\n  This paper analyses the feasibility of including emerging IT-topics into\nsecondary education (K9-12). We developed a class on Blockchain for K9-12\nstudents which delivers relevant content but is also indented to motivate them\nto further engage with the topic. Our course consists of theory material as\nwell as a hands-on application. Gamification techniques are utilised to\nencourage students and improve the learning experience. The assessment,\nconsisting of 166 students, shows a significant knowledge gain of students\nexposed to our materials especially when the theory and the hands-on materials\nare combined. Both, i.e., teaching material and chat application, are freely\navailable.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11883,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"CodEval: Improving Student Success In Programming Assignments\n\n  CodEval is a code evaluation tool that integrates with the Canvas Learning\nManagement System to automatically evaluates students' work within a few\nminutes of the submission. This early feedback allows students to catch and\ncorrect problems in their submissions before their submission is graded and\ngives them a clear idea of the quality of their submission. CodEval handles the\ntedious aspects of grading, such as compiling and running tests, leaving\ngraders more time to spend on the qualitative aspect of grading.\n  Before using CodEval, instructors would not have a clear view of the\nstudent's comprehension of the concept evaluated by the assignment until after\nthe due date. CodeEval helps instructors identify and address the gaps in\nstudents' understanding and thus helps more students successfully complete the\nassignment.\n  We implemented CodEval using Python using the public Canvas API. Any\ninstructor or grader for a Canvas course can use CodEval to automatically\nevaluate submissions for programming assignments. We developed a syntax to\nexpress requirements of submissions such as compilation parameters, inputs,\noutputs, command-line arguments, timeouts, exit codes, functions used, files\ngenerated, output validators, and more. We have made CodEval open source.\n  CodEval is an easy tool for students, graders, and instructors and seamlessly\nintegrates with Canvas. We share our experience with using CodEval in two\nclasses with a total of 90 students and multiple coding assignments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05832,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000068545,
      "text":"A new technology perspective of the Metaverse: its essence, framework\n  and challenges\n\n  The Metaverse depicts a parallel digitalized world where virtuality and\nreality are fused. It has economic and social systems like those in the real\nworld and provides intelligent services and applications. In this paper, we\nintroduce the Metaverse from a new technology perspective, including its\nessence, corresponding technical framework, and potential technical challenges.\nSpecifically, we analyze the essence of the Metaverse from its etymology and\npoint out breakthroughs promising to be made in time, space, and contents of\nthe Metaverse by citing Maslow's Hierarchy of Needs. Subsequently, we conclude\nfour pillars of the Metaverse, named ubiquitous connections, space convergence,\nvirtuality and reality interaction, and human-centered communication, and\nestablish a corresponding technical framework. Additionally, we envision open\nissues and challenges of the Metaverse in the technical aspect. The work\nproposes a new technology perspective of the Metaverse and will provide further\nguidance for its technology development in the future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.0583,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Turning disruptive power of Blockchain in the insurance market into\n  innovative opportunities\n\n  Insurance has been around for more than centuries. This risk mitigation\nstrategy has been utilized in maritime commerce as early thousand years ago,\nwhere Asian merchant seafarers were pooling together their wares in collective\nfunds to pay for damages of individual capsized ship. In 2018, insurance\nindustry made up 6 percent of global GDP while financial industry amounted to\nabout 7 to 9 percent of the US GDP.2020, the industry net premiums totaled\nUSD1.28 trillion, by 2030, blockchain insurance market value is estimated to\nreach USD39.5 Billion. Despite of growing reform, the insurance market is\ndominated by intermediaries assisting people to match their insurance needs.\nWhile many predictions focused on artificial intelligence, cloud computing,\nblockchain stands out as the most disruptive technology that can change the\ndriving forces underlying the global economy. This paper presents a blockchain\nbusiness use case and how insurance industry can turn blockchain disruptive\npower into innovative opportunities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.09865,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000059605,
      "text":"Gender Bias in Big Data Analysis\n\n  This article combines humanistic \"data critique\" with informed inspection of\nbig data analysis. It measures gender bias when gender prediction software\ntools (Gender API, Namsor, and Genderize.io) are used in historical big data\nresearch. Gender bias is measured by contrasting personally identified computer\nscience authors in the well-regarded DBLP dataset (1950-1980) with exactly\ncomparable results from the software tools. Implications for public\nunderstanding of gender bias in computing and the nature of the computing\nprofession are outlined. Preliminary assessment of the Semantic Scholar dataset\nis presented. The conclusion combines humanistic approaches with selective use\nof big data methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05934,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Decarbonizing Indian Electricity Grid\n\n  India, being one of the fastest growing economies of the world, must take a\nsustainable path for development. India is responsible for 7 percent of global\nCO2 emissions. The electricity sector accounts for nearly 35 percent of\nemissions from the country. The switch from fossil fuels to renewable sources\nis the key in decarbonizing this sector and is considered as the crucial step\nfor climate mitigation. This research investigates the potential of renewable\nenergy sources; wind, solar and hydro. The optimization model developed in this\nstudy analyzes various scenarios for the transition to a sustainable future.\nThe results show that India aims to achieve 450 GW of installed capacity from\nRES is far from a Net Zero future. Results confirm that India has the potential\nto meet 100 percent of electricity demand in 2030 from RES including wind,\nsolar and hydro. Introducing Social Cost of Carbon is a viable option to reduce\nemissions in India. However, due to the low cost of coal, high coal taxes do\nnot lead to reduced emissions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06203,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Normative Challenges of Risk Regulation of Artificial Intelligence and\n  Automated Decision-Making\n\n  Recent proposals aiming at regulating artificial intelligence (AI) and\nautomated decision-making (ADM) suggest a particular form of risk regulation,\ni.e. a risk-based approach. The most salient example is the Artificial\nIntelligence Act (AIA) proposed by the European Commission. The article\naddresses challenges for adequate risk regulation that arise primarily from the\nspecific type of risks involved, i.e. risks to the protection of fundamental\nrights and fundamental societal values. They result mainly from the normative\nambiguity of the fundamental rights and societal values in interpreting,\nspecifying or operationalising them for risk assessments. This is exemplified\nfor (1) human dignity, (2) informational self-determination, data protection\nand privacy, (3) justice and fairness, and (4) the common good. Normative\nambiguities require normative choices, which are distributed among different\nactors in the proposed AIA. Particularly critical normative choices are those\nof selecting normative conceptions for specifying risks, aggregating and\nquantifying risks including the use of metrics, balancing of value conflicts,\nsetting levels of acceptable risks, and standardisation. To avoid a lack of\ndemocratic legitimacy and legal uncertainty, scientific and political debates\nare suggested.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.02274,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000118547,
      "text":"Rally and WebScience: A Platform and Toolkit for Browser-Based Research\n  on Technology and Society Problems\n\n  Empirical technology and society research is in a methodological crisis.\nProblems increasingly involve closed platforms, targeted content, and\ncontext-specific behavior. Prevailing research methods, such as surveys, tasks,\nand web crawls, pose design and ecological validity limitations.\n  Deploying studies in participant browsers and devices is a promising\ndirection. These vantage points can observe individualized experiences and\nimplement UI interventions in real settings.\n  We survey scholarship that uses these methods, annotating 284 sampled papers.\nOur analysis demonstrates their potential, but also recurring implementation\nbarriers and shortcomings.\n  We then present Rally and sdkName, a platform and toolkit for browser-based\nresearch. These systems lower implementation barriers and advance the science\nof measuring online behavior.\n  Finally, we evaluate Rally and sdkName against our design goals. We report\nresults from a one-month pilot study on news engagement, analyzing 4,466,200\nwebpage visits from 1,817 participants. We also present observations from\ninterviews with researchers using these systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01941,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000402,
      "text":"Systematic Design and Evaluation of Social Determinants of Health\n  Ontology (SDoHO)\n\n  Social determinants of health (SDoH) have a significant impact on health\noutcomes and well-being. Addressing SDoH is the key to reducing healthcare\ninequalities and transforming a \"sick care\" system into a \"health promoting\"\nsystem. To address the SDOH terminology gap and better embed relevant elements\nin advanced biomedical informatics, we propose an SDoH ontology (SDoHO), which\nrepresents fundamental SDoH factors and their relationships in a standardized\nand measurable way. The ontology formally models classes, relationships, and\nconstraints based on multiple SDoH-related resources. Expert review and\ncoverage evaluation, using clinical notes data and a national survey, showed\nsatisfactory results. SDoHO could potentially play an essential role in\nproviding a foundation for a comprehensive understanding of the associations\nbetween SDoH and health outcomes and providing a path toward health equity\nacross populations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.1105,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"CNN waste classification project report\n\n  This report is about waste management project. We used CNN as classifier to\nclassify waste image captured from mobile phone. Our model can identify 6 waste\nclasses with highly accurate and our model is successfully transferred into IOS\nplatform as application by swift. In addition, this report also introduced some\nbasic project management from planning project to landing project, for instance\nusing agile development to develop this waste app.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.11864,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"How can we combat online misinformation? A systematic overview of\n  current interventions and their efficacy\n\n  The spread of misinformation is a pressing global problem that has elicited a\nrange of responses from researchers, policymakers, civil society and industry.\nOver the past decade, these stakeholders have developed many interventions to\ntackle misinformation that vary across factors such as which effects of\nmisinformation they hope to target, at what stage in the misinformation\nlifecycle they are aimed at, and who they are implemented by. These\ninterventions also differ in how effective they are at reducing susceptibility\nto (and curbing the spread of) misinformation. In recent years, a vast amount\nof scholarly work on misinformation has become available, which extends across\nmultiple disciplines and methodologies. It has become increasingly difficult to\ncomprehensively map all of the available interventions, assess their efficacy,\nand understand the challenges, opportunities and tradeoffs associated with\nusing them. Few papers have systematically assessed and compared the various\ninterventions, which has led to a lack of understanding in civic and\npolicymaking discourses. With this in mind, we develop a new hierarchical\nframework for understanding interventions against misinformation online. The\nframework comprises three key elements: Interventions that Prepare people to be\nless susceptible; Interventions that Curb the spread and effects of\nmisinformation; and Interventions that Respond to misinformation. We outline\nhow different interventions are thought to work, categorise them, and summarise\nthe available evidence on their efficacy; offering researchers, policymakers\nand practitioners working to combat online misinformation both an analytical\nframework that they can use to understand and evaluate different interventions\n(and which could be extended to address new interventions that we do not\ndescribe here) and a summary of the range of interventions that have been\nproposed to date.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05828,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"Wikipedia's Balancing Act: A Tool for Collective Intelligence or Mass\n  Surveillance?\n\n  Wikipedia has evolved beyond its original function as an online encyclopedia\nin an increasingly complex data-driven society. The social platform is met with\na balancing act between collective intelligence and mass surveillance;\nprocesses need to be developed to protect individuals and the community from\ngovernment mass surveillance without sacrificing the important contributions\nmade through prohibited anonymous communication software. Case studies are\nprovided from NSA government surveillance practices, the anti-SOPA legislation\nmovement, and research that covers Wikipedia's involvement with participatory\njournalism, disinformation, self-censorship, and the use of Tor. This paper\nproposes that a common ground can be developed between individuals, public and\nprivate institutions through future research in socio-cultural anthropology and\npolicy frameworks around data retention and government accountability.\nWikipedia is used as an example within the US intelligence community as a\ncomplex organisation that can adapt to changes through its iterative nature,\nwhich draws insight into how policy frameworks can be future-proofed. Finally,\nthis paper is a wake-up call to individuals, private institutions, and\ngovernments to remain vigilant about the storage and use of personal\ninformation as a result of contributing to online communities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.0381,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0001146396,
      "text":"The Social Emotional Web\n\n  The social web has linked people on a global scale, transforming how we\ncommunicate and interact. The massive interconnectedness has created new\nvulnerabilities in the form of social manipulation and misinformation. As the\nsocial web matures, we are entering a new phase, where people share their\nprivate feelings and emotions. This so-called social emotional web creates new\nopportunities for human flourishing, but also exposes new vulnerabilities. To\nreap the benefits of the social emotional web, and reduce potential harms, we\nmust anticipate how it will evolve and create policies that minimize risks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.0496,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"BigScience: A Case Study in the Social Construction of a Multilingual\n  Large Language Model\n\n  The BigScience Workshop was a value-driven initiative that spanned one and\nhalf years of interdisciplinary research and culminated in the creation of\nROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the\nlargest multilingual language models to date. In addition to the technical\noutcomes and artifacts, the workshop fostered multidisciplinary collaborations\naround large models, datasets, and their analysis. This in turn led to a wide\nrange of research publications spanning topics from ethics to law, data\ngovernance, modeling choices and distributed training. This paper focuses on\nthe collaborative research aspects of BigScience and takes a step back to look\nat the challenges of large-scale participatory research, with respect to\nparticipant diversity and the tasks required to successfully carry out such a\nproject. Our main goal is to share the lessons we learned from this experience,\nwhat we could have done better and what we did well. We show how the impact of\nsuch a social approach to scientific research goes well beyond the technical\nartifacts that were the basis of its inception.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01225,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000144707,
      "text":"A Game of NFTs: Characterizing NFT Wash Trading in the Ethereum\n  Blockchain\n\n  The Non-Fungible Token (NFT) market in the Ethereum blockchain experienced\nexplosive growth in 2021, with a monthly trade volume reaching \\$6 billion in\nJanuary 2022. However, concerns have emerged about possible wash trading, a\nform of market manipulation in which one party repeatedly trades an NFT to\ninflate its volume artificially. Our research examines the effects of wash\ntrading on the NFT market in Ethereum from the beginning until January 2022,\nusing multiple approaches. We find that wash trading affects 5.66% of all NFT\ncollections, with a total artificial volume of \\$3,406,110,774. We look at two\nways to profit from wash trading: Artificially increasing the price of the NFT\nand taking advantage of the token reward systems provided by some marketplaces.\nOur findings show that exploiting the token reward systems of NFTMs is much\nmore profitable (mean gain of successful operations is \\$1.055M on LooksRare),\nmore likely to succeed (more than 80% of operations), and less risky than\nreselling an NFT at a higher price using wash trading (50% of activities result\nin a loss). Our research highlights that wash trading is frequent in Ethereum\nand that NFTMs should implement protective mechanisms to stop such illicit\nbehavior.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.06894,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"IIVA: A Simulation Based Generalized Framework for Interdependent\n  Infrastructure Vulnerability Assessment\n\n  Accurate vulnerability assessment of critical infrastructure systems is\ncardinal to enhance infrastructure resilience. Unlike traditional approaches,\nthis paper proposes a novel infrastructure vulnerability assessment framework\nthat accounts for: various types of infrastructure interdependencies including\nphysical, logical and geographical from a holistic perspective; lack\nof\/incomplete information on supply-demand flow characteristics of\ninterdependent infrastructure; and, unavailability\/inadequate data on\ninfrastructure network topology and\/or interdependencies. Specifically, this\npaper models multi-infrastructure vulnerabilities leveraging simulation-based\nhybrid approach coupled with time-dependent Bayesian network analysis while\nconsidering cascading failures within and across CIS networks, under incomplete\ninformation. Existing synthetic data on electricity, water and supply chain\nnetworks are used to implement\/validate the framework. Infrastructure\nvulnerabilities are depicted on a geo-map using Voronoi polygons. Our results\nindicate that infrastructure vulnerability is inversely proportional to the\nnumber of redundancies inbuilt in the infrastructure system, indicating that\nallocating resources to add redundancies in an existing infrastructure system\nis essential to reduce its risk of failure. It is observed that higher the\ninitial failure rate of the components, higher is the vulnerability of the\ninfrastructure, highlighting the importance of modernizing and upgrading the\ninfrastructure system aiming to reduce the initial failure probabilities. Our\nresults also underline the importance of collaborative working and sharing the\nnecessary information among multiple infrastructure systems, aiming towards\nminimizing the overall failure risk of interdependent infrastructure systems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07903,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000145038,
      "text":"The First IEEE UV2022 Mathematical Modelling Competition: Backgrounds\n  and Problems\n\n  Economic growth, people's health, and urban development face challenges in\nthe post-epidemic era. How to promote high-quality and sustainable urban\ndevelopment, improve citizens' sense of happiness, and solve problems in city\nmanagement have become a heated and crucial topic. Mathematical modeling is a\nresearch method that uses mathematical symbols to express practical problems,\nestablish mathematical models, and then propose solutions. The 1$^{st}$ IEEE\nUV2022 Mathematical Modelling Competition is a satellite activity of the\n6$^{th}$ IEEE International Conference on Universal Village, which expects\nparticipants to use mathematical modeling methods for practical problems and\nprovide guidelines for sustainable social progress. This short paper introduces\nthe background of the competition and publishes the problems to be solved.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.02795,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000059936,
      "text":"Emerging Technology and Policy Co-Design Considerations for the Safe and\n  Transparent Use of Small Unmanned Aerial Systems\n\n  The rapid technological growth observed in the sUAS sector over the past\ndecade has been unprecedented and has left gaps in policies and regulations to\nadequately provide for a safe and trusted environment in which to operate these\ndevices. The Center for Security in Politics at UC Berkeley, via a two-day\nworkshop, analyzed these gaps by addressing the entire sUAS vertical. From\nhuman factors to autonomy, we recommend a series of steps that can be taken by\npartners in the academic, commercial, and government sectors to reduce policy\ngaps introduced in the wake of the growth of the sUAS industry.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.06495,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Inherent Limitations of AI Fairness\n\n  As the real-world impact of Artificial Intelligence (AI) systems has been\nsteadily growing, so too have these systems come under increasing scrutiny. In\nresponse, the study of AI fairness has rapidly developed into a rich field of\nresearch with links to computer science, social science, law, and philosophy.\nMany technical solutions for measuring and achieving AI fairness have been\nproposed, yet their approach has been criticized in recent years for being\nmisleading, unrealistic and harmful.\n  In our paper, we survey these criticisms of AI fairness and identify key\nlimitations that are inherent to the prototypical paradigm of AI fairness. By\ncarefully outlining the extent to which technical solutions can realistically\nhelp in achieving AI fairness, we aim to provide the background necessary to\nform a nuanced opinion on developments in fair AI. This delineation also\nprovides research opportunities for non-AI solutions peripheral to AI systems\nin supporting fair decision processes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07676,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Inequality, Crime and Public Health: A Survey of Emerging Trends in\n  Urban Data Science\n\n  Urban agglomerations are constantly and rapidly evolving ecosystems, with\nglobalization and increasing urbanization posing new challenges in sustainable\nurban development well summarized in the United Nations' Sustainable\nDevelopment Goals (SDGs). The advent of the digital age generated by modern\nalternative data sources provides new tools to tackle these challenges with\nspatio-temporal scales that were previously unavailable with census statistics.\nIn this review, we present how new digital data sources are employed to provide\ndata-driven insights to study and track (i) urban crime and public safety; (ii)\nsocioeconomic inequalities and segregation; and (iii) public health, with a\nparticular focus on the city scale.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2301.10232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Combating harmful Internet use with peer assessment and differential\n  evolution\n\n  Harmful Internet use (HIU) is a term coined for the unintended use of the\nInternet. In this study, we propose a more accurate HIU measuring method based\non the peer assessment and differential evolution approach. The sample data\ncomprises a juvenile population in Poland; 267 subjects assessed 1,513 peers.\nIn addition to classic statistical analysis, differential evolution has been\nemployed. Results indicate that there may be a substantially higher rate of HIU\nthan other studies have indicated. More accurate measurement of the adolescent\npopulation influx affected by HIU is needed for healthcare and welfare system\nplanning.\n  Presented in Prague, Czech Republic, 20-22 July 2022.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.14688,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Trust Management in the Internet of Everything\n\n  Digitalization is leading us towards a future where people, processes, data\nand things are not only interacting with each other, but might start forming\nsocieties on their own. In these dynamic systems enhanced by artificial\nintelligence, trust management on the level of human-to-machine as well as\nmachine-to-machine interaction becomes an essential ingredient in supervising\nsafe and secure progress of our digitalized future. This tutorial paper\ndiscusses the essential elements of trust management in complex digital\necosystems, guiding the reader through the definitions and core concepts of\ntrust management. Furthermore, it explains how trust-building can be leveraged\nto support people in safe interaction with other (possibly autonomous) digital\nagents, as trust governance may allow the ecosystem to trigger an auto-immune\nresponse towards untrusted digital agents, protecting human safety.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.12007,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Designing Equitable Transit Networks\n\n  Public transit is an essential infrastructure enabling access to employment,\nhealthcare, education, and recreational facilities. While accessibility to\ntransit is important in general, some sections of the population depend\ncritically on transit. However, existing public transit is often not designed\nequitably, and often, equity is only considered as an additional objective post\nhoc, which hampers systemic changes. We present a formulation for transit\nnetwork design that considers different notions of equity and welfare\nexplicitly. We study the interaction between network design and various\nconcepts of equity and present trade-offs and results based on real-world data\nfrom a large metropolitan area in the United States of America.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01714,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"A survey on grading format of automated grading tools for programming\n  assignments\n\n  The prevalence of online platforms and studies has generated the demand for\nautomated grading tools, and as a result, there are plenty in the market. Such\ntools are developed to grade coding assignments quickly, accurately, and\neffortlessly. Since there are varieties of tools available to cater to the\ndiverse options of programming languages and concepts, it is overwhelming for\nany instructor to decide which one suits one's requirements. There are several\nsurveys studying the tools and giving insights into how they function and what\nthey support. However other than knowing the functionality, it is important for\nan instructor to know how the assignments are graded and what is the format of\nthe test cases. This is crucial since the instructor has to design the grading\nformat and therefore requires a learning curve. This survey studies and\nevaluates the automated grading tools based on their evaluation format. This in\nturn helps a reader in deciding which tool to choose and provides an insight\ninto what are the assessment settings and approaches used in grading the coding\nassignment in any specific grading tool.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05435,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Wireless earbuds for low-cost hearing screening\n\n  We present the first wireless earbud hardware that can perform hearing\nscreening by detecting otoacoustic emissions. The conventional wisdom has been\nthat detecting otoacoustic emissions, which are the faint sounds generated by\nthe cochlea, requires sensitive and expensive acoustic hardware. Thus, medical\ndevices for hearing screening cost thousands of dollars and are inaccessible in\nlow and middle income countries. We show that by designing wireless earbuds\nusing low-cost acoustic hardware and combining them with wireless sensing\nalgorithms, we can reliably identify otoacoustic emissions and perform hearing\nscreening. Our algorithms combine frequency modulated chirps with wideband\npulses emitted from a low-cost speaker to reliably separate otoacoustic\nemissions from in-ear reflections and echoes. We conducted a clinical study\nwith 50 ears across two healthcare sites. Our study shows that the low-cost\nearbuds detect hearing loss with 100% sensitivity and 89.7% specificity, which\nis comparable to the performance of a $8000 medical device. By developing\nlow-cost and open-source wearable technology, our work may help address global\nhealth inequities in hearing screening by democratizing these medical devices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03497,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap\n  Adaptation for Highway On-Ramp Merging\n\n  A platoon refers to a group of vehicles traveling together in very close\nproximity using automated driving technology. Owing to its immense capacity to\nimprove fuel efficiency, driving safety, and driver comfort, platooning\ntechnology has garnered substantial attention from the autonomous vehicle\nresearch community. Although highly advantageous, recent research has uncovered\nthat an excessively small intra-platoon gap can impede traffic flow during\nhighway on-ramp merging. While existing control-based methods allow for\nadaptation of the intra-platoon gap to improve traffic flow, making an optimal\ncontrol decision under the complex dynamics of traffic conditions remains a\nchallenge due to the massive computational complexity. In this paper, we\npresent the design, implementation, and evaluation of a novel reinforcement\nlearning framework that adaptively adjusts the intra-platoon gap of an\nindividual platoon member to maximize traffic flow in response to dynamically\nchanging, complex traffic conditions for highway on-ramp merging. The\nframework's state space has been meticulously designed in consultation with the\ntransportation literature to take into account critical traffic parameters that\nbear direct relevance to merging efficiency. An intra-platoon gap decision\nmaking method based on the deep deterministic policy gradient algorithm is\ncreated to incorporate the continuous action space to ensure precise and\ncontinuous adaptation of the intra-platoon gap. An extensive simulation study\ndemonstrates the effectiveness of the reinforcement learning-based approach for\nsignificantly improving traffic flow in various highway on-ramp merging\nscenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.13995,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"The Right to be an Exception to a Data-Driven Rule\n\n  Data-driven tools are increasingly used to make consequential decisions. They\nhave begun to advise employers on which job applicants to interview, judges on\nwhich defendants to grant bail, lenders on which homeowners to give loans, and\nmore. In such settings, different data-driven rules result in different\ndecisions. The problem is: to every data-driven rule, there are exceptions.\nWhile a data-driven rule may be appropriate for some, it may not be appropriate\nfor all. As data-driven decisions become more common, there are cases in which\nit becomes necessary to protect the individuals who, through no fault of their\nown, are the data-driven exceptions. At the same time, it is impossible to\nscrutinize every one of the increasing number of data-driven decisions, begging\nthe question: When and how should data-driven exceptions be protected?\n  In this piece, we argue that individuals have the right to be an exception to\na data-driven rule. That is, the presumption should not be that a data-driven\nrule--even one with high accuracy--is suitable for an arbitrary\ndecision-subject of interest. Rather, a decision-maker should apply the rule\nonly if they have exercised due care and due diligence (relative to the risk of\nharm) in excluding the possibility that the decision-subject is an exception to\nthe data-driven rule. In some cases, the risk of harm may be so low that only\ncursory consideration is required. Although applying due care and due diligence\nis meaningful in human-driven decision contexts, it is unclear what it means\nfor a data-driven rule to do so. We propose that determining whether a\ndata-driven rule is suitable for a given decision-subject requires the\nconsideration of three factors: individualization, uncertainty, and harm. We\nunpack this right in detail, providing a framework for assessing data-driven\nrules and describing what it would mean to invoke the right in practice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.14471,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000181463,
      "text":"Voices of Workers: Why a Worker-Centered Approach to Crowd Work Is\n  Challenging\n\n  How can we better understand the broad, diverse, shifting, and invisible\ncrowd workforce, so that we can better support it? We present findings from\nonline observations and analysis of publicly available postings from a\ncommunity forum of crowd workers. In particular, we observed recurring tensions\nbetween crowd workers and journalists regarding media depictions of crowd work.\nWe found that crowd diversity makes any one-dimensional representation\ninadequate in addressing the wide-ranging experiences of crowd work. We argue\nthat the scale, diversity, invisibility, and the crowds' resistance to\npublicity make a worker-centered approach to crowd work particularly\nchallenging, necessitating better understanding the diversity of workers and\ntheir lived experiences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  }
]