[
  {
    "arxiv_id":2001.04062,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Modeling of Pruning Techniques for Deep Neural Networks Simplification\n\n  Convolutional Neural Networks (CNNs) suffer from different issues, such as\ncomputational complexity and the number of parameters. In recent years pruning\ntechniques are employed to reduce the number of operations and model size in\nCNNs. Different pruning methods are proposed, which are based on pruning the\nconnections, channels, and filters. Various techniques and tricks accompany\npruning methods, and there is not a unifying framework to model all the pruning\nmethods. In this paper pruning methods are investigated, and a general model\nwhich is contained the majority of pruning techniques is proposed. The\nadvantages and disadvantages of the pruning methods can be identified, and all\nof them can be summarized under this model. The final goal of this model is to\nprovide a general approach for all of the pruning methods with different\nstructures and applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.01802,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000063909,
      "text":"Implementation of the VBM3D Video Denoising Method and Some Variants\n\n  VBM3D is an extension to video of the well known image denoising algorithm\nBM3D, which takes advantage of the sparse representation of stacks of similar\npatches in a transform domain. The extension is rather straightforward: the\nsimilar 2D patches are taken from a spatio-temporal neighborhood which includes\nneighboring frames. In spite of its simplicity, the algorithm offers a good\ntrade-off between denoising performance and computational complexity. In this\nwork we revisit this method, providing an open-source C++ implementation\nreproducing the results. A detailed description is given and the choice of\nparameters is thoroughly discussed. Furthermore, we discuss several extensions\nof the original algorithm: (1) a multi-scale implementation, (2) the use of 3D\npatches, (3) the use of optical flow to guide the patch search. These\nextensions allow to obtain results which are competitive with even the most\nrecent state of the art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.00187,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation\n\n  Human gaze is essential for various appealing applications. Aiming at more\naccurate gaze estimation, a series of recent works propose to utilize face and\neye images simultaneously. Nevertheless, face and eye images only serve as\nindependent or parallel feature sources in those works, the intrinsic\ncorrelation between their features is overlooked. In this paper we make the\nfollowing contributions: 1) We propose a coarse-to-fine strategy which\nestimates a basic gaze direction from face image and refines it with\ncorresponding residual predicted from eye images. 2) Guided by the proposed\nstrategy, we design a framework which introduces a bi-gram model to bridge gaze\nresidual and basic gaze direction, and an attention component to adaptively\nacquire suitable fine-grained feature. 3) Integrating the above innovations, we\nconstruct a coarse-to-fine adaptive network named CA-Net and achieve\nstate-of-the-art performances on MPIIGaze and EyeDiap.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.08514,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000089076,
      "text":"Filter Sketch for Network Pruning\n\n  We propose a novel network pruning approach by information preserving of\npre-trained network weights (filters). Network pruning with the information\npreserving is formulated as a matrix sketch problem, which is efficiently\nsolved by the off-the-shelf Frequent Direction method. Our approach, referred\nto as FilterSketch, encodes the second-order information of pre-trained\nweights, which enables the representation capacity of pruned networks to be\nrecovered with a simple fine-tuning procedure. FilterSketch requires neither\ntraining from scratch nor data-driven iterative optimization, leading to a\nseveral-orders-of-magnitude reduction of time cost in the optimization of\npruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3% of FLOPs\nand prunes 59.9% of network parameters with negligible accuracy cost for\nResNet-110. On ILSVRC-2012, it reduces 45.5% of FLOPs and removes 43.0% of\nparameters with only 0.69% accuracy drop for ResNet-50. Our code and pruned\nmodels can be found at https:\/\/github.com\/lmbxmu\/FilterSketch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.09308,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0001511309,
      "text":"Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of\n  Sentence in Video\n\n  In this paper, we study the problem of weakly-supervised temporal grounding\nof sentence in video. Specifically, given an untrimmed video and a query\nsentence, our goal is to localize a temporal segment in the video that\nsemantically corresponds to the query sentence, with no reliance on any\ntemporal annotation during training. We propose a two-stage model to tackle\nthis problem in a coarse-to-fine manner. In the coarse stage, we first generate\na set of fixed-length temporal proposals using multi-scale sliding windows, and\nmatch their visual features against the sentence features to identify the\nbest-matched proposal as a coarse grounding result. In the fine stage, we\nperform a fine-grained matching between the visual features of the frames in\nthe best-matched proposal and the sentence features to locate the precise frame\nboundary of the fine grounding result. Comprehensive experiments on the\nActivityNet Captions dataset and the Charades-STA dataset demonstrate that our\ntwo-stage model achieves compelling performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.11122,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in\n  Untrimmed Sequences\n\n  Understanding the structure of complex activities in untrimmed videos is a\nchallenging task in the area of action recognition. One problem here is that\nthis task usually requires a large amount of hand-annotated minute- or even\nhour-long video data, but annotating such data is very time consuming and can\nnot easily be automated or scaled. To address this problem, this paper proposes\nan approach for the unsupervised learning of actions in untrimmed video\nsequences based on a joint visual-temporal embedding space. To this end, we\ncombine a visual embedding based on a predictive U-Net architecture with a\ntemporal continuous function. The resulting representation space allows\ndetecting relevant action clusters based on their visual as well as their\ntemporal appearance. The proposed method is evaluated on three standard\nbenchmark datasets, Breakfast Actions, INRIA YouTube Instructional Videos, and\n50 Salads. We show that the proposed approach is able to provide a meaningful\nvisual and temporal embedding out of the visual cues present in contiguous\nvideo frames and is suitable for the task of unsupervised temporal segmentation\nof actions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.07323,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000051657,
      "text":"Face Verification via learning the kernel matrix\n\n  The kernel function is introduced to solve the nonlinear pattern recognition\nproblem. The advantage of a kernel method often depends critically on a proper\nchoice of the kernel function. A promising approach is to learn the kernel from\ndata automatically. Over the past few years, some methods which have been\nproposed to learn the kernel have some limitations: learning the parameters of\nsome prespecified kernel function and so on. In this paper, the nonlinear face\nverification via learning the kernel matrix is proposed. A new criterion is\nused in the new algorithm to avoid inverting the possibly singular within-class\nwhich is a computational problem. The experimental results obtained on the\nfacial database XM2VTS using the Lausanne protocol show that the verification\nperformance of the new method is superior to that of the primary method Client\nSpecific Kernel Discriminant Analysis (CSKDA). The method CSKDA needs to choose\na proper kernel function through many experiments, while the new method could\nlearn the kernel from data automatically which could save a lot of time and\nhave the robust performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04735,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"NODIS: Neural Ordinary Differential Scene Understanding\n\n  Semantic image understanding is a challenging topic in computer vision. It\nrequires to detect all objects in an image, but also to identify all the\nrelations between them. Detected objects, their labels and the discovered\nrelations can be used to construct a scene graph which provides an abstract\nsemantic interpretation of an image. In previous works, relations were\nidentified by solving an assignment problem formulated as Mixed-Integer Linear\nPrograms. In this work, we interpret that formulation as Ordinary Differential\nEquation (ODE). The proposed architecture performs scene graph inference by\nsolving a neural variant of an ODE by end-to-end learning. It achieves\nstate-of-the-art results on all three benchmark tasks: scene graph generation\n(SGGen), classification (SGCls) and visual relationship detection (PredCls) on\nVisual Genome benchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03444,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Improving Image Autoencoder Embeddings with Perceptual Loss\n\n  Autoencoders are commonly trained using element-wise loss. However,\nelement-wise loss disregards high-level structures in the image which can lead\nto embeddings that disregard them as well. A recent improvement to autoencoders\nthat helps alleviate this problem is the use of perceptual loss. This work\ninvestigates perceptual loss from the perspective of encoder embeddings\nthemselves. Autoencoders are trained to embed images from three different\ncomputer vision datasets using perceptual loss based on a pretrained model as\nwell as pixel-wise loss. A host of different predictors are trained to perform\nobject positioning and classification on the datasets given the embedded images\nas input. The two kinds of losses are evaluated by comparing how the predictors\nperformed with embeddings from the differently trained autoencoders. The\nresults show that, in the image domain, the embeddings generated by\nautoencoders trained with perceptual loss enable more accurate predictions than\nthose trained with element-wise loss. Furthermore, the results show that, on\nthe task of object positioning of a small-scale feature, perceptual loss can\nimprove the results by a factor 10. The experimental setup is available online:\nhttps:\/\/github.com\/guspih\/Perceptual-Autoencoders\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.11561,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000078811,
      "text":"Dual Convolutional LSTM Network for Referring Image Segmentation\n\n  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.02359,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000272857,
      "text":"Weakly Supervised Visual Semantic Parsing\n\n  Scene Graph Generation (SGG) aims to extract entities, predicates and their\nsemantic structure from images, enabling deep understanding of visual content,\nwith many applications such as visual reasoning and image retrieval.\nNevertheless, existing SGG methods require millions of manually annotated\nbounding boxes for training, and are computationally inefficient, as they\nexhaustively process all pairs of object proposals to detect predicates. In\nthis paper, we address those two limitations by first proposing a generalized\nformulation of SGG, namely Visual Semantic Parsing, which disentangles entity\nand predicate recognition, and enables sub-quadratic performance. Then we\npropose the Visual Semantic Parsing Network, VSPNet, based on a dynamic,\nattention-based, bipartite message passing framework that jointly infers graph\nnodes and edges through an iterative process. Additionally, we propose the\nfirst graph-based weakly supervised learning framework, based on a novel graph\nalignment algorithm, which enables training without bounding box annotations.\nThrough extensive experiments, we show that VSPNet outperforms weakly\nsupervised baselines significantly and approaches fully supervised performance,\nwhile being several times faster. We publicly release the source code of our\nmethod.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.0868,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"Rethinking the Distribution Gap of Person Re-identification with\n  Camera-based Batch Normalization\n\n  The fundamental difficulty in person re-identification (ReID) lies in\nlearning the correspondence among individual cameras. It strongly demands\ncostly inter-camera annotations, yet the trained models are not guaranteed to\ntransfer well to previously unseen cameras. These problems significantly limit\nthe application of ReID. This paper rethinks the working mechanism of\nconventional ReID approaches and puts forward a new solution. With an effective\noperator named Camera-based Batch Normalization (CBN), we force the image data\nof all cameras to fall onto the same subspace, so that the distribution gap\nbetween any camera pair is largely shrunk. This alignment brings two benefits.\nFirst, the trained model enjoys better abilities to generalize across scenarios\nwith unseen cameras as well as transfer across multiple training sets. Second,\nwe can rely on intra-camera annotations, which have been undervalued before due\nto the lack of cross-camera information, to achieve competitive ReID\nperformance. Experiments on a wide range of ReID tasks demonstrate the\neffectiveness of our approach. The code is available at\nhttps:\/\/github.com\/automan000\/Camera-based-Person-ReID.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05979,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Contextual Sense Making by Fusing Scene Classification, Detections, and\n  Events in Full Motion Video\n\n  With the proliferation of imaging sensors, the volume of multi-modal imagery\nfar exceeds the ability of human analysts to adequately consume and exploit it.\nFull motion video (FMV) possesses the extra challenge of containing large\namounts of redundant temporal data. We aim to address the needs of human\nanalysts to consume and exploit data given aerial FMV. We have investigated and\ndesigned a system capable of detecting events and activities of interest that\ndeviate from the baseline patterns of observation given FMV feeds. We have\ndivided the problem into three tasks: (1) Context awareness, (2) object\ncataloging, and (3) event detection. The goal of context awareness is to\nconstraint the problem of visual search and detection in video data. A custom\nimage classifier categorizes the scene with one or multiple labels to identify\nthe operating context and environment. This step helps reducing the semantic\nsearch space of downstream tasks in order to increase their accuracy. The\nsecond step is object cataloging, where an ensemble of object detectors locates\nand labels any known objects found in the scene (people, vehicles, boats,\nplanes, buildings, etc.). Finally, context information and detections are sent\nto the event detection engine to monitor for certain behaviors. A series of\nanalytics monitor the scene by tracking object counts, and object interactions.\nIf these object interactions are not declared to be commonly observed in the\ncurrent scene, the system will report, geolocate, and log the event. Events of\ninterest include identifying a gathering of people as a meeting and\/or a crowd,\nalerting when there are boats on a beach unloading cargo, increased count of\npeople entering a building, people getting in and\/or out of vehicles of\ninterest, etc. We have applied our methods on data from different sensors at\ndifferent resolutions in a variety of geographical areas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.05744,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000155634,
      "text":"SketchDesc: Learning Local Sketch Descriptors for Multi-view\n  Correspondence\n\n  In this paper, we study the problem of multi-view sketch correspondence,\nwhere we take as input multiple freehand sketches with different views of the\nsame object and predict as output the semantic correspondence among the\nsketches. This problem is challenging since the visual features of\ncorresponding points at different views can be very different. To this end, we\ntake a deep learning approach and learn a novel local sketch descriptor from\ndata. We contribute a training dataset by generating the pixel-level\ncorrespondence for the multi-view line drawings synthesized from 3D shapes. To\nhandle the sparsity and ambiguity of sketches, we design a novel multi-branch\nneural network that integrates a patch-based representation and a multi-scale\nstrategy to learn the pixel-level correspondence among multi-view sketches. We\ndemonstrate the effectiveness of our proposed approach with extensive\nexperiments on hand-drawn sketches and multi-view line drawings rendered from\nmultiple 3D shape datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.04388,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000208616,
      "text":"RoutedFusion: Learning Real-time Depth Map Fusion\n\n  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.11207,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000139409,
      "text":"Weakly Supervised Instance Segmentation by Deep Community Learning\n\n  We present a weakly supervised instance segmentation algorithm based on deep\ncommunity learning with multiple tasks. This task is formulated as a\ncombination of weakly supervised object detection and semantic segmentation,\nwhere individual objects of the same class are identified and segmented\nseparately. We address this problem by designing a unified deep neural network\narchitecture, which has a positive feedback loop of object detection with\nbounding box regression, instance mask generation, instance segmentation, and\nfeature extraction. Each component of the network makes active interactions\nwith others to improve accuracy, and the end-to-end trainability of our model\nmakes our results more robust and reproducible. The proposed algorithm achieves\nstate-of-the-art performance in the weakly supervised setting without any\nadditional training such as Fast R-CNN and Mask R-CNN on the standard benchmark\ndataset. The implementation of our algorithm is available on the project\nwebpage: https:\/\/cv.snu.ac.kr\/research\/WSIS_CL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.0129,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000087751,
      "text":"General Partial Label Learning via Dual Bipartite Graph Autoencoder\n\n  We formulate a practical yet challenging problem: General Partial Label\nLearning (GPLL). Compared to the traditional Partial Label Learning (PLL)\nproblem, GPLL relaxes the supervision assumption from instance-level -- a label\nset partially labels an instance -- to group-level: 1) a label set partially\nlabels a group of instances, where the within-group instance-label link\nannotations are missing, and 2) cross-group links are allowed -- instances in a\ngroup may be partially linked to the label set from another group. Such\nambiguous group-level supervision is more practical in real-world scenarios as\nadditional annotation on the instance-level is no longer required, e.g.,\nface-naming in videos where the group consists of faces in a frame, labeled by\na name set in the corresponding caption. In this paper, we propose a novel\ngraph convolutional network (GCN) called Dual Bipartite Graph Autoencoder\n(DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the\ncross-group correlations to represent the instance groups as dual bipartite\ngraphs: within-group and cross-group, which reciprocally complements each other\nto resolve the linking ambiguities. Second, we design a GCN autoencoder to\nencode and decode them, where the decodings are considered as the refined\nresults. It is worth noting that DB-GAE is self-supervised and transductive, as\nit only uses the group-level supervision without a separate offline training\nstage. Extensive experiments on two real-world datasets demonstrate that DB-GAE\nsignificantly outperforms the best baseline over absolute 0.159 F1-score and\n24.8% accuracy. We further offer analysis on various levels of label\nambiguities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.0375,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000094705,
      "text":"An Overview of Two Age Synthesis and Estimation Techniques\n\n  Age estimation is a technique for predicting human ages from digital facial\nimages, which analyzes a person's face image and estimates his\/her age based on\nthe year measure. Nowadays, intelligent age estimation and age synthesis have\nbecome particularly prevalent research topics in computer vision and face\nverification systems. Age synthesis is defined to render a facial image\naesthetically with rejuvenating and natural aging effects on the person's face.\nAge estimation is defined to label a facial image automatically with the age\ngroup (year range) or the exact age (year) of the person's face. In this case\nstudy, we overview the existing models, popular techniques, system\nperformances, and technical challenges related to the facial image-based age\nsynthesis and estimation topics. The main goal of this review is to provide an\neasy understanding and promising future directions with systematic discussions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.03509,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Deformable Groupwise Image Registration using Low-Rank and Sparse\n  Decomposition\n\n  Low-rank and sparse decompositions and robust PCA (RPCA) are highly\nsuccessful techniques in image processing and have recently found use in\ngroupwise image registration. In this paper, we investigate the drawbacks of\nthe most common RPCA-dissimi\\-larity metric in image registration and derive an\nimproved version. In particular, this new metric models low-rank requirements\nthrough explicit constraints instead of penalties and thus avoids the pitfalls\nof the established metric. Equipped with total variation regularization, we\npresent a theoretically justified multilevel scheme based on first-order\nprimal-dual optimization to solve the resulting non-parametric registration\nproblem. As confirmed by numerical experiments, our metric especially lends\nitself to data involving recurring changes in object appearance and potential\nsparse perturbations. We numerically compare its peformance to a number of\nrelated approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2001.10063,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Towards Open-Set Semantic Segmentation of Aerial Images\n\n  Classical and more recently deep computer vision methods are optimized for\nvisible spectrum images, commonly encoded in grayscale or RGB colorspaces\nacquired from smartphones or cameras. A more uncommon source of images\nexploited in the remote sensing field are satellite and aerial images. However,\nthe development of pattern recognition approaches for these data is relatively\nrecent, mainly due to the limited availability of this type of images, as until\nrecently they were used exclusively for military purposes. Access to aerial\nimagery, including spectral information, has been increasing mainly due to the\nlow cost of drones, cheapening of imaging satellite launch costs, and novel\npublic datasets. Usually remote sensing applications employ computer vision\ntechniques strictly modeled for classification tasks in closed set scenarios.\nHowever, real-world tasks rarely fit into closed set contexts, frequently\npresenting previously unknown classes, characterizing them as open set\nscenarios. Focusing on this problem, this is the first paper to study and\ndevelop semantic segmentation techniques for open set scenarios applied to\nremote sensing images. The main contributions of this paper are: 1) a\ndiscussion of related works in open set semantic segmentation, showing evidence\nthat these techniques can be adapted for open set remote sensing tasks; 2) the\ndevelopment and evaluation of a novel approach for open set semantic\nsegmentation. Our method yielded competitive results when compared to closed\nset methods for the same dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Adapted Center and Scale Prediction: More Stable and More Accurate\n\n  Pedestrian detection benefits from deep learning technology and gains rapid\ndevelopment in recent years. Most of detectors follow general object detection\nframe, i.e. default boxes and two-stage process. Recently, anchor-free and\none-stage detectors have been introduced into this area. However, their\naccuracies are unsatisfactory. Therefore, in order to enjoy the simplicity of\nanchor-free detectors and the accuracy of two-stage ones simultaneously, we\npropose some adaptations based on a detector, Center and Scale Prediction(CSP).\nThe main contributions of our paper are: (1) We improve the robustness of CSP\nand make it easier to train. (2) We propose a novel method to predict width,\nnamely compressing width. (3) We achieve the second best performance on\nCityPersons benchmark, i.e. 9.3% log-average miss rate(MR) on reasonable set,\n8.7% MR on partial set and 5.6% MR on bare set, which shows an anchor-free and\none-stage detector can still have high accuracy. (4) We explore some\ncapabilities of Switchable Normalization which are not mentioned in its\noriginal paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03651,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000374185,
      "text":"CRVOS: Clue Refining Network for Video Object Segmentation\n\n  The encoder-decoder based methods for semi-supervised video object\nsegmentation (Semi-VOS) have received extensive attention due to their superior\nperformances. However, most of them have complex intermediate networks which\ngenerate strong specifiers to be robust against challenging scenarios, and this\nis quite inefficient when dealing with relatively simple scenarios. To solve\nthis problem, we propose a real-time network, Clue Refining Network for Video\nObject Segmentation (CRVOS), that does not have any intermediate network to\nefficiently deal with these scenarios. In this work, we propose a simple\nspecifier, referred to as the Clue, which consists of the previous frame's\ncoarse mask and coordinates information. We also propose a novel refine module\nwhich shows the better performance compared with the general ones by using a\ndeconvolution layer instead of a bilinear upsampling layer. Our proposed method\nshows the fastest speed among the existing methods with a competitive accuracy.\nOn DAVIS 2016 validation set, our method achieves 63.5 fps and J&F score of\n81.6%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03312,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"FSD-10: A Dataset for Competitive Sports Content Analysis\n\n  Action recognition is an important and challenging problem in video analysis.\nAlthough the past decade has witnessed progress in action recognition with the\ndevelopment of deep learning, such process has been slow in competitive sports\ncontent analysis. To promote the research on action recognition from\ncompetitive sports video clips, we introduce a Figure Skating Dataset (FSD-10)\nfor finegrained sports content analysis. To this end, we collect 1484 clips\nfrom the worldwide figure skating championships in 2017-2018, which consist of\n10 different actions in men\/ladies programs. Each clip is at a rate of 30\nframes per second with resolution 1080 $\\times$ 720. These clips are then\nannotated by experts in type, grade of execution, skater info, .etc. To build a\nbaseline for action recognition in figure skating, we evaluate state-of-the-art\naction recognition methods on FSD-10. Motivated by the idea that domain\nknowledge is of great concern in sports field, we propose a keyframe based\ntemporal segment network (KTSN) for classification and achieve remarkable\nperformance. Experimental results demonstrate that FSD-10 is an ideal dataset\nfor benchmarking action recognition algorithms, as it requires to accurately\nextract action motions rather than action poses. We hope FSD-10, which is\ndesigned to have a large collection of finegrained actions, can serve as a new\nchallenge to develop more robust and advanced action recognition models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03985,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Unconstrained Periocular Recognition: Using Generative Deep Learning\n  Frameworks for Attribute Normalization\n\n  Ocular biometric systems working in unconstrained environments usually face\nthe problem of small within-class compactness caused by the multiple factors\nthat jointly degrade the quality of the obtained data. In this work, we propose\nan attribute normalization strategy based on deep learning generative\nframeworks, that reduces the variability of the samples used in pairwise\ncomparisons, without reducing their discriminability. The proposed method can\nbe seen as a preprocessing step that contributes for data regularization and\nimproves the recognition accuracy, being fully agnostic to the recognition\nstrategy used. As proof of concept, we consider the \"eyeglasses\" and \"gaze\"\nfactors, comparing the levels of performance of five different recognition\nmethods with\/without using the proposed normalization strategy. Also, we\nintroduce a new dataset for unconstrained periocular recognition, composed of\nimages acquired by mobile devices, particularly suited to perceive the impact\nof \"wearing eyeglasses\" in recognition effectiveness. Our experiments were\nperformed in two different datasets, and support the usefulness of our\nattribute normalization scheme to improve the recognition performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.00213,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000093381,
      "text":"Cross-Spectrum Dual-Subspace Pairing for RGB-infrared Cross-Modality\n  Person Re-Identification\n\n  Due to its potential wide applications in video surveillance and other\ncomputer vision tasks like tracking, person re-identification (ReID) has become\npopular and been widely investigated. However, conventional person\nre-identification can only handle RGB color images, which will fail at dark\nconditions. Thus RGB-infrared ReID (also known as Infrared-Visible ReID or\nVisible-Thermal ReID) is proposed. Apart from appearance discrepancy in\ntraditional ReID caused by illumination, pose variations and viewpoint changes,\nmodality discrepancy produced by cameras of the different spectrum also exists,\nwhich makes RGB-infrared ReID more difficult. To address this problem, we focus\non extracting the shared cross-spectrum features of different modalities. In\nthis paper, a novel multi-spectrum image generation method is proposed and the\ngenerated samples are utilized to help the network to find discriminative\ninformation for re-identifying the same person across modalities. Another\nchallenge of RGB-infrared ReID is that the intra-person (images from the same\nperson) discrepancy is often larger than the inter-person (images from\ndifferent persons) discrepancy, so a dual-subspace pairing strategy is proposed\nto alleviate this problem. Combining those two parts together, we also design a\none-stream neural network combining the aforementioned methods to extract\ncompact representations of person images, called Cross-spectrum Dual-subspace\nPairing (CDP) model. Furthermore, during the training process, we also propose\na Dynamic Hard Spectrum Mining method to automatically mine more hard samples\nfrom hard spectrum based on the current model state to further boost the\nperformance. Extensive experimental results on two public datasets, SYSU-MM01\nwith RGB + near-infrared images and RegDB with RGB + far-infrared images, have\ndemonstrated the efficiency and generality of our proposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09181,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Unsupervised Enhancement of Soft-biometric Privacy with Negative Face\n  Recognition\n\n  Current research on soft-biometrics showed that privacy-sensitive information\ncan be deduced from biometric templates of an individual. Since for many\napplications, these templates are expected to be used for recognition purposes\nonly, this raises major privacy issues. Previous works focused on supervised\nprivacy-enhancing solutions that require privacy-sensitive information about\nindividuals and limit their application to the suppression of single and\npre-defined attributes. Consequently, they do not take into account attributes\nthat are not considered in the training. In this work, we present Negative Face\nRecognition (NFR), a novel face recognition approach that enhances the\nsoft-biometric privacy on the template-level by representing face templates in\na complementary (negative) domain. While ordinary templates characterize facial\nproperties of an individual, negative templates describe facial properties that\ndoes not exist for this individual. This suppresses privacy-sensitive\ninformation from stored templates. Experiments are conducted on two publicly\navailable datasets captured under controlled and uncontrolled scenarios on\nthree privacy-sensitive attributes. The experiments demonstrate that our\nproposed approach reaches higher suppression rates than previous work, while\nmaintaining higher recognition performances as well. Unlike previous works, our\napproach does not require privacy-sensitive labels and offers a more\ncomprehensive privacy-protection not limited to pre-defined attributes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.08694,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000093049,
      "text":"Bi-directional Dermoscopic Feature Learning and Multi-scale Consistent\n  Decision Fusion for Skin Lesion Segmentation\n\n  Accurate segmentation of skin lesion from dermoscopic images is a crucial\npart of computer-aided diagnosis of melanoma. It is challenging due to the fact\nthat dermoscopic images from different patients have non-negligible lesion\nvariation, which causes difficulties in anatomical structure learning and\nconsistent skin lesion delineation. In this paper, we propose a novel\nbi-directional dermoscopic feature learning (biDFL) framework to model the\ncomplex correlation between skin lesions and their informative context. By\ncontrolling feature information passing through two complementary directions, a\nsubstantially rich and discriminative feature representation is achieved.\nSpecifically, we place biDFL module on the top of a CNN network to enhance\nhigh-level parsing performance. Furthermore, we propose a multi-scale\nconsistent decision fusion (mCDF) that is capable of selectively focusing on\nthe informative decisions generated from multiple classification layers. By\nanalysis of the consistency of the decision at each position, mCDF\nautomatically adjusts the reliability of decisions and thus allows a more\ninsightful skin lesion delineation. The comprehensive experimental results show\nthe effectiveness of the proposed method on skin lesion segmentation, achieving\nstate-of-the-art performance consistently on two publicly available dermoscopic\nimage databases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00575,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Unsupervised Domain Adaptive Object Detection using Forward-Backward\n  Cyclic Adaptation\n\n  We present a novel approach to perform the unsupervised domain adaptation for\nobject detection through forward-backward cyclic (FBC) training. Recent\nadversarial training based domain adaptation methods have shown their\neffectiveness on minimizing domain discrepancy via marginal feature\ndistributions alignment. However, aligning the marginal feature distributions\ndoes not guarantee the alignment of class conditional distributions. This\nlimitation is more evident when adapting object detectors as the domain\ndiscrepancy is larger compared to the image classification task, e.g. various\nnumber of objects exist in one image and the majority of content in an image is\nthe background. This motivates us to learn domain invariance for category level\nsemantics via gradient alignment. Intuitively, if the gradients of two domains\npoint in similar directions, then the learning of one domain can improve that\nof another domain. To achieve gradient alignment, we propose Forward-Backward\nCyclic Adaptation, which iteratively computes adaptation from source to target\nvia backward hopping and from target to source via forward passing. In\naddition, we align low-level features for adapting holistic color\/texture via\nadversarial training. However, the detector performs well on both domains is\nnot ideal for target domain. As such, in each cycle, domain diversity is\nenforced by maximum entropy regularization on the source domain to penalize\nconfident source-specific learning and minimum entropy regularization on target\ndomain to intrigue target-specific learning. Theoretical analysis of the\ntraining process is provided, and extensive experiments on challenging\ncross-domain object detection datasets have shown the superiority of our\napproach over the state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03264,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000120865,
      "text":"GradMix: Multi-source Transfer across Domains and Tasks\n\n  The computer vision community is witnessing an unprecedented rate of new\ntasks being proposed and addressed, thanks to the deep convolutional networks'\ncapability to find complex mappings from X to Y. The advent of each task often\naccompanies the release of a large-scale annotated dataset, for supervised\ntraining of deep network. However, it is expensive and time-consuming to\nmanually label sufficient amount of training data. Therefore, it is important\nto develop algorithms that can leverage off-the-shelf labeled dataset to learn\nuseful knowledge for the target task. While previous works mostly focus on\ntransfer learning from a single source, we study multi-source transfer across\ndomains and tasks (MS-DTT), in a semi-supervised setting. We propose GradMix, a\nmodel-agnostic method applicable to any model trained with gradient-based\nlearning rule, to transfer knowledge via gradient descent by weighting and\nmixing the gradients from all sources during training. GradMix follows a\nmeta-learning objective, which assigns layer-wise weights to the source\ngradients, such that the combined gradient follows the direction that minimize\nthe loss for a small set of samples from the target dataset. In addition, we\npropose to adaptively adjust the learning rate for each mini-batch based on its\nimportance to the target task, and a pseudo-labeling method to leverage the\nunlabeled samples in the target domain. We conduct MS-DTT experiments on two\ntasks: digit recognition and action recognition, and demonstrate the\nadvantageous performance of the proposed method against multiple baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.02362,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Lane Boundary Geometry Extraction from Satellite Imagery\n\n  Autonomous driving car is becoming more of a reality, as a key\ncomponent,high-definition(HD) maps shows its value in both market place and\nindustry. Even though HD maps generation from LiDAR or stereo\/perspective\nimagery has achieved impressive success, its inherent defects cannot be\nignored. In this paper, we proposal a novel method for Highway HD maps modeling\nusing pixel-wise segmentation on satellite imagery and formalized hypotheses\nlinking, which is cheaper and faster than current HD maps modeling approaches\nfrom LiDAR point cloud and perspective view imagery, and let it becomes an\nideal complementary of state of the art. We also manual code\/label an HD road\nmodel dataset as ground truth, aligned with Bing tile image server, to train,\ntest and evaluate our methodology. This dataset will be publish at same time to\ncontribute research in HD maps modeling from aerial imagery.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.03138,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Multi-Modality Cascaded Fusion Technology for Autonomous Driving\n\n  Multi-modality fusion is the guarantee of the stability of autonomous driving\nsystems. In this paper, we propose a general multi-modality cascaded fusion\nframework, exploiting the advantages of decision-level and feature-level\nfusion, utilizing target position, size, velocity, appearance and confidence to\nachieve accurate fusion results. In the fusion process, dynamic coordinate\nalignment(DCA) is conducted to reduce the error between sensors from different\nmodalities. In addition, the calculation of affinity matrix is the core module\nof sensor fusion, we propose an affinity loss that improves the performance of\ndeep affinity network(DAN). Last, the proposed step-by-step cascaded fusion\nframework is more interpretable and flexible compared to the end-toend fusion\nmethods. Extensive experiments on Nuscenes [2] dataset show that our approach\nachieves the state-of-theart performance.dataset show that our approach\nachieves the state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.10686,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Globally Optimal Contrast Maximisation for Event-based Motion Estimation\n\n  Contrast maximisation estimates the motion captured in an event stream by\nmaximising the sharpness of the motion compensated event image. To carry out\ncontrast maximisation, many previous works employ iterative optimisation\nalgorithms, such as conjugate gradient, which require good initialisation to\navoid converging to bad local minima. To alleviate this weakness, we propose a\nnew globally optimal event-based motion estimation algorithm. Based on\nbranch-and-bound (BnB), our method solves rotational (3DoF) motion estimation\non event streams, which supports practical applications such as video\nstabilisation and attitude estimation. Underpinning our method are novel\nbounding functions for contrast maximisation, whose theoretical validity is\nrigorously established. We show concrete examples from public datasets where\nglobally optimal solutions are vital to the success of contrast maximisation.\nDespite its exact nature, our algorithm is currently able to process a 50,000\nevent input in 300 seconds (a locally optimal solver takes 30 seconds on the\nsame input), and has the potential to be further speeded-up using GPUs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.05654,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"Summarizing the performances of a background subtraction algorithm\n  measured on several videos\n\n  There exist many background subtraction algorithms to detect motion in\nvideos. To help comparing them, datasets with ground-truth data such as CDNET\nor LASIESTA have been proposed. These datasets organize videos in categories\nthat represent typical challenges for background subtraction. The evaluation\nprocedure promoted by their authors consists in measuring performance\nindicators for each video separately and to average them hierarchically, within\na category first, then between categories, a procedure which we name\n\"summarization\". While the summarization by averaging performance indicators is\na valuable effort to standardize the evaluation procedure, it has no\ntheoretical justification and it breaks the intrinsic relationships between\nsummarized indicators. This leads to interpretation inconsistencies. In this\npaper, we present a theoretical approach to summarize the performances for\nmultiple videos that preserves the relationships between performance\nindicators. In addition, we give formulas and an algorithm to calculate\nsummarized performances. Finally, we showcase our observations on CDNET 2014.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.09479,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000031458,
      "text":"Kullback-Leibler Divergence-Based Fuzzy $C$-Means Clustering\n  Incorporating Morphological Reconstruction and Wavelet Frames for Image\n  Segmentation\n\n  Although spatial information of images usually enhance the robustness of the\nFuzzy C-Means (FCM) algorithm, it greatly increases the computational costs for\nimage segmentation. To achieve a sound trade-off between the segmentation\nperformance and the speed of clustering, we come up with a Kullback-Leibler\n(KL) divergence-based FCM algorithm by incorporating a tight wavelet frame\ntransform and a morphological reconstruction operation. To enhance FCM's\nrobustness, an observed image is first filtered by using the morphological\nreconstruction. A tight wavelet frame system is employed to decompose the\nobserved and filtered images so as to form their feature sets. Considering\nthese feature sets as data of clustering, an modified FCM algorithm is\nproposed, which introduces a KL divergence term in the partition matrix into\nits objective function. The KL divergence term aims to make membership degrees\nof each image pixel closer to those of its neighbors, which brings that the\nmembership partition becomes more suitable and the parameter setting of FCM\nbecomes simplified. On the basis of the obtained partition matrix and\nprototypes, the segmented feature set is reconstructed by minimizing the\ninverse process of the modified objective function. To modify abnormal features\nproduced in the reconstruction process, each reconstructed feature is\nreassigned to the closest prototype. As a result, the segmentation accuracy of\nKL divergence-based FCM is further improved. What's more, the segmented image\nis reconstructed by using a tight wavelet frame reconstruction operation.\nFinally, supporting experiments coping with synthetic, medical and color images\nare reported. Experimental results exhibit that the proposed algorithm works\nwell and comes with better segmentation performance than other comparative\nalgorithms. Moreover, the proposed algorithm requires less time than most of\nthe FCM-related algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07362,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"MILA: Multi-Task Learning from Videos via Efficient Inter-Frame\n  Attention\n\n  Prior work in multi-task learning has mainly focused on predictions on a\nsingle image. In this work, we present a new approach for multi-task learning\nfrom videos via efficient inter-frame local attention (MILA). Our approach\ncontains a novel inter-frame attention module which allows learning of\ntask-specific attention across frames. We embed the attention module in a\n``slow-fast'' architecture, where the slower network runs on sparsely sampled\nkeyframes and the light-weight shallow network runs on non-keyframes at a high\nframe rate. We also propose an effective adversarial learning strategy to\nencourage the slow and fast network to learn similar features. Our approach\nensures low-latency multi-task learning while maintaining high quality\npredictions. Experiments show competitive accuracy compared to state-of-the-art\non two multi-task learning benchmarks while reducing the number of floating\npoint operations (FLOPs) by up to 70\\%. In addition, our attention based\nfeature propagation method (ILA) outperforms prior work in terms of task\naccuracy while also reducing up to 90\\% of FLOPs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.00892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"Effect of top-down connections in Hierarchical Sparse Coding\n\n  Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent\nmulti-dimensional, structured data such as images. The simplest solution to\nsolve this computationally hard problem is to decompose it into independent\nlayer-wise subproblems. However, neuroscientific evidence would suggest\ninter-connecting these subproblems as in the Predictive Coding (PC) theory,\nwhich adds top-down connections between consecutive layers. In this study, a\nnew model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to\nassess the impact of this inter-layer feedback connection. In particular, the\n2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a\nsequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La\nnetworks are trained on 4 different databases and with different sparsity\nparameters on each layer. First, we show that the overall prediction error\ngenerated by 2L-SPC is lower thanks to the feedback mechanism as it transfers\nprediction error between layers. Second, we demonstrate that the inference\nstage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we\nshow that the 2L-SPC also accelerates the learning process. Finally, the\nqualitative analysis of both models dictionaries, supported by their activation\nprobability, show that the 2L-SPC features are more generic and informative.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.10381,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"Sketchformer: Transformer-based Representation for Sketched Structure\n\n  Sketchformer is a novel transformer-based representation for encoding\nfree-hand sketches input in a vector form, i.e. as a sequence of strokes.\nSketchformer effectively addresses multiple tasks: sketch classification,\nsketch based image retrieval (SBIR), and the reconstruction and interpolation\nof sketches. We report several variants exploring continuous and tokenized\ninput representations, and contrast their performance. Our learned embedding,\ndriven by a dictionary learning tokenization scheme, yields state of the art\nperformance in classification and image retrieval tasks, when compared against\nbaseline representations driven by LSTM sequence to sequence architectures:\nSketchRNN and derivatives. We show that sketch reconstruction and interpolation\nare improved significantly by the Sketchformer embedding for complex sketches\nwith longer stroke sequences.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07358,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Bottom-Up Temporal Action Localization with Mutual Regularization\n\n  Recently, temporal action localization (TAL), i.e., finding specific action\nsegments in untrimmed videos, has attracted increasing attentions of the\ncomputer vision community. State-of-the-art solutions for TAL involves\nevaluating the frame-level probabilities of three action-indicating phases,\ni.e. starting, continuing, and ending; and then post-processing these\npredictions for the final localization. This paper delves deep into this\nmechanism, and argues that existing methods, by modeling these phases as\nindividual classification tasks, ignored the potential temporal constraints\nbetween them. This can lead to incorrect and\/or inconsistent predictions when\nsome frames of the video input lack sufficient discriminative information. To\nalleviate this problem, we introduce two regularization terms to mutually\nregularize the learning procedure: the Intra-phase Consistency (IntraC)\nregularization is proposed to make the predictions verified inside each phase;\nand the Inter-phase Consistency (InterC) regularization is proposed to keep\nconsistency between these phases. Jointly optimizing these two terms, the\nentire framework is aware of these potential constraints during an end-to-end\noptimization process. Experiments are performed on two popular TAL datasets,\nTHUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both\nquantitatively and qualitatively. The proposed regularization also generalizes\nto other TAL methods (e.g., TSA-Net and PGCN). code:\nhttps:\/\/github.com\/PeisenZhao\/Bottom-Up-TAL-with-MR\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2002.07471,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Knowledge Integration Networks for Action Recognition\n\n  In this work, we propose Knowledge Integration Networks (referred as KINet)\nfor video action recognition. KINet is capable of aggregating meaningful\ncontext features which are of great importance to identifying an action, such\nas human information and scene context. We design a three-branch architecture\nconsisting of a main branch for action recognition, and two auxiliary branches\nfor human parsing and scene recognition which allow the model to encode the\nknowledge of human and scene for action recognition. We explore two pre-trained\nmodels as teacher networks to distill the knowledge of human and scene for\ntraining the auxiliary tasks of KINet. Furthermore, we propose a two-level\nknowledge encoding mechanism which contains a Cross Branch Integration (CBI)\nmodule for encoding the auxiliary knowledge into medium-level convolutional\nfeatures, and an Action Knowledge Graph (AKG) for effectively fusing high-level\ncontext information. This results in an end-to-end trainable framework where\nthe three tasks can be trained collaboratively, allowing the model to compute\nstrong context knowledge efficiently. The proposed KINet achieves the\nstate-of-the-art performance on a large-scale action recognition benchmark\nKinetics-400, with a top-1 accuracy of 77.8%. We further demonstrate that our\nKINet has strong capability by transferring the Kinetics-trained model to\nUCF-101, where it obtains 97.8% top-1 accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.00835,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Deep Variational Luenberger-type Observer for Stochastic Video\n  Prediction\n\n  Considering the inherent stochasticity and uncertainty, predicting future\nvideo frames is exceptionally challenging. In this work, we study the problem\nof video prediction by combining interpretability of stochastic state space\nmodels and representation learning of deep neural networks. Our model builds\nupon an variational encoder which transforms the input video into a latent\nfeature space and a Luenberger-type observer which captures the dynamic\nevolution of the latent features. This enables the decomposition of videos into\nstatic features and dynamics in an unsupervised manner. By deriving the\nstability theory of the nonlinear Luenberger-type observer, the hidden states\nin the feature space become insensitive with respect to the initial values,\nwhich improves the robustness of the overall model. Furthermore, the\nvariational lower bound on the data log-likelihood can be derived to obtain the\ntractable posterior prediction distribution based on the variational principle.\nFinally, the experiments such as the Bouncing Balls dataset and the Pendulum\ndataset are provided to demonstrate the proposed model outperforms concurrent\nworks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.06534,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Towards Causality-Aware Inferring: A Sequential Discriminative Approach\n  for Medical Diagnosis\n\n  Medical diagnosis assistant (MDA) aims to build an interactive diagnostic\nagent to sequentially inquire about symptoms for discriminating diseases.\nHowever, since the dialogue records used to build a patient simulator are\ncollected passively, the data might be deteriorated by some task-unrelated\nbiases, such as the preference of the collectors. These biases might hinder the\ndiagnostic agent to capture transportable knowledge from the simulator. This\nwork attempts to address these critical issues in MDA by taking advantage of\nthe causal diagram to identify and resolve two representative non-causal\nbiases, i.e., (i) default-answer bias and (ii) distributional inquiry bias.\nSpecifically, Bias (i) originates from the patient simulator which tries to\nanswer the unrecorded inquiries with some biased default answers. Consequently,\nthe diagnostic agents cannot fully demonstrate their advantages due to the\nbiased answers. To eliminate this bias and inspired by the propensity score\nmatching technique with causal diagram, we propose a propensity-based patient\nsimulator to effectively answer unrecorded inquiry by drawing knowledge from\nthe other records; Bias (ii) inherently comes along with the passively\ncollected data, and is one of the key obstacles for training the agent towards\n\"learning how\" rather than \"remembering what\". For example, within the\ndistribution of training data, if a symptom is highly coupled with a certain\ndisease, the agent might learn to only inquire about that symptom to\ndiscriminate that disease, thus might not generalize to the out-of-distribution\ncases. To this end, we propose a progressive assurance agent, which includes\nthe dual processes accounting for symptom inquiry and disease diagnosis\nrespectively. The inquiry process is driven by the diagnosis process in a\ntop-down manner to inquire about symptoms for enhancing diagnostic confidence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.06752,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Learning 2D-3D Correspondences To Solve The Blind Perspective-n-Point\n  Problem\n\n  Conventional absolute camera pose via a Perspective-n-Point (PnP) solver\noften assumes that the correspondences between 2D image pixels and 3D points\nare given. When the correspondences between 2D and 3D points are not known a\npriori, the task becomes the much more challenging blind PnP problem. This\npaper proposes a deep CNN model which simultaneously solves for both the 6-DoF\nabsolute camera pose and 2D--3D correspondences. Our model comprises three\nneural modules connected in sequence. First, a two-stream PointNet-inspired\nnetwork is applied directly to both the 2D image keypoints and the 3D scene\npoints in order to extract discriminative point-wise features harnessing both\nlocal and contextual information. Second, a global feature matching module is\nemployed to estimate a matchability matrix among all 2D--3D pairs. Third, the\nobtained matchability matrix is fed into a classification module to\ndisambiguate inlier matches. The entire network is trained end-to-end, followed\nby a robust model fitting (P3P-RANSAC) at test time only to recover the 6-DoF\ncamera pose. Extensive tests on both real and simulated data have shown that\nour method substantially outperforms existing approaches, and is capable of\nprocessing thousands of points a second with the state-of-the-art accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03836,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000092387,
      "text":"Fine-Grained Visual Classification via Progressive Multi-Granularity\n  Training of Jigsaw Patches\n\n  Fine-grained visual classification (FGVC) is much more challenging than\ntraditional classification tasks due to the inherently subtle intra-class\nobject variations. Recent works mainly tackle this problem by focusing on how\nto locate the most discriminative parts, more complementary parts, and parts of\nvarious granularities. However, less effort has been placed to which\ngranularities are the most discriminative and how to fuse information cross\nmulti-granularity. In this work, we propose a novel framework for fine-grained\nvisual classification to tackle these problems. In particular, we propose: (i)\na progressive training strategy that effectively fuses features from different\ngranularities, and (ii) a random jigsaw patch generator that encourages the\nnetwork to learn features at specific granularities. We obtain state-of-the-art\nperformances on several standard FGVC benchmark datasets, where the proposed\nmethod consistently outperforms existing methods or delivers competitive\nresults. The code will be available at\nhttps:\/\/github.com\/PRIS-CV\/PMG-Progressive-Multi-Granularity-Training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.06498,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Explainable Deep Classification Models for Domain Generalization\n\n  Conventionally, AI models are thought to trade off explainability for lower\naccuracy. We develop a training strategy that not only leads to a more\nexplainable AI system for object classification, but as a consequence, suffers\nno perceptible accuracy degradation. Explanations are defined as regions of\nvisual evidence upon which a deep classification network makes a decision. This\nis represented in the form of a saliency map conveying how much each pixel\ncontributed to the network's decision. Our training strategy enforces a\nperiodic saliency-based feedback to encourage the model to focus on the image\nregions that directly correspond to the ground-truth object. We quantify\nexplainability using an automated metric, and using human judgement. We propose\nexplainability as a means for bridging the visual-semantic gap between\ndifferent domains where model explanations are used as a means of disentagling\ndomain specific information from otherwise relevant features. We demonstrate\nthat this leads to improved generalization to new domains without hindering\nperformance on the original domain.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09093,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000057949,
      "text":"Masked Face Recognition Dataset and Application\n\n  In order to effectively prevent the spread of COVID-19 virus, almost everyone\nwears a mask during coronavirus epidemic. This almost makes conventional facial\nrecognition technology ineffective in many cases, such as community access\ncontrol, face access control, facial attendance, facial security checks at\ntrain stations, etc. Therefore, it is very urgent to improve the recognition\nperformance of the existing face recognition technology on the masked faces.\nMost current advanced face recognition approaches are designed based on deep\nlearning, which depend on a large number of face samples. However, at present,\nthere are no publicly available masked face recognition datasets. To this end,\nthis work proposes three types of masked face datasets, including Masked Face\nDetection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD)\nand Simulated Masked Face Recognition Dataset (SMFRD). Among them, to the best\nof our knowledge, RMFRD is currently theworld's largest real-world masked face\ndataset. These datasets are freely available to industry and academia, based on\nwhich various applications on masked faces can be developed. The\nmulti-granularity masked face recognition model we developed achieves 95%\naccuracy, exceeding the results reported by the industry. Our datasets are\navailable at: https:\/\/github.com\/X-zhangyang\/Real-World-Masked-Face-Dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.10987,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Generalizing Spatial Transformers to Projective Geometry with\n  Applications to 2D\/3D Registration\n\n  Differentiable rendering is a technique to connect 3D scenes with\ncorresponding 2D images. Since it is differentiable, processes during image\nformation can be learned. Previous approaches to differentiable rendering focus\non mesh-based representations of 3D scenes, which is inappropriate for medical\napplications where volumetric, voxelized models are used to represent anatomy.\nWe propose a novel Projective Spatial Transformer module that generalizes\nspatial transformers to projective geometry, thus enabling differentiable\nvolume rendering. We demonstrate the usefulness of this architecture on the\nexample of 2D\/3D registration between radiographs and CT scans. Specifically,\nwe show that our transformer enables end-to-end learning of an image processing\nand projection model that approximates an image similarity function that is\nconvex with respect to the pose parameters, and can thus be optimized\neffectively using conventional gradient descent. To the best of our knowledge,\nthis is the first time that spatial transformers have been described for\nprojective geometry. The source code will be made public upon publication of\nthis manuscript and we hope that our developments will benefit related 3D\nresearch applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.00137,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Revisiting Few-shot Activity Detection with Class Similarity Control\n\n  Many interesting events in the real world are rare making preannotated\nmachine learning ready videos a rarity in consequence. Thus, temporal activity\ndetection models that are able to learn from a few examples are desirable. In\nthis paper, we present a conceptually simple and general yet novel framework\nfor few-shot temporal activity detection based on proposal regression which\ndetects the start and end time of the activities in untrimmed videos. Our model\nis end-to-end trainable, takes into account the frame rate differences between\nfew-shot activities and untrimmed test videos, and can benefit from additional\nfew-shot examples. We experiment on three large scale benchmarks for temporal\nactivity detection (ActivityNet1.2, ActivityNet1.3 and THUMOS14 datasets) in a\nfew-shot setting. We also study the effect on performance of different amount\nof overlap with activities used to pretrain the video classification backbone\nand propose corrective measures for future works in this domain. Our code will\nbe made available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.13048,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Attentive CutMix: An Enhanced Data Augmentation Approach for Deep\n  Learning Based Image Classification\n\n  Convolutional neural networks (CNN) are capable of learning robust\nrepresentation with different regularization methods and activations as\nconvolutional layers are spatially correlated. Based on this property, a large\nvariety of regional dropout strategies have been proposed, such as Cutout,\nDropBlock, CutMix, etc. These methods aim to promote the network to generalize\nbetter by partially occluding the discriminative parts of objects. However, all\nof them perform this operation randomly, without capturing the most important\nregion(s) within an object. In this paper, we propose Attentive CutMix, a\nnaturally enhanced augmentation strategy based on CutMix. In each training\niteration, we choose the most descriptive regions based on the intermediate\nattention maps from a feature extractor, which enables searching for the most\ndiscriminative parts in an image. Our proposed method is simple yet effective,\neasy to implement and can boost the baseline significantly. Extensive\nexperiments on CIFAR-10\/100, ImageNet datasets with various CNN architectures\n(in a unified setting) demonstrate the effectiveness of our proposed method,\nwhich consistently outperforms the baseline CutMix and other methods by a\nsignificant margin.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0508,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"SOS: Selective Objective Switch for Rapid Immunofluorescence Whole Slide\n  Image Classification\n\n  The difficulty of processing gigapixel whole slide images (WSIs) in clinical\nmicroscopy has been a long-standing barrier to implementing computer aided\ndiagnostic systems. Since modern computing resources are unable to perform\ncomputations at this extremely large scale, current state of the art methods\nutilize patch-based processing to preserve the resolution of WSIs. However,\nthese methods are often resource intensive and make significant compromises on\nprocessing time. In this paper, we demonstrate that conventional patch-based\nprocessing is redundant for certain WSI classification tasks where high\nresolution is only required in a minority of cases. This reflects what is\nobserved in clinical practice; where a pathologist may screen slides using a\nlow power objective and only switch to a high power in cases where they are\nuncertain about their findings. To eliminate these redundancies, we propose a\nmethod for the selective use of high resolution processing based on the\nconfidence of predictions on downscaled WSIs --- we call this the Selective\nObjective Switch (SOS). Our method is validated on a novel dataset of 684\nLiver-Kidney-Stomach immunofluorescence WSIs routinely used in the\ninvestigation of autoimmune liver disease. By limiting high resolution\nprocessing to cases which cannot be classified confidently at low resolution,\nwe maintain the accuracy of patch-level analysis whilst reducing the inference\ntime by a factor of 7.74.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.11228,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"ASFD: Automatic and Scalable Face Detector\n\n  In this paper, we propose a novel Automatic and Scalable Face Detector\n(ASFD), which is based on a combination of neural architecture search\ntechniques as well as a new loss design. First, we propose an automatic feature\nenhance module named Auto-FEM by improved differential architecture search,\nwhich allows efficient multi-scale feature fusion and context enhancement.\nSecond, we use Distance-based Regression and Margin-based Classification (DRMC)\nmulti-task loss to predict accurate bounding boxes and learn highly\ndiscriminative deep features. Third, we adopt compound scaling methods and\nuniformly scale the backbone, feature modules, and head networks to develop a\nfamily of ASFD, which are consistently more efficient than the state-of-the-art\nface detectors. Extensive experiments conducted on popular benchmarks, e.g.\nWIDER FACE and FDDB, demonstrate that our ASFD-D6 outperforms the prior strong\ncompetitors, and our lightweight ASFD-D0 runs at more than 120 FPS with\nMobilenet for VGA-resolution images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Identity Recognition in Intelligent Cars with Behavioral Data and\n  LSTM-ResNet Classifier\n\n  Identity recognition in a car cabin is a critical task nowadays and offers a\ngreat field of applications ranging from personalizing intelligent cars to suit\ndrivers physical and behavioral needs to increasing safety and security.\nHowever, the performance and applicability of published approaches are still\nnot suitable for use in series cars and need to be improved. In this paper, we\ninvestigate Human Identity Recognition in a car cabin with Time Series\nClassification (TSC) and deep neural networks. We use gas and brake pedal\npressure as input to our models. This data is easily collectable during driving\nin everyday situations. Since our classifiers have very little memory\nrequirements and do not require any input data preproccesing, we were able to\ntrain on one Intel i5-3210M processor only. Our classification approach is\nbased on a combination of LSTM and ResNet. The network trained on a subset of\nNUDrive outperforms the ResNet and LSTM models trained solely by 35.9 % and\n53.85 % accuracy respectively. We reach a final accuracy of 79.49 % on a\n10-drivers subset of NUDrive and 96.90 % on a 5-drivers subset of UTDrive.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09088,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN\n\n  Recent advances in deep learning have provided procedures for learning one\nnetwork to amalgamate multiple streams of knowledge from the pre-trained\nConvolutional Neural Network (CNN) models, thus reduce the annotation cost.\nHowever, almost all existing methods demand massive training data, which may be\nunavailable due to privacy or transmission issues. In this paper, we propose a\ndata-free knowledge amalgamate strategy to craft a well-behaved multi-task\nstudent network from multiple single\/multi-task teachers. The main idea is to\nconstruct the group-stack generative adversarial networks (GANs) which have two\ndual generators. First one generator is trained to collect the knowledge by\nreconstructing the images approximating the original dataset utilized for\npre-training the teachers. Then a dual generator is trained by taking the\noutput from the former generator as input. Finally we treat the dual part\ngenerator as the target network and regroup it. As demonstrated on several\nbenchmarks of multi-label classification, the proposed method without any\ntraining data achieves the surprisingly competitive results, even compared with\nsome full-supervised methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08021,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Applying r-spatiogram in object tracking for occlusion handling\n\n  Object tracking is one of the most important problems in computer vision. The\naim of video tracking is to extract the trajectories of a target or object of\ninterest, i.e. accurately locate a moving target in a video sequence and\ndiscriminate target from non-targets in the feature space of the sequence. So,\nfeature descriptors can have significant effects on such discrimination. In\nthis paper, we use the basic idea of many trackers which consists of three main\ncomponents of the reference model, i.e., object modeling, object detection and\nlocalization, and model updating. However, there are major improvements in our\nsystem. Our forth component, occlusion handling, utilizes the r-spatiogram to\ndetect the best target candidate. While spatiogram contains some moments upon\nthe coordinates of the pixels, r-spatiogram computes region-based compactness\non the distribution of the given feature in the image that captures richer\nfeatures to represent the objects. The proposed research develops an efficient\nand robust way to keep tracking the object throughout video sequences in the\npresence of significant appearance variations and severe occlusions. The\nproposed method is evaluated on the Princeton RGBD tracking dataset considering\nsequences with different challenges and the obtained results demonstrate the\neffectiveness of the proposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.09392,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Comprehensive Instructional Video Analysis: The COIN Dataset and\n  Performance Evaluation\n\n  Thanks to the substantial and explosively inscreased instructional videos on\nthe Internet, novices are able to acquire knowledge for completing various\ntasks. Over the past decade, growing efforts have been devoted to investigating\nthe problem on instructional video analysis. However, the most existing\ndatasets in this area have limitations in diversity and scale, which makes them\nfar from many real-world applications where more diverse activities occur. To\naddress this, we present a large-scale dataset named as \"COIN\" for\nCOmprehensive INstructional video analysis. Organized with a hierarchical\nstructure, the COIN dataset contains 11,827 videos of 180 tasks in 12 domains\n(e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed\ntoolbox, all the videos are annotated efficiently with a series of step labels\nand the corresponding temporal boundaries. In order to provide a benchmark for\ninstructional video analysis, we evaluate plenty of approaches on the COIN\ndataset under five different settings. Furthermore, we exploit two important\ncharacteristics (i.e., task-consistency and ordering-dependency) for localizing\nimportant steps in instructional videos. Accordingly, we propose two simple yet\neffective methods, which can be easily plugged into conventional proposal-based\naction detection models. We believe the introduction of the COIN dataset will\npromote the future in-depth research on instructional video analysis for the\ncommunity. Our dataset, annotation toolbox and source code are available at\nhttp:\/\/coin-dataset.github.io.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.14407,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Probabilistic Pixel-Adaptive Refinement Networks\n\n  Encoder-decoder networks have found widespread use in various dense\nprediction tasks. However, the strong reduction of spatial resolution in the\nencoder leads to a loss of location information as well as boundary artifacts.\nTo address this, image-adaptive post-processing methods have shown beneficial\nby leveraging the high-resolution input image(s) as guidance data. We extend\nsuch approaches by considering an important orthogonal source of information:\nthe network's confidence in its own predictions. We introduce probabilistic\npixel-adaptive convolutions (PPACs), which not only depend on image guidance\ndata for filtering, but also respect the reliability of per-pixel predictions.\nAs such, PPACs allow for image-adaptive smoothing and simultaneously\npropagating pixels of high confidence into less reliable regions, while\nrespecting object boundaries. We demonstrate their utility in refinement\nnetworks for optical flow and semantic segmentation, where PPACs lead to a\nclear reduction in boundary artifacts. Moreover, our proposed refinement step\nis able to substantially improve the accuracy on various widely used\nbenchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.05128,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Cars Can't Fly up in the Sky: Improving Urban-Scene Segmentation via\n  Height-driven Attention Networks\n\n  This paper exploits the intrinsic features of urban-scene images and proposes\na general add-on module, called height-driven attention networks (HANet), for\nimproving semantic segmentation for urban-scene images. It emphasizes\ninformative features or classes selectively according to the vertical position\nof a pixel. The pixel-wise class distributions are significantly different from\neach other among horizontally segmented sections in the urban-scene images.\nLikewise, urban-scene images have their own distinct characteristics, but most\nsemantic segmentation networks do not reflect such unique attributes in the\narchitecture. The proposed network architecture incorporates the capability\nexploiting the attributes to handle the urban scene dataset effectively. We\nvalidate the consistent performance (mIoU) increase of various semantic\nsegmentation models on two datasets when HANet is adopted. This extensive\nquantitative analysis demonstrates that adding our module to existing models is\neasy and cost-effective. Our method achieves a new state-of-the-art performance\non the Cityscapes benchmark with a large margin among ResNet-101 based\nsegmentation models. Also, we show that the proposed model is coherent with the\nfacts observed in the urban scene by visualizing and interpreting the attention\nmap. Our code and trained models are publicly available at\nhttps:\/\/github.com\/shachoi\/HANet\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.08282,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Event Probability Mask (EPM) and Event Denoising Convolutional Neural\n  Network (EDnCNN) for Neuromorphic Cameras\n\n  This paper presents a novel method for labeling real-world neuromorphic\ncamera sensor data by calculating the likelihood of generating an event at each\npixel within a short time window, which we refer to as \"event probability mask\"\nor EPM. Its applications include (i) objective benchmarking of event denoising\nperformance, (ii) training convolutional neural networks for noise removal\ncalled \"event denoising convolutional neural network\" (EDnCNN), and (iii)\nestimating internal neuromorphic camera parameters. We provide the first\ndataset (DVSNOISE20) of real-world labeled neuromorphic camera events for noise\nremoval.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.0542,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Bi-Directional Attention for Joint Instance and Semantic Segmentation in\n  Point Clouds\n\n  Instance segmentation in point clouds is one of the most fine-grained ways to\nunderstand the 3D scene. Due to its close relationship to semantic\nsegmentation, many works approach these two tasks simultaneously and leverage\nthe benefits of multi-task learning. However, most of them only considered\nsimple strategies such as element-wise feature fusion, which may not lead to\nmutual promotion. In this work, we build a Bi-Directional Attention module on\nbackbone neural networks for 3D point cloud perception, which uses similarity\nmatrix measured from features for one task to help aggregate non-local\ninformation for the other task, avoiding the potential feature exclusion and\ntask conflict. From comprehensive experiments and ablation studies on the S3DIS\ndataset and the PartNet dataset, the superiority of our method is verified.\nMoreover, the mechanism of how bi-directional attention module helps joint\ninstance and semantic segmentation is also analyzed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.03164,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"D3Feat: Joint Learning of Dense Detection and Description of 3D Local\n  Features\n\n  A successful point cloud registration often lies on robust establishment of\nsparse matches through discriminative 3D local features. Despite the fast\nevolution of learning-based 3D feature descriptors, little attention has been\ndrawn to the learning of 3D feature detectors, even less for a joint learning\nof the two tasks. In this paper, we leverage a 3D fully convolutional network\nfor 3D point clouds, and propose a novel and practical learning mechanism that\ndensely predicts both a detection score and a description feature for each 3D\npoint. In particular, we propose a keypoint selection strategy that overcomes\nthe inherent density variations of 3D point clouds, and further propose a\nself-supervised detector loss guided by the on-the-fly feature matching results\nduring training. Finally, our method achieves state-of-the-art results in both\nindoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and\nshows its strong generalization ability on the ETH dataset. Towards practical\nuse, we show that by adopting a reliable feature detector, sampling a smaller\nnumber of features is sufficient to achieve accurate and fast point cloud\nalignment.[code release](https:\/\/github.com\/XuyangBai\/D3Feat)\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2003.10566,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Broad Area Search and Detection of Surface-to-Air Missile Sites Using\n  Spatial Fusion of Component Object Detections from Deep Neural Networks\n\n  Here we demonstrate how Deep Neural Network (DNN) detections of multiple\nconstitutive or component objects that are part of a larger, more complex, and\nencompassing feature can be spatially fused to improve the search, detection,\nand retrieval (ranking) of the larger complex feature. First, scores computed\nfrom a spatial clustering algorithm are normalized to a reference space so that\nthey are independent of image resolution and DNN input chip size. Then,\nmulti-scale DNN detections from various component objects are fused to improve\nthe detection and retrieval of DNN detections of a larger complex feature. We\ndemonstrate the utility of this approach for broad area search and detection of\nSurface-to-Air Missile (SAM) sites that have a very low occurrence rate (only\n16 sites) over a ~90,000 km^2 study area in SE China. The results demonstrate\nthat spatial fusion of multi-scale component-object DNN detections can reduce\nthe detection error rate of SAM Sites by $>$85% while still maintaining a 100%\nrecall. The novel spatial fusion approach demonstrated here can be easily\nextended to a wide variety of other challenging object search and detection\nproblems in large-scale remote sensing image datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.00797,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"SSHFD: Single Shot Human Fall Detection with Occluded Joints Resilience\n\n  Falling can have fatal consequences for elderly people especially if the\nfallen person is unable to call for help due to loss of consciousness or any\ninjury. Automatic fall detection systems can assist through prompt fall alarms\nand by minimizing the fear of falling when living independently at home.\nExisting vision-based fall detection systems lack generalization to unseen\nenvironments due to challenges such as variations in physical appearances,\ndifferent camera viewpoints, occlusions, and background clutter. In this paper,\nwe explore ways to overcome the above challenges and present Single Shot Human\nFall Detector (SSHFD), a deep learning based framework for automatic fall\ndetection from a single image. This is achieved through two key innovations.\nFirst, we present a human pose based fall representation which is invariant to\nappearance characteristics. Second, we present neural network models for 3d\npose estimation and fall recognition which are resilient to missing joints due\nto occluded body parts. Experiments on public fall datasets show that our\nframework successfully transfers knowledge of 3d pose estimation and fall\nrecognition learnt purely from synthetic data to unseen real-world data,\nshowcasing its generalization capability for accurate fall detection in\nreal-world scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.04962,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000159608,
      "text":"3D IoU-Net: IoU Guided 3D Object Detector for Point Clouds\n\n  Most existing point cloud based 3D object detectors focus on the tasks of\nclassification and box regression. However, another bottleneck in this area is\nachieving an accurate detection confidence for the Non-Maximum Suppression\n(NMS) post-processing. In this paper, we add a 3D IoU prediction branch to the\nregular classification and regression branches. The predicted IoU is used as\nthe detection confidence for NMS. In order to obtain a more accurate IoU\nprediction, we propose a 3D IoU-Net with IoU sensitive feature learning and an\nIoU alignment operation. To obtain a perspective-invariant prediction head, we\npropose an Attentive Corner Aggregation (ACA) module by aggregating a local\npoint cloud feature from each perspective of eight corners and adaptively\nweighting the contribution of each perspective with different attentions. We\npropose a Corner Geometry Encoding (CGE) module for geometry information\nembedding. To the best of our knowledge, this is the first time geometric\nembedding information has been introduced in proposal feature learning. These\ntwo feature parts are then adaptively fused by a multi-layer perceptron (MLP)\nnetwork as our IoU sensitive feature. The IoU alignment operation is introduced\nto resolve the mismatching between the bounding box regression head and IoU\nprediction, thereby further enhancing the accuracy of IoU prediction. The\nexperimental results on the KITTI car detection benchmark show that 3D IoU-Net\nwith IoU perception achieves state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.13629,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Colon Shape Estimation Method for Colonoscope Tracking using Recurrent\n  Neural Networks\n\n  We propose an estimation method using a recurrent neural network (RNN) of the\ncolon's shape where deformation was occurred by a colonoscope insertion.\nColonoscope tracking or a navigation system that navigates physician to polyp\npositions is needed to reduce such complications as colon perforation. Previous\ntracking methods caused large tracking errors at the transverse and sigmoid\ncolons because these areas largely deform during colonoscope insertion. Colon\ndeformation should be taken into account in tracking processes. We propose a\ncolon deformation estimation method using RNN and obtain the colonoscope shape\nfrom electromagnetic sensors during its insertion into the colon. This method\nobtains positional, directional, and an insertion length from the colonoscope\nshape. From its shape, we also calculate the relative features that represent\nthe positional and directional relationships between two points on a\ncolonoscope. Long short-term memory is used to estimate the current colon shape\nfrom the past transition of the features of the colonoscope shape. We performed\ncolon shape estimation in a phantom study and correctly estimated the colon\nshapes during colonoscope insertion with 12.39 (mm) estimation error.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.02707,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Sub-Instruction Aware Vision-and-Language Navigation\n\n  Vision-and-language navigation requires an agent to navigate through a real\n3D environment following natural language instructions. Despite significant\nadvances, few previous works are able to fully utilize the strong\ncorrespondence between the visual and textual sequences. Meanwhile, due to the\nlack of intermediate supervision, the agent's performance at following each\npart of the instruction cannot be assessed during navigation. In this work, we\nfocus on the granularity of the visual and language sequences as well as the\ntraceability of agents through the completion of an instruction. We provide\nagents with fine-grained annotations during training and find that they are\nable to follow the instruction better and have a higher chance of reaching the\ntarget at test time. We enrich the benchmark dataset Room-to-Room (R2R) with\nsub-instructions and their corresponding paths. To make use of this data, we\npropose effective sub-instruction attention and shifting modules that select\nand attend to a single sub-instruction at each time-step. We implement our\nsub-instruction modules in four state-of-the-art agents, compare with their\nbaseline models, and show that our proposed method improves the performance of\nall four agents.\n  We release the Fine-Grained R2R dataset (FGR2R) and the code at\nhttps:\/\/github.com\/YicongHong\/Fine-Grained-R2R.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.11823,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Facial Expression Recognition with Deep Learning\n\n  One of the most universal ways that people communicate is through facial\nexpressions. In this paper, we take a deep dive, implementing multiple deep\nlearning models for facial expression recognition (FER). Our goals are twofold:\nwe aim not only to maximize accuracy, but also to apply our results to the\nreal-world. By leveraging numerous techniques from recent research, we\ndemonstrate a state-of-the-art 75.8% accuracy on the FER2013 test set,\noutperforming all existing publications. Additionally, we showcase a mobile web\napp which runs our FER models on-device in real time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.09802,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000131461,
      "text":"Spatio-Temporal Dual Affine Differential Invariant for Skeleton-based\n  Action Recognition\n\n  The dynamics of human skeletons have significant information for the task of\naction recognition. The similarity between trajectories of corresponding joints\nis an indicating feature of the same action, while this similarity may subject\nto some distortions that can be modeled as the combination of spatial and\ntemporal affine transformations. In this work, we propose a novel feature\ncalled spatio-temporal dual affine differential invariant (STDADI).\nFurthermore, in order to improve the generalization ability of neural networks,\na channel augmentation method is proposed. On the large scale action\nrecognition dataset NTU-RGB+D, and its extended version NTU-RGB+D 120, it\nachieves remarkable improvements over previous state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.0678,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from X-ray Scans\n\n  In the last two decades, baggage scanning has globally become one of the\nprime aviation security concerns. Manual screening of the baggage items is\ntedious, error-prone, and compromise privacy. Hence, many researchers have\ndeveloped X-ray imagery-based autonomous systems to address these shortcomings.\nThis paper presents a cascaded structure tensor framework that can\nautomatically extract and recognize suspicious items in heavily occluded and\ncluttered baggage. The proposed framework is unique, as it intelligently\nextracts each object by iteratively picking contour-based transitional\ninformation from different orientations and uses only a single feed-forward\nconvolutional neural network for the recognition. The proposed framework has\nbeen rigorously evaluated using a total of 1,067,381 X-ray scans from publicly\navailable GDXray and SIXray datasets where it outperformed the state-of-the-art\nsolutions by achieving the mean average precision score of 0.9343 on GDXray and\n0.9595 on SIXray for recognizing the highly cluttered and overlapping\nsuspicious items. Furthermore, the proposed framework computationally achieves\n4.76\\% superior run-time performance as compared to the existing solutions\nbased on publicly available object detectors\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03708,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Context-Aware Group Captioning via Self-Attention and Contrastive\n  Features\n\n  While image captioning has progressed rapidly, existing works focus mainly on\ndescribing single images. In this paper, we introduce a new task, context-aware\ngroup captioning, which aims to describe a group of target images in the\ncontext of another group of related reference images. Context-aware group\ncaptioning requires not only summarizing information from both the target and\nreference image group but also contrasting between them. To solve this problem,\nwe propose a framework combining self-attention mechanism with contrastive\nfeature construction to effectively summarize common information from each\nimage group while capturing discriminative information between them. To build\nthe dataset for this task, we propose to group the images and generate the\ngroup captions based on single image captions using scene graphs matching. Our\ndatasets are constructed on top of the public Conceptual Captions dataset and\nour new Stock Captions dataset. Experiments on the two datasets show the\neffectiveness of our method on this new task. Related Datasets and code are\nreleased at https:\/\/lizw14.github.io\/project\/groupcap .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03023,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000002318,
      "text":"Field-Level Crop Type Classification with k Nearest Neighbors: A\n  Baseline for a New Kenya Smallholder Dataset\n\n  Accurate crop type maps provide critical information for ensuring food\nsecurity, yet there has been limited research on crop type classification for\nsmallholder agriculture, particularly in sub-Saharan Africa where risk of food\ninsecurity is highest. Publicly-available ground-truth data such as the\nnewly-released training dataset of crop types in Kenya (Radiant MLHub) are\ncatalyzing this research, but it is important to understand the context of\nwhen, where, and how these datasets were obtained when evaluating\nclassification performance and using them as a benchmark across methods. In\nthis paper, we provide context for the new western Kenya dataset which was\ncollected during an atypical 2019 main growing season and demonstrate\nclassification accuracy up to 64% for maize and 70% for cassava using k Nearest\nNeighbors--a fast, interpretable, and scalable method that can serve as a\nbaseline for future work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000048346,
      "text":"Multi-View Matching (MVM): Facilitating Multi-Person 3D Pose Estimation\n  Learning with Action-Frozen People Video\n\n  To tackle the challeging problem of multi-person 3D pose estimation from a\nsingle image, we propose a multi-view matching (MVM) method in this work. The\nMVM method generates reliable 3D human poses from a large-scale video dataset,\ncalled the Mannequin dataset, that contains action-frozen people immitating\nmannequins. With a large amount of in-the-wild video data labeled by 3D\nsupervisions automatically generated by MVM, we are able to train a neural\nnetwork that takes a single image as the input for multi-person 3D pose\nestimation. The core technology of MVM lies in effective alignment of 2D poses\nobtained from multiple views of a static scene that has a strong geometric\nconstraint. Our objective is to maximize mutual consistency of 2D poses\nestimated in multiple frames, where geometric constraints as well as appearance\nsimilarities are taken into account simultaneously. To demonstrate the\neffectiveness of 3D supervisions provided by the MVM method, we conduct\nexperiments on the 3DPW and the MSCOCO datasets and show that our proposed\nsolution offers the state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.12104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Offline Signature Verification on Real-World Documents\n\n  Research on offline signature verification has explored a large variety of\nmethods on multiple signature datasets, which are collected under controlled\nconditions. However, these datasets may not fully reflect the characteristics\nof the signatures in some practical use cases. Real-world signatures extracted\nfrom the formal documents may contain different types of occlusions, for\nexample, stamps, company seals, ruling lines, and signature boxes. Moreover,\nthey may have very high intra-class variations, where even genuine signatures\nresemble forgeries. In this paper, we address a real-world writer independent\noffline signature verification problem, in which, a bank's customers'\ntransaction request documents that contain their occluded signatures are\ncompared with their clean reference signatures. Our proposed method consists of\ntwo main components, a stamp cleaning method based on CycleGAN and signature\nrepresentation based on CNNs. We extensively evaluate different verification\nsetups, fine-tuning strategies, and signature representation approaches to have\na thorough analysis of the problem. Moreover, we conduct a human evaluation to\nshow the challenging nature of the problem. We run experiments both on our\ncustom dataset, as well as on the publicly available Tobacco-800 dataset. The\nexperimental results validate the difficulty of offline signature verification\non real-world documents. However, by employing the stamp cleaning process, we\nimprove the signature verification performance significantly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.01398,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"TEA: Temporal Excitation and Aggregation for Action Recognition\n\n  Temporal modeling is key for action recognition in videos. It normally\nconsiders both short-range motions and long-range aggregations. In this paper,\nwe propose a Temporal Excitation and Aggregation (TEA) block, including a\nmotion excitation (ME) module and a multiple temporal aggregation (MTA) module,\nspecifically designed to capture both short- and long-range temporal evolution.\nIn particular, for short-range motion modeling, the ME module calculates the\nfeature-level temporal differences from spatiotemporal features. It then\nutilizes the differences to excite the motion-sensitive channels of the\nfeatures. The long-range temporal aggregations in previous works are typically\nachieved by stacking a large number of local temporal convolutions. Each\nconvolution processes a local temporal window at a time. In contrast, the MTA\nmodule proposes to deform the local convolution to a group of sub-convolutions,\nforming a hierarchical residual architecture. Without introducing additional\nparameters, the features will be processed with a series of sub-convolutions,\nand each frame could complete multiple temporal aggregations with\nneighborhoods. The final equivalent receptive field of temporal dimension is\naccordingly enlarged, which is capable of modeling the long-range temporal\nrelationship over distant frames. The two components of the TEA block are\ncomplementary in temporal modeling. Finally, our approach achieves impressive\nresults at low FLOPs on several action recognition benchmarks, such as\nKinetics, Something-Something, HMDB51, and UCF101, which confirms its\neffectiveness and efficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03327,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000103977,
      "text":"Cascaded Refinement Network for Point Cloud Completion\n\n  Point clouds are often sparse and incomplete. Existing shape completion\nmethods are incapable of generating details of objects or learning the complex\npoint distributions. To this end, we propose a cascaded refinement network\ntogether with a coarse-to-fine strategy to synthesize the detailed object\nshapes. Considering the local details of partial input with the global shape\ninformation together, we can preserve the existing details in the incomplete\npoint set and generate the missing parts with high fidelity. We also design a\npatch discriminator that guarantees every local area has the same pattern with\nthe ground truth to learn the complicated point distribution. Quantitative and\nqualitative experiments on different datasets show that our method achieves\nsuperior results compared to existing state-of-the-art approaches on the 3D\npoint cloud completion task. Our source code is available at\nhttps:\/\/github.com\/xiaogangw\/cascaded-point-completion.git.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.12255,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000006126,
      "text":"TPNet: Trajectory Proposal Network for Motion Prediction\n\n  Making accurate motion prediction of the surrounding traffic agents such as\npedestrians, vehicles, and cyclists is crucial for autonomous driving. Recent\ndata-driven motion prediction methods have attempted to learn to directly\nregress the exact future position or its distribution from massive amount of\ntrajectory data. However, it remains difficult for these methods to provide\nmultimodal predictions as well as integrate physical constraints such as\ntraffic rules and movable areas. In this work we propose a novel two-stage\nmotion prediction framework, Trajectory Proposal Network (TPNet). TPNet first\ngenerates a candidate set of future trajectories as hypothesis proposals, then\nmakes the final predictions by classifying and refining the proposals which\nmeets the physical constraints. By steering the proposal generation process,\nsafe and multimodal predictions are realized. Thus this framework effectively\nmitigates the complexity of motion prediction problem while ensuring the\nmultimodal output. Experiments on four large-scale trajectory prediction\ndatasets, i.e. the ETH, UCY, Apollo and Argoverse datasets, show that TPNet\nachieves the state-of-the-art results both quantitatively and qualitatively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.07684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Joint Semantic Segmentation and Boundary Detection using Iterative\n  Pyramid Contexts\n\n  In this paper, we present a joint multi-task learning framework for semantic\nsegmentation and boundary detection. The critical component in the framework is\nthe iterative pyramid context module (PCM), which couples two tasks and stores\nthe shared latent semantics to interact between the two tasks. For semantic\nboundary detection, we propose the novel spatial gradient fusion to suppress\nnonsemantic edges. As semantic boundary detection is the dual task of semantic\nsegmentation, we introduce a loss function with boundary consistency constraint\nto improve the boundary pixel accuracy for semantic segmentation. Our extensive\nexperiments demonstrate superior performance over state-of-the-art works, not\nonly in semantic segmentation but also in semantic boundary detection. In\nparticular, a mean IoU score of 81:8% on Cityscapes test set is achieved\nwithout using coarse data or any external data for semantic segmentation. For\nsemantic boundary detection, we improve over previous state-of-the-art works by\n9.9% in terms of AP and 6:8% in terms of MF(ODS).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.05085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"LIAAD: Lightweight Attentive Angular Distillation for Large-scale\n  Age-Invariant Face Recognition\n\n  Disentangled representations have been commonly adopted to Age-invariant Face\nRecognition (AiFR) tasks. However, these methods have reached some limitations\nwith (1) the requirement of large-scale face recognition (FR) training data\nwith age labels, which is limited in practice; (2) heavy deep network\narchitectures for high performance; and (3) their evaluations are usually taken\nplace on age-related face databases while neglecting the standard large-scale\nFR databases to guarantee robustness. This work presents a novel Lightweight\nAttentive Angular Distillation (LIAAD) approach to Large-scale Lightweight AiFR\nthat overcomes these limitations. Given two high-performance heavy networks as\nteachers with different specialized knowledge, LIAAD introduces a learning\nparadigm to efficiently distill the age-invariant attentive and angular\nknowledge from those teachers to a lightweight student network making it more\npowerful with higher FR accuracy and robust against age factor. Consequently,\nLIAAD approach is able to take the advantages of both FR datasets with and\nwithout age labels to train an AiFR model. Far apart from prior distillation\nmethods mainly focusing on accuracy and compression ratios in closed-set\nproblems, our LIAAD aims to solve the open-set problem, i.e. large-scale face\nrecognition. Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and\nMegaFace-FGNet with one million distractors have demonstrated the efficiency of\nthe proposed approach on light-weight structure. This work also presents a new\nlongitudinal face aging (LogiFace) database \\footnote{This database will be\nmade available} for further studies in age-related facial problems in future.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.03791,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000081857,
      "text":"Learning A Single Network for Scale-Arbitrary Super-Resolution\n\n  Recently, the performance of single image super-resolution (SR) has been\nsignificantly improved with powerful networks. However, these networks are\ndeveloped for image SR with a single specific integer scale (e.g., x2;x3,x4),\nand cannot be used for non-integer and asymmetric SR. In this paper, we propose\nto learn a scale-arbitrary image SR network from scale-specific networks.\nSpecifically, we propose a plug-in module for existing SR networks to perform\nscale-arbitrary SR, which consists of multiple scale-aware feature adaption\nblocks and a scale-aware upsampling layer. Moreover, we introduce a scale-aware\nknowledge transfer paradigm to transfer knowledge from scale-specific networks\nto the scale-arbitrary network. Our plug-in module can be easily adapted to\nexisting networks to achieve scale-arbitrary SR. These networks plugged with\nour module can achieve promising results for non-integer and asymmetric SR\nwhile maintaining state-of-the-art performance for SR with integer scale\nfactors. Besides, the additional computational and memory cost of our module is\nvery small.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.00364,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"Generative Adversarial Data Programming\n\n  The paucity of large curated hand-labeled training data forms a major\nbottleneck in the deployment of machine learning models in computer vision and\nother fields. Recent work (Data Programming) has shown how distant supervision\nsignals in the form of labeling functions can be used to obtain labels for\ngiven data in near-constant time. In this work, we present Adversarial Data\nProgramming (ADP), which presents an adversarial methodology to generate data\nas well as a curated aggregated label, given a set of weak labeling functions.\nMore interestingly, such labeling functions are often easily generalizable,\nthus allowing our framework to be extended to different setups, including\nself-supervised labeled image generation, zero-shot text to labeled image\ngeneration, transfer learning, and multi-task learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.01946,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild\n\n  We introduce a simple and effective network architecture for monocular 3D\nhand pose estimation consisting of an image encoder followed by a mesh\nconvolutional decoder that is trained through a direct 3D hand mesh\nreconstruction loss. We train our network by gathering a large-scale dataset of\nhand action in YouTube videos and use it as a source of weak supervision. Our\nweakly-supervised mesh convolutions-based system largely outperforms\nstate-of-the-art methods, even halving the errors on the in the wild benchmark.\nThe dataset and additional resources are available at\nhttps:\/\/arielai.com\/mesh_hands.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2004.00186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000109275,
      "text":"Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object\n  Detection from Point Clouds\n\n  3D object detection based on point clouds has become more and more popular.\nSome methods propose localizing 3D objects directly from raw point clouds to\navoid information loss. However, these methods come with complex structures and\nsignificant computational overhead, limiting its broader application in\nreal-time scenarios. Some methods choose to transform the point cloud data into\ncompact tensors first and leverage off-the-shelf 2D detectors to propose 3D\nobjects, which is much faster and achieves state-of-the-art results. However,\nbecause of the inconsistency between 2D and 3D data, we argue that the\nperformance of compact tensor-based 3D detectors is restricted if we use 2D\ndetectors without corresponding modification. Specifically, the distribution of\npoint clouds is uneven, with most points gather on the boundary of objects,\nwhile detectors for 2D data always extract features evenly. Motivated by this\nobservation, we propose DENse Feature Indicator (DENFI), a universal module\nthat helps 3D detectors focus on the densest region of the point clouds in a\nboundary-aware manner. Moreover, DENFI is lightweight and guarantees real-time\nspeed when applied to 3D object detectors. Experiments on KITTI dataset show\nthat DENFI improves the performance of the baseline single-stage detector\nremarkably, which achieves new state-of-the-art performance among previous 3D\ndetectors, including both two-stage and multi-sensor fusion methods, in terms\nof mAP with a 34FPS detection speed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.13044,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Pay Attention to What You Read: Non-recurrent Handwritten Text-Line\n  Recognition\n\n  The advent of recurrent neural networks for handwriting recognition marked an\nimportant milestone reaching impressive recognition accuracies despite the\ngreat variability that we observe across different writing styles. Sequential\narchitectures are a perfect fit to model text lines, not only because of the\ninherent temporal aspect of text, but also to learn probability distributions\nover sequences of characters and words. However, using such recurrent paradigms\ncomes at a cost at training stage, since their sequential pipelines prevent\nparallelization. In this work, we introduce a non-recurrent approach to\nrecognize handwritten text by the use of transformer models. We propose a novel\nmethod that bypasses any recurrence. By using multi-head self-attention layers\nboth at the visual and textual stages, we are able to tackle character\nrecognition as well as to learn language-related dependencies of the character\nsequences to be decoded. Our model is unconstrained to any predefined\nvocabulary, being able to recognize out-of-vocabulary words, i.e. words that do\nnot appear in the training vocabulary. We significantly advance over prior art\nand demonstrate that satisfactory recognition accuracies are yielded even in\nfew-shot learning scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.12548,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Deepzzle: Solving Visual Jigsaw Puzzles with Deep Learning andShortest\n  Path Optimization\n\n  We tackle the image reassembly problem with wide space between the fragments,\nin such a way that the patterns and colors continuity is mostly unusable. The\nspacing emulates the erosion of which the archaeological fragments suffer. We\ncrop-square the fragments borders to compel our algorithm to learn from the\ncontent of the fragments. We also complicate the image reassembly by removing\nfragments and adding pieces from other sources. We use a two-step method to\nobtain the reassemblies: 1) a neural network predicts the positions of the\nfragments despite the gaps between them; 2) a graph that leads to the best\nreassemblies is made from these predictions. In this paper, we notably\ninvestigate the effect of branch-cut in the graph of reassemblies. We also\nprovide a comparison with the literature, solve complex images reassemblies,\nexplore at length the dataset, and propose a new metric that suits its\nspecificities.\n  Keywords: image reassembly, jigsaw puzzle, deep learning, graph, branch-cut,\ncultural heritage\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.13117,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000051657,
      "text":"SPIN: Structure-Preserving Inner Offset Network for Scene Text\n  Recognition\n\n  Arbitrary text appearance poses a great challenge in scene text recognition\ntasks. Existing works mostly handle with the problem in consideration of the\nshape distortion, including perspective distortions, line curvature or other\nstyle variations. Therefore, methods based on spatial transformers are\nextensively studied. However, chromatic difficulties in complex scenes have not\nbeen paid much attention on. In this work, we introduce a new learnable\ngeometric-unrelated module, the Structure-Preserving Inner Offset Network\n(SPIN), which allows the color manipulation of source data within the network.\nThis differentiable module can be inserted before any recognition architecture\nto ease the downstream tasks, giving neural networks the ability to actively\ntransform input intensity rather than the existing spatial rectification. It\ncan also serve as a complementary module to known spatial transformations and\nwork in both independent and collaborative ways with them. Extensive\nexperiments show that the use of SPIN results in a significant improvement on\nmultiple text recognition benchmarks compared to the state-of-the-arts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.0481,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"A Closed-Form Uncertainty Propagation in Non-Rigid Structure from Motion\n\n  Semi-Definite Programming (SDP) with low-rank prior has been widely applied\nin Non-Rigid Structure from Motion (NRSfM). Based on a low-rank constraint, it\navoids the inherent ambiguity of basis number selection in conventional\nbase-shape or base-trajectory methods. Despite the efficiency in deformable\nshape reconstruction, it remains unclear how to assess the uncertainty of the\nrecovered shape from the SDP process. In this paper, we present a statistical\ninference on the element-wise uncertainty quantification of the estimated\ndeforming 3D shape points in the case of the exact low-rank SDP problem. A\nclosed-form uncertainty quantification method is proposed and tested. Moreover,\nwe extend the exact low-rank uncertainty quantification to the approximate\nlow-rank scenario with a numerical optimal rank selection method, which enables\nsolving practical application in SDP based NRSfM scenario. The proposed method\nprovides an independent module to the SDP method and only requires the\nstatistic information of the input 2D tracked points. Extensive experiments\nprove that the output 3D points have identical normal distribution to the 2D\ntrackings, the proposed method and quantify the uncertainty accurately, and\nsupports that it has desirable effects on routinely SDP low-rank based NRSfM\nsolver.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.08186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000014239,
      "text":"Co-occurrence Based Texture Synthesis\n\n  As image generation techniques mature, there is a growing interest in\nexplainable representations that are easy to understand and intuitive to\nmanipulate. In this work, we turn to co-occurrence statistics, which have long\nbeen used for texture analysis, to learn a controllable texture synthesis\nmodel. We propose a fully convolutional generative adversarial network,\nconditioned locally on co-occurrence statistics, to generate arbitrarily large\nimages while having local, interpretable control over the texture appearance.\nTo encourage fidelity to the input condition, we introduce a novel\ndifferentiable co-occurrence loss that is integrated seamlessly into our\nframework in an end-to-end fashion. We demonstrate that our solution offers a\nstable, intuitive and interpretable latent representation for texture\nsynthesis, which can be used to generate a smooth texture morph between\ndifferent textures. We further show an interactive texture tool that allows a\nuser to adjust local characteristics of the synthesized texture image using the\nco-occurrence values directly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.05632,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Detecting CNN-Generated Facial Images in Real-World Scenarios\n\n  Artificial, CNN-generated images are now of such high quality that humans\nhave trouble distinguishing them from real images. Several algorithmic\ndetection methods have been proposed, but these appear to generalize poorly to\ndata from unknown sources, making them infeasible for real-world scenarios. In\nthis work, we present a framework for evaluating detection methods under\nreal-world conditions, consisting of cross-model, cross-data, and\npost-processing evaluation, and we evaluate state-of-the-art detection methods\nusing the proposed framework. Furthermore, we examine the usefulness of\ncommonly used image pre-processing methods. Lastly, we evaluate human\nperformance on detecting CNN-generated images, along with factors that\ninfluence this performance, by conducting an online survey. Our results suggest\nthat CNN-based detection methods are not yet robust enough to be used in\nreal-world scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.07232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"DiResNet: Direction-aware Residual Network for Road Extraction in VHR\n  Remote Sensing Images\n\n  The binary segmentation of roads in very high resolution (VHR) remote sensing\nimages (RSIs) has always been a challenging task due to factors such as\nocclusions (caused by shadows, trees, buildings, etc.) and the intra-class\nvariances of road surfaces. The wide use of convolutional neural networks\n(CNNs) has greatly improved the segmentation accuracy and made the task\nend-to-end trainable. However, there are still margins to improve in terms of\nthe completeness and connectivity of the results. In this paper, we consider\nthe specific context of road extraction and present a direction-aware residual\nnetwork (DiResNet) that includes three main contributions: 1) An asymmetric\nresidual segmentation network with deconvolutional layers and a structural\nsupervision to enhance the learning of road topology (DiResSeg); 2) A\npixel-level supervision of local directions to enhance the embedding of linear\nfeatures; 3) A refinement network to optimize the segmentation results\n(DiResRef). Ablation studies on two benchmark datasets (the Massachusetts\ndataset and the DeepGlobe dataset) have confirmed the effectiveness of the\npresented designs. Comparative experiments with other approaches show that the\nproposed method has advantages in both overall accuracy and F1-score. The code\nis available at: https:\/\/github.com\/ggsDing\/DiResNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.11472,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"Delving into the Imbalance of Positive Proposals in Two-stage Object\n  Detection\n\n  Imbalance issue is a major yet unsolved bottleneck for the current object\ndetection models. In this work, we observe two crucial yet never discussed\nimbalance issues. The first imbalance lies in the large number of low-quality\nRPN proposals, which makes the R-CNN module (i.e., post-classification layers)\nbecome highly biased towards the negative proposals in the early training\nstage. The second imbalance stems from the unbalanced ground-truth numbers\nacross different testing images, resulting in the imbalance of the number of\npotentially existing positive proposals in testing phase. To tackle these two\nimbalance issues, we incorporates two innovations into Faster R-CNN: 1) an\nR-CNN Gradient Annealing (RGA) strategy to enhance the impact of positive\nproposals in the early training stage. 2) a set of Parallel R-CNN Modules (PRM)\nwith different positive\/negative sampling ratios during training on one same\nbackbone. Our RGA and PRM can totally bring 2.0% improvements on AP on COCO\nminival. Experiments on CrowdHuman further validates the effectiveness of our\ninnovations across various kinds of object detection tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.08455,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels\n\n  Training with more data has always been the most stable and effective way of\nimproving performance in deep learning era. As the largest object detection\ndataset so far, Open Images brings great opportunities and challenges for\nobject detection in general and sophisticated scenarios. However, owing to its\nsemi-automatic collecting and labeling pipeline to deal with the huge data\nscale, Open Images dataset suffers from label-related problems that objects may\nexplicitly or implicitly have multiple labels and the label distribution is\nextremely imbalanced. In this work, we quantitatively analyze these label\nproblems and provide a simple but effective solution. We design a concurrent\nsoftmax to handle the multi-label problems in object detection and propose a\nsoft-sampling methods with hybrid training scheduler to deal with the label\nimbalance. Overall, our method yields a dramatic improvement of 3.34 points,\nleading to the best single model with 60.90 mAP on the public object detection\ntest set of Open Images. And our ensembling result achieves 67.17 mAP, which is\n4.29 points higher than the best result of Open Images public test 2018.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.11549,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Self-supervised Robust Object Detectors from Partially Labelled Datasets\n\n  In the object detection task, merging various datasets from similar contexts\nbut with different sets of Objects of Interest (OoI) is an inexpensive way (in\nterms of labor cost) for crafting a large-scale dataset covering a wide range\nof objects. Moreover, merging datasets allows us to train one integrated object\ndetector, instead of training several ones, which in turn resulting in the\nreduction of computational and time costs. However, merging the datasets from\nsimilar contexts causes samples with partial labeling as each constituent\ndataset is originally annotated for its own set of OoI and ignores to annotate\nthose objects that are become interested after merging the datasets. With the\ngoal of training \\emph{one integrated robust object detector with high\ngeneralization performance}, we propose a training framework to overcome\nmissing-label challenge of the merged datasets. More specifically, we propose a\ncomputationally efficient self-supervised framework to create on-the-fly\npseudo-labels for the unlabeled positive instances in the merged dataset in\norder to train the object detector jointly on both ground truth and pseudo\nlabels. We evaluate our proposed framework for training Yolo on a simulated\nmerged dataset with missing rate $\\approx\\!48\\%$ using VOC2012 and VOC2007. We\nempirically show that generalization performance of Yolo trained on both ground\ntruth and the pseudo-labels created by our method is on average $4\\%$ higher\nthan the ones trained only with the ground truth labels of the merged dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.02315,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"Multi-interactive Dual-decoder for RGB-thermal Salient Object Detection\n\n  RGB-thermal salient object detection (SOD) aims to segment the common\nprominent regions of visible image and corresponding thermal infrared image\nthat we call it RGBT SOD. Existing methods don't fully explore and exploit the\npotentials of complementarity of different modalities and multi-type cues of\nimage contents, which play a vital role in achieving accurate results. In this\npaper, we propose a multi-interactive dual-decoder to mine and model the\nmulti-type interactions for accurate RGBT SOD. In specific, we first encode two\nmodalities into multi-level multi-modal feature representations. Then, we\ndesign a novel dual-decoder to conduct the interactions of multi-level\nfeatures, two modalities and global contexts. With these interactions, our\nmethod works well in diversely challenging scenarios even in the presence of\ninvalid modality. Finally, we carry out extensive experiments on public RGBT\nand RGBD SOD datasets, and the results show that the proposed method achieves\nthe outstanding performance against state-of-the-art algorithms. The source\ncode has been released\nat:https:\/\/github.com\/lz118\/Multi-interactive-Dual-decoder.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.11945,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000133779,
      "text":"Multi-Margin based Decorrelation Learning for Heterogeneous Face\n  Recognition\n\n  Heterogeneous face recognition (HFR) refers to matching face images acquired\nfrom different domains with wide applications in security scenarios. This paper\npresents a deep neural network approach namely Multi-Margin based Decorrelation\nLearning (MMDL) to extract decorrelation representations in a hyperspherical\nspace for cross-domain face images. The proposed framework can be divided into\ntwo components: heterogeneous representation network and decorrelation\nrepresentation learning. First, we employ a large scale of accessible visual\nface images to train heterogeneous representation network. The decorrelation\nlayer projects the output of the first component into decorrelation latent\nsubspace and obtains decorrelation representation. In addition, we design a\nmulti-margin loss (MML), which consists of quadruplet margin loss (QML) and\nheterogeneous angular margin loss (HAML), to constrain the proposed framework.\nExperimental results on two challenging heterogeneous face databases show that\nour approach achieves superior performance on both verification and recognition\ntasks, comparing with state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.13452,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Improve bone age assessment by learning from anatomical local regions\n\n  Skeletal bone age assessment (BAA), as an essential imaging examination, aims\nat evaluating the biological and structural maturation of human bones. In the\nclinical practice, Tanner and Whitehouse (TW2) method is a widely-used method\nfor radiologists to perform BAA. The TW2 method splits the hands into Region Of\nInterests (ROI) and analyzes each of the anatomical ROI separately to estimate\nthe bone age. Because of considering the analysis of local information, the TW2\nmethod shows accurate results in practice. Following the spirit of TW2, we\npropose a novel model called Anatomical Local-Aware Network (ALA-Net) for\nautomatic bone age assessment. In ALA-Net, anatomical local extraction module\nis introduced to learn the hand structure and extract local information.\nMoreover, we design an anatomical patch training strategy to provide extra\nregularization during the training process. Our model can detect the anatomical\nROIs and estimate bone age jointly in an end-to-end manner. The experimental\nresults show that our ALA-Net achieves a new state-of-the-art single model\nperformance of 3.91 mean absolute error (MAE) on the public available RSNA\ndataset. Since the design of our model is well consistent with the well\nrecognized TW2 method, it is interpretable and reliable for clinical usage.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.06136,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Self-Supervised Deep Visual Odometry with Online Adaptation\n\n  Self-supervised VO methods have shown great success in jointly estimating\ncamera pose and depth from videos. However, like most data-driven methods,\nexisting VO networks suffer from a notable decrease in performance when\nconfronted with scenes different from the training data, which makes them\nunsuitable for practical applications. In this paper, we propose an online\nmeta-learning algorithm to enable VO networks to continuously adapt to new\nenvironments in a self-supervised manner. The proposed method utilizes\nconvolutional long short-term memory (convLSTM) to aggregate rich\nspatial-temporal information in the past. The network is able to memorize and\nlearn from its past experience for better estimation and fast adaptation to the\ncurrent frame. When running VO in the open world, in order to deal with the\nchanging environment, we propose an online feature alignment method by aligning\nfeature distributions at different time. Our VO network is able to seamlessly\nadapt to different environments. Extensive experiments on unseen outdoor\nscenes, virtual to real world and outdoor to indoor environments demonstrate\nthat our method consistently outperforms state-of-the-art self-supervised VO\nbaselines considerably.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04414,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Memory-Augmented Relation Network for Few-Shot Learning\n\n  Metric-based few-shot learning methods concentrate on learning transferable\nfeature embedding that generalizes well from seen categories to unseen\ncategories under the supervision of limited number of labelled instances.\nHowever, most of them treat each individual instance in the working context\nseparately without considering its relationships with the others. In this work,\nwe investigate a new metric-learning method, Memory-Augmented Relation Network\n(MRN), to explicitly exploit these relationships. In particular, for an\ninstance, we choose the samples that are visually similar from the working\ncontext, and perform weighted information propagation to attentively aggregate\nhelpful information from the chosen ones to enhance its representation. In MRN,\nwe also formulate the distance metric as a learnable relation module which\nlearns to compare for similarity measurement, and augment the working context\nwith memory slots, both contributing to its generality. We empirically\ndemonstrate that MRN yields significant improvement over its ancestor and\nachieves competitive or even better performance when compared with other\nfew-shot learning approaches on the two major benchmark datasets, i.e.\nminiImagenet and tieredImagenet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.03101,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Scale-Equalizing Pyramid Convolution for Object Detection\n\n  Feature pyramid has been an efficient method to extract features at different\nscales. Development over this method mainly focuses on aggregating contextual\ninformation at different levels while seldom touching the inter-level\ncorrelation in the feature pyramid. Early computer vision methods extracted\nscale-invariant features by locating the feature extrema in both spatial and\nscale dimension. Inspired by this, a convolution across the pyramid level is\nproposed in this study, which is termed pyramid convolution and is a modified\n3-D convolution. Stacked pyramid convolutions directly extract 3-D (scale and\nspatial) features and outperforms other meticulously designed feature fusion\nmodules. Based on the viewpoint of 3-D convolution, an integrated batch\nnormalization that collects statistics from the whole feature pyramid is\nnaturally inserted after the pyramid convolution. Furthermore, we also show\nthat the naive pyramid convolution, together with the design of RetinaNet head,\nactually best applies for extracting features from a Gaussian pyramid, whose\nproperties can hardly be satisfied by a feature pyramid. In order to alleviate\nthis discrepancy, we build a scale-equalizing pyramid convolution (SEPC) that\naligns the shared pyramid convolution kernel only at high-level feature maps.\nBeing computationally efficient and compatible with the head design of most\nsingle-stage object detectors, the SEPC module brings significant performance\nimprovement ($>4$AP increase on MS-COCO2017 dataset) in state-of-the-art\none-stage object detectors, and a light version of SEPC also has $\\sim3.5$AP\ngain with only around 7% inference time increase. The pyramid convolution also\nfunctions well as a stand-alone module in two-stage object detectors and is\nable to improve the performance by $\\sim2$AP. The source code can be found at\nhttps:\/\/github.com\/jshilong\/SEPC.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Variational Clustering: Leveraging Variational Autoencoders for Image\n  Clustering\n\n  Recent advances in deep learning have shown their ability to learn strong\nfeature representations for images. The task of image clustering naturally\nrequires good feature representations to capture the distribution of the data\nand subsequently differentiate data points from one another. Often these two\naspects are dealt with independently and thus traditional feature learning\nalone does not suffice in partitioning the data meaningfully. Variational\nAutoencoders (VAEs) naturally lend themselves to learning data distributions in\na latent space. Since we wish to efficiently discriminate between different\nclusters in the data, we propose a method based on VAEs where we use a Gaussian\nMixture prior to help cluster the images accurately. We jointly learn the\nparameters of both the prior and the posterior distributions. Our method\nrepresents a true Gaussian Mixture VAE. This way, our method simultaneously\nlearns a prior that captures the latent distribution of the images and a\nposterior to help discriminate well between data points. We also propose a\nnovel reparametrization of the latent space consisting of a mixture of discrete\nand continuous variables. One key takeaway is that our method generalizes\nbetter across different datasets without using any pre-training or learnt\nmodels, unlike existing methods, allowing it to be trained from scratch in an\nend-to-end manner. We verify our efficacy and generalizability experimentally\nby achieving state-of-the-art results among unsupervised methods on a variety\nof datasets. To the best of our knowledge, we are the first to pursue image\nclustering using VAEs in a purely unsupervised manner on real image datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.09623,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Focus on defocus: bridging the synthetic to real domain gap for depth\n  estimation\n\n  Data-driven depth estimation methods struggle with the generalization outside\ntheir training scenes due to the immense variability of the real-world scenes.\nThis problem can be partially addressed by utilising synthetically generated\nimages, but closing the synthetic-real domain gap is far from trivial. In this\npaper, we tackle this issue by using domain invariant defocus blur as direct\nsupervision. We leverage defocus cues by using a permutation invariant\nconvolutional neural network that encourages the network to learn from the\ndifferences between images with a different point of focus. Our proposed\nnetwork uses the defocus map as an intermediate supervisory signal. We are able\nto train our model completely on synthetic data and directly apply it to a wide\nrange of real-world images. We evaluate our model on synthetic and real\ndatasets, showing compelling generalization results and state-of-the-art depth\nprediction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.04668,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000041061,
      "text":"Domain Adaptation for Image Dehazing\n\n  Image dehazing using learning-based methods has achieved state-of-the-art\nperformance in recent years. However, most existing methods train a dehazing\nmodel on synthetic hazy images, which are less able to generalize well to real\nhazy images due to domain shift. To address this issue, we propose a domain\nadaptation paradigm, which consists of an image translation module and two\nimage dehazing modules. Specifically, we first apply a bidirectional\ntranslation network to bridge the gap between the synthetic and real domains by\ntranslating images from one domain to another. And then, we use images before\nand after translation to train the proposed two image dehazing networks with a\nconsistency constraint. In this phase, we incorporate the real hazy image into\nthe dehazing training via exploiting the properties of the clear image (e.g.,\ndark channel prior and image gradient smoothing) to further improve the domain\nadaptivity. By training image translation and dehazing network in an end-to-end\nmanner, we can obtain better effects of both image translation and dehazing.\nExperimental results on both synthetic and real-world images demonstrate that\nour model performs favorably against the state-of-the-art dehazing algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2005.13298,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"An Iteratively Optimized Patch Label Inference Network for Automatic\n  Pavement Distress Detection\n\n  We present a novel deep learning framework named the Iteratively Optimized\nPatch Label Inference Network (IOPLIN) for automatically detecting various\npavement distresses that are not solely limited to specific ones, such as\ncracks and potholes. IOPLIN can be iteratively trained with only the image\nlabel via the Expectation-Maximization Inspired Patch Label Distillation\n(EMIPLD) strategy, and accomplish this task well by inferring the labels of\npatches from the pavement images. IOPLIN enjoys many desirable properties over\nthe state-of-the-art single branch CNN models such as GoogLeNet and\nEfficientNet. It is able to handle images in different resolutions, and\nsufficiently utilize image information particularly for the high-resolution\nones, since IOPLIN extracts the visual features from unrevised image patches\ninstead of the resized entire image. Moreover, it can roughly localize the\npavement distress without using any prior localization information in the\ntraining phase. In order to better evaluate the effectiveness of our method in\npractice, we construct a large-scale Bituminous Pavement Disease Detection\ndataset named CQU-BPDD consisting of 60,059 high-resolution pavement images,\nwhich are acquired from different areas at different times. Extensive results\non this dataset demonstrate the superiority of IOPLIN over the state-of-the-art\nimage classification approaches in automatic pavement distress detection. The\nsource codes of IOPLIN are released on\n\\url{https:\/\/github.com\/DearCaat\/ioplin}, and the CQU-BPDD dataset is able to\nbe accessed on \\url{https:\/\/dearcaat.github.io\/CQU-BPDD\/}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.03656,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000127157,
      "text":"AutoHAS: Efficient Hyperparameter and Architecture Search\n\n  Efficient hyperparameter or architecture search methods have shown remarkable\nresults, but each of them is only applicable to searching for either\nhyperparameters (HPs) or architectures. In this work, we propose a unified\npipeline, AutoHAS, to efficiently search for both architectures and\nhyperparameters. AutoHAS learns to alternately update the shared network\nweights and a reinforcement learning (RL) controller, which learns the\nprobability distribution for the architecture candidates and HP candidates. A\ntemporary weight is introduced to store the updated weight from the selected\nHPs (by the controller), and a validation accuracy based on this temporary\nweight serves as a reward to update the controller. In experiments, we show\nAutoHAS is efficient and generalizable to different search spaces, baselines\nand datasets. In particular, AutoHAS can improve the accuracy over popular\nnetwork architectures, such as ResNet and EfficientNet, on CIFAR-10\/100,\nImageNet, and four more other datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.13517,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000041657,
      "text":"3D Pose Detection in Videos: Focusing on Occlusion\n\n  In this work, we build upon existing methods for occlusion-aware 3D pose\ndetection in videos. We implement a two stage architecture that consists of the\nstacked hourglass network to produce 2D pose predictions, which are then\ninputted into a temporal convolutional network to produce 3D pose predictions.\nTo facilitate prediction on poses with occluded joints, we introduce an\nintuitive generalization of the cylinder man model used to generate occlusion\nlabels. We find that the occlusion-aware network is able to achieve a\nmean-per-joint-position error 5 mm less than our linear baseline model on the\nHuman3.6M dataset. Compared to our temporal convolutional network baseline, we\nachieve a comparable mean-per-joint-position error of 0.1 mm less at reduced\ncomputational cost.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.15373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"MTStereo 2.0: improved accuracy of stereo depth estimation withMax-trees\n\n  Efficient yet accurate extraction of depth from stereo image pairs is\nrequired by systems with low power resources, such as robotics and embedded\nsystems. State-of-the-art stereo matching methods based on convolutional neural\nnetworks require intensive computations on GPUs and are difficult to deploy on\nembedded systems. In this paper, we propose a stereo matching method, called\nMTStereo 2.0, for limited-resource systems that require efficient and accurate\ndepth estimation. It is based on a Max-tree hierarchical representation of\nimage pairs, which we use to identify matching regions along image scan-lines.\nThe method includes a cost function that considers similarity of region\ncontextual information based on the Max-trees and a disparity border preserving\ncost aggregation approach. MTStereo 2.0 improves on its predecessor MTStereo\n1.0 as it a) deploys a more robust cost function, b) performs more thorough\ndetection of incorrect matches, c) computes disparity maps with pixel-level\nrather than node-level precision. MTStereo provides accurate sparse and\nsemi-dense depth estimation and does not require intensive GPU computations\nlike methods based on CNNs. Thus it can run on embedded and robotics devices\nwith low-power requirements. We tested the proposed approach on several\nbenchmark data sets, namely KITTI 2015, Driving, FlyingThings3D, Middlebury\n2014, Monkaa and the TrimBot2020 garden data sets, and achieved competitive\naccuracy and efficiency. The code is available at\nhttps:\/\/github.com\/rbrandt1\/MaxTreeS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.00809,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"Foreground-aware Semantic Representations for Image Harmonization\n\n  Image harmonization is an important step in photo editing to achieve visual\nconsistency in composite images by adjusting the appearances of foreground to\nmake it compatible with background. Previous approaches to harmonize composites\nare based on training of encoder-decoder networks from scratch, which makes it\nchallenging for a neural network to learn a high-level representation of\nobjects. We propose a novel architecture to utilize the space of high-level\nfeatures learned by a pre-trained classification network. We create our models\nas a combination of existing encoder-decoder architectures and a pre-trained\nforeground-aware deep high-resolution network. We extensively evaluate the\nproposed method on existing image harmonization benchmark and set up a new\nstate-of-the-art in terms of MSE and PSNR metrics. The code and trained models\nare available at \\url{https:\/\/github.com\/saic-vul\/image_harmonization}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.07802,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Geometry-Aware Instance Segmentation with Disparity Maps\n\n  Most previous works of outdoor instance segmentation for images only use\ncolor information. We explore a novel direction of sensor fusion to exploit\nstereo cameras. Geometric information from disparities helps separate\noverlapping objects of the same or different classes. Moreover, geometric\ninformation penalizes region proposals with unlikely 3D shapes thus suppressing\nfalse positive detections. Mask regression is based on 2D, 2.5D, and 3D ROI\nusing the pseudo-lidar and image-based representations. These mask predictions\nare fused by a mask scoring process. However, public datasets only adopt stereo\nsystems with shorter baseline and focal legnth, which limit measuring ranges of\nstereo cameras. We collect and utilize High-Quality Driving Stereo (HQDS)\ndataset, using much longer baseline and focal length with higher resolution.\nOur performance attains state of the art. Please refer to our project page. The\nfull paper is available here.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.03708,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000087089,
      "text":"Dilated Convolutions with Lateral Inhibitions for Semantic Image\n  Segmentation\n\n  Dilated convolutions are widely used in deep semantic segmentation models as\nthey can enlarge the filters' receptive field without adding additional weights\nnor sacrificing spatial resolution. However, as dilated convolutional filters\ndo not possess positional knowledge about the pixels on semantically meaningful\ncontours, they could lead to ambiguous predictions on object boundaries. In\naddition, although dilating the filter can expand its receptive field, the\ntotal number of sampled pixels remains unchanged, which usually comprises a\nsmall fraction of the receptive field's total area. Inspired by the Lateral\nInhibition (LI) mechanisms in human visual systems, we propose the dilated\nconvolution with lateral inhibitions (LI-Convs) to overcome these limitations.\nIntroducing LI mechanisms improves the convolutional filter's sensitivity to\nsemantic object boundaries. Moreover, since LI-Convs also implicitly take the\npixels from the laterally inhibited zones into consideration, they can also\nextract features at a denser scale. By integrating LI-Convs into the Deeplabv3+\narchitecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling\n(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral\nInhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets\n(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation\nmodels outperform the baseline on all of them, thus verify the effectiveness\nand generality of the proposed LI-Convs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.02802,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"A Computational Model of Early Word Learning from the Infant's Point of\n  View\n\n  Human infants have the remarkable ability to learn the associations between\nobject names and visual objects from inherently ambiguous experiences.\nResearchers in cognitive science and developmental psychology have built formal\nmodels that implement in-principle learning algorithms, and then used\npre-selected and pre-cleaned datasets to test the abilities of the models to\nfind statistical regularities in the input data. In contrast to previous\nmodeling approaches, the present study used egocentric video and gaze data\ncollected from infant learners during natural toy play with their parents. This\nallowed us to capture the learning environment from the perspective of the\nlearner's own point of view. We then used a Convolutional Neural Network (CNN)\nmodel to process sensory data from the infant's point of view and learn\nname-object associations from scratch. As the first model that takes raw\negocentric video to simulate infant word learning, the present study provides a\nproof of principle that the problem of early word learning can be solved, using\nactual visual data perceived by infant learners. Moreover, we conducted\nsimulation experiments to systematically determine how visual, perceptual, and\nattentional properties of infants' sensory experiences may affect word\nlearning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04473,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Deep hierarchical pooling design for cross-granularity action\n  recognition\n\n  In this paper, we introduce a novel hierarchical aggregation design that\ncaptures different levels of temporal granularity in action recognition. Our\ndesign principle is coarse-to-fine and achieved using a tree-structured\nnetwork; as we traverse this network top-down, pooling operations are getting\nless invariant but timely more resolute and well localized. Learning the\ncombination of operations in this network -- which best fits a given\nground-truth -- is obtained by solving a constrained minimization problem whose\nsolution corresponds to the distribution of weights that capture the\ncontribution of each level (and thereby temporal granularity) in the global\nhierarchical pooling process. Besides being principled and well grounded, the\nproposed hierarchical pooling is also video-length agnostic and resilient to\nmisalignments in actions. Extensive experiments conducted on the challenging\nUCF-101 database corroborate these statements.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.1225,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000354979,
      "text":"Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from\n  Single and Multiple Images\n\n  Recovering the 3D shape of an object from single or multiple images with deep\nneural networks has been attracting increasing attention in the past few years.\nMainstream works (e.g. 3D-R2N2) use recurrent neural networks (RNNs) to\nsequentially fuse feature maps of input images. However, RNN-based approaches\nare unable to produce consistent reconstruction results when given the same\ninput images with different orders. Moreover, RNNs may forget important\nfeatures from early input images due to long-term memory loss. To address these\nissues, we propose a novel framework for single-view and multi-view 3D object\nreconstruction, named Pix2Vox++. By using a well-designed encoder-decoder, it\ngenerates a coarse 3D volume from each input image. A multi-scale context-aware\nfusion module is then introduced to adaptively select high-quality\nreconstructions for different parts from all coarse 3D volumes to obtain a\nfused 3D volume. To further correct the wrongly recovered parts in the fused 3D\nvolume, a refiner is adopted to generate the final output. Experimental results\non the ShapeNet, Pix3D, and Things3D benchmarks show that Pix2Vox++ performs\nfavorably against state-of-the-art methods in terms of both accuracy and\nefficiency.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04991,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Beyond Triplet Loss: Meta Prototypical N-tuple Loss for Person\n  Re-identification\n\n  Person Re-identification (ReID) aims at matching a person of interest across\nimages. In convolutional neural network (CNN) based approaches, loss design\nplays a vital role in pulling closer features of the same identity and pushing\nfar apart features of different identities. In recent years, triplet loss\nachieves superior performance and is predominant in ReID. However, triplet loss\nconsiders only three instances of two classes in per-query optimization (with\nan anchor sample as query) and it is actually equivalent to a two-class\nclassification. There is a lack of loss design which enables the joint\noptimization of multiple instances (of multiple classes) within per-query\noptimization for person ReID. In this paper, we introduce a multi-class\nclassification loss, i.e., N-tuple loss, to jointly consider multiple (N)\ninstances for per-query optimization. This in fact aligns better with the ReID\ntest\/inference process, which conducts the ranking\/comparisons among multiple\ninstances. Furthermore, for more efficient multi-class classification, we\npropose a new meta prototypical N-tuple loss. With the multi-class\nclassification incorporated, our model achieves the state-of-the-art\nperformance on the benchmark person ReID datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16242,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network\n  Architectures\n\n  In this paper, we tackle the problem of convolutional neural network design.\nInstead of focusing on the design of the overall architecture, we investigate a\ndesign space that is usually overlooked, i.e. adjusting the channel\nconfigurations of predefined networks. We find that this adjustment can be\nachieved by shrinking widened baseline networks and leads to superior\nperformance. Based on that, we articulate the heterogeneity hypothesis: with\nthe same training protocol, there exists a layer-wise differentiated network\narchitecture (LW-DNA) that can outperform the original network with regular\nchannel configurations but with a lower level of model complexity.\n  The LW-DNA models are identified without extra computational cost or training\ntime compared with the original network. This constraint leads to controlled\nexperiments which direct the focus to the importance of layer-wise specific\nchannel configurations. LW-DNA models come with advantages related to\noverfitting, i.e. the relative relationship between model complexity and\ndataset size. Experiments are conducted on various networks and datasets for\nimage classification, visual tracking and image restoration. The resultant\nLW-DNA models consistently outperform the baseline models. Code is available at\nhttps:\/\/github.com\/ofsoundof\/Heterogeneity_Hypothesis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.05077,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"SEKD: Self-Evolving Keypoint Detection and Description\n\n  Researchers have attempted utilizing deep neural network (DNN) to learn novel\nlocal features from images inspired by its recent successes on a variety of\nvision tasks. However, existing DNN-based algorithms have not achieved such\nremarkable progress that could be partly attributed to insufficient utilization\nof the interactive characters between local feature detector and descriptor. To\nalleviate these difficulties, we emphasize two desired properties, i.e.,\nrepeatability and reliability, to simultaneously summarize the inherent and\ninteractive characters of local feature detector and descriptor. Guided by\nthese properties, a self-supervised framework, namely self-evolving keypoint\ndetection and description (SEKD), is proposed to learn an advanced local\nfeature model from unlabeled natural images. Additionally, to have performance\nguarantees, novel training strategies have also been dedicatedly designed to\nminimize the gap between the learned feature and its properties. We benchmark\nthe proposed method on homography estimation, relative pose estimation, and\nstructure-from-motion tasks. Extensive experimental results demonstrate that\nthe proposed method outperforms popular hand-crafted and DNN-based methods by\nremarkable margins. Ablation studies also verify the effectiveness of each\ncritical training strategy. We will release our code along with the trained\nmodel publicly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.07606,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000205967,
      "text":"Faces \\`a la Carte: Text-to-Face Generation via Attribute\n  Disentanglement\n\n  Text-to-Face (TTF) synthesis is a challenging task with great potential for\ndiverse computer vision applications. Compared to Text-to-Image (TTI) synthesis\ntasks, the textual description of faces can be much more complicated and\ndetailed due to the variety of facial attributes and the parsing of high\ndimensional abstract natural language. In this paper, we propose a Text-to-Face\nmodel that not only produces images in high resolution (1024x1024) with\ntext-to-image consistency, but also outputs multiple diverse faces to cover a\nwide range of unspecified facial features in a natural way. By fine-tuning the\nmulti-label classifier and image encoder, our model obtains the vectors and\nimage embeddings which are used to transform the input noise vector sampled\nfrom the normal distribution. Afterwards, the transformed noise vector is fed\ninto a pre-trained high-resolution image generator to produce a set of faces\nwith the desired facial attributes. We refer to our model as TTF-HD.\nExperimental results show that TTF-HD generates high-quality faces with\nstate-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04898,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Reposing Humans by Warping 3D Features\n\n  We address the problem of reposing an image of a human into any desired novel\npose. This conditional image-generation task requires reasoning about the 3D\nstructure of the human, including self-occluded body parts. Most prior works\nare either based on 2D representations or require fitting and manipulating an\nexplicit 3D body mesh. Based on the recent success in deep learning-based\nvolumetric representations, we propose to implicitly learn a dense feature\nvolume from human images, which lends itself to simple and intuitive\nmanipulation through explicit geometric warping. Once the latent feature volume\nis warped according to the desired pose change, the volume is mapped back to\nRGB space by a convolutional decoder. Our state-of-the-art results on the\nDeepFashion and the iPER benchmarks indicate that dense volumetric human\nrepresentations are worth investigating in more detail.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.05941,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000211265,
      "text":"MultiResolution Attention Extractor for Small Object Detection\n\n  Small objects are difficult to detect because of their low resolution and\nsmall size. The existing small object detection methods mainly focus on data\npreprocessing or narrowing the differences between large and small objects.\nInspired by human vision \"attention\" mechanism, we exploit two feature\nextraction methods to mine the most useful information of small objects. Both\nmethods are based on multiresolution feature extraction. We initially design\nand explore the soft attention method, but we find that its convergence speed\nis slow. Then we present the second method, an attention-based feature\ninteraction method, called a MultiResolution Attention Extractor (MRAE),\nshowing significant improvement as a generic feature extractor in small object\ndetection. After each building block in the vanilla feature extractor, we\nappend a small network to generate attention weights followed by a weighted-sum\noperation to get the final attention maps. Our attention-based feature\nextractor is 2.0 times the AP of the \"hard\" attention counterpart (plain\narchitecture) on the COCO small object detection benchmark, proving that MRAE\ncan capture useful location and contextual information through adaptive\nlearning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.1038,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000086427,
      "text":"Video Semantic Segmentation with Distortion-Aware Feature Correction\n\n  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.02176,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000065896,
      "text":"Multi-Temporal Scene Classification and Scene Change Detection with\n  Correlation based Fusion\n\n  Classifying multi-temporal scene land-use categories and detecting their\nsemantic scene-level changes for imagery covering urban regions could\nstraightly reflect the land-use transitions. Existing methods for scene change\ndetection rarely focus on the temporal correlation of bi-temporal features, and\nare mainly evaluated on small scale scene change detection datasets. In this\nwork, we proposed a CorrFusion module that fuses the highly correlated\ncomponents in bi-temporal feature embeddings. We firstly extracts the deep\nrepresentations of the bi-temporal inputs with deep convolutional networks.\nThen the extracted features will be projected into a lower dimension space to\ncomputed the instance-level correlation. The cross-temporal fusion will be\nperformed based on the computed correlation in CorrFusion module. The final\nscene classification are obtained with softmax activation layers. In the\nobjective function, we introduced a new formulation for calculating the\ntemporal correlation. The detailed derivation of backpropagation gradients for\nthe proposed module is also given in this paper. Besides, we presented a much\nlarger scale scene change detection dataset and conducted experiments on this\ndataset. The experimental results demonstrated that our proposed CorrFusion\nmodule could remarkably improve the multi-temporal scene classification and\nscene change detection results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.0674,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000042386,
      "text":"Gaze estimation problem tackled through synthetic images\n\n  In this paper, we evaluate a synthetic framework to be used in the field of\ngaze estimation employing deep learning techniques. The lack of sufficient\nannotated data could be overcome by the utilization of a synthetic evaluation\nframework as far as it resembles the behavior of a real scenario. In this work,\nwe use U2Eyes synthetic environment employing I2Head datataset as real\nbenchmark for comparison based on alternative training and testing strategies.\nThe results obtained show comparable average behavior between both frameworks\nalthough significantly more robust and stable performance is retrieved by the\nsynthetic images. Additionally, the potential of synthetically pretrained\nmodels in order to be applied in user's specific calibration strategies is\nshown with outstanding performances.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.16992,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000135766,
      "text":"Deep Isometric Learning for Visual Recognition\n\n  Initialization, normalization, and skip connections are believed to be three\nindispensable techniques for training very deep convolutional neural networks\nand obtaining state-of-the-art performance. This paper shows that deep vanilla\nConvNets without normalization nor skip connections can also be trained to\nachieve surprisingly good performance on standard image recognition benchmarks.\nThis is achieved by enforcing the convolution kernels to be near isometric\nduring initialization and training, as well as by using a variant of ReLU that\nis shifted towards being isometric. Further experiments show that if combined\nwith skip connections, such near isometric networks can achieve performances on\npar with (for ImageNet) and better than (for COCO) the standard ResNet, even\nwithout normalization at all. Our code is available at\nhttps:\/\/github.com\/HaozhiQi\/ISONet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2006.04203,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000089738,
      "text":"Thoracic Disease Identification and Localization using Distance Learning\n  and Region Verification\n\n  The identification and localization of diseases in medical images using deep\nlearning models have recently attracted significant interest. Existing methods\nonly consider training the networks with each image independently and most\nleverage an activation map for disease localization. In this paper, we propose\nan alternative approach that learns discriminative features among triplets of\nimages and cyclically trains on region features to verify whether attentive\nregions contain information indicative of a disease. Concretely, we adapt a\ndistance learning framework for multi-label disease classification to\ndifferentiate subtle disease features. Additionally, we feed back the features\nof the predicted class-specific regions to a separate classifier during\ntraining to better verify the localized diseases. Our model can achieve\nstate-of-the-art classification performance on the challenging ChestX-ray14\ndataset, and our ablation studies indicate that both distance learning and\nregion verification contribute to overall classification performance. Moreover,\nthe distance learning and region verification modules can capture essential\ninformation for better localization than baseline models without these modules.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09629,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Character Region Attention For Text Spotting\n\n  A scene text spotter is composed of text detection and recognition modules.\nMany studies have been conducted to unify these modules into an end-to-end\ntrainable model to achieve better performance. A typical architecture places\ndetection and recognition modules into separate branches, and a RoI pooling is\ncommonly used to let the branches share a visual feature. However, there still\nexists a chance of establishing a more complimentary connection between the\nmodules when adopting recognizer that uses attention-based decoder and detector\nthat represents spatial information of the character regions. This is possible\nsince the two modules share a common sub-task which is to find the location of\nthe character regions. Based on the insight, we construct a tightly coupled\nsingle pipeline model. This architecture is formed by utilizing detection\noutputs in the recognizer and propagating the recognition loss through the\ndetection stage. The use of character score map helps the recognizer attend\nbetter to the character center points, and the recognition loss propagation to\nthe detector module enhances the localization of the character regions. Also, a\nstrengthened sharing stage allows feature rectification and boundary\nlocalization of arbitrary-shaped text regions. Extensive experiments\ndemonstrate state-of-the-art performance in publicly available straight and\ncurved benchmark dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.11349,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000315242,
      "text":"Learning Directional Feature Maps for Cardiac MRI Segmentation\n\n  Cardiac MRI segmentation plays a crucial role in clinical diagnosis for\nevaluating personalized cardiac performance parameters. Due to the indistinct\nboundaries and heterogeneous intensity distributions in the cardiac MRI, most\nexisting methods still suffer from two aspects of challenges: inter-class\nindistinction and intra-class inconsistency. To tackle these two problems, we\npropose a novel method to exploit the directional feature maps, which can\nsimultaneously strengthen the differences between classes and the similarities\nwithin classes. Specifically, we perform cardiac segmentation and learn a\ndirection field pointing away from the nearest cardiac tissue boundary to each\npixel via a direction field (DF) module. Based on the learned direction field,\nwe then propose a feature rectification and fusion (FRF) module to improve the\noriginal segmentation features, and obtain the final segmentation. The proposed\nmodules are simple yet effective and can be flexibly added to any existing\nsegmentation network without excessively increasing time and space complexity.\nWe evaluate the proposed method on the 2017 MICCAI Automated Cardiac Diagnosis\nChallenge (ACDC) dataset and a large-scale self-collected dataset, showing good\nsegmentation performance and robust generalization ability of the proposed\nmethod.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.0469,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Pollen13K: A Large Scale Microscope Pollen Grain Image Dataset\n\n  Pollen grain classification has a remarkable role in many fields from\nmedicine to biology and agronomy. Indeed, automatic pollen grain classification\nis an important task for all related applications and areas. This work presents\nthe first large-scale pollen grain image dataset, including more than 13\nthousands objects. After an introduction to the problem of pollen grain\nclassification and its motivations, the paper focuses on the employed data\nacquisition steps, which include aerobiological sampling, microscope image\nacquisition, object detection, segmentation and labelling. Furthermore, a\nbaseline experimental assessment for the task of pollen classification on the\nbuilt dataset, together with discussion on the achieved results, is presented.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.0818,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000031127,
      "text":"Challenge report:VIPriors Action Recognition Challenge\n\n  This paper is a brief report to our submission to the VIPriors Action\nRecognition Challenge. Action recognition has attracted many researchers\nattention for its full application, but it is still challenging. In this paper,\nwe study previous methods and propose our method. In our method, we are\nprimarily making improvements on the SlowFast Network and fusing with TSM to\nmake further breakthroughs. Also, we use a fast but effective way to extract\nmotion features from videos by using residual frames as input. Better motion\nfeatures can be extracted using residual frames with SlowFast, and the\nresidual-frame-input path is an excellent supplement for existing\nRGB-frame-input models. And better performance obtained by combining 3D\nconvolution(SlowFast) with 2D convolution(TSM). The above experiments were all\ntrained from scratch on UCF101.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.08247,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"Autoregressive Unsupervised Image Segmentation\n\n  In this work, we propose a new unsupervised image segmentation approach based\non mutual information maximization between different constructed views of the\ninputs. Taking inspiration from autoregressive generative models that predict\nthe current pixel from past pixels in a raster-scan ordering created with\nmasked convolutions, we propose to use different orderings over the inputs\nusing various forms of masked convolutions to construct different views of the\ndata. For a given input, the model produces a pair of predictions with two\nvalid orderings, and is then trained to maximize the mutual information between\nthe two outputs. These outputs can either be low-dimensional features for\nrepresentation learning or output clusters corresponding to semantic labels for\nclustering. While masked convolutions are used during training, in inference,\nno masking is applied and we fall back to the standard convolution where the\nmodel has access to the full input. The proposed method outperforms current\nstate-of-the-art on unsupervised image segmentation. It is simple and easy to\nimplement, and can be extended to other visual tasks and integrated seamlessly\ninto existing unsupervised learning methods requiring different views of the\ndata.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.10985,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"PointContrast: Unsupervised Pre-training for 3D Point Cloud\n  Understanding\n\n  Arguably one of the top success stories of deep learning is transfer\nlearning. The finding that pre-training a network on a rich source set (eg.,\nImageNet) can help boost performance once fine-tuned on a usually much smaller\ntarget set, has been instrumental to many applications in language and vision.\nYet, very little is known about its usefulness in 3D point cloud understanding.\nWe see this as an opportunity considering the effort required for annotating\ndata in 3D. In this work, we aim at facilitating research on 3D representation\nlearning. Different from previous works, we focus on high-level scene\nunderstanding tasks. To this end, we select a suite of diverse datasets and\ntasks to measure the effect of unsupervised pre-training on a large source set\nof 3D scenes. Our findings are extremely encouraging: using a unified triplet\nof architecture, source dataset, and contrastive loss for pre-training, we\nachieve improvement over recent best results in segmentation and detection\nacross 6 different benchmarks for indoor and outdoor, real and synthetic\ndatasets -- demonstrating that the learned representation can generalize across\ndomains. Furthermore, the improvement was similar to supervised pre-training,\nsuggesting that future efforts should favor scaling data collection over more\ndetailed annotation. We hope these findings will encourage more research on\nunsupervised pretext task design for 3D deep learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.12577,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"A Lightweight Neural Network for Monocular View Generation with\n  Occlusion Handling\n\n  In this article, we present a very lightweight neural network architecture,\ntrained on stereo data pairs, which performs view synthesis from one single\nimage. With the growing success of multi-view formats, this problem is indeed\nincreasingly relevant. The network returns a prediction built from disparity\nestimation, which fills in wrongly predicted regions using a occlusion handling\ntechnique. To do so, during training, the network learns to estimate the\nleft-right consistency structural constraint on the pair of stereo input\nimages, to be able to replicate it at test time from one single image. The\nmethod is built upon the idea of blending two predictions: a prediction based\non disparity estimation, and a prediction based on direct minimization in\noccluded regions. The network is also able to identify these occluded areas at\ntraining and at test time by checking the pixelwise left-right consistency of\nthe produced disparity maps. At test time, the approach can thus generate a\nleft-side and a right-side view from one input image, as well as a depth map\nand a pixelwise confidence measure in the prediction. The work outperforms\nvisually and metric-wise state-of-the-art approaches on the challenging KITTI\ndataset, all while reducing by a very significant order of magnitude (5 or 10\ntimes) the required number of parameters (6.5 M).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.1161,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size\n  Sensitive 3D Clothing\n\n  While models of 3D clothing learned from real data exist, no method can\npredict clothing deformation as a function of garment size. In this paper, we\nintroduce SizerNet to predict 3D clothing conditioned on human body shape and\ngarment size parameters, and ParserNet to infer garment meshes and shape under\nclothing with personal details in a single pass from an input mesh. SizerNet\nallows to estimate and visualize the dressing effect of a garment in various\nsizes, and ParserNet allows to edit clothing of an input mesh directly,\nremoving the need for scan segmentation, which is a challenging problem in\nitself. To learn these models, we introduce the SIZER dataset of clothing size\nvariation which includes $100$ different subjects wearing casual clothing items\nin various sizes, totaling to approximately 2000 scans. This dataset includes\nthe scans, registrations to the SMPL model, scans segmented in clothing parts,\ngarment category and size labels. Our experiments show better parsing accuracy\nand size prediction than baseline methods trained on SIZER. The code, model and\ndataset will be released for research purposes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00096,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000077817,
      "text":"KAPLAN: A 3D Point Descriptor for Shape Completion\n\n  We present a novel 3D shape completion method that operates directly on\nunstructured point clouds, thus avoiding resource-intensive data structures\nlike voxel grids. To this end, we introduce KAPLAN, a 3D point descriptor that\naggregates local shape information via a series of 2D convolutions. The key\nidea is to project the points in a local neighborhood onto multiple planes with\ndifferent orientations. In each of those planes, point properties like normals\nor point-to-plane distances are aggregated into a 2D grid and abstracted into a\nfeature representation with an efficient 2D convolutional encoder. Since all\nplanes are encoded jointly, the resulting representation nevertheless can\ncapture their correlations and retains knowledge about the underlying 3D shape,\nwithout expensive 3D convolutions. Experiments on public datasets show that\nKAPLAN achieves state-of-the-art performance for 3D shape completion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09919,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000001457,
      "text":"Robust Tracking against Adversarial Attacks\n\n  While deep convolutional neural networks (CNNs) are vulnerable to adversarial\nattacks, considerably few efforts have been paid to construct robust deep\ntracking algorithms against adversarial attacks. Current studies on adversarial\nattack and defense mainly reside in a single image. In this work, we first\nattempt to generate adversarial examples on top of video sequences to improve\nthe tracking robustness against adversarial attacks. To this end, we take\ntemporal motion into consideration when generating lightweight perturbations\nover the estimated tracking results frame-by-frame. On one hand, we add the\ntemporal perturbations into the original video sequences as adversarial\nexamples to greatly degrade the tracking performance. On the other hand, we\nsequentially estimate the perturbations from input sequences and learn to\neliminate their effect for performance restoration. We apply the proposed\nadversarial attack and defense approaches to state-of-the-art deep tracking\nalgorithms. Extensive evaluations on the benchmark datasets demonstrate that\nour defense method not only eliminates the large performance drops caused by\nadversarial attacks, but also achieves additional performance gains when deep\ntrackers are not under adversarial attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.04543,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000103977,
      "text":"Blur Invariant Kernel-Adaptive Network for Single Image Blind deblurring\n\n  We present a novel, blind, single image deblurring method that utilizes\ninformation regarding blur kernels. Our model solves the deblurring problem by\ndividing it into two successive tasks: (1) blur kernel estimation and (2) sharp\nimage restoration. We first introduce a kernel estimation network that produces\nadaptive blur kernels based on the analysis of the blurred image. The network\nlearns the blur pattern of the input image and trains to generate the\nestimation of image-specific blur kernels. Subsequently, we propose a\ndeblurring network that restores sharp images using the estimated blur kernel.\nTo use the kernel efficiently, we propose a kernel-adaptive AE block that\nencodes features from both blurred images and blur kernels into a low\ndimensional space and then decodes them simultaneously to obtain an\nappropriately synthesized feature representation. We evaluate our model on\nREDS, GOPRO and Flickr2K datasets using various Gaussian blur kernels.\nExperiments show that our model can achieve state-of-the-art results on each\ndataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.09271,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"OnlineAugment: Online Data Augmentation with Less Domain Knowledge\n\n  Data augmentation is one of the most important tools in training modern deep\nneural networks. Recently, great advances have been made in searching for\noptimal augmentation policies in the image classification domain. However, two\nkey points related to data augmentation remain uncovered by the current\nmethods. First is that most if not all modern augmentation search methods are\noffline and learning policies are isolated from their usage. The learned\npolicies are mostly constant throughout the training process and are not\nadapted to the current training model state. Second, the policies rely on\nclass-preserving image processing functions. Hence applying current offline\nmethods to new tasks may require domain knowledge to specify such kind of\noperations. In this work, we offer an orthogonal online data augmentation\nscheme together with three new augmentation networks, co-trained with the\ntarget learning task. It is both more efficient, in the sense that it does not\nrequire expensive offline training when entering a new domain, and more\nadaptive as it adapts to the learner state. Our augmentation networks require\nless domain knowledge and are easily applicable to new tasks. Extensive\nexperiments demonstrate that the proposed scheme alone performs on par with the\nstate-of-the-art offline data augmentation methods, as well as improving upon\nthe state-of-the-art in combination with those methods. Code is available at\nhttps:\/\/github.com\/zhiqiangdon\/online-augment .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.08897,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Superpixel-Guided Label Softening for Medical Image Segmentation\n\n  Segmentation of objects of interest is one of the central tasks in medical\nimage analysis, which is indispensable for quantitative analysis. When\ndeveloping machine-learning based methods for automated segmentation, manual\nannotations are usually used as the ground truth toward which the models learn\nto mimic. While the bulky parts of the segmentation targets are relatively easy\nto label, the peripheral areas are often difficult to handle due to ambiguous\nboundaries and the partial volume effect, etc., and are likely to be labeled\nwith uncertainty. This uncertainty in labeling may, in turn, result in\nunsatisfactory performance of the trained models. In this paper, we propose\nsuperpixel-based label softening to tackle the above issue. Generated by\nunsupervised over-segmentation, each superpixel is expected to represent a\nlocally homogeneous area. If a superpixel intersects with the annotation\nboundary, we consider a high probability of uncertain labeling within this\narea. Driven by this intuition, we soften labels in this area based on signed\ndistances to the annotation boundary and assign probability values within [0,\n1] to them, in comparison with the original \"hard\", binary labels of either 0\nor 1. The softened labels are then used to train the segmentation models\ntogether with the hard labels. Experimental results on a brain MRI dataset and\nan optical coherence tomography dataset demonstrate that this conceptually\nsimple and implementation-wise easy method achieves overall superior\nsegmentation performances to baseline and comparison methods for both 3D and 2D\nmedical images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human\n  Pose Estimation\n\n  Heatmap representations have formed the basis of human pose estimation\nsystems for many years, and their extension to 3D has been a fruitful line of\nrecent research. This includes 2.5D volumetric heatmaps, whose X and Y axes\ncorrespond to image space and Z to metric depth around the subject. To obtain\nmetric-scale predictions, 2.5D methods need a separate post-processing step to\nresolve scale ambiguity. Further, they cannot localize body joints outside the\nimage boundaries, leading to incomplete estimates for truncated images. To\naddress these limitations, we propose metric-scale truncation-robust (MeTRo)\nvolumetric heatmaps, whose dimensions are all defined in metric 3D space,\ninstead of being aligned with image space. This reinterpretation of heatmap\ndimensions allows us to directly estimate complete, metric-scale poses without\ntest-time knowledge of distance or relying on anthropometric heuristics, such\nas bone lengths. To further demonstrate the utility our representation, we\npresent a differentiable combination of our 3D metric-scale heatmaps with 2D\nimage-space ones to estimate absolute 3D pose (our MeTRAbs architecture). We\nfind that supervision via absolute pose loss is crucial for accurate\nnon-root-relative localization. Using a ResNet-50 backbone without further\nlearned layers, we obtain state-of-the-art results on Human3.6M, MPI-INF-3DHP\nand MuPoTS-3D. Our code will be made publicly available to facilitate further\nresearch.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.07875,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000059936,
      "text":"Adaptive L2 Regularization in Person Re-Identification\n\n  We introduce an adaptive L2 regularization mechanism in the setting of person\nre-identification. In the literature, it is common practice to utilize\nhand-picked regularization factors which remain constant throughout the\ntraining procedure. Unlike existing approaches, the regularization factors in\nour proposed method are updated adaptively through backpropagation. This is\nachieved by incorporating trainable scalar variables as the regularization\nfactors, which are further fed into a scaled hard sigmoid function. Extensive\nexperiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the\neffectiveness of our framework. Most notably, we obtain state-of-the-art\nperformance on MSMT17, which is the largest dataset for person\nre-identification. Source code is publicly available at\nhttps:\/\/github.com\/nixingyang\/AdaptiveL2Regularization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.15861,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Saliency-driven Class Impressions for Feature Visualization of Deep\n  Neural Networks\n\n  In this paper, we propose a data-free method of extracting Impressions of\neach class from the classifier's memory. The Deep Learning regime empowers\nclassifiers to extract distinct patterns (or features) of a given class from\ntraining data, which is the basis on which they generalize to unseen data.\nBefore deploying these models on critical applications, it is advantageous to\nvisualize the features considered to be essential for classification. Existing\nvisualization methods develop high confidence images consisting of both\nbackground and foreground features. This makes it hard to judge what the\ncrucial features of a given class are. In this work, we propose a\nsaliency-driven approach to visualize discriminative features that are\nconsidered most important for a given task. Another drawback of existing\nmethods is that confidence of the generated visualizations is increased by\ncreating multiple instances of the given class. We restrict the algorithm to\ndevelop a single object per image, which helps further in extracting features\nof high confidence and also results in better visualizations. We further\ndemonstrate the generation of negative images as naturally fused images of two\nor more classes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.03221,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000170867,
      "text":"AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses\n\n  Facial landmark localization aims to detect the predefined points of human\nfaces, and the topic has been rapidly improved with the recent development of\nneural network based methods. However, it remains a challenging task when\ndealing with faces in unconstrained scenarios, especially with large pose\nvariations. In this paper, we target the problem of facial landmark\nlocalization across large poses and address this task based on a\nsplit-and-aggregate strategy. To split the search space, we propose a set of\nanchor templates as references for regression, which well addresses the large\nvariations of face poses. Based on the prediction of each anchor template, we\npropose to aggregate the results, which can reduce the landmark uncertainty due\nto the large poses. Overall, our proposed approach, named AnchorFace, obtains\nstate-of-the-art results with extremely efficient inference speed on four\nchallenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be\navailable at https:\/\/github.com\/nothingelse92\/AnchorFace.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.05481,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame\n  Optical Flow Estimation\n\n  We present a new lightweight CNN-based algorithm for multi-frame optical flow\nestimation. Our solution introduces a double recurrence over spatial scale and\ntime through repeated use of a generic \"STaR\" (SpatioTemporal Recurrent) cell.\nIt includes (i) a temporal recurrence based on conveying learned features\nrather than optical flow estimates; (ii) an occlusion detection process which\nis coupled with optical flow estimation and therefore uses a very limited\nnumber of extra parameters. The resulting STaRFlow algorithm gives\nstate-of-the-art performances on MPI Sintel and Kitti2015 and involves\nsignificantly less parameters than all other methods with comparable results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.03169,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Spatial Semantic Embedding Network: Fast 3D Instance Segmentation with\n  Deep Metric Learning\n\n  We propose spatial semantic embedding network (SSEN), a simple, yet efficient\nalgorithm for 3D instance segmentation using deep metric learning. The raw 3D\nreconstruction of an indoor environment suffers from occlusions, noise, and is\nproduced without any meaningful distinction between individual entities. For\nhigh-level intelligent tasks from a large scale scene, 3D instance segmentation\nrecognizes individual instances of objects. We approach the instance\nsegmentation by simply learning the correct embedding space that maps\nindividual instances of objects into distinct clusters that reflect both\nspatial and semantic information. Unlike previous approaches that require\ncomplex pre-processing or post-processing, our implementation is compact and\nfast with competitive performance, maintaining scalability on large scenes with\nhigh resolution voxels. We demonstrate the state-of-the-art performance of our\nalgorithm in the ScanNet 3D instance segmentation benchmark on AP score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2007.03858,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000065565,
      "text":"PaMIR: Parametric Model-Conditioned Implicit Representation for\n  Image-based Human Reconstruction\n\n  Modeling 3D humans accurately and robustly from a single image is very\nchallenging, and the key for such an ill-posed problem is the 3D representation\nof the human models. To overcome the limitations of regular 3D representations,\nwe propose Parametric Model-Conditioned Implicit Representation (PaMIR), which\ncombines the parametric body model with the free-form deep implicit function.\nIn our PaMIR-based reconstruction framework, a novel deep neural network is\nproposed to regularize the free-form deep implicit function using the semantic\nfeatures of the parametric model, which improves the generalization ability\nunder the scenarios of challenging poses and various clothing topologies.\nMoreover, a novel depth-ambiguity-aware training loss is further integrated to\nresolve depth ambiguities and enable successful surface detail reconstruction\nwith imperfect body reference. Finally, we propose a body reference\noptimization method to improve the parametric model estimation accuracy and to\nenhance the consistency between the parametric model and the implicit function.\nWith the PaMIR representation, our framework can be easily extended to\nmulti-image input scenarios without the need of multi-camera calibration and\npose synchronization. Experimental results demonstrate that our method achieves\nstate-of-the-art performance for image-based 3D human reconstruction in the\ncases of challenging poses and clothing types.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12197,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000489089,
      "text":"Instance Adaptive Self-Training for Unsupervised Domain Adaptation\n\n  The divergence between labeled training data and unlabeled testing data is a\nsignificant challenge for recent deep learning models. Unsupervised domain\nadaptation (UDA) attempts to solve such a problem. Recent works show that\nself-training is a powerful approach to UDA. However, existing methods have\ndifficulty in balancing scalability and performance. In this paper, we propose\nan instance adaptive self-training framework for UDA on the task of semantic\nsegmentation. To effectively improve the quality of pseudo-labels, we develop a\nnovel pseudo-label generation strategy with an instance adaptive selector.\nBesides, we propose the region-guided regularization to smooth the pseudo-label\nregion and sharpen the non-pseudo-label region. Our method is so concise and\nefficient that it is easy to be generalized to other unsupervised domain\nadaptation methods. Experiments on 'GTA5 to Cityscapes' and 'SYNTHIA to\nCityscapes' demonstrate the superior performance of our approach compared with\nthe state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.06266,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Optimized Deep Encoder-Decoder Methods for Crack Segmentation\n\n  Surface crack segmentation poses a challenging computer vision task as\nbackground, shape, colour and size of cracks vary. In this work we propose\noptimized deep encoder-decoder methods consisting of a combination of\ntechniques which yield an increase in crack segmentation performance.\nSpecifically we propose a decoder-part for an encoder-decoder based deep\nlearning architecture for semantic segmentation and study its components to\nachieve increased performance. We also examine the use of different encoder\nstrategies and introduce a data augmentation policy to increase the amount of\navailable training data. The performance evaluation of our method is carried\nout on four publicly available crack segmentation datasets. Additionally, we\nintroduce two techniques into the field of surface crack segmentation,\npreviously not used there: Generating results using test-time-augmentation and\nperforming a statistical result analysis over multiple training runs. The\nformer approach generally yields increased performance results, whereas the\nlatter allows for more reproducible and better representability of a methods\nresults. Using those aforementioned strategies with our proposed\nencoder-decoder architecture we are able to achieve new state of the art\nresults in all datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.05721,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000052982,
      "text":"Learning Temporally Invariant and Localizable Features via Data\n  Augmentation for Video Recognition\n\n  Deep-Learning-based video recognition has shown promising improvements along\nwith the development of large-scale datasets and spatiotemporal network\narchitectures. In image recognition, learning spatially invariant features is a\nkey factor in improving recognition performance and robustness. Data\naugmentation based on visual inductive priors, such as cropping, flipping,\nrotating, or photometric jittering, is a representative approach to achieve\nthese features. Recent state-of-the-art recognition solutions have relied on\nmodern data augmentation strategies that exploit a mixture of augmentation\noperations. In this study, we extend these strategies to the temporal dimension\nfor videos to learn temporally invariant or temporally localizable features to\ncover temporal perturbations or complex actions in videos. Based on our novel\ntemporal data augmentation algorithms, video recognition performances are\nimproved using only a limited amount of training data compared to the\nspatial-only data augmentation algorithms, including the 1st Visual Inductive\nPriors (VIPriors) for data-efficient action recognition challenge. Furthermore,\nlearned features are temporally localizable that cannot be achieved using\nspatial augmentation algorithms. Our source code is available at\nhttps:\/\/github.com\/taeoh-kim\/temporal_data_augmentation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.08261,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000085433,
      "text":"Learning Connectivity of Neural Networks from a Topological Perspective\n\n  Seeking effective neural networks is a critical and practical field in deep\nlearning. Besides designing the depth, type of convolution, normalization, and\nnonlinearities, the topological connectivity of neural networks is also\nimportant. Previous principles of rule-based modular design simplify the\ndifficulty of building an effective architecture, but constrain the possible\ntopologies in limited spaces. In this paper, we attempt to optimize the\nconnectivity in neural networks. We propose a topological perspective to\nrepresent a network into a complete graph for analysis, where nodes carry out\naggregation and transformation of features, and edges determine the flow of\ninformation. By assigning learnable parameters to the edges which reflect the\nmagnitude of connections, the learning process can be performed in a\ndifferentiable manner. We further attach auxiliary sparsity constraint to the\ndistribution of connectedness, which promotes the learned topology focus on\ncritical connections. This learning process is compatible with existing\nnetworks and owns adaptability to larger search spaces and different tasks.\nQuantitative results of experiments reflect the learned connectivity is\nsuperior to traditional rule-based ones, such as random, residual, and\ncomplete. In addition, it obtains significant improvements in image\nclassification and object detection without introducing excessive computation\nburden.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.01574,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Revisiting Robust Model Fitting Using Truncated Loss\n\n  Robust fitting is a fundamental problem in low-level vision, which is\ntypically achieved by maximum consensus (MC) estimators to identify inliers\nfirst or by M-estimators directly. While these two methods are discriminately\npreferred in different applications, truncated loss based M-estimators are\nsimilar to MC as they can also identify inliers. This work revisits a\nformulation that achieves simultaneous inlier identification and model\nestimation (SIME) using truncated loss. It has a generalized form adapts to\nboth linear and nonlinear residual models. We show that as SIME takes fitting\nresidual into account in finding inliers, its lowest achievable residual in\nmodel fitting is lower than that of MC robust fitting. Then, an alternating\nminimization (AM) algorithm is employed to solve the SIME formulation.\nMeanwhile, a semidefinite relaxation (SDR) embedded AM algorithm is developed\nin order to ease the high nonconvexity of the SIME formulation. Furthermore,\nthe new algorithms are applied to various 2D\/3D registration problems.\nExperimental results show that the new algorithms significantly outperform\nRANSAC and deterministic approximate MC methods at high outlier ratios.\nBesides, in rotation and Euclidean registration problems, the new algorithms\nalso compare favorably with state-of-the-art registration methods, especially\nin high noise and outliers. Code is available at\n\\textit{https:\/\/github.com\/FWen\/mcme.git}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.09234,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Learning to Abstract and Predict Human Actions\n\n  Human activities are naturally structured as hierarchies unrolled over time.\nFor action prediction, temporal relations in event sequences are widely\nexploited by current methods while their semantic coherence across different\nlevels of abstraction has not been well explored. In this work we model the\nhierarchical structure of human activities in videos and demonstrate the power\nof such structure in action prediction. We propose Hierarchical\nEncoder-Refresher-Anticipator, a multi-level neural machine that can learn the\nstructure of human activities by observing a partial hierarchy of events and\nroll-out such structure into a future prediction in multiple levels of\nabstraction. We also introduce a new coarse-to-fine action annotation on the\nBreakfast Actions videos to create a comprehensive, consistent, and cleanly\nstructured video hierarchical activity dataset. Through our experiments, we\nexamine and rethink the settings and metrics of activity prediction tasks\ntoward unbiased evaluation of prediction systems, and demonstrate the role of\nhierarchical modeling toward reliable and detailed long-term action\nforecasting.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.02986,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000020199,
      "text":"Global Context Aware Convolutions for 3D Point Cloud Understanding\n\n  Recent advances in deep learning for 3D point clouds have shown great\npromises in scene understanding tasks thanks to the introduction of convolution\noperators to consume 3D point clouds directly in a neural network. Point cloud\ndata, however, could have arbitrary rotations, especially those acquired from\n3D scanning. Recent works show that it is possible to design point cloud\nconvolutions with rotation invariance property, but such methods generally do\nnot perform as well as translation-invariant only convolution. We found that a\nkey reason is that compared to point coordinates, rotation-invariant features\nconsumed by point cloud convolution are not as distinctive. To address this\nproblem, we propose a novel convolution operator that enhances feature\ndistinction by integrating global context information from the input point\ncloud to the convolution. To this end, a globally weighted local reference\nframe is constructed in each point neighborhood in which the local point set is\ndecomposed into bins. Anchor points are generated in each bin to represent\nglobal shape features. A convolution can then be performed to transform the\npoints and anchor features into final rotation-invariant features. We conduct\nseveral experiments on point cloud classification, part segmentation, shape\nretrieval, and normals estimation to evaluate our convolution, which achieves\nstate-of-the-art accuracy under challenging rotations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11201,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Deep Active Learning in Remote Sensing for data efficient Change\n  Detection\n\n  We investigate active learning in the context of deep neural network models\nfor change detection and map updating. Active learning is a natural choice for\na number of remote sensing tasks, including the detection of local surface\nchanges: changes are on the one hand rare and on the other hand their\nappearance is varied and diffuse, making it hard to collect a representative\ntraining set in advance. In the active learning setting, one starts from a\nminimal set of training examples and progressively chooses informative samples\nthat are annotated by a user and added to the training set. Hence, a core\ncomponent of an active learning system is a mechanism to estimate model\nuncertainty, which is then used to pick uncertain, informative samples. We\nstudy different mechanisms to capture and quantify this uncertainty when\nworking with deep networks, based on the variance or entropy across explicit or\nimplicit model ensembles. We show that active learning successfully finds\nhighly informative samples and automatically balances the training\ndistribution, and reaches the same performance as a model supervised with a\nlarge, pre-annotated training set, with $\\approx$99% fewer annotated samples.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.0688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000012583,
      "text":"Poet: Product-oriented Video Captioner for E-commerce\n\n  In e-commerce, a growing number of user-generated videos are used for product\npromotion. How to generate video descriptions that narrate the user-preferred\nproduct characteristics depicted in the video is vital for successful\npromoting. Traditional video captioning methods, which focus on routinely\ndescribing what exists and happens in a video, are not amenable for\nproduct-oriented video captioning. To address this problem, we propose a\nproduct-oriented video captioner framework, abbreviated as Poet. Poet firstly\nrepresents the videos as product-oriented spatial-temporal graphs. Then, based\non the aspects of the video-associated product, we perform knowledge-enhanced\nspatial-temporal inference on those graphs for capturing the dynamic change of\nfine-grained product-part characteristics. The knowledge leveraging module in\nPoet differs from the traditional design by performing knowledge filtering and\ndynamic memory modeling. We show that Poet achieves consistent performance\nimprovement over previous methods concerning generation quality, product\naspects capturing, and lexical diversity. Experiments are performed on two\nproduct-oriented video captioning datasets, buyer-generated fashion video\ndataset (BFVD) and fan-generated fashion video dataset (FFVD), collected from\nMobile Taobao. We will release the desensitized datasets to promote further\ninvestigations on both video captioning and general video analysis problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11603,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000212921,
      "text":"An End-to-End Attack on Text-based CAPTCHAs Based on Cycle-Consistent\n  Generative Adversarial Network\n\n  As a widely deployed security scheme, text-based CAPTCHAs have become more\nand more difficult to resist machine learning-based attacks. So far, many\nresearchers have conducted attacking research on text-based CAPTCHAs deployed\nby different companies (such as Microsoft, Amazon, and Apple) and achieved\ncertain results.However, most of these attacks have some shortcomings, such as\npoor portability of attack methods, requiring a series of data preprocessing\nsteps, and relying on large amounts of labeled CAPTCHAs. In this paper, we\npropose an efficient and simple end-to-end attack method based on\ncycle-consistent generative adversarial networks. Compared with previous\nstudies, our method greatly reduces the cost of data labeling. In addition,\nthis method has high portability. It can attack common text-based CAPTCHA\nschemes only by modifying a few configuration parameters, which makes the\nattack easier. Firstly, we train CAPTCHA synthesizers based on the cycle-GAN to\ngenerate some fake samples. Basic recognizers based on the convolutional\nrecurrent neural network are trained with the fake data. Subsequently, an\nactive transfer learning method is employed to optimize the basic recognizer\nutilizing tiny amounts of labeled real-world CAPTCHA samples. Our approach\nefficiently cracked the CAPTCHA schemes deployed by 10 popular websites,\nindicating that our attack is likely very general. Additionally, we analyzed\nthe current most popular anti-recognition mechanisms. The results show that the\ncombination of more anti-recognition mechanisms can improve the security of\nCAPTCHA, but the improvement is limited. Conversely, generating more complex\nCAPTCHAs may cost more resources and reduce the availability of CAPTCHAs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.01338,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"Hierarchical Context Embedding for Region-based Object Detection\n\n  State-of-the-art two-stage object detectors apply a classifier to a sparse\nset of object proposals, relying on region-wise features extracted by RoIPool\nor RoIAlign as inputs. The region-wise features, in spite of aligning well with\nthe proposal locations, may still lack the crucial context information which is\nnecessary for filtering out noisy background detections, as well as recognizing\nobjects possessing no distinctive appearances. To address this issue, we\npresent a simple but effective Hierarchical Context Embedding (HCE) framework,\nwhich can be applied as a plug-and-play component, to facilitate the\nclassification ability of a series of region-based detectors by mining\ncontextual cues. Specifically, to advance the recognition of context-dependent\nobject categories, we propose an image-level categorical embedding module which\nleverages the holistic image-level context to learn object-level concepts.\nThen, novel RoI features are generated by exploiting hierarchically embedded\ncontext information beneath both whole images and interested regions, which are\nalso complementary to conventional RoI features. Moreover, to make full use of\nour hierarchical contextual RoI features, we propose the early-and-late fusion\nstrategies (i.e., feature fusion and confidence fusion), which can be combined\nto boost the classification accuracy of region-based detectors. Comprehensive\nexperiments demonstrate that our HCE framework is flexible and generalizable,\nleading to significant and consistent improvements upon various region-based\ndetectors, including FPN, Cascade R-CNN and Mask R-CNN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.12599,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"PV-RCNN: The Top-Performing LiDAR-only Solutions for 3D Detection \/ 3D\n  Tracking \/ Domain Adaptation of Waymo Open Dataset Challenges\n\n  In this technical report, we present the top-performing LiDAR-only solutions\nfor 3D detection, 3D tracking and domain adaptation three tracks in Waymo Open\nDataset Challenges 2020. Our solutions for the competition are built upon our\nrecent proposed PV-RCNN 3D object detection framework. Several variants of our\nPV-RCNN are explored, including temporal information incorporation, dynamic\nvoxelization, adaptive training sample selection, classification with RoI\nfeatures, etc. A simple model ensemble strategy with non-maximum-suppression\nand box voting is adopted to generate the final results. By using only LiDAR\npoint cloud data, our models finally achieve the 1st place among all LiDAR-only\nmethods, and the 2nd place among all multi-modal methods, on the 3D Detection,\n3D Tracking and Domain Adaptation three tracks of Waymo Open Dataset\nChallenges. Our solutions will be available at\nhttps:\/\/github.com\/open-mmlab\/OpenPCDet\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.06181,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000049671,
      "text":"Apparel-invariant Feature Learning for Apparel-changed Person\n  Re-identification\n\n  With the rise of deep learning methods, person Re-Identification (ReID)\nperformance has been improved tremendously in many public datasets. However,\nmost public ReID datasets are collected in a short time window in which\npersons' appearance rarely changes. In real-world applications such as in a\nshopping mall, the same person's clothing may change, and different persons may\nwearing similar clothes. All these cases can result in an inconsistent ReID\nperformance, revealing a critical problem that current ReID models heavily rely\non person's apparels. Therefore, it is critical to learn an apparel-invariant\nperson representation under cases like cloth changing or several persons\nwearing similar clothes. In this work, we tackle this problem from the\nviewpoint of invariant feature representation learning. The main contributions\nof this work are as follows. (1) We propose the semi-supervised\nApparel-invariant Feature Learning (AIFL) framework to learn an\napparel-invariant pedestrian representation using images of the same person\nwearing different clothes. (2) To obtain images of the same person wearing\ndifferent clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN)\nto synthesize cloth changing images according to the target cloth embedding.\nIt's worth noting that the images used in ReID tasks were cropped from\nreal-world low-quality CCTV videos, making it more challenging to synthesize\ncloth changing images. We conduct extensive experiments on several datasets\ncomparing with several baselines. Experimental results demonstrate that our\nproposal can improve the ReID performance of the baseline models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00878,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Fusion of Deep and Non-Deep Methods for Fast Super-Resolution of\n  Satellite Images\n\n  In the emerging commercial space industry there is a drastic increase in\naccess to low cost satellite imagery. The price for satellite images depends on\nthe sensor quality and revisit rate. This work proposes to bridge the gap\nbetween image quality and the price by improving the image quality via\nsuper-resolution (SR). Recently, a number of deep SR techniques have been\nproposed to enhance satellite images. However, none of these methods utilize\nthe region-level context information, giving equal importance to each region in\nthe image. This, along with the fact that most state-of-the-art SR methods are\ncomplex and cumbersome deep models, the time taken to process very large\nsatellite images can be impractically high. We, propose to handle this\nchallenge by designing an SR framework that analyzes the regional information\ncontent on each patch of the low-resolution image and judiciously chooses to\nuse more computationally complex deep models to super-resolve more\nstructure-rich regions on the image, while using less resource-intensive\nnon-deep methods on non-salient regions. Through extensive experiments on a\nlarge satellite image, we show substantial decrease in inference time while\nachieving similar performance to that of existing deep SR methods over several\nevaluation measures like PSNR, MSE and SSIM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.00261,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000257625,
      "text":"Distilling Visual Priors from Self-Supervised Learning\n\n  Convolutional Neural Networks (CNNs) are prone to overfit small training\ndatasets. We present a novel two-phase pipeline that leverages self-supervised\nlearning and knowledge distillation to improve the generalization ability of\nCNN models for image classification under the data-deficient setting. The first\nphase is to learn a teacher model which possesses rich and generalizable visual\nrepresentations via self-supervised learning, and the second phase is to\ndistill the representations into a student model in a self-distillation manner,\nand meanwhile fine-tune the student model for the image classification task. We\nalso propose a novel margin loss for the self-supervised contrastive learning\nproxy task to better learn the representation under the data-deficient\nscenario. Together with other tricks, we achieve competitive performance in the\nVIPriors image classification challenge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.11055,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"On estimating gaze by self-attention augmented convolutions\n\n  Estimation of 3D gaze is highly relevant to multiple fields, including but\nnot limited to interactive systems, specialized human-computer interfaces, and\nbehavioral research. Although recently deep learning methods have boosted the\naccuracy of appearance-based gaze estimation, there is still room for\nimprovement in the network architectures for this particular task. Therefore we\npropose here a novel network architecture grounded on self-attention augmented\nconvolutions to improve the quality of the learned features during the training\nof a shallower residual network. The rationale is that self-attention mechanism\ncan help outperform deeper architectures by learning dependencies between\ndistant regions in full-face images. This mechanism can also create better and\nmore spatially-aware feature representations derived from the face and eye\nimages before gaze regression. We dubbed our framework ARes-gaze, which\nexplores our Attention-augmented ResNet (ARes-14) as twin convolutional\nbackbones. In our experiments, results showed a decrease of the average angular\nerror by 2.38% when compared to state-of-the-art methods on the MPIIFaceGaze\ndata set, and a second-place on the EyeDiap data set. It is noteworthy that our\nproposed framework was the only one to reach high accuracy simultaneously on\nboth data sets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.05511,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Free View Synthesis\n\n  We present a method for novel view synthesis from input images that are\nfreely distributed around a scene. Our method does not rely on a regular\narrangement of input views, can synthesize images for free camera movement\nthrough the scene, and works for general scenes with unconstrained geometric\nlayouts. We calibrate the input images via SfM and erect a coarse geometric\nscaffold via MVS. This scaffold is used to create a proxy depth map for a novel\nview of the scene. Based on this depth map, a recurrent encoder-decoder network\nprocesses reprojected features from nearby views and synthesizes the new view.\nOur network does not need to be optimized for a given scene. After training on\na dataset, it works in previously unseen environments with no fine-tuning or\nper-scene optimization. We evaluate the presented approach on challenging\nreal-world datasets, including Tanks and Temples, where we demonstrate\nsuccessful view synthesis for the first time and substantially outperform prior\nand concurrent work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.01701,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Progressive Update Guided Interdependent Networks for Single Image\n  Dehazing\n\n  Images with haze of different varieties often pose a significant challenge to\ndehazing. Therefore, guidance by estimates of haze parameters related to the\nvariety would be beneficial, and their progressive update jointly with haze\nreduction will allow effective dehazing. To this end, we propose a\nmulti-network dehazing framework containing novel interdependent dehazing and\nhaze parameter updater networks that operate in a progressive manner. The haze\nparameters, transmission map and atmospheric light, are first estimated using\ndedicated convolutional networks that allow color-cast handling. The estimated\nparameters are then used to guide our dehazing module, where the estimates are\nprogressively updated by novel convolutional networks. The updating takes place\njointly with progressive dehazing using a network that invokes inter-step\ndependencies. The joint progressive updating and dehazing gradually modify the\nhaze parameter values toward achieving effective dehazing. Through different\nstudies, our dehazing framework is shown to be more effective than\nimage-to-image mapping and predefined haze formation model based dehazing. The\nframework is also found capable of handling a wide variety of hazy conditions\nwtih different types and amounts of haze and color casts. Our dehazing\nframework is qualitatively and quantitatively found to outperform the\nstate-of-the-art on synthetic and real-world hazy images of multiple datasets\nwith varied haze conditions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.04378,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"Unsupervised Deep Metric Learning with Transformed Attention Consistency\n  and Contrastive Clustering Loss\n\n  Existing approaches for unsupervised metric learning focus on exploring\nself-supervision information within the input image itself. We observe that,\nwhen analyzing images, human eyes often compare images against each other\ninstead of examining images individually. In addition, they often pay attention\nto certain keypoints, image regions, or objects which are discriminative\nbetween image classes but highly consistent within classes. Even if the image\nis being transformed, the attention pattern will be consistent. Motivated by\nthis observation, we develop a new approach to unsupervised deep metric\nlearning where the network is learned based on self-supervision information\nacross images instead of within one single image. To characterize the\nconsistent pattern of human attention during image comparisons, we introduce\nthe idea of transformed attention consistency. It assumes that visually similar\nimages, even undergoing different image transforms, should share the same\nconsistent visual attention map. This consistency leads to a pairwise\nself-supervision loss, allowing us to learn a Siamese deep neural network to\nencode and compare images against their transformed or matched pairs. To\nfurther enhance the inter-class discriminative power of the feature generated\nby this network, we adapt the concept of triplet loss from supervised metric\nlearning to our unsupervised case and introduce the contrastive clustering\nloss. Our extensive experimental results on benchmark datasets demonstrate that\nour proposed method outperforms current state-of-the-art methods for\nunsupervised metric learning by a large margin.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2008.03713,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000047684,
      "text":"I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human\n  Pose and Mesh Estimation from a Single RGB Image\n\n  Most of the previous image-based 3D human pose and mesh estimation methods\nestimate parameters of the human mesh model from an input image. However,\ndirectly regressing the parameters from the input image is a highly non-linear\nmapping because it breaks the spatial relationship between pixels in the input\nimage. In addition, it cannot model the prediction uncertainty, which can make\ntraining harder. To resolve the above issues, we propose I2L-MeshNet, an\nimage-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet\npredicts the per-lixel likelihood on 1D heatmaps for each mesh vertex\ncoordinate instead of directly regressing the parameters. Our lixel-based 1D\nheatmap preserves the spatial relationship in the input image and models the\nprediction uncertainty. We demonstrate the benefit of the image-to-lixel\nprediction and show that the proposed I2L-MeshNet outperforms previous methods.\nThe code is publicly available https:\/\/github.com\/mks0601\/I2L-MeshNet_RELEASE.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.11528,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000052651,
      "text":"MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object\n  Detection\n\n  Modern object detection methods can be divided into one-stage approaches and\ntwo-stage ones. One-stage detectors are more efficient owing to straightforward\narchitectures, but the two-stage detectors still take the lead in accuracy.\nAlthough recent work try to improve the one-stage detectors by imitating the\nstructural design of the two-stage ones, the accuracy gap is still significant.\nIn this paper, we propose MimicDet, a novel and efficient framework to train a\none-stage detector by directly mimic the two-stage features, aiming to bridge\nthe accuracy gap between one-stage and two-stage detectors. Unlike conventional\nmimic methods, MimicDet has a shared backbone for one-stage and two-stage\ndetectors, then it branches into two heads which are well designed to have\ncompatible features for mimicking. Thus MimicDet can be end-to-end trained\nwithout the pre-train of the teacher network. And the cost does not increase\nmuch, which makes it practical to adopt large networks as backbones. We also\nmake several specialized designs such as dual-path mimicking and staggered\nfeature pyramid to facilitate the mimicking process. Experiments on the\nchallenging COCO detection benchmark demonstrate the effectiveness of MimicDet.\nIt achieves 46.1 mAP with ResNeXt-101 backbone on the COCO test-dev set, which\nsignificantly surpasses current state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.04998,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"Proposal-Free Volumetric Instance Segmentation from Latent\n  Single-Instance Masks\n\n  This work introduces a new proposal-free instance segmentation method that\nbuilds on single-instance segmentation masks predicted across the entire image\nin a sliding window style. In contrast to related approaches, our method\nconcurrently predicts all masks, one for each pixel, and thus resolves any\nconflict jointly across the entire image. Specifically, predictions from\noverlapping masks are combined into edge weights of a signed graph that is\nsubsequently partitioned to obtain all final instances concurrently. The result\nis a parameter-free method that is strongly robust to noise and prioritizes\npredictions with the highest consensus across overlapping masks. All masks are\ndecoded from a low dimensional latent representation, which results in great\nmemory savings strictly required for applications to large volumetric images.\nWe test our method on the challenging CREMI 2016 neuron segmentation benchmark\nwhere it achieves competitive scores.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.1441,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Pruning Filter in Filter\n\n  Pruning has become a very powerful and effective technique to compress and\naccelerate modern neural networks. Existing pruning methods can be grouped into\ntwo categories: filter pruning (FP) and weight pruning (WP). FP wins at\nhardware compatibility but loses at the compression ratio compared with WP. To\nconverge the strength of both methods, we propose to prune the filter in the\nfilter. Specifically, we treat a filter $F \\in \\mathbb{R}^{C\\times K\\times K}$\nas $K \\times K$ stripes, i.e., $1\\times 1$ filters $\\in \\mathbb{R}^{C}$, then\nby pruning the stripes instead of the whole filter, we can achieve finer\ngranularity than traditional FP while being hardware friendly. We term our\nmethod as SWP (\\emph{Stripe-Wise Pruning}). SWP is implemented by introducing a\nnovel learnable matrix called Filter Skeleton, whose values reflect the shape\nof each filter. As some recent work has shown that the pruned architecture is\nmore crucial than the inherited important weights, we argue that the\narchitecture of a single filter, i.e., the shape, also matters. Through\nextensive experiments, we demonstrate that SWP is more effective compared to\nthe previous FP-based methods and achieves the state-of-art pruning ratio on\nCIFAR-10 and ImageNet datasets without obvious accuracy drop. Code is available\nat https:\/\/github.com\/fxmeng\/Pruning-Filter-in-Filter\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.06205,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Joint Demosaicking and Denoising Benefits from a Two-stage Training\n  Strategy\n\n  Image demosaicking and denoising are the first two key steps of the color\nimage production pipeline. The classical processing sequence has for a long\ntime consisted of applying denoising first, and then demosaicking. Applying the\noperations in this order leads to oversmoothing and checkerboard effects. Yet,\nit was difficult to change this order, because once the image is demosaicked,\nthe statistical properties of the noise are dramatically changed and hard to\nhandle by traditional denoising models. In this paper, we address this problem\nby a hybrid machine learning method. We invert the traditional color filter\narray (CFA) processing pipeline by first demosaicking and then denoising. Our\ndemosaicking algorithm, trained on noiseless images, combines a traditional\nmethod and a residual convolutional neural network (CNN). This first stage\nretains all known information, which is the key point to obtain faithful final\nresults. The noisy demosaicked image is then passed through a second CNN\nrestoring a noiseless full-color image. This pipeline order completely avoids\ncheckerboard effects and restores fine image detail. Although CNNs can be\ntrained to solve jointly demosaicking-denoising end-to-end, we find that this\ntwo-stage training performs better and is less prone to failure. It is shown\nexperimentally to improve on the state of the art, both quantitatively and in\nterms of visual quality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.07506,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"The 1st Tiny Object Detection Challenge:Methods and Results\n\n  The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in\ndeveloping novel and accurate methods for tiny object detection in images which\nhave wide views, with a current focus on tiny person detection. The TinyPerson\ndataset was used for the TOD Challenge and is publicly released. It has 1610\nimages and 72651 box-levelannotations. Around 36 participating teams from the\nglobe competed inthe 1st TOD Challenge. In this paper, we provide a brief\nsummary of the1st TOD Challenge including brief introductions to the top three\nmethods.The submission leaderboard will be reopened for researchers that\nareinterested in the TOD challenge. The benchmark dataset and other information\ncan be found at: https:\/\/github.com\/ucas-vg\/TinyBenchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01972,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Attribute Adaptive Margin Softmax Loss using Privileged Information\n\n  We present a novel framework to exploit privileged information for\nrecognition which is provided only during the training phase. Here, we focus on\nrecognition task where images are provided as the main view and soft biometric\ntraits (attributes) are provided as the privileged data (only available during\ntraining phase). We demonstrate that more discriminative feature space can be\nlearned by enforcing a deep network to adjust adaptive margins between classes\nutilizing attributes. This tight constraint also effectively reduces the class\nimbalance inherent in the local data neighborhood, thus carving more balanced\nclass boundaries locally and using feature space more efficiently. Extensive\nexperiments are performed on five different datasets and the results show the\nsuperiority of our method compared to the state-of-the-art models in both tasks\nof face recognition and person re-identification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09724,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Conditional Automated Channel Pruning for Deep Neural Networks\n\n  Model compression aims to reduce the redundancy of deep networks to obtain\ncompact models. Recently, channel pruning has become one of the predominant\ncompression methods to deploy deep models on resource-constrained devices. Most\nchannel pruning methods often use a fixed compression rate for all the layers\nof the model, which, however, may not be optimal. To address this issue, given\na target compression rate for the whole model, one can search for the optimal\ncompression rate for each layer. Nevertheless, these methods perform channel\npruning for a specific target compression rate. When we consider multiple\ncompression rates, they have to repeat the channel pruning process multiple\ntimes, which is very inefficient yet unnecessary. To address this issue, we\npropose a Conditional Automated Channel Pruning(CACP) method to obtain the\ncompressed models with different compression rates through single channel\npruning process. To this end, we develop a conditional model that takes an\narbitrary compression rate as input and outputs the corresponding compressed\nmodel. In the experiments, the resultant models with different compression\nrates consistently outperform the models compressed by existing methods with a\nchannel pruning process for each target compression rate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0945,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000079473,
      "text":"Knowledge-Guided Multi-Label Few-Shot Learning for General Image\n  Recognition\n\n  Recognizing multiple labels of an image is a practical yet challenging task,\nand remarkable progress has been achieved by searching for semantic regions and\nexploiting label dependencies. However, current works utilize RNN\/LSTM to\nimplicitly capture sequential region\/label dependencies, which cannot fully\nexplore mutual interactions among the semantic regions\/labels and do not\nexplicitly integrate label co-occurrences. In addition, these works require\nlarge amounts of training samples for each category, and they are unable to\ngeneralize to novel categories with limited samples. To address these issues,\nwe propose a knowledge-guided graph routing (KGGR) framework, which unifies\nprior knowledge of statistical label correlations with deep neural networks.\nThe framework exploits prior knowledge to guide adaptive information\npropagation among different categories to facilitate multi-label analysis and\nreduce the dependency of training samples. Specifically, it first builds a\nstructured knowledge graph to correlate different labels based on statistical\nlabel co-occurrence. Then, it introduces the label semantics to guide learning\nsemantic-specific features to initialize the graph, and it exploits a graph\npropagation network to explore graph node interactions, enabling learning\ncontextualized image feature representations. Moreover, we initialize each\ngraph node with the classifier weights for the corresponding label and apply\nanother propagation network to transfer node messages through the graph. In\nthis way, it can facilitate exploiting the information of correlated labels to\nhelp train better classifiers. We conduct extensive experiments on the\ntraditional multi-label image recognition (MLR) and multi-label few-shot\nlearning (ML-FSL) tasks and show that our KGGR framework outperforms the\ncurrent state-of-the-art methods by sizable margins on the public benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0466,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"CAD-PU: A Curvature-Adaptive Deep Learning Solution for Point Set\n  Upsampling\n\n  Point set is arguably the most direct approximation of an object or scene\nsurface, yet its practical acquisition often suffers from the shortcoming of\nbeing noisy, sparse, and possibly incomplete, which restricts its use for a\nhigh-quality surface recovery. Point set upsampling aims to increase its\ndensity and regularity such that a better surface recovery could be achieved.\nThe problem is severely ill-posed and challenging, considering that the\nupsampling target itself is only an approximation of the underlying surface.\nMotivated to improve the surface approximation via point set upsampling, we\nidentify the factors that are critical to the objective, by pairing the surface\napproximation error bounds of the input and output point sets. It suggests that\ngiven a fixed budget of points in the upsampling result, more points should be\ndistributed onto the surface regions where local curvatures are relatively\nhigh. To implement the motivation, we propose a novel design of\nCurvature-ADaptive Point set Upsampling network (CAD-PU), the core of which is\na module of curvature-adaptive feature expansion. To train CAD-PU, we follow\nthe same motivation and propose geometrically intuitive surrogates that\napproximate discrete notions of surface curvature for the upsampled point set.\nWe further integrate the proposed surrogates into an adversarial learning based\ncurvature minimization objective, which gives a practically effective learning\nof CAD-PU. We conduct thorough experiments that show the efficacy of our\ncontributions and the advantages of our method over existing ones. Our\nimplementation codes are publicly available at\nhttps:\/\/github.com\/JiehongLin\/CAD-PU.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.13331,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000471539,
      "text":"Addressing Class Imbalance in Scene Graph Parsing by Learning to\n  Contrast and Score\n\n  Scene graph parsing aims to detect objects in an image scene and recognize\ntheir relations. Recent approaches have achieved high average scores on some\npopular benchmarks, but fail in detecting rare relations, as the highly\nlong-tailed distribution of data biases the learning towards frequent labels.\nMotivated by the fact that detecting these rare relations can be critical in\nreal-world applications, this paper introduces a novel integrated framework of\nclassification and ranking to resolve the class imbalance problem in scene\ngraph parsing. Specifically, we design a new Contrasting Cross-Entropy loss,\nwhich promotes the detection of rare relations by suppressing incorrect\nfrequent ones. Furthermore, we propose a novel scoring module, termed as\nScorer, which learns to rank the relations based on the image features and\nrelation features to improve the recall of predictions. Our framework is simple\nand effective, and can be incorporated into current scene graph models.\nExperimental results show that the proposed approach improves the current\nstate-of-the-art methods, with a clear advantage of detecting rare relations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00749,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Iris Liveness Detection Competition (LivDet-Iris) -- The 2020 Edition\n\n  Launched in 2013, LivDet-Iris is an international competition series open to\nacademia and industry with the aim to assess and report advances in iris\nPresentation Attack Detection (PAD). This paper presents results from the\nfourth competition of the series: LivDet-Iris 2020. This year's competition\nintroduced several novel elements: (a) incorporated new types of attacks\n(samples displayed on a screen, cadaver eyes and prosthetic eyes), (b)\ninitiated LivDet-Iris as an on-going effort, with a testing protocol available\nnow to everyone via the Biometrics Evaluation and Testing\n(BEAT)(https:\/\/www.idiap.ch\/software\/beat\/) open-source platform to facilitate\nreproducibility and benchmarking of new algorithms continuously, and (c)\nperformance comparison of the submitted entries with three baseline methods\n(offered by the University of Notre Dame and Michigan State University), and\nthree open-source iris PAD methods available in the public domain. The best\nperforming entry to the competition reported a weighted average APCER of\n59.10\\% and a BPCER of 0.46\\% over all five attack types. This paper serves as\nthe latest evaluation of iris PAD on a large spectrum of presentation attack\ninstruments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0714,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000558297,
      "text":"HGCN-GJS: Hierarchical Graph Convolutional Network with Groupwise Joint\n  Sampling for Trajectory Prediction\n\n  Accurate pedestrian trajectory prediction is of great importance for\ndownstream tasks such as autonomous driving and mobile robot navigation. Fully\ninvestigating the social interactions within the crowd is crucial for accurate\npedestrian trajectory prediction. However, most existing methods do not capture\ngroup level interactions well, focusing only on pairwise interactions and\nneglecting group-wise interactions. In this work, we propose a hierarchical\ngraph convolutional network, HGCN-GJS, for trajectory prediction which well\nleverages group level interactions within the crowd. Furthermore, we introduce\na novel joint sampling scheme for modeling the joint distribution of multiple\npedestrians in the future trajectories. Based on the group information, this\nscheme associates the trajectory of one person with the trajectory of other\npeople in the group, but maintains the independence of the trajectories of\noutsiders. We demonstrate the performance of our network on several trajectory\nprediction datasets, achieving state-of-the-art results on all datasets\nconsidered.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.13289,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000144376,
      "text":"Multi-scale Receptive Fields Graph Attention Network for Point Cloud\n  Classification\n\n  Understanding the implication of point cloud is still challenging to achieve\nthe goal of classification or segmentation due to the irregular and sparse\nstructure of point cloud. As we have known, PointNet architecture as a\nground-breaking work for point cloud which can learn efficiently shape features\ndirectly on unordered 3D point cloud and have achieved favorable performance.\nHowever, this model fail to consider the fine-grained semantic information of\nlocal structure for point cloud. Afterwards, many valuable works are proposed\nto enhance the performance of PointNet by means of semantic features of local\npatch for point cloud. In this paper, a multi-scale receptive fields graph\nattention network (named after MRFGAT) for point cloud classification is\nproposed. By focusing on the local fine features of point cloud and applying\nmulti attention modules based on channel affinity, the learned feature map for\nour network can well capture the abundant features information of point cloud.\nThe proposed MRFGAT architecture is tested on ModelNet10 and ModelNet40\ndatasets, and results show it achieves state-of-the-art performance in shape\nclassification tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.00782,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"A perception centred self-driving system without HD Maps\n\n  Building a fully autonomous self-driving system has been discussed for more\nthan 20 years yet remains unsolved. Previous systems have limited ability to\nscale. Their localization subsystem needs labor-intensive map recording for\nrunning in a new area, and the accuracy decreases after the changes occur in\nthe environment. In this paper, a new localization method is proposed to solve\nthe scalability problems, with a new method for detecting and making sense of\ndiverse traffic lines. Like the way human drives, a self-driving system should\nnot rely on an exact position to travel in most scenarios. As a result, without\nHD Maps, GPS or IMU, the proposed localization subsystem relies only on\ndetecting driving-related features around (like lane lines, stop lines, and\nmerging lane lines). For spotting and reasoning all these features, a new line\ndetector is proposed and tested against multiple datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01427,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000055631,
      "text":"Spatial Transformer Point Convolution\n\n  Point clouds are unstructured and unordered in the embedded 3D space. In\norder to produce consistent responses under different permutation layouts, most\nexisting methods aggregate local spatial points through maximum or summation\noperation. But such an aggregation essentially belongs to the isotropic\nfiltering on all operated points therein, which tends to lose the information\nof geometric structures. In this paper, we propose a spatial transformer point\nconvolution (STPC) method to achieve anisotropic convolution filtering on point\nclouds. To capture and represent implicit geometric structures, we specifically\nintroduce spatial direction dictionary to learn those latent geometric\ncomponents. To better encode unordered neighbor points, we design sparse\ndeformer to transform them into the canonical ordered dictionary space by using\ndirection dictionary learning. In the transformed space, the standard\nimage-like convolution can be leveraged to generate anisotropic filtering,\nwhich is more robust to express those finer variances of local regions.\nDictionary learning and encoding processes are encapsulated into a network\nmodule and jointly learnt in an end-to-end manner. Extensive experiments on\nseveral public datasets (including S3DIS, Semantic3D, SemanticKITTI)\ndemonstrate the effectiveness of our proposed method in point clouds semantic\nsegmentation task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.13957,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000430809,
      "text":"A Prototype-Based Generalized Zero-Shot Learning Framework for Hand\n  Gesture Recognition\n\n  Hand gesture recognition plays a significant role in human-computer\ninteraction for understanding various human gestures and their intent. However,\nmost prior works can only recognize gestures of limited labeled classes and\nfail to adapt to new categories. The task of Generalized Zero-Shot Learning\n(GZSL) for hand gesture recognition aims to address the above issue by\nleveraging semantic representations and detecting both seen and unseen class\nsamples. In this paper, we propose an end-to-end prototype-based GZSL framework\nfor hand gesture recognition which consists of two branches. The first branch\nis a prototype-based detector that learns gesture representations and\ndetermines whether an input sample belongs to a seen or unseen category. The\nsecond branch is a zero-shot label predictor which takes the features of unseen\nclasses as input and outputs predictions through a learned mapping mechanism\nbetween the feature and the semantic space. We further establish a hand gesture\ndataset that specifically targets this GZSL task, and comprehensive experiments\non this dataset demonstrate the effectiveness of our proposed approach on\nrecognizing both seen and unseen gestures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.01559,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0001793438,
      "text":"1st Place Solution of LVIS Challenge 2020: A Good Box is not a Guarantee\n  of a Good Mask\n\n  This article introduces the solutions of the team lvisTraveler for LVIS\nChallenge 2020. In this work, two characteristics of LVIS dataset are mainly\nconsidered: the long-tailed distribution and high quality instance segmentation\nmask. We adopt a two-stage training pipeline. In the first stage, we\nincorporate EQL and self-training to learn generalized representation. In the\nsecond stage, we utilize Balanced GroupSoftmax to promote the classifier, and\npropose a novel proposal assignment strategy and a new balanced mask loss for\nmask head to get more precise mask predictions. Finally, we achieve 41.5 and\n41.2 AP on LVIS v1.0 val and test-dev splits respectively, outperforming the\nbaseline based on X101-FPN-MaskRCNN by a large margin.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.09238,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"EfficientDeRain: Learning Pixel-wise Dilation Filtering for\n  High-Efficiency Single-Image Deraining\n\n  Single-image deraining is rather challenging due to the unknown rain model.\nExisting methods often make specific assumptions of the rain model, which can\nhardly cover many diverse circumstances in the real world, making them have to\nemploy complex optimization or progressive refinement. This, however,\nsignificantly affects these methods' efficiency and effectiveness for many\nefficiency-critical applications. To fill this gap, in this paper, we regard\nthe single-image deraining as a general image-enhancing problem and originally\npropose a model-free deraining method, i.e., EfficientDeRain, which is able to\nprocess a rainy image within 10~ms (i.e., around 6~ms on average), over 80\ntimes faster than the state-of-the-art method (i.e., RCDNet), while achieving\nsimilar de-rain effects. We first propose the novel pixel-wise dilation\nfiltering. In particular, a rainy image is filtered with the pixel-wise kernels\nestimated from a kernel prediction network, by which suitable multi-scale\nkernels for each pixel can be efficiently predicted. Then, to eliminate the gap\nbetween synthetic and real data, we further propose an effective data\naugmentation method (i.e., RainMix) that helps to train network for real rainy\nimage handling.We perform comprehensive evaluation on both synthetic and\nreal-world rainy datasets to demonstrate the effectiveness and efficiency of\nour method. We release the model and code in\nhttps:\/\/github.com\/tsingqguo\/efficientderain.git.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.10292,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000007285,
      "text":"PennSyn2Real: Training Object Recognition Models without Human Labeling\n\n  Scalable training data generation is a critical problem in deep learning. We\npropose PennSyn2Real - a photo-realistic synthetic dataset consisting of more\nthan 100,000 4K images of more than 20 types of micro aerial vehicles (MAVs).\nThe dataset can be used to generate arbitrary numbers of training images for\nhigh-level computer vision tasks such as MAV detection and classification. Our\ndata generation framework bootstraps chroma-keying, a mature cinematography\ntechnique with a motion tracking system, providing artifact-free and curated\nannotated images where object orientations and lighting are controlled. This\nframework is easy to set up and can be applied to a broad range of objects,\nreducing the gap between synthetic and real-world data. We show that synthetic\ndata generated using this framework can be directly used to train CNN models\nfor common object recognition tasks such as detection and segmentation. We\ndemonstrate competitive performance in comparison with training using only real\nimages. Furthermore, bootstrapping the generated synthetic data in few-shot\nlearning can significantly improve the overall performance, reducing the number\nof required training data samples to achieve the desired accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2009.0668,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"SML: Semantic Meta-learning for Few-shot Semantic Segmentation\n\n  The significant amount of training data required for training Convolutional\nNeural Networks has become a bottleneck for applications like semantic\nsegmentation. Few-shot semantic segmentation algorithms address this problem,\nwith an aim to achieve good performance in the low-data regime, with few\nannotated training images. Recently, approaches based on class-prototypes\ncomputed from available training data have achieved immense success for this\ntask. In this work, we propose a novel meta-learning framework, Semantic\nMeta-Learning (SML) which incorporates class level semantic descriptions in the\ngenerated prototypes for this problem. In addition, we propose to use the well\nestablished technique, ridge regression, to not only bring in the class-level\nsemantic information, but also to effectively utilise the information available\nfrom multiple images present in the training data for prototype computation.\nThis has a simple closed-form solution, and thus can be implemented easily and\nefficiently. Extensive experiments on the benchmark PASCAL-5i dataset under\ndifferent experimental settings show the effectiveness of the proposed\nframework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.03743,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000066227,
      "text":"Visual News: Benchmark and Challenges in News Image Captioning\n\n  We propose Visual News Captioner, an entity-aware model for the task of news\nimage captioning. We also introduce Visual News, a large-scale benchmark\nconsisting of more than one million news images along with associated news\narticles, image captions, author information, and other metadata. Unlike the\nstandard image captioning task, news images depict situations where people,\nlocations, and events are of paramount importance. Our proposed method can\neffectively combine visual and textual features to generate captions with\nricher information such as events and entities. More specifically, built upon\nthe Transformer architecture, our model is further equipped with novel\nmulti-modal feature fusion techniques and attention mechanisms, which are\ndesigned to generate named entities more accurately. Our method utilizes much\nfewer parameters while achieving slightly better prediction results than\ncompeting methods. Our larger and more diverse Visual News dataset further\nhighlights the remaining challenges in captioning news images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.13844,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Enhancing road signs segmentation using photometric invariants\n\n  Road signs detection and recognition in natural scenes is one of the most\nimportant tasksin the design of Intelligent Transport Systems (ITS). However,\nillumination changes remain a major problem. In this paper, an efficient\nap-proach of road signs segmentation based on photometric invariants is\nproposed. This method is based on color in-formation using a hybrid distance,\nby exploiting the chro-matic distance and the red and blue ratio, on l Theta\nPhi color space which is invariant to highlight, shading and shadow changes. A\ncomparative study is performed to demonstrate the robustness of this approach\nover the most frequently used methods for road sign segmentation. The\nexperimental results and the detailed analysis show the high performance of the\nalgorithm described in this paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.16117,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"PyraPose: Feature Pyramids for Fast and Accurate Object Pose Estimation\n  under Domain Shift\n\n  Object pose estimation enables robots to understand and interact with their\nenvironments. Training with synthetic data is necessary in order to adapt to\nnovel situations. Unfortunately, pose estimation under domain shift, i.e.,\ntraining on synthetic data and testing in the real world, is challenging. Deep\nlearning-based approaches currently perform best when using encoder-decoder\nnetworks but typically do not generalize to new scenarios with different scene\ncharacteristics. We argue that patch-based approaches, instead of\nencoder-decoder networks, are more suited for synthetic-to-real transfer\nbecause local to global object information is better represented. To that end,\nwe present a novel approach based on a specialized feature pyramid network to\ncompute multi-scale features for creating pose hypotheses on different feature\nmap resolutions in parallel. Our single-shot pose estimation approach is\nevaluated on multiple standard datasets and outperforms the state of the art by\nup to 35%. We also perform grasping experiments in the real world to\ndemonstrate the advantage of using synthetic data to generalize to novel\nenvironments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.12435,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000057618,
      "text":"Pathological Visual Question Answering\n\n  Is it possible to develop an \"AI Pathologist\" to pass the board-certified\nexamination of the American Board of Pathology (ABP)? To build such a system,\nthree challenges need to be addressed. First, we need to create a visual\nquestion answering (VQA) dataset where the AI agent is presented with a\npathology image together with a question and is asked to give the correct\nanswer. Due to privacy concerns, pathology images are usually not publicly\navailable. Besides, only well-trained pathologists can understand pathology\nimages, but they barely have time to help create datasets for AI research. The\nsecond challenge is: since it is difficult to hire highly experienced\npathologists to create pathology visual questions and answers, the resulting\npathology VQA dataset may contain errors. Training pathology VQA models using\nthese noisy or even erroneous data will lead to problematic models that cannot\ngeneralize well on unseen images. The third challenge is: the medical concepts\nand knowledge covered in pathology question-answer (QA) pairs are very diverse\nwhile the number of QA pairs available for modeling training is limited. How to\nlearn effective representations of diverse medical concepts based on limited\ndata is technically demanding. In this paper, we aim to address these three\nchallenges. To our best knowledge, our work represents the first one addressing\nthe pathology VQA problem. To deal with the issue that a publicly available\npathology VQA dataset is lacking, we create PathVQA dataset. To address the\nsecond challenge, we propose a learning-by-ignoring approach. To address the\nthird challenge, we propose to use cross-modal self-supervised learning. We\nperform experiments on our created PathVQA dataset and the results demonstrate\nthe effectiveness of our proposed learning-by-ignoring method and cross-modal\nself-supervised learning methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.00573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based\n  Localization\n\n  Long-Term visual localization under changing environments is a challenging\nproblem in autonomous driving and mobile robotics due to season, illumination\nvariance, etc. Image retrieval for localization is an efficient and effective\nsolution to the problem. In this paper, we propose a novel multi-task\narchitecture to fuse the geometric and semantic information into the\nmulti-scale latent embedding representation for visual place recognition. To\nuse the high-quality ground truths without any human effort, the effective\nmulti-scale feature discriminator is proposed for adversarial training to\nachieve the domain adaptation from synthetic virtual KITTI dataset to\nreal-world KITTI dataset. The proposed approach is validated on the Extended\nCMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial\ncomparison experiments, where our performance outperforms state-of-the-art\nbaselines for retrieval-based localization and large-scale place recognition\nunder the challenging environment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.10027,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000130468,
      "text":"Fast Video Salient Object Detection via Spatiotemporal Knowledge\n  Distillation\n\n  Since the wide employment of deep learning frameworks in video salient object\ndetection, the accuracy of the recent approaches has made stunning progress.\nThese approaches mainly adopt the sequential modules, based on optical flow or\nrecurrent neural network (RNN), to learn robust spatiotemporal features. These\nmodules are effective but significantly increase the computational burden of\nthe corresponding deep models. In this paper, to simplify the network and\nmaintain the accuracy, we present a lightweight network tailored for video\nsalient object detection through the spatiotemporal knowledge distillation.\nSpecifically, in the spatial aspect, we combine a saliency guidance feature\nembedding structure and spatial knowledge distillation to refine the spatial\nfeatures. In the temporal aspect, we propose a temporal knowledge distillation\nstrategy, which allows the network to learn the robust temporal features\nthrough the infer-frame feature encoding and distilling information from\nadjacent frames. The experiments on widely used video datasets (e.g., DAVIS,\nDAVSOD, SegTrack-V2) prove that our approach achieves competitive performance.\nFurthermore, without the employment of the complex sequential modules, the\nproposed network can obtain high efficiency with 0.01s per frame.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.12573,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000062254,
      "text":"Object-aware Feature Aggregation for Video Object Detection\n\n  We present an Object-aware Feature Aggregation (OFA) module for video object\ndetection (VID). Our approach is motivated by the intriguing property that\nvideo-level object-aware knowledge can be employed as a powerful semantic prior\nto help object recognition. As a consequence, augmenting features with such\nprior knowledge can effectively improve the classification and localization\nperformance. To make features get access to more content about the whole video,\nwe first capture the object-aware knowledge of proposals and incorporate such\nknowledge with the well-established pair-wise contexts. With extensive\nexperimental results on the ImageNet VID dataset, our approach demonstrates the\neffectiveness of object-aware knowledge with the superior performance of 83.93%\nand 86.09% mAP with ResNet-101 and ResNeXt-101, respectively. When further\nequipped with Sequence DIoU NMS, we obtain the best-reported mAP of 85.07% and\n86.88% upon the paper submitted. The code to reproduce our results will be\nreleased after acceptance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.063,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000135766,
      "text":"MixCo: Mix-up Contrastive Learning for Visual Representation\n\n  Contrastive learning has shown remarkable results in recent self-supervised\napproaches for visual representation. By learning to contrast positive pairs'\nrepresentation from the corresponding negatives pairs, one can train good\nvisual representations without human annotations. This paper proposes Mix-up\nContrast (MixCo), which extends the contrastive learning concept to\nsemi-positives encoded from the mix-up of positive and negative images. MixCo\naims to learn the relative similarity of representations, reflecting how much\nthe mixed images have the original positives. We validate the efficacy of MixCo\nwhen applied to the recent self-supervised learning algorithms under the\nstandard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In\nthe experiments, MixCo consistently improves test accuracy. Remarkably, the\nimprovement is more significant when the learning capacity (e.g., model size)\nis limited, suggesting that MixCo might be more useful in real-world scenarios.\nThe code is available at: https:\/\/github.com\/Lee-Gihun\/MixCo-Mixup-Contrast.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.09409,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal\n  Scenes\n\n  Conventional SLAM techniques strongly rely on scene rigidity to solve data\nassociation, ignoring dynamic parts of the scene. In this work we present\nSemi-Direct DefSLAM (SD-DefSLAM), a novel monocular deformable SLAM method able\nto map highly deforming environments, built on top of DefSLAM. To robustly\nsolve data association in challenging deforming scenes, SD-DefSLAM combines\ndirect and indirect methods: an enhanced illumination-invariant Lucas-Kanade\ntracker for data association, geometric Bundle Adjustment for pose and\ndeformable map estimation, and bag-of-words based on feature descriptors for\ncamera relocation. Dynamic objects are detected and segmented-out using a CNN\ntrained for the specific application domain. We thoroughly evaluate our system\nin two public datasets. The mandala dataset is a SLAM benchmark with\nincreasingly aggressive deformations. The Hamlyn dataset contains\nintracorporeal sequences that pose serious real-life challenges beyond\ndeformation like weak texture, specular reflections, surgical tools and\nocclusions. Our results show that SD-DefSLAM outperforms DefSLAM in point\ntracking, reconstruction accuracy and scale drift thanks to the improvement in\nall the data association steps, being the first system able to robustly perform\nSLAM inside the human body.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.05713,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000070201,
      "text":"Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2\n  Network\n\n  Image-to-Image (I2I) translation is a heated topic in academia, and it also\nhas been applied in real-world industry for tasks like image synthesis,\nsuper-resolution, and colorization. However, traditional I2I translation\nmethods train data in two or more domains together. This requires lots of\ncomputation resources. Moreover, the results are of lower quality, and they\ncontain many more artifacts. The training process could be unstable when the\ndata in different domains are not balanced, and modal collapse is more likely\nto happen. We proposed a new I2I translation method that generates a new model\nin the target domain via a series of model transformations on a pre-trained\nStyleGAN2 model in the source domain. After that, we proposed an inversion\nmethod to achieve the conversion between an image and its latent vector. By\nfeeding the latent vector into the generated model, we can perform I2I\ntranslation between the source domain and target domain. Both qualitative and\nquantitative evaluations were conducted to prove that the proposed method can\nachieve outstanding performance in terms of image quality, diversity and\nsemantic similarity to the input and reference images compared to\nstate-of-the-art works.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.02315,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"SMILE: Semantically-guided Multi-attribute Image and Layout Editing\n\n  Attribute image manipulation has been a very active topic since the\nintroduction of Generative Adversarial Networks (GANs). Exploring the\ndisentangled attribute space within a transformation is a very challenging task\ndue to the multiple and mutually-inclusive nature of the facial images, where\ndifferent labels (eyeglasses, hats, hair, identity, etc.) can co-exist at the\nsame time. Several works address this issue either by exploiting the modality\nof each domain\/attribute using a conditional random vector noise, or extracting\nthe modality from an exemplary image. However, existing methods cannot handle\nboth random and reference transformations for multiple attributes, which limits\nthe generality of the solutions. In this paper, we successfully exploit a\nmultimodal representation that handles all attributes, be it guided by random\nnoise or exemplar images, while only using the underlying domain information of\nthe target domain. We present extensive qualitative and quantitative results\nfor facial datasets and several different attributes that show the superiority\nof our method. Additionally, our method is capable of adding, removing or\nchanging either fine-grained or coarse attributes by using an image as a\nreference or by exploring the style distribution space, and it can be easily\nextended to head-swapping and face-reenactment applications without being\ntrained on videos.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.10717,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"High-Capacity Complex Convolutional Neural Networks For I\/Q Modulation\n  Classification\n\n  I\/Q modulation classification is a unique pattern recognition problem as the\ndata for each class varies in quality, quantified by signal to noise ratio\n(SNR), and has structure in the complex-plane. Previous work shows treating\nthese samples as complex-valued signals and computing complex-valued\nconvolutions within deep learning frameworks significantly increases the\nperformance over comparable shallow CNN architectures. In this work, we claim\nstate of the art performance by enabling high-capacity architectures containing\nresidual and\/or dense connections to compute complex-valued convolutions, with\npeak classification accuracy of 92.4% on a benchmark classification problem,\nthe RadioML 2016.10a dataset. We show statistically significant improvements in\nall networks with complex convolutions for I\/Q modulation classification.\nComplexity and inference speed analyses show models with complex convolutions\nsubstantially outperform architectures with a comparable number of parameters\nand comparable speed by over 10% in each case.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.00702,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000065565,
      "text":"Learned Dual-View Reflection Removal\n\n  Traditional reflection removal algorithms either use a single image as input,\nwhich suffers from intrinsic ambiguities, or use multiple images from a moving\ncamera, which is inconvenient for users. We instead propose a learning-based\ndereflection algorithm that uses stereo images as input. This is an effective\ntrade-off between the two extremes: the parallax between two views provides\ncues to remove reflections, and two views are easy to capture due to the\nadoption of stereo cameras in smartphones. Our model consists of a\nlearning-based reflection-invariant flow model for dual-view registration, and\na learned synthesis model for combining aligned image pairs. Because no dataset\nfor dual-view reflection removal exists, we render a synthetic dataset of\ndual-views with and without reflections for use in training. Our evaluation on\nan additional real-world dataset of stereo pairs shows that our algorithm\noutperforms existing single-image and multi-image dereflection approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.00793,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"A Parallel Down-Up Fusion Network for Salient Object Detection in\n  Optical Remote Sensing Images\n\n  The diverse spatial resolutions, various object types, scales and\norientations, and cluttered backgrounds in optical remote sensing images (RSIs)\nchallenge the current salient object detection (SOD) approaches. It is commonly\nunsatisfactory to directly employ the SOD approaches designed for nature scene\nimages (NSIs) to RSIs. In this paper, we propose a novel Parallel Down-up\nFusion network (PDF-Net) for SOD in optical RSIs, which takes full advantage of\nthe in-path low- and high-level features and cross-path multi-resolution\nfeatures to distinguish diversely scaled salient objects and suppress the\ncluttered backgrounds. To be specific, keeping a key observation that the\nsalient objects still are salient no matter the resolutions of images are in\nmind, the PDF-Net takes successive down-sampling to form five parallel paths\nand perceive scaled salient objects that are commonly existed in optical RSIs.\nMeanwhile, we adopt the dense connections to take advantage of both low- and\nhigh-level information in the same path and build up the relations of cross\npaths, which explicitly yield strong feature representations. At last, we fuse\nthe multiple-resolution features in parallel paths to combine the benefits of\nthe features with different resolutions, i.e., the high-resolution feature\nconsisting of complete structure and clear details while the low-resolution\nfeatures highlighting the scaled salient objects. Extensive experiments on the\nORSSD dataset demonstrate that the proposed network is superior to the\nstate-of-the-art approaches both qualitatively and quantitatively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.03449,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Super-Human Performance in Online Low-latency Recognition of\n  Conversational Speech\n\n  Achieving super-human performance in recognizing human speech has been a goal\nfor several decades, as researchers have worked on increasingly challenging\ntasks. In the 1990's it was discovered, that conversational speech between two\nhumans turns out to be considerably more difficult than read speech as\nhesitations, disfluencies, false starts and sloppy articulation complicate\nacoustic processing and require robust handling of acoustic, lexical and\nlanguage context, jointly. Early attempts with statistical models could only\nreach error rates over 50% and far from human performance (WER of around 5.5%).\nNeural hybrid models and recent attention-based encoder-decoder models have\nconsiderably improved performance as such contexts can now be learned in an\nintegral fashion. However, processing such contexts requires an entire\nutterance presentation and thus introduces unwanted delays before a recognition\nresult can be output. In this paper, we address performance as well as latency.\nWe present results for a system that can achieve super-human performance (at a\nWER of 5.0%, over the Switchboard conversational benchmark) at a word based\nlatency of only 1 second behind a speaker's speech. The system uses multiple\nattention-based encoder-decoder networks integrated within a novel low latency\nincremental inference approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000383788,
      "text":"Development of Open Informal Dataset Affecting Autonomous Driving\n\n  This document is a document that has written procedures and methods for\ncollecting objects and unstructured dynamic data on the road for the\ndevelopment of object recognition technology for self-driving cars, and\noutlines the methods of collecting data, annotation data, object classifier\ncriteria, and data processing methods. On-road object and unstructured dynamic\ndata were collected in various environments, such as weather, time and traffic\nconditions, and additional reception calls for police and safety personnel were\ncollected. Finally, 100,000 images of various objects existing on pedestrians\nand roads, 200,000 images of police and traffic safety personnel, 5,000 images\nof police and traffic safety personnel, and data sets consisting of 5,000 image\ndata were collected and built.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.04517,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000046028,
      "text":"Real Time Face Recognition Using Convoluted Neural Networks\n\n  Face Recognition is one of the process of identifying people using their\nface, it has various applications like authentication systems, surveillance\nsystems and law enforcement. Convolutional Neural Networks are proved to be\nbest for facial recognition. Detecting faces using core-ml api and processing\nthe extracted face through a coreML model, which is trained to recognize\nspecific persons. The creation of dataset is done by converting face videos of\nthe persons to be recognized into Hundreds of images of person, which is\nfurther used for training and validation of the model to provide accurate\nreal-time results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.061,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Invariant Representation Learning for Infant Pose Estimation with Small\n  Data\n\n  Infant motion analysis is a topic with critical importance in early childhood\ndevelopment studies. However, while the applications of human pose estimation\nhave become more and more broad, models trained on large-scale adult pose\ndatasets are barely successful in estimating infant poses due to the\nsignificant differences in their body ratio and the versatility of their poses.\nMoreover, the privacy and security considerations hinder the availability of\nadequate infant pose data required for training of a robust model from scratch.\nTo address this problem, this paper presents (1) building and publicly\nreleasing a hybrid synthetic and real infant pose (SyRIP) dataset with small\nyet diverse real infant images as well as generated synthetic infant poses and\n(2) a multi-stage invariant representation learning strategy that could\ntransfer the knowledge from the adjacent domains of adult poses and synthetic\ninfant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation\nmodel. In our ablation study, with identical network structure, models trained\non SyRIP dataset show noticeable improvement over the ones trained on the only\nother public infant pose datasets. Integrated with pose estimation backbone\nnetworks with varying complexity, FiDIP performs consistently better than the\nfine-tuned versions of those models. One of our best infant pose estimation\nperformers on the state-of-the-art DarkPose model shows mean average precision\n(mAP) of 93.6.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.01892,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Joint Pruning & Quantization for Extremely Sparse Neural Networks\n\n  We investigate pruning and quantization for deep neural networks. Our goal is\nto achieve extremely high sparsity for quantized networks to enable\nimplementation on low cost and low power accelerator hardware. In a practical\nscenario, there are particularly many applications for dense prediction tasks,\nhence we choose stereo depth estimation as target.\n  We propose a two stage pruning and quantization pipeline and introduce a\nTaylor Score alongside a new fine-tuning mode to achieve extreme sparsity\nwithout sacrificing performance.\n  Our evaluation does not only show that pruning and quantization should be\ninvestigated jointly, but also shows that almost 99% of memory demand can be\ncut while hardware costs can be reduced up to 99.9%. In addition, to compare\nwith other works, we demonstrate that our pruning stage alone beats the\nstate-of-the-art when applied to ResNet on CIFAR10 and ImageNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2010.11563,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":1.0,
      "text":"Advancing Toward Robust and Scalable Fingerprint Orientation Estimation: From Gradients to Deep Learning\n\nThe study identifies a clear evolution from traditional methods to more advanced machine learning approaches. Current algorithms face persistent challenges, including degraded image quality, damaged ridge structures, and background noise, which impact performance. To overcome these limitations, future research must focus on developing efficient algorithms with lower computational complexity while maintaining robust performance across varied conditions. Hybrid methods that combine the simplicity and efficiency of gradient-based techniques with the adaptability and robustness of machine learning are particularly promising for advancing fingerprint recognition systems. Fingerprint orientation estimation plays a crucial role in improving the reliability and accuracy of biometric systems. This study highlights the limitations of current approaches and underscores the importance of designing next-generation algorithms that can operate efficiently across diverse application domains. By addressing these challenges, future developments could enhance the scalability, reliability, and applicability of biometric systems, paving the way for broader use in security and identification technologies.",
      "prediction":"Highly Likely AI",
      "llm_prediction":{
        "GPT35":0.0008201599,
        "GPT4":0.9619140625,
        "CLAUDE":0.0015382767,
        "GOOGLE":0.0153503418,
        "OPENAI_O_SERIES":0.0202331543,
        "DEEPSEEK":0.0000298023,
        "GROK":0.0000004768,
        "NOVA":0.0000088811,
        "OTHER":0.0000058413,
        "HUMAN":0.0000190735
      }
    }
  },
  {
    "arxiv_id":2011.10251,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"On-Device Text Image Super Resolution\n\n  Recent research on super-resolution (SR) has witnessed major developments\nwith the advancements of deep convolutional neural networks. There is a need\nfor information extraction from scenic text images or even document images on\ndevice, most of which are low-resolution (LR) images. Therefore, SR becomes an\nessential pre-processing step as Bicubic Upsampling, which is conventionally\npresent in smartphones, performs poorly on LR images. To give the user more\ncontrol over his privacy, and to reduce the carbon footprint by reducing the\noverhead of cloud computing and hours of GPU usage, executing SR models on the\nedge is a necessity in the recent times. There are various challenges in\nrunning and optimizing a model on resource-constrained platforms like\nsmartphones. In this paper, we present a novel deep neural network that\nreconstructs sharper character edges and thus boosts OCR confidence. The\nproposed architecture not only achieves significant improvement in PSNR over\nbicubic upsampling on various benchmark datasets but also runs with an average\ninference time of 11.7 ms per image. We have outperformed state-of-the-art on\nthe Text330 dataset. We also achieve an OCR accuracy of 75.89% on the ICDAR\n2015 TextSR dataset, where ground truth has an accuracy of 78.10%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.11757,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Learning Translation Invariance in CNNs\n\n  When seeing a new object, humans can immediately recognize it across\ndifferent retinal locations: we say that the internal object representation is\ninvariant to translation. It is commonly believed that Convolutional Neural\nNetworks (CNNs) are architecturally invariant to translation thanks to the\nconvolution and\/or pooling operations they are endowed with. In fact, several\nworks have found that these networks systematically fail to recognise new\nobjects on untrained locations. In this work we show how, even though CNNs are\nnot 'architecturally invariant' to translation, they can indeed 'learn' to be\ninvariant to translation. We verified that this can be achieved by pretraining\non ImageNet, and we found that it is also possible with much simpler datasets\nin which the items are fully translated across the input canvas. We\ninvestigated how this pretraining affected the internal network\nrepresentations, finding that the invariance was almost always acquired, even\nthough it was some times disrupted by further training due to catastrophic\nforgetting\/interference. These experiments show how pretraining a network on an\nenvironment with the right 'latent' characteristics (a more naturalistic\nenvironment) can result in the network learning deep perceptual rules which\nwould dramatically improve subsequent generalization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.02264,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000007583,
      "text":"Handwriting Classification for the Analysis of Art-Historical Documents\n\n  Digitized archives contain and preserve the knowledge of generations of\nscholars in millions of documents. The size of these archives calls for\nautomatic analysis since a manual analysis by specialists is often too\nexpensive. In this paper, we focus on the analysis of handwriting in scanned\ndocuments from the art-historic archive of the WPI. Since the archive consists\nof documents written in several languages and lacks annotated training data for\nthe creation of recognition models, we propose the task of handwriting\nclassification as a new step for a handwriting OCR pipeline. We propose a\nhandwriting classification model that labels extracted text fragments, eg,\nnumbers, dates, or words, based on their visual structure. Such a\nclassification supports historians by highlighting documents that contain a\nspecific class of text without the need to read the entire content. To this\nend, we develop and compare several deep learning-based models for text\nclassification. In extensive experiments, we show the advantages and\ndisadvantages of our proposed approach and discuss possible usage scenarios on\na real-world dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.1029,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Image Denoising by Gaussian Patch Mixture Model and Low Rank Patches\n\n  Non-local self-similarity based low rank algorithms are the state-of-the-art\nmethods for image denoising. In this paper, a new method is proposed by solving\ntwo issues: how to improve similar patches matching accuracy and build an\nappropriate low rank matrix approximation model for Gaussian noise. For the\nfirst issue, similar patches can be found locally or globally. Local patch\nmatching is to find similar patches in a large neighborhood which can alleviate\nnoise effect, but the number of patches may be insufficient. Global patch\nmatching is to determine enough similar patches but the error rate of patch\nmatching may be higher. Based on this, we first use local patch matching method\nto reduce noise and then use Gaussian patch mixture model to achieve global\npatch matching. The second issue is that there is no low rank matrix\napproximation model to adapt to Gaussian noise. We build a new model according\nto the characteristics of Gaussian noise, then prove that there is a globally\noptimal solution of the model. By solving the two issues, experimental results\nare reported to show that the proposed approach outperforms the\nstate-of-the-art denoising methods includes several deep learning ones in both\nPSNR \/ SSIM values and visual quality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.01404,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Faraway-Frustum: Dealing with Lidar Sparsity for 3D Object Detection\n  using Fusion\n\n  Learned pointcloud representations do not generalize well with an increase in\ndistance to the sensor. For example, at a range greater than 60 meters, the\nsparsity of lidar pointclouds reaches to a point where even humans cannot\ndiscern object shapes from each other. However, this distance should not be\nconsidered very far for fast-moving vehicles: A vehicle can traverse 60 meters\nunder two seconds while moving at 70 mph. For safe and robust driving\nautomation, acute 3D object detection at these ranges is indispensable. Against\nthis backdrop, we introduce faraway-frustum: a novel fusion strategy for\ndetecting faraway objects. The main strategy is to depend solely on the 2D\nvision for recognizing object class, as object shape does not change\ndrastically with an increase in depth, and use pointcloud data for object\nlocalization in the 3D space for faraway objects. For closer objects, we use\nlearned pointcloud representations instead, following state-of-the-art. This\nstrategy alleviates the main shortcoming of object detection with learned\npointcloud representations. Experiments on the KITTI dataset demonstrate that\nour method outperforms state-of-the-art by a considerable margin for faraway\nobject detection in bird's-eye-view and 3D. Our code is open-source and\npublicly available: https:\/\/github.com\/dongfang-steven-yang\/faraway-frustum.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13817,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"Generalized Pose-and-Scale Estimation using 4-Point Congruence\n  Constraints\n\n  We present gP4Pc, a new method for computing the absolute pose of a\ngeneralized camera with unknown internal scale from four corresponding 3D\npoint-and-ray pairs. Unlike most pose-and-scale methods, gP4Pc is based on\nconstraints arising from the congruence of shapes defined by two sets of four\npoints related by an unknown similarity transformation. By choosing a novel\nparametrization for the problem, we derive a system of four quadratic equations\nin four scalar variables. The variables represent the distances of 3D points\nalong the rays from the camera centers. After solving this system via Groebner\nbasis-based automatic polynomial solvers, we compute the similarity\ntransformation using an efficient 3D point-point alignment method. We also\npropose a specialized variant of our solver for the case of coplanar points,\nwhich is computationally very efficient and about 3x faster than the fastest\nexisting solver. Our experiments on real and synthetic datasets, demonstrate\nthat gP4Pc is among the fastest methods in terms of total running time when\nused within a RANSAC framework, while achieving competitive numerical\nstability, accuracy, and robustness to noise.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.13688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"MEBOW: Monocular Estimation of Body Orientation In the Wild\n\n  Body orientation estimation provides crucial visual cues in many\napplications, including robotics and autonomous driving. It is particularly\ndesirable when 3-D pose estimation is difficult to infer due to poor image\nresolution, occlusion or indistinguishable body parts. We present COCO-MEBOW\n(Monocular Estimation of Body Orientation in the Wild), a new large-scale\ndataset for orientation estimation from a single in-the-wild image. The\nbody-orientation labels for around 130K human bodies within 55K images from the\nCOCO dataset have been collected using an efficient and high-precision\nannotation pipeline. We also validated the benefits of the dataset. First, we\nshow that our dataset can substantially improve the performance and the\nrobustness of a human body orientation estimation model, the development of\nwhich was previously limited by the scale and diversity of the available\ntraining data. Additionally, we present a novel triple-source solution for 3-D\nhuman pose estimation, where 3-D pose labels, 2-D pose labels, and our\nbody-orientation labels are all used in joint training. Our model significantly\noutperforms state-of-the-art dual-source solutions for monocular 3-D human pose\nestimation, where training only uses 3-D pose labels and 2-D pose labels. This\nsubstantiates an important advantage of MEBOW for 3-D human pose estimation,\nwhich is particularly appealing because the per-instance labeling cost for body\norientations is far less than that for 3-D poses. The work demonstrates high\npotential of MEBOW in addressing real-world challenges involving understanding\nhuman behaviors. Further information of this work is available at\nhttps:\/\/chenyanwu.github.io\/MEBOW\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14311,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"BSNet: Bi-Similarity Network for Few-shot Fine-grained Image\n  Classification\n\n  Few-shot learning for fine-grained image classification has gained recent\nattention in computer vision. Among the approaches for few-shot learning, due\nto the simplicity and effectiveness, metric-based methods are favorably\nstate-of-the-art on many tasks. Most of the metric-based methods assume a\nsingle similarity measure and thus obtain a single feature space. However, if\nsamples can simultaneously be well classified via two distinct similarity\nmeasures, the samples within a class can distribute more compactly in a smaller\nfeature space, producing more discriminative feature maps. Motivated by this,\nwe propose a so-called \\textit{Bi-Similarity Network} (\\textit{BSNet}) that\nconsists of a single embedding module and a bi-similarity module of two\nsimilarity measures. After the support images and the query images pass through\nthe convolution-based embedding module, the bi-similarity module learns feature\nmaps according to two similarity measures of diverse characteristics. In this\nway, the model is enabled to learn more discriminative and less\nsimilarity-biased features from few shots of fine-grained images, such that the\nmodel generalization ability can be significantly improved. Through extensive\nexperiments by slightly modifying established metric\/similarity based networks,\nwe show that the proposed approach produces a substantial improvement on\nseveral fine-grained image benchmark datasets. Codes are available at:\nhttps:\/\/github.com\/spraise\/BSNet\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.03279,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"\"What's This?\" -- Learning to Segment Unknown Objects from Manipulation\n  Sequences\n\n  We present a novel framework for self-supervised grasped object segmentation\nwith a robotic manipulator. Our method successively learns an agnostic\nforeground segmentation followed by a distinction between manipulator and\nobject solely by observing the motion between consecutive RGB frames. In\ncontrast to previous approaches, we propose a single, end-to-end trainable\narchitecture which jointly incorporates motion cues and semantic knowledge.\nFurthermore, while the motion of the manipulator and the object are substantial\ncues for our algorithm, we present means to robustly deal with distraction\nobjects moving in the background, as well as with completely static scenes. Our\nmethod neither depends on any visual registration of a kinematic robot or 3D\nobject models, nor on precise hand-eye calibration or any additional sensor\ndata. By extensive experimental evaluation we demonstrate the superiority of\nour framework and provide detailed insights on its capability of dealing with\nthe aforementioned extreme cases of motion. We also show that training a\nsemantic segmentation network with the automatically labeled data achieves\nresults on par with manually annotated training data. Code and pretrained model\nare available at https:\/\/github.com\/DLR-RM\/DistinctNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.1163,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000042054,
      "text":"Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation\n\n  The objective of this paper is to design a computational architecture that\ndiscovers camouflaged objects in videos, specifically by exploiting motion\ninformation to perform object segmentation. We make the following three\ncontributions: (i) We propose a novel architecture that consists of two\nessential components for breaking camouflage, namely, a differentiable\nregistration module to align consecutive frames based on the background, which\neffectively emphasises the object boundary in the difference image, and a\nmotion segmentation module with memory that discovers the moving objects, while\nmaintaining the object permanence even when motion is absent at some point.\n(ii) We collect the first large-scale Moving Camouflaged Animals (MoCA) video\ndataset, which consists of over 140 clips across a diverse range of animals (67\ncategories). (iii) We demonstrate the effectiveness of the proposed model on\nMoCA, and achieve competitive performance on the unsupervised segmentation\nprotocol on DAVIS2016 by only relying on motion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.14714,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000125501,
      "text":"CM-Net: Concentric Mask based Arbitrary-Shaped Text Detection\n\n  Recently fast arbitrary-shaped text detection has become an attractive\nresearch topic. However, most existing methods are non-real-time, which may\nfall short in intelligent systems. Although a few real-time text methods are\nproposed, the detection accuracy is far behind non-real-time methods. To\nimprove the detection accuracy and speed simultaneously, we propose a novel\nfast and accurate text detection framework, namely CM-Net, which is constructed\nbased on a new text representation method and a multi-perspective feature (MPF)\nmodule. The former can fit arbitrary-shaped text contours by concentric mask\n(CM) in an efficient and robust way. The latter encourages the network to learn\nmore CM-related discriminative features from multiple perspectives and brings\nno extra computational cost. Benefiting the advantages of CM and MPF, the\nproposed CM-Net only needs to predict one CM of the text instance to rebuild\nthe text contour and achieves the best balance between detection accuracy and\nspeed compared with previous works. Moreover, to ensure that multi-perspective\nfeatures are effectively learned, the multi-factor constraints loss is\nproposed. Extensive experiments demonstrate the proposed CM is efficient and\nrobust to fit arbitrary-shaped text instances, and also validate the\neffectiveness of MPF and constraints loss for discriminative text features\nrecognition. Furthermore, experimental results show that the proposed CM-Net is\nsuperior to existing state-of-the-art (SOTA) real-time text detection methods\nin both detection speed and accuracy on MSRA-TD500, CTW1500, Total-Text, and\nICDAR2015 datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Masked Linear Regression for Learning Local Receptive Fields for Facial\n  Expression Synthesis\n\n  Compared to facial expression recognition, expression synthesis requires a\nvery high-dimensional mapping. This problem exacerbates with increasing image\nsizes and limits existing expression synthesis approaches to relatively small\nimages. We observe that facial expressions often constitute sparsely\ndistributed and locally correlated changes from one expression to another. By\nexploiting this observation, the number of parameters in an expression\nsynthesis model can be significantly reduced. Therefore, we propose a\nconstrained version of ridge regression that exploits the local and sparse\nstructure of facial expressions. We consider this model as masked regression\nfor learning local receptive fields. In contrast to the existing approaches,\nour proposed model can be efficiently trained on larger image sizes.\nExperiments using three publicly available datasets demonstrate that our model\nis significantly better than $\\ell_0, \\ell_1$ and $\\ell_2$-regression, SVD\nbased approaches, and kernelized regression in terms of mean-squared-error,\nvisual quality as well as computational and spatial complexities. The reduction\nin the number of parameters allows our method to generalize better even after\ntraining on smaller datasets. The proposed algorithm is also compared with\nstate-of-the-art GANs including Pix2Pix, CycleGAN, StarGAN and GANimation.\nThese GANs produce photo-realistic results as long as the testing and the\ntraining distributions are similar. In contrast, our results demonstrate\nsignificant generalization of the proposed algorithm over out-of-dataset human\nphotographs, pencil sketches and even animal faces.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.12276,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Insights From A Large-Scale Database of Material Depictions In Paintings\n\n  Deep learning has paved the way for strong recognition systems which are\noften both trained on and applied to natural images. In this paper, we examine\nthe give-and-take relationship between such visual recognition systems and the\nrich information available in the fine arts. First, we find that visual\nrecognition systems designed for natural images can work surprisingly well on\npaintings. In particular, we find that interactive segmentation tools can be\nused to cleanly annotate polygonal segments within paintings, a task which is\ntime consuming to undertake by hand. We also find that FasterRCNN, a model\nwhich has been designed for object recognition in natural scenes, can be\nquickly repurposed for detection of materials in paintings. Second, we show\nthat learning from paintings can be beneficial for neural networks that are\nintended to be used on natural images. We find that training on paintings\ninstead of natural images can improve the quality of learned features and we\nfurther find that a large number of paintings can be a valuable source of test\ndata for evaluating domain adaptation algorithms. Our experiments are based on\na novel large-scale annotated database of material depictions in paintings\nwhich we detail in a separate manuscript.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.10974,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Learnable Sampling 3D Convolution for Video Enhancement and Action\n  Recognition\n\n  A key challenge in video enhancement and action recognition is to fuse useful\ninformation from neighboring frames. Recent works suggest establishing accurate\ncorrespondences between neighboring frames before fusing temporal information.\nHowever, the generated results heavily depend on the quality of correspondence\nestimation. In this paper, we propose a more robust solution: \\emph{sampling\nand fusing multi-level features} across neighborhood frames to generate the\nresults. Based on this idea, we introduce a new module to improve the\ncapability of 3D convolution, namely, learnable sampling 3D convolution\n(\\emph{LS3D-Conv}). We add learnable 2D offsets to 3D convolution which aims to\nsample locations on spatial feature maps across frames. The offsets can be\nlearned for specific tasks. The \\emph{LS3D-Conv} can flexibly replace 3D\nconvolution layers in existing 3D networks and get new architectures, which\nlearns the sampling at multiple feature levels. The experiments on video\ninterpolation, video super-resolution, video denoising, and action recognition\ndemonstrate the effectiveness of our approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.09783,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings\n\n  We introduce DeepMorph, an information embedding technique for vector\ndrawings. Provided a vector drawing, such as a Scalable Vector Graphics (SVG)\nfile, our method embeds bitstrings in the image by perturbing the drawing\nprimitives (lines, circles, etc.). This results in a morphed image that can be\ndecoded to recover the original bitstring. The use-case is similar to that of\nthe well-known QR code, but our solution provides creatives with artistic\nfreedom to transfer digital information via drawings of their own design. The\nmethod comprises two neural networks, which are trained jointly: an encoder\nnetwork that transforms a bitstring into a perturbation of the drawing\nprimitives, and a decoder network that recovers the bitstring from an image of\nthe morphed drawing. To enable end-to-end training via back propagation, we\nintroduce a soft rasterizer, which is differentiable with respect to\nperturbations of the drawing primitives. In order to add robustness towards\nreal-world image capture conditions, image corruptions are injected between the\nsoft rasterizer and the decoder. Further, the addition of an object detection\nand camera pose estimation system enables decoding of drawings in complex\nscenes as well as use of the drawings as markers for use in augmented reality\napplications. We demonstrate that our method reliably recovers bitstrings from\nreal-world photos of printed drawings, thereby providing a novel solution for\ncreatives to transfer digital information via artistic imagery.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.04844,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Ellipse Detection and Localization with Applications to Knots in Sawn\n  Lumber Images\n\n  While general object detection has seen tremendous progress, localization of\nelliptical objects has received little attention in the literature. Our\nmotivating application is the detection of knots in sawn timber images, which\nis an important problem since the number and types of knots are visual\ncharacteristics that adversely affect the quality of sawn timber. We\ndemonstrate how models can be tailored to the elliptical shape and thereby\nimprove on general purpose detectors; more generally, elliptical defects are\ncommon in industrial production, such as enclosed air bubbles when casting\nglass or plastic. In this paper, we adapt the Faster R-CNN with its Region\nProposal Network (RPN) to model elliptical objects with a Gaussian function,\nand extend the existing Gaussian Proposal Network (GPN) architecture by adding\nthe region-of-interest pooling and regression branches, as well as using the\nWasserstein distance as the loss function to predict the precise locations of\nelliptical objects. Our proposed method has promising results on the lumber\nknot dataset: knots are detected with an average intersection over union of\n73.05%, compared to 63.63% for general purpose detectors. Specific to the\nlumber application, we also propose an algorithm to correct any misalignment in\nthe raw timber images during scanning, and contribute the first open-source\nlumber knot dataset by labeling the elliptical knots in the preprocessed\nimages.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.07526,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000038412,
      "text":"Domain Adaptation Gaze Estimation by Embedding with Prediction\n  Consistency\n\n  Gaze is the essential manifestation of human attention. In recent years, a\nseries of work has achieved high accuracy in gaze estimation. However, the\ninter-personal difference limits the reduction of the subject-independent gaze\nestimation error. This paper proposes an unsupervised method for domain\nadaptation gaze estimation to eliminate the impact of inter-personal diversity.\nIn domain adaption, we design an embedding representation with prediction\nconsistency to ensure that the linear relationship between gaze directions in\ndifferent domains remains consistent on gaze space and embedding space.\nSpecifically, we employ source gaze to form a locally linear representation in\nthe gaze space for each target domain prediction. Then the same linear\ncombinations are applied in the embedding space to generate hypothesis\nembedding for the target domain sample, remaining prediction consistency. The\ndeviation between the target and source domain is reduced by approximating the\npredicted and hypothesis embedding for the target domain sample. Guided by the\nproposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN),\nwhich learns embedding with prediction consistency and achieves\nstate-of-the-art results on both the MPIIGaze and the EYEDIAP datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.12483,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.00000553,
      "text":"CRACT: Cascaded Regression-Align-Classification for Robust Visual\n  Tracking\n\n  High quality object proposals are crucial in visual tracking algorithms that\nutilize region proposal network (RPN). Refinement of these proposals, typically\nby box regression and classification in parallel, has been popularly adopted to\nboost tracking performance. However, it still meets problems when dealing with\ncomplex and dynamic background. Thus motivated, in this paper we introduce an\nimproved proposal refinement module, Cascaded Regression-Align-Classification\n(CRAC), which yields new state-of-the-art performances on many benchmarks.\n  First, having observed that the offsets from box regression can serve as\nguidance for proposal feature refinement, we design CRAC as a cascade of box\nregression, feature alignment and box classification. The key is to bridge box\nregression and classification via an alignment step, which leads to more\naccurate features for proposal classification with improved robustness. To\naddress the variation in object appearance, we introduce an\nidentification-discrimination component for box classification, which leverages\noffline reliable fine-grained template and online rich background information\nto distinguish the target from background. Moreover, we present pyramid\nRoIAlign that benefits CRAC by exploiting both the local and global cues of\nproposals. During inference, tracking proceeds by ranking all refined proposals\nand selecting the best one. In experiments on seven benchmarks including\nOTB-2015, UAV123, NfS, VOT-2018, TrackingNet, GOT-10k and LaSOT, our CRACT\nexhibits very promising results in comparison with state-of-the-art competitors\nand runs in real-time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.03363,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000640088,
      "text":"Domain Adaptive Person Re-Identification via Coupling Optimization\n\n  Domain adaptive person Re-Identification (ReID) is challenging owing to the\ndomain gap and shortage of annotations on target scenarios. To handle those two\nchallenges, this paper proposes a coupling optimization method including the\nDomain-Invariant Mapping (DIM) method and the Global-Local distance\nOptimization (GLO), respectively. Different from previous methods that transfer\nknowledge in two stages, the DIM achieves a more efficient one-stage knowledge\ntransfer by mapping images in labeled and unlabeled datasets to a shared\nfeature space. GLO is designed to train the ReID model with unsupervised\nsetting on the target domain. Instead of relying on existing optimization\nstrategies designed for supervised training, GLO involves more images in\ndistance optimization, and achieves better robustness to noisy label\nprediction. GLO also integrates distance optimizations in both the global\ndataset and local training batch, thus exhibits better training efficiency.\nExtensive experiments on three large-scale datasets, i.e., Market-1501,\nDukeMTMC-reID, and MSMT17, show that our coupling optimization outperforms\nstate-of-the-art methods by a large margin. Our method also works well in\nunsupervised training, and even outperforms several recent domain adaptive\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2011.12616,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000080466,
      "text":"Unsupervised Domain Adaptation in Semantic Segmentation via Orthogonal\n  and Clustered Embeddings\n\n  Deep learning frameworks allowed for a remarkable advancement in semantic\nsegmentation, but the data hungry nature of convolutional networks has rapidly\nraised the demand for adaptation techniques able to transfer learned knowledge\nfrom label-abundant domains to unlabeled ones. In this paper we propose an\neffective Unsupervised Domain Adaptation (UDA) strategy, based on a feature\nclustering method that captures the different semantic modes of the feature\ndistribution and groups features of the same class into tight and\nwell-separated clusters. Furthermore, we introduce two novel learning\nobjectives to enhance the discriminative clustering performance: an\northogonality loss forces spaced out individual representations to be\northogonal, while a sparsity loss reduces class-wise the number of active\nfeature channels. The joint effect of these modules is to regularize the\nstructure of the feature space. Extensive evaluations in the synthetic-to-real\nscenario show that we achieve state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"VideoMix: Rethinking Data Augmentation for Video Classification\n\n  State-of-the-art video action classifiers often suffer from overfitting. They\ntend to be biased towards specific objects and scene cues, rather than the\nforeground action content, leading to sub-optimal generalization performances.\nRecent data augmentation strategies have been reported to address the\noverfitting problems in static image classifiers. Despite the effectiveness on\nthe static image classifiers, data augmentation has rarely been studied for\nvideos. For the first time in the field, we systematically analyze the efficacy\nof various data augmentation strategies on the video classification task. We\nthen propose a powerful augmentation strategy VideoMix. VideoMix creates a new\ntraining video by inserting a video cuboid into another video. The ground truth\nlabels are mixed proportionally to the number of voxels from each video. We\nshow that VideoMix lets a model learn beyond the object and scene biases and\nextract more robust cues for action recognition. VideoMix consistently\noutperforms other augmentation baselines on Kinetics and the challenging\nSomething-Something-V2 benchmarks. It also improves the weakly-supervised\naction localization performance on THUMOS'14. VideoMix pretrained models\nexhibit improved accuracies on the video detection task (AVA).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03265,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000292063,
      "text":"Towards Better Object Detection in Scale Variation with Adaptive Feature\n  Selection\n\n  It is a common practice to exploit pyramidal feature representation to tackle\nthe problem of scale variation in object instances. However, most of them still\npredict the objects in a certain range of scales based solely or mainly on a\nsingle-level representation, yielding inferior detection performance. To this\nend, we propose a novel adaptive feature selection module (AFSM), to\nautomatically learn the way to fuse multi-level representations in the channel\ndimension, in a data-driven manner. It significantly improves the performance\nof the detectors that have a feature pyramid structure, while introducing\nnearly free inference overhead. Moreover, a class-aware sampling mechanism\n(CASM) is proposed to tackle the class imbalance problem, by re-weighting the\nsampling ratio to each of the training images, based on the statistical\ncharacteristics of each class. This is crucial to improve the performance of\nthe minor classes. Experimental results demonstrate the effectiveness of the\nproposed method, with 83.04% mAP at 15.96 FPS on the VOC dataset, and 39.48% AP\non the VisDrone-DET validation subset, respectively, outperforming other\nstate-of-the-art detectors considerably. The code is available at\nhttps:\/\/github.com\/ZeHuiGong\/AFSM.git.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.07248,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"TDAF: Top-Down Attention Framework for Vision Tasks\n\n  Human attention mechanisms often work in a top-down manner, yet it is not\nwell explored in vision research. Here, we propose the Top-Down Attention\nFramework (TDAF) to capture top-down attentions, which can be easily adopted in\nmost existing models. The designed Recursive Dual-Directional Nested Structure\nin it forms two sets of orthogonal paths, recursive and structural ones, where\nbottom-up spatial features and top-down attention features are extracted\nrespectively. Such spatial and attention features are nested deeply, therefore,\nthe proposed framework works in a mixed top-down and bottom-up manner.\nEmpirical evidence shows that our TDAF can capture effective stratified\nattention information and boost performance. ResNet with TDAF achieves 2.0%\nimprovements on ImageNet. For object detection, the performance is improved by\n2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And\nfor action recognition, the 3D-ResNet adopting TDAF achieves improvements of\n1.7% accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02733,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"Seed the Views: Hierarchical Semantic Alignment for Contrastive\n  Representation Learning\n\n  Self-supervised learning based on instance discrimination has shown\nremarkable progress. In particular, contrastive learning, which regards each\nimage as well as its augmentations as an individual class and tries to\ndistinguish them from all other images, has been verified effective for\nrepresentation learning. However, pushing away two images that are de facto\nsimilar is suboptimal for general representation. In this paper, we propose a\nhierarchical semantic alignment strategy via expanding the views generated by a\nsingle image to \\textbf{Cross-samples and Multi-level} representation, and\nmodels the invariance to semantically similar images in a hierarchical way.\nThis is achieved by extending the contrastive loss to allow for multiple\npositives per anchor, and explicitly pulling semantically similar\nimages\/patches together at different layers of the network. Our method, termed\nas CsMl, has the ability to integrate multi-level visual representations across\nsamples in a robust way. CsMl is applicable to current contrastive learning\nbased methods and consistently improves the performance. Notably, using the\nmoco as an instantiation, CsMl achieves a \\textbf{76.6\\% }top-1 accuracy with\nlinear evaluation using ResNet-50 as backbone, and \\textbf{66.7\\%} and\n\\textbf{75.1\\%} top-1 accuracy with only 1\\% and 10\\% labels, respectively.\n\\textbf{All these numbers set the new state-of-the-art.}\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.04329,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"StacMR: Scene-Text Aware Cross-Modal Retrieval\n\n  Recent models for cross-modal retrieval have benefited from an increasingly\nrich understanding of visual scenes, afforded by scene graphs and object\ninteractions to mention a few. This has resulted in an improved matching\nbetween the visual representation of an image and the textual representation of\nits caption. Yet, current visual representations overlook a key aspect: the\ntext appearing in images, which may contain crucial information for retrieval.\nIn this paper, we first propose a new dataset that allows exploration of\ncross-modal retrieval where images contain scene-text instances. Then, armed\nwith this dataset, we describe several approaches which leverage scene text,\nincluding a better scene-text aware cross-modal retrieval method which uses\nspecialized representations for text from the captions and text from the visual\nscene, and reconcile them in a common embedding space. Extensive experiments\nconfirm that cross-modal retrieval approaches benefit from scene text and\nhighlight interesting research questions worth exploring further. Dataset and\ncode are available at http:\/\/europe.naverlabs.com\/stacmr\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.1288,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000019206,
      "text":"Exploring Instance-Level Uncertainty for Medical Detection\n\n  The ability of deep learning to predict with uncertainty is recognized as key\nfor its adoption in clinical routines. Moreover, performance gain has been\nenabled by modelling uncertainty according to empirical evidence. While\nprevious work has widely discussed the uncertainty estimation in segmentation\nand classification tasks, its application on bounding-box-based detection has\nbeen limited, mainly due to the challenge of bounding box aligning. In this\nwork, we explore to augment a 2.5D detection CNN with two different\nbounding-box-level (or instance-level) uncertainty estimates, i.e., predictive\nvariance and Monte Carlo (MC) sample variance. Experiments are conducted for\nlung nodule detection on LUNA16 dataset, a task where significant semantic\nambiguities can exist between nodules and non-nodules. Results show that our\nmethod improves the evaluating score from 84.57% to 88.86% by utilizing a\ncombination of both types of variances. Moreover, we show the generated\nuncertainty enables superior operating points compared to using the probability\nthreshold only, and can further boost the performance to 89.52%. Example nodule\ndetections are visualized to further illustrate the advantages of our method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.02047,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000043048,
      "text":"CoCosNet v2: Full-Resolution Correspondence Learning for Image\n  Translation\n\n  We present the full-resolution correspondence learning for cross-domain\nimages, which aids image translation. We adopt a hierarchical strategy that\nuses the correspondence from coarse level to guide the fine levels. At each\nhierarchy, the correspondence can be efficiently computed via PatchMatch that\niteratively leverages the matchings from the neighborhood. Within each\nPatchMatch iteration, the ConvGRU module is employed to refine the current\ncorrespondence considering not only the matchings of larger context but also\nthe historic estimates. The proposed CoCosNet v2, a GRU-assisted PatchMatch\napproach, is fully differentiable and highly efficient. When jointly trained\nwith image translation, full-resolution semantic correspondence can be\nestablished in an unsupervised manner, which in turn facilitates the\nexemplar-based image translation. Experiments on diverse translation tasks show\nthat CoCosNet v2 performs considerably better than state-of-the-art literature\non producing high-resolution images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.03438,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"Selective Pseudo-Labeling with Reinforcement Learning for\n  Semi-Supervised Domain Adaptation\n\n  Recent domain adaptation methods have demonstrated impressive improvement on\nunsupervised domain adaptation problems. However, in the semi-supervised domain\nadaptation (SSDA) setting where the target domain has a few labeled instances\navailable, these methods can fail to improve performance. Inspired by the\neffectiveness of pseudo-labels in domain adaptation, we propose a reinforcement\nlearning based selective pseudo-labeling method for semi-supervised domain\nadaptation. It is difficult for conventional pseudo-labeling methods to balance\nthe correctness and representativeness of pseudo-labeled data. To address this\nlimitation, we develop a deep Q-learning model to select both accurate and\nrepresentative pseudo-labeled instances. Moreover, motivated by large margin\nloss's capacity on learning discriminative features with little data, we\nfurther propose a novel target margin loss for our base model training to\nimprove its discriminability. Our proposed method is evaluated on several\nbenchmark datasets for SSDA, and demonstrates superior performance to all the\ncomparison methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.04462,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"Multi-Objective Interpolation Training for Robustness to Label Noise\n\n  Deep neural networks trained with standard cross-entropy loss memorize noisy\nlabels, which degrades their performance. Most research to mitigate this\nmemorization proposes new robust classification loss functions. Conversely, we\npropose a Multi-Objective Interpolation Training (MOIT) approach that jointly\nexploits contrastive learning and classification to mutually help each other\nand boost performance against label noise. We show that standard supervised\ncontrastive learning degrades in the presence of label noise and propose an\ninterpolation training strategy to mitigate this behavior. We further propose a\nnovel label noise detection method that exploits the robust feature\nrepresentations learned via contrastive learning to estimate per-sample\nsoft-labels whose disagreements with the original labels accurately identify\nnoisy samples. This detection allows treating noisy samples as unlabeled and\ntraining a classifier in a semi-supervised manner to prevent noise memorization\nand improve representation learning. We further propose MOIT+, a refinement of\nMOIT by fine-tuning on detected clean samples. Hyperparameter and ablation\nstudies verify the key components of our method. Experiments on synthetic and\nreal-world noise benchmarks demonstrate that MOIT\/MOIT+ achieves\nstate-of-the-art results. Code is available at https:\/\/git.io\/JI40X.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.06178,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000042386,
      "text":"Detailed 3D Human Body Reconstruction from Multi-view Images Combining\n  Voxel Super-Resolution and Learned Implicit Representation\n\n  The task of reconstructing detailed 3D human body models from images is\ninteresting but challenging in computer vision due to the high freedom of human\nbodies. In order to tackle the problem, we propose a coarse-to-fine method to\nreconstruct a detailed 3D human body from multi-view images combining voxel\nsuper-resolution based on learning the implicit representation. Firstly, the\ncoarse 3D models are estimated by learning an implicit representation based on\nmulti-scale features which are extracted by multi-stage hourglass networks from\nthe multi-view images. Then, taking the low resolution voxel grids which are\ngenerated by the coarse 3D models as input, the voxel super-resolution based on\nan implicit representation is learned through a multi-stage 3D convolutional\nneural network. Finally, the refined detailed 3D human body models can be\nproduced by the voxel super-resolution which can preserve the details and\nreduce the false reconstruction of the coarse 3D models. Benefiting from the\nimplicit representation, the training process in our method is memory efficient\nand the detailed 3D human body produced by our method from multi-view images is\nthe continuous decision boundary with high-resolution geometry. In addition,\nthe coarse-to-fine method based on voxel super-resolution can remove false\nreconstructions and preserve the appearance details in the final\nreconstruction, simultaneously. In the experiments, our method quantitatively\nand qualitatively achieves the competitive 3D human body reconstructions from\nimages with various poses and shapes on both the real and synthetic datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.13853,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000067552,
      "text":"ANL: Anti-Noise Learning for Cross-Domain Person Re-Identification\n\n  Due to the lack of labels and the domain diversities, it is a challenge to\nstudy person re-identification in the cross-domain setting. An admirable method\nis to optimize the target model by assigning pseudo-labels for unlabeled\nsamples through clustering. Usually, attributed to the domain gaps, the\npre-trained source domain model cannot extract appropriate target domain\nfeatures, which will dramatically affect the clustering performance and the\naccuracy of pseudo-labels. Extensive label noise will lead to sub-optimal\nsolutions doubtlessly. To solve these problems, we propose an Anti-Noise\nLearning (ANL) approach, which contains two modules. The Feature Distribution\nAlignment (FDA) module is designed to gather the id-related samples and\ndisperse id-unrelated samples, through the camera-wise contrastive learning and\nadversarial adaptation. Creating a friendly cross-feature foundation for\nclustering that is to reduce clustering noise. Besides, the Reliable Sample\nSelection (RSS) module utilizes an Auxiliary Model to correct noisy labels and\nselect reliable samples for the Main Model. In order to effectively utilize the\noutlier information generated by the clustering algorithm and RSS module, we\ntrain these samples at the instance-level. The experiments demonstrate that our\nproposed ANL framework can effectively reduce the domain conflicts and\nalleviate the influence of noisy samples, as well as superior performance\ncompared with the state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.1087,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0001166264,
      "text":"Computer Vision based Accident Detection for Autonomous Vehicles\n\n  Numerous Deep Learning and sensor-based models have been developed to detect\npotential accidents with an autonomous vehicle. However, a self-driving car\nneeds to be able to detect accidents between other vehicles in its path and\ntake appropriate actions such as to slow down or stop and inform the concerned\nauthorities. In this paper, we propose a novel support system for self-driving\ncars that detects vehicular accidents through a dashboard camera. The system\nleverages the Mask R-CNN framework for vehicle detection and a centroid\ntracking algorithm to track the detected vehicle. Additionally, the framework\ncalculates various parameters such as speed, acceleration, and trajectory to\ndetermine whether an accident has occurred between any of the tracked vehicles.\nThe framework has been tested on a custom dataset of dashcam footage and\nachieves a high accident detection rate while maintaining a low false alarm\nrate.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.10013,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Flow-based Generative Models for Learning Manifold to Manifold Mappings\n\n  Many measurements or observations in computer vision and machine learning\nmanifest as non-Euclidean data. While recent proposals (like spherical CNN)\nhave extended a number of deep neural network architectures to manifold-valued\ndata, and this has often provided strong improvements in performance, the\nliterature on generative models for manifold data is quite sparse. Partly due\nto this gap, there are also no modality transfer\/translation models for\nmanifold-valued data whereas numerous such methods based on generative models\nare available for natural images. This paper addresses this gap, motivated by a\nneed in brain imaging -- in doing so, we expand the operating range of certain\ngenerative models (as well as generative models for modality transfer) from\nnatural images to images with manifold-valued measurements. Our main result is\nthe design of a two-stream version of GLOW (flow-based invertible generative\nmodels) that can synthesize information of a field of one type of\nmanifold-valued measurements given another. On the theoretical side, we\nintroduce three kinds of invertible layers for manifold-valued data, which are\nnot only analogous to their functionality in flow-based generative models\n(e.g., GLOW) but also preserve the key benefits (determinants of the Jacobian\nare easy to calculate). For experiments, on a large dataset from the Human\nConnectome Project (HCP), we show promising results where we can reliably and\naccurately reconstruct brain images of a field of orientation distribution\nfunctions (ODF) from diffusion tensor images (DTI), where the latter has a\n$5\\times$ faster acquisition time but at the expense of worse angular\nresolution.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.13212,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Memory-Efficient Hierarchical Neural Architecture Search for Image\n  Restoration\n\n  Recently, much attention has been spent on neural architecture search (NAS),\naiming to outperform those manually-designed neural architectures on high-level\nvision recognition tasks. Inspired by the success, here we attempt to leverage\nNAS techniques to automatically design efficient network architectures for\nlow-level image restoration tasks. In particular, we propose a memory-efficient\nhierarchical NAS (termed HiNAS) and apply it to two such tasks: image denoising\nand image super-resolution. HiNAS adopts gradient based search strategies and\nbuilds a flexible hierarchical search space, including the inner search space\nand outer search space. They are in charge of designing cell architectures and\ndeciding cell widths, respectively. For the inner search space, we propose a\nlayer-wise architecture sharing strategy (LWAS), resulting in more flexible\narchitectures and better performance. For the outer search space, we design a\ncell-sharing strategy to save memory, and considerably accelerate the search\nspeed. The proposed HiNAS method is both memory and computation efficient. With\na single GTX1080Ti GPU, it takes only about 1 hour for searching for denoising\nnetwork on the BSD-500 dataset and 3.5 hours for searching for the\nsuper-resolution structure on the DIV2K dataset. Experiments show that the\narchitectures found by HiNAS have fewer parameters and enjoy a faster inference\nspeed, while achieving highly competitive performance compared with\nstate-of-the-art methods. Code is available at:\nhttps:\/\/github.com\/hkzhang91\/HiNAS\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.10844,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000410941,
      "text":"PTN: A Poisson Transfer Network for Semi-supervised Few-shot Learning\n\n  The predicament in semi-supervised few-shot learning (SSFSL) is to maximize\nthe value of the extra unlabeled data to boost the few-shot learner. In this\npaper, we propose a Poisson Transfer Network (PTN) to mine the unlabeled\ninformation for SSFSL from two aspects. First, the Poisson Merriman Bence Osher\n(MBO) model builds a bridge for the communications between labeled and\nunlabeled examples. This model serves as a more stable and informative\nclassifier than traditional graph-based SSFSL methods in the message-passing\nprocess of the labels. Second, the extra unlabeled samples are employed to\ntransfer the knowledge from base classes to novel classes through contrastive\nlearning. Specifically, we force the augmented positive pairs close while push\nthe negative ones distant. Our contrastive transfer scheme implicitly learns\nthe novel-class embeddings to alleviate the over-fitting problem on the few\nlabeled data. Thus, we can mitigate the degeneration of embedding generality in\nnovel classes. Extensive experiments indicate that PTN outperforms the\nstate-of-the-art few-shot and SSFSL models on miniImageNet and tieredImageNet\nbenchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.08512,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n\n  A majority of methods for video frame interpolation compute bidirectional\noptical flow between adjacent frames of a video, followed by a suitable warping\nalgorithm to generate the output frames. However, approaches relying on optical\nflow often fail to model occlusions and complex non-linear motions directly\nfrom the video and introduce additional bottlenecks unsuitable for widespread\ndeployment. We address these limitations with FLAVR, a flexible and efficient\narchitecture that uses 3D space-time convolutions to enable end-to-end learning\nand inference for video frame interpolation. Our method efficiently learns to\nreason about non-linear motions, complex occlusions and temporal abstractions,\nresulting in improved performance on video interpolation, while requiring no\nadditional inputs in the form of optical flow or depth maps. Due to its\nsimplicity, FLAVR can deliver 3x faster inference speed compared to the current\nmost accurate method on multi-frame interpolation without losing interpolation\naccuracy. In addition, we evaluate FLAVR on a wide range of challenging\nsettings and consistently demonstrate superior qualitative and quantitative\nresults compared with prior methods on various popular benchmarks including\nVimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR\nfor video frame interpolation can serve as a useful self-supervised pretext\ntask for action recognition, optical flow estimation, and motion magnification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.06777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Uncalibrated Neural Inverse Rendering for Photometric Stereo of General\n  Surfaces\n\n  This paper presents an uncalibrated deep neural network framework for the\nphotometric stereo problem. For training models to solve the problem, existing\nneural network-based methods either require exact light directions or\nground-truth surface normals of the object or both. However, in practice, it is\nchallenging to procure both of this information precisely, which restricts the\nbroader adoption of photometric stereo algorithms for vision application. To\nbypass this difficulty, we propose an uncalibrated neural inverse rendering\napproach to this problem. Our method first estimates the light directions from\nthe input images and then optimizes an image reconstruction loss to calculate\nthe surface normals, bidirectional reflectance distribution function value, and\ndepth. Additionally, our formulation explicitly models the concave and convex\nparts of a complex surface to consider the effects of interreflections in the\nimage formation process. Extensive evaluation of the proposed method on the\nchallenging subjects generally shows comparable or better results than the\nsupervised and classical approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.1053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Dynamic Traffic Modeling From Overhead Imagery\n\n  Our goal is to use overhead imagery to understand patterns in traffic flow,\nfor instance answering questions such as how fast could you traverse Times\nSquare at 3am on a Sunday. A traditional approach for solving this problem\nwould be to model the speed of each road segment as a function of time.\nHowever, this strategy is limited in that a significant amount of data must\nfirst be collected before a model can be used and it fails to generalize to new\nareas. Instead, we propose an automatic approach for generating dynamic maps of\ntraffic speeds using convolutional neural networks. Our method operates on\noverhead imagery, is conditioned on location and time, and outputs a local\nmotion model that captures likely directions of travel and corresponding travel\nspeeds. To train our model, we take advantage of historical traffic data\ncollected from New York City. Experimental results demonstrate that our method\ncan be applied to generate accurate city-scale traffic models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.01782,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000060598,
      "text":"Attributes Aware Face Generation with Generative Adversarial Networks\n\n  Recent studies have shown remarkable success in face image generations.\nHowever, most of the existing methods only generate face images from random\nnoise, and cannot generate face images according to the specific attributes. In\nthis paper, we focus on the problem of face synthesis from attributes, which\naims at generating faces with specific characteristics corresponding to the\ngiven attributes. To this end, we propose a novel attributes aware face image\ngenerator method with generative adversarial networks called AFGAN.\nSpecifically, we firstly propose a two-path embedding layer and self-attention\nmechanism to convert binary attribute vector to rich attribute features. Then\nthree stacked generators generate $64 \\times 64$, $128 \\times 128$ and $256\n\\times 256$ resolution face images respectively by taking the attribute\nfeatures as input. In addition, an image-attribute matching loss is proposed to\nenhance the correlation between the generated images and input attributes.\nExtensive experiments on CelebA demonstrate the superiority of our AFGAN in\nterms of both qualitative and quantitative evaluations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2012.12645,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2020,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"SWA Object Detection\n\n  Do you want to improve 1.0 AP for your object detector without any inference\ncost and any change to your detector? Let us tell you such a recipe. It is\nsurprisingly simple: train your detector for an extra 12 epochs using cyclical\nlearning rates and then average these 12 checkpoints as your final detection\nmodel}. This potent recipe is inspired by Stochastic Weights Averaging (SWA),\nwhich is proposed in arXiv:1803.05407 for improving generalization in deep\nneural networks. We found it also very effective in object detection. In this\ntechnique report, we systematically investigate the effects of applying SWA to\nobject detection as well as instance segmentation. Through extensive\nexperiments, we discover the aforementioned workable policy of performing SWA\nin object detection, and we consistently achieve $\\sim$1.0 AP improvement over\nvarious popular detectors on the challenging COCO benchmark, including Mask\nRCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will\nmake more researchers in object detection know this technique and help them\ntrain better object detectors. Code is available at:\nhttps:\/\/github.com\/hyz-xmaster\/swa_object_detection .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06498,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Bladder segmentation based on deep learning approaches: current\n  limitations and lessons\n\n  Precise determination and assessment of bladder cancer (BC) extent of muscle\ninvasion involvement guides proper risk stratification and personalized therapy\nselection. In this context, segmentation of both bladder walls and cancer are\nof pivotal importance, as it provides invaluable information to stage the\nprimary tumour. Hence, multi region segmentation on patients presenting with\nsymptoms of bladder tumours using deep learning heralds a new level of staging\naccuracy and prediction of the biologic behaviour of the tumour. Nevertheless,\ndespite the success of these models in other medical problems, progress in\nmulti region bladder segmentation is still at a nascent stage, with just a\nhandful of works tackling a multi region scenario. Furthermore, most existing\napproaches systematically follow prior literature in other clinical problems,\nwithout casting a doubt on the validity of these methods on bladder\nsegmentation, which may present different challenges. Inspired by this, we\nprovide an in-depth look at bladder cancer segmentation using deep learning\nmodels. The critical determinants for accurate differentiation of muscle\ninvasive disease, current status of deep learning based bladder segmentation,\nlessons and limitations of prior work are highlighted.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.11604,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Shape or Texture: Understanding Discriminative Features in CNNs\n\n  Contrasting the previous evidence that neurons in the later layers of a\nConvolutional Neural Network (CNN) respond to complex object shapes, recent\nstudies have shown that CNNs actually exhibit a `texture bias': given an image\nwith both texture and shape cues (e.g., a stylized image), a CNN is biased\ntowards predicting the category corresponding to the texture. However, these\nprevious studies conduct experiments on the final classification output of the\nnetwork, and fail to robustly evaluate the bias contained (i) in the latent\nrepresentations, and (ii) on a per-pixel level. In this paper, we design a\nseries of experiments that overcome these issues. We do this with the goal of\nbetter understanding what type of shape information contained in the network is\ndiscriminative, where shape information is encoded, as well as when the network\nlearns about object shape during training. We show that a network learns the\nmajority of overall shape information at the first few epochs of training and\nthat this information is largely encoded in the last few layers of a CNN.\nFinally, we show that the encoding of shape does not imply the encoding of\nlocalized per-pixel semantic information. The experimental results and findings\nprovide a more accurate understanding of the behaviour of current CNNs, thus\nhelping to inform future design choices.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.07406,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Initialization Using Perlin Noise for Training Networks with a Limited\n  Amount of Data\n\n  We propose a novel network initialization method using Perlin noise for\ntraining image classification networks with a limited amount of data. Our main\nidea is to initialize the network parameters by solving an artificial noise\nclassification problem, where the aim is to classify Perlin noise samples into\ntheir noise categories. Specifically, the proposed method consists of two\nsteps. First, it generates Perlin noise samples with category labels defined\nbased on noise complexity. Second, it solves a classification problem, in which\nnetwork parameters are optimized to classify the generated noise samples. This\nmethod produces a reasonable set of initial weights (filters) for image\nclassification. To the best of our knowledge, this is the first work to\ninitialize networks by solving an artificial optimization problem without using\nany real-world images. Our experiments show that the proposed method\noutperforms conventional initialization methods on four image classification\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09499,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Contrastive Prototype Learning with Augmented Embeddings for Few-Shot\n  Learning\n\n  Most recent few-shot learning (FSL) methods are based on meta-learning with\nepisodic training. In each meta-training episode, a discriminative feature\nembedding and\/or classifier are first constructed from a support set in an\ninner loop, and then evaluated in an outer loop using a query set for model\nupdating. This query set sample centered learning objective is however\nintrinsically limited in addressing the lack of training data problem in the\nsupport set. In this paper, a novel contrastive prototype learning with\naugmented embeddings (CPLAE) model is proposed to overcome this limitation.\nFirst, data augmentations are introduced to both the support and query sets\nwith each sample now being represented as an augmented embedding (AE) composed\nof concatenated embeddings of both the original and augmented versions. Second,\na novel support set class prototype centered contrastive loss is proposed for\ncontrastive prototype learning (CPL). With a class prototype as an anchor, CPL\naims to pull the query samples of the same class closer and those of different\nclasses further away. This support set sample centered loss is highly\ncomplementary to the existing query centered loss, fully exploiting the limited\ntraining data in each episode. Extensive experiments on several benchmarks\ndemonstrate that our proposed CPLAE achieves new state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.04544,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0003303422,
      "text":"Resolution-invariant Person ReID Based on Feature Transformation and\n  Self-weighted Attention\n\n  Person Re-identification (ReID) is a critical computer vision task which aims\nto match the same person in images or video sequences. Most current works focus\non settings where the resolution of images is kept the same. However, the\nresolution is a crucial factor in person ReID, especially when the cameras are\nat different distances from the person or the camera's models are different\nfrom each other. In this paper, we propose a novel two-stream network with a\nlightweight resolution association ReID feature transformation (RAFT) module\nand a self-weighted attention (SWA) ReID module to evaluate features under\ndifferent resolutions. RAFT transforms the low resolution features to\ncorresponding high resolution features. SWA evaluates both features to get\nweight factors for the person ReID. Both modules are jointly trained to get a\nresolution-invariant representation. Extensive experiments on five benchmark\ndatasets show the effectiveness of our method. For instance, we achieve Rank-1\naccuracy of 43.3% and 83.2% on CAVIAR and MLR-CUHK03, outperforming the\nstate-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.00359,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000231796,
      "text":"Video Captioning in Compressed Video\n\n  Existing approaches in video captioning concentrate on exploring global frame\nfeatures in the uncompressed videos, while the free of charge and critical\nsaliency information already encoded in the compressed videos is generally\nneglected. We propose a video captioning method which operates directly on the\nstored compressed videos. To learn a discriminative visual representation for\nvideo captioning, we design a residuals-assisted encoder (RAE), which spots\nregions of interest in I-frames under the assistance of the residuals frames.\nFirst, we obtain the spatial attention weights by extracting features of\nresiduals as the saliency value of each location in I-frame and design a\nspatial attention module to refine the attention weights. We further propose a\ntemporal gate module to determine how much the attended features contribute to\nthe caption generation, which enables the model to resist the disturbance of\nsome noisy signals in the compressed videos. Finally, Long Short-Term Memory is\nutilized to decode the visual representations into descriptions. We evaluate\nour method on two benchmark datasets and demonstrate the effectiveness of our\napproach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.05725,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Stereo camera system calibration: the need of two sets of parameters\n\n  The reconstruction of a scene via a stereo-camera system is a two-steps\nprocess, where at first images from different cameras are matched to identify\nthe set of point-to-point correspondences that then will actually be\nreconstructed in the three dimensional real world. The performance of the\nsystem strongly relies of the calibration procedure, which has to be carefully\ndesigned to guarantee optimal results. We implemented three different\ncalibration methods and we compared their performance over 19 datasets. We\npresent the experimental evidence that, due to the image noise, a single set of\nparameters is not sufficient to achieve high accuracy in the identification of\nthe correspondences and in the 3D reconstruction at the same time. We propose\nto calibrate the system twice to estimate two different sets of parameters: the\none obtained by minimizing the reprojection error that will be used when\ndealing with quantities defined in the 2D space of the cameras, and the one\nobtained by minimizing the reconstruction error that will be used when dealing\nwith quantities defined in the real 3D world.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06898,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000161264,
      "text":"What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space\n\n  Deep neural networks (DNNs) are increasingly deployed in different\napplications to achieve state-of-the-art performance. However, they are often\napplied as a black box with limited understanding of what knowledge the model\nhas learned from the data. In this paper, we focus on image classification and\npropose a method to visualize and understand the class-wise knowledge\n(patterns) learned by DNNs under three different settings including natural,\nbackdoor and adversarial. Different to existing visualization methods, our\nmethod searches for a single predictive pattern in the pixel space to represent\nthe knowledge learned by the model for each class. Based on the proposed\nmethod, we show that DNNs trained on natural (clean) data learn abstract shapes\nalong with some texture, and backdoored models learn a suspicious pattern for\nthe backdoored class. Interestingly, the phenomenon that DNNs can learn a\nsingle predictive pattern for each class indicates that DNNs can learn a\nbackdoor even from clean data, and the pattern itself is a backdoor trigger. In\nthe adversarial setting, we show that adversarially trained models tend to\nlearn more simplified shape patterns. Our method can serve as a useful tool to\nbetter understand the knowledge learned by DNNs on different datasets under\ndifferent settings.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06747,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Intestinal Parasites Classification Using Deep Belief Networks\n\n  Currently, approximately $4$ billion people are infected by intestinal\nparasites worldwide. Diseases caused by such infections constitute a public\nhealth problem in most tropical countries, leading to physical and mental\ndisorders, and even death to children and immunodeficient individuals. Although\nsubjected to high error rates, human visual inspection is still in charge of\nthe vast majority of clinical diagnoses. In the past years, some works\naddressed intelligent computer-aided intestinal parasites classification, but\nthey usually suffer from misclassification due to similarities between\nparasites and fecal impurities. In this paper, we introduce Deep Belief\nNetworks to the context of automatic intestinal parasites classification.\nExperiments conducted over three datasets composed of eggs, larvae, and\nprotozoa provided promising results, even considering unbalanced classes and\nalso fecal impurities.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.0445,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000115567,
      "text":"Two-stage CNN-based wood log recognition\n\n  The proof of origin of logs is becoming increasingly important. In the\ncontext of Industry 4.0 and to combat illegal logging there is an increasing\nmotivation to track each individual log. Our previous works in this field\nfocused on log tracking using digital log end images based on methods inspired\nby fingerprint and iris-recognition. This work presents a convolutional neural\nnetwork (CNN) based approach which comprises a CNN-based segmentation of the\nlog end combined with a final CNN-based recognition of the segmented log end\nusing the triplet loss function for CNN training. Results show that the\nproposed two-stage CNN-based approach outperforms traditional approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.05791,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"U-Noise: Learnable Noise Masks for Interpretable Image Segmentation\n\n  Deep Neural Networks (DNNs) are widely used for decision making in a myriad\nof critical applications, ranging from medical to societal and even judicial.\nGiven the importance of these decisions, it is crucial for us to be able to\ninterpret these models. We introduce a new method for interpreting image\nsegmentation models by learning regions of images in which noise can be applied\nwithout hindering downstream model performance. We apply this method to\nsegmentation of the pancreas in CT scans, and qualitatively compare the quality\nof the method to existing explainability techniques, such as Grad-CAM and\nocclusion sensitivity. Additionally we show that, unlike other methods, our\ninterpretability model can be quantitatively evaluated based on the downstream\nperformance over obscured images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.04935,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000508626,
      "text":"Single-path Bit Sharing for Automatic Loss-aware Model Compression\n\n  Network pruning and quantization are proven to be effective ways for deep\nmodel compression. To obtain a highly compact model, most methods first perform\nnetwork pruning and then conduct network quantization based on the pruned\nmodel. However, this strategy may ignore that they would affect each other and\nthus performing them separately may lead to sub-optimal performance. To address\nthis, performing pruning and quantization jointly is essential. Nevertheless,\nhow to make a trade-off between pruning and quantization is non-trivial.\nMoreover, existing compression methods often rely on some pre-defined\ncompression configurations. Some attempts have been made to search for optimal\nconfigurations, which however may take unbearable optimization cost. To address\nthe above issues, we devise a simple yet effective method named Single-path Bit\nSharing (SBS). Specifically, we first consider network pruning as a special\ncase of quantization, which provides a unified view for pruning and\nquantization. We then introduce a single-path model to encode all candidate\ncompression configurations. In this way, the configuration search problem is\ntransformed into a subset selection problem, which significantly reduces the\nnumber of parameters, computational cost and optimization difficulty. Relying\non the single-path model, we further introduce learnable binary gates to encode\nthe choice of bitwidth. By jointly training the binary gates in conjunction\nwith network parameters, the compression configurations of each layer can be\nautomatically determined. Extensive experiments on both CIFAR-100 and ImageNet\nshow that SBS is able to significantly reduce computational cost while\nachieving promising performance. For example, our SBS compressed MobileNetV2\nachieves 22.6x Bit-Operation (BOP) reduction with only 0.1% drop in the Top-1\naccuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.06407,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000113249,
      "text":"ACP: Automatic Channel Pruning via Clustering and Swarm Intelligence\n  Optimization for CNN\n\n  As the convolutional neural network (CNN) gets deeper and wider in recent\nyears, the requirements for the amount of data and hardware resources have\ngradually increased. Meanwhile, CNN also reveals salient redundancy in several\ntasks. The existing magnitude-based pruning methods are efficient, but the\nperformance of the compressed network is unpredictable. While the accuracy loss\nafter pruning based on the structure sensitivity is relatively slight, the\nprocess is time-consuming and the algorithm complexity is notable. In this\narticle, we propose a novel automatic channel pruning method (ACP).\nSpecifically, we firstly perform layer-wise channel clustering via the\nsimilarity of the feature maps to perform preliminary pruning on the network.\nThen a population initialization method is introduced to transform the pruned\nstructure into a candidate population. Finally, we conduct searching and\noptimizing iteratively based on the particle swarm optimization (PSO) to find\nthe optimal compressed structure. The compact network is then retrained to\nmitigate the accuracy loss from pruning. Our method is evaluated against\nseveral state-of-the-art CNNs on three different classification datasets\nCIFAR-10\/100 and ILSVRC-2012. On the ILSVRC-2012, when removing 64.36%\nparameters and 63.34% floating-point operations (FLOPs) of ResNet-50, the Top-1\nand Top-5 accuracy drop are less than 0.9%. Moreover, we demonstrate that\nwithout harming overall performance it is possible to compress SSD by more than\n50% on the target detection dataset PASCAL VOC. It further verifies that the\nproposed method can also be applied to other CNNs and application scenarios.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.05954,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Recent Advances in Video Question Answering: A Review of Datasets and\n  Methods\n\n  Video Question Answering (VQA) is a recent emerging challenging task in the\nfield of Computer Vision. Several visual information retrieval techniques like\nVideo Captioning\/Description and Video-guided Machine Translation have preceded\nthe task of VQA. VQA helps to retrieve temporal and spatial information from\nthe video scenes and interpret it. In this survey, we review a number of\nmethods and datasets for the task of VQA. To the best of our knowledge, no\nprevious survey has been conducted for the VQA task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.07299,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000040399,
      "text":"Diagnostic Captioning: A Survey\n\n  Diagnostic Captioning (DC) concerns the automatic generation of a diagnostic\ntext from a set of medical images of a patient collected during an examination.\nDC can assist inexperienced physicians, reducing clinical errors. It can also\nhelp experienced physicians produce diagnostic reports faster. Following the\nadvances of deep learning, especially in generic image captioning, DC has\nrecently attracted more attention, leading to several systems and datasets.\nThis article is an extensive overview of DC. It presents relevant datasets,\nevaluation measures, and up to date systems. It also highlights shortcomings\nthat hinder DC's progress and proposes future directions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.10511,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"Generic Event Boundary Detection: A Benchmark for Event Segmentation\n\n  This paper presents a novel task together with a new benchmark for detecting\ngeneric, taxonomy-free event boundaries that segment a whole video into chunks.\nConventional work in temporal video segmentation and action detection focuses\non localizing pre-defined action categories and thus does not scale to generic\nvideos. Cognitive Science has known since last century that humans consistently\nsegment videos into meaningful temporal chunks. This segmentation happens\nnaturally, without pre-defined event categories and without being explicitly\nasked to do so. Here, we repeat these cognitive experiments on mainstream CV\ndatasets; with our novel annotation guideline which addresses the complexities\nof taxonomy-free event boundary annotation, we introduce the task of Generic\nEvent Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our\nKinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8\nof EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event\nchange, and respect human perception diversity. We view GEBD as an important\nstepping stone towards understanding the video as a whole, and believe it has\nbeen previously neglected due to a lack of proper task definition and\nannotations. Through experiment and human study we demonstrate the value of the\nannotations. Further, we benchmark supervised and un-supervised GEBD approaches\non the TAPOS dataset and our Kinetics-GEBD. We release our annotations and\nbaseline codes at CVPR'21 LOVEU Challenge:\nhttps:\/\/sites.google.com\/view\/loveucvpr21.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.09658,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000048677,
      "text":"Computational Intelligence Approach to Improve the Classification\n  Accuracy of Brain Neoplasm in MRI Data\n\n  Automatic detection of brain neoplasm in Magnetic Resonance Imaging (MRI) is\ngaining importance in many medical diagnostic applications. This report\npresents two improvements for brain neoplasm detection in MRI data: an advanced\npreprocessing technique is proposed to improve the area of interest in MRI data\nand a hybrid technique using Convolutional Neural Network (CNN) for feature\nextraction followed by Support Vector Machine (SVM) for classification. The\nlearning algorithm for SVM is modified with the addition of cost function to\nminimize false positive prediction addressing the errors in MRI data diagnosis.\nThe proposed approach can effectively detect the presence of neoplasm and also\npredict whether it is cancerous (malignant) or non-cancerous (benign). To check\nthe effectiveness of the proposed preprocessing technique, it is inspected\nvisually and evaluated using training performance metrics. A comparison study\nbetween the proposed classification technique and the existing techniques was\nperformed. The result showed that the proposed approach outperformed in terms\nof accuracy and can handle errors in classification better than the existing\napproaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.07518,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000049671,
      "text":"BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring\n\n  Image motion blur results from a combination of object motions and camera\nshakes, and such blurring effect is generally directional and non-uniform.\nPrevious research attempted to solve non-uniform blurs using self-recurrent\nmultiscale, multi-patch, or multi-temporal architectures with self-attention to\nobtain decent results. However, using self-recurrent frameworks typically lead\nto a longer inference time, while inter-pixel or inter-channel self-attention\nmay cause excessive memory usage. This paper proposes a Blur-aware Attention\nNetwork (BANet), that accomplishes accurate and efficient deblurring via a\nsingle forward pass. Our BANet utilizes region-based self-attention with\nmulti-kernel strip pooling to disentangle blur patterns of different magnitudes\nand orientations and cascaded parallel dilated convolution to aggregate\nmulti-scale content features. Extensive experimental results on the GoPro and\nRealBlur benchmarks demonstrate that the proposed BANet performs favorably\nagainst the state-of-the-arts in blurred image restoration and can provide\ndeblurred results in real-time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.11002,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"New Algorithms for Computing Field of Vision over 2D Grids\n\n  The aim of this paper is to propose new algorithms for Field of Vision (FOV)\ncomputation which improve on existing work at high resolutions. FOV refers to\nthe set of locations that are visible from a specific position in a scene of a\ncomputer game.\n  We summarize existing algorithms for FOV computation, describe their\nlimitations, and present new algorithms which aim to address these limitations.\nWe first present an algorithm which makes use of spatial data structures in a\nway which is new for FOV calculation. We then present a novel technique which\nupdates a previously calculated FOV, rather than re-calculating an FOV from\nscratch.\n  We compare our algorithms to existing FOV algorithms and show they provide\nsubstantial improvements to running time. Our algorithms provide the largest\nimprovement over existing FOV algorithms at large grid sizes, thus allowing the\npossibility of the design of high resolution FOV-based video games.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2101.00932,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000263254,
      "text":"Weakly-Supervised Saliency Detection via Salient Object Subitizing\n\n  Salient object detection aims at detecting the most visually distinct objects\nand producing the corresponding masks. As the cost of pixel-level annotations\nis high, image tags are usually used as weak supervisions. However, an image\ntag can only be used to annotate one class of objects. In this paper, we\nintroduce saliency subitizing as the weak supervision since it is\nclass-agnostic. This allows the supervision to be aligned with the property of\nsaliency detection, where the salient objects of an image could be from more\nthan one class. To this end, we propose a model with two modules, Saliency\nSubitizing Module (SSM) and Saliency Updating Module (SUM). While SSM learns to\ngenerate the initial saliency masks using the subitizing information, without\nthe need for any unsupervised methods or some random seeds, SUM helps\niteratively refine the generated saliency masks. We conduct extensive\nexperiments on five benchmark datasets. The experimental results show that our\nmethod outperforms other weakly-supervised methods and even performs comparably\nto some fully-supervised methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09528,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Image Compositing for Segmentation of Surgical Tools without Manual\n  Annotations\n\n  Producing manual, pixel-accurate, image segmentation labels is tedious and\ntime-consuming. This is often a rate-limiting factor when large amounts of\nlabeled images are required, such as for training deep convolutional networks\nfor instrument-background segmentation in surgical scenes. No large datasets\ncomparable to industry standards in the computer vision community are available\nfor this task. To circumvent this problem, we propose to automate the creation\nof a realistic training dataset by exploiting techniques stemming from special\neffects and harnessing them to target training performance rather than visual\nappeal. Foreground data is captured by placing sample surgical instruments over\na chroma key (a.k.a. green screen) in a controlled environment, thereby making\nextraction of the relevant image segment straightforward. Multiple lighting\nconditions and viewpoints can be captured and introduced in the simulation by\nmoving the instruments and camera and modulating the light source. Background\ndata is captured by collecting videos that do not contain instruments. In the\nabsence of pre-existing instrument-free background videos, minimal labeling\neffort is required, just to select frames that do not contain surgical\ninstruments from videos of surgical interventions freely available online. We\ncompare different methods to blend instruments over tissue and propose a novel\ndata augmentation approach that takes advantage of the plurality of options. We\nshow that by training a vanilla U-Net on semi-synthetic data only and applying\na simple post-processing, we are able to match the results of the same network\ntrained on a publicly available manually labeled real dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.05105,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000199013,
      "text":"Urdu Handwritten Text Recognition Using ResNet18\n\n  Handwritten text recognition is an active research area in the field of deep\nlearning and artificial intelligence to convert handwritten text into\nmachine-understandable. A lot of work has been done for other languages,\nespecially for English, but work for the Urdu language is very minimal due to\nthe cursive nature of Urdu characters. The need for Urdu HCR systems is\nincreasing because of the advancement of technology. In this paper, we propose\na ResNet18 model for handwritten text recognition using Urdu Nastaliq\nHandwritten Dataset (UNHD) which contains 3,12000 words written by 500\ncandidates.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.09186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Hierarchical Attention Fusion for Geo-Localization\n\n  Geo-localization is a critical task in computer vision. In this work, we cast\nthe geo-localization as a 2D image retrieval task. Current state-of-the-art\nmethods for 2D geo-localization are not robust to locate a scene with drastic\nscale variations because they only exploit features from one semantic level for\nimage representations. To address this limitation, we introduce a hierarchical\nattention fusion network using multi-scale features for geo-localization. We\nextract the hierarchical feature maps from a convolutional neural network (CNN)\nand organically fuse the extracted features for image representations. Our\ntraining is self-supervised using adaptive weights to control the attention of\nfeature emphasis from each hierarchical level. Evaluation results on the image\nretrieval and the large-scale geo-localization benchmarks indicate that our\nmethod outperforms the existing state-of-the-art methods. Code is available\nhere: \\url{https:\/\/github.com\/YanLiqi\/HAF}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.05645,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Automated Video Labelling: Identifying Faces by Corroborative Evidence\n\n  We present a method for automatically labelling all faces in video archives,\nsuch as TV broadcasts, by combining multiple evidence sources and multiple\nmodalities (visual and audio). We target the problem of ever-growing online\nvideo archives, where an effective, scalable indexing solution cannot require a\nuser to provide manual annotation or supervision. To this end, we make three\nkey contributions: (1) We provide a novel, simple, method for determining if a\nperson is famous or not using image-search engines. In turn this enables a\nface-identity model to be built reliably and robustly, and used for high\nprecision automatic labelling; (2) We show that even for less-famous people,\nimage-search engines can then be used for corroborative evidence to accurately\nlabel faces that are named in the scene or the speech; (3) Finally, we\nquantitatively demonstrate the benefits of our approach on different video\ndomains and test settings, such as TV shows and news broadcasts. Our method\nworks across three disparate datasets without any explicit domain adaptation,\nand sets new state-of-the-art results on all the public benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11996,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000073512,
      "text":"On Relative Pose Recovery for Multi-Camera Systems\n\n  The point correspondence (PC) and affine correspondence (AC) are widely used\nfor relative pose estimation. An AC consists of a PC across two views and an\naffine transformation between the small patches around this PC. Previous work\ndemonstrates that one AC generally provides three independent constraints for\nrelative pose estimation. For multi-camera systems, there is still not any\nAC-based minimal solver for general relative pose estimation. To deal with this\nproblem, we propose a complete solution to relative pose estimation from two\nACs for multi-camera systems, consisting of a series of minimal solvers. The\nsolver generation in our solution is based on Cayley or quaternion\nparameterization for rotation and hidden variable technique to eliminate\ntranslation. This solver generation method is also naturally applied to\nrelative pose estimation from PCs, resulting in a new six-point method for\nmulti-camera systems. A few extensions are made, including relative pose\nestimation with known rotation angle and\/or with unknown focal lengths.\nExtensive experiments demonstrate that the proposed AC-based solvers and\nPC-based solvers are effective and efficient on synthetic and real-world\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10086,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000037418,
      "text":"Compact and adaptive multiplane images for view synthesis\n\n  Recently, learning methods have been designed to create Multiplane Images\n(MPIs) for view synthesis. While MPIs are extremely powerful and facilitate\nhigh quality renderings, a great amount of memory is required, making them\nimpractical for many applications. In this paper, we propose a learning method\nthat optimizes the available memory to render compact and adaptive MPIs. Our\nMPIs avoid redundant information and take into account the scene geometry to\ndetermine the depth sampling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04035,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000109937,
      "text":"In-game Residential Home Planning via Visual Context-aware Global\n  Relation Learning\n\n  In this paper, we propose an effective global relation learning algorithm to\nrecommend an appropriate location of a building unit for in-game customization\nof residential home complex. Given a construction layout, we propose a visual\ncontext-aware graph generation network that learns the implicit global\nrelations among the scene components and infers the location of a new building\nunit. The proposed network takes as input the scene graph and the corresponding\ntop-view depth image. It provides the location recommendations for a\nnewly-added building units by learning an auto-regressive edge distribution\nconditioned on existing scenes. We also introduce a global graph-image matching\nloss to enhance the awareness of essential geometry semantics of the site.\nQualitative and quantitative experiments demonstrate that the recommended\nlocation well reflects the implicit spatial rules of components in the\nresidential estates, and it is instructive and practical to locate the building\nunits in the 3D scene of the complex construction.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03141,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.00000553,
      "text":"CharacterGAN: Few-Shot Keypoint Character Animation and Reposing\n\n  We introduce CharacterGAN, a generative model that can be trained on only a\nfew samples (8 - 15) of a given character. Our model generates novel poses\nbased on keypoint locations, which can be modified in real time while providing\ninteractive feedback, allowing for intuitive reposing and animation. Since we\nonly have very limited training samples, one of the key challenges lies in how\nto address (dis)occlusions, e.g. when a hand moves behind or in front of a\nbody. To address this, we introduce a novel layering approach which explicitly\nsplits the input keypoints into different layers which are processed\nindependently. These layers represent different parts of the character and\nprovide a strong implicit bias that helps to obtain realistic results even with\nstrong (dis)occlusions. To combine the features of individual layers we use an\nadaptive scaling approach conditioned on all keypoints. Finally, we introduce a\nmask connectivity constraint to reduce distortion artifacts that occur with\nextreme out-of-distribution poses at test time. We show that our approach\noutperforms recent baselines and creates realistic animations for diverse\ncharacters. We also show that our model can handle discrete state changes, for\nexample a profile facing left or right, that the different layers do indeed\nlearn features specific for the respective keypoints in those layers, and that\nour model scales to larger datasets when more data is available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.04379,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000087751,
      "text":"End-to-end Generative Zero-shot Learning via Few-shot Learning\n\n  Contemporary state-of-the-art approaches to Zero-Shot Learning (ZSL) train\ngenerative nets to synthesize examples conditioned on the provided metadata.\nThereafter, classifiers are trained on these synthetic data in a supervised\nmanner. In this work, we introduce Z2FSL, an end-to-end generative ZSL\nframework that uses such an approach as a backbone and feeds its synthesized\noutput to a Few-Shot Learning (FSL) algorithm. The two modules are trained\njointly. Z2FSL solves the ZSL problem with a FSL algorithm, reducing, in\neffect, ZSL to FSL. A wide class of algorithms can be integrated within our\nframework. Our experimental results show consistent improvement over several\nbaselines. The proposed method, evaluated across standard benchmarks, shows\nstate-of-the-art or competitive performance in ZSL and Generalized ZSL tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10884,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Revisiting Classification Perspective on Scene Text Recognition\n\n  The prevalent perspectives of scene text recognition are from sequence to\nsequence (seq2seq) and segmentation. Nevertheless, the former is composed of\nmany components which makes implementation and deployment complicated, while\nthe latter requires character level annotations that is expensive. In this\npaper, we revisit classification perspective that models scene text recognition\nas an image classification problem. Classification perspective has a simple\npipeline and only needs word level annotations. We revive classification\nperspective by devising a scene text recognition model named as CSTR, which\nperforms as well as methods from other perspectives. The CSTR model consists of\nCPNet (classification perspective network) and SPPN (separated conv with global\naverage pooling prediction network). CSTR is as simple as image classification\nmodel like ResNet \\cite{he2016deep} which makes it easy to implement and\ndeploy. We demonstrate the effectiveness of the classification perspective on\nscene text recognition with extensive experiments. Futhermore, CSTR achieves\nnearly state-of-the-art performance on six public benchmarks including regular\ntext, irregular text. The code will be available at\nhttps:\/\/github.com\/Media-Smart\/vedastr.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.11731,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.000022418,
      "text":"Rethinking Natural Adversarial Examples for Classification Models\n\n  Recently, it was found that many real-world examples without intentional\nmodifications can fool machine learning models, and such examples are called\n\"natural adversarial examples\". ImageNet-A is a famous dataset of natural\nadversarial examples. By analyzing this dataset, we hypothesized that large,\ncluttered and\/or unusual background is an important reason why the images in\nthis dataset are difficult to be classified. We validated the hypothesis by\nreducing the background influence in ImageNet-A examples with object detection\ntechniques. Experiments showed that the object detection models with various\nclassification models as backbones obtained much higher accuracy than their\ncorresponding classification models. A detection model based on the\nclassification model EfficientNet-B7 achieved a top-1 accuracy of 53.95%,\nsurpassing previous state-of-the-art classification models trained on ImageNet,\nsuggesting that accurate localization information can significantly boost the\nperformance of classification models on ImageNet-A. We then manually cropped\nthe objects in images from ImageNet-A and created a new dataset, named\nImageNet-A-Plus. A human test on the new dataset showed that the deep\nlearning-based classifiers still performed quite poorly compared with humans.\nTherefore, the new dataset can be used to study the robustness of\nclassification models to the internal variance of objects without considering\nthe background disturbance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.12256,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"An Enhanced Prohibited Items Recognition Model\n\n  We proposed a new modeling method to promote the performance of prohibited\nitems recognition via X-ray image. We analyzed the characteristics of\nprohibited items and X-ray images. We found the fact that the scales of some\nitems are too small to be recognized which encumber the model performance. Then\nwe adopted a set of data augmentation and modified the model to adapt the field\nof prohibited items recognition. The Convolutional Block Attention Module(CBAM)\nand rescoring mechanism has been assembled into the model. By the modification,\nour model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.1152,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Automatic Ship Classification Utilizing Bag of Deep Features\n\n  Detection and classification of ships based on their silhouette profiles in\nnatural imagery is an important undertaking in computer science. This problem\ncan be viewed from a variety of perspectives, including security, traffic\ncontrol, and even militarism. Therefore, in each of the aforementioned\napplications, specific processing is required. In this paper, by applying the\n\"bag of words\" (BoW), a new method is presented that its words are the features\nthat are obtained using pre-trained models of deep convolutional networks. ,\nThree VGG models are utilized which provide superior accuracy in identifying\nobjects. The regions of the image that are selected as the initial proposals\nare derived from a greedy algorithm on the key points generated by the Scale\nInvariant Feature Transform (SIFT) method. Using the deep features in the BOW\nmethod provides a good improvement in the recognition and classification of\nships. Eventually, we obtained an accuracy of 91.8% in the classification of\nthe ships which shows the improvement of about 5% compared to previous methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.01285,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"GCF-Net: Gated Clip Fusion Network for Video Action Recognition\n\n  In recent years, most of the accuracy gains for video action recognition have\ncome from the newly designed CNN architectures (e.g., 3D-CNNs). These models\nare trained by applying a deep CNN on single clip of fixed temporal length.\nSince each video segment are processed by the 3D-CNN module separately, the\ncorresponding clip descriptor is local and the inter-clip relationships are\ninherently implicit. Common method that directly averages the clip-level\noutputs as a video-level prediction is prone to fail due to the lack of\nmechanism that can extract and integrate relevant information to represent the\nvideo.\n  In this paper, we introduce the Gated Clip Fusion Network (GCF-Net) that can\ngreatly boost the existing video action classifiers with the cost of a tiny\ncomputation overhead. The GCF-Net explicitly models the inter-dependencies\nbetween video clips to strengthen the receptive field of local clip\ndescriptors. Furthermore, the importance of each clip to an action event is\ncalculated and a relevant subset of clips is selected accordingly for a\nvideo-level analysis. On a large benchmark dataset (Kinetics-600), the proposed\nGCF-Net elevates the accuracy of existing action classifiers by 11.49% (based\non central clip) and 3.67% (based on densely sampled clips) respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.0146,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Learning to Segment Human Body Parts with Synthetically Trained Deep\n  Convolutional Networks\n\n  This paper presents a new framework for human body part segmentation based on\nDeep Convolutional Neural Networks trained using only synthetic data. The\nproposed approach achieves cutting-edge results without the need of training\nthe models with real annotated data of human body parts. Our contributions\ninclude a data generation pipeline, that exploits a game engine for the\ncreation of the synthetic data used for training the network, and a novel\npre-processing module, that combines edge response maps and adaptive histogram\nequalization to guide the network to learn the shape of the human body parts\nensuring robustness to changes in the illumination conditions. For selecting\nthe best candidate architecture, we perform exhaustive tests on manually\nannotated images of real human body limbs. We further compare our method\nagainst several high-end commercial segmentation tools on the body parts\nsegmentation task. The results show that our method outperforms the other\nmodels by a significant margin. Finally, we present an ablation study to\nvalidate our pre-processing module. With this paper, we release an\nimplementation of the proposed approach along with the acquired datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10928,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Escaping Poor Local Minima in Large Scale Robust Estimation\n\n  Robust parameter estimation is a crucial task in several 3D computer vision\npipelines such as Structure from Motion (SfM). State-of-the-art algorithms for\nrobust estimation, however, still suffer from difficulties in converging to\nsatisfactory solutions due to the presence of many poor local minima or flat\nregions in the optimization landscapes. In this paper, we introduce two novel\napproaches for robust parameter estimation. The first algorithm utilizes the\nFilter Method (FM), which is a framework for constrained optimization allowing\ngreat flexibility in algorithmic choices, to derive an adaptive kernel scaling\nstrategy that enjoys a strong ability to escape poor minima and achieves fast\nconvergence rates. Our second algorithm combines a generalized Majorization\nMinimization (GeMM) framework with the half-quadratic lifting formulation to\nobtain a simple yet efficient solver for robust estimation. We empirically show\nthat both proposed approaches show encouraging capability on avoiding poor\nlocal minima and achieve competitive results compared to existing state-of-the\nart robust fitting algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.10681,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Probabilistic Vehicle Reconstruction Using a Multi-Task CNN\n\n  The retrieval of the 3D pose and shape of objects from images is an ill-posed\nproblem. A common way to object reconstruction is to match entities such as\nkeypoints, edges, or contours of a deformable 3D model, used as shape prior, to\ntheir corresponding entities inferred from the image. However, such approaches\nare highly sensitive to model initialisation, imprecise keypoint localisations\nand\/or illumination conditions. In this paper, we present a probabilistic\napproach for shape-aware 3D vehicle reconstruction from stereo images that\nleverages the outputs of a novel multi-task CNN. Specifically, we train a CNN\nthat outputs probability distributions for the vehicle's orientation and for\nboth, vehicle keypoints and wireframe edges. Together with 3D stereo\ninformation we integrate the predicted distributions into a common\nprobabilistic framework. We believe that the CNN-based detection of wireframe\nedges reduces the sensitivity to illumination conditions and object contrast\nand that using the raw probability maps instead of inferring keypoint positions\nreduces the sensitivity to keypoint localisation errors. We show that our\nmethod achieves state-of-the-art results, evaluating our method on the\nchallenging KITTI benchmark and on our own new 'Stereo-Vehicle' dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.00798,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Landmark Breaker: Obstructing DeepFake By Disturbing Landmark Extraction\n\n  The recent development of Deep Neural Networks (DNN) has significantly\nincreased the realism of AI-synthesized faces, with the most notable examples\nbeing the DeepFakes. The DeepFake technology can synthesize a face of target\nsubject from a face of another subject, while retains the same face attributes.\nWith the rapidly increased social media portals (Facebook, Instagram, etc),\nthese realistic fake faces rapidly spread though the Internet, causing a broad\nnegative impact to the society. In this paper, we describe Landmark Breaker,\nthe first dedicated method to disrupt facial landmark extraction, and apply it\nto the obstruction of the generation of DeepFake videos.Our motivation is that\ndisrupting the facial landmark extraction can affect the alignment of input\nface so as to degrade the DeepFake quality. Our method is achieved using\nadversarial perturbations. Compared to the detection methods that only work\nafter DeepFake generation, Landmark Breaker goes one step ahead to prevent\nDeepFake generation. The experiments are conducted on three state-of-the-art\nfacial landmark extractors using the recent Celeb-DF dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.03586,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000105633,
      "text":"CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression\n  LSTM for Predictive Learning\n\n  Spatiotemporal predictive learning (ST-PL) is a hotspot with numerous\napplications, such as object movement and meteorological prediction. It aims at\npredicting the subsequent frames via observed sequences. However, inherent\nuncertainty among consecutive frames exacerbates the difficulty in long-term\nprediction. To tackle the increasing ambiguity during forecasting, we design\nCMS-LSTM to focus on context correlations and multi-scale spatiotemporal flow\nwith details on fine-grained locals, containing two elaborate designed blocks:\nContext Embedding (CE) and Spatiotemporal Expression (SE) blocks. CE is\ndesigned for abundant context interactions, while SE focuses on multi-scale\nspatiotemporal expression in hidden states. The newly introduced blocks also\nfacilitate other spatiotemporal models (e.g., PredRNN, SA-ConvLSTM) to produce\nrepresentative implicit features for ST-PL and improve prediction quality.\nQualitative and quantitative experiments demonstrate the effectiveness and\nflexibility of our proposed method. With fewer params, CMS-LSTM outperforms\nstate-of-the-art methods in numbers of metrics on two representative benchmarks\nand scenarios. Code is available at https:\/\/github.com\/czh-98\/CMS-LSTM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2102.0882,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Crop mapping from image time series: deep learning with multi-scale\n  label hierarchies\n\n  The aim of this paper is to map agricultural crops by classifying satellite\nimage time series. Domain experts in agriculture work with crop type labels\nthat are organised in a hierarchical tree structure, where coarse classes (like\norchards) are subdivided into finer ones (like apples, pears, vines, etc.). We\ndevelop a crop classification method that exploits this expert knowledge and\nsignificantly improves the mapping of rare crop types. The three-level label\nhierarchy is encoded in a convolutional, recurrent neural network (convRNN),\nsuch that for each pixel the model predicts three labels at different level of\ngranularity. This end-to-end trainable, hierarchical network architecture\nallows the model to learn joint feature representations of rare classes (e.g.,\napples, pears) at a coarser level (e.g., orchard), thereby boosting\nclassification performance at the fine-grained level. Additionally, labelling\nat different granularity also makes it possible to adjust the output according\nto the classification scores; as coarser labels with high confidence are\nsometimes more useful for agricultural practice than fine-grained but very\nuncertain labels. We validate the proposed method on a new, large dataset that\nwe make public. ZueriCrop covers an area of 50 km x 48 km in the Swiss cantons\nof Zurich and Thurgau with a total of 116'000 individual fields spanning 48\ncrop classes, and 28,000 (multi-temporal) image patches from Sentinel-2. We\ncompare our proposed hierarchical convRNN model with several baselines,\nincluding methods designed for imbalanced class distributions. The hierarchical\napproach performs superior by at least 9.9 percentage points in F1-score.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.03027,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Modeling Multi-Label Action Dependencies for Temporal Action\n  Localization\n\n  Real-world videos contain many complex actions with inherent relationships\nbetween action classes. In this work, we propose an attention-based\narchitecture that models these action relationships for the task of temporal\naction localization in untrimmed videos. As opposed to previous works that\nleverage video-level co-occurrence of actions, we distinguish the relationships\nbetween actions that occur at the same time-step and actions that occur at\ndifferent time-steps (i.e. those which precede or follow each other). We define\nthese distinct relationships as action dependencies. We propose to improve\naction localization performance by modeling these action dependencies in a\nnovel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer\nconsists of two branches: a Co-occurrence Dependency Branch and a Temporal\nDependency Branch to model co-occurrence action dependencies and temporal\naction dependencies, respectively. We observe that existing metrics used for\nmulti-label classification do not explicitly measure how well action\ndependencies are modeled, therefore, we propose novel metrics that consider\nboth co-occurrence and temporal dependencies between action classes. Through\nempirical evaluation and extensive analysis, we show improved performance over\nstate-of-the-art methods on multi-label action localization\nbenchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08064,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Versailles-FP dataset: Wall Detection in Ancient\n\n  Access to historical monuments' floor plans over a time period is necessary\nto understand the architectural evolution and history. Such knowledge bases\nalso helps to rebuild the history by establishing connection between different\nevent, person and facts which are once part of the buildings. Since the\ntwo-dimensional plans do not capture the entire space, 3D modeling sheds new\nlight on the reading of these unique archives and thus opens up great\nperspectives for understanding the ancient states of the monument. Since the\nfirst step in the building's or monument's 3D model is the wall detection in\nthe floor plan, we introduce in this paper the new and unique Versailles FP\ndataset of wall groundtruthed images of the Versailles Palace dated between\n17th and 18th century. The dataset's wall masks are generated using an\nautomatic approach based on multi directional steerable filters. The generated\nwall masks are then validated and corrected manually. We validate our approach\nof wall mask generation in state-of-the-art modern datasets. Finally we propose\na U net based convolutional framework for wall detection. Our method achieves\nstate of the art result surpassing fully connected network based approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.00841,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Learning Frequency Domain Approximation for Binary Neural Networks\n\n  Binary neural networks (BNNs) represent original full-precision weights and\nactivations into 1-bit with sign function. Since the gradient of the\nconventional sign function is almost zero everywhere which cannot be used for\nback-propagation, several attempts have been proposed to alleviate the\noptimization difficulty by using approximate gradient. However, those\napproximations corrupt the main direction of factual gradient. To this end, we\npropose to estimate the gradient of sign function in the Fourier frequency\ndomain using the combination of sine functions for training BNNs, namely\nfrequency domain approximation (FDA). The proposed approach does not affect the\nlow-frequency information of the original sign function which occupies most of\nthe overall energy, and high-frequency coefficients will be ignored to avoid\nthe huge computational overhead. In addition, we embed a noise adaptation\nmodule into the training phase to compensate the approximation error. The\nexperiments on several benchmark datasets and neural architectures illustrate\nthat the binary network learned using our method achieves the state-of-the-art\naccuracy. Code will be available at\n\\textit{https:\/\/gitee.com\/mindspore\/models\/tree\/master\/research\/cv\/FDA-BNN}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08213,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000793669,
      "text":"Cascaded Feature Warping Network for Unsupervised Medical Image\n  Registration\n\n  Deformable image registration is widely utilized in medical image analysis,\nbut most proposed methods fail in the situation of complex deformations. In\nthis paper, we pre-sent a cascaded feature warping network to perform the\ncoarse-to-fine registration. To achieve this, a shared-weights encoder network\nis adopted to generate the feature pyramids for the unaligned images. The\nfeature warping registration module is then used to estimate the deformation\nfield at each level. The coarse-to-fine manner is implemented by cascading the\nmodule from the bottom level to the top level. Furthermore, the multi-scale\nloss is also introduced to boost the registration performance. We employ two\npublic benchmark datasets and conduct various experiments to evaluate our\nmethod. The results show that our method outperforms the state-of-the-art\nmethods, which also demonstrates that the cascaded feature warping network can\nperform the coarse-to-fine registration effectively and efficiently.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.10895,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and\n  Benchmark\n\n  Perhaps surprisingly sewerage infrastructure is one of the most costly\ninfrastructures in modern society. Sewer pipes are manually inspected to\ndetermine whether the pipes are defective. However, this process is limited by\nthe number of qualified inspectors and the time it takes to inspect a pipe.\nAutomatization of this process is therefore of high interest. So far, the\nsuccess of computer vision approaches for sewer defect classification has been\nlimited when compared to the success in other fields mainly due to the lack of\npublic datasets. To this end, in this work we present a large novel and\npublicly available multi-label classification dataset for image-based sewer\ndefect classification called Sewer-ML.\n  The Sewer-ML dataset consists of 1.3 million images annotated by professional\nsewer inspectors from three different utility companies across nine years.\nTogether with the dataset, we also present a benchmark algorithm and a novel\nmetric for assessing performance. The benchmark algorithm is a result of\nevaluating 12 state-of-the-art algorithms, six from the sewer defect\nclassification domain and six from the multi-label classification domain, and\ncombining the best performing algorithms. The novel metric is a\nclass-importance weighted F2 score, $\\text{F}2_{\\text{CIW}}$, reflecting the\neconomic impact of each class, used together with the normal pipe F1 score,\n$\\text{F}1_{\\text{Normal}}$. The benchmark algorithm achieves an\n$\\text{F}2_{\\text{CIW}}$ score of 55.11% and $\\text{F}1_{\\text{Normal}}$ score\nof 90.94%, leaving ample room for improvement on the Sewer-ML dataset. The\ncode, models, and dataset are available at the project page\nhttps:\/\/vap.aau.dk\/sewer-ml\/\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01451,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Explainable Person Re-Identification with Attribute-guided Metric\n  Distillation\n\n  Despite the great progress of person re-identification (ReID) with the\nadoption of Convolutional Neural Networks, current ReID models are opaque and\nonly outputs a scalar distance between two persons. There are few methods\nproviding users semantically understandable explanations for why two persons\nare the same one or not. In this paper, we propose a post-hoc method, named\nAttribute-guided Metric Distillation (AMD), to explain existing ReID models.\nThis is the first method to explore attributes to answer: 1) what and where the\nattributes make two persons different, and 2) how much each attribute\ncontributes to the difference. In AMD, we design a pluggable interpreter\nnetwork for target models to generate quantitative contributions of attributes\nand visualize accurate attention maps of the most discriminative attributes. To\nachieve this goal, we propose a metric distillation loss by which the\ninterpreter learns to decompose the distance of two persons into components of\nattributes with knowledge distilled from the target model. Moreover, we propose\nan attribute prior loss to make the interpreter generate attribute-guided\nattention maps and to eliminate biases caused by the imbalanced distribution of\nattributes. This loss can guide the interpreter to focus on the exclusive and\ndiscriminative attributes rather than the large-area but common attributes of\ntwo persons. Comprehensive experiments show that the interpreter can generate\neffective and intuitive explanations for varied models and generalize well\nunder cross-domain settings. As a by-product, the accuracy of target models can\nbe further improved with our interpreter.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.14803,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Face Transformer for Recognition\n\n  Recently there has been a growing interest in Transformer not only in NLP but\nalso in computer vision. We wonder if transformer can be used in face\nrecognition and whether it is better than CNNs. Therefore, we investigate the\nperformance of Transformer models in face recognition. Considering the original\nTransformer may neglect the inter-patch information, we modify the patch\ngeneration process and make the tokens with sliding patches which overlaps with\neach others. The models are trained on CASIA-WebFace and MS-Celeb-1M databases,\nand evaluated on several mainstream benchmarks, including LFW, SLLFW, CALFW,\nCPLFW, TALFW, CFP-FP, AGEDB and IJB-C databases. We demonstrate that Face\nTransformer models trained on a large-scale database, MS-Celeb-1M, achieve\ncomparable performance as CNN with similar number of parameters and MACs. To\nfacilitate further researches, Face Transformer models and codes are available\nat https:\/\/github.com\/zhongyy\/Face-Transformer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.10559,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"CDFI: Compression-Driven Network Design for Frame Interpolation\n\n  DNN-based frame interpolation--that generates the intermediate frames given\ntwo consecutive frames--typically relies on heavy model architectures with a\nhuge number of features, preventing them from being deployed on systems with\nlimited resources, e.g., mobile devices. We propose a compression-driven\nnetwork design for frame interpolation (CDFI), that leverages model pruning\nthrough sparsity-inducing optimization to significantly reduce the model size\nwhile achieving superior performance. Concretely, we first compress the\nrecently proposed AdaCoF model and show that a 10X compressed AdaCoF performs\nsimilarly as its original counterpart; then we further improve this compressed\nmodel by introducing a multi-resolution warping module, which boosts visual\nconsistencies with multi-level details. As a consequence, we achieve a\nsignificant performance gain with only a quarter in size compared with the\noriginal AdaCoF. Moreover, our model performs favorably against other\nstate-of-the-arts in a broad range of datasets. Finally, the proposed\ncompression-driven framework is generic and can be easily transferred to other\nDNN-based frame interpolation algorithm. Our source code is available at\nhttps:\/\/github.com\/tding1\/CDFI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.0457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000057949,
      "text":"Differentiable Multi-Granularity Human Representation Learning for\n  Instance-Aware Human Semantic Parsing\n\n  To address the challenging task of instance-aware human part parsing, a new\nbottom-up regime is proposed to learn category-level human semantic\nsegmentation as well as multi-person pose estimation in a joint and end-to-end\nmanner. It is a compact, efficient and powerful framework that exploits\nstructural information over different human granularities and eases the\ndifficulty of person partitioning. Specifically, a dense-to-sparse projection\nfield, which allows explicitly associating dense human semantics with sparse\nkeypoints, is learnt and progressively improved over the network feature\npyramid for robustness. Then, the difficult pixel grouping problem is cast as\nan easier, multi-person joint assembling task. By formulating joint association\nas maximum-weight bipartite matching, a differentiable solution is developed to\nexploit projected gradient descent and Dykstra's cyclic projection algorithm.\nThis makes our method end-to-end trainable and allows back-propagating the\ngrouping error to directly supervise multi-granularity human representation\nlearning. This is distinguished from current bottom-up human parsers or pose\nestimators which require sophisticated post-processing or heuristic greedy\nalgorithms. Experiments on three instance-aware human parsing datasets show\nthat our model outperforms other bottom-up alternatives with much more\nefficient inference.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.07531,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000070201,
      "text":"Uncertainty-guided Model Generalization to Unseen Domains\n\n  We study a worst-case scenario in generalization: Out-of-domain\ngeneralization from a single source. The goal is to learn a robust model from a\nsingle source and expect it to generalize over many unknown distributions. This\nchallenging problem has been seldom investigated while existing solutions\nsuffer from various limitations. In this paper, we propose a new solution. The\nkey idea is to augment the source capacity in both input and label spaces,\nwhile the augmentation is guided by uncertainty assessment. To the best of our\nknowledge, this is the first work to (1) access the generalization uncertainty\nfrom a single source and (2) leverage it to guide both input and label\naugmentation for robust generalization. The model training and deployment are\neffectively organized in a Bayesian meta-learning framework. We conduct\nextensive comparisons and ablation study to validate our approach. The results\nprove our superior performance in a wide scope of tasks including image\nclassification, semantic segmentation, text classification, and speech\nrecognition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.16449,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005298,
      "text":"Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction\n\n  This paper considers a new problem of adapting a pre-trained model of human\nmesh reconstruction to out-of-domain streaming videos. However, most previous\nmethods based on the parametric SMPL model \\cite{loper2015smpl} underperform in\nnew domains with unexpected, domain-specific attributes, such as camera\nparameters, lengths of bones, backgrounds, and occlusions. Our general idea is\nto dynamically fine-tune the source model on test video streams with additional\ntemporal constraints, such that it can mitigate the domain gaps without\nover-fitting the 2D information of individual test frames. A subsequent\nchallenge is how to avoid conflicts between the 2D and temporal constraints. We\npropose to tackle this problem using a new training algorithm named Bilevel\nOnline Adaptation (BOA), which divides the optimization process of overall\nmulti-objective into two steps of weight probe and weight update in a training\niteration. We demonstrate that BOA leads to state-of-the-art results on two\nhuman mesh reconstruction benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15808,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"CvT: Introducing Convolutions to Vision Transformers\n\n  We present in this paper a new architecture, named Convolutional vision\nTransformer (CvT), that improves Vision Transformer (ViT) in performance and\nefficiency by introducing convolutions into ViT to yield the best of both\ndesigns. This is accomplished through two primary modifications: a hierarchy of\nTransformers containing a new convolutional token embedding, and a\nconvolutional Transformer block leveraging a convolutional projection. These\nchanges introduce desirable properties of convolutional neural networks (CNNs)\nto the ViT architecture (\\ie shift, scale, and distortion invariance) while\nmaintaining the merits of Transformers (\\ie dynamic attention, global context,\nand better generalization). We validate CvT by conducting extensive\nexperiments, showing that this approach achieves state-of-the-art performance\nover other Vision Transformers and ResNets on ImageNet-1k, with fewer\nparameters and lower FLOPs. In addition, performance gains are maintained when\npretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream\ntasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of\n87.7\\% on the ImageNet-1k val set. Finally, our results show that the\npositional encoding, a crucial component in existing Vision Transformers, can\nbe safely removed in our model, simplifying the design for higher resolution\nvision tasks. Code will be released at \\url{https:\/\/github.com\/leoxiaobin\/CvT}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.13744,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000039405,
      "text":"KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs\n\n  NeRF synthesizes novel views of a scene with unprecedented quality by fitting\na neural radiance field to RGB images. However, NeRF requires querying a deep\nMulti-Layer Perceptron (MLP) millions of times, leading to slow rendering\ntimes, even on modern GPUs. In this paper, we demonstrate that real-time\nrendering is possible by utilizing thousands of tiny MLPs instead of one single\nlarge MLP. In our setting, each individual MLP only needs to represent parts of\nthe scene, thus smaller and faster-to-evaluate MLPs can be used. By combining\nthis divide-and-conquer strategy with further optimizations, rendering is\naccelerated by three orders of magnitude compared to the original NeRF model\nwithout incurring high storage costs. Further, using teacher-student\ndistillation for training, we show that this speed-up can be achieved without\nsacrificing visual quality.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.01039,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000032783,
      "text":"Self-Supervised Simultaneous Multi-Step Prediction of Road Dynamics and\n  Cost Map\n\n  While supervised learning is widely used for perception modules in\nconventional autonomous driving solutions, scalability is hindered by the huge\namount of data labeling needed. In contrast, while end-to-end architectures do\nnot require labeled data and are potentially more scalable, interpretability is\nsacrificed. We introduce a novel architecture that is trained in a fully\nself-supervised fashion for simultaneous multi-step prediction of space-time\ncost map and road dynamics. Our solution replaces the manually designed cost\nfunction for motion planning with a learned high dimensional cost map that is\nnaturally interpretable and allows diverse contextual information to be\nintegrated without manual data labeling. Experiments on real world driving data\nshow that our solution leads to lower number of collisions and road violations\nin long planning horizons in comparison to baselines, demonstrating the\nfeasibility of fully self-supervised prediction without sacrificing either\nscalability or interpretability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.12459,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Dual Mesh Convolutional Networks for Human Shape Correspondence\n\n  Convolutional networks have been extremely successful for regular data\nstructures such as 2D images and 3D voxel grids. The transposition to meshes\nis, however, not straight-forward due to their irregular structure. We explore\nhow the dual, face-based representation of triangular meshes can be leveraged\nas a data structure for graph convolutional networks. In the dual mesh, each\nnode (face) has a fixed number of neighbors, which makes the networks less\nsusceptible to overfitting on the mesh topology, and also al-lows the use of\ninput features that are naturally defined over faces, such as surface normals\nand face areas. We evaluate the dual approach on the shape correspondence task\non theFaust human shape dataset and variants of it with differ-ent mesh\ntopologies. Our experiments show that results of graph convolutional networks\nimprove when defined over the dual rather than primal mesh. Moreover, our\nmodels that explicitly leverage the neighborhood regularity of dual meshes\nallow improving results further while being more robust to changes in the mesh\ntopology.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.08292,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000049671,
      "text":"Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging\n\n  Under mild conditions on the noise level of the measurements, rotation\naveraging satisfies strong duality, which enables global solutions to be\nobtained via semidefinite programming (SDP) relaxation. However, generic\nsolvers for SDP are rather slow in practice, even on rotation averaging\ninstances of moderate size, thus developing specialised algorithms is vital. In\nthis paper, we present a fast algorithm that achieves global optimality called\nrotation coordinate descent (RCD). Unlike block coordinate descent (BCD) which\nsolves SDP by updating the semidefinite matrix in a row-by-row fashion, RCD\ndirectly maintains and updates all valid rotations throughout the iterations.\nThis obviates the need to store a large dense semidefinite matrix. We\nmathematically prove the convergence of our algorithm and empirically show its\nsuperior efficiency over state-of-the-art global methods on a variety of\nproblem configurations. Maintaining valid rotations also facilitates\nincorporating local optimisation routines for further speed-ups. Moreover, our\nalgorithm is simple to implement; see supplementary material for a\ndemonstration program.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.16024,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000053313,
      "text":"Augmented Transformer with Adaptive Graph for Temporal Action Proposal\n  Generation\n\n  Temporal action proposal generation (TAPG) is a fundamental and challenging\ntask in video understanding, especially in temporal action detection. Most\nprevious works focus on capturing the local temporal context and can well\nlocate simple action instances with clean frames and clear boundaries. However,\nthey generally fail in complicated scenarios where interested actions involve\nirrelevant frames and background clutters, and the local temporal context\nbecomes less effective. To deal with these problems, we present an augmented\ntransformer with adaptive graph network (ATAG) to exploit both long-range and\nlocal temporal contexts for TAPG. Specifically, we enhance the vanilla\ntransformer by equipping a snippet actionness loss and a front block, dubbed\naugmented transformer, and it improves the abilities of capturing long-range\ndependencies and learning robust feature for noisy action instances.Moreover,\nan adaptive graph convolutional network (GCN) is proposed to build local\ntemporal context by mining the position information and difference between\nadjacent features. The features from the two modules carry rich semantic\ninformation of the video, and are fused for effective sequential proposal\ngeneration. Extensive experiments are conducted on two challenging datasets,\nTHUMOS14 and ActivityNet1.3, and the results demonstrate that our method\noutperforms state-of-the-art TAPG methods. Our code will be released soon.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.12346,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Co-Grounding Networks with Semantic Attention for Referring Expression\n  Comprehension in Videos\n\n  In this paper, we address the problem of referring expression comprehension\nin videos, which is challenging due to complex expression and scene dynamics.\nUnlike previous methods which solve the problem in multiple stages (i.e.,\ntracking, proposal-based matching), we tackle the problem from a novel\nperspective, \\textbf{co-grounding}, with an elegant one-stage framework. We\nenhance the single-frame grounding accuracy by semantic attention learning and\nimprove the cross-frame grounding consistency with co-grounding feature\nlearning. Semantic attention learning explicitly parses referring cues in\ndifferent attributes to reduce the ambiguity in the complex expression.\nCo-grounding feature learning boosts visual feature representations by\nintegrating temporal correlation to reduce the ambiguity caused by scene\ndynamics. Experiment results demonstrate the superiority of our framework on\nthe video grounding datasets VID and LiOTB in generating accurate and stable\nresults across frames. Our model is also applicable to referring expression\ncomprehension in images, illustrated by the improved performance on the RefCOCO\ndataset. Our project is available at https:\/\/sijiesong.github.io\/co-grounding.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.02394,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000084109,
      "text":"Self-Distribution Binary Neural Networks\n\n  In this work, we study the binary neural networks (BNNs) of which both the\nweights and activations are binary (i.e., 1-bit representation). Feature\nrepresentation is critical for deep neural networks, while in BNNs, the\nfeatures only differ in signs. Prior work introduces scaling factors into\nbinary weights and activations to reduce the quantization error and effectively\nimproves the classification accuracy of BNNs. However, the scaling factors not\nonly increase the computational complexity of networks, but also make no sense\nto the signs of binary features. To this end, Self-Distribution Binary Neural\nNetwork (SD-BNN) is proposed. Firstly, we utilize Activation Self Distribution\n(ASD) to adaptively adjust the sign distribution of activations, thereby\nimprove the sign differences of the outputs of the convolution. Secondly, we\nadjust the sign distribution of weights through Weight Self Distribution (WSD)\nand then fine-tune the sign distribution of the outputs of the convolution.\nExtensive experiments on CIFAR-10 and ImageNet datasets with various network\nstructures show that the proposed SD-BNN consistently outperforms the\nstate-of-the-art (SOTA) BNNs (e.g., achieves 92.5% on CIFAR-10 and 66.5% on\nImageNet with ResNet-18) with less computation cost. Code is available at\nhttps:\/\/github.com\/ pingxue-hfut\/SD-BNN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2103.15331,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000076029,
      "text":"POSEFusion: Pose-guided Selective Fusion for Single-view Human\n  Volumetric Capture\n\n  We propose POse-guided SElective Fusion (POSEFusion), a single-view human\nvolumetric capture method that leverages tracking-based methods and\ntracking-free inference to achieve high-fidelity and dynamic 3D reconstruction.\nBy contributing a novel reconstruction framework which contains pose-guided\nkeyframe selection and robust implicit surface fusion, our method fully\nutilizes the advantages of both tracking-based methods and tracking-free\ninference methods, and finally enables the high-fidelity reconstruction of\ndynamic surface details even in the invisible regions. We formulate the\nkeyframe selection as a dynamic programming problem to guarantee the temporal\ncontinuity of the reconstructed sequence. Moreover, the novel robust implicit\nsurface fusion involves an adaptive blending weight to preserve high-fidelity\nsurface details and an automatic collision handling method to deal with the\npotential self-collisions. Overall, our method enables high-fidelity and\ndynamic capture in both visible and invisible regions from a single RGBD\ncamera, and the results and experiments show that our method outperforms\nstate-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.09993,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000034438,
      "text":"Fine-grained Anomaly Detection via Multi-task Self-Supervision\n\n  Detecting anomalies using deep learning has become a major challenge over the\nlast years, and is becoming increasingly promising in several fields. The\nintroduction of self-supervised learning has greatly helped many methods\nincluding anomaly detection where simple geometric transformation recognition\ntasks are used. However these methods do not perform well on fine-grained\nproblems since they lack finer features. By combining in a multi-task framework\nhigh-scale shape features oriented task with low-scale fine features oriented\ntask, our method greatly improves fine-grained anomaly detection. It\noutperforms state-of-the-art with up to 31% relative error reduction measured\nwith AUROC on various anomaly detection problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.00633,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering\n\n  We present RePOSE, a fast iterative refinement method for 6D object pose\nestimation. Prior methods perform refinement by feeding zoomed-in input and\nrendered RGB images into a CNN and directly regressing an update of a refined\npose. Their runtime is slow due to the computational cost of CNN, which is\nespecially prominent in multiple-object pose refinement. To overcome this\nproblem, RePOSE leverages image rendering for fast feature extraction using a\n3D model with a learnable texture. We call this deep texture rendering, which\nuses a shallow multi-layer perceptron to directly regress a view-invariant\nimage representation of an object. Furthermore, we utilize differentiable\nLevenberg-Marquardt (LM) optimization to refine a pose fast and accurately by\nminimizing the feature-metric error between the input and rendered image\nrepresentations without the need of zooming in. These image representations are\ntrained such that differentiable LM optimization converges within few\niterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art\naccuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute\nimprovement over the prior art, and comparable result on the YCB-Video dataset\nwith a much faster runtime. The code is available at\nhttps:\/\/github.com\/sh8\/repose.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0173,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"Training Deep Neural Networks via Branch-and-Bound\n\n  In this paper, we propose BPGrad, a novel approximate algorithm for deep\nnueral network training, based on adaptive estimates of feasible region via\nbranch-and-bound. The method is based on the assumption of Lipschitz continuity\nin objective function, and as a result, it can adaptively determine the step\nsize for the current gradient given the history of previous updates. We prove\nthat, by repeating such a branch-and-pruning procedure, it can achieve the\noptimal solution within finite iterations. A computationally efficient solver\nbased on BPGrad has been proposed to train the deep neural networks. Empirical\nresults demonstrate that BPGrad solver works well in practice and compares\nfavorably to other stochastic optimization methods in the tasks of object\nrecognition, detection, and segmentation. The code is available at\n\\url{https:\/\/github.com\/RyanCV\/BPGrad}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11288,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000039405,
      "text":"H-Net: Unsupervised Attention-based Stereo Depth Estimation Leveraging\n  Epipolar Geometry\n\n  Depth estimation from a stereo image pair has become one of the most explored\napplications in computer vision, with most of the previous methods relying on\nfully supervised learning settings. However, due to the difficulty in acquiring\naccurate and scalable ground truth data, the training of fully supervised\nmethods is challenging. As an alternative, self-supervised methods are becoming\nmore popular to mitigate this challenge. In this paper, we introduce the H-Net,\na deep-learning framework for unsupervised stereo depth estimation that\nleverages epipolar geometry to refine stereo matching. For the first time, a\nSiamese autoencoder architecture is used for depth estimation which allows\nmutual information between the rectified stereo images to be extracted. To\nenforce the epipolar constraint, the mutual epipolar attention mechanism has\nbeen designed which gives more emphasis to correspondences of features which\nlie on the same epipolar line while learning mutual information between the\ninput stereo pair. Stereo correspondences are further enhanced by incorporating\nsemantic information to the proposed attention mechanism. More specifically,\nthe optimal transport algorithm is used to suppress attention and eliminate\noutliers in areas not visible in both cameras. Extensive experiments on\nKITTI2015 and Cityscapes show that our method outperforms the state-ofthe-art\nunsupervised stereo depth estimation methods while closing the gap with the\nfully supervised approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.05668,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000069208,
      "text":"Learning Robust Visual-semantic Mapping for Zero-shot Learning\n\n  Zero-shot learning (ZSL) aims at recognizing unseen class examples (e.g.,\nimages) with knowledge transferred from seen classes. This is typically\nachieved by exploiting a semantic feature space shared by both seen and unseen\nclasses, e.g., attributes or word vectors, as the bridge. In ZSL, the common\npractice is to train a mapping function between the visual and semantic feature\nspaces with labeled seen class examples. When inferring, given unseen class\nexamples, the learned mapping function is reused to them and recognizes the\nclass labels on some metrics among their semantic relations. However, the\nvisual and semantic feature spaces are generally independent and exist in\nentirely different manifolds. Under such a paradigm, the ZSL models may easily\nsuffer from the domain shift problem when constructing and reusing the mapping\nfunction, which becomes the major challenge in ZSL. In this thesis, we explore\neffective ways to mitigate the domain shift problem and learn a robust mapping\nfunction between the visual and semantic feature spaces. We focus on fully\nempowering the semantic feature space, which is one of the key building blocks\nof ZSL. In summary, this thesis targets fully empowering the semantic feature\nspace and design effective solutions to mitigate the domain shift problem and\nhence obtain a more robust visual-semantic mapping function for ZSL. Extensive\nexperiments on various datasets demonstrate the effectiveness of our proposed\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.01735,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000072519,
      "text":"A Dual-Critic Reinforcement Learning Framework for Frame-level Bit\n  Allocation in HEVC\/H.265\n\n  This paper introduces a dual-critic reinforcement learning (RL) framework to\naddress the problem of frame-level bit allocation in HEVC\/H.265. The objective\nis to minimize the distortion of a group of pictures (GOP) under a rate\nconstraint. Previous RL-based methods tackle such a constrained optimization\nproblem by maximizing a single reward function that often combines a distortion\nand a rate reward. However, the way how these rewards are combined is usually\nad hoc and may not generalize well to various coding conditions and video\nsequences. To overcome this issue, we adapt the deep deterministic policy\ngradient (DDPG) reinforcement learning algorithm for use with two critics, with\none learning to predict the distortion reward and the other the rate reward. In\nparticular, the distortion critic works to update the agent when the rate\nconstraint is satisfied. By contrast, the rate critic makes the rate constraint\na priority when the agent goes over the bit budget. Experimental results on\ncommonly used datasets show that our method outperforms the bit allocation\nscheme in x265 and the single-critic baseline by a significant margin in terms\nof rate-distortion performance while offering fairly precise rate control.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.11712,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Skeletor: Skeletal Transformers for Robust Body-Pose Estimation\n\n  Predicting 3D human pose from a single monoscopic video can be highly\nchallenging due to factors such as low resolution, motion blur and occlusion,\nin addition to the fundamental ambiguity in estimating 3D from 2D. Approaches\nthat directly regress the 3D pose from independent images can be particularly\nsusceptible to these factors and result in jitter, noise and\/or inconsistencies\nin skeletal estimation. Much of which can be overcome if the temporal evolution\nof the scene and skeleton are taken into account. However, rather than tracking\nbody parts and trying to temporally smooth them, we propose a novel transformer\nbased network that can learn a distribution over both pose and motion in an\nunsupervised fashion. We call our approach Skeletor. Skeletor overcomes\ninaccuracies in detection and corrects partial or entire skeleton corruption.\nSkeletor uses strong priors learn from on 25 million frames to correct skeleton\nsequences smoothly and consistently. Skeletor can achieve this as it implicitly\nlearns the spatio-temporal context of human motion via a transformer based\nneural network. Extensive experiments show that Skeletor achieves improved\nperformance on 3D human pose estimation and further provides benefits for\ndownstream tasks such as sign language translation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.07841,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000214908,
      "text":"Pareto Self-Supervised Training for Few-Shot Learning\n\n  While few-shot learning (FSL) aims for rapid generalization to new concepts\nwith little supervision, self-supervised learning (SSL) constructs supervisory\nsignals directly computed from unlabeled data. Exploiting the complementarity\nof these two manners, few-shot auxiliary learning has recently drawn much\nattention to deal with few labeled data. Previous works benefit from sharing\ninductive bias between the main task (FSL) and auxiliary tasks (SSL), where the\nshared parameters of tasks are optimized by minimizing a linear combination of\ntask losses. However, it is challenging to select a proper weight to balance\ntasks and reduce task conflict. To handle the problem as a whole, we propose a\nnovel approach named as Pareto self-supervised training (PSST) for FSL. PSST\nexplicitly decomposes the few-shot auxiliary problem into multiple constrained\nmulti-objective subproblems with different trade-off preferences, and here a\npreference region in which the main task achieves the best performance is\nidentified. Then, an effective preferred Pareto exploration is proposed to find\na set of optimal solutions in such a preference region. Extensive experiments\non several public benchmark datasets validate the effectiveness of our approach\nby achieving state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.06114,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Back-tracing Representative Points for Voting-based 3D Object Detection\n  in Point Clouds\n\n  3D object detection in point clouds is a challenging vision task that\nbenefits various applications for understanding the 3D visual world. Lots of\nrecent research focuses on how to exploit end-to-end trainable Hough voting for\ngenerating object proposals. However, the current voting strategy can only\nreceive partial votes from the surfaces of potential objects together with\nsevere outlier votes from the cluttered backgrounds, which hampers full\nutilization of the information from the input point clouds. Inspired by the\nback-tracing strategy in the conventional Hough voting methods, in this work,\nwe introduce a new 3D object detection method, named as Back-tracing\nRepresentative Points Network (BRNet), which generatively back-traces the\nrepresentative points from the vote centers and also revisits complementary\nseed points around these generated points, so as to better capture the fine\nlocal structural features surrounding the potential objects from the raw point\nclouds. Therefore, this bottom-up and then top-down strategy in our BRNet\nenforces mutual consistency between the predicted vote centers and the raw\nsurface points and thus achieves more reliable and flexible object localization\nand class prediction results. Our BRNet is simple but effective, which\nsignificantly outperforms the state-of-the-art methods on two large-scale point\ncloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50) and SUN RGB-D (+4.7% in\nterms of mAP@0.50), while it is still lightweight and efficient. Code will be\navailable at https:\/\/github.com\/cheng052\/BRNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12537,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Computer vision in automated parking systems: Design, implementation and\n  challenges\n\n  Automated driving is an active area of research in both industry and\nacademia. Automated Parking, which is automated driving in a restricted\nscenario of parking with low speed manoeuvring, is a key enabling product for\nfully autonomous driving systems. It is also an important milestone from the\nperspective of a higher end system built from the previous generation driver\nassistance systems comprising of collision warning, pedestrian detection, etc.\nIn this paper, we discuss the design and implementation of an automated parking\nsystem from the perspective of computer vision algorithms. Designing a low-cost\nsystem with functional safety is challenging and leads to a large gap between\nthe prototype and the end product, in order to handle all the corner cases. We\ndemonstrate how camera systems are crucial for addressing a range of automated\nparking use cases and also, to add robustness to systems based on active\ndistance measuring sensors, such as ultrasonics and radar. The key vision\nmodules which realize the parking use cases are 3D reconstruction, parking slot\nmarking recognition, freespace and vehicle\/pedestrian detection. We detail the\nimportant parking use cases and demonstrate how to combine the vision modules\nto form a robust parking system. To the best of the authors' knowledge, this is\nthe first detailed discussion of a systemic view of a commercial automated\nparking system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.0816,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Locally Aware Piecewise Transformation Fields for 3D Human Mesh\n  Registration\n\n  Registering point clouds of dressed humans to parametric human models is a\nchallenging task in computer vision. Traditional approaches often rely on\nheavily engineered pipelines that require accurate manual initialization of\nhuman poses and tedious post-processing. More recently, learning-based methods\nare proposed in hope to automate this process. We observe that pose\ninitialization is key to accurate registration but existing methods often fail\nto provide accurate pose initialization. One major obstacle is that, regressing\njoint rotations from point clouds or images of humans is still very\nchallenging. To this end, we propose novel piecewise transformation fields\n(PTF), a set of functions that learn 3D translation vectors to map any query\npoint in posed space to its correspond position in rest-pose space. We combine\nPTF with multi-class occupancy networks, obtaining a novel learning-based\nframework that learns to simultaneously predict shape and per-point\ncorrespondences between the posed space and the canonical space for clothed\nhuman. Our key insight is that the translation vector for each query point can\nbe effectively estimated using the point-aligned local features; consequently,\nrigid per bone transformations and joint rotations can be obtained efficiently\nvia a least-square fitting given the estimated point correspondences,\ncircumventing the challenging task of directly regressing joint rotations from\nneural networks. Furthermore, the proposed PTF facilitate canonicalized\noccupancy estimation, which greatly improves generalization capability and\nresults in more accurate surface reconstruction with only half of the\nparameters compared with the state-of-the-art. Both qualitative and\nquantitative studies show that fitting parametric models with poses initialized\nby our network results in much better registration quality, especially for\nextreme poses.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.02538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Visual Camera Re-Localization Using Graph Neural Networks and Relative\n  Pose Supervision\n\n  Visual re-localization means using a single image as input to estimate the\ncamera's location and orientation relative to a pre-recorded environment. The\nhighest-scoring methods are \"structure based,\" and need the query camera's\nintrinsics as an input to the model, with careful geometric optimization. When\nintrinsics are absent, methods vie for accuracy by making various other\nassumptions. This yields fairly good localization scores, but the models are\n\"narrow\" in some way, eg., requiring costly test-time computations, or depth\nsensors, or multiple query frames. In contrast, our proposed method makes few\nspecial assumptions, and is fairly lightweight in training and testing.\n  Our pose regression network learns from only relative poses of training\nscenes. For inference, it builds a graph connecting the query image to training\ncounterparts and uses a graph neural network (GNN) with image representations\non nodes and image-pair representations on edges. By efficiently passing\nmessages between them, both representation types are refined to produce a\nconsistent camera pose estimate. We validate the effectiveness of our approach\non both standard indoor (7-Scenes) and outdoor (Cambridge Landmarks) camera\nre-localization benchmarks. Our relative pose regression method matches the\naccuracy of absolute pose regression networks, while retaining the\nrelative-pose models' test-time speed and ability to generalize to non-training\nscenes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08278,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Fusing the Old with the New: Learning Relative Camera Pose with\n  Geometry-Guided Uncertainty\n\n  Learning methods for relative camera pose estimation have been developed\nlargely in isolation from classical geometric approaches. The question of how\nto integrate predictions from deep neural networks (DNNs) and solutions from\ngeometric solvers, such as the 5-point algorithm, has as yet remained\nunder-explored. In this paper, we present a novel framework that involves\nprobabilistic fusion between the two families of predictions during network\ntraining, with a view to leveraging their complementary benefits in a learnable\nway. The fusion is achieved by learning the DNN uncertainty under explicit\nguidance by the geometric uncertainty, thereby learning to take into account\nthe geometric solution in relation to the DNN prediction. Our network features\na self-attention graph neural network, which drives the learning by enforcing\nstrong interactions between different correspondences and potentially modeling\ncomplex relationships between points. We propose motion parmeterizations\nsuitable for learning and show that our method achieves state-of-the-art\nperformance on the challenging DeMoN and ScanNet datasets. While we focus on\nrelative pose, we envision that our pipeline is broadly applicable for fusing\nclassical geometry and deep learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.00613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"The surprising impact of mask-head architecture on novel class\n  segmentation\n\n  Instance segmentation models today are very accurate when trained on large\nannotated datasets, but collecting mask annotations at scale is prohibitively\nexpensive. We address the partially supervised instance segmentation problem in\nwhich one can train on (significantly cheaper) bounding boxes for all\ncategories but use masks only for a subset of categories. In this work, we\nfocus on a popular family of models which apply differentiable cropping to a\nfeature map and predict a mask based on the resulting crop. Under this family,\nwe study Mask R-CNN and discover that instead of its default strategy of\ntraining the mask-head with a combination of proposals and groundtruth boxes,\ntraining the mask-head with only groundtruth boxes dramatically improves its\nperformance on novel classes. This training strategy also allows us to take\nadvantage of alternative mask-head architectures, which we exploit by replacing\nthe typical mask-head of 2-4 layers with significantly deeper off-the-shelf\narchitectures (e.g. ResNet, Hourglass models). While many of these\narchitectures perform similarly when trained in fully supervised mode, our main\nfinding is that they can generalize to novel classes in dramatically different\nways. We call this ability of mask-heads to generalize to unseen classes the\nstrong mask generalization effect and show that without any specialty modules\nor losses, we can achieve state-of-the-art results in the partially supervised\nCOCO instance segmentation benchmark. Finally, we demonstrate that our effect\nis general, holding across underlying detection methodologies (including\nanchor-based, anchor-free or no detector at all) and across different backbone\nnetworks. Code and pre-trained models are available at https:\/\/git.io\/deepmac.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.1269,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Towards Good Practices for Efficiently Annotating Large-Scale Image\n  Classification Datasets\n\n  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https:\/\/fidler-lab.github.io\/efficient-annotation-cookbook\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.12663,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"CAGAN: Text-To-Image Generation with Combined Attention GANs\n\n  Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and\/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08575,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000065896,
      "text":"VSpSR: Explorable Super-Resolution via Variational Sparse Representation\n\n  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https:\/\/zmiclab.github.io\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.04104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000100666,
      "text":"Image-based Virtual Fitting Room\n\n  Virtual fitting room is a challenging task yet useful feature for e-commerce\nplatforms and fashion designers. Existing works can only detect very few types\nof fashion items. Besides they did poorly in changing the texture and style of\nthe selected fashion items. In this project, we propose a novel approach to\naddress this problem. We firstly used Mask R-CNN to find the regions of\ndifferent fashion items, and secondly used Neural Style Transfer to change the\nstyle of the selected fashion items. The dataset we used is composed of images\nfrom PaperDoll dataset and annotations provided by eBay's ModaNet. We trained 8\nmodels and our best model massively outperformed baseline models both\nquantitatively and qualitatively, with 68.72% mAP, 0.2% ASDR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.08277,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction\n\n  Trajectory prediction is a safety-critical tool for autonomous vehicles to\nplan and execute actions. Our work addresses two key challenges in trajectory\nprediction, learning multimodal outputs, and better predictions by imposing\nconstraints using driving knowledge. Recent methods have achieved strong\nperformances using Multi-Choice Learning objectives like winner-takes-all (WTA)\nor best-of-many. But the impact of those methods in learning diverse hypotheses\nis under-studied as such objectives highly depend on their initialization for\ndiversity. As our first contribution, we propose a novel Divide-And-Conquer\n(DAC) approach that acts as a better initialization technique to WTA objective,\nresulting in diverse outputs without any spurious modes. Our second\ncontribution is a novel trajectory prediction framework called ALAN that uses\nexisting lane centerlines as anchors to provide trajectories constrained to the\ninput lanes. Our framework provides multi-agent trajectory outputs in a forward\npass by capturing interactions through hypercolumn descriptors and\nincorporating scene information in the form of rasterized images and per-agent\nlane anchors. Experiments on synthetic and real data show that the proposed DAC\ncaptures the data distribution better compare to other WTA family of\nobjectives. Further, we show that our ALAN approach provides on par or better\nperformance with SOTA methods evaluated on Nuscenes urban driving benchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2104.09441,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"One More Check: Making \"Fake Background\" Be Tracked Again\n\n  The one-shot multi-object tracking, which integrates object detection and ID\nembedding extraction into a unified network, has achieved groundbreaking\nresults in recent years. However, current one-shot trackers solely rely on\nsingle-frame detections to predict candidate bounding boxes, which may be\nunreliable when facing disastrous visual degradation, e.g., motion blur,\nocclusions. Once a target bounding box is mistakenly classified as background\nby the detector, the temporal consistency of its corresponding tracklet will be\nno longer maintained. In this paper, we set out to restore the bounding boxes\nmisclassified as ``fake background'' by proposing a re-check network. The\nre-check network innovatively expands the role of ID embedding from data\nassociation to motion forecasting by effectively propagating previous tracklets\nto the current frame with a small overhead. Note that the propagation results\nare yielded by an independent and efficient embedding search, preventing the\nmodel from over-relying on detection results. Eventually, it helps to reload\nthe ``fake background'' and repair the broken tracklets. Building on a strong\nbaseline CSTrack, we construct a new one-shot tracker and achieve favorable\ngains by 70.7 $\\rightarrow$ 76.4, 70.6 $\\rightarrow$ 76.3 MOTA on MOT16 and\nMOT17, respectively. It also reaches a new state-of-the-art MOTA and IDF1\nperformance. Code is released at https:\/\/github.com\/JudasDie\/SOTS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.07147,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000127157,
      "text":"FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol\n  Spotting\n\n  Access to large and diverse computer-aided design (CAD) drawings is critical\nfor developing symbol spotting algorithms. In this paper, we present\nFloorPlanCAD, a large-scale real-world CAD drawing dataset containing over\n10,000 floor plans, ranging from residential to commercial buildings. CAD\ndrawings in the dataset are all represented as vector graphics, which enable us\nto provide line-grained annotations of 30 object categories. Equipped by such\nannotations, we introduce the task of panoptic symbol spotting, which requires\nto spot not only instances of countable things, but also the semantic of\nuncountable stuff. Aiming to solve this task, we propose a novel method by\ncombining Graph Convolutional Networks (GCNs) with Convolutional Neural\nNetworks (CNNs), which captures both non-Euclidean and Euclidean features and\ncan be trained end-to-end. The proposed CNN-GCN method achieved\nstate-of-the-art (SOTA) performance on the task of semantic symbol spotting,\nand help us build a baseline network for the panoptic symbol spotting task. Our\ncontributions are three-fold: 1) to the best of our knowledge, the presented\nCAD drawing dataset is the first of its kind; 2) the panoptic symbol spotting\ntask considers the spotting of both thing instances and stuff semantic as one\nrecognition problem; and 3) we presented a baseline solution to the panoptic\nsymbol spotting task based on a novel CNN-GCN method, which achieved SOTA\nperformance on semantic symbol spotting. We believe that these contributions\nwill boost research in related areas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.08229,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000007285,
      "text":"Single View Geocentric Pose in the Wild\n\n  Current methods for Earth observation tasks such as semantic mapping, map\nalignment, and change detection rely on near-nadir images; however, often the\nfirst available images in response to dynamic world events such as natural\ndisasters are oblique. These tasks are much more difficult for oblique images\ndue to observed object parallax. There has been recent success in learning to\nregress geocentric pose, defined as height above ground and orientation with\nrespect to gravity, by training with airborne lidar registered to satellite\nimages. We present a model for this novel task that exploits affine invariance\nproperties to outperform state of the art performance by a wide margin. We also\naddress practical issues required to deploy this method in the wild for\nreal-world applications. Our data and code are publicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Large-Scale Attribute-Object Compositions\n\n  We study the problem of learning how to predict attribute-object compositions\nfrom images, and its generalization to unseen compositions missing from the\ntraining data. To the best of our knowledge, this is a first large-scale study\nof this problem, involving hundreds of thousands of compositions. We train our\nframework with images from Instagram using hashtags as noisy weak supervision.\nWe make careful design choices for data collection and modeling, in order to\nhandle noisy annotations and unseen compositions. Finally, extensive\nevaluations show that learning to compose classifiers outperforms late fusion\nof individual attribute and object predictions, especially in the case of\nunseen attribute-object pairs.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.04836,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000058942,
      "text":"Found a Reason for me? Weakly-supervised Grounded Visual Question\n  Answering using Capsules\n\n  The problem of grounding VQA tasks has seen an increased attention in the\nresearch community recently, with most attempts usually focusing on solving\nthis task by using pretrained object detectors. However, pre-trained object\ndetectors require bounding box annotations for detecting relevant objects in\nthe vocabulary, which may not always be feasible for real-life large-scale\napplications. In this paper, we focus on a more relaxed setting: the grounding\nof relevant visual entities in a weakly supervised manner by training on the\nVQA task alone. To address this problem, we propose a visual capsule module\nwith a query-based selection mechanism of capsule features, that allows the\nmodel to focus on relevant regions based on the textual cues about visual\ninformation in the question. We show that integrating the proposed capsule\nmodule in existing VQA systems significantly improves their performance on the\nweakly supervised grounding task. Overall, we demonstrate the effectiveness of\nour approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the\nCLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with\nground truth bounding boxes for objects that are relevant for the correct\nanswer, as well as on GQA, a real world VQA dataset with compositional\nquestions. We show that the systems with the proposed capsule module\nconsistently outperform the respective baseline systems in terms of answer\ngrounding, while achieving comparable performance on VQA task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05353,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000046359,
      "text":"Video Frame Interpolation via Structure-Motion based Iterative Fusion\n\n  Video Frame Interpolation synthesizes non-existent images between adjacent\nframes, with the aim of providing a smooth and consistent visual experience.\nTwo approaches for solving this challenging task are optical flow based and\nkernel-based methods. In existing works, optical flow based methods can provide\naccurate point-to-point motion description, however, they lack constraints on\nobject structure. On the contrary, kernel-based methods focus on structural\nalignment, which relies on semantic and apparent features, but tends to blur\nresults. Based on these observations, we propose a structure-motion based\niterative fusion method. The framework is an end-to-end learnable structure\nwith two stages. First, interpolated frames are synthesized by structure-based\nand motion-based learning branches respectively, then, an iterative refinement\nmodule is established via spatial and temporal feature integration. Inspired by\nthe observation that audiences have different visual preferences on foreground\nand background objects, we for the first time propose to use saliency masks in\nthe evaluation processes of the task of video frame interpolation. Experimental\nresults on three typical benchmarks show that the proposed method achieves\nsuperior performance on all evaluation metrics over the state-of-the-art\nmethods, even when our models are trained with only one-tenth of the data other\nmethods use.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.10201,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000144376,
      "text":"DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain\n  Adaptation\n\n  Domain shift has always been one of the primary issues in video object\nsegmentation (VOS), for which models suffer from degeneration when tested on\nunfamiliar datasets. Recently, many online methods have emerged to narrow the\nperformance gap between training data (source domain) and test data (target\ndomain) by fine-tuning on annotations of test data which are usually in\nshortage. In this paper, we propose a novel method to tackle domain shift by\nfirst introducing adversarial domain adaptation to the VOS task, with\nsupervised training on the source domain and unsupervised training on the\ntarget domain. By fusing appearance and motion features with a convolution\nlayer, and by adding supervision onto the motion branch, our model achieves\nstate-of-the-art performance on DAVIS2016 with 82.6% mean IoU score after\nsupervised training. Meanwhile, our adversarial domain adaptation strategy\nsignificantly raises the performance of the trained model when applied on\nFBMS59 and Youtube-Object, without exploiting extra annotations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11527,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000596,
      "text":"Unsupervised Visual Representation Learning by Online Constrained\n  K-Means\n\n  Cluster discrimination is an effective pretext task for unsupervised\nrepresentation learning, which often consists of two phases: clustering and\ndiscrimination. Clustering is to assign each instance a pseudo label that will\nbe used to learn representations in discrimination. The main challenge resides\nin clustering since prevalent clustering methods (e.g., k-means) have to run in\na batch mode. Besides, there can be a trivial solution consisting of a\ndominating cluster. To address these challenges, we first investigate the\nobjective of clustering-based representation learning. Based on this, we\npropose a novel clustering-based pretext task with online \\textbf{Co}nstrained\n\\textbf{K}-m\\textbf{e}ans (\\textbf{CoKe}). Compared with the balanced\nclustering that each cluster has exactly the same size, we only constrain the\nminimal size of each cluster to flexibly capture the inherent data structure.\nMore importantly, our online assignment method has a theoretical guarantee to\napproach the global optimum. By decoupling clustering and discrimination, CoKe\ncan achieve competitive performance when optimizing with only a single view\nfrom each instance. Extensive experiments on ImageNet and other benchmark data\nsets verify both the efficacy and efficiency of our proposal. Code is available\nat \\url{https:\/\/github.com\/idstcv\/CoKe}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01951,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Multi-scale Image Decomposition using a Local Statistical Edge Model\n\n  We present a progressive image decomposition method based on a novel\nnon-linear filter named Sub-window Variance filter. Our method is specifically\ndesigned for image detail enhancement purpose; this application requires\nextraction of image details which are small in terms of both spatial and\nvariation scales. We propose a local statistical edge model which develops its\nedge awareness using spatially defined image statistics. Our decomposition\nmethod is controlled by two intuitive parameters which allow the users to\ndefine what image details to suppress or enhance. By using the summed-area\ntable acceleration method, our decomposition pipeline is highly parallel. The\nproposed filter is gradient preserving and this allows our enhancement results\nfree from the gradient-reversal artefact. In our evaluations, we compare our\nmethod in various multi-scale image detail manipulation applications with other\nmainstream solutions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.12362,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0,
      "text":"How to Calibrate Your Event Camera\n\n  We propose a generic event camera calibration framework using image\nreconstruction. Instead of relying on blinking LED patterns or external\nscreens, we show that neural-network-based image reconstruction is well suited\nfor the task of intrinsic and extrinsic calibration of event cameras. The\nadvantage of our proposed approach is that we can use standard calibration\npatterns that do not rely on active illumination. Furthermore, our approach\nenables the possibility to perform extrinsic calibration between frame-based\nand event-based sensors without additional complexity. Both simulation and\nreal-world experiments indicate that calibration through image reconstruction\nis accurate under common distortion models and a wide variety of distortion\nparameters\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.01456,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"Computer vision for liquid samples in hospitals and medical labs using\n  hierarchical image segmentation and relations prediction\n\n  This work explores the use of computer vision for image segmentation and\nclassification of medical fluid samples in transparent containers (for example,\ntubes, syringes, infusion bags). Handling fluids such as infusion fluids,\nblood, and urine samples is a significant part of the work carried out in\nmedical labs and hospitals. The ability to accurately identify and segment the\nliquids and the vessels that contain them from images can help in automating\nsuch processes. Modern computer vision typically involves training deep neural\nnets on large datasets of annotated images. This work presents a new dataset\ncontaining 1,300 annotated images of medical samples involving vessels\ncontaining liquids and solid material. The images are annotated with the type\nof liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid,\nfoam, suspension), the type of vessel (e.g., syringe, tube, cup, infusion\nbottle\/bag), and the properties of the vessel (transparent, opaque). In\naddition, vessel parts such as corks, labels, spikes, and valves are annotated.\nRelations and hierarchies between vessels and materials are also annotated,\nsuch as which vessel contains which material or which vessels are linked or\ncontain each other. Three neural networks are trained on the dataset: One\nnetwork learns to detect vessels, a second net detects the materials and parts\ninside each vessel, and a third net identifies relationships and connectivity\nbetween vessels.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.03569,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000558959,
      "text":"Improving Robustness for Pose Estimation via Stable Heatmap Regression\n\n  Deep learning methods have achieved excellent performance in pose estimation,\nbut the lack of robustness causes the keypoints to change drastically between\nsimilar images. In view of this problem, a stable heatmap regression method is\nproposed to alleviate network vulnerability to small perturbations. We utilize\nthe correlation between different rows and columns in a heatmap to alleviate\nthe multi-peaks problem, and design a highly differentiated heatmap regression\nto make a keypoint discriminative from surrounding points. A maximum stability\ntraining loss is used to simplify the optimization difficulty when minimizing\nthe prediction gap of two similar images. The proposed method achieves a\nsignificant advance in robustness over state-of-the-art approaches on two\nbenchmark datasets and maintains high performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.14184,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"E2ETag: An End-to-End Trainable Method for Generating and Detecting\n  Fiducial Markers\n\n  Existing fiducial markers solutions are designed for efficient detection and\ndecoding, however, their ability to stand out in natural environments is\ndifficult to infer from relatively limited analysis. Furthermore, worsening\nperformance in challenging image capture scenarios - such as poor exposure,\nmotion blur, and off-axis viewing - sheds light on their limitations. E2ETag\nintroduces an end-to-end trainable method for designing fiducial markers and a\ncomplimentary detector. By introducing back-propagatable marker augmentation\nand superimposition into training, the method learns to generate markers that\ncan be detected and classified in challenging real-world environments using a\nfully convolutional detector network. Results demonstrate that E2ETag\noutperforms existing methods in ideal conditions and performs much better in\nthe presence of motion blur, contrast fluctuations, noise, and off-axis viewing\nangles. Source code and trained models are available at\nhttps:\/\/github.com\/jbpeace\/E2ETag.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06988,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000077817,
      "text":"Automatic Non-Linear Video Editing Transfer\n\n  We propose an automatic approach that extracts editing styles in a source\nvideo and applies the edits to matched footage for video creation. Our Computer\nVision based techniques considers framing, content type, playback speed, and\nlighting of each input video segment. By applying a combination of these\nfeatures, we demonstrate an effective method that automatically transfers the\nvisual and temporal styles from professionally edited videos to unseen raw\nfootage. We evaluated our approach with real-world videos that contained a\ntotal of 3872 video shots of a variety of editing styles, including different\nsubjects, camera motions, and lighting. We reported feedback from survey\nparticipants who reviewed a set of our results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.00957,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive\n  Learning\n\n  Weakly supervised segmentation requires assigning a label to every pixel\nbased on training instances with partial annotations such as image-level tags,\nobject bounding boxes, labeled points and scribbles. This task is challenging,\nas coarse annotations (tags, boxes) lack precise pixel localization whereas\nsparse annotations (points, scribbles) lack broad region coverage. Existing\nmethods tackle these two types of weak supervision differently: Class\nactivation maps are used to localize coarse labels and iteratively refine the\nsegmentation model, whereas conditional random fields are used to propagate\nsparse labels to the entire image.\n  We formulate weakly supervised segmentation as a semi-supervised metric\nlearning problem, where pixels of the same (different) semantics need to be\nmapped to the same (distinctive) features. We propose 4 types of contrastive\nrelationships between pixels and segments in the feature space, capturing\nlow-level image similarity, semantic annotation, co-occurrence, and feature\naffinity They act as priors; the pixel-wise feature can be learned from\ntraining images with any partial annotations in a data-driven fashion. In\nparticular, unlabeled pixels in training images participate not only in\ndata-driven grouping within each image, but also in discriminative feature\nlearning within and across images. We deliver a universal weakly supervised\nsegmenter with significant gains on Pascal VOC and DensePose. Our code is\npublicly available at https:\/\/github.com\/twke18\/SPML.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.02186,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"RandCrowns: A Quantitative Metric for Imprecisely Labeled Tree Crown\n  Delineation\n\n  Supervised methods for object delineation in remote sensing require labeled\nground-truth data. Gathering sufficient high quality ground-truth data is\ndifficult, especially when targets are of irregular shape or difficult to\ndistinguish from background or neighboring objects. Tree crown delineation\nprovides key information from remote sensing images for forestry, ecology, and\nmanagement. However, tree crowns in remote sensing imagery are often difficult\nto label and annotate due to irregular shape, overlapping canopies, shadowing,\nand indistinct edges. There are also multiple approaches to annotation in this\nfield (e.g., rectangular boxes vs. convex polygons) that further contribute to\nannotation imprecision. However, current evaluation methods do not account for\nthis uncertainty in annotations, and quantitative metrics for evaluation can\nvary across multiple annotators. In this paper, we address these limitations by\ndeveloping an adaptation of the Rand index for weakly-labeled crown delineation\nthat we call RandCrowns. Our new RandCrowns evaluation metric provides a method\nto appropriately evaluate delineated tree crowns while taking into account\nimprecision in the ground-truth delineations. The RandCrowns metric\nreformulates the Rand index by adjusting the areas over which each term of the\nindex is computed to account for uncertain and imprecise object delineation\nlabels. Quantitative comparisons to the commonly used intersection over union\nmethod shows a decrease in the variance generated by differences among multiple\nannotators. Combined with qualitative examples, our results suggest that the\nRandCrowns metric is more robust for scoring target delineations in the\npresence of uncertainty and imprecision in annotations that are inherent to\ntree crown delineation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.02582,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Vision based Pedestrian Potential Risk Analysis based on Automated\n  Behavior Feature Extraction for Smart and Safe City\n\n  Despite recent advances in vehicle safety technologies, road traffic\naccidents still pose a severe threat to human lives and have become a leading\ncause of premature deaths. In particular, crosswalks present a major threat to\npedestrians, but we lack dense behavioral data to investigate the risks they\nface. Therefore, we propose a comprehensive analytical model for pedestrian\npotential risk using video footage gathered by road security cameras deployed\nat such crossings. The proposed system automatically detects vehicles and\npedestrians, calculates trajectories by frames, and extracts behavioral\nfeatures affecting the likelihood of potentially dangerous scenes between these\nobjects. Finally, we design a data cube model by using the large amount of the\nextracted features accumulated in a data warehouse to perform multidimensional\nanalysis for potential risk scenes with levels of abstraction, but this is\nbeyond the scope of this paper, and will be detailed in a future study. In our\nexperiment, we focused on extracting the various behavioral features from\nmultiple crosswalks, and visualizing and interpreting their behaviors and\nrelationships among them by camera location to show how they may or may not\ncontribute to potential risk. We validated feasibility and applicability by\napplying it in multiple crosswalks in Osan city, Korea.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.11494,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation\n\n  In this paper, we propose a method for coarse camera pose computation which\nis robust to viewing conditions and does not require a detailed model of the\nscene. This method meets the growing need of easy deployment of robotics or\naugmented reality applications in any environments, especially those for which\nno accurate 3D model nor huge amount of ground truth data are available. It\nexploits the ability of deep learning techniques to reliably detect objects\nregardless of viewing conditions. Previous works have also shown that\nabstracting the geometry of a scene of objects by an ellipsoid cloud allows to\ncompute the camera pose accurately enough for various application needs. Though\npromising, these approaches use the ellipses fitted to the detection bounding\nboxes as an approximation of the imaged objects. In this paper, we go one step\nfurther and propose a learning-based method which detects improved elliptic\napproximations of objects which are coherent with the 3D ellipsoid in terms of\nperspective projection. Experiments prove that the accuracy of the computed\npose significantly increases thanks to our method and is more robust to the\nvariability of the boundaries of the detection boxes. This is achieved with\nvery little effort in terms of training data acquisition -- a few hundred\ncalibrated images of which only three need manual object annotation. Code and\nmodels are released at\nhttps:\/\/github.com\/zinsmatt\/3D-Aware-Ellipses-for-Visual-Localization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.1161,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000011921,
      "text":"Unsupervised Scale-consistent Depth Learning from Video\n\n  We propose a monocular depth estimator SC-Depth, which requires only\nunlabelled videos for training and enables the scale-consistent prediction at\ninference time. Our contributions include: (i) we propose a geometry\nconsistency loss, which penalizes the inconsistency of predicted depths between\nadjacent views; (ii) we propose a self-discovered mask to automatically\nlocalize moving objects that violate the underlying static scene assumption and\ncause noisy signals during training; (iii) we demonstrate the efficacy of each\ncomponent with a detailed ablation study and show high-quality depth estimation\nresults in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of\nscale-consistent prediction, we show that our monocular-trained deep networks\nare readily integrated into the ORB-SLAM2 system for more robust and accurate\ntracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in\nKITTI, and it generalizes well to the KAIST dataset without additional\ntraining. Finally, we provide several demos for qualitative evaluation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.06182,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000225504,
      "text":"Global Wheat Challenge 2020: Analysis of the competition design and\n  winning models\n\n  Data competitions have become a popular approach to crowdsource new data\nanalysis methods for general and specialized data science problems. In plant\nphenotyping, data competitions have a rich history, and new outdoor field\ndatasets have potential for new data competitions. We developed the Global\nWheat Challenge as a generalization competition to see if solutions for wheat\nhead detection from field images would work in different regions around the\nworld. In this paper, we analyze the winning challenge solutions in terms of\ntheir robustness and the relative importance of model and data augmentation\ndesign decisions. We found that the design of the competition influence the\nselection of winning solutions and provide recommendations for future\ncompetitions in an attempt to garner more robust winning solutions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2105.05926,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Semantic Diversity Learning for Zero-Shot Multi-label Classification\n\n  Training a neural network model for recognizing multiple labels associated\nwith an image, including identifying unseen labels, is challenging, especially\nfor images that portray numerous semantically diverse labels. As challenging as\nthis task is, it is an essential task to tackle since it represents many\nreal-world cases, such as image retrieval of natural images. We argue that\nusing a single embedding vector to represent an image, as commonly practiced,\nis not sufficient to rank both relevant seen and unseen labels accurately. This\nstudy introduces an end-to-end model training for multi-label zero-shot\nlearning that supports semantic diversity of the images and labels. We propose\nto use an embedding matrix having principal embedding vectors trained using a\ntailored loss function. In addition, during training, we suggest up-weighting\nin the loss function image samples presenting higher semantic diversity to\nencourage the diversity of the embedding matrix. Extensive experiments show\nthat our proposed method improves the zero-shot model's quality in tag-based\nimage retrieval achieving SoTA results on several common datasets (NUS-Wide,\nCOCO, Open Images).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.14412,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000170536,
      "text":"Progressive Class-based Expansion Learning For Image Classification\n\n  In this paper, we propose a novel image process scheme called class-based\nexpansion learning for image classification, which aims at improving the\nsupervision-stimulation frequency for the samples of the confusing classes.\nClass-based expansion learning takes a bottom-up growing strategy in a\nclass-based expansion optimization fashion, which pays more attention to the\nquality of learning the fine-grained classification boundaries for the\npreferentially selected classes. Besides, we develop a class confusion\ncriterion to select the confusing class preferentially for training. In this\nway, the classification boundaries of the confusing classes are frequently\nstimulated, resulting in a fine-grained form. Experimental results demonstrate\nthe effectiveness of the proposed scheme on several benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.02733,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"DISCO: accurate Discrete Scale Convolutions\n\n  Scale is often seen as a given, disturbing factor in many vision tasks. When\ndoing so it is one of the factors why we need more data during learning. In\nrecent work scale equivariance was added to convolutional neural networks. It\nwas shown to be effective for a range of tasks. We aim for accurate\nscale-equivariant convolutional neural networks (SE-CNNs) applicable for\nproblems where high granularity of scale and small kernel sizes are required.\nCurrent SE-CNNs rely on weight sharing and kernel rescaling, the latter of\nwhich is accurate for integer scales only. To reach accurate scale\nequivariance, we derive general constraints under which scale-convolution\nremains equivariant to discrete rescaling. We find the exact solution for all\ncases where it exists, and compute the approximation for the rest. The discrete\nscale-convolution pays off, as demonstrated in a new state-of-the-art\nclassification on MNIST-scale and on STL-10 in the supervised learning setting.\nWith the same SE scheme, we also improve the computational effort of a\nscale-equivariant Siamese tracker on OTB-13.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08816,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV\n  Tracking\n\n  Recently, the Siamese-based method has stood out from multitudinous tracking\nmethods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to\nvarious special challenges in UAV tracking, \\textit{e.g.}, severe occlusion and\nfast motion, most existing Siamese-based trackers hardly combine superior\nperformance with high efficiency. To this concern, in this paper, a novel\nattentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.\nBy virtue of the attention mechanism, we conduct a special attentional\naggregation network (AAN) consisting of self-AAN and cross-AAN for raising the\nrepresentation ability of features eventually. The former AAN aggregates and\nmodels the self-semantic interdependencies of the single feature map via\nspatial and channel dimensions. The latter aims to aggregate the\ncross-interdependencies of two different semantic features including the\nlocation information of anchors. In addition, the anchor proposal network based\non dual features is proposed to raise its robustness of tracking objects with\nvarious scales. Experiments on two well-known authoritative benchmarks are\nconducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA\ntrackers. Besides, real-world tests onboard a typical embedded platform\ndemonstrate that SiamAPN++ achieves promising tracking results with real-time\nspeed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.13389,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000065896,
      "text":"Energy-Based Generative Cooperative Saliency Prediction\n\n  Conventional saliency prediction models typically learn a deterministic\nmapping from an image to its saliency map, and thus fail to explain the\nsubjective nature of human attention. In this paper, to model the uncertainty\nof visual saliency, we study the saliency prediction problem from the\nperspective of generative models by learning a conditional probability\ndistribution over the saliency map given an input image, and treating the\nsaliency prediction as a sampling process from the learned distribution.\nSpecifically, we propose a generative cooperative saliency prediction\nframework, where a conditional latent variable model (LVM) and a conditional\nenergy-based model (EBM) are jointly trained to predict salient objects in a\ncooperative manner. The LVM serves as a fast but coarse predictor to\nefficiently produce an initial saliency map, which is then refined by the\niterative Langevin revision of the EBM that serves as a slow but fine\npredictor. Such a coarse-to-fine cooperative saliency prediction strategy\noffers the best of both worlds. Moreover, we propose a \"cooperative learning\nwhile recovering\" strategy and apply it to weakly supervised saliency\nprediction, where saliency annotations of training images are partially\nobserved. Lastly, we find that the learned energy function in the EBM can serve\nas a refinement module that can refine the results of other pre-trained\nsaliency prediction models. Experimental results show that our model can\nproduce a set of diverse and plausible saliency maps of an image, and obtain\nstate-of-the-art performance in both fully supervised and weakly supervised\nsaliency prediction tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.1392,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"CAMS: Color-Aware Multi-Style Transfer\n\n  Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps:\/\/github.com\/mahmoudnafifi\/color-aware-style-transfer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.11915,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000276499,
      "text":"Enhanced Separable Disentanglement for Unsupervised Domain Adaptation\n\n  Domain adaptation aims to mitigate the domain gap when transferring knowledge\nfrom an existing labeled domain to a new domain. However, existing\ndisentanglement-based methods do not fully consider separation between\ndomain-invariant and domain-specific features, which means the domain-invariant\nfeatures are not discriminative. The reconstructed features are also not\nsufficiently used during training. In this paper, we propose a novel enhanced\nseparable disentanglement (ESD) model. We first employ a disentangler to\ndistill domain-invariant and domain-specific features. Then, we apply feature\nseparation enhancement processes to minimize contamination between\ndomain-invariant and domain-specific features. Finally, our model reconstructs\ncomplete feature vectors, which are used for further disentanglement during the\ntraining phase. Extensive experiments from three benchmark datasets outperform\nstate-of-the-art methods, especially on challenging cross-domain tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03899,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000175171,
      "text":"Shifting Transformation Learning for Out-of-Distribution Detection\n\n  Detecting out-of-distribution (OOD) samples plays a key role in open-world\nand safety-critical applications such as autonomous systems and healthcare.\nRecently, self-supervised representation learning techniques (via contrastive\nlearning and pretext learning) have shown effective in improving OOD detection.\nHowever, one major issue with such approaches is the choice of shifting\ntransformations and pretext tasks which depends on the in-domain distribution.\nIn this paper, we propose a simple framework that leverages a shifting\ntransformation learning setting for learning multiple shifted representations\nof the training set for improved OOD detection. To address the problem of\nselecting optimal shifting transformation and pretext tasks, we propose a\nsimple mechanism for automatically selecting the transformations and modulating\ntheir effect on representation learning without requiring any OOD training\nsamples. In extensive experiments, we show that our simple framework\noutperforms state-of-the-art OOD detection models on several image datasets. We\nalso characterize the criteria for a desirable OOD detector for real-world\napplications and demonstrate the efficacy of our proposed technique against\nstate-of-the-art OOD detection techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.06684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Multistream ValidNet: Improving 6D Object Pose Estimation by Automatic\n  Multistream Validation\n\n  This work presents a novel approach to improve the results of pose estimation\nby detecting and distinguishing between the occurrence of True and False\nPositive results. It achieves this by training a binary classifier on the\noutput of an arbitrary pose estimation algorithm, and returns a binary label\nindicating the validity of the result. We demonstrate that our approach\nimproves upon a state-of-the-art pose estimation result on the Sil\\'eane\ndataset, outperforming a variation of the alternative CullNet method by 4.15%\nin average class accuracy and 0.73% in overall accuracy at validation. Applying\nour method can also improve the pose estimation average precision results of\nOp-Net by 6.06% on average.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03069,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000237425,
      "text":"Multi-Level Graph Encoding with Structural-Collaborative Relation\n  Learning for Skeleton-Based Person Re-Identification\n\n  Skeleton-based person re-identification (Re-ID) is an emerging open topic\nproviding great value for safety-critical applications. Existing methods\ntypically extract hand-crafted features or model skeleton dynamics from the\ntrajectory of body joints, while they rarely explore valuable relation\ninformation contained in body structure or motion. To fully explore body\nrelations, we construct graphs to model human skeletons from different levels,\nand for the first time propose a Multi-level Graph encoding approach with\nStructural-Collaborative Relation learning (MG-SCR) to encode discriminative\ngraph features for person Re-ID. Specifically, considering that\nstructurally-connected body components are highly correlated in a skeleton, we\nfirst propose a multi-head structural relation layer to learn different\nrelations of neighbor body-component nodes in graphs, which helps aggregate key\ncorrelative features for effective node representations. Second, inspired by\nthe fact that body-component collaboration in walking usually carries\nrecognizable patterns, we propose a cross-level collaborative relation layer to\ninfer collaboration between different level components, so as to capture more\ndiscriminative skeleton graph features. Finally, to enhance graph dynamics\nencoding, we propose a novel self-supervised sparse sequential prediction task\nfor model pre-training, which facilitates encoding high-level graph semantics\nfor person Re-ID. MG-SCR outperforms state-of-the-art skeleton-based methods,\nand it achieves superior performance to many multi-modal methods that utilize\nextra RGB or depth features. Our codes are available at\nhttps:\/\/github.com\/Kali-Hac\/MG-SCR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03158,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Transferring Knowledge from Text to Video: Zero-Shot Anticipation for\n  Procedural Actions\n\n  Can we teach a robot to recognize and make predictions for activities that it\nhas never seen before? We tackle this problem by learning models for video from\ntext. This paper presents a hierarchical model that generalizes instructional\nknowledge from large-scale text corpora and transfers the knowledge to video.\nGiven a portion of an instructional video, our model recognizes and predicts\ncoherent and plausible actions multiple steps into the future, all in rich\nnatural language. To demonstrate the capabilities of our model, we introduce\nthe \\emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot\nlearning, recognition and anticipation. Extensive experiments with various\nevaluation metrics demonstrate the potential of our method for generalization,\ngiven limited video data for training models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.03956,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Novel View Video Prediction Using a Dual Representation\n\n  We address the problem of novel view video prediction; given a set of input\nvideo clips from a single\/multiple views, our network is able to predict the\nvideo from a novel view. The proposed approach does not require any priors and\nis able to predict the video from wider angular distances, upto 45 degree, as\ncompared to the recent studies predicting small variations in viewpoint.\nMoreover, our method relies only onRGB frames to learn a dual representation\nwhich is used to generate the video from a novel viewpoint. The dual\nrepresentation encompasses a view-dependent and a global representation which\nincorporates complementary details to enable novel view video prediction. We\ndemonstrate the effectiveness of our framework on two real world datasets:\nNTU-RGB+D and CMU Panoptic. A comparison with the State-of-the-art novel view\nvideo prediction methods shows an improvement of 26.1% in SSIM, 13.6% in PSNR,\nand 60% inFVD scores without using explicit priors from target views.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Compositional Sketch Search\n\n  We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.08503,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Understanding and Evaluating Racial Biases in Image Captioning\n\n  Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we\nexamine racial and intersectional biases using manual annotations. Our first\ncontribution is in annotating the perceived gender and skin color of 28,315 of\nthe depicted people after obtaining IRB approval. Using these annotations, we\ncompare racial biases present in both manual and automatically-generated image\ncaptions. We demonstrate differences in caption performance, sentiment, and\nword choice between images of lighter versus darker-skinned people. Further, we\nfind the magnitude of these differences to be greater in modern captioning\nsystems compared to older ones, thus leading to concerns that without proper\nconsideration and mitigation these differences will only become increasingly\nprevalent. Code and data is available at\nhttps:\/\/princetonvisualai.github.io\/imagecaptioning-bias .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.07159,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000039074,
      "text":"Object-Guided Instance Segmentation With Auxiliary Feature Refinement\n  for Biological Images\n\n  Instance segmentation is of great importance for many biological\napplications, such as study of neural cell interactions, plant phenotyping, and\nquantitatively measuring how cells react to drug treatment. In this paper, we\npropose a novel box-based instance segmentation method. Box-based instance\nsegmentation methods capture objects via bounding boxes and then perform\nindividual segmentation within each bounding box region. However, existing\nmethods can hardly differentiate the target from its neighboring objects within\nthe same bounding box region due to their similar textures and low-contrast\nboundaries. To deal with this problem, in this paper, we propose an\nobject-guided instance segmentation method. Our method first detects the center\npoints of the objects, from which the bounding box parameters are then\npredicted. To perform segmentation, an object-guided coarse-to-fine\nsegmentation branch is built along with the detection branch. The segmentation\nbranch reuses the object features as guidance to separate target object from\nthe neighboring ones within the same bounding box region. To further improve\nthe segmentation quality, we design an auxiliary feature refinement module that\ndensely samples and refines point-wise features in the boundary regions.\nExperimental results on three biological image datasets demonstrate the\nadvantages of our method. The code will be available at\nhttps:\/\/github.com\/yijingru\/ObjGuided-Instance-Segmentation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12902,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Attention Toward Neighbors: A Context Aware Framework for High\n  Resolution Image Segmentation\n\n  High-resolution image segmentation remains challenging and error-prone due to\nthe enormous size of intermediate feature maps. Conventional methods avoid this\nproblem by using patch based approaches where each patch is segmented\nindependently. However, independent patch segmentation induces errors,\nparticularly at the patch boundary due to the lack of contextual information in\nvery high-resolution images where the patch size is much smaller compared to\nthe full image. To overcome these limitations, in this paper, we propose a\nnovel framework to segment a particular patch by incorporating contextual\ninformation from its neighboring patches. This allows the segmentation network\nto see the target patch with a wider field of view without the need of larger\nfeature maps. Comparative analysis from a number of experiments shows that our\nproposed framework is able to segment high resolution images with significantly\nimproved mean Intersection over Union and overall accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12832,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000350674,
      "text":"Detection of Deepfake Videos Using Long Distance Attention\n\n  With the rapid progress of deepfake techniques in recent years, facial video\nforgery can generate highly deceptive video contents and bring severe security\nthreats. And detection of such forgery videos is much more urgent and\nchallenging. Most existing detection methods treat the problem as a vanilla\nbinary classification problem. In this paper, the problem is treated as a\nspecial fine-grained classification problem since the differences between fake\nand real faces are very subtle. It is observed that most existing face forgery\nmethods left some common artifacts in the spatial domain and time domain,\nincluding generative defects in the spatial domain and inter-frame\ninconsistencies in the time domain. And a spatial-temporal model is proposed\nwhich has two components for capturing spatial and temporal forgery traces in\nglobal perspective respectively. The two components are designed using a novel\nlong distance attention mechanism. The one component of the spatial domain is\nused to capture artifacts in a single frame, and the other component of the\ntime domain is used to capture artifacts in consecutive frames. They generate\nattention maps in the form of patches. The attention method has a broader\nvision which contributes to better assembling global information and extracting\nlocal statistic information. Finally, the attention maps are used to guide the\nnetwork to focus on pivotal parts of the face, just like other fine-grained\nclassification methods. The experimental results on different public datasets\ndemonstrate that the proposed method achieves the state-of-the-art performance,\nand the proposed long distance attention method can effectively capture pivotal\nparts for face forgery.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.1085,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Robust Pooling through the Data Mode\n\n  The task of learning from point cloud data is always challenging due to the\noften occurrence of noise and outliers in the data. Such data inaccuracies can\nsignificantly influence the performance of state-of-the-art deep learning\nnetworks and their ability to classify or segment objects. While there are some\nrobust deep learning approaches, they are computationally too expensive for\nreal-time applications. This paper proposes a deep learning solution that\nincludes a novel robust pooling layer which greatly enhances network robustness\nand performs significantly faster than state-of-the-art approaches. The\nproposed pooling layer looks for data a mode\/cluster using two methods, RANSAC,\nand histogram, as clusters are indicative of models. We tested the pooling\nlayer into frameworks such as Point-based and graph-based neural networks, and\nthe tests showed enhanced robustness as compared to robust state-of-the-art\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.15117,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"SDL: New data generation tools for full-level annotated document layout\n\n  We present a novel data generation tool for document processing. The tool\nfocuses on providing a maximal level of visual information in a normal type\ndocument, ranging from character position to paragraph-level position. It also\nenables working with a large dataset on low-resource languages as well as\nproviding a mean of processing thorough full-level information of the\ndocumented text. The data generation tools come with a dataset of 320000\nVietnamese synthetic document images and an instruction to generate a dataset\nof similar size in other languages. The repository can be found at:\nhttps:\/\/github.com\/tson1997\/SDL-Document-Image-Generation\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.12859,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000069539,
      "text":"Unsupervised Deep Image Stitching: Reconstructing Stitched Features to\n  Images\n\n  Traditional feature-based image stitching technologies rely heavily on\nfeature detection quality, often failing to stitch images with few features or\nlow resolution. The learning-based image stitching solutions are rarely studied\ndue to the lack of labeled data, making the supervised methods unreliable. To\naddress the above limitations, we propose an unsupervised deep image stitching\nframework consisting of two stages: unsupervised coarse image alignment and\nunsupervised image reconstruction. In the first stage, we design an\nablation-based loss to constrain an unsupervised homography network, which is\nmore suitable for large-baseline scenes. Moreover, a transformer layer is\nintroduced to warp the input images in the stitching-domain space. In the\nsecond stage, motivated by the insight that the misalignments in pixel-level\ncan be eliminated to a certain extent in feature-level, we design an\nunsupervised image reconstruction network to eliminate the artifacts from\nfeatures to pixels. Specifically, the reconstruction network can be implemented\nby a low-resolution deformation branch and a high-resolution refined branch,\nlearning the deformation rules of image stitching and enhancing the resolution\nsimultaneously. To establish an evaluation benchmark and train the learning\nframework, a comprehensive real-world image dataset for unsupervised deep image\nstitching is presented and released. Extensive experiments well demonstrate the\nsuperiority of our method over other state-of-the-art solutions. Even compared\nwith the supervised solutions, our image stitching quality is still preferred\nby users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2106.00609,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000018213,
      "text":"Robust Mutual Learning for Semi-supervised Semantic Segmentation\n\n  Recent semi-supervised learning (SSL) methods are commonly based on pseudo\nlabeling. Since the SSL performance is greatly influenced by the quality of\npseudo labels, mutual learning has been proposed to effectively suppress the\nnoises in the pseudo supervision. In this work, we propose robust mutual\nlearning that improves the prior approach in two aspects. First, the vanilla\nmutual learners suffer from the coupling issue that models may converge to\nlearn homogeneous knowledge. We resolve this issue by introducing mean teachers\nto generate mutual supervisions so that there is no direct interaction between\nthe two students. We also show that strong data augmentations, model noises and\nheterogeneous network architectures are essential to alleviate the model\ncoupling. Second, we notice that mutual learning fails to leverage the\nnetwork's own ability for pseudo label refinement. Therefore, we introduce\nself-rectification that leverages the internal knowledge and explicitly\nrectifies the pseudo labels before the mutual teaching. Such self-rectification\nand mutual teaching collaboratively improve the pseudo label accuracy\nthroughout the learning. The proposed robust mutual learning demonstrates\nstate-of-the-art performance on semantic segmentation in low-data regime.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.11159,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000039405,
      "text":"Learning Discriminative Representations for Multi-Label Image\n  Recognition\n\n  Multi-label recognition is a fundamental, and yet is a challenging task in\ncomputer vision. Recently, deep learning models have achieved great progress\ntowards learning discriminative features from input images. However,\nconventional approaches are unable to model the inter-class discrepancies among\nfeatures in multi-label images, since they are designed to work for image-level\nfeature discrimination. In this paper, we propose a unified deep network to\nlearn discriminative features for the multi-label task. Given a multi-label\nimage, the proposed method first disentangles features corresponding to\ndifferent classes. Then, it discriminates between these classes via increasing\nthe inter-class distance while decreasing the intra-class differences in the\noutput space. By regularizing the whole network with the proposed loss, the\nperformance of applying the wellknown ResNet-101 is improved significantly.\nExtensive experiments have been performed on COCO-2014, VOC2007 and VOC2012\ndatasets, which demonstrate that the proposed method outperforms\nstate-of-the-art approaches by a significant margin of 3:5% on large-scale COCO\ndataset. Moreover, analysis of the discriminative feature learning approach\nshows that it can be plugged into various types of multi-label methods as a\ngeneral module.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00583,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Inter Extreme Points Geodesics for End-to-End Weakly Supervised Image\n  Segmentation\n\n  We introduce $\\textit{InExtremIS}$, a weakly supervised 3D approach to train\na deep image segmentation network using particularly weak train-time\nannotations: only 6 extreme clicks at the boundary of the objects of interest.\nOur fully-automatic method is trained end-to-end and does not require any\ntest-time annotations. From the extreme points, 3D bounding boxes are extracted\naround objects of interest. Then, deep geodesics connecting extreme points are\ngenerated to increase the amount of \"annotated\" voxels within the bounding\nboxes. Finally, a weakly supervised regularised loss derived from a Conditional\nRandom Field formulation is used to encourage prediction consistency over\nhomogeneous regions. Extensive experiments are performed on a large open\ndataset for Vestibular Schwannoma segmentation. $\\textit{InExtremIS}$ obtained\ncompetitive performance, approaching full supervision and outperforming\nsignificantly other weakly supervised techniques based on bounding boxes.\nMoreover, given a fixed annotation time budget, $\\textit{InExtremIS}$\noutperforms full supervision. Our code and data are available online.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04476,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"MutualEyeContact: A conversation analysis tool with focus on eye contact\n\n  Eye contact between individuals is particularly important for understanding\nhuman behaviour. To further investigate the importance of eye contact in social\ninteractions, portable eye tracking technology seems to be a natural choice.\nHowever, the analysis of available data can become quite complex. Scientists\nneed data that is calculated quickly and accurately. Additionally, the relevant\ndata must be automatically separated to save time. In this work, we propose a\ntool called MutualEyeContact which excels in those tasks and can help\nscientists to understand the importance of (mutual) eye contact in social\ninteractions. We combine state-of-the-art eye tracking with face recognition\nbased on machine learning and provide a tool for analysis and visualization of\nsocial interaction sessions. This work is a joint collaboration of computer\nscientists and cognitive scientists. It combines the fields of social and\nbehavioural science with computer vision and deep learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.04224,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000077486,
      "text":"Contrast R-CNN for Continual Learning in Object Detection\n\n  The continual learning problem has been widely studied in image\nclassification, while rare work has been explored in object detection. Some\nrecent works apply knowledge distillation to constrain the model to retain old\nknowledge, but this rigid constraint is detrimental for learning new knowledge.\nIn our paper, we propose a new scheme for continual learning of object\ndetection, namely Contrast R-CNN, an approach strikes a balance between\nretaining the old knowledge and learning the new knowledge. Furthermore, we\ndesign a Proposal Contrast to eliminate the ambiguity between old and new\ninstance to make the continual learning more robust. Extensive evaluation on\nthe PASCAL VOC dataset demonstrates the effectiveness of our approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.04805,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000059936,
      "text":"Few-Shot Domain Adaptation with Polymorphic Transformers\n\n  Deep neural networks (DNNs) trained on one set of medical images often\nexperience severe performance drop on unseen test images, due to various domain\ndiscrepancy between the training images (source domain) and the test images\n(target domain), which raises a domain adaptation issue. In clinical settings,\nit is difficult to collect enough annotated target domain data in a short\nperiod. Few-shot domain adaptation, i.e., adapting a trained model with a\nhandful of annotations, is highly practical and useful in this case. In this\npaper, we propose a Polymorphic Transformer (Polyformer), which can be\nincorporated into any DNN backbones for few-shot domain adaptation.\nSpecifically, after the polyformer layer is inserted into a model trained on\nthe source domain, it extracts a set of prototype embeddings, which can be\nviewed as a \"basis\" of the source-domain features. On the target domain, the\npolyformer layer adapts by only updating a projection layer which controls the\ninteractions between image features and the prototype embeddings. All other\nmodel weights (except BatchNorm parameters) are frozen during adaptation. Thus,\nthe chance of overfitting the annotations is greatly reduced, and the model can\nperform robustly on the target domain after being trained on a few annotated\nimages. We demonstrate the effectiveness of Polyformer on two medical\nsegmentation tasks (i.e., optic disc\/cup segmentation, and polyp segmentation).\nThe source code of Polyformer is released at\nhttps:\/\/github.com\/askerlee\/segtran.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.14391,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000161595,
      "text":"From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object\n  Detection\n\n  As an emerging data modal with precise distance sensing, LiDAR point clouds\nhave been placed great expectations on 3D scene understanding. However, point\nclouds are always sparsely distributed in the 3D space, and with unstructured\nstorage, which makes it difficult to represent them for effective 3D object\ndetection. To this end, in this work, we regard point clouds as hollow-3D data\nand propose a new architecture, namely Hallucinated Hollow-3D R-CNN\n($\\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our\napproach, we first extract the multi-view features by sequentially projecting\nthe point clouds into the perspective view and the bird-eye view. Then, we\nhallucinate the 3D representation by a novel bilaterally guided multi-view\nfusion block. Finally, the 3D objects are detected via a box refinement module\nwith a novel Hierarchical Voxel RoI Pooling operation. The proposed\n$\\text{H}^2$3D R-CNN provides a new angle to take full advantage of\ncomplementary information in the perspective view and the bird-eye view with an\nefficient framework. We evaluate our approach on the public KITTI Dataset and\nWaymo Open Dataset. Extensive experiments demonstrate the superiority of our\nmethod over the state-of-the-art algorithms with respect to both effectiveness\nand efficiency. The code will be made available at\n\\url{https:\/\/github.com\/djiajunustc\/H-23D_R-CNN}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03225,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000231796,
      "text":"Categorical Relation-Preserving Contrastive Knowledge Distillation for\n  Medical Image Classification\n\n  The amount of medical images for training deep classification models is\ntypically very scarce, making these deep models prone to overfit the training\ndata. Studies showed that knowledge distillation (KD), especially the\nmean-teacher framework which is more robust to perturbations, can help mitigate\nthe over-fitting effect. However, directly transferring KD from computer vision\nto medical image classification yields inferior performance as medical images\nsuffer from higher intra-class variance and class imbalance. To address these\nissues, we propose a novel Categorical Relation-preserving Contrastive\nKnowledge Distillation (CRCKD) algorithm, which takes the commonly used\nmean-teacher model as the supervisor. Specifically, we propose a novel\nClass-guided Contrastive Distillation (CCD) module to pull closer positive\nimage pairs from the same class in the teacher and student models, while\npushing apart negative image pairs from different classes. With this\nregularization, the feature distribution of the student model shows higher\nintra-class similarity and inter-class variance. Besides, we propose a\nCategorical Relation Preserving (CRP) loss to distill the teacher's relational\nknowledge in a robust and class-balanced manner. With the contribution of the\nCCD and CRP, our CRCKD algorithm can distill the relational knowledge more\ncomprehensively. Extensive experiments on the HAM10000 and APTOS datasets\ndemonstrate the superiority of the proposed CRCKD method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02557,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving\n  in Structural Scenes\n\n  Robust and accurate localization is an essential component for robotic\nnavigation and autonomous driving. The use of cameras for localization with\nhigh definition map (HD Map) provides an affordable localization sensor set.\nExisting methods suffer from pose estimation failure due to error prone data\nassociation or initialization with accurate initial pose requirement. In this\npaper, we propose a cost-effective vehicle localization system with HD map for\nautonomous driving that uses cameras as primary sensors. To this end, we\nformulate vision-based localization as a data association problem that maps\nvisual semantics to landmarks in HD map. Specifically, system initialization is\nfinished in a coarse to fine manner by combining coarse GPS (Global Positioning\nSystem) measurement and fine pose searching. In tracking stage, vehicle pose is\nrefined by implicitly aligning the semantic segmentation result between image\nand landmarks in HD maps with photometric consistency. Finally, vehicle pose is\ncomputed by pose graph optimization in a sliding window fashion. We evaluate\nour method on two datasets and demonstrate that the proposed approach yields\npromising localization results in different driving scenarios. Additionally,\nour approach is suitable for both monocular camera and multi-cameras that\nprovides flexibility and improves robustness for the localization system.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.06935,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000006954,
      "text":"Object Retrieval and Localization in Large Art Collections using Deep\n  Multi-Style Feature Fusion and Iterative Voting\n\n  The search for specific objects or motifs is essential to art history as both\nassist in decoding the meaning of artworks. Digitization has produced large art\ncollections, but manual methods prove to be insufficient to analyze them. In\nthe following, we introduce an algorithm that allows users to search for image\nregions containing specific motifs or objects and find similar regions in an\nextensive dataset, helping art historians to analyze large digitized art\ncollections. Computer vision has presented efficient methods for visual\ninstance retrieval across photographs. However, applied to art collections,\nthey reveal severe deficiencies because of diverse motifs and massive domain\nshifts induced by differences in techniques, materials, and styles. In this\npaper, we present a multi-style feature fusion approach that successfully\nreduces the domain gap and improves retrieval results without labelled data or\ncurated image collections. Our region-based voting with GPU-accelerated\napproximate nearest-neighbour search allows us to find and localize even small\nmotifs within an extensive dataset in a few seconds. We obtain state-of-the-art\nresults on the Brueghel dataset and demonstrate its generalization to\ninhomogeneous collections with a large number of distractors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.0584,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000050333,
      "text":"NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic\n  Millimeter Scale\n\n  Segmenting 3D cell nuclei from microscopy image volumes is critical for\nbiological and clinical analysis, enabling the study of cellular expression\npatterns and cell lineages. However, current datasets for neuronal nuclei\nusually contain volumes smaller than $10^{\\text{-}3}\\ mm^3$ with fewer than 500\ninstances per volume, unable to reveal the complexity in large brain regions\nand restrict the investigation of neuronal structures. In this paper, we have\npushed the task forward to the sub-cubic millimeter scale and curated the NucMM\ndataset with two fully annotated volumes: one $0.1\\ mm^3$ electron microscopy\n(EM) volume containing nearly the entire zebrafish brain with around 170,000\nnuclei; and one $0.25\\ mm^3$ micro-CT (uCT) volume containing part of a mouse\nvisual cortex with about 7,000 nuclei. With two imaging modalities and\nsignificantly increased volume size and instance numbers, we discover a great\ndiversity of neuronal nuclei in appearance and density, introducing new\nchallenges to the field. We also perform a statistical analysis to illustrate\nthose challenges quantitatively. To tackle the challenges, we propose a novel\nhybrid-representation learning model that combines the merits of foreground\nmask, contour map, and signed distance transform to produce high-quality 3D\nmasks. The benchmark comparisons on the NucMM dataset show that our proposed\nmethod significantly outperforms state-of-the-art nuclei segmentation\napproaches. Code and data are available at\nhttps:\/\/connectomics-bazaar.github.io\/proj\/nucMM\/index.html.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02622,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"Detecting Outliers with Poisson Image Interpolation\n\n  Supervised learning of every possible pathology is unrealistic for many\nprimary care applications like health screening. Image anomaly detection\nmethods that learn normal appearance from only healthy data have shown\npromising results recently. We propose an alternative to image\nreconstruction-based and image embedding-based methods and propose a new\nself-supervised method to tackle pathological anomaly detection. Our approach\noriginates in the foreign patch interpolation (FPI) strategy that has shown\nsuperior performance on brain MRI and abdominal CT data. We propose to use a\nbetter patch interpolation strategy, Poisson image interpolation (PII), which\nmakes our method suitable for applications in challenging data regimes. PII\noutperforms state-of-the-art methods by a good margin when tested on surrogate\ntasks like identifying common lung anomalies in chest X-rays or hypo-plastic\nleft heart syndrome in prenatal, fetal cardiac ultrasound images. Code\navailable at https:\/\/github.com\/jemtan\/PII.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.147,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000025166,
      "text":"Seeing poverty from space, how much can it be tuned?\n\n  Since the United Nations launched the Sustainable Development Goals (SDG) in\n2015, numerous universities, NGOs and other organizations have attempted to\ndevelop tools for monitoring worldwide progress in achieving them. Led by\nadvancements in the fields of earth observation techniques, data sciences and\nthe emergence of artificial intelligence, a number of research teams have\ndeveloped innovative tools for highlighting areas of vulnerability and tracking\nthe implementation of SDG targets. In this paper we demonstrate that\nindividuals with no organizational affiliation and equipped only with common\nhardware, publicly available datasets and cloud-based computing services can\nparticipate in the improvement of predicting machine-learning-based approaches\nto predicting local poverty levels in a given agro-ecological environment. The\napproach builds upon several pioneering efforts over the last five years\nrelated to mapping poverty by deep learning to process satellite imagery and\n\"ground-truth\" data from the field to link features with incidence of poverty\nin a particular context. The approach employs new methods for object\nidentification in order to optimize the modeled results and achieve\nsignificantly high accuracy. A key goal of the project was to intentionally\nkeep costs as low as possible - by using freely available resources - so that\ncitizen scientists, students and organizations could replicate the method in\nother areas of interest. Moreover, for simplicity, the input data used were\nderived from just a handful of sources (involving only earth observation and\npopulation headcounts). The results of the project could therefore certainly be\nstrengthened further through the integration of proprietary data from social\nnetworks, mobile phone providers, and other sources.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.00651,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"AutoFormer: Searching Transformers for Visual Recognition\n\n  Recently, pure transformer-based models have shown great potentials for\nvision tasks such as image classification and detection. However, the design of\ntransformer networks is challenging. It has been observed that the depth,\nembedding dimension, and number of heads can largely affect the performance of\nvision transformers. Previous models configure these dimensions based upon\nmanual crafting. In this work, we propose a new one-shot architecture search\nframework, namely AutoFormer, dedicated to vision transformer search.\nAutoFormer entangles the weights of different blocks in the same layers during\nsupernet training. Benefiting from the strategy, the trained supernet allows\nthousands of subnets to be very well-trained. Specifically, the performance of\nthese subnets with weights inherited from the supernet is comparable to those\nretrained from scratch. Besides, the searched models, which we refer to\nAutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In\nparticular, AutoFormer-tiny\/small\/base achieve 74.7%\/81.7%\/82.4% top-1 accuracy\non ImageNet with 5.7M\/22.9M\/53.7M parameters, respectively. Lastly, we verify\nthe transferability of AutoFormer by providing the performance on downstream\nbenchmarks and distillation experiments. Code and models are available at\nhttps:\/\/github.com\/microsoft\/AutoML.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.02108,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Can Super Resolution be used to improve Human Pose Estimation in Low\n  Resolution Scenarios?\n\n  The results obtained from state of the art human pose estimation (HPE) models\ndegrade rapidly when evaluating people of a low resolution, but can super\nresolution (SR) be used to help mitigate this effect? By using various SR\napproaches we enhanced two low resolution datasets and evaluated the change in\nperformance of both an object and keypoint detector as well as end-to-end HPE\nresults. We remark the following observations. First we find that for people\nwho were originally depicted at a low resolution (segmentation area in pixels),\ntheir keypoint detection performance would improve once SR was applied. Second,\nthe keypoint detection performance gained is dependent on that persons pixel\ncount in the original image prior to any application of SR; keypoint detection\nperformance was improved when SR was applied to people with a small initial\nsegmentation area, but degrades as this becomes larger. To address this we\nintroduced a novel Mask-RCNN approach, utilising a segmentation area threshold\nto decide when to use SR during the keypoint detection step. This approach\nachieved the best results on our low resolution datasets for each HPE\nperformance metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.13221,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.00001457,
      "text":"Normalization Matters in Weakly Supervised Object Localization\n\n  Weakly-supervised object localization (WSOL) enables finding an object using\na dataset without any localization information. By simply training a\nclassification model using only image-level annotations, the feature map of the\nmodel can be utilized as a score map for localization. In spite of many WSOL\nmethods proposing novel strategies, there has not been any de facto standard\nabout how to normalize the class activation map (CAM). Consequently, many WSOL\nmethods have failed to fully exploit their own capacity because of the misuse\nof a normalization method. In this paper, we review many existing normalization\nmethods and point out that they should be used according to the property of the\ngiven dataset. Additionally, we propose a new normalization method which\nsubstantially enhances the performance of any CAM-based WSOL methods. Using the\nproposed normalization method, we provide a comprehensive evaluation over three\ndatasets (CUB, ImageNet and OpenImages) on three different architectures and\nobserve significant performance gains over the conventional min-max\nnormalization method in all the evaluated cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.07837,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000074837,
      "text":"Progressive Deep Video Dehazing without Explicit Alignment Estimation\n\n  To solve the issue of video dehazing, there are two main tasks to attain: how\nto align adjacent frames to the reference frame; how to restore the reference\nframe. Some papers adopt explicit approaches (e.g., the Markov random field,\noptical flow, deformable convolution, 3D convolution) to align neighboring\nframes with the reference frame in feature space or image space, they then use\nvarious restoration methods to achieve the final dehazing results. In this\npaper, we propose a progressive alignment and restoration method for video\ndehazing. The alignment process aligns consecutive neighboring frames stage by\nstage without using the optical flow estimation. The restoration process is not\nonly implemented under the alignment process but also uses a refinement network\nto improve the dehazing performance of the whole network. The proposed networks\ninclude four fusion networks and one refinement network. To decrease the\nparameters of networks, three fusion networks in the first fusion stage share\nthe same parameters. Extensive experiments demonstrate that the proposed video\ndehazing method achieves outstanding performance against the-state-of-art\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.0733,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"DynaDog+T: A Parametric Animal Model for Synthetic Canine Image\n  Generation\n\n  Synthetic data is becoming increasingly common for training computer vision\nmodels for a variety of tasks. Notably, such data has been applied in tasks\nrelated to humans such as 3D pose estimation where data is either difficult to\ncreate or obtain in realistic settings. Comparatively, there has been less work\ninto synthetic animal data and it's uses for training models. Consequently, we\nintroduce a parametric canine model, DynaDog+T, for generating synthetic canine\nimages and data which we use for a common computer vision task, binary\nsegmentation, which would otherwise be difficult due to the lack of available\ndata.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.0256,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000083778,
      "text":"Online Hashing with Similarity Learning\n\n  Online hashing methods usually learn the hash functions online, aiming to\nefficiently adapt to the data variations in the streaming environment. However,\nwhen the hash functions are updated, the binary codes for the whole database\nhave to be updated to be consistent with the hash functions, resulting in the\ninefficiency in the online image retrieval process. In this paper, we propose a\nnovel online hashing framework without updating binary codes. In the proposed\nframework, the hash functions are fixed and a parametric similarity function\nfor the binary codes is learnt online to adapt to the streaming data.\nSpecifically, a parametric similarity function that has a bilinear form is\nadopted and a metric learning algorithm is proposed to learn the similarity\nfunction online based on the characteristics of the hashing methods. The\nexperiments on two multi-label image datasets show that our method is\ncompetitive or outperforms the state-of-the-art online hashing methods in terms\nof both accuracy and efficiency for multi-label image retrieval.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.05828,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Dynamic Distribution of Edge Intelligence at the Node Level for Internet\n  of Things\n\n  In this paper, dynamic deployment of Convolutional Neural Network (CNN)\narchitecture is proposed utilizing only IoT-level devices. By partitioning and\npipelining the CNN, it horizontally distributes the computation load among\nresource-constrained devices (called horizontal collaboration), which in turn\nincreases the throughput. Through partitioning, we can decrease the computation\nand energy consumption on individual IoT devices and increase the throughput\nwithout sacrificing accuracy. Also, by processing the data at the generation\npoint, data privacy can be achieved. The results show that throughput can be\nincreased by 1.55x to 1.75x for sharing the CNN into two and three\nresource-constrained devices, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2107.03352,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000048015,
      "text":"IntraLoss: Further Margin via Gradient-Enhancing Term for Deep Face\n  Recognition\n\n  Existing classification-based face recognition methods have achieved\nremarkable progress, introducing large margin into hypersphere manifold to\nlearn discriminative facial representations. However, the feature distribution\nis ignored. Poor feature distribution will wipe out the performance improvement\nbrought about by margin scheme. Recent studies focus on the unbalanced\ninter-class distribution and form a equidistributed feature representations by\npenalizing the angle between identity and its nearest neighbor. But the problem\nis more than that, we also found the anisotropy of intra-class distribution. In\nthis paper, we propose the `gradient-enhancing term' that concentrates on the\ndistribution characteristics within the class. This method, named IntraLoss,\nexplicitly performs gradient enhancement in the anisotropic region so that the\nintra-class distribution continues to shrink, resulting in isotropic and more\ncompact intra-class distribution and further margin between identities. The\nexperimental results on LFW, YTF and CFP-FP show that our outperforms\nstate-of-the-art methods by gradient enhancement, demonstrating the superiority\nof our method. In addition, our method has intuitive geometric interpretation\nand can be easily combined with existing methods to solve the previously\nignored problems.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00033,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to\n  the Third Dimension\n\n  We tackle the problem of monocular 3D reconstruction of articulated objects\nlike humans and animals. We contribute DensePose 3D, a method that can learn\nsuch reconstructions in a weakly supervised fashion from 2D image annotations\nonly. This is in stark contrast with previous deformable reconstruction methods\nthat use parametric models such as SMPL pre-trained on a large dataset of 3D\nobject scans. Because it does not require 3D scans, DensePose 3D can be used\nfor learning a wide range of articulated categories such as different animal\nspecies. The method learns, in an end-to-end fashion, a soft partition of a\ngiven category-specific 3D template mesh into rigid parts together with a\nmonocular reconstruction network that predicts the part motions such that they\nreproject correctly onto 2D DensePose-like surface annotations of the object.\nThe decomposition of the object into parts is regularized by expressing part\nassignments as a combination of the smooth eigenfunctions of the\nLaplace-Beltrami operator. We show significant improvements compared to\nstate-of-the-art non-rigid structure-from-motion baselines on both synthetic\nand real data on categories of humans and animals.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10031,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000012252,
      "text":"Realistic Image Synthesis with Configurable 3D Scene Layouts\n\n  Recent conditional image synthesis approaches provide high-quality\nsynthesized images. However, it is still challenging to accurately adjust image\ncontents such as the positions and orientations of objects, and synthesized\nimages often have geometrically invalid contents. To provide users with rich\ncontrollability on synthesized images in the aspect of 3D geometry, we propose\na novel approach to realistic-looking image synthesis based on a configurable\n3D scene layout. Our approach takes a 3D scene with semantic class labels as\ninput and trains a 3D scene painting network that synthesizes color values for\nthe input 3D scene. With the trained painting network, realistic-looking images\nfor the input 3D scene can be rendered and manipulated. To train the painting\nnetwork without 3D color supervision, we exploit an off-the-shelf 2D semantic\nimage synthesis method. In experiments, we show that our approach produces\nimages with geometrically correct structures and supports geometric\nmanipulation such as the change of the viewpoint and object poses as well as\nmanipulation of the painting style.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02947,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000028478,
      "text":"A review on vision-based analysis for automatic dietary assessment\n\n  Background: Maintaining a healthy diet is vital to avoid health-related\nissues, e.g., undernutrition, obesity and many non-communicable diseases. An\nindispensable part of the health diet is dietary assessment. Traditional manual\nrecording methods are not only burdensome but time-consuming, and contain\nsubstantial biases and errors. Recent advances in Artificial Intelligence (AI),\nespecially computer vision technologies, have made it possible to develop\nautomatic dietary assessment solutions, which are more convenient, less\ntime-consuming and even more accurate to monitor daily food intake. Scope and\napproach: This review presents Vision-Based Dietary Assessment (VBDA)\narchitectures, including multi-stage architecture and end-to-end one. The\nmulti-stage dietary assessment generally consists of three stages: food image\nanalysis, volume estimation and nutrient derivation. The prosperity of deep\nlearning makes VBDA gradually move to an end-to-end implementation, which\napplies food images to a single network to directly estimate the nutrition. The\nrecently proposed end-to-end methods are also discussed. We further analyze\nexisting dietary assessment datasets, indicating that one large-scale benchmark\nis urgently needed, and finally highlight critical challenges and future trends\nfor VBDA. Key findings and conclusions: After thorough exploration, we find\nthat multi-task end-to-end deep learning approaches are one important trend of\nVBDA. Despite considerable research progress, many challenges remain for VBDA\ndue to the meal complexity. We also provide the latest ideas for future\ndevelopment of VBDA, e.g., fine-grained food analysis and accurate volume\nestimation. This review aims to encourage researchers to propose more practical\nsolutions for VBDA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05082,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Automatic Polyp Segmentation via Multi-scale Subtraction Network\n\n  More than 90\\% of colorectal cancer is gradually transformed from colorectal\npolyps. In clinical practice, precise polyp segmentation provides important\ninformation in the early detection of colorectal cancer. Therefore, automatic\npolyp segmentation techniques are of great importance for both patients and\ndoctors. Most existing methods are based on U-shape structure and use\nelement-wise addition or concatenation to fuse different level features\nprogressively in decoder. However, both the two operations easily generate\nplenty of redundant information, which will weaken the complementarity between\ndifferent level features, resulting in inaccurate localization and blurred\nedges of polyps. To address this challenge, we propose a multi-scale\nsubtraction network (MSNet) to segment polyp from colonoscopy image.\nSpecifically, we first design a subtraction unit (SU) to produce the difference\nfeatures between adjacent levels in encoder. Then, we pyramidally equip the SUs\nat different levels with varying receptive fields, thereby obtaining rich\nmulti-scale difference information. In addition, we build a training-free\nnetwork \"LossNet\" to comprehensively supervise the polyp-aware features from\nbottom layer to top layer, which drives the MSNet to capture the detailed and\nstructural cues simultaneously. Extensive experiments on five benchmark\ndatasets demonstrate that our MSNet performs favorably against most\nstate-of-the-art methods under different evaluation metrics. Furthermore, MSNet\nruns at a real-time speed of $\\sim$70fps when processing a $352 \\times 352$\nimage. The source code will be publicly available at\n\\url{https:\/\/github.com\/Xiaoqi-Zhao-DLUT\/MSNet}. \\keywords{Colorectal Cancer\n\\and Automatic Polyp Segmentation \\and Subtraction \\and LossNet.}\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05565,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000072519,
      "text":"Vision-Language Transformer and Query Generation for Referring\n  Segmentation\n\n  In this work, we address the challenging task of referring segmentation. The\nquery expression in referring segmentation typically indicates the target\nobject by describing its relationship with others. Therefore, to find the\ntarget one among all instances in the image, the model must have a holistic\nunderstanding of the whole image. To achieve this, we reformulate referring\nsegmentation as a direct attention problem: finding the region in the image\nwhere the query language expression is most attended to. We introduce\ntransformer and multi-head attention to build a network with an encoder-decoder\nattention mechanism architecture that \"queries\" the given image with the\nlanguage expression. Furthermore, we propose a Query Generation Module, which\nproduces multiple sets of queries with different attention weights that\nrepresent the diversified comprehensions of the language expression from\ndifferent aspects. At the same time, to find the best way from these\ndiversified comprehensions based on visual clues, we further propose a Query\nBalance Module to adaptively select the output features of these queries for a\nbetter mask generation. Without bells and whistles, our approach is\nlight-weight and achieves new state-of-the-art performance consistently on\nthree referring segmentation datasets, RefCOCO, RefCOCO+, and G-Ref. Our code\nis available at https:\/\/github.com\/henghuiding\/Vision-Language-Transformer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.07413,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000225504,
      "text":"Cross-Image Region Mining with Region Prototypical Network for Weakly\n  Supervised Segmentation\n\n  Weakly supervised image segmentation trained with image-level labels usually\nsuffers from inaccurate coverage of object areas during the generation of the\npseudo groundtruth. This is because the object activation maps are trained with\nthe classification objective and lack the ability to generalize. To improve the\ngenerality of the objective activation maps, we propose a region prototypical\nnetwork RPNet to explore the cross-image object diversity of the training set.\nSimilar object parts across images are identified via region feature\ncomparison. Object confidence is propagated between regions to discover new\nobject areas while background regions are suppressed. Experiments show that the\nproposed method generates more complete and accurate pseudo object masks, while\nachieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In\naddition, we investigate the robustness of the proposed method on reduced\ntraining sets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.13002,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"A Battle of Network Structures: An Empirical Study of CNN, Transformer,\n  and MLP\n\n  Convolutional neural networks (CNN) are the dominant deep neural network\n(DNN) architecture for computer vision. Recently, Transformer and multi-layer\nperceptron (MLP)-based models, such as Vision Transformer and MLP-Mixer,\nstarted to lead new trends as they showed promising results in the ImageNet\nclassification task. In this paper, we conduct empirical studies on these DNN\nstructures and try to understand their respective pros and cons. To ensure a\nfair comparison, we first develop a unified framework called SPACH which adopts\nseparate modules for spatial and channel processing. Our experiments under the\nSPACH framework reveal that all structures can achieve competitive performance\nat a moderate scale. However, they demonstrate distinctive behaviors when the\nnetwork size scales up. Based on our findings, we propose two hybrid models\nusing convolution and Transformer modules. The resulting Hybrid-MS-S+ model\nachieves 83.9% top-1 accuracy with 63M parameters and 12.3G FLOPS. It is\nalready on par with the SOTA models with sophisticated designs. The code and\nmodels are publicly available at https:\/\/github.com\/microsoft\/SPACH.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10879,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000017219,
      "text":"Are socially-aware trajectory prediction models really socially-aware?\n\n  Our field has recently witnessed an arms race of neural network-based\ntrajectory predictors. While these predictors are at the core of many\napplications such as autonomous navigation or pedestrian flow simulations,\ntheir adversarial robustness has not been carefully studied. In this paper, we\nintroduce a socially-attended attack to assess the social understanding of\nprediction models in terms of collision avoidance. An attack is a small yet\ncarefully-crafted perturbations to fail predictors. Technically, we define\ncollision as a failure mode of the output, and propose hard- and soft-attention\nmechanisms to guide our attack. Thanks to our attack, we shed light on the\nlimitations of the current models in terms of their social understanding. We\ndemonstrate the strengths of our method on the recent trajectory prediction\nmodels. Finally, we show that our attack can be employed to increase the social\nunderstanding of state-of-the-art models. The code is available online:\nhttps:\/\/s-attack.github.io\/\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00894,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Generative Wind Power Curve Modeling Via Machine Vision: A Self-learning\n  Deep Convolutional Network Based Method\n\n  This paper develops a novel self-training U-net (STU-net) based method for\nthe automated WPC model generation without requiring data pre-processing. The\nself-training (ST) process of STU-net has two steps. First, different from\ntraditional studies regarding the WPC modeling as a curve fitting problem, in\nthis paper, we renovate the WPC modeling formulation from a machine vision\naspect. To develop sufficiently diversified training samples, we synthesize\nsupervisory control and data acquisition (SCADA) data based on a set of S-shape\nfunctions depicting WPCs. These synthesized SCADA data and WPC functions are\nvisualized as images and paired as training samples(I_x, I_wpc). A U-net is\nthen developed to approximate the model recovering I_wpc from I_x. The\ndeveloped U-net is applied into observed SCADA data and can successfully\ngenerate the I_wpc. Moreover, we develop a pixel mapping and correction process\nto derive a mathematical form f_wpc representing I_wpcgenerated previously. The\nproposed STU-net only needs to train once and does not require any data\npreprocessing in applications. Numerical experiments based on 76 WTs are\nconducted to validate the superiority of the proposed method by benchmarking\nagainst classical WPC modeling methods. To demonstrate the repeatability of the\npresented research, we release our code at https:\/\/github.com\/IkeYang\/STU-net.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05137,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"Zero-Shot Day-Night Domain Adaptation with a Physics Prior\n\n  We explore the zero-shot setting for day-night domain adaptation. The\ntraditional domain adaptation setting is to train on one domain and adapt to\nthe target domain by exploiting unlabeled data samples from the test set. As\ngathering relevant test data is expensive and sometimes even impossible, we\nremove any reliance on test data imagery and instead exploit a visual inductive\nprior derived from physics-based reflection models for domain adaptation. We\ncast a number of color invariant edge detectors as trainable layers in a\nconvolutional neural network and evaluate their robustness to illumination\nchanges. We show that the color invariant layer reduces the day-night\ndistribution shift in feature map activations throughout the network. We\ndemonstrate improved performance for zero-shot day to night domain adaptation\non both synthetic as well as natural datasets in various tasks, including\nclassification, segmentation and place recognition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.11727,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000139409,
      "text":"An Underwater Image Semantic Segmentation Method Focusing on Boundaries\n  and a Real Underwater Scene Semantic Segmentation Dataset\n\n  With the development of underwater object grabbing technology, underwater\nobject recognition and segmentation of high accuracy has become a challenge.\nThe existing underwater object detection technology can only give the general\nposition of an object, unable to give more detailed information such as the\noutline of the object, which seriously affects the grabbing efficiency. To\naddress this problem, we label and establish the first underwater semantic\nsegmentation dataset of real scene(DUT-USEG:DUT Underwater Segmentation\nDataset). The DUT- USEG dataset includes 6617 images, 1487 of which have\nsemantic segmentation and instance segmentation annotations, and the remaining\n5130 images have object detection box annotations. Based on this dataset, we\npropose a semi-supervised underwater semantic segmentation network focusing on\nthe boundaries(US-Net: Underwater Segmentation Network). By designing a pseudo\nlabel generator and a boundary detection subnetwork, this network realizes the\nfine learning of boundaries between underwater objects and background, and\nimproves the segmentation effect of boundary areas. Experiments show that the\nproposed method improves by 6.7% in three categories of holothurian, echinus,\nstarfish in DUT-USEG dataset, and achieves state-of-the-art results. The DUT-\nUSEG dataset will be released at https:\/\/github.com\/baxiyi\/DUT-USEG.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10743,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000038081,
      "text":"DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene\n  Context Graph and Relation-based Optimization\n\n  Panorama images have a much larger field-of-view thus naturally encode\nenriched scene context information compared to standard perspective images,\nwhich however is not well exploited in the previous scene understanding\nmethods. In this paper, we propose a novel method for panoramic 3D scene\nunderstanding which recovers the 3D room layout and the shape, pose, position,\nand semantic category for each object from a single full-view panorama image.\nIn order to fully utilize the rich context information, we design a novel graph\nneural network based context model to predict the relationship among objects\nand room layout, and a differentiable relationship-based optimization module to\noptimize object arrangement with well-designed objective functions on-the-fly.\nRealizing the existing data are either with incomplete ground truth or\noverly-simplified scene, we present a new synthetic dataset with good diversity\nin room layout and furniture placement, and realistic image quality for total\npanoramic 3D scene understanding. Experiments demonstrate that our method\noutperforms existing methods on panoramic scene understanding in terms of both\ngeometry accuracy and object arrangement. Code is available at\nhttps:\/\/chengzhag.github.io\/publication\/dpc.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10869,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000035763,
      "text":"DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras\n\n  We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM\nconsists of recurrent iterative updates of camera pose and pixelwise depth\nthrough a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving\nlarge improvements over prior work, and robust, suffering from substantially\nfewer catastrophic failures. Despite training on monocular video, it can\nleverage stereo or RGB-D video to achieve improved performance at test time.\nThe URL to our open source code is https:\/\/github.com\/princeton-vl\/DROID-SLAM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.10272,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000116229,
      "text":"Vogtareuth Rehab Depth Datasets: Benchmark for Marker-less Posture\n  Estimation in Rehabilitation\n\n  Posture estimation using a single depth camera has become a useful tool for\nanalyzing movements in rehabilitation. Recent advances in posture estimation in\ncomputer vision research have been possible due to the availability of\nlarge-scale pose datasets. However, the complex postures involved in\nrehabilitation exercises are not represented in the existing benchmark depth\ndatasets. To address this limitation, we propose two rehabilitation-specific\npose datasets containing depth images and 2D pose information of patients, both\nadult and children, performing rehab exercises. We use a state-of-the-art\nmarker-less posture estimation model which is trained on a non-rehab benchmark\ndataset. We evaluate it on our rehab datasets, and observe that the performance\ndegrades significantly from non-rehab to rehab, highlighting the need for these\ndatasets. We show that our dataset can be used to train pose models to detect\nrehab-specific complex postures. The datasets will be released for the benefit\nof the research community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.129,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000129475,
      "text":"Layout-to-Image Translation with Double Pooling Generative Adversarial\n  Networks\n\n  In this paper, we address the task of layout-to-image translation, which aims\nto translate an input semantic layout to a realistic image. One open challenge\nwidely observed in existing methods is the lack of effective semantic\nconstraints during the image translation process, leading to models that cannot\npreserve the semantic information and ignore the semantic dependencies within\nthe same object. To address this issue, we propose a novel Double Pooing GAN\n(DPGAN) for generating photo-realistic and semantically-consistent results from\nthe input layout. We also propose a novel Double Pooling Module (DPM), which\nconsists of the Square-shape Pooling Module (SPM) and the Rectangle-shape\nPooling Module (RPM). Specifically, SPM aims to capture short-range semantic\ndependencies of the input layout with different spatial scales, while RPM aims\nto capture long-range semantic dependencies from both horizontal and vertical\ndirections. We then effectively fuse both outputs of SPM and RPM to further\nenlarge the receptive field of our generator. Extensive experiments on five\npopular datasets show that the proposed DPGAN achieves better results than\nstate-of-the-art methods. Finally, both SPM and SPM are general and can be\nseamlessly integrated into any GAN-based architectures to strengthen the\nfeature representation. The code is available at\nhttps:\/\/github.com\/Ha0Tang\/DPGAN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.06681,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000064572,
      "text":"Multi-granularity for knowledge distillation\n\n  Considering the fact that students have different abilities to understand the\nknowledge imparted by teachers, a multi-granularity distillation mechanism is\nproposed for transferring more understandable knowledge for student networks. A\nmulti-granularity self-analyzing module of the teacher network is designed,\nwhich enables the student network to learn knowledge from different teaching\npatterns. Furthermore, a stable excitation scheme is proposed for robust\nsupervision for the student training. The proposed distillation mechanism can\nbe embedded into different distillation frameworks, which are taken as\nbaselines. Experiments show the mechanism improves the accuracy by 0.58% on\naverage and by 1.08% in the best over the baselines, which makes its\nperformance superior to the state-of-the-arts. It is also exploited that the\nstudent's ability of fine-tuning and robustness to noisy inputs can be improved\nvia the proposed mechanism. The code is available at\nhttps:\/\/github.com\/shaoeric\/multi-granularity-distillation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.00392,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000393722,
      "text":"Developing a Compressed Object Detection Model based on YOLOv4 for\n  Deployment on Embedded GPU Platform of Autonomous System\n\n  Latest CNN-based object detection models are quite accurate but require a\nhigh-performance GPU to run in real-time. They still are heavy in terms of\nmemory size and speed for an embedded system with limited memory space. Since\nthe object detection for autonomous system is run on an embedded processor, it\nis preferable to compress the detection network as light as possible while\npreserving the detection accuracy. There are several popular lightweight\ndetection models but their accuracy is too low for safe driving applications.\nTherefore, this paper proposes a new object detection model, referred as\nYOffleNet, which is compressed at a high ratio while minimizing the accuracy\nloss for real-time and safe driving application on an autonomous system. The\nbackbone network architecture is based on YOLOv4, but we could compress the\nnetwork greatly by replacing the high-calculation-load CSP DenseNet with the\nlighter modules of ShuffleNet. Experiments with KITTI dataset showed that the\nproposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could\nachieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).\nCompared to the high compression ratio, the accuracy is reduced slightly to\n85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network\nshowed a high potential to be deployed on the embedded system of the autonomous\nsystem for the real-time and accurate object detection applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.04644,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000276499,
      "text":"FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via\n  Multi-Scale Feature Decoupling Network\n\n  Food logo detection plays an important role in the multimedia for its wide\nreal-world applications, such as food recommendation of the self-service shop\nand infringement detection on e-commerce platforms. A large-scale food logo\ndataset is urgently needed for developing advanced food logo detection\nalgorithms. However, there are no available food logo datasets with food brand\ninformation. To support efforts towards food logo detection, we introduce the\ndataset FoodLogoDet-1500, a new large-scale publicly available food logo\ndataset, which has 1,500 categories, about 100,000 images and about 150,000\nmanually annotated food logo objects. We describe the collection and annotation\nprocess of FoodLogoDet-1500, analyze its scale and diversity, and compare it\nwith other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the\nfirst largest publicly available high-quality dataset for food logo detection.\nThe challenge of food logo detection lies in the large-scale categories and\nsimilarities between food logo categories. For that, we propose a novel food\nlogo detection method Multi-scale Feature Decoupling Network (MFDNet), which\ndecouples classification and regression into two branches and focuses on the\nclassification branch to solve the problem of distinguishing multiple food logo\ncategories. Specifically, we introduce the feature offset module, which\nutilizes the deformation-learning for optimal classification offset and can\neffectively obtain the most representative features of classification in\ndetection. In addition, we adopt a balanced feature pyramid in MFDNet, which\npays attention to global information, balances the multi-scale feature maps,\nand enhances feature extraction capability. Comprehensive experiments on\nFoodLogoDet-1500 and other two benchmark logo datasets demonstrate the\neffectiveness of the proposed method. The FoodLogoDet-1500 can be found at this\nhttps URL.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.02456,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"Residual Attention: A Simple but Effective Method for Multi-Label\n  Recognition\n\n  Multi-label image recognition is a challenging computer vision task of\npractical use. Progresses in this area, however, are often characterized by\ncomplicated methods, heavy computations, and lack of intuitive explanations. To\neffectively capture different spatial regions occupied by objects from\ndifferent categories, we propose an embarrassingly simple module, named\nclass-specific residual attention (CSRA). CSRA generates class-specific\nfeatures for every category by proposing a simple spatial attention score, and\nthen combines it with the class-agnostic average pooling feature. CSRA achieves\nstate-of-the-art results on multilabel recognition, and at the same time is\nmuch simpler than them. Furthermore, with only 4 lines of code, CSRA also leads\nto consistent improvement across many diverse pretrained models and datasets\nwithout any extra training. CSRA is both easy to implement and light in\ncomputations, which also enjoys intuitive explanations and visualizations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2108.05465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000172853,
      "text":"SIDER: Single-Image Neural Optimization for Facial Geometric Detail\n  Recovery\n\n  We present SIDER(Single-Image neural optimization for facial geometric DEtail\nRecovery), a novel photometric optimization method that recovers detailed\nfacial geometry from a single image in an unsupervised manner. Inspired by\nclassical techniques of coarse-to-fine optimization and recent advances in\nimplicit neural representations of 3D shape, SIDER combines a geometry prior\nbased on statistical models and Signed Distance Functions (SDFs) to recover\nfacial details from single images. First, it estimates a coarse geometry using\na morphable model represented as an SDF. Next, it reconstructs facial geometry\ndetails by optimizing a photometric loss with respect to the ground truth\nimage. In contrast to prior work, SIDER does not rely on any dataset priors and\ndoes not require additional supervision from multiple views, lighting changes\nor ground truth 3D shape. Extensive qualitative and quantitative evaluation\ndemonstrates that our method achieves state-of-the-art on facial geometric\ndetail recovery, using only a single in-the-wild image.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.10161,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000078479,
      "text":"3D Point Cloud Completion with Geometric-Aware Adversarial Augmentation\n\n  With the popularity of 3D sensors in self-driving and other robotics\napplications, extensive research has focused on designing novel neural network\narchitectures for accurate 3D point cloud completion. However, unlike in point\ncloud classification and reconstruction, the role of adversarial samples in3D\npoint cloud completion has seldom been explored. In this work, we show that\ntraining with adversarial samples can improve the performance of neural\nnetworks on 3D point cloud completion tasks. We propose a novel approach to\ngenerate adversarial samples that benefit both the performance of clean and\nadversarial samples. In contrast to the PGD-k attack, our method generates\nadversarial samples that keep the geometric features in clean samples and\ncontain few outliers. In particular, we use principal directions to constrain\nthe adversarial perturbations for each input point. The gradient components in\nthe mean direction of principal directions are taken as adversarial\nperturbations. In addition, we also investigate the effect of using the minimum\ncurvature direction. Besides, we adopt attack strength accumulation and\nauxiliary Batch Normalization layers method to speed up the training process\nand alleviate the distribution mismatch between clean and adversarial samples.\nExperimental results show that training with the adversarial samples crafted by\nour method effectively enhances the performance of PCN on the ShapeNet dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.06671,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000003212,
      "text":"High-Resolution Image Harmonization via Collaborative Dual\n  Transformations\n\n  Given a composite image, image harmonization aims to adjust the foreground to\nmake it compatible with the background. High-resolution image harmonization is\nin high demand, but still remains unexplored. Conventional image harmonization\nmethods learn global RGB-to-RGB transformation which could effortlessly scale\nto high resolution, but ignore diverse local context. Recent deep learning\nmethods learn the dense pixel-to-pixel transformation which could generate\nharmonious outputs, but are highly constrained in low resolution. In this work,\nwe propose a high-resolution image harmonization network with Collaborative\nDual Transformation (CDTNet) to combine pixel-to-pixel transformation and\nRGB-to-RGB transformation coherently in an end-to-end network. Our CDTNet\nconsists of a low-resolution generator for pixel-to-pixel transformation, a\ncolor mapping module for RGB-to-RGB transformation, and a refinement module to\ntake advantage of both. Extensive experiments on high-resolution benchmark\ndataset and our created high-resolution real composite images demonstrate that\nour CDTNet strikes a good balance between efficiency and effectiveness. Our\nused datasets can be found in\nhttps:\/\/github.com\/bcmi\/CDTNet-High-Resolution-Image-Harmonization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Image Synthesis via Semantic Composition\n\n  In this paper, we present a novel approach to synthesize realistic images\nbased on their semantic layouts. It hypothesizes that for objects with similar\nappearance, they share similar representation. Our method establishes\ndependencies between regions according to their appearance correlation,\nyielding both spatially variant and associated representations. Conditioning on\nthese features, we propose a dynamic weighted network constructed by spatially\nconditional computation (with both convolution and normalization). More than\npreserving semantic distinctions, the given dynamic network strengthens\nsemantic relevance, benefiting global structure and detail synthesis. We\ndemonstrate that our method gives the compelling generation performance\nqualitatively and quantitatively with extensive experiments on benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.03585,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000001159,
      "text":"Matching in the Dark: A Dataset for Matching Image Pairs of Low-light\n  Scenes\n\n  This paper considers matching images of low-light scenes, aiming to widen the\nfrontier of SfM and visual SLAM applications. Recent image sensors can record\nthe brightness of scenes with more than eight-bit precision, available in their\nRAW-format image. We are interested in making full use of such high-precision\ninformation to match extremely low-light scene images that conventional methods\ncannot handle. For extreme low-light scenes, even if some of their brightness\ninformation exists in the RAW format images' low bits, the standard raw image\nprocessing on cameras fails to utilize them properly. As was recently shown by\nChen et al., CNNs can learn to produce images with a natural appearance from\nsuch RAW-format images. To consider if and how well we can utilize such\ninformation stored in RAW-format images for image matching, we have created a\nnew dataset named MID (matching in the dark). Using it, we experimentally\nevaluated combinations of eight image-enhancing methods and eleven image\nmatching methods consisting of classical\/neural local descriptors and\nclassical\/neural initial point-matching methods. The results show the advantage\nof using the RAW-format images and the strengths and weaknesses of the above\ncomponent methods. They also imply there is room for further research.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00853,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.000013974,
      "text":"Stain-Robust Mitotic Figure Detection for the Mitosis Domain\n  Generalization Challenge\n\n  The detection of mitotic figures from different scanners\/sites remains an\nimportant topic of research, owing to its potential in assisting clinicians\nwith tumour grading. The MItosis DOmain Generalization (MIDOG) challenge aims\nto test the robustness of detection models on unseen data from multiple\nscanners for this task. We present a short summary of the approach employed by\nthe TIA Centre team to address this challenge. Our approach is based on a\nhybrid detection model, where mitotic candidates are segmented on stain\nnormalised images, before being refined by a deep learning classifier.\nCross-validation on the training images achieved the F1-score of 0.786 and\n0.765 on the preliminary test set, demonstrating the generalizability of our\nmodel to unseen data from new scanners.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00778,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Better Self-training for Image Classification through Self-supervision\n\n  Self-training is a simple semi-supervised learning approach: Unlabelled\nexamples that attract high-confidence predictions are labelled with their\npredictions and added to the training set, with this process being repeated\nmultiple times. Recently, self-supervision -- learning without manual\nsupervision by solving an automatically-generated pretext task -- has gained\nprominence in deep learning. This paper investigates three different ways of\nincorporating self-supervision into self-training to improve accuracy in image\nclassification: self-supervision as pretraining only, self-supervision\nperformed exclusively in the first iteration of self-training, and\nself-supervision added to every iteration of self-training. Empirical results\non the SVHN, CIFAR-10, and PlantVillage datasets, using both training from\nscratch, and Imagenet-pretrained weights, show that applying self-supervision\nonly in the first iteration of self-training can greatly improve accuracy, for\na modest increase in computation time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07734,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Few-Shot Object Detection by Attending to Per-Sample-Prototype\n\n  Few-shot object detection aims to detect instances of specific categories in\na query image with only a handful of support samples. Although this takes less\neffort than obtaining enough annotated images for supervised object detection,\nit results in a far inferior performance compared to the conventional object\ndetection methods. In this paper, we propose a meta-learning-based approach\nthat considers the unique characteristics of each support sample. Rather than\nsimply averaging the information of the support samples to generate a single\nprototype per category, our method can better utilize the information of each\nsupport sample by treating each support sample as an individual prototype.\nSpecifically, we introduce two types of attention mechanisms for aggregating\nthe query and support feature maps. The first is to refine the information of\nfew-shot samples by extracting shared information between the support samples\nthrough attention. Second, each support sample is used as a class code to\nleverage the information by comparing similarities between each support feature\nand query features. Our proposed method is complementary to the previous\nmethods, making it easy to plug and play for further improvement. We have\nevaluated our method on PASCAL VOC and COCO benchmarks, and the results verify\nthe effectiveness of our method. In particular, the advantages of our method\nare maximized when there is more diversity among support data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.14137,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000184443,
      "text":"Geometry-Entangled Visual Semantic Transformer for Image Captioning\n\n  Recent advancements of image captioning have featured Visual-Semantic Fusion\nor Geometry-Aid attention refinement. However, those fusion-based models, they\nare still criticized for the lack of geometry information for inter and intra\nattention refinement. On the other side, models based on Geometry-Aid attention\nstill suffer from the modality gap between visual and semantic information. In\nthis paper, we introduce a novel Geometry-Entangled Visual Semantic Transformer\n(GEVST) network to realize the complementary advantages of Visual-Semantic\nFusion and Geometry-Aid attention refinement. Concretely, a Dense-Cap model\nproposes some dense captions with corresponding geometry information at first.\nThen, to empower GEVST with the ability to bridge the modality gap among visual\nand semantic information, we build four parallel transformer encoders VV(Pure\nVisual), VS(Semantic fused to Visual), SV(Visual fused to Semantic), SS(Pure\nSemantic) for final caption generation. Both visual and semantic geometry\nfeatures are used in the Fusion module and also the Self-Attention module for\nbetter attention measurement. To validate our model, we conduct extensive\nexperiments on the MS-COCO dataset, the experimental results show that our\nGEVST model can obtain promising performance gains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.0727,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000083778,
      "text":"Distract Your Attention: Multi-head Cross Attention Network for Facial\n  Expression Recognition\n\n  We present a novel facial expression recognition network, called Distract\nyour Attention Network (DAN). Our method is based on two key observations.\nFirstly, multiple classes share inherently similar underlying facial\nappearance, and their differences could be subtle. Secondly, facial expressions\nexhibit themselves through multiple facial regions simultaneously, and the\nrecognition requires a holistic approach by encoding high-order interactions\namong local features. To address these issues, we propose our DAN with three\nkey components: Feature Clustering Network (FCN), Multi-head cross Attention\nNetwork (MAN), and Attention Fusion Network (AFN). The FCN extracts robust\nfeatures by adopting a large-margin learning objective to maximize class\nseparability. In addition, the MAN instantiates a number of attention heads to\nsimultaneously attend to multiple facial areas and build attention maps on\nthese regions. Further, the AFN distracts these attentions to multiple\nlocations before fusing the attention maps to a comprehensive one. Extensive\nexperiments on three public datasets (including AffectNet, RAF-DB, and SFEW\n2.0) verified that the proposed method consistently achieves state-of-the-art\nfacial expression recognition performance. Code will be made available at\nhttps:\/\/github.com\/yaoing\/DAN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.02227,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000149343,
      "text":"Learning to Generate Scene Graph from Natural Language Supervision\n\n  Learning from image-text data has demonstrated recent success for many\nrecognition tasks, yet is currently limited to visual features or individual\nvisual concepts such as objects. In this paper, we propose one of the first\nmethods that learn from image-sentence pairs to extract a graphical\nrepresentation of localized objects and their relationships within an image,\nknown as scene graph. To bridge the gap between images and texts, we leverage\nan off-the-shelf object detector to identify and localize object instances,\nmatch labels of detected regions to concepts parsed from captions, and thus\ncreate \"pseudo\" labels for learning scene graph. Further, we design a\nTransformer-based model to predict these \"pseudo\" labels via a masked token\nprediction task. Learning from only image-sentence pairs, our model achieves\n30% relative gain over a latest method trained with human-annotated unlocalized\nscene graphs. Our model also shows strong results for weakly and fully\nsupervised scene graph generation. In addition, we explore an open-vocabulary\nsetting for detecting scene graphs, and present the first result for open-set\nscene graph generation. Our code is available at\nhttps:\/\/github.com\/YiwuZhong\/SGG_from_NLS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.01504,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000003643,
      "text":"Ghost Loss to Question the Reliability of Training Data\n\n  Supervised image classification problems rely on training data assumed to\nhave been correctly annotated; this assumption underpins most works in the\nfield of deep learning. In consequence, during its training, a network is\nforced to match the label provided by the annotator and is not given the\nflexibility to choose an alternative to inconsistencies that it might be able\nto detect. Therefore, erroneously labeled training images may end up\n``correctly'' classified in classes which they do not actually belong to. This\nmay reduce the performances of the network and thus incite to build more\ncomplex networks without even checking the quality of the training data. In\nthis work, we question the reliability of the annotated datasets. For that\npurpose, we introduce the notion of ghost loss, which can be seen as a regular\nloss that is zeroed out for some predicted values in a deterministic way and\nthat allows the network to choose an alternative to the given label without\nbeing penalized. After a proof of concept experiment, we use the ghost loss\nprinciple to detect confusing images and erroneously labeled images in\nwell-known training datasets (MNIST, Fashion-MNIST, SVHN, CIFAR10) and we\nprovide a new tool, called sanity matrix, for summarizing these confusions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.07407,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Semi-supervised Contrastive Learning for Label-efficient Medical Image\n  Segmentation\n\n  The success of deep learning methods in medical image segmentation tasks\nheavily depends on a large amount of labeled data to supervise the training. On\nthe other hand, the annotation of biomedical images requires domain knowledge\nand can be laborious. Recently, contrastive learning has demonstrated great\npotential in learning latent representation of images even without any label.\nExisting works have explored its application to biomedical image segmentation\nwhere only a small portion of data is labeled, through a pre-training phase\nbased on self-supervised contrastive learning without using any labels followed\nby a supervised fine-tuning phase on the labeled portion of data only. In this\npaper, we establish that by including the limited label in formation in the\npre-training phase, it is possible to boost the performance of contrastive\nlearning. We propose a supervised local contrastive loss that leverages limited\npixel-wise annotation to force pixels with the same label to gather around in\nthe embedding space. Such loss needs pixel-wise computation which can be\nexpensive for large images, and we further propose two strategies, downsampling\nand block division, to address the issue. We evaluate our methods on two public\nbiomedical image datasets of different modalities. With different amounts of\nlabeled data, our methods consistently outperform the state-of-the-art\ncontrast-based methods and other semi-supervised learning techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08924,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000026822,
      "text":"A Studious Approach to Semi-Supervised Learning\n\n  The problem of learning from few labeled examples while using large amounts\nof unlabeled data has been approached by various semi-supervised methods.\nAlthough these methods can achieve superior performance, the models are often\nnot deployable due to the large number of parameters. This paper is an ablation\nstudy of distillation in a semi-supervised setting, which not just reduces the\nnumber of parameters of the model but can achieve this while improving the\nperformance over the baseline supervised model and making it better at\ngeneralizing. After the supervised pretraining, the network is used as a\nteacher model, and a student network is trained over the soft labels that the\nteacher model generates over the entire unlabeled data. We find that the fewer\nthe labels, the more this approach benefits from a smaller student network.\nThis brings forward the potential of distillation as an effective solution to\nenhance performance in semi-supervised computer vision tasks while maintaining\ndeployability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00211,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Efficient Person Search: An Anchor-Free Approach\n\n  Person search aims to simultaneously localize and identify a query person\nfrom realistic, uncropped images. To achieve this goal, state-of-the-art models\ntypically add a re-id branch upon two-stage detectors like Faster R-CNN. Owing\nto the ROI-Align operation, this pipeline yields promising accuracy as re-id\nfeatures are explicitly aligned with the corresponding object regions, but in\nthe meantime, it introduces high computational overhead due to dense object\nanchors. In this work, we present an anchor-free approach to efficiently\ntackling this challenging task, by introducing the following dedicated designs.\nFirst, we select an anchor-free detector (i.e., FCOS) as the prototype of our\nframework. Due to the lack of dense object anchors, it exhibits significantly\nhigher efficiency compared with existing person search models. Second, when\ndirectly accommodating this anchor-free detector for person search, there exist\nseveral major challenges in learning robust re-id features, which we summarize\nas the misalignment issues in different levels (i.e., scale, region, and task).\nTo address these issues, we propose an aligned feature aggregation module to\ngenerate more discriminative and robust feature embeddings. Accordingly, we\nname our model as Feature-Aligned Person Search Network (AlignPS). Third, by\ninvestigating the advantages of both anchor-based and anchor-free models, we\nfurther augment AlignPS with an ROI-Align head, which significantly improves\nthe robustness of re-id features while still keeping our model highly\nefficient. Extensive experiments conducted on two challenging benchmarks (i.e.,\nCUHK-SYSU and PRW) demonstrate that our framework achieves state-of-the-art or\ncompetitive performance, while displaying higher efficiency. All the source\ncodes, data, and trained models are available at:\nhttps:\/\/github.com\/daodaofr\/alignps.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.00317,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000141064,
      "text":"BVMatch: Lidar-based Place Recognition Using Bird's-eye View Images\n\nRecognizing places using Lidar in large-scale environments is challenging due to the sparse nature of point cloud data. In this paper we present BVMatch, a Lidar-based frame-to-frame place recognition framework, that is capable of estimating 2D relative poses. Based on the assumption that the ground area can be approximated as a plane, we uniformly discretize the ground area into grids and project 3D Lidar scans to bird's-eye view (BV) images. We further use a bank of Log-Gabor filters to build a maximum index map (MIM) that encodes the orientation information of the structures in the images. We analyze the orientation characteristics of MIM theoretically and introduce a novel descriptor called bird's-eye view feature transform (BVFT). The proposed BVFT is insensitive to rotation and intensity variations of BV images. Leveraging the BVFT descriptors, we unify the Lidar place recognition and pose estimation tasks into the BVMatch framework. The experiments conducted on three large-scale datasets show that BVMatch outperforms the state-of-the-art methods in terms of both recall rate of place recognition and pose estimation accuracy. The source code of our method is publicly available at https:\/\/github.com\/zjuluolun\/BVMatch.",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.09827,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Viewpoint Invariant Dense Matching for Visual Geolocalization\n\n  In this paper we propose a novel method for image matching based on dense\nlocal features and tailored for visual geolocalization. Dense local features\nmatching is robust against changes in illumination and occlusions, but not\nagainst viewpoint shifts which are a fundamental aspect of geolocalization. Our\nmethod, called GeoWarp, directly embeds invariance to viewpoint shifts in the\nprocess of extracting dense features. This is achieved via a trainable module\nwhich learns from the data an invariance that is meaningful for the task of\nrecognizing places. We also devise a new self-supervised loss and two new\nweakly supervised losses to train this module using only unlabeled data and\nweak labels. GeoWarp is implemented efficiently as a re-ranking method that can\nbe easily embedded into pre-existing visual geolocalization pipelines.\nExperimental validation on standard geolocalization benchmarks demonstrates\nthat GeoWarp boosts the accuracy of state-of-the-art retrieval architectures.\nThe code and trained models are available at\nhttps:\/\/github.com\/gmberton\/geo_warp\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.02762,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"STRIVE: Scene Text Replacement In Videos\n\n  We propose replacing scene text in videos using deep style transfer and\nlearned photometric transformations.Building on recent progress on still image\ntext replacement,we present extensions that alter text while preserving the\nappearance and motion characteristics of the original video.Compared to the\nproblem of still image text replacement,our method addresses additional\nchallenges introduced by video, namely effects induced by changing lighting,\nmotion blur, diverse variations in camera-object pose over time,and\npreservation of temporal consistency. We parse the problem into three steps.\nFirst, the text in all frames is normalized to a frontal pose using a\nspatio-temporal trans-former network. Second, the text is replaced in a single\nreference frame using a state-of-art still-image text replacement method.\nFinally, the new text is transferred from the reference to remaining frames\nusing a novel learned image transformation network that captures lighting and\nblur effects in a temporally consistent manner. Results on synthetic and\nchallenging real videos show realistic text trans-fer, competitive quantitative\nand qualitative performance,and superior inference speed relative to\nalternatives. We introduce new synthetic and real-world datasets with paired\ntext objects. To the best of our knowledge this is the first attempt at deep\nvideo text replacement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.05457,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000023511,
      "text":"What happens in Face during a facial expression? Using data mining\n  techniques to analyze facial expression motion vectors\n\n  One of the most common problems encountered in human-computer interaction is\nautomatic facial expression recognition. Although it is easy for human observer\nto recognize facial expressions, automatic recognition remains difficult for\nmachines. One of the methods that machines can recognize facial expression is\nanalyzing the changes in face during facial expression presentation. In this\npaper, optical flow algorithm was used to extract deformation or motion vectors\ncreated in the face because of facial expressions. Then, these extracted motion\nvectors are used to be analyzed. Their positions and directions were exploited\nfor automatic facial expression recognition using different data mining\ntechniques. It means that by employing motion vector features used as our data,\nfacial expressions were recognized. Some of the most state-of-the-art\nclassification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL),\nSVM and Discriminant algorithms were used to classify the extracted motion\nvectors. Using 10-fold cross validation, their performances were calculated. To\ncompare their performance more precisely, the test was repeated 50 times.\nMeanwhile, the deformation of face was also analyzed in this research. For\nexample, what exactly happened in each part of face when a person showed fear?\nExperimental results on Extended Cohen-Kanade (CK+) facial expression dataset\ndemonstrated that the best methods were DL, SVM and C5.0, with the accuracy of\n95.3%, 92.8% and 90.2% respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.08809,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000113911,
      "text":"HYouTube: Video Harmonization Dataset\n\n  Video composition aims to generate a composite video by combining the\nforeground of one video with the background of another video, but the inserted\nforeground may be incompatible with the background in terms of color and\nillumination. Video harmonization aims to adjust the foreground of a composite\nvideo to make it compatible with the background. So far, video harmonization\nhas only received limited attention and there is no public dataset for video\nharmonization. In this work, we construct a new video harmonization dataset\nHYouTube by adjusting the foreground of real videos to create synthetic\ncomposite videos. Considering the domain gap between real composite videos and\nsynthetic composite videos, we additionally create 100 real composite videos\nvia copy-and-paste. Datasets are available at\nhttps:\/\/github.com\/bcmi\/Video-Harmonization-Dataset-HYouTube.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2109.11204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000024504,
      "text":"Towards Fine-grained 3D Face Dense Registration: An Optimal Dividing and\n  Diffusing Method\n\n  Dense vertex-to-vertex correspondence between 3D faces is a fundamental and\nchallenging issue for 3D&2D face analysis. While the sparse landmarks have\nanatomically ground-truth correspondence, the dense vertex correspondences on\nmost facial regions are unknown. In this view, the current literatures commonly\nresult in reasonable but diverse solutions, which deviate from the optimum to\nthe 3D face dense registration problem. In this paper, we revisit dense\nregistration by a dimension-degraded problem, i.e. proportional segmentation of\na line, and employ an iterative dividing and diffusing method to reach the\nfinal solution uniquely. This method is then extended to 3D surface by\nformulating a local registration problem for dividing and a linear least-square\nproblem for diffusing, with constraints on fixed features. On this basis, we\nfurther propose a multi-resolution algorithm to accelerate the computational\nprocess. The proposed method is linked to a novel local scaling metric, where\nwe illustrate the physical meaning as smooth rearrangement for local cells of\n3D facial shapes. Extensive experiments on public datasets demonstrate the\neffectiveness of the proposed method in various aspects. Generally, the\nproposed method leads to coherent local registrations and elegant mesh grid\nroutines for fine-grained 3D face dense registrations, which benefits many\ndownstream applications significantly. It can also be applied to dense\ncorrespondence for other format of data which are not limited to face. The core\ncode will be publicly available at\nhttps:\/\/github.com\/NaughtyZZ\/3D_face_dense_registration.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01758,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000002914,
      "text":"Quantified Facial Expressiveness for Affective Behavior Analytics\n\n  The quantified measurement of facial expressiveness is crucial to analyze\nhuman affective behavior at scale. Unfortunately, methods for expressiveness\nquantification at the video frame-level are largely unexplored, unlike the\nstudy of discrete expression. In this work, we propose an algorithm that\nquantifies facial expressiveness using a bounded, continuous expressiveness\nscore using multimodal facial features, such as action units (AUs), landmarks,\nhead pose, and gaze. The proposed algorithm more heavily weights AUs with high\nintensities and large temporal changes. The proposed algorithm can compute the\nexpressiveness in terms of discrete expression, and can be used to perform\ntasks including facial behavior tracking and subjectivity quantification in\ncontext. Our results on benchmark datasets show the proposed algorithm is\neffective in terms of capturing temporal changes and expressiveness, measuring\nsubjective differences in context, and extracting useful insight.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10536,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000070201,
      "text":"Improving Model Generalization by Agreement of Learned Representations\n  from Data Augmentation\n\n  Data augmentation reduces the generalization error by forcing a model to\nlearn invariant representations given different transformations of the input\nimage. In computer vision, on top of the standard image processing functions,\ndata augmentation techniques based on regional dropout such as CutOut, MixUp,\nand CutMix and policy-based selection such as AutoAugment demonstrated\nstate-of-the-art (SOTA) results. With an increasing number of data augmentation\nalgorithms being proposed, the focus is always on optimizing the input-output\nmapping while not realizing that there might be an untapped value in the\ntransformed images with the same label. We hypothesize that by forcing the\nrepresentations of two transformations to agree, we can further reduce the\nmodel generalization error. We call our proposed method Agreement Maximization\nor simply AgMax. With this simple constraint applied during training, empirical\nresults show that data augmentation algorithms can further improve the\nclassification accuracy of ResNet50 on ImageNet by up to 1.5%, WideResNet40-2\non CIFAR10 by up to 0.7%, WideResNet40-2 on CIFAR100 by up to 1.6%, and LeNet5\non Speech Commands Dataset by up to 1.4%. Experimental results further show\nthat unlike other regularization terms such as label smoothing, AgMax can take\nadvantage of the data augmentation to consistently improve model generalization\nby a significant margin. On downstream tasks such as object detection and\nsegmentation on PascalVOC and COCO, AgMax pre-trained models outperforms other\ndata augmentation methods by as much as 1.0mAP (box) and 0.5mAP (mask). Code is\navailable at https:\/\/github.com\/roatienza\/agmax.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00869,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010265,
      "text":"BdSL36: A Dataset for Bangladeshi Sign Letters Recognition\n\n  Bangladeshi Sign Language (BdSL) is a commonly used medium of communication\nfor the hearing-impaired people in Bangladesh. A real-time BdSL interpreter\nwith no controlled lab environment has a broad social impact and an interesting\navenue of research as well. Also, it is a challenging task due to the variation\nin different subjects (age, gender, color, etc.), complex features, and\nsimilarities of signs and clustered backgrounds. However, the existing dataset\nfor BdSL classification task is mainly built in a lab friendly setup which\nlimits the application of powerful deep learning technology. In this paper, we\nintroduce a dataset named BdSL36 which incorporates background augmentation to\nmake the dataset versatile and contains over four million images belonging to\n36 categories. Besides, we annotate about 40,000 images with bounding boxes to\nutilize the potentiality of object detection algorithms. Furthermore, several\nintensive experiments are performed to establish the baseline performance of\nour BdSL36. Moreover, we employ beta testing of our classifiers at the user\nlevel to justify the possibilities of real-world application with this dataset.\nWe believe our BdSL36 will expedite future research on practical sign letter\nclassification. We make the datasets and all the pre-trained models available\nfor further researcher.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01774,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021855,
      "text":"HighlightMe: Detecting Highlights from Human-Centric Videos\n\n  We present a domain- and user-preference-agnostic approach to detect\nhighlightable excerpts from human-centric videos. Our method works on the\ngraph-based representation of multiple observable human-centric modalities in\nthe videos, such as poses and faces. We use an autoencoder network equipped\nwith spatial-temporal graph convolutions to detect human activities and\ninteractions based on these modalities. We train our network to map the\nactivity- and interaction-based latent structural representations of the\ndifferent modalities to per-frame highlight scores based on the\nrepresentativeness of the frames. We use these scores to compute which frames\nto highlight and stitch contiguous frames to produce the excerpts. We train our\nnetwork on the large-scale AVA-Kinetics action dataset and evaluate it on four\nbenchmark video highlight datasets: DSH, TVSum, PHD2, and SumMe. We observe a\n4-12% improvement in the mean average precision of matching the human-annotated\nhighlights over state-of-the-art methods in these datasets, without requiring\nany user-provided preferences or dataset-specific fine-tuning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.00644,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"RoomStructNet: Learning to Rank Non-Cuboidal Room Layouts From Single\n  View\n\n  In this paper, we present a new approach to estimate the layout of a room\nfrom its single image. While recent approaches for this task use robust\nfeatures learnt from data, they resort to optimization for detecting the final\nlayout. In addition to using learnt robust features, our approach learns an\nadditional ranking function to estimate the final layout instead of using\noptimization. To learn this ranking function, we propose a framework to train a\nCNN using max-margin structure cost. Also, while most approaches aim at\ndetecting cuboidal layouts, our approach detects non-cuboidal layouts for which\nwe explicitly estimates layout complexity parameters. We use these parameters\nto propose layout candidates in a novel way. Our approach shows\nstate-of-the-art results on standard datasets with mostly cuboidal layouts and\nalso performs well on a dataset containing rooms with non-cuboidal layouts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.07774,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"4D flight trajectory prediction using a hybrid Deep Learning prediction\n  method based on ADS-B technology: a case study of Hartsfield-Jackson Atlanta\n  International Airport(ATL)\n\n  The core of any flight schedule is the trajectories. In particular, 4D\ntrajectories are the most crucial component for flight attribute prediction. In\nparticular, 4D trajectories are the most crucial component for flight attribute\nprediction. Each trajectory contains spatial and temporal features that are\nassociated with uncertainties that make the prediction process complex. Today\nbecause of the increasing demand for air transportation, it is compulsory for\nairports and airlines to have an optimized schedule to use all of the airport's\ninfrastructure potential. This is possible using advanced trajectory prediction\nmethods. This paper proposes a novel hybrid deep learning model to extract the\nspatial and temporal features considering the uncertainty of the prediction\nmodel for Hartsfield-Jackson Atlanta International Airport(ATL). Automatic\nDependent Surveillance-Broadcast (ADS-B) data are used as input to the models.\nThis research is conducted in three steps: (a) data preprocessing; (b)\nprediction by a hybrid Convolutional Neural Network and Gated Recurrent Unit\n(CNN-GRU) along with a 3D-CNN model; (c) The third and last step is the\ncomparison of the model's performance with the proposed model by comparing the\nexperimental results. The deep model uncertainty is considered using the\nMont-Carlo dropout (MC-Dropout). Mont-Carlo dropouts are added to the network\nlayers to enhance the model's prediction performance by a robust approach of\nswitching off between different neurons. The results show that the proposed\nmodel has low error measurements compared to the other models (i.e., 3D CNN,\nCNN-GRU). The model with MC-dropout reduces the error further by an average of\n21 %.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.05119,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"An automated threshold Edge Drawing algorithm\n\n  Parameter choosing in classical edge detection algorithms can be a costly and\ncomplex task. Choosing the correct parameters can improve considerably the\nresulting edge-map. In this paper we present a version of Edge Drawing\nalgorithm in which we include an automated threshold choosing step. To better\nhighlight the effect of this additional step we use different first order\noperators in the algorithm. Visual and statistical results are presented to\nsustain the benefits of the proposed automated threshold scheme.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.05379,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Point Cloud Augmentation with Weighted Local Transformations\n\n  Despite the extensive usage of point clouds in 3D vision, relatively limited\ndata are available for training deep neural networks. Although data\naugmentation is a standard approach to compensate for the scarcity of data, it\nhas been less explored in the point cloud literature. In this paper, we propose\na simple and effective augmentation method called PointWOLF for point cloud\naugmentation. The proposed method produces smoothly varying non-rigid\ndeformations by locally weighted transformations centered at multiple anchor\npoints. The smooth deformations allow diverse and realistic augmentations.\nFurthermore, in order to minimize the manual efforts to search the optimal\nhyperparameters for augmentation, we present AugTune, which generates augmented\nsamples of desired difficulties producing targeted confidence scores. Our\nexperiments show our framework consistently improves the performance for both\nshape classification and part segmentation tasks. Particularly, with\nPointNet++, PointWOLF achieves the state-of-the-art 89.7 accuracy on shape\nclassification with the real-world ScanObjectNN dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.10734,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Self-Supervision and Spatial-Sequential Attention Based Loss for\n  Multi-Person Pose Estimation\n\n  Bottom-up based multi-person pose estimation approaches use heatmaps with\nauxiliary predictions to estimate joint positions and belonging at one time.\nRecently, various combinations between auxiliary predictions and heatmaps have\nbeen proposed for higher performance, these predictions are supervised by the\ncorresponding L2 loss function directly. However, the lack of more explicit\nsupervision results in low features utilization and contradictions between\npredictions in one model. To solve these problems, this paper proposes (i) a\nnew loss organization method which uses self-supervised heatmaps to reduce\nprediction contradictions and spatial-sequential attention to enhance networks'\nfeatures extraction; (ii) a new combination of predictions composed by\nheatmaps, Part Affinity Fields (PAFs) and our block-inside offsets to fix\npixel-level joints positions and further demonstrates the effectiveness of\nproposed loss function. Experiments are conducted on the MS COCO keypoint\ndataset and adopting OpenPose as the baseline model. Our method outperforms the\nbaseline overall. On the COCO verification dataset, the mAP of OpenPose trained\nwith our proposals outperforms the OpenPose baseline by over 5.5%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.05052,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"LSC-GAN: Latent Style Code Modeling for Continuous Image-to-image\n  Translation\n\n  Image-to-image (I2I) translation is usually carried out among discrete\ndomains. However, image domains, often corresponding to a physical value, are\nusually continuous. In other words, images gradually change with the value, and\nthere exists no obvious gap between different domains. This paper intends to\nbuild the model for I2I translation among continuous varying domains. We first\ndivide the whole domain coverage into discrete intervals, and explicitly model\nthe latent style code for the center of each interval. To deal with continuous\ntranslation, we design the editing modules, changing the latent style code\nalong two directions. These editing modules help to constrain the codes for\ndomain centers during training, so that the model can better understand the\nrelation among them. To have diverse results, the latent style code is further\ndiversified with either the random noise or features from the reference image,\ngiving the individual style code to the decoder for label-based or\nreference-based synthesis. Extensive experiments on age and viewing angle\ntranslation show that the proposed method can achieve high-quality results, and\nit is also flexible for users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.02551,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0001114607,
      "text":"A Survey of Fish Tracking Techniques Based on Computer Vision\n\n  Fish tracking is a key technology for obtaining movement trajectories and\nidentifying abnormal behavior. However, it faces considerable challenges,\nincluding occlusion, multi-scale tracking, and fish deformation. Notably,\nextant reviews have focused more on behavioral analysis rather than providing a\ncomprehensive overview of computer vision-based fish tracking approaches. This\npaper presents a comprehensive review of the advancements of fish tracking\ntechnologies over the past seven years (2017-2023). It explores diverse fish\ntracking techniques with an emphasis on fundamental localization and tracking\nmethods. Auxiliary plugins commonly integrated into fish tracking systems, such\nas underwater image enhancement and re-identification, are also examined.\nAdditionally, this paper summarizes open-source datasets, evaluation metrics,\nchallenges, and applications in fish tracking research. Finally, a\ncomprehensive discussion offers insights and future directions for vision-based\nfish tracking techniques. We hope that our work could provide a partial\nreference in the development of fish tracking algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.03479,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"Camera Calibration through Camera Projection Loss\n\n  Camera calibration is a necessity in various tasks including 3D\nreconstruction, hand-eye coordination for a robotic interaction, autonomous\ndriving, etc. In this work we propose a novel method to predict extrinsic\n(baseline, pitch, and translation), intrinsic (focal length and principal point\noffset) parameters using an image pair. Unlike existing methods, instead of\ndesigning an end-to-end solution, we proposed a new representation that\nincorporates camera model equations as a neural network in multi-task learning\nframework. We estimate the desired parameters via novel camera projection loss\n(CPL) that uses the camera model neural network to reconstruct the 3D points\nand uses the reconstruction loss to estimate the camera parameters. To the best\nof our knowledge, ours is the first method to jointly estimate both the\nintrinsic and extrinsic parameters via a multi-task learning methodology that\ncombines analytical equations in learning framework for the estimation of\ncamera parameters. We also proposed a novel dataset using CARLA Simulator.\nEmpirically, we demonstrate that our proposed approach achieves better\nperformance with respect to both deep learning-based and traditional methods on\n8 out of 10 parameters evaluated using both synthetic and real data. Our code\nand generated dataset are available at\nhttps:\/\/github.com\/thanif\/Camera-Calibration-through-Camera-Projection-Loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.02526,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000211265,
      "text":"Coarse-to-Fine Reasoning for Visual Question Answering\n\n  Bridging the semantic gap between image and question is an important step to\nimprove the accuracy of the Visual Question Answering (VQA) task. However, most\nof the existing VQA methods focus on attention mechanisms or visual relations\nfor reasoning the answer, while the features at different semantic levels are\nnot fully utilized. In this paper, we present a new reasoning framework to fill\nthe gap between visual features and semantic clues in the VQA task. Our method\nfirst extracts the features and predicates from the image and question. We then\npropose a new reasoning framework to effectively jointly learn these features\nand predicates in a coarse-to-fine manner. The intensively experimental results\non three large-scale VQA datasets show that our proposed approach achieves\nsuperior accuracy comparing with other state-of-the-art methods. Furthermore,\nour reasoning framework also provides an explainable way to understand the\ndecision of the deep neural network when predicting the answer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.14862,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000032451,
      "text":"Audio-visual Representation Learning for Anomaly Events Detection in\n  Crowds\n\n  In recent years, anomaly events detection in crowd scenes attracts many\nresearchers' attention, because of its importance to public safety. Existing\nmethods usually exploit visual information to analyze whether any abnormal\nevents have occurred due to only visual sensors are generally equipped in\npublic places. However, when an abnormal event in crowds occurs, sound\ninformation may be discriminative to assist the crowd analysis system to\ndetermine whether there is an abnormality. Compare with vision information that\nis easily occluded, audio signals have a certain degree of penetration. Thus,\nthis paper attempt to exploit multi-modal learning for modeling the audio and\nvisual signals simultaneously. To be specific, we design a two-branch network\nto model different types of information. The first is a typical 3D CNN model to\nextract temporal appearance features from video clips. The second is an audio\nCNN for encoding Log Mel-Spectrogram of audio signals. Finally, by fusing the\nabove features, a more accurate prediction will be produced. We conduct the\nexperiments on SHADE dataset, a synthetic audio-visual dataset in surveillance\nscenes, and find introducing audio signals effectively improves the performance\nof anomaly events detection and outperforms other state-of-the-art methods.\nFurthermore, we will release the code and the pre-trained models as soon as\npossible.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.05926,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"Weakly-Supervised Semantic Segmentation by Learning Label Uncertainty\n\n  Since the rise of deep learning, many computer vision tasks have seen\nsignificant advancements. However, the downside of deep learning is that it is\nvery data-hungry. Especially for segmentation problems, training a deep neural\nnet requires dense supervision in the form of pixel-perfect image labels, which\nare very costly. In this paper, we present a new loss function to train a\nsegmentation network with only a small subset of pixel-perfect labels, but take\nthe advantage of weakly-annotated training samples in the form of cheap\nbounding-box labels. Unlike recent works which make use of box-to-mask proposal\ngenerators, our loss trains the network to learn a label uncertainty within the\nbounding-box, which can be leveraged to perform online bootstrapping (i.e.\ntransforming the boxes to segmentation masks), while training the network. We\nevaluated our method on binary segmentation tasks, as well as a multi-class\nsegmentation task (CityScapes vehicles and persons). We trained each task on a\ndataset comprised of only 18% pixel-perfect and 82% bounding-box labels, and\ncompared the results to a baseline model trained on a completely pixel-perfect\ndataset. For the binary segmentation tasks, our method achieves an IoU score\nwhich is ~98.33% as good as our baseline model, while for the multi-class task,\nour method is 97.12% as good as our baseline model (77.5 vs. 79.8 mIoU).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.06554,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000025829,
      "text":"Towards Mixed-Precision Quantization of Neural Networks via Constrained\n  Optimization\n\n  Quantization is a widely used technique to compress and accelerate deep\nneural networks. However, conventional quantization methods use the same\nbit-width for all (or most of) the layers, which often suffer significant\naccuracy degradation in the ultra-low precision regime and ignore the fact that\nemergent hardware accelerators begin to support mixed-precision computation.\nConsequently, we present a novel and principled framework to solve the\nmixed-precision quantization problem in this paper. Briefly speaking, we first\nformulate the mixed-precision quantization as a discrete constrained\noptimization problem. Then, to make the optimization tractable, we approximate\nthe objective function with second-order Taylor expansion and propose an\nefficient approach to compute its Hessian matrix. Finally, based on the above\nsimplification, we show that the original problem can be reformulated as a\nMultiple-Choice Knapsack Problem (MCKP) and propose a greedy search algorithm\nto solve it efficiently. Compared with existing mixed-precision quantization\nworks, our method is derived in a principled way and much more computationally\nefficient. Moreover, extensive experiments conducted on the ImageNet dataset\nand various kinds of network architectures also demonstrate its superiority\nover existing uniform and mixed-precision quantization approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.00538,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000006292,
      "text":"From Face to Gait: Weakly-Supervised Learning of Gender Information from\n  Walking Patterns\n\n  Obtaining demographics information from video is valuable for a range of\nreal-world applications. While approaches that leverage facial features for\ngender inference are very successful in restrained environments, they do not\nwork in most real-world scenarios when the subject is not facing the camera,\nhas the face obstructed or the face is not clear due to distance from the\ncamera or poor resolution. We propose a weakly-supervised method for learning\ngender information of people based on their manner of walking. We make use of\nstate-of-the art facial analysis models to automatically annotate front-view\nwalking sequences and generalise to unseen angles by leveraging gait-based\nlabel propagation. Our results show on par or higher performance with facial\nanalysis models with an F1 score of 91% and the ability to successfully\ngeneralise to scenarios in which facial analysis is unfeasible due to subjects\nnot facing the camera or having the face obstructed.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.15028,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000041723,
      "text":"Facial Emotion Recognition: A multi-task approach using deep learning\n\n  Facial Emotion Recognition is an inherently difficult problem, due to vast\ndifferences in facial structures of individuals and ambiguity in the emotion\ndisplayed by a person. Recently, a lot of work is being done in the field of\nFacial Emotion Recognition, and the performance of the CNNs for this task has\nbeen inferior compared to the results achieved by CNNs in other fields like\nObject detection, Facial recognition etc. In this paper, we propose a\nmulti-task learning algorithm, in which a single CNN detects gender, age and\nrace of the subject along with their emotion. We validate this proposed\nmethodology using two datasets containing real-world images. The results show\nthat this approach is significantly better than the current State of the art\nalgorithms for this task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.01382,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"3d sequential image mosaicing for underwater navigation and mapping\n\n  Although fully autonomous mapping methods are becoming more and more common\nand reliable, still the human operator is regularly employed in many 3D\nsurveying missions. In a number of underwater applications, divers or pilots of\nremotely operated vehicles (ROVs) are still considered irreplaceable, and tools\nfor real-time visualization of the mapped scene are essential to support and\nmaximize the navigation and surveying efforts. For underwater exploration,\nimage mosaicing has proved to be a valid and effective approach to visualize\nlarge mapped areas, often employed in conjunction with autonomous underwater\nvehicles (AUVs) and ROVs. In this work, we propose the use of a modified image\nmosaicing algorithm that coupled with image-based real-time navigation and\nmapping algorithms provides two visual navigation aids. The first is a classic\nimage mosaic, where the recorded and processed images are incrementally added,\nnamed 2D sequential image mosaicing (2DSIM). The second one geometrically\ntransform the images so that they are projected as planar point clouds in the\n3D space providing an incremental point cloud mosaicing, named 3D sequential\nimage plane projection (3DSIP). In the paper, the implemented procedure is\ndetailed, and experiments in different underwater scenarios presented and\ndiscussed. Technical considerations about computational efforts, frame rate\ncapabilities and scalability to different and more compact architectures (i.e.\nembedded systems) is also provided.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2110.11867,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000122521,
      "text":"CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking\n  Detection\n\n  In this paper, we introduce a novel road marking benchmark dataset for road\nmarking detection, addressing the limitations in the existing publicly\navailable datasets such as lack of challenging scenarios, prominence given to\nlane markings, unavailability of an evaluation script, lack of annotation\nformats and lower resolutions. Our dataset consists of 2887 total images with\n4706 road marking instances belonging to 11 classes. The images have a high\nresolution of 1920 x 1080 and capture a wide range of traffic, lighting and\nweather conditions. We provide road marking annotations in polygons, bounding\nboxes and pixel-level segmentation masks to facilitate a diverse range of road\nmarking detection algorithms. The evaluation metrics and the evaluation script\nwe provide, will further promote direct comparison of novel approaches for road\nmarking detection with existing methods. Furthermore, we evaluate the\neffectiveness of using both instance segmentation and object detection based\napproaches for the road marking detection task. Speed and accuracy scores for\ntwo instance segmentation models and two object detector models are provided as\na performance baseline for our benchmark dataset. The dataset and the\nevaluation script is publicly available at https:\/\/github.com\/oshadajay\/CeyMo.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09027,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Discriminative Dictionary Learning based on Statistical Methods\n\n  Sparse Representation (SR) of signals or data has a well founded theory with\nrigorous mathematical error bounds and proofs. SR of a signal is given by\nsuperposition of very few columns of a matrix called Dictionary, implicitly\nreducing dimensionality. Training dictionaries such that they represent each\nclass of signals with minimal loss is called Dictionary Learning (DL).\nDictionary learning methods like Method of Optimal Directions (MOD) and K-SVD\nhave been successfully used in reconstruction based applications in image\nprocessing like image \"denoising\", \"inpainting\" and others. Other dictionary\nlearning algorithms such as Discriminative K-SVD and Label Consistent K-SVD are\nsupervised learning methods based on K-SVD. In our experience, one of the\ndrawbacks of current methods is that the classification performance is not\nimpressive on datasets like Telugu OCR datasets, with large number of classes\nand high dimensionality. There is scope for improvement in this direction and\nmany researchers have used statistical methods to design dictionaries for\nclassification. This chapter presents a review of statistical techniques and\ntheir application to learning discriminative dictionaries. The objective of the\nmethods described here is to improve classification using sparse\nrepresentation. In this chapter a hybrid approach is described, where sparse\ncoefficients of input data are generated. We use a simple three layer Multi\nLayer Perceptron with back-propagation training as a classifier with those\nsparse codes as input. The results are quite comparable with other computation\nintensive methods.\n  Keywords: Statistical modeling, Dictionary Learning, Discriminative\nDictionary, Sparse representation, Gaussian prior, Cauchy prior, Entropy,\nHidden Markov model, Hybrid Dictionary Learning\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.1233,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Hidden-Fold Networks: Random Recurrent Residuals Using Sparse Supermasks\n\n  Deep neural networks (DNNs) are so over-parametrized that recent research has\nfound them to already contain a subnetwork with high accuracy at their randomly\ninitialized state. Finding these subnetworks is a viable alternative training\nmethod to weight learning. In parallel, another line of work has hypothesized\nthat deep residual networks (ResNets) are trying to approximate the behaviour\nof shallow recurrent neural networks (RNNs) and has proposed a way for\ncompressing them into recurrent models. This paper proposes blending these\nlines of research into a highly compressed yet accurate model: Hidden-Fold\nNetworks (HFNs). By first folding ResNet into a recurrent structure and then\nsearching for an accurate subnetwork hidden within the randomly initialized\nmodel, a high-performing yet tiny HFN is obtained without ever updating the\nweights. As a result, HFN achieves equivalent performance to ResNet50 on\nCIFAR100 while occupying 38.5x less memory, and similar performance to ResNet34\non ImageNet with a memory size 26.8x smaller. The HFN will become even more\nattractive by minimizing data transfers while staying accurate when it runs on\nhighly-quantized and randomly-weighted DNN inference accelerators. Code\navailable at https:\/\/github.com\/Lopez-Angel\/hidden-fold-networks\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0517,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000268552,
      "text":"Exploiting Robust Unsupervised Video Person Re-identification\n\n  Unsupervised video person re-identification (reID) methods usually depend on\nglobal-level features. And many supervised reID methods employed local-level\nfeatures and achieved significant performance improvements. However, applying\nlocal-level features to unsupervised methods may introduce an unstable\nperformance. To improve the performance stability for unsupervised video reID,\nthis paper introduces a general scheme fusing part models and unsupervised\nlearning. In this scheme, the global-level feature is divided into equal\nlocal-level feature. A local-aware module is employed to explore the poentials\nof local-level feature for unsupervised learning. A global-aware module is\nproposed to overcome the disadvantages of local-level features. Features from\nthese two modules are fused to form a robust feature representation for each\ninput image. This feature representation has the advantages of local-level\nfeature without suffering from its disadvantages. Comprehensive experiments are\nconducted on three benchmarks, including PRID2011, iLIDS-VID, and\nDukeMTMC-VideoReID, and the results demonstrate that the proposed approach\nachieves state-of-the-art performance. Extensive ablation studies demonstrate\nthe effectiveness and robustness of proposed scheme, local-aware module and\nglobal-aware module. The code and generated features are available at\nhttps:\/\/github.com\/deropty\/uPMnet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12289,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000051657,
      "text":"Real-time smart vehicle surveillance system\n\n  Over the last decade, there has been a spike in criminal activity all around\nthe globe. According to the Indian police department, vehicle theft is one of\nthe least solved offenses, and almost 19% of all recorded cases are related to\nmotor vehicle theft. To overcome these adversaries, we propose a real-time\nvehicle surveillance system, which detects and tracks the suspect vehicle using\nthe CCTV video feed. The proposed system extracts various attributes of the\nvehicle such as Make, Model, Color, License plate number, and type of the\nlicense plate. Various image processing and deep learning algorithms are\nemployed to meet the objectives of the proposed system. The extracted features\ncan be used as evidence to report violations of law. Although the system uses\nmore parameters, it is still able to make real time predictions with minimal\nlatency and accuracy loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12698,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000091725,
      "text":"Open-Vocabulary Instance Segmentation via Robust Cross-Modal\n  Pseudo-Labeling\n\n  Open-vocabulary instance segmentation aims at segmenting novel classes\nwithout mask annotations. It is an important step toward reducing laborious\nhuman supervision. Most existing works first pretrain a model on captioned\nimages covering many novel classes and then finetune it on limited base classes\nwith mask annotations. However, the high-level textual information learned from\ncaption pretraining alone cannot effectively encode the details required for\npixel-wise segmentation. To address this, we propose a cross-modal\npseudo-labeling framework, which generates training pseudo masks by aligning\nword semantics in captions with visual features of object masks in images.\nThus, our framework is capable of labeling novel classes in captions via their\nword semantics to self-train a student model. To account for noises in pseudo\nmasks, we design a robust student model that selectively distills mask\nknowledge by estimating the mask noise levels, hence mitigating the adverse\nimpact of noisy pseudo masks. By extensive experiments, we show the\neffectiveness of our framework, where we significantly improve mAP score by\n4.5% on MS-COCO and 5.1% on the large-scale Open Images & Conceptual Captions\ndatasets compared to the state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.13817,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000269545,
      "text":"Video Frame Interpolation Transformer\n\n  Existing methods for video interpolation heavily rely on deep convolution\nneural networks, and thus suffer from their intrinsic limitations, such as\ncontent-agnostic kernel weights and restricted receptive field. To address\nthese issues, we propose a Transformer-based video interpolation framework that\nallows content-aware aggregation weights and considers long-range dependencies\nwith the self-attention operations. To avoid the high computational cost of\nglobal self-attention, we introduce the concept of local attention into video\ninterpolation and extend it to the spatial-temporal domain. Furthermore, we\npropose a space-time separation strategy to save memory usage, which also\nimproves performance. In addition, we develop a multi-scale frame synthesis\nscheme to fully realize the potential of Transformers. Extensive experiments\ndemonstrate the proposed model performs favorably against the state-of-the-art\nmethods both quantitatively and qualitatively on a variety of benchmark\ndatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.11783,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000005232,
      "text":"GenReg: Deep Generative Method for Fast Point Cloud Registration\n\n  Accurate and efficient point cloud registration is a challenge because the\nnoise and a large number of points impact the correspondence search. This\nchallenge is still a remaining research problem since most of the existing\nmethods rely on correspondence search. To solve this challenge, we propose a\nnew data-driven registration algorithm by investigating deep generative neural\nnetworks to point cloud registration. Given two point clouds, the motivation is\nto generate the aligned point clouds directly, which is very useful in many\napplications like 3D matching and search. We design an end-to-end generative\nneural network for aligned point clouds generation to achieve this motivation,\ncontaining three novel components. Firstly, a point multi-perception layer\n(MLP) mixer (PointMixer) network is proposed to efficiently maintain both the\nglobal and local structure information at multiple levels from the self point\nclouds. Secondly, a feature interaction module is proposed to fuse information\nfrom cross point clouds. Thirdly, a parallel and differential sample consensus\nmethod is proposed to calculate the transformation matrix of the input point\nclouds based on the generated registration results. The proposed generative\nneural network is trained in a GAN framework by maintaining the data\ndistribution and structure similarity. The experiments on both ModelNet40 and\n7Scene datasets demonstrate that the proposed algorithm achieves\nstate-of-the-art accuracy and efficiency. Notably, our method reduces $2\\times$\nin registration error (CD) and $12\\times$ running time compared to the\nstate-of-the-art correspondence-based algorithm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12792,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Improving the Perceptual Quality of 2D Animation Interpolation\n\n  Traditional 2D animation is labor-intensive, often requiring animators to\nmanually draw twelve illustrations per second of movement. While automatic\nframe interpolation may ease this burden, 2D animation poses additional\ndifficulties compared to photorealistic video. In this work, we address\nchallenges unexplored in previous animation interpolation systems, with a focus\non improving perceptual quality. Firstly, we propose SoftsplatLite (SSL), a\nforward-warping interpolation architecture with fewer trainable parameters and\nbetter perceptual performance. Secondly, we design a Distance Transform Module\n(DTM) that leverages line proximity cues to correct aberrations in difficult\nsolid-color regions. Thirdly, we define a Restricted Relative Linear\nDiscrepancy metric (RRLD) to automate the previously manual training data\ncollection process. Lastly, we explore evaluation of 2D animation generation\nthrough a user study, and establish that the LPIPS perceptual metric and\nchamfer line distance (CD) are more appropriate measures of quality than PSNR\nand SSIM used in prior art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.06925,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Action2video: Generating Videos of Human 3D Actions\n\n  We aim to tackle the interesting yet challenging problem of generating videos\nof diverse and natural human motions from prescribed action categories. The key\nissue lies in the ability to synthesize multiple distinct motion sequences that\nare realistic in their visual appearances. It is achieved in this paper by a\ntwo-step process that maintains internal 3D pose and shape representations,\naction2motion and motion2video. Action2motion stochastically generates\nplausible 3D pose sequences of a prescribed action category, which are\nprocessed and rendered by motion2video to form 2D videos. Specifically, the Lie\nalgebraic theory is engaged in representing natural human motions following the\nphysical law of human kinematics; a temporal variational auto-encoder (VAE) is\ndeveloped that encourages diversity of output motions. Moreover, given an\nadditional input image of a clothed human character, an entire pipeline is\nproposed to extract his\/her 3D detailed shape, and to render in videos the\nplausible motions from different views. This is realized by improving existing\nmethods to extract 3D human shapes and textures from single 2D images, rigging,\nanimating, and rendering to form 2D videos of human motions. It also\nnecessitates the curation and reannotation of 3D human motion datasets for\ntraining purpose. Thorough empirical experiments including ablation study,\nqualitative and quantitative evaluations manifest the applicability of our\napproach, and demonstrate its competitiveness in addressing related tasks,\nwhere components of our approach are compared favorably to the\nstate-of-the-arts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.15606,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Robust Partial-to-Partial Point Cloud Registration in a Full Range\n\n  Point cloud registration for 3D objects is a challenging task due to sparse\nand noisy measurements, incomplete observations and large transformations. In\nthis work, we propose \\textbf{G}raph \\textbf{M}atching \\textbf{C}onsensus\n\\textbf{Net}work (\\textbf{GMCNet}), which estimates pose-invariant\ncorrespondences for full-range Partial-to-Partial point cloud Registration\n(PPR) in the object-level registration scenario. To encode robust point\ndescriptors, \\textbf{1)} we first comprehensively investigate\ntransformation-robustness and noise-resilience of various geometric features.\n\\textbf{2)} Then, we employ a novel {T}ransformation-robust {P}oint\n{T}ransformer (\\textbf{TPT}) module to adaptively aggregate local features\nregarding the structural relations, which takes advantage from both handcrafted\nrotation-invariant ({\\textit{RI}}) features and noise-resilient spatial\ncoordinates. \\textbf{3)} Based on a synergy of hierarchical graph networks and\ngraphical modeling, we propose the {H}ierarchical {G}raphical {M}odeling\n(\\textbf{HGM}) architecture to encode robust descriptors consisting of i) a\nunary term learned from {\\textit{RI}} features; and ii) multiple smoothness\nterms encoded from neighboring point relations at different scales through our\nTPT modules. Moreover, we construct a challenging PPR dataset (\\textbf{MVP-RG})\nbased on the recent MVP dataset that features high-quality scans. Extensive\nexperiments show that GMCNet outperforms previous state-of-the-art methods for\nPPR. Notably, GMCNet encodes point descriptors for each point cloud\nindividually without using cross-contextual information, or ground truth\ncorrespondences for training. Our code and datasets are available at:\nhttps:\/\/github.com\/paul007pl\/GMCNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.04237,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Template NeRF: Towards Modeling Dense Shape Correspondences from\n  Category-Specific Object Images\n\n  We present neural radiance fields (NeRF) with templates, dubbed\nTemplate-NeRF, for modeling appearance and geometry and generating dense shape\ncorrespondences simultaneously among objects of the same category from only\nmulti-view posed images, without the need of either 3D supervision or\nground-truth correspondence knowledge. The learned dense correspondences can be\nreadily used for various image-based tasks such as keypoint detection, part\nsegmentation, and texture transfer that previously require specific model\ndesigns. Our method can also accommodate annotation transfer in a one or\nfew-shot manner, given only one or a few instances of the category. Using\nperiodic activation and feature-wise linear modulation (FiLM) conditioning, we\nintroduce deep implicit templates on 3D data into the 3D-aware image synthesis\npipeline NeRF. By representing object instances within the same category as\nshape and appearance variation of a shared NeRF template, our proposed method\ncan achieve dense shape correspondences reasoning on images for a wide range of\nobject classes. We demonstrate the results and applications on both synthetic\nand real-world data with competitive results compared with other methods based\non 3D information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.15581,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Automated Damage Inspection of Power Transmission Towers from UAV Images\n\n  Infrastructure inspection is a very costly task, requiring technicians to\naccess remote or hard-to-reach places. This is the case for power transmission\ntowers, which are sparsely located and require trained workers to climb them to\nsearch for damages. Recently, the use of drones or helicopters for remote\nrecording is increasing in the industry, sparing the technicians this perilous\ntask. This, however, leaves the problem of analyzing big amounts of images,\nwhich has great potential for automation. This is a challenging task for\nseveral reasons. First, the lack of freely available training data and the\ndifficulty to collect it complicate this problem. Additionally, the boundaries\nof what constitutes a damage are fuzzy, introducing a degree of subjectivity in\nthe labelling of the data. The unbalanced class distribution in the images also\nplays a role in increasing the difficulty of the task. This paper tackles the\nproblem of structural damage detection in transmission towers, addressing these\nissues. Our main contributions are the development of a system for damage\ndetection on remotely acquired drone images, applying techniques to overcome\nthe issue of data scarcity and ambiguity, as well as the evaluation of the\nviability of such an approach to solve this particular problem.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.05688,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Robust reconstructions by multi-scale\/irregular tangential covering\n\n  In this paper, we propose an original manner to employ a tangential cover\nalgorithm - minDSS - in order to geometrically reconstruct noisy digital\ncontours. To do so, we exploit the representation of graphical objects by\nmaximal primitives we have introduced in previous works. By calculating\nmulti-scale and irregular isothetic representations of the contour, we obtained\n1-D (one-dimensional) intervals, and achieved afterwards a decomposition into\nmaximal line segments or circular arcs. By adapting minDSS to this sparse and\nirregular data of 1-D intervals supporting the maximal primitives, we are now\nable to reconstruct the input noisy objects into cyclic contours made of lines\nor arcs with a minimal number of primitives. In this work, we explain our novel\ncomplete pipeline, and present its experimental evaluation by considering both\nsynthetic and real image data. We also show that this is a robust approach,\nwith respect to selected references from state-of-the-art, and by considering a\nmulti-scale noise evaluation process.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08567,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000096361,
      "text":"Joint Learning of Visual-Audio Saliency Prediction and Sound Source\n  Localization on Multi-face Videos\n\n  Visual and audio events simultaneously occur and both attract attention.\nHowever, most existing saliency prediction works ignore the influence of audio\nand only consider vision modality. In this paper, we propose a multitask\nlearning method for visual-audio saliency prediction and sound source\nlocalization on multi-face video by leveraging visual, audio and face\ninformation. Specifically, we first introduce a large-scale database of\nmulti-face video in visual-audio condition (MVVA), containing eye-tracking data\nand sound source annotations. Using this database, we find that sound\ninfluences human attention, and conversly attention offers a cue to determine\nsound source on multi-face video. Guided by these findings, a visual-audio\nmulti-task network (VAM-Net) is introduced to predict saliency and locate sound\nsource. VAM-Net consists of three branches corresponding to visual, audio and\nface modalities. Visual branch has a two-stream architecture to capture spatial\nand temporal information. Face and audio branches encode audio signals and\nfaces, respectively. Finally, a spatio-temporal multi-modal graph (STMG) is\nconstructed to model the interaction among multiple faces. With joint\noptimization of these branches, the intrinsic correlation of the tasks of\nsaliency prediction and sound source localization is utilized and their\nperformance is boosted by each other. Experiments show that the proposed method\noutperforms 12 state-of-the-art saliency prediction methods, and achieves\ncompetitive results in sound source localization.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.0423,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000045697,
      "text":"A Study of the Human Perception of Synthetic Faces\n\n  Advances in face synthesis have raised alarms about the deceptive use of\nsynthetic faces. Can synthetic identities be effectively used to fool human\nobservers? In this paper, we introduce a study of the human perception of\nsynthetic faces generated using different strategies including a\nstate-of-the-art deep learning-based GAN model. This is the first rigorous\nstudy of the effectiveness of synthetic face generation techniques grounded in\nexperimental techniques from psychology. We answer important questions such as\nhow often do GAN-based and more traditional image processing-based techniques\nconfuse human observers, and are there subtle cues within a synthetic face\nimage that cause humans to perceive it as a fake without having to search for\nobvious clues? To answer these questions, we conducted a series of large-scale\ncrowdsourced behavioral experiments with different sources of face imagery.\nResults show that humans are unable to distinguish synthetic faces from real\nfaces under several different circumstances. This finding has serious\nimplications for many different applications where face images are presented to\nhuman users.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.1465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0001211299,
      "text":"Buildings Classification using Very High Resolution Satellite Imagery\n\n  Buildings classification using satellite images is becoming more important\nfor several applications such as damage assessment, resource allocation, and\npopulation estimation. We focus, in this work, on buildings damage assessment\n(BDA) and buildings type classification (BTC) of residential and\nnon-residential buildings. We propose to rely solely on RGB satellite images\nand follow a 2-stage deep learning-based approach, where first, buildings'\nfootprints are extracted using a semantic segmentation model, followed by\nclassification of the cropped images. Due to the lack of an appropriate dataset\nfor the residential\/non-residential building classification, we introduce a new\ndataset of high-resolution satellite images. We conduct extensive experiments\nto select the best hyper-parameters, model architecture, and training paradigm,\nand we propose a new transfer learning-based approach that outperforms\nclassical methods. Finally, we validate the proposed approach on two\napplications showing excellent accuracy and F1-score metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.09692,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised\n  Monocular Depth Estimation\n\n  We propose SUB-Depth, a universal multi-task training framework for\nself-supervised monocular depth estimation (SDE). Depth models trained with\nSUB-Depth outperform the same models trained in a standard single-task SDE\nframework. By introducing an additional self-distillation task into a standard\nSDE training framework, SUB-Depth trains a depth network, not only to predict\nthe depth map for an image reconstruction task, but also to distill knowledge\nfrom a trained teacher network with unlabelled data. To take advantage of this\nmulti-task setting, we propose homoscedastic uncertainty formulations for each\ntask to penalize areas likely to be affected by teacher network noise, or\nviolate SDE assumptions. We present extensive evaluations on KITTI to\ndemonstrate the improvements achieved by training a range of existing networks\nusing the proposed framework, and we achieve state-of-the-art performance on\nthis task. Additionally, SUB-Depth enables models to estimate uncertainty on\ndepth output.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.12912,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"A War Beyond Deepfake: Benchmarking Facial Counterfeits and\n  Countermeasures\n\n  In recent years, visual forgery has reached a level of sophistication that\nhumans cannot identify fraud, which poses a significant threat to information\nsecurity. A wide range of malicious applications have emerged, such as fake\nnews, defamation or blackmailing of celebrities, impersonation of politicians\nin political warfare, and the spreading of rumours to attract views. As a\nresult, a rich body of visual forensic techniques has been proposed in an\nattempt to stop this dangerous trend. In this paper, we present a benchmark\nthat provides in-depth insights into visual forgery and visual forensics, using\na comprehensive and empirical approach. More specifically, we develop an\nindependent framework that integrates state-of-the-arts counterfeit generators\nand detectors, and measure the performance of these techniques using various\ncriteria. We also perform an exhaustive analysis of the benchmarking results,\nto determine the characteristics of the methods that serve as a comparative\nreference in this never-ending war between measures and countermeasures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.07547,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000019868,
      "text":"Searching for TrioNet: Combining Convolution with Local and Global\n  Self-Attention\n\n  Recently, self-attention operators have shown superior performance as a\nstand-alone building block for vision models. However, existing self-attention\nmodels are often hand-designed, modified from CNNs, and obtained by stacking\none operator only. A wider range of architecture space which combines different\nself-attention operators and convolution is rarely explored. In this paper, we\nexplore this novel architecture space with weight-sharing Neural Architecture\nSearch (NAS) algorithms. The result architecture is named TrioNet for combining\nconvolution, local self-attention, and global (axial) self-attention operators.\nIn order to effectively search in this huge architecture space, we propose\nHierarchical Sampling for better training of the supernet. In addition, we\npropose a novel weight-sharing strategy, Multi-head Sharing, specifically for\nmulti-head self-attention operators. Our searched TrioNet that combines\nself-attention and convolution outperforms all stand-alone models with fewer\nFLOPs on ImageNet classification where self-attention performs better than\nconvolution. Furthermore, on various small datasets, we observe inferior\nperformance for self-attention models, but our TrioNet is still able to match\nthe best operator, convolution in this case. Our code is available at\nhttps:\/\/github.com\/phj128\/TrioNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2111.08774,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"Finding the Right Moment: Human-Assisted Trailer Creation via Task\n  Composition\n\n  Movie trailers perform multiple functions: they introduce viewers to the\nstory, convey the mood and artistic style of the film, and encourage audiences\nto see the movie. These diverse functions make trailer creation a challenging\nendeavor. In this work, we focus on finding trailer moments in a movie, i.e.,\nshots that could be potentially included in a trailer. We decompose this task\ninto two subtasks: narrative structure identification and sentiment prediction.\nWe model movies as graphs, where nodes are shots and edges denote semantic\nrelations between them. We learn these relations using joint contrastive\ntraining which distills rich textual information (e.g., characters, actions,\nsituations) from screenplays. An unsupervised algorithm then traverses the\ngraph and selects trailer moments from the movie that human judges prefer to\nones selected by competitive supervised approaches. A main advantage of our\nalgorithm is that it uses interpretable criteria, which allows us to deploy it\nin an interactive tool for trailer creation with a human in the loop. Our tool\nallows users to select trailer shots in under 30 minutes that are superior to\nfully automatic methods and comparable to (exclusive) manual selection by\nexperts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06343,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000072519,
      "text":"Change Detection Meets Visual Question Answering\n\n  The Earth's surface is continually changing, and identifying changes plays an\nimportant role in urban planning and sustainability. Although change detection\ntechniques have been successfully developed for many years, these techniques\nare still limited to experts and facilitators in related fields. In order to\nprovide every user with flexible access to change information and help them\nbetter understand land-cover changes, we introduce a novel task: change\ndetection-based visual question answering (CDVQA) on multi-temporal aerial\nimages. In particular, multi-temporal images can be queried to obtain high\nlevel change-based information according to content changes between two input\nimages. We first build a CDVQA dataset including multi-temporal\nimage-question-answer triplets using an automatic question-answer generation\nmethod. Then, a baseline CDVQA framework is devised in this work, and it\ncontains four parts: multi-temporal feature encoding, multi-temporal fusion,\nmulti-modal fusion, and answer prediction. In addition, we also introduce a\nchange enhancing module to multi-temporal feature encoding, aiming at\nincorporating more change-related information. Finally, effects of different\nbackbones and multi-temporal fusion strategies are studied on the performance\nof CDVQA task. The experimental results provide useful insights for developing\nbetter CDVQA models, which are important for future research on this task.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.03649,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000109606,
      "text":"Regularity Learning via Explicit Distribution Modeling for Skeletal\n  Video Anomaly Detection\n\n  Anomaly detection in surveillance videos is challenging and important for\nensuring public security. Different from pixel-based anomaly detection methods,\npose-based methods utilize highly-structured skeleton data, which decreases the\ncomputational burden and also avoids the negative impact of background noise.\nHowever, unlike pixel-based methods, which could directly exploit explicit\nmotion features such as optical flow, pose-based methods suffer from the lack\nof alternative dynamic representation. In this paper, a novel Motion Embedder\n(ME) is proposed to provide a pose motion representation from the probability\nperspective. Furthermore, a novel task-specific Spatial-Temporal Transformer\n(STT) is deployed for self-supervised pose sequence reconstruction. These two\nmodules are then integrated into a unified framework for pose regularity\nlearning, which is referred to as Motion Prior Regularity Learner (MoPRL).\nMoPRL achieves the state-of-the-art performance by an average improvement of\n4.7% AUC on several challenging datasets. Extensive experiments validate the\nversatility of each proposed module.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06103,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000047021,
      "text":"Improving Vision Transformers for Incremental Learning\n\n  This paper proposes a working recipe of using Vision Transformer (ViT) in\nclass incremental learning. Although this recipe only combines existing\ntechniques, developing the combination is not trivial. Firstly, naive\napplication of ViT to replace convolutional neural networks (CNNs) in\nincremental learning results in serious performance degradation. Secondly, we\nnail down three issues of naively using ViT: (a) ViT has very slow convergence\nwhen the number of classes is small, (b) more bias towards new classes is\nobserved in ViT than CNN-based architectures, and (c) the conventional learning\nrate of ViT is too low to learn a good classifier layer. Finally, our solution,\nnamed ViTIL (ViT for Incremental Learning) achieves new state-of-the-art on\nboth CIFAR and ImageNet datasets for all three class incremental learning\nsetups by a clear margin. We believe this advances the knowledge of transformer\nin the incremental learning community. Code will be publicly released.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.03328,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"Learning Connectivity with Graph Convolutional Networks for\n  Skeleton-based Action Recognition\n\n  Learning graph convolutional networks (GCNs) is an emerging field which aims\nat generalizing convolutional operations to arbitrary non-regular domains. In\nparticular, GCNs operating on spatial domains show superior performances\ncompared to spectral ones, however their success is highly dependent on how the\ntopology of input graphs is defined. In this paper, we introduce a novel\nframework for graph convolutional networks that learns the topological\nproperties of graphs. The design principle of our method is based on the\noptimization of a constrained objective function which learns not only the\nusual convolutional parameters in GCNs but also a transformation basis that\nconveys the most relevant topological relationships in these graphs.\nExperiments conducted on the challenging task of skeleton-based action\nrecognition shows the superiority of the proposed method compared to\nhandcrafted graph design as well as the related work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.03485,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000119209,
      "text":"VizExtract: Automatic Relation Extraction from Data Visualizations\n\n  Visual graphics, such as plots, charts, and figures, are widely used to\ncommunicate statistical conclusions. Extracting information directly from such\nvisualizations is a key sub-problem for effective search through scientific\ncorpora, fact-checking, and data extraction. This paper presents a framework\nfor automatically extracting compared variables from statistical charts. Due to\nthe diversity and variation of charting styles, libraries, and tools, we\nleverage a computer vision based framework to automatically identify and\nlocalize visualization facets in line graphs, scatter plots, or bar graphs and\ncan include multiple series per graph. The framework is trained on a large\nsynthetically generated corpus of matplotlib charts and we evaluate the trained\nmodel on other chart datasets. In controlled experiments, our framework is able\nto classify, with 87.5% accuracy, the correlation between variables for graphs\nwith 1-3 series per graph, varying colors, and solid line styles. When deployed\non real-world graphs scraped from the internet, it achieves 72.8% accuracy\n(81.2% accuracy when excluding \"hard\" graphs). When deployed on the FigureQA\ndataset, it achieves 84.7% accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.04665,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Style Mixing and Patchwise Prototypical Matching for One-Shot\n  Unsupervised Domain Adaptive Semantic Segmentation\n\n  In this paper, we tackle the problem of one-shot unsupervised domain\nadaptation (OSUDA) for semantic segmentation where the segmentors only see one\nunlabeled target image during training. In this case, traditional unsupervised\ndomain adaptation models usually fail since they cannot adapt to the target\ndomain with over-fitting to one (or few) target samples. To address this\nproblem, existing OSUDA methods usually integrate a style-transfer module to\nperform domain randomization based on the unlabeled target sample, with which\nmultiple domains around the target sample can be explored during training.\nHowever, such a style-transfer module relies on an additional set of images as\nstyle reference for pre-training and also increases the memory demand for\ndomain adaptation. Here we propose a new OSUDA method that can effectively\nrelieve such computational burden. Specifically, we integrate several\nstyle-mixing layers into the segmentor which play the role of style-transfer\nmodule to stylize the source images without introducing any learned parameters.\nMoreover, we propose a patchwise prototypical matching (PPM) method to weighted\nconsider the importance of source pixels during the supervised training to\nrelieve the negative adaptation. Experimental results show that our method\nachieves new state-of-the-art performance on two commonly used benchmarks for\ndomain adaptive semantic segmentation under the one-shot setting and is more\nefficient than all comparison approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01176,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Overcoming the Domain Gap in Neural Action Representations\n\n  Relating animal behaviors to brain activity is a fundamental goal in\nneuroscience, with practical applications in building robust brain-machine\ninterfaces. However, the domain gap between individuals is a major issue that\nprevents the training of general models that work on unlabeled subjects.\n  Since 3D pose data can now be reliably extracted from multi-view video\nsequences without manual intervention, we propose to use it to guide the\nencoding of neural action representations together with a set of neural and\nbehavioral augmentations exploiting the properties of microscopy imaging. To\nreduce the domain gap, during training, we swap neural and behavioral data\nacross animals that seem to be performing similar actions.\n  To demonstrate this, we test our methods on three very different multimodal\ndatasets; one that features flies and their neural activity, one that contains\nhuman neural Electrocorticography (ECoG) data, and lastly the RGB video data of\nhuman activities from different viewpoints.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.09428,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Dynamics-aware Adversarial Attack of 3D Sparse Convolution Network\n\n  In this paper, we investigate the dynamics-aware adversarial attack problem\nin deep neural networks. Most existing adversarial attack algorithms are\ndesigned under a basic assumption -- the network architecture is fixed\nthroughout the attack process. However, this assumption does not hold for many\nrecently proposed networks, e.g. 3D sparse convolution network, which contains\ninput-dependent execution to improve computational efficiency. It results in a\nserious issue of lagged gradient, making the learned attack at the current step\nineffective due to the architecture changes afterward. To address this issue,\nwe propose a Leaded Gradient Method (LGM) and show the significant effects of\nthe lagged gradient. More specifically, we re-formulate the gradients to be\naware of the potential dynamic changes of network architectures, so that the\nlearned attack better \"leads\" the next step than the dynamics-unaware methods\nwhen network architecture changes dynamically. Extensive experiments on various\ndatasets show that our LGM achieves impressive performance on semantic\nsegmentation and classification. Compared with the dynamic-unaware methods, LGM\nachieves about 20% lower mIoU averagely on the ScanNet and S3DIS datasets. LGM\nalso outperforms the recent point cloud attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.00343,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Camera Motion Agnostic 3D Human Pose Estimation\n\n  Although the performance of 3D human pose and shape estimation methods has\nimproved significantly in recent years, existing approaches typically generate\n3D poses defined in camera or human-centered coordinate system. This makes it\ndifficult to estimate a person's pure pose and motion in world coordinate\nsystem for a video captured using a moving camera. To address this issue, this\npaper presents a camera motion agnostic approach for predicting 3D human pose\nand mesh defined in the world coordinate system. The core idea of the proposed\napproach is to estimate the difference between two adjacent global poses (i.e.,\nglobal motion) that is invariant to selecting the coordinate system, instead of\nthe global pose coupled to the camera motion. To this end, we propose a network\nbased on bidirectional gated recurrent units (GRUs) that predicts the global\nmotion sequence from the local pose sequence consisting of relative rotations\nof joints called global motion regressor (GMR). We use 3DPW and synthetic\ndatasets, which are constructed in a moving-camera environment, for evaluation.\nWe conduct extensive experiments and prove the effectiveness of the proposed\nmethod empirically. Code and datasets are available at\nhttps:\/\/github.com\/seonghyunkim1212\/GMR\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.1275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000228153,
      "text":"SLIP: Self-supervision meets Language-Image Pre-training\n\n  Recent work has shown that self-supervised pre-training leads to improvements\nover supervised learning on challenging visual recognition tasks. CLIP, an\nexciting new approach to learning with language supervision, demonstrates\npromising performance on a wide variety of benchmarks. In this work, we explore\nwhether self-supervised learning can aid in the use of language supervision for\nvisual representation learning. We introduce SLIP, a multi-task learning\nframework for combining self-supervised learning and CLIP pre-training. After\npre-training with Vision Transformers, we thoroughly evaluate representation\nquality and compare performance to both CLIP and self-supervised learning under\nthree distinct settings: zero-shot transfer, linear classification, and\nend-to-end finetuning. Across ImageNet and a battery of additional datasets, we\nfind that SLIP improves accuracy by a large margin. We validate our results\nfurther with experiments on different model sizes, training schedules, and\npre-training datasets. Our findings show that SLIP enjoys the best of both\nworlds: better performance than self-supervision (+8.1% linear accuracy) and\nlanguage supervision (+5.2% zero-shot accuracy).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.03205,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000088082,
      "text":"Simultaneously Predicting Multiple Plant Traits from Multiple Sensors\n  via Deformable CNN Regression\n\n  Trait measurement is critical for the plant breeding and agricultural\nproduction pipeline. Typically, a suite of plant traits is measured using\nlaborious manual measurements and then used to train and\/or validate higher\nthroughput trait estimation techniques. Here, we introduce a relatively simple\nconvolutional neural network (CNN) model that accepts multiple sensor inputs\nand predicts multiple continuous trait outputs - i.e. a multi-input,\nmulti-output CNN (MIMO-CNN). Further, we introduce deformable convolutional\nlayers into this network architecture (MIMO-DCNN) to enable the model to\nadaptively adjust its receptive field, model complex variable geometric\ntransformations in the data, and fine-tune the continuous trait outputs. We\nexamine how the MIMO-CNN and MIMO-DCNN models perform on a multi-input (i.e.\nRGB and depth images), multi-trait output lettuce dataset from the 2021\nAutonomous Greenhouse Challenge. Ablation studies were conducted to examine the\neffect of using single versus multiple inputs, and single versus multiple\noutputs. The MIMO-DCNN model resulted in a normalized mean squared error (NMSE)\nof 0.068 - a substantial improvement over the top 2021 leaderboard score of\n0.081. Open-source code is provided.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.01335,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000324514,
      "text":"Semantic-Sparse Colorization Network for Deep Exemplar-based\n  Colorization\n\n  Exemplar-based colorization approaches rely on reference image to provide\nplausible colors for target gray-scale image. The key and difficulty of\nexemplar-based colorization is to establish an accurate correspondence between\nthese two images. Previous approaches have attempted to construct such a\ncorrespondence but are faced with two obstacles. First, using luminance\nchannels for the calculation of correspondence is inaccurate. Second, the dense\ncorrespondence they built introduces wrong matching results and increases the\ncomputation burden. To address these two problems, we propose Semantic-Sparse\nColorization Network (SSCN) to transfer both the global image style and\ndetailed semantic-related colors to the gray-scale image in a coarse-to-fine\nmanner. Our network can perfectly balance the global and local colors while\nalleviating the ambiguous matching problem. Experiments show that our method\noutperforms existing methods in both quantitative and qualitative evaluation\nand achieves state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.06398,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000060929,
      "text":"Shaping Visual Representations with Attributes for Few-Shot Recognition\n\n  Few-shot recognition aims to recognize novel categories under low-data\nregimes. Some recent few-shot recognition methods introduce auxiliary semantic\nmodality, i.e., category attribute information, into representation learning,\nwhich enhances the feature discrimination and improves the recognition\nperformance. Most of these existing methods only consider the attribute\ninformation of support set while ignoring the query set, resulting in a\npotential loss of performance. In this letter, we propose a novel\nattribute-shaped learning (ASL) framework, which can jointly perform query\nattributes generation and discriminative visual representation learning for\nfew-shot recognition. Specifically, a visual-attribute predictor (VAP) is\nconstructed to predict the attributes of queries. By leveraging the attributes\ninformation, an attribute-visual attention module (AVAM) is designed, which can\nadaptively utilize attributes and visual representations to learn more\ndiscriminative features. Under the guidance of attribute modality, our method\ncan learn enhanced semantic-aware representation for classification.\nExperiments demonstrate that our method can achieve competitive results on CUB\nand SUN benchmarks. Our source code is available at:\n\\url{https:\/\/github.com\/chenhaoxing\/ASL}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.10324,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Product Re-identification System in Fully Automated Defect Detection\n\n  In this work, we introduce a method and present an improved neural work to\nperform product re-identification, which is an essential core function of a\nfully automated product defect detection system. Our method is based on feature\ndistance. It is the combination of feature extraction neural networks, such as\nVGG16, AlexNet, with an image search engine - Vearch. The dataset that we used\nto develop product re-identification systems is a water-bottle dataset that\nconsists of 400 images of 18 types of water bottles. This is a small dataset,\nwhich was the biggest challenge of our work. However, the combination of neural\nnetworks with Vearch shows potential to tackle the product re-identification\nproblems. Especially, our new neural network - AlphaAlexNet that a neural\nnetwork was improved based on AlexNet could improve the production\nidentification accuracy by four percent. This indicates that an ideal\nproduction identification accuracy could be achieved when efficient feature\nextraction methods could be introduced and redesigned for image feature\nextractions of nearly identical products. In order to solve the biggest\nchallenges caused by the small size of the dataset and the difficult nature of\nidentifying productions that have little differences from each other. In our\nfuture work, we propose a new roadmap to tackle nearly-identical production\nidentifications: to introduce or develop new algorithms that need very few\nimages to train themselves.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.04771,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000015232,
      "text":"Progressive Attention on Multi-Level Dense Difference Maps for Generic\n  Event Boundary Detection\n\n  Generic event boundary detection is an important yet challenging task in\nvideo understanding, which aims at detecting the moments where humans naturally\nperceive event boundaries. The main challenge of this task is perceiving\nvarious temporal variations of diverse event boundaries. To this end, this\npaper presents an effective and end-to-end learnable framework (DDM-Net). To\ntackle the diversity and complicated semantics of event boundaries, we make\nthree notable improvements. First, we construct a feature bank to store\nmulti-level features of space and time, prepared for difference calculation at\nmultiple scales. Second, to alleviate inadequate temporal modeling of previous\nmethods, we present dense difference maps (DDM) to comprehensively characterize\nthe motion pattern. Finally, we exploit progressive attention on multi-level\nDDM to jointly aggregate appearance and motion clues. As a result, DDM-Net\nrespectively achieves a significant boost of 14% and 8% on Kinetics-GEBD and\nTAPOS benchmark, and outperforms the top-1 winner solution of LOVEU\nChallenge@CVPR 2021 without bells and whistles. The state-of-the-art result\ndemonstrates the effectiveness of richer motion representation and more\nsophisticated aggregation, in handling the diversity of generic event boundary\ndetection. The code is made available at \\url{https:\/\/github.com\/MCG-NJU\/DDM}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.11641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000000993,
      "text":"JoJoGAN: One Shot Face Stylization\n\n  A style mapper applies some fixed style to its input images (so, for example,\ntaking faces to cartoons). This paper describes a simple procedure -- JoJoGAN\n-- to learn a style mapper from a single example of the style. JoJoGAN uses a\nGAN inversion procedure and StyleGAN's style-mixing property to produce a\nsubstantial paired dataset from a single example style. The paired dataset is\nthen used to fine-tune a StyleGAN. An image can then be style mapped by\nGAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one\nreference and as little as 30 seconds of training time. JoJoGAN can use extreme\nstyle references (say, animal faces) successfully. Furthermore, one can control\nwhat aspects of the style are used and how much of the style is applied.\nQualitative and quantitative evaluation show that JoJoGAN produces high quality\nhigh resolution images that vastly outperform the current state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.13539,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000004636,
      "text":"Few-Shot Classification in Unseen Domains by Episodic Meta-Learning\n  Across Visual Domains\n\n  Few-shot classification aims to carry out classification given only few\nlabeled examples for the categories of interest. Though several approaches have\nbeen proposed, most existing few-shot learning (FSL) models assume that base\nand novel classes are drawn from the same data domain. When it comes to\nrecognizing novel-class data in an unseen domain, this becomes an even more\nchallenging task of domain generalized few-shot classification. In this paper,\nwe present a unique learning framework for domain-generalized few-shot\nclassification, where base classes are from homogeneous multiple source\ndomains, while novel classes to be recognized are from target domains which are\nnot seen during training. By advancing meta-learning strategies, our learning\nframework exploits data across multiple source domains to capture\ndomain-invariant features, with FSL ability introduced by metric-learning based\nmechanisms across support and query data. We conduct extensive experiments to\nverify the effectiveness of our proposed learning framework and show learning\nfrom small yet homogeneous source data is able to perform preferably against\nlearning from large-scale one. Moreover, we provide insights into choices of\nbackbone models for domain-generalized few-shot classification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.00995,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000336766,
      "text":"SwinTrack: A Simple and Strong Baseline for Transformer Tracking\n\n  Recently Transformer has been largely explored in tracking and shown\nstate-of-the-art (SOTA) performance. However, existing efforts mainly focus on\nfusing and enhancing features generated by convolutional neural networks\n(CNNs). The potential of Transformer in representation learning remains\nunder-explored. In this paper, we aim to further unleash the power of\nTransformer by proposing a simple yet efficient fully-attentional tracker,\ndubbed SwinTrack, within classic Siamese framework. In particular, both\nrepresentation learning and feature fusion in SwinTrack leverage the\nTransformer architecture, enabling better feature interactions for tracking\nthan pure CNN or hybrid CNN-Transformer frameworks. Besides, to further enhance\nrobustness, we present a novel motion token that embeds historical target\ntrajectory to improve tracking by providing temporal context. Our motion token\nis lightweight with negligible computation but brings clear gains. In our\nthorough experiments, SwinTrack exceeds existing approaches on multiple\nbenchmarks. Particularly, on the challenging LaSOT, SwinTrack sets a new record\nwith 0.713 SUC score. It also achieves SOTA results on other benchmarks. We\nexpect SwinTrack to serve as a solid baseline for Transformer tracking and\nfacilitate future research. Our codes and results are released at\nhttps:\/\/github.com\/LitingLin\/SwinTrack.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.02582,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic\n  Segmentation\n\n  The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging\nvision problem that aims to predict panoptic segmentation and depth in a video\nsimultaneously. The previous work solves this task by extending the existing\npanoptic segmentation method with an extra dense depth prediction and instance\ntracking head. However, the relationship between the depth and panoptic\nsegmentation is not well explored -- simply combining existing methods leads to\ncompetition and needs carefully weight balancing. In this paper, we present\nPolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS\ntask and lead to more robust results. Our principal insight is that the depth\ncan be harmonized with the panoptic segmentation with our proposed new paradigm\nof predicting instance level depth maps with object queries. Then the\nrelationship between the two tasks via query-based learning is explored. From\nthe experiments, we demonstrate the benefits of our design from both depth\nestimation and panoptic segmentation aspects. Since each thing query also\nencodes the instance-wise information, it is natural to perform tracking\ndirectly with appearance learning. Our method achieves state-of-the-art results\non two DVPS datasets (Semantic KITTI, Cityscapes), and ranks 1st on the\nICCV-2021 BMTT Challenge video + depth track. Code is available at\nhttps:\/\/github.com\/HarborYuan\/PolyphonicFormer .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2112.12409,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2021,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000063909,
      "text":"InstaIndoor and Multi-modal Deep Learning for Indoor Scene Recognition\n\n  Indoor scene recognition is a growing field with great potential for\nbehaviour understanding, robot localization, and elderly monitoring, among\nothers. In this study, we approach the task of scene recognition from a novel\nstandpoint, using multi-modal learning and video data gathered from social\nmedia. The accessibility and variety of social media videos can provide\nrealistic data for modern scene recognition techniques and applications. We\npropose a model based on fusion of transcribed speech to text and visual\nfeatures, which is used for classification on a novel dataset of social media\nvideos of indoor scenes named InstaIndoor. Our model achieves up to 70%\naccuracy and 0.7 F1-Score. Furthermore, we highlight the potential of our\napproach by benchmarking on a YouTube-8M subset of indoor scenes as well, where\nit achieves 74% accuracy and 0.74 F1-Score. We hope the contributions of this\nwork pave the way to novel research in the challenging field of indoor scene\nrecognition.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.09169,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000274181,
      "text":"Rich Action-semantic Consistent Knowledge for Early Action Prediction\n\n  Early action prediction (EAP) aims to recognize human actions from a part of\naction execution in ongoing videos, which is an important task for many\npractical applications. Most prior works treat partial or full videos as a\nwhole, ignoring rich action knowledge hidden in videos, i.e., semantic\nconsistencies among different partial videos. In contrast, we partition\noriginal partial or full videos to form a new series of partial videos and mine\nthe Action-Semantic Consistent Knowledge (ASCK) among these new partial videos\nevolving in arbitrary progress levels. Moreover, a novel Rich Action-semantic\nConsistent Knowledge network (RACK) under the teacher-student framework is\nproposed for EAP. Firstly, we use a two-stream pre-trained model to extract\nfeatures of videos. Secondly, we treat the RGB or flow features of the partial\nvideos as nodes and their action semantic consistencies as edges. Next, we\nbuild a bi-directional semantic graph for the teacher network and a\nsingle-directional semantic graph for the student network to model rich ASCK\namong partial videos. The MSE and MMD losses are incorporated as our\ndistillation loss to enrich the ASCK of partial videos from the teacher to the\nstudent network. Finally, we obtain the final prediction by summering the\nlogits of different subnetworks and applying a softmax layer. Extensive\nexperiments and ablative studies have been conducted, demonstrating the\neffectiveness of modeling rich ASCK for EAP. With the proposed RACK, we have\nachieved state-of-the-art performance on three benchmarks. The code is\navailable at https:\/\/github.com\/lily2lab\/RACK.git.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.10739,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.00000904,
      "text":"Infrared and visible image fusion based on Multi-State Contextual Hidden\n  Markov Model\n\n  The traditional two-state hidden Markov model divides the high frequency\ncoefficients only into two states (large and small states). Such scheme is\nprone to produce an inaccurate statistical model for the high frequency subband\nand reduces the quality of fusion result. In this paper, a fine-grained\nmulti-state contextual hidden Markov model (MCHMM) is proposed for infrared and\nvisible image fusion in the non-subsampled Shearlet domain, which takes full\nconsideration of the strong correlations and level of details of NSST\ncoefficients. To this end, an accurate soft context variable is designed\ncorrespondingly from the perspective of context correlation. Then, the\nstatistical features provided by MCHMM are utilized for the fusion of high\nfrequency subbands. To ensure the visual quality, a fusion strategy based on\nthe difference in regional energy is proposed as well for lowfrequency\nsubbands. Experimental results demonstrate that the proposed method can achieve\na superior performance compared with other fusion methods in both subjective\nand objective aspects.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.09373,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000101328,
      "text":"Unsupervised Severely Deformed Mesh Reconstruction (DMR) from a\n  Single-View Image\n\n  Much progress has been made in the supervised learning of 3D reconstruction\nof rigid objects from multi-view images or a video. However, it is more\nchallenging to reconstruct severely deformed objects from a single-view RGB\nimage in an unsupervised manner. Although training-based methods, such as\nspecific category-level training, have been shown to successfully reconstruct\nrigid objects and slightly deformed objects like birds from a single-view\nimage, they cannot effectively handle severely deformed objects and neither can\nbe applied to some downstream tasks in the real world due to the inconsistent\nsemantic meaning of vertices, which are crucial in defining the adopted 3D\ntemplates of objects to be reconstructed. In this work, we introduce a\ntemplate-based method to infer 3D shapes from a single-view image and apply the\nreconstructed mesh to a downstream task, i.e., absolute length measurement.\nWithout using 3D ground truth, our method faithfully reconstructs 3D meshes and\nachieves state-of-the-art accuracy in a length measurement task on a severely\ndeformed fish dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.00267,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000049671,
      "text":"On the Cross-dataset Generalization in License Plate Recognition\n\n  Automatic License Plate Recognition (ALPR) systems have shown remarkable\nperformance on license plates (LPs) from multiple regions due to advances in\ndeep learning and the increasing availability of datasets. The evaluation of\ndeep ALPR systems is usually done within each dataset; therefore, it is\nquestionable if such results are a reliable indicator of generalization\nability. In this paper, we propose a traditional-split versus\nleave-one-dataset-out experimental setup to empirically assess the\ncross-dataset generalization of 12 Optical Character Recognition (OCR) models\napplied to LP recognition on nine publicly available datasets with a great\nvariety in several aspects (e.g., acquisition settings, image resolution, and\nLP layouts). We also introduce a public dataset for end-to-end ALPR that is the\nfirst to contain images of vehicles with Mercosur LPs and the one with the\nhighest number of motorcycle images. The experimental results shed light on the\nlimitations of the traditional-split protocol for evaluating approaches in the\nALPR context, as there are significant drops in performance for most datasets\nwhen training and testing the models in a leave-one-dataset-out fashion.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08425,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000139409,
      "text":"FaceOcc: A Diverse, High-quality Face Occlusion Dataset for Human Face\n  Extraction\n\n  Occlusions often occur in face images in the wild, troubling face-related\ntasks such as landmark detection, 3D reconstruction, and face recognition. It\nis beneficial to extract face regions from unconstrained face images\naccurately. However, current face segmentation datasets suffer from small data\nvolumes, few occlusion types, low resolution, and imprecise annotation,\nlimiting the performance of data-driven-based algorithms. This paper proposes a\nnovel face occlusion dataset with manually labeled face occlusions from the\nCelebA-HQ and the internet. The occlusion types cover sunglasses, spectacles,\nhands, masks, scarfs, microphones, etc. To the best of our knowledge, it is by\nfar the largest and most comprehensive face occlusion dataset. Combining it\nwith the attribute mask in CelebAMask-HQ, we trained a straightforward face\nsegmentation model but obtained SOTA performance, convincingly demonstrating\nthe effectiveness of the proposed dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07661,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Open Source Handwritten Text Recognition on Medieval Manuscripts using\n  Mixed Models and Document-Specific Finetuning\n\n  This paper deals with the task of practical and open source Handwritten Text\nRecognition (HTR) on German medieval manuscripts. We report on our efforts to\nconstruct mixed recognition models which can be applied out-of-the-box without\nany further document-specific training but also serve as a starting point for\nfinetuning by training a new model on a few pages of transcribed text (ground\ntruth). To train the mixed models we collected a corpus of 35 manuscripts and\nca. 12.5k text lines for two widely used handwriting styles, Gothic and\nBastarda cursives. Evaluating the mixed models out-of-the-box on four unseen\nmanuscripts resulted in an average Character Error Rate (CER) of 6.22%. After\ntraining on 2, 4 and eventually 32 pages the CER dropped to 3.27%, 2.58%, and\n1.65%, respectively. While the in-domain recognition and training of models\n(Bastarda model to Bastarda material, Gothic to Gothic) unsurprisingly yielded\nthe best results, finetuning out-of-domain models to unseen scripts was still\nshown to be superior to training from scratch.\n  Our new mixed models have been made openly available to the community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08746,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000084109,
      "text":"ERS: a novel comprehensive endoscopy image dataset for machine learning,\n  compliant with the MST 3.0 specification\n\n  The article presents a new multi-label comprehensive image dataset from\nflexible endoscopy, colonoscopy and capsule endoscopy, named ERS. The\ncollection has been labeled according to the full medical specification of\n'Minimum Standard Terminology 3.0' (MST 3.0), describing all possible findings\nin the gastrointestinal tract (104 possible labels), extended with an\nadditional 19 labels useful in common machine learning applications.\n  The dataset contains around 6000 precisely and 115,000 approximately labeled\nframes from endoscopy videos, 3600 precise and 22,600 approximate segmentation\nmasks, and 1.23 million unlabeled frames from flexible and capsule endoscopy\nvideos. The labeled data cover almost entirely the MST 3.0 standard. The data\ncame from 1520 videos of 1135 patients.\n  Additionally, this paper proposes and describes four exemplary experiments in\ngastrointestinal image classification task performed using the created dataset.\nThe obtained results indicate the high usefulness and flexibility of the\ndataset in training and testing machine learning algorithms in the field of\nendoscopic data analysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.09246,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"Face recognition via compact second order image gradient orientations\n\n  Conventional subspace learning approaches based on image gradient\norientations only employ the first-order gradient information. However, recent\nresearches on human vision system (HVS) uncover that the neural image is a\nlandscape or a surface whose geometric properties can be captured through the\nsecond order gradient information. The second order image gradient orientations\n(SOIGO) can mitigate the adverse effect of noises in face images. To reduce the\nredundancy of SOIGO, we propose compact SOIGO (CSOIGO) by applying linear\ncomplex principal component analysis (PCA) in SOIGO. Combined with\ncollaborative representation based classification (CRC) algorithm, the\nclassification performance of CSOIGO is further enhanced. CSOIGO is evaluated\nunder real-world disguise, synthesized occlusion and mixed variations.\nExperimental results indicate that the proposed method is superior to its\ncompeting approaches with few training samples, and even outperforms some\nprevailing deep neural network based approaches. The source code of CSOIGO is\navailable at https:\/\/github.com\/yinhefeng\/SOIGO.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.01115,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000120865,
      "text":"Data Augmentation for Depression Detection Using Skeleton-Based Gait\n  Information\n\n  In recent years, the incidence of depression is rising rapidly worldwide, but\nlarge-scale depression screening is still challenging. Gait analysis provides a\nnon-contact, low-cost, and efficient early screening method for depression.\nHowever, the early screening of depression based on gait analysis lacks\nsufficient effective sample data. In this paper, we propose a skeleton data\naugmentation method for assessing the risk of depression. First, we propose\nfive techniques to augment skeleton data and apply them to depression and\nemotion datasets. Then, we divide augmentation methods into two types\n(non-noise augmentation and noise augmentation) based on the mutual information\nand the classification accuracy. Finally, we explore which augmentation\nstrategies can capture the characteristics of human skeleton data more\neffectively. Experimental results show that the augmented training data set\nthat retains more of the raw skeleton data properties determines the\nperformance of the detection model. Specifically, rotation augmentation and\nchannel mask augmentation make the depression detection accuracy reach 92.15%\nand 91.34%, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.09384,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"A Comprehensive Survey on Federated Learning: Concept and Applications\n\n  This paper provides a comprehensive study of Federated Learning (FL) with an\nemphasis on components, challenges, applications and FL environment. FL can be\napplicable in multiple fields and domains in real-life models. in the medical\nsystem, the privacy of patients records and their medical condition is critical\ndata, therefore collaborative learning or federated learning comes into the\npicture. On other hand build an intelligent system assist the medical staff\nwithout sharing the data lead into the FL concept and one of the applications\nthat are used is a brain tumor diagnosis intelligent system based on AI methods\nthat can efficiently work in a collaborative environment.this paper will\nintroduce some of the applications and related work in the medical field and\nwork under the FL concept then summarize them to introduce the main limitations\nof their work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.04797,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000320541,
      "text":"Scalable Cluster-Consistency Statistics for Robust Multi-Object Matching\n\n  We develop new statistics for robustly filtering corrupted keypoint matches\nin the structure from motion pipeline. The statistics are based on consistency\nconstraints that arise within the clustered structure of the graph of keypoint\nmatches. The statistics are designed to give smaller values to corrupted\nmatches and than uncorrupted matches. These new statistics are combined with an\niterative reweighting scheme to filter keypoints, which can then be fed into\nany standard structure from motion pipeline. This filtering method can be\nefficiently implemented and scaled to massive datasets as it only requires\nsparse matrix multiplication. We demonstrate the efficacy of this method on\nsynthetic and real structure from motion datasets and show that it achieves\nstate-of-the-art accuracy and speed in these tasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.00443,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000079142,
      "text":"Scene Graph Generation: A Comprehensive Survey\n\n  Deep learning techniques have led to remarkable breakthroughs in the field of\ngeneric object detection and have spawned a lot of scene-understanding tasks in\nrecent years. Scene graph has been the focus of research because of its\npowerful semantic representation and applications to scene understanding. Scene\nGraph Generation (SGG) refers to the task of automatically mapping an image\ninto a semantic structural scene graph, which requires the correct labeling of\ndetected objects and their relationships. Although this is a challenging task,\nthe community has proposed a lot of SGG approaches and achieved good results.\nIn this paper, we provide a comprehensive survey of recent achievements in this\nfield brought about by deep learning techniques. We review 138 representative\nworks that cover different input modalities, and systematically summarize\nexisting methods of image-based SGG from the perspective of feature extraction\nand fusion. We attempt to connect and systematize the existing visual\nrelationship detection methods, to summarize, and interpret the mechanisms and\nthe strategies of SGG in a comprehensive way. Finally, we finish this survey\nwith deep discussions about current existing problems and future research\ndirections. This survey will help readers to develop a better understanding of\nthe current research status and ideas.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.0005,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"Deep-Disaster: Unsupervised Disaster Detection and Localization Using\n  Visual Data\n\n  Social media plays a significant role in sharing essential information, which\nhelps humanitarian organizations in rescue operations during and after disaster\nincidents. However, developing an efficient method that can provide rapid\nanalysis of social media images in the early hours of disasters is still\nlargely an open problem, mainly due to the lack of suitable datasets and the\nsheer complexity of this task. In addition, supervised methods can not\ngeneralize well to novel disaster incidents. In this paper, inspired by the\nsuccess of Knowledge Distillation (KD) methods, we propose an unsupervised deep\nneural network to detect and localize damages in social media images. Our\nproposed KD architecture is a feature-based distillation approach that\ncomprises a pre-trained teacher and a smaller student network, with both\nnetworks having similar GAN architecture containing a generator and a\ndiscriminator. The student network is trained to emulate the behavior of the\nteacher on training input samples, which, in turn, contain images that do not\ninclude any damaged regions. Therefore, the student network only learns the\ndistribution of no damage data and would have different behavior from the\nteacher network-facing damages. To detect damage, we utilize the difference\nbetween features generated by two networks using a defined score function that\ndemonstrates the probability of damages occurring. Our experimental results on\nthe benchmark dataset confirm that our approach outperforms state-of-the-art\nmethods in detecting and localizing the damaged areas, especially for novel\ndisaster types.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02963,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level\n  Supervision\n\n  Learning dense point-wise semantics from unstructured 3D point clouds with\nfewer labels, although a realistic problem, has been under-explored in\nliterature. While existing weakly supervised methods can effectively learn\nsemantics with only a small fraction of point-level annotations, we find that\nthe vanilla bounding box-level annotation is also informative for semantic\nsegmentation of large-scale 3D point clouds. In this paper, we introduce a\nneural architecture, termed Box2Seg, to learn point-level semantics of 3D point\nclouds with bounding box-level supervision. The key to our approach is to\ngenerate accurate pseudo labels by exploring the geometric and topological\nstructure inside and outside each bounding box. Specifically, an\nattention-based self-training (AST) technique and Point Class Activation\nMapping (PCAM) are utilized to estimate pseudo-labels. The network is further\ntrained and refined with pseudo labels. Experiments on two large-scale\nbenchmarks including S3DIS and ScanNet demonstrate the competitive performance\nof the proposed method. In particular, the proposed network can be trained with\ncheap, or even off-the-shelf bounding box-level annotations and subcloud-level\ntags.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.00454,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000089738,
      "text":"Memory-Guided Semantic Learning Network for Temporal Sentence Grounding\n\n  Temporal sentence grounding (TSG) is crucial and fundamental for video\nunderstanding. Although the existing methods train well-designed deep networks\nwith a large amount of data, we find that they can easily forget the rarely\nappeared cases in the training stage due to the off-balance data distribution,\nwhich influences the model generalization and leads to undesirable performance.\nTo tackle this issue, we propose a memory-augmented network, called\nMemory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes\nthe rarely appeared content in TSG tasks. Specifically, MGSL-Net consists of\nthree main parts: a cross-modal inter-action module, a memory augmentation\nmodule, and a heterogeneous attention module. We first align the given\nvideo-query pair by a cross-modal graph convolutional network, and then utilize\na memory module to record the cross-modal shared semantic features in the\ndomain-specific persistent memory. During training, the memory slots are\ndynamically associated with both common and rare cases, alleviating the\nforgetting issue. In testing, the rare cases can thus be enhanced by retrieving\nthe stored memories, resulting in better generalization. At last, the\nheterogeneous attention module is utilized to integrate the enhanced\nmulti-modal features in both video and query domains. Experimental results on\nthree benchmarks show the superiority of our method on both effectiveness and\nefficiency, which substantially improves the accuracy not only on the entire\ndataset but also on rare cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07989,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000036756,
      "text":"Self-supervised Video Representation Learning with Cascade Positive\n  Retrieval\n\n  Self-supervised video representation learning has been shown to effectively\nimprove downstream tasks such as video retrieval and action recognition. In\nthis paper, we present the Cascade Positive Retrieval (CPR) that successively\nmines positive examples w.r.t. the query for contrastive learning in a cascade\nof stages. Specifically, CPR exploits multiple views of a query example in\ndifferent modalities, where an alternative view may help find another positive\nexample dissimilar in the query view. We explore the effects of possible CPR\nconfigurations in ablations including the number of mining stages, the top\nsimilar example selection ratio in each stage, and progressive training with an\nincremental number of the final Top-k selection. The overall mining quality is\nmeasured to reflect the recall across training set classes. CPR reaches a\nmedian class mining recall of 83.3%, outperforming previous work by 5.5%.\nImplementation-wise, CPR is complementary to pretext tasks and can be easily\napplied to previous work. In the evaluation of pretraining on UCF101, CPR\nconsistently improves existing work and even achieves state-of-the-art R@1 of\n56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action\nrecognition on UCF101 and HMDB51. The code is available at\nhttps:\/\/github.com\/necla-ml\/CPR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.02784,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000093049,
      "text":"Relieving Long-tailed Instance Segmentation via Pairwise Class Balance\n\n  Long-tailed instance segmentation is a challenging task due to the extreme\nimbalance of training samples among classes. It causes severe biases of the\nhead classes (with majority samples) against the tailed ones. This renders \"how\nto appropriately define and alleviate the bias\" one of the most important\nissues. Prior works mainly use label distribution or mean score information to\nindicate a coarse-grained bias. In this paper, we explore to excavate the\nconfusion matrix, which carries the fine-grained misclassification details, to\nrelieve the pairwise biases, generalizing the coarse one. To this end, we\npropose a novel Pairwise Class Balance (PCB) method, built upon a confusion\nmatrix which is updated during training to accumulate the ongoing prediction\npreferences. PCB generates fightback soft labels for regularization during\ntraining. Besides, an iterative learning paradigm is developed to support a\nprogressive and smooth regularization in such debiasing. PCB can be plugged and\nplayed to any existing method as a complement. Experimental results on LVIS\ndemonstrate that our method achieves state-of-the-art performance without bells\nand whistles. Superior results across various architectures show the\ngeneralization ability. The code and trained models are available at\nhttps:\/\/github.com\/megvii-research\/PCB.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.12728,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000028147,
      "text":"Video-based Facial Micro-Expression Analysis: A Survey of Datasets,\n  Features and Algorithms\n\n  Unlike the conventional facial expressions, micro-expressions are involuntary\nand transient facial expressions capable of revealing the genuine emotions that\npeople attempt to hide. Therefore, they can provide important information in a\nbroad range of applications such as lie detection, criminal detection, etc.\nSince micro-expressions are transient and of low intensity, however, their\ndetection and recognition is difficult and relies heavily on expert\nexperiences. Due to its intrinsic particularity and complexity, video-based\nmicro-expression analysis is attractive but challenging, and has recently\nbecome an active area of research. Although there have been numerous\ndevelopments in this area, thus far there has been no comprehensive survey that\nprovides researchers with a systematic overview of these developments with a\nunified evaluation. Accordingly, in this survey paper, we first highlight the\nkey differences between macro- and micro-expressions, then use these\ndifferences to guide our research survey of video-based micro-expression\nanalysis in a cascaded structure, encompassing the neuropsychological basis,\ndatasets, features, spotting algorithms, recognition algorithms, applications\nand evaluation of state-of-the-art approaches. For each aspect, the basic\ntechniques, advanced developments and major challenges are addressed and\ndiscussed. Furthermore, after considering the limitations of existing\nmicro-expression datasets, we present and release a new dataset - called\nmicro-and-macro expression warehouse (MMEW) - containing more video samples and\nmore labeled emotion types. We then perform a unified comparison of\nrepresentative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for\nrecognition, respectively. Finally, some potential future research directions\nare explored and outlined.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.07264,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Exploring Kervolutional Neural Networks\n\n  A paper published in the CVPR 2019 conference outlines a new technique called\n'kervolution' used in a new type of augmented convolutional neural network\n(CNN) called a 'kervolutional neural network' (KNN). The paper asserts that\nKNNs achieve faster convergence and higher accuracies than CNNs. This \"mini\npaper\" will further examine the findings in the original paper and perform a\nmore in depth analysis of the KNN architecture. This will be done by analyzing\nthe impact of hyper parameters (specifically the learning rate) on KNNs versus\nCNNs, experimenting with other types of kervolution operations not tested in\nthe original paper, a more rigourous statistical analysis of accuracies and\nconvergence times and additional theoretical analysis. The accompanying code is\npublicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2201.08001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":1,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"CELESTIAL: Classification Enabled via Labelless Embeddings with\n  Self-supervised Telescope Image Analysis Learning\n\n  A common class of problems in remote sensing is scene classification, a\nfundamentally important task for natural hazards identification, geographic\nimage retrieval, and environment monitoring. Recent developments in this field\nrely label-dependent supervised learning techniques which is antithetical to\nthe 35 petabytes of unlabelled satellite imagery in NASA GIBS. To solve this\nproblem, we establish CELESTIAL-a self-supervised learning pipeline for\neffectively leveraging sparsely-labeled satellite imagery. This pipeline\nsuccessfully adapts SimCLR, an algorithm that first learns image\nrepresentations on unlabelled data and then fine-tunes this knowledge on the\nprovided labels. Our results show CELESTIAL requires only a third of the labels\nthat the supervised method needs to attain the same accuracy on an experimental\ndataset. The first unsupervised tier can enable applications such as reverse\nimage search for NASA Worldview (i.e. searching similar atmospheric phenomenon\nover years of unlabelled data with minimal samples) and the second supervised\ntier can lower the necessity of expensive data annotation significantly. In the\nfuture, we hope we can generalize the CELESTIAL pipeline to other data types,\nalgorithms, and applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.038,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000217888,
      "text":"Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the\n  Structure Space\n\n  Face clustering has attracted rising research interest recently to take\nadvantage of massive amounts of face images on the web. State-of-the-art\nperformance has been achieved by Graph Convolutional Networks (GCN) due to\ntheir powerful representation capacity. However, existing GCN-based methods\nbuild face graphs mainly according to kNN relations in the feature space, which\nmay lead to a lot of noise edges connecting two faces of different classes. The\nface features will be polluted when messages pass along these noise edges, thus\ndegrading the performance of GCNs. In this paper, a novel algorithm named\nAda-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In\nAda-NETS, each face is transformed to a new structure space, obtaining robust\nfeatures by considering face features of the neighbour images. Then, an\nadaptive neighbour discovery strategy is proposed to determine a proper number\nof edges connecting to each face image. It significantly reduces the noise\nedges while maintaining the good ones to build a graph with clean yet rich\nedges for GCNs to cluster faces. Experiments on multiple public clustering\ndatasets show that Ada-NETS significantly outperforms current state-of-the-art\nmethods, proving its superiority and generalization. Code is available at\nhttps:\/\/github.com\/damo-cv\/Ada-NETS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.11998,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Effective Actor-centric Human-object Interaction Detection\n\n  While Human-Object Interaction(HOI) Detection has achieved tremendous\nadvances in recent, it still remains challenging due to complex interactions\nwith multiple humans and objects occurring in images, which would inevitably\nlead to ambiguities. Most existing methods either generate all human-object\npair candidates and infer their relationships by cropped local features\nsuccessively in a two-stage manner, or directly predict interaction points in a\none-stage procedure. However, the lack of spatial configurations or reasoning\nsteps of two- or one- stage methods respectively limits their performance in\nsuch complex scenes. To avoid this ambiguity, we propose a novel actor-centric\nframework. The main ideas are that when inferring interactions: 1) the\nnon-local features of the entire image guided by actor position are obtained to\nmodel the relationship between the actor and context, and then 2) we use an\nobject branch to generate pixel-wise interaction area prediction, where the\ninteraction area denotes the object central area. Moreover, we also use an\nactor branch to get interaction prediction of the actor and propose a novel\ncomposition strategy based on center-point indexing to generate the final HOI\nprediction. Thanks to the usage of the non-local features and the\npartly-coupled property of the human-objects composition strategy, our proposed\nframework can detect HOI more accurately especially for complex images.\nExtensive experimental results show that our method achieves the\nstate-of-the-art on the challenging V-COCO and HICO-DET benchmarks and is more\nrobust especially in multiple persons and\/or objects scenes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.13018,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"HCIL: Hierarchical Class Incremental Learning for Longline Fishing\n  Visual Monitoring\n\n  The goal of electronic monitoring of longline fishing is to visually monitor\nthe fish catching activities on fishing vessels based on cameras, either for\nregulatory compliance or catch counting. The previous hierarchical\nclassification method demonstrates efficient fish species identification of\ncatches from longline fishing, where fishes are under severe deformation and\nself-occlusion during the catching process. Although the hierarchical\nclassification mitigates the laborious efforts of human reviews by providing\nconfidence scores in different hierarchical levels, its performance drops\ndramatically under the class incremental learning (CIL) scenario. A CIL system\nshould be able to learn about more and more classes over time from a stream of\ndata, i.e., only the training data for a small number of classes have to be\npresent at the beginning and new classes can be added progressively. In this\nwork, we introduce a Hierarchical Class Incremental Learning (HCIL) model,\nwhich significantly improves the state-of-the-art hierarchical classification\nmethods under the CIL scenario.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06857,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"A Graph-Matching Approach for Cross-view Registration of Over-view 2 and\n  Street-view based Point Clouds\n\n  In this paper, based on the assumption that the object boundaries (e.g.,\nbuildings) from the over-view data should coincide with footprints of\nfa\\c{c}ade 3D points generated from street-view photogrammetric images, we aim\nto address this problem by proposing a fully automated geo-registration method\nfor cross-view data, which utilizes semantically segmented object boundaries as\nview-invariant features under a global optimization framework through\ngraph-matching: taking the over-view point clouds generated from\nstereo\/multi-stereo satellite images and the street-view point clouds generated\nfrom monocular video images as the inputs, the proposed method models segments\nof buildings as nodes of graphs, both detected from the satellite-based and\nstreet-view based point clouds, thus to form the registration as a\ngraph-matching problem to allow non-rigid matches; to enable a robust solution\nand fully utilize the topological relations between these segments, we propose\nto address the graph-matching problem on its conjugate graph solved through a\nbelief-propagation algorithm. The matched nodes will be subject to a further\noptimization to allow precise-registration, followed by a constrained bundle\nadjustment on the street-view image to keep 2D29 3D consistencies, which yields\nwell-registered street-view images and point clouds to the satellite point\nclouds.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.063,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction\n\n  Lighting prediction from a single image is becoming increasingly important in\nmany vision and augmented reality (AR) applications in which shading and shadow\nconsistency between virtual and real objects should be guaranteed. However,\nthis is a notoriously ill-posed problem, especially for indoor scenarios,\nbecause of the complexity of indoor luminaires and the limited information\ninvolved in 2D images. In this paper, we propose a graph learning-based\nframework for indoor lighting estimation. At its core is a new lighting model\n(dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph\nConvolutional Network (GCN) that infers the new lighting representation from a\nsingle LDR image of limited field-of-view. Our lighting model builds 128 evenly\ndistributed SGs over the indoor panorama, where each SG encoding the lighting\nand the depth around that node. The proposed GCN then learns the mapping from\nthe input image to DSGLight. Compared with existing lighting models, our\nDSGLight encodes both direct lighting and indirect environmental lighting more\nfaithfully and compactly. It also makes network training and inference more\nstable. The estimated depth distribution enables temporally stable shading and\nshadows under spatially-varying lighting. Through thorough experiments, we show\nthat our method obviously outperforms existing methods both qualitatively and\nquantitatively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.11915,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Interpolation-based Contrastive Learning for Few-Label Semi-Supervised\n  Learning\n\n  Semi-supervised learning (SSL) has long been proved to be an effective\ntechnique to construct powerful models with limited labels. In the existing\nliterature, consistency regularization-based methods, which force the perturbed\nsamples to have similar predictions with the original ones have attracted much\nattention for their promising accuracy. However, we observe that, the\nperformance of such methods decreases drastically when the labels get extremely\nlimited, e.g., 2 or 3 labels for each category. Our empirical study finds that\nthe main problem lies with the drifting of semantic information in the\nprocedure of data augmentation. The problem can be alleviated when enough\nsupervision is provided. However, when little guidance is available, the\nincorrect regularization would mislead the network and undermine the\nperformance of the algorithm. To tackle the problem, we (1) propose an\ninterpolation-based method to construct more reliable positive sample pairs;\n(2) design a novel contrastive loss to guide the embedding of the learned\nnetwork to change linearly between samples so as to improve the discriminative\ncapability of the network by enlarging the margin decision boundaries. Since no\ndestructive regularization is introduced, the performance of our proposed\nalgorithm is largely improved. Specifically, the proposed algorithm outperforms\nthe second best algorithm (Comatch) with 5.3% by achieving 88.73%\nclassification accuracy when only two labels are available for each class on\nthe CIFAR-10 dataset. Moreover, we further prove the generality of the proposed\nmethod by improving the performance of the existing state-of-the-art algorithms\nconsiderably with our proposed strategy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06687,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000095699,
      "text":"Domain Adaptation via Prompt Learning\n\n  Unsupervised domain adaption (UDA) aims to adapt models learned from a\nwell-annotated source domain to a target domain, where only unlabeled samples\nare given. Current UDA approaches learn domain-invariant features by aligning\nsource and target feature spaces. Such alignments are imposed by constraints\nsuch as statistical discrepancy minimization or adversarial training. However,\nthese constraints could lead to the distortion of semantic feature structures\nand loss of class discriminability. In this paper, we introduce a novel prompt\nlearning paradigm for UDA, named Domain Adaptation via Prompt Learning (DAPL).\nIn contrast to prior works, our approach makes use of pre-trained\nvision-language models and optimizes only very few parameters. The main idea is\nto embed domain information into prompts, a form of representations generated\nfrom natural language, which is then used to perform classification. This\ndomain information is shared only by images from the same domain, thereby\ndynamically adapting the classifier according to each domain. By adopting this\nparadigm, we show that our model not only outperforms previous methods on\nseveral cross-domain benchmarks but also is very efficient to train and easy to\nimplement.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.02703,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000027153,
      "text":"Multi-modal Sensor Fusion for Auto Driving Perception: A Survey\n\n  Multi-modal fusion is a fundamental task for the perception of an autonomous\ndriving system, which has recently intrigued many researchers. However,\nachieving a rather good performance is not an easy task due to the noisy raw\ndata, underutilized information, and the misalignment of multi-modal sensors.\nIn this paper, we provide a literature review of the existing multi-modal-based\nmethods for perception tasks in autonomous driving. Generally, we make a\ndetailed analysis including over 50 papers leveraging perception sensors\nincluding LiDAR and camera trying to solve object detection and semantic\nsegmentation tasks. Different from traditional fusion methodology for\ncategorizing fusion models, we propose an innovative way that divides them into\ntwo major classes, four minor classes by a more reasonable taxonomy in the view\nof the fusion stage. Moreover, we dive deep into the current fusion methods,\nfocusing on the remaining problems and open-up discussions on the potential\nresearch opportunities. In conclusion, what we expect to do in this paper is to\npresent a new taxonomy of multi-modal fusion methods for the autonomous driving\nperception tasks and provoke thoughts of the fusion-based techniques in the\nfuture.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.12588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000676844,
      "text":"Active Learning for Point Cloud Semantic Segmentation via\n  Spatial-Structural Diversity Reasoning\n\n  The expensive annotation cost is notoriously known as the main constraint for\nthe development of the point cloud semantic segmentation technique. Active\nlearning methods endeavor to reduce such cost by selecting and labeling only a\nsubset of the point clouds, yet previous attempts ignore the spatial-structural\ndiversity of the selected samples, inducing the model to select clustered\ncandidates with similar shapes in a local area while missing other\nrepresentative ones in the global environment. In this paper, we propose a new\n3D region-based active learning method to tackle this problem. Dubbed SSDR-AL,\nour method groups the original point clouds into superpoints and incrementally\nselects the most informative and representative ones for label acquisition. We\nachieve the selection mechanism via a graph reasoning network that considers\nboth the spatial and structural diversities of superpoints. To deploy SSDR-AL\nin a more practical scenario, we design a noise-aware iterative labeling\nstrategy to confront the \"noisy annotation\" problem introduced by the previous\n\"dominant labeling\" strategy in superpoints. Extensive experiments on two point\ncloud benchmarks demonstrate the effectiveness of SSDR-AL in the semantic\nsegmentation task. Particularly, SSDR-AL significantly outperforms the baseline\nmethod and reduces the annotation cost by up to 63.0% and 24.0% when achieving\n90% performance of fully supervised learning, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.10773,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Deep learning based domain adaptation for mitochondria segmentation on\n  EM volumes\n\n  Accurate segmentation of electron microscopy (EM) volumes of the brain is\nessential to characterize neuronal structures at a cell or organelle level.\nWhile supervised deep learning methods have led to major breakthroughs in that\ndirection during the past years, they usually require large amounts of\nannotated data to be trained, and perform poorly on other data acquired under\nsimilar experimental and imaging conditions. This is a problem known as domain\nadaptation, since models that learned from a sample distribution (or source\ndomain) struggle to maintain their performance on samples extracted from a\ndifferent distribution or target domain. In this work, we address the complex\ncase of deep learning based domain adaptation for mitochondria segmentation\nacross EM datasets from different tissues and species. We present three\nunsupervised domain adaptation strategies to improve mitochondria segmentation\nin the target domain based on (1) state-of-the-art style transfer between\nimages of both domains; (2) self-supervised learning to pre-train a model using\nunlabeled source and target images, and then fine-tune it only with the source\nlabels; and (3) multi-task neural network architectures trained end-to-end with\nboth labeled and unlabeled images. Additionally, we propose a new training\nstopping criterion based on morphological priors obtained exclusively in the\nsource domain. We carried out all possible cross-dataset experiments using\nthree publicly available EM datasets. We evaluated our proposed strategies on\nthe mitochondria semantic labels predicted on the target datasets. The methods\nintroduced here outperform the baseline methods and compare favorably to the\nstate of the art. In the absence of validation labels, monitoring our proposed\nmorphology-based metric is an intuitive and effective way to stop the training\nprocess and select in average optimal models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06266,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000028809,
      "text":"Improve Deep Image Inpainting by Emphasizing the Complexity of Missing\n  Regions\n\n  Deep image inpainting research mainly focuses on constructing various neural\nnetwork architectures or imposing novel optimization objectives. However, on\nthe one hand, building a state-of-the-art deep inpainting model is an extremely\ncomplex task, and on the other hand, the resulting performance gains are\nsometimes very limited. We believe that besides the frameworks of inpainting\nmodels, lightweight traditional image processing techniques, which are often\noverlooked, can actually be helpful to these deep models. In this paper, we\nenhance the deep image inpainting models with the help of classical image\ncomplexity metrics. A knowledge-assisted index composed of missingness\ncomplexity and forward loss is presented to guide the batch selection in the\ntraining procedure. This index helps find samples that are more conducive to\noptimization in each iteration and ultimately boost the overall inpainting\nperformance. The proposed approach is simple and can be plugged into many deep\ninpainting models by changing only a few lines of code. We experimentally\ndemonstrate the improvements for several recently developed image inpainting\nmodels on various datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.06568,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000094374,
      "text":"Online-updated High-order Collaborative Networks for Single Image\n  Deraining\n\n  Single image deraining is an important and challenging task for some\ndownstream artificial intelligence applications such as video surveillance and\nself-driving systems. Most of the existing deep-learning-based methods\nconstrain the network to generate derained images but few of them explore\nfeatures from intermediate layers, different levels, and different modules\nwhich are beneficial for rain streaks removal. In this paper, we propose a\nhigh-order collaborative network with multi-scale compact constraints and a\nbidirectional scale-content similarity mining module to exploit features from\ndeep networks externally and internally for rain streaks removal. Externally,\nwe design a deraining framework with three sub-networks trained in a\ncollaborative manner, where the bottom network transmits intermediate features\nto the middle network which also receives shallower rainy features from the top\nnetwork and sends back features to the bottom network. Internally, we enforce\nmulti-scale compact constraints on the intermediate layers of deep networks to\nlearn useful features via a Laplacian pyramid. Further, we develop a\nbidirectional scale-content similarity mining module to explore features at\ndifferent scales in a down-to-up and up-to-down manner. To improve the model\nperformance on real-world images, we propose an online-update learning\napproach, which uses real-world rainy images to fine-tune the network and\nupdate the deraining results in a self-supervised manner. Extensive experiments\ndemonstrate that our proposed method performs favorably against eleven\nstate-of-the-art methods on five public synthetic datasets and one real-world\ndataset. The source code will be available at\n\\url{https:\/\/supercong94.wixsite.com\/supercong94}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07308,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"HAA4D: Few-Shot Human Atomic Action Recognition via 3D Spatio-Temporal\n  Skeletal Alignment\n\n  Human actions involve complex pose variations and their 2D projections can be\nhighly ambiguous. Thus 3D spatio-temporal or 4D (i.e., 3D+T) human skeletons,\nwhich are photometric and viewpoint invariant, are an excellent alternative to\n2D+T skeletons\/pixels to improve action recognition accuracy. This paper\nproposes a new 4D dataset HAA4D which consists of more than 3,300 RGB videos in\n300 human atomic action classes. HAA4D is clean, diverse, class-balanced where\neach class is viewpoint-balanced with the use of 4D skeletons, in which as few\nas one 4D skeleton per class is sufficient for training a deep recognition\nmodel. Further, the choice of atomic actions makes annotation even easier,\nbecause each video clip lasts for only a few seconds. All training and testing\n3D skeletons in HAA4D are globally aligned, using a deep alignment model to the\nsame global space, making each skeleton face the negative z-direction. Such\nalignment makes matching skeletons more stable by reducing intraclass\nvariations and thus with fewer training samples per class needed for action\nrecognition. Given the high diversity and skeletal alignment in HAA4D, we\nconstruct the first baseline few-shot 4D human atomic action recognition\nnetwork without bells and whistles, which produces comparable or higher\nperformance than relevant state-of-the-art techniques relying on embedded space\nencoding without explicit skeletal alignment, using the same small number of\ntraining samples of unseen classes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.121,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000177821,
      "text":"DeepFusionMOT: A 3D Multi-Object Tracking Framework Based on\n  Camera-LiDAR Fusion with Deep Association\n\n  In the recent literature, on the one hand, many 3D multi-object tracking\n(MOT) works have focused on tracking accuracy and neglected computation speed,\ncommonly by designing rather complex cost functions and feature extractors. On\nthe other hand, some methods have focused too much on computation speed at the\nexpense of tracking accuracy. In view of these issues, this paper proposes a\nrobust and fast camera-LiDAR fusion-based MOT method that achieves a good\ntrade-off between accuracy and speed. Relying on the characteristics of camera\nand LiDAR sensors, an effective deep association mechanism is designed and\nembedded in the proposed MOT method. This association mechanism realizes\ntracking of an object in a 2D domain when the object is far away and only\ndetected by the camera, and updating of the 2D trajectory with 3D information\nobtained when the object appears in the LiDAR field of view to achieve a smooth\nfusion of 2D and 3D trajectories. Extensive experiments based on the typical\ndatasets indicate that our proposed method presents obvious advantages over the\nstate-of-the-art MOT methods in terms of both tracking accuracy and processing\nspeed. Our code is made publicly available for the benefit of the community.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09048,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Task Specific Attention is one more thing you need for object detection\n\n  Various models have been proposed to perform object detection. However, most\nrequire many handdesigned components such as anchors and\nnon-maximum-suppression(NMS) to demonstrate good performance. To mitigate these\nissues, Transformer-based DETR and its variant, Deformable DETR, were\nsuggested. These have solved much of the complex issue in designing a head for\nobject detection models; however, doubts about performance still exist when\nconsidering Transformer-based models as state-of-the-art methods in object\ndetection for other models depending on anchors and NMS revealed better\nresults. Furthermore, it has been unclear whether it would be possible to build\nan end-to-end pipeline in combination only with attention modules, because the\nDETR-adapted Transformer method used a convolutional neural network (CNN) for\nthe backbone body. In this study, we propose that combining several attention\nmodules with our new Task Specific Split Transformer (TSST) is a powerful\nmethod to produce the state-of-the art performance on COCO results without\ntraditionally hand-designed components. By splitting the general-purpose\nattention module into two separated goal-specific attention modules, the\nproposed method allows for the design of simpler object detection models.\nExtensive experiments on the COCO benchmark demonstrate the effectiveness of\nour approach. Code is available at https:\/\/github.com\/navervision\/tsst\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.0945,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Modern Augmented Reality: Applications, Trends, and Future Directions\n\n  Augmented reality (AR) is one of the relatively old, yet trending areas in\nthe intersection of computer vision and computer graphics with numerous\napplications in several areas, from gaming and entertainment, to education and\nhealthcare. Although it has been around for nearly fifty years, it has seen a\nlot of interest by the research community in the recent years, mainly because\nof the huge success of deep learning models for various computer vision and AR\napplications, which made creating new generations of AR technologies possible.\nThis work tries to provide an overview of modern augmented reality, from both\napplication-level and technical perspective. We first give an overview of main\nAR applications, grouped into more than ten categories. We then give an\noverview of around 100 recent promising machine learning based works developed\nfor AR systems, such as deep learning works for AR shopping (clothing, makeup),\nAR based image filters (such as Snapchat's lenses), AR animations, and more. In\nthe end we discuss about some of the current challenges in AR domain, and the\nfuture directions in this area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.09545,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000023842,
      "text":"Going Deeper into Recognizing Actions in Dark Environments: A\n  Comprehensive Benchmark Study\n\n  While action recognition (AR) has gained large improvements with the\nintroduction of large-scale video datasets and the development of deep neural\nnetworks, AR models robust to challenging environments in real-world scenarios\nare still under-explored. We focus on the task of action recognition in dark\nenvironments, which can be applied to fields such as surveillance and\nautonomous driving at night. Intuitively, current deep networks along with\nvisual enhancement techniques should be able to handle AR in dark environments,\nhowever, it is observed that this is not always the case in practice. To dive\ndeeper into exploring solutions for AR in dark environments, we launched the\nUG2+ Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evaluating and\nadvancing the robustness of AR models in dark environments. The challenge\nbuilds and expands on top of a novel ARID dataset, the first dataset for the\ntask of dark video AR, and guides models to tackle such a task in both fully\nand semi-supervised manners. Baseline results utilizing current AR models and\nenhancement methods are reported, justifying the challenging nature of this\ntask with substantial room for improvements. Thanks to the active participation\nfrom the research community, notable advances have been made in participants'\nsolutions, while analysis of these solutions helped better identify possible\ndirections to tackle the challenge of AR in dark environments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.04241,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Distillation with Contrast is All You Need for Self-Supervised Point\n  Cloud Representation Learning\n\n  In this paper, we propose a simple and general framework for self-supervised\npoint cloud representation learning. Human beings understand the 3D world by\nextracting two levels of information and establishing the relationship between\nthem. One is the global shape of an object, and the other is the local\nstructures of it. However, few existing studies in point cloud representation\nlearning explored how to learn both global shapes and local-to-global\nrelationships without a specified network architecture. Inspired by how human\nbeings understand the world, we utilize knowledge distillation to learn both\nglobal shape information and the relationship between global shape and local\nstructures. At the same time, we combine contrastive learning with knowledge\ndistillation to make the teacher network be better updated. Our method achieves\nthe state-of-the-art performance on linear classification and multiple other\ndownstream tasks. Especially, we develop a variant of ViT for 3D point cloud\nfeature extraction, which also achieves comparable results with existing\nbackbones when combined with our framework, and visualization of the attention\nmaps show that our model does understand the point cloud by combining the\nglobal shape information and multiple local structural information, which is\nconsistent with the inspiration of our representation learning method. Our code\nwill be released soon.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.07712,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0000060267,
      "text":"Privacy Preserving Visual Question Answering\n\n  We introduce a novel privacy-preserving methodology for performing Visual\nQuestion Answering on the edge. Our method constructs a symbolic representation\nof the visual scene, using a low-complexity computer vision model that jointly\npredicts classes, attributes and predicates. This symbolic representation is\nnon-differentiable, which means it cannot be used to recover the original\nimage, thereby keeping the original image private. Our proposed hybrid solution\nuses a vision model which is more than 25 times smaller than the current\nstate-of-the-art (SOTA) vision models, and 100 times smaller than end-to-end\nSOTA VQA models. We report detailed error analysis and discuss the trade-offs\nof using a distilled vision model and a symbolic representation of the visual\nscene.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2202.10645,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":2,
    "pangram_prediction":{
      "ai_likelihood":0.0002042452,
      "text":"Combining the Silhouette and Skeleton Data for Gait Recognition\n\n  Gait recognition, a long-distance biometric technology, has aroused intense\ninterest recently. Currently, the two dominant gait recognition works are\nappearance-based and model-based, which extract features from silhouettes and\nskeletons, respectively. However, appearance-based methods are greatly affected\nby clothes-changing and carrying conditions, while model-based methods are\nlimited by the accuracy of pose estimation. To tackle this challenge, a simple\nyet effective two-branch network is proposed in this paper, which contains a\nCNN-based branch taking silhouettes as input and a GCN-based branch taking\nskeletons as input. In addition, for better gait representation in the\nGCN-based branch, we present a fully connected graph convolution operator to\nintegrate multi-scale graph convolutions and alleviate the dependence on\nnatural joint connections. Also, we deploy a multi-dimension attention module\nnamed STC-Att to learn spatial, temporal and channel-wise attention\nsimultaneously. The experimental results on CASIA-B and OUMVLP show that our\nmethod achieves state-of-the-art performance in various conditions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.03376,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000580814,
      "text":"Spatio-temporal Gait Feature with Global Distance Alignment\n\n  Gait recognition is an important recognition technology, because gait is not\neasy to camouflage and does not need cooperation to recognize subjects.\nHowever, many existing methods are inadequate in preserving both temporal\ninformation and fine-grained information, thus reducing its discrimination.\nThis problem is more serious when the subjects with similar walking postures\nare identified. In this paper, we try to enhance the discrimination of\nspatio-temporal gait features from two aspects: effective extraction of\nspatio-temporal gait features and reasonable refinement of extracted features.\nThus our method is proposed, it consists of Spatio-temporal Feature Extraction\n(SFE) and Global Distance Alignment (GDA). SFE uses Temporal Feature Fusion\n(TFF) and Fine-grained Feature Extraction (FFE) to effectively extract the\nspatio-temporal features from raw silhouettes. GDA uses a large number of\nunlabeled gait data in real life as a benchmark to refine the extracted\nspatio-temporal features. GDA can make the extracted features have low\ninter-class similarity and high intra-class similarity, thus enhancing their\ndiscrimination. Extensive experiments on mini-OUMVLP and CASIA-B have proved\nthat we have a better result than some state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.08216,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"Interactive Portrait Harmonization\n\n  Current image harmonization methods consider the entire background as the\nguidance for harmonization. However, this may limit the capability for user to\nchoose any specific object\/person in the background to guide the harmonization.\nTo enable flexible interaction between user and harmonization, we introduce\ninteractive harmonization, a new setting where the harmonization is performed\nwith respect to a selected \\emph{region} in the reference image instead of the\nentire background. A new flexible framework that allows users to pick certain\nregions of the background image and use it to guide the harmonization is\nproposed. Inspired by professional portrait harmonization users, we also\nintroduce a new luminance matching loss to optimally match the color\/luminance\nconditions between the composite foreground and select reference region. This\nframework provides more control to the image harmonization pipeline achieving\nvisually pleasing portrait edits. Furthermore, we also introduce a new dataset\ncarefully curated for validating portrait harmonization. Extensive experiments\non both synthetic and real-world datasets show that the proposed approach is\nefficient and robust compared to previous harmonization baselines, especially\nfor portraits. Project Webpage at\n\\href{https:\/\/jeya-maria-jose.github.io\/IPH-web\/}{https:\/\/jeya-maria-jose.github.io\/IPH-web\/}\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.1436,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"Observation-Centric SORT: Rethinking SORT for Robust Multi-Object\n  Tracking\n\n  Kalman filter (KF) based methods for multi-object tracking (MOT) make an\nassumption that objects move linearly. While this assumption is acceptable for\nvery short periods of occlusion, linear estimates of motion for prolonged time\ncan be highly inaccurate. Moreover, when there is no measurement available to\nupdate Kalman filter parameters, the standard convention is to trust the priori\nstate estimations for posteriori update. This leads to the accumulation of\nerrors during a period of occlusion. The error causes significant motion\ndirection variance in practice. In this work, we show that a basic Kalman\nfilter can still obtain state-of-the-art tracking performance if proper care is\ntaken to fix the noise accumulated during occlusion. Instead of relying only on\nthe linear state estimate (i.e., estimation-centric approach), we use object\nobservations (i.e., the measurements by object detector) to compute a virtual\ntrajectory over the occlusion period to fix the error accumulation of filter\nparameters during the occlusion period. This allows more time steps to correct\nerrors accumulated during occlusion. We name our method Observation-Centric\nSORT (OC-SORT). It remains Simple, Online, and Real-Time but improves\nrobustness during occlusion and non-linear motion. Given off-the-shelf\ndetections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves\nstate-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head\ntracking, and especially DanceTrack where the object motion is highly\nnon-linear. The code and models are available at\n\\url{https:\/\/github.com\/noahcao\/OC_SORT}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.0068,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000046359,
      "text":"CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D\n  Point Cloud Understanding\n\n  Manual annotation of large-scale point cloud dataset for varying tasks such\nas 3D object classification, segmentation and detection is often laborious\nowing to the irregular structure of point clouds. Self-supervised learning,\nwhich operates without any human labeling, is a promising approach to address\nthis issue. We observe in the real world that humans are capable of mapping the\nvisual concepts learnt from 2D images to understand the 3D world. Encouraged by\nthis insight, we propose CrossPoint, a simple cross-modal contrastive learning\napproach to learn transferable 3D point cloud representations. It enables a\n3D-2D correspondence of objects by maximizing agreement between point clouds\nand the corresponding rendered 2D image in the invariant space, while\nencouraging invariance to transformations in the point cloud modality. Our\njoint training objective combines the feature correspondences within and across\nmodalities, thus ensembles a rich learning signal from both 3D point cloud and\n2D image modalities in a self-supervised fashion. Experimental results show\nthat our approach outperforms the previous unsupervised learning methods on a\ndiverse range of downstream tasks including 3D object classification and\nsegmentation. Further, the ablation studies validate the potency of our\napproach for a better point cloud understanding. Code and pretrained models are\navailable at http:\/\/github.com\/MohamedAfham\/CrossPoint.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11173,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000016557,
      "text":"Interpreting Class Conditional GANs with Channel Awareness\n\n  Understanding the mechanism of generative adversarial networks (GANs) helps\nus better use GANs for downstream applications. Existing efforts mainly target\ninterpreting unconditional models, leaving it less explored how a conditional\nGAN learns to render images regarding various categories. This work fills in\nthis gap by investigating how a class conditional generator unifies the\nsynthesis of multiple classes. For this purpose, we dive into the widely used\nclass-conditional batch normalization (CCBN), and observe that each feature\nchannel is activated at varying degrees given different categorical embeddings.\nTo describe such a phenomenon, we propose channel awareness, which\nquantitatively characterizes how a single channel contributes to the final\nsynthesis. Extensive evaluations and analyses on the BigGAN model pre-trained\non ImageNet reveal that only a subset of channels is primarily responsible for\nthe generation of a particular category, similar categories (e.g., cat and dog)\nusually get related to some same channels, and some channels turn out to share\ninformation across all classes. For good measure, our algorithm enables several\nnovel applications with conditional GANs. Concretely, we achieve (1) versatile\nimage editing via simply altering a single channel and manage to (2)\nharmoniously hybridize two different classes. We further verify that the\nproposed channel awareness shows promising potential in (3) segmenting the\nsynthesized image and (4) evaluating the category-wise synthesis performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.16482,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"RFNet-4D++: Joint Object Reconstruction and Flow Estimation from 4D\n  Point Clouds with Cross-Attention Spatio-Temporal Features\n\n  Object reconstruction from 3D point clouds has been a long-standing research\nproblem in computer vision and computer graphics, and achieved impressive\nprogress. However, reconstruction from time-varying point clouds (a.k.a. 4D\npoint clouds) is generally overlooked. In this paper, we propose a new network\narchitecture, namely RFNet-4D++, that jointly reconstructs objects and their\nmotion flows from 4D point clouds. The key insight is simultaneously performing\nboth tasks via learning of spatial and temporal features from a sequence of\npoint clouds can leverage individual tasks, leading to improved overall\nperformance. To prove this ability, we design a temporal vector field learning\nmodule using an unsupervised learning approach for flow estimation task,\nleveraged by supervised learning of spatial structures for object\nreconstruction. Extensive experiments and analyses on benchmark datasets\nvalidated the effectiveness and efficiency of our method. As shown in\nexperimental results, our method achieves state-of-the-art performance on both\nflow estimation and object reconstruction while performing much faster than\nexisting methods in both training and inference. Our code and data are\navailable at https:\/\/github.com\/hkust-vgd\/RFNet-4D\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.14512,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Expressive Talking Head Video Encoding in StyleGAN2 Latent-Space\n\n  While the recent advances in research on video reenactment have yielded\npromising results, the approaches fall short in capturing the fine, detailed,\nand expressive facial features (e.g., lip-pressing, mouth puckering, mouth\ngaping, and wrinkles) which are crucial in generating realistic animated face\nvideos. To this end, we propose an end-to-end expressive face video encoding\napproach that facilitates data-efficient high-quality video re-synthesis by\noptimizing low-dimensional edits of a single Identity-latent. The approach\nbuilds on StyleGAN2 image inversion and multi-stage non-linear latent-space\nediting to generate videos that are nearly comparable to input videos. While\nexisting StyleGAN latent-based editing techniques focus on simply generating\nplausible edits of static images, we automate the latent-space editing to\ncapture the fine expressive facial deformations in a sequence of frames using\nan encoding that resides in the Style-latent-space (StyleSpace) of StyleGAN2.\nThe encoding thus obtained could be super-imposed on a single Identity-latent\nto facilitate re-enactment of face videos at $1024^2$. The proposed framework\neconomically captures face identity, head-pose, and complex expressive facial\nmotions at fine levels, and thereby bypasses training, person modeling,\ndependence on landmarks\/ keypoints, and low-resolution synthesis which tend to\nhamper most re-enactment approaches. The approach is designed with maximum data\nefficiency, where a single $W+$ latent and 35 parameters per frame enable\nhigh-fidelity video rendering. This pipeline can also be used for puppeteering\n(i.e., motion transfer).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.04771,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Multiscale Convolutional Transformer with Center Mask Pretraining for\n  Hyperspectral Image Classification\n\n  Hyperspectral images (HSI) not only have a broad macroscopic field of view\nbut also contain rich spectral information, and the types of surface objects\ncan be identified through spectral information, which is one of the main\napplications in hyperspectral image related research.In recent years, more and\nmore deep learning methods have been proposed, among which convolutional neural\nnetworks (CNN) are the most influential. However, CNN-based methods are\ndifficult to capture long-range dependencies, and also require a large amount\nof labeled data for model training.Besides, most of the self-supervised\ntraining methods in the field of HSI classification are based on the\nreconstruction of input samples, and it is difficult to achieve effective use\nof unlabeled samples. To address the shortcomings of CNN networks, we propose a\nnoval multi-scale convolutional embedding module for HSI to realize effective\nextraction of spatial-spectral information, which can be better combined with\nTransformer network.In order to make more efficient use of unlabeled data, we\npropose a new self-supervised pretask. Similar to Mask autoencoder, but our\npre-training method only masks the corresponding token of the central pixel in\nthe encoder, and inputs the remaining token into the decoder to reconstruct the\nspectral information of the central pixel.Such a pretask can better model the\nrelationship between the central feature and the domain feature, and obtain\nmore stable training results.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00097,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000013676,
      "text":"TransGeo: Transformer Is All You Need for Cross-view Image\n  Geo-localization\n\n  The dominant CNN-based methods for cross-view image geo-localization rely on\npolar transform and fail to model global correlation. We propose a pure\ntransformer-based approach (TransGeo) to address these limitations from a\ndifferent perspective. TransGeo takes full advantage of the strengths of\ntransformer related to global information modeling and explicit position\ninformation encoding. We further leverage the flexibility of transformer input\nand propose an attention-guided non-uniform cropping method, so that\nuninformative image patches are removed with negligible drop on performance to\nreduce computation cost. The saved computation can be reallocated to increase\nresolution only for informative patches, resulting in performance improvement\nwith no additional computation cost. This \"attend and zoom-in\" strategy is\nhighly similar to human behavior when observing images. Remarkably, TransGeo\nachieves state-of-the-art results on both urban and rural datasets, with\nsignificantly less computation cost than CNN-based methods. It does not rely on\npolar transform and infers faster than CNN-based methods. Code is available at\nhttps:\/\/github.com\/Jeff-Zilence\/TransGeo2022.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.05684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000015563,
      "text":"PC-SwinMorph: Patch Representation for Unsupervised Medical Image\n  Registration and Segmentation\n\n  Medical image registration and segmentation are critical tasks for several\nclinical procedures. Manual realisation of those tasks is time-consuming and\nthe quality is highly dependent on the level of expertise of the physician. To\nmitigate that laborious task, automatic tools have been developed where the\nmajority of solutions are supervised techniques. However, in medical domain,\nthe strong assumption of having a well-representative ground truth is far from\nbeing realistic. To overcome this challenge, unsupervised techniques have been\ninvestigated. However, they are still limited in performance and they fail to\nproduce plausible results. In this work, we propose a novel unified\nunsupervised framework for image registration and segmentation that we called\nPC-SwinMorph. The core of our framework is two patch-based strategies, where we\ndemonstrate that patch representation is key for performance gain. We first\nintroduce a patch-based contrastive strategy that enforces locality conditions\nand richer feature representation. Secondly, we utilise a 3D\nwindow\/shifted-window multi-head self-attention module as a patch stitching\nstrategy to eliminate artifacts from the patch splitting. We demonstrate,\nthrough a set of numerical and visual results, that our technique outperforms\ncurrent state-of-the-art unsupervised techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.06551,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000481473,
      "text":"CEKD:Cross Ensemble Knowledge Distillation for Augmented Fine-grained\n  Data\n\n  Data augmentation has been proved effective in training deep models. Existing\ndata augmentation methods tackle the fine-grained problem by blending image\npairs and fusing corresponding labels according to the statistics of mixed\npixels, which produces additional noise harmful to the performance of networks.\nMotivated by this, we present a simple yet effective cross ensemble knowledge\ndistillation (CEKD) model for fine-grained feature learning. We innovatively\npropose a cross distillation module to provide additional supervision to\nalleviate the noise problem, and propose a collaborative ensemble module to\novercome the target conflict problem. The proposed model can be trained in an\nend-to-end manner, and only requires image-level label supervision. Extensive\nexperiments on widely used fine-grained benchmarks demonstrate the\neffectiveness of our proposed model. Specifically, with the backbone of\nResNet-101, CEKD obtains the accuracy of 89.59%, 95.96% and 94.56% in three\ndatasets respectively, outperforming state-of-the-art API-Net by 0.99%, 1.06%\nand 1.16%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.10291,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Exploring Motion Ambiguity and Alignment for High-Quality Video Frame\n  Interpolation\n\n  For video frame interpolation (VFI), existing deep-learning-based approaches\nstrongly rely on the ground-truth (GT) intermediate frames, which sometimes\nignore the non-unique nature of motion judging from the given adjacent frames.\nAs a result, these methods tend to produce averaged solutions that are not\nclear enough. To alleviate this issue, we propose to relax the requirement of\nreconstructing an intermediate frame as close to the GT as possible. Towards\nthis end, we develop a texture consistency loss (TCL) upon the assumption that\nthe interpolated content should maintain similar structures with their\ncounterparts in the given frames. Predictions satisfying this constraint are\nencouraged, though they may differ from the pre-defined GT. Without the bells\nand whistles, our plug-and-play TCL is capable of improving the performance of\nexisting VFI frameworks. On the other hand, previous methods usually adopt the\ncost volume or correlation map to achieve more accurate image\/feature warping.\nHowever, the O(N^2) ({N refers to the pixel count}) computational complexity\nmakes it infeasible for high-resolution cases. In this work, we design a\nsimple, efficient (O(N)) yet powerful cross-scale pyramid alignment (CSPA)\nmodule, where multi-scale information is highly exploited. Extensive\nexperiments justify the efficiency and effectiveness of the proposed strategy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.15143,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000116891,
      "text":"Towards End-to-End Unified Scene Text Detection and Layout Analysis\n\n  Scene text detection and document layout analysis have long been treated as\ntwo separate tasks in different image domains. In this paper, we bring them\ntogether and introduce the task of unified scene text detection and layout\nanalysis. The first hierarchical scene text dataset is introduced to enable\nthis novel research task. We also propose a novel method that is able to\nsimultaneously detect scene text and form text clusters in a unified way.\nComprehensive experiments show that our unified model achieves better\nperformance than multiple well-designed baseline methods. Additionally, this\nmodel achieves state-of-the-art results on multiple scene text detection\ndatasets without the need of complex post-processing. Dataset and code:\nhttps:\/\/github.com\/google-research-datasets\/hiertext and\nhttps:\/\/github.com\/tensorflow\/models\/tree\/master\/official\/projects\/unified_detector.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11506,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000149674,
      "text":"Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition\n\n  Deep neural networks perform poorly on heavily class-imbalanced datasets.\nGiven the promising performance of contrastive learning, we propose Rebalanced\nSiamese Contrastive Mining (ResCom) to tackle imbalanced recognition. Based on\nthe mathematical analysis and simulation results, we claim that supervised\ncontrastive learning suffers a dual class-imbalance problem at both the\noriginal batch and Siamese batch levels, which is more serious than long-tailed\nclassification learning. In this paper, at the original batch level, we\nintroduce a class-balanced supervised contrastive loss to assign adaptive\nweights for different classes. At the Siamese batch level, we present a\nclass-balanced queue, which maintains the same number of keys for all classes.\nFurthermore, we note that the imbalanced contrastive loss gradient with respect\nto the contrastive logits can be decoupled into the positives and negatives,\nand easy positives and easy negatives will make the contrastive gradient\nvanish. We propose supervised hard positive and negative pairs mining to pick\nup informative pairs for contrastive computation and improve representation\nlearning. Finally, to approximately maximize the mutual information between the\ntwo views, we propose Siamese Balanced Softmax and joint it with the\ncontrastive loss for one-stage training. Extensive experiments demonstrate that\nResCom outperforms the previous methods by large margins on multiple\nlong-tailed recognition benchmarks. Our code and models are made publicly\navailable at: https:\/\/github.com\/dvlab-research\/ResCom.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.10785,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.000031756,
      "text":"GroupTransNet: Group Transformer Network for RGB-D Salient Object\n  Detection\n\n  Salient object detection on RGB-D images is an active topic in computer\nvision. Although the existing methods have achieved appreciable performance,\nthere are still some challenges. The locality of convolutional neural network\nrequires that the model has a sufficiently deep global receptive field, which\nalways leads to the loss of local details. To address the challenge, we propose\na novel Group Transformer Network (GroupTransNet) for RGB-D salient object\ndetection. This method is good at learning the long-range dependencies of cross\nlayer features to promote more perfect feature expression. At the beginning,\nthe features of the slightly higher classes of the middle three levels and the\nlatter three levels are soft grouped to absorb the advantages of the high-level\nfeatures. The input features are repeatedly purified and enhanced by the\nattention mechanism to purify the cross modal features of color modal and depth\nmodal. The features of the intermediate process are first fused by the features\nof different layers, and then processed by several transformers in multiple\ngroups, which not only makes the size of the features of each scale unified and\ninterrelated, but also achieves the effect of sharing the weight of the\nfeatures within the group. The output features in different groups complete the\nclustering staggered by two owing to the level difference, and combine with the\nlow-level features. Extensive experiments demonstrate that GroupTransNet\noutperforms the comparison models and achieves the new state-of-the-art\nperformance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.17219,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"SimVQA: Exploring Simulated Environments for Visual Question Answering\n\n  Existing work on VQA explores data augmentation to achieve better\ngeneralization by perturbing the images in the dataset or modifying the\nexisting questions and answers. While these methods exhibit good performance,\nthe diversity of the questions and answers are constrained by the available\nimage set. In this work we explore using synthetic computer-generated data to\nfully control the visual and language space, allowing us to provide more\ndiverse scenarios. We quantify the effect of synthetic data in real-world VQA\nbenchmarks and to which extent it produces results that generalize to real\ndata. By exploiting 3D and physics simulation platforms, we provide a pipeline\nto generate synthetic data to expand and replace type-specific questions and\nanswers without risking the exposure of sensitive or personal data that might\nbe present in real images. We offer a comprehensive analysis while expanding\nexisting hyper-realistic datasets to be used for VQA. We also propose Feature\nSwapping (F-SWAP) -- where we randomly switch object-level features during\ntraining to make a VQA model more domain invariant. We show that F-SWAP is\neffective for enhancing a currently existing VQA dataset of real images without\ncompromising on the accuracy to answer existing questions in the dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.00955,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"GRASP EARTH: Intuitive Software for Discovering Changes on the Planet\n\n  Detecting changes on the Earth, such as urban development, deforestation, or\nnatural disaster, is one of the research fields that is attracting a great deal\nof attention. One promising tool to solve these problems is satellite imagery.\nHowever, satellite images require huge amount of storage, therefore users are\nrequired to set Area of Interests first, which was not suitable for detecting\npotential areas for disaster or development. To tackle with this problem, we\ndevelop the novel tool, namely GRASP EARTH, which is the simple change\ndetection application based on Google Earth Engine. GRASP EARTH allows us to\nhandle satellite imagery easily and it has used for disaster monitoring and\nurban development monitoring.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.11405,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000067883,
      "text":"Hindsight is 20\/20: Leveraging Past Traversals to Aid 3D Perception\n\n  Self-driving cars must detect vehicles, pedestrians, and other traffic\nparticipants accurately to operate safely. Small, far-away, or highly occluded\nobjects are particularly challenging because there is limited information in\nthe LiDAR point clouds for detecting them. To address this challenge, we\nleverage valuable information from the past: in particular, data collected in\npast traversals of the same scene. We posit that these past data, which are\ntypically discarded, provide rich contextual information for disambiguating the\nabove-mentioned challenging cases. To this end, we propose a novel, end-to-end\ntrainable Hindsight framework to extract this contextual information from past\ntraversals and store it in an easy-to-query data structure, which can then be\nleveraged to aid future 3D object detection of the same scene. We show that\nthis framework is compatible with most modern 3D detection architectures and\ncan substantially improve their average precision on multiple autonomous\ndriving datasets, most notably by more than 300% on the challenging cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.1494,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0000054306,
      "text":"Learning to Prompt for Open-Vocabulary Object Detection with\n  Vision-Language Model\n\n  Recently, vision-language pre-training shows great potential in\nopen-vocabulary object detection, where detectors trained on base classes are\ndevised for detecting new classes. The class text embedding is firstly\ngenerated by feeding prompts to the text encoder of a pre-trained\nvision-language model. It is then used as the region classifier to supervise\nthe training of a detector. The key element that leads to the success of this\nmodel is the proper prompt, which requires careful words tuning and ingenious\ndesign. To avoid laborious prompt engineering, there are some prompt\nrepresentation learning methods being proposed for the image classification\ntask, which however can only be sub-optimal solutions when applied to the\ndetection task. In this paper, we introduce a novel method, detection prompt\n(DetPro), to learn continuous prompt representations for open-vocabulary object\ndetection based on the pre-trained vision-language model. Different from the\nprevious classification-oriented methods, DetPro has two highlights: 1) a\nbackground interpretation scheme to include the proposals in image background\ninto the prompt training; 2) a context grading scheme to separate proposals in\nimage foreground for tailored prompt training. We assemble DetPro with ViLD, a\nrecent state-of-the-art open-world object detector, and conduct experiments on\nthe LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365\ndatasets. Experimental results show that our DetPro outperforms the baseline\nViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the\nnovel classes of LVIS. Code and models are available at\nhttps:\/\/github.com\/dyabel\/detpro.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2203.0086,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":3,
    "pangram_prediction":{
      "ai_likelihood":0.0001037121,
      "text":"D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale\n  Attention\n\n  DETR is the first fully end-to-end detector that predicts a final set of\npredictions without post-processing. However, it suffers from problems such as\nlow performance and slow convergence. A series of works aim to tackle these\nissues in different ways, but the computational cost is yet expensive due to\nthe sophisticated encoder-decoder architecture. To alleviate this issue, we\npropose a decoder-only detector called D^2ETR. In the absence of encoder, the\ndecoder directly attends to the fine-fused feature maps generated by the\nTransformer backbone with a novel computationally efficient cross-scale\nattention module. D^2ETR demonstrates low computational complexity and high\ndetection accuracy in evaluations on the COCO benchmark, outperforming DETR and\nits variants.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12696,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"Grasping the Arrow of Time from the Singularity: Decoding Micromotion in\n  Low-dimensional Latent Spaces from StyleGAN\n\n  The disentanglement of StyleGAN latent space has paved the way for realistic\nand controllable image editing, but does StyleGAN know anything about temporal\nmotion, as it was only trained on static images? To study the motion features\nin the latent space of StyleGAN, in this paper, we hypothesize and demonstrate\nthat a series of meaningful, natural, and versatile small, local movements\n(referred to as \"micromotion\", such as expression, head movement, and aging\neffect) can be represented in low-rank spaces extracted from the latent space\nof a conventionally pre-trained StyleGAN-v2 model for face generation, with the\nguidance of proper \"anchors\" in the form of either short text or video clips.\nStarting from one target face image, with the editing direction decoded from\nthe low-rank space, its micromotion features can be represented as simple as an\naffine transformation over its latent feature. Perhaps more surprisingly, such\nmicromotion subspace, even learned from just single target face, can be\npainlessly transferred to other unseen face images, even those from vastly\ndifferent domains (such as oil painting, cartoon, and sculpture faces). It\ndemonstrates that the local feature geometry corresponding to one type of\nmicromotion is aligned across different face subjects, and hence that\nStyleGAN-v2 is indeed \"secretly\" aware of the subject-disentangled feature\nvariations caused by that micromotion. We present various successful examples\nof applying our low-dimensional micromotion subspace technique to directly and\neffortlessly manipulate faces, showing high robustness, low computational\noverhead, and impressive domain transferability. Our codes are available at\nhttps:\/\/github.com\/wuqiuche\/micromotion-StyleGAN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.09774,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Attention in Reasoning: Dataset, Analysis, and Modeling\n\n  While attention has been an increasingly popular component in deep neural\nnetworks to both interpret and boost the performance of models, little work has\nexamined how attention progresses to accomplish a task and whether it is\nreasonable. In this work, we propose an Attention with Reasoning capability\n(AiR) framework that uses attention to understand and improve the process\nleading to task outcomes. We first define an evaluation metric based on a\nsequence of atomic reasoning operations, enabling a quantitative measurement of\nattention that considers the reasoning process. We then collect human\neye-tracking and answer correctness data, and analyze various machine and human\nattention mechanisms on their reasoning capability and how they impact task\nperformance. To improve the attention and reasoning ability of visual question\nanswering models, we propose to supervise the learning of attention\nprogressively along the reasoning process and to differentiate the correct and\nincorrect attention patterns. We demonstrate the effectiveness of the proposed\nframework in analyzing and modeling attention with better reasoning capability\nand task performance. The code and data are available at\nhttps:\/\/github.com\/szzexpoi\/AiR\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.01267,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000009603,
      "text":"FoV-Net: Field-of-View Extrapolation Using Self-Attention and\n  Uncertainty\n\n  The ability to make educated predictions about their surroundings, and\nassociate them with certain confidence, is important for intelligent systems,\nlike autonomous vehicles and robots. It allows them to plan early and decide\naccordingly. Motivated by this observation, in this paper we utilize\ninformation from a video sequence with a narrow field-of-view to infer the\nscene at a wider field-of-view. To this end, we propose a temporally consistent\nfield-of-view extrapolation framework, namely FoV-Net, that: (1) leverages 3D\ninformation to propagate the observed scene parts from past frames; (2)\naggregates the propagated multi-frame information using an attention-based\nfeature aggregation module and a gated self-attention module, simultaneously\nhallucinating any unobserved scene parts; and (3) assigns an interpretable\nuncertainty value at each pixel. Extensive experiments show that FoV-Net does\nnot only extrapolate the temporally consistent wide field-of-view scene better\nthan existing alternatives, but also provides the associated uncertainty which\nmay benefit critical decision-making downstream applications. Project page is\nat http:\/\/charliememory.github.io\/RAL21_FoV.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.03057,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"Thermal to Visible Image Synthesis under Atmospheric Turbulence\n\n  In many practical applications of long-range imaging such as biometrics and\nsurveillance, thermal imagining modalities are often used to capture images in\nlow-light and nighttime conditions. However, such imaging systems often suffer\nfrom atmospheric turbulence, which introduces severe blur and deformation\nartifacts to the captured images. Such an issue is unavoidable in long-range\nimaging and significantly decreases the face verification accuracy. In this\npaper, we first investigate the problem with a turbulence simulation method on\nreal-world thermal images. An end-to-end reconstruction method is then proposed\nwhich can directly transform thermal images into visible-spectrum images by\nutilizing natural image priors based on a pre-trained StyleGAN2 network.\nCompared with the existing two-steps methods of consecutive turbulence\nmitigation and thermal to visible image translation, our method is demonstrated\nto be effective in terms of both the visual quality of the reconstructed\nresults and face verification accuracy. Moreover, to the best of our knowledge,\nthis is the first work that studies the problem of thermal to visible image\ntranslation under atmospheric turbulence.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07311,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000051988,
      "text":"MetaSets: Meta-Learning on Point Sets for Generalizable Representations\n\n  Deep learning techniques for point clouds have achieved strong performance on\na range of 3D vision tasks. However, it is costly to annotate large-scale point\nsets, making it critical to learn generalizable representations that can\ntransfer well across different point sets. In this paper, we study a new\nproblem of 3D Domain Generalization (3DDG) with the goal to generalize the\nmodel to other unseen domains of point clouds without any access to them in the\ntraining process. It is a challenging problem due to the substantial geometry\nshift from simulated to real data, such that most existing 3D models\nunderperform due to overfitting the complete geometries in the source domain.\nWe propose to tackle this problem via MetaSets, which meta-learns point cloud\nrepresentations from a group of classification tasks on carefully-designed\ntransformed point sets containing specific geometry priors. The learned\nrepresentations are more generalizable to various unseen domains of different\ngeometries. We design two benchmarks for Sim-to-Real transfer of 3D point\nclouds. Experimental results show that MetaSets outperforms existing 3D deep\nlearning methods by large margins.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06837,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000021524,
      "text":"Modeling Indirect Illumination for Inverse Rendering\n\n  Recent advances in implicit neural representations and differentiable\nrendering make it possible to simultaneously recover the geometry and materials\nof an object from multi-view RGB images captured under unknown static\nillumination. Despite the promising results achieved, indirect illumination is\nrarely modeled in previous methods, as it requires expensive recursive path\ntracing which makes the inverse rendering computationally intractable. In this\npaper, we propose a novel approach to efficiently recovering spatially-varying\nindirect illumination. The key insight is that indirect illumination can be\nconveniently derived from the neural radiance field learned from input images\ninstead of being estimated jointly with direct illumination and materials. By\nproperly modeling the indirect illumination and visibility of direct\nillumination, interreflection- and shadow-free albedo can be recovered. The\nexperiments on both synthetic and real data demonstrate the superior\nperformance of our approach compared to previous work and its capability to\nsynthesize realistic renderings under novel viewpoints and illumination. Our\ncode and data are available at https:\/\/zju3dv.github.io\/invrender\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.01931,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000003311,
      "text":"High-Quality Pluralistic Image Completion via Code Shared VQGAN\n\n  PICNet pioneered the generation of multiple and diverse results for image\ncompletion task, but it required a careful balance between $\\mathcal{KL}$ loss\n(diversity) and reconstruction loss (quality), resulting in a limited diversity\nand quality . Separately, iGPT-based architecture has been employed to infer\ndistributions in a discrete space derived from a pixel-level pre-clustered\npalette, which however cannot generate high-quality results directly. In this\nwork, we present a novel framework for pluralistic image completion that can\nachieve both high quality and diversity at much faster inference speed. The\ncore of our design lies in a simple yet effective code sharing mechanism that\nleads to a very compact yet expressive image representation in a discrete\nlatent domain. The compactness and the richness of the representation further\nfacilitate the subsequent deployment of a transformer to effectively learn how\nto composite and complete a masked image at the discrete code domain. Based on\nthe global context well-captured by the transformer and the available visual\nregions, we are able to sample all tokens simultaneously, which is completely\ndifferent from the prevailing autoregressive approach of iGPT-based works, and\nleads to more than 100$\\times$ faster inference speed. Experiments show that\nour framework is able to learn semantically-rich discrete codes efficiently and\nrobustly, resulting in much better image reconstruction quality. Our diverse\nimage completion framework significantly outperforms the state-of-the-art both\nquantitatively and qualitatively on multiple benchmark datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07935,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"Causal Intervention for Subject-Deconfounded Facial Action Unit\n  Recognition\n\n  Subject-invariant facial action unit (AU) recognition remains challenging for\nthe reason that the data distribution varies among subjects. In this paper, we\npropose a causal inference framework for subject-invariant facial action unit\nrecognition. To illustrate the causal effect existing in AU recognition task,\nwe formulate the causalities among facial images, subjects, latent AU semantic\nrelations, and estimated AU occurrence probabilities via a structural causal\nmodel. By constructing such a causal diagram, we clarify the causal effect\namong variables and propose a plug-in causal intervention module, CIS, to\ndeconfound the confounder \\emph{Subject} in the causal diagram. Extensive\nexperiments conducted on two commonly used AU benchmark datasets, BP4D and\nDISFA, show the effectiveness of our CIS, and the model with CIS inserted,\nCISNet, has achieved state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00379,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000064903,
      "text":"Weakly Supervised Regional and Temporal Learning for Facial Action Unit\n  Recognition\n\n  Automatic facial action unit (AU) recognition is a challenging task due to\nthe scarcity of manual annotations. To alleviate this problem, a large amount\nof efforts has been dedicated to exploiting various weakly supervised methods\nwhich leverage numerous unlabeled data. However, many aspects with regard to\nsome unique properties of AUs, such as the regional and relational\ncharacteristics, are not sufficiently explored in previous works. Motivated by\nthis, we take the AU properties into consideration and propose two auxiliary AU\nrelated tasks to bridge the gap between limited annotations and the model\nperformance in a self-supervised manner via the unlabeled data. Specifically,\nto enhance the discrimination of regional features with AU relation embedding,\nwe design a task of RoI inpainting to recover the randomly cropped AU patches.\nMeanwhile, a single image based optical flow estimation task is proposed to\nleverage the dynamic change of facial muscles and encode the motion information\ninto the global feature representation. Based on these two self-supervised\nauxiliary tasks, local features, mutual relation and motion cues of AUs are\nbetter captured in the backbone network. Furthermore, by incorporating\nsemi-supervised learning, we propose an end-to-end trainable framework named\nweakly supervised regional and temporal learning (WSRTL) for AU recognition.\nExtensive experiments on BP4D and DISFA demonstrate the superiority of our\nmethod and new state-of-the-art performances are achieved.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.04715,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000004371,
      "text":"Image Harmonization by Matching Regional References\n\n  To achieve visual consistency in composite images, recent image harmonization\nmethods typically summarize the appearance pattern of global background and\napply it to the global foreground without location discrepancy. However, for a\nreal image, the appearances (illumination, color temperature, saturation, hue,\ntexture, etc) of different regions can vary significantly. So previous methods,\nwhich transfer the appearance globally, are not optimal. Trying to solve this\nissue, we firstly match the contents between the foreground and background and\nthen adaptively adjust every foreground location according to the appearance of\nits content-related background regions. Further, we design a residual\nreconstruction strategy, that uses the predicted residual to adjust the\nappearance, and the composite foreground to reserve the image details.\nExtensive experiments demonstrate the effectiveness of our method. The source\ncode will be available publicly.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.05503,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000165237,
      "text":"FSOINet: Feature-Space Optimization-Inspired Network for Image\n  Compressive Sensing\n\n  In recent years, deep learning-based image compressive sensing (ICS) methods\nhave achieved brilliant success. Many optimization-inspired networks have been\nproposed to bring the insights of optimization algorithms into the network\nstructure design and have achieved excellent reconstruction quality with low\ncomputational complexity. But they keep the information flow in pixel space as\ntraditional algorithms by updating and transferring the image in pixel space,\nwhich does not fully use the information in the image features. In this paper,\nwe propose the idea of achieving information flow phase by phase in feature\nspace and design a Feature-Space Optimization-Inspired Network (dubbed FSOINet)\nto implement it by mapping both steps of proximal gradient descent algorithm\nfrom pixel space to feature space. Moreover, the sampling matrix is learned\nend-to-end with other network parameters. Experiments show that the proposed\nFSOINet outperforms the existing state-of-the-art methods by a large margin\nboth quantitatively and qualitatively. The source code is available on\nhttps:\/\/github.com\/cwjjun\/FSOINet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.11582,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000017186,
      "text":"Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object\n  Detection\n\n  3D object detection from multiple image views is a fundamental and\nchallenging task for visual scene understanding. Due to its low cost and high\nefficiency, multi-view 3D object detection has demonstrated promising\napplication prospects. However, accurately detecting objects through\nperspective views in the 3D space is extremely difficult due to the lack of\ndepth information. Recently, DETR3D introduces a novel 3D-2D query paradigm in\naggregating multi-view images for 3D object detection and achieves\nstate-of-the-art performance. In this paper, with intensive pilot experiments,\nwe quantify the objects located at different regions and find that the\n\"truncated instances\" (i.e., at the border regions of each image) are the main\nbottleneck hindering the performance of DETR3D. Although it merges multiple\nfeatures from two adjacent views in the overlapping regions, DETR3D still\nsuffers from insufficient feature aggregation, thus missing the chance to fully\nboost the detection performance. In an effort to tackle the problem, we propose\nGraph-DETR3D to automatically aggregate multi-view imagery information through\ngraph structure learning (GSL). It constructs a dynamic 3D graph between each\nobject query and 2D feature maps to enhance the object representations,\nespecially at the border regions. Besides, Graph-DETR3D benefits from a novel\ndepth-invariant multi-scale training strategy, which maintains the visual depth\nconsistency by simultaneously scaling the image size and the object depth.\nExtensive experiments on the nuScenes dataset demonstrate the effectiveness and\nefficiency of our Graph-DETR3D. Notably, our best model achieves 49.5 NDS on\nthe nuScenes test leaderboard, achieving new state-of-the-art in comparison\nwith various published image-view 3D object detectors.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.13001,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Relevance-based Margin for Contrastively-trained Video Retrieval Models\n\n  Video retrieval using natural language queries has attracted increasing\ninterest due to its relevance in real-world applications, from intelligent\naccess in private media galleries to web-scale video search. Learning the\ncross-similarity of video and text in a joint embedding space is the dominant\napproach. To do so, a contrastive loss is usually employed because it organizes\nthe embedding space by putting similar items close and dissimilar items far.\nThis framework leads to competitive recall rates, as they solely focus on the\nrank of the groundtruth items. Yet, assessing the quality of the ranking list\nis of utmost importance when considering intelligent retrieval systems, since\nmultiple items may share similar semantics, hence a high relevance. Moreover,\nthe aforementioned framework uses a fixed margin to separate similar and\ndissimilar items, treating all non-groundtruth items as equally irrelevant. In\nthis paper we propose to use a variable margin: we argue that varying the\nmargin used during training based on how much relevant an item is to a given\nquery, i.e. a relevance-based margin, easily improves the quality of the\nranking lists measured through nDCG and mAP. We demonstrate the advantages of\nour technique using different models on EPIC-Kitchens-100 and YouCook2. We show\nthat even if we carefully tuned the fixed margin, our technique (which does not\nhave the margin as a hyper-parameter) would still achieve better performance.\nFinally, extensive ablation studies and qualitative analysis support the\nrobustness of our approach. Code will be released at\n\\url{https:\/\/github.com\/aranciokov\/RelevanceMargin-ICMR22}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.07118,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"DeiT III: Revenge of the ViT\n\n  A Vision Transformer (ViT) is a simple neural architecture amenable to serve\nseveral computer vision tasks. It has limited built-in architectural priors, in\ncontrast to more recent architectures that incorporate priors either about the\ninput data or of specific tasks. Recent works show that ViTs benefit from\nself-supervised pre-training, in particular BerT-like pre-training like BeiT.\nIn this paper, we revisit the supervised training of ViTs. Our procedure builds\nupon and simplifies a recipe introduced for training ResNet-50. It includes a\nnew simple data-augmentation procedure with only 3 augmentations, closer to the\npractice in self-supervised learning. Our evaluations on Image classification\n(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning\nand semantic segmentation show that our procedure outperforms by a large margin\nprevious fully supervised training recipes for ViT. It also reveals that the\nperformance of our ViT trained with supervision is comparable to that of more\nrecent architectures. Our results could serve as better baselines for recent\nself-supervised approaches demonstrated on ViT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00795,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000011259,
      "text":"Unsupervised Coherent Video Cartoonization with Perceptual Motion\n  Consistency\n\n  In recent years, creative content generations like style transfer and neural\nphoto editing have attracted more and more attention. Among these,\ncartoonization of real-world scenes has promising applications in entertainment\nand industry. Different from image translations focusing on improving the style\neffect of generated images, video cartoonization has additional requirements on\nthe temporal consistency. In this paper, we propose a spatially-adaptive\nsemantic alignment framework with perceptual motion consistency for coherent\nvideo cartoonization in an unsupervised manner. The semantic alignment module\nis designed to restore deformation of semantic structure caused by spatial\ninformation lost in the encoder-decoder architecture. Furthermore, we devise\nthe spatio-temporal correlative map as a style-independent, global-aware\nregularization on the perceptual motion consistency. Deriving from similarity\nmeasurement of high-level features in photo and cartoon frames, it captures\nglobal semantic information beyond raw pixel-value in optical flow. Besides,\nthe similarity measurement disentangles temporal relationships from\ndomain-specific style properties, which helps regularize the temporal\nconsistency without hurting style effects of cartoon images. Qualitative and\nquantitative experiments demonstrate our method is able to generate highly\nstylistic and temporal consistent cartoon videos.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.06275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"Assessing cloudiness in nonwovens\n\n  The homogeneity of filter media is important for material selection and\nquality control, along with the specific weight (nominal grammage) and the\ndistribution of the local weight. Cloudiness or formation is a concept used to\ndescribe deviations from homogeneity in filter media. We suggest to derive the\ncloudiness index from the power spectrum of the relative local areal weight,\nintegrated over a selected frequency range. The power spectrum captures the\nenergy density in a broad spectral range. Moreover, under certain conditions,\nthe structure of a nonwoven is fully characterized by the areal weight, the\nvariance of the local areal weight, and the power spectrum. Consequently, the\npower spectrum is the parameter that exclusively reflects the cloudiness. Here,\nwe address questions arising from practical application. The most prominent is\nthe choice of the spectral band. It certainly depends on the characteristic\n\"size of the clouds\", but is limited by the size and lateral resolution of the\nimages. We show that the cloudiness index based on the power spectrum of the\nrelative local areal weight is theoretically well founded and can be robustly\nmeasured from image data. Choosing the spectral band allows to capture the\ncloudiness either visually perceived or found to be decisive for product\nproperties. It is thus well suited to build a technical standard on it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.10606,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000085102,
      "text":"Enhancing the Transferability via Feature-Momentum Adversarial Attack\n\n  Transferable adversarial attack has drawn increasing attention due to their\npractical threaten to real-world applications. In particular, the feature-level\nadversarial attack is one recent branch that can enhance the transferability\nvia disturbing the intermediate features. The existing methods usually create a\nguidance map for features, where the value indicates the importance of the\ncorresponding feature element and then employs an iterative algorithm to\ndisrupt the features accordingly. However, the guidance map is fixed in\nexisting methods, which can not consistently reflect the behavior of networks\nas the image is changed during iteration. In this paper, we describe a new\nmethod called Feature-Momentum Adversarial Attack (FMAA) to further improve\ntransferability. The key idea of our method is that we estimate a guidance map\ndynamically at each iteration using momentum to effectively disturb the\ncategory-relevant features. Extensive experiments demonstrate that our method\nsignificantly outperforms other state-of-the-art methods by a large margin on\ndifferent target models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.00951,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"A Sentinel-2 multi-year, multi-country benchmark dataset for crop\n  classification and segmentation with deep learning\n\n  In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi\ncountry benchmark dataset, tailored for agricultural monitoring applications\nwith Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer\ndeclarations collected via the Land Parcel Identification System (LPIS) for\nharmonizing country wide labels. These declarations have only recently been\nmade available as open data, allowing for the first time the labeling of\nsatellite imagery from ground truth data. We proceed to propose and standardise\na new crop type taxonomy across Europe that address Common Agriculture Policy\n(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative\nCrop Classification scheme. Sen4AgriNet is the only multi-country, multi-year\ndataset that includes all spectral information. It is constructed to cover the\nperiod 2016-2020 for Catalonia and France, while it can be extended to include\nadditional countries. Currently, it contains 42.5 million parcels, which makes\nit significantly larger than other available archives. We extract two\nsub-datasets to highlight its value for diverse Deep Learning applications; the\nObject Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD\ncapitalizes zonal statistics of each parcel, thus creating a powerful\nlabel-to-features instance for classification algorithms. On the other hand,\nPAD structure generalizes the classification problem to parcel extraction and\nsemantic segmentation and labeling. The PAD and OAD are examined under three\ndifferent scenarios to showcase and model the effects of spatial and temporal\nvariability across different years and different countries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.1183,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.0000082453,
      "text":"Proto2Proto: Can you recognize the car, the way I do?\n\n  Prototypical methods have recently gained a lot of attention due to their\nintrinsic interpretable nature, which is obtained through the prototypes. With\ngrowing use cases of model reuse and distillation, there is a need to also\nstudy transfer of interpretability from one model to another. We present\nProto2Proto, a novel method to transfer interpretability of one prototypical\npart network to another via knowledge distillation. Our approach aims to add\ninterpretability to the \"dark\" knowledge transferred from the teacher to the\nshallower student model. We propose two novel losses: \"Global Explanation\" loss\nand \"Patch-Prototype Correspondence\" loss to facilitate such a transfer. Global\nExplanation loss forces the student prototypes to be close to teacher\nprototypes, and Patch-Prototype Correspondence loss enforces the local\nrepresentations of the student to be similar to that of the teacher. Further,\nwe propose three novel metrics to evaluate the student's proximity to the\nteacher as measures of interpretability transfer in our settings. We\nqualitatively and quantitatively demonstrate the effectiveness of our method on\nCUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed\nmethod indeed achieves interpretability transfer from teacher to student while\nsimultaneously exhibiting competitive performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2204.12367,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":4,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"ROMA: Cross-Domain Region Similarity Matching for Unpaired Nighttime\n  Infrared to Daytime Visible Video Translation\n\n  Infrared cameras are often utilized to enhance the night vision since the\nvisible light cameras exhibit inferior efficacy without sufficient\nillumination. However, infrared data possesses inadequate color contrast and\nrepresentation ability attributed to its intrinsic heat-related imaging\nprinciple. This makes it arduous to capture and analyze information for human\nbeings, meanwhile hindering its application. Although, the domain gaps between\nunpaired nighttime infrared and daytime visible videos are even huger than\npaired ones that captured at the same time, establishing an effective\ntranslation mapping will greatly contribute to various fields. In this case,\nthe structural knowledge within nighttime infrared videos and semantic\ninformation contained in the translated daytime visible pairs could be utilized\nsimultaneously. To this end, we propose a tailored framework ROMA that couples\nwith our introduced cRoss-domain regiOn siMilarity mAtching technique for\nbridging the huge gaps. To be specific, ROMA could efficiently translate the\nunpaired nighttime infrared videos into fine-grained daytime visible ones,\nmeanwhile maintain the spatiotemporal consistency via matching the cross-domain\nregion similarity. Furthermore, we design a multiscale region-wise\ndiscriminator to distinguish the details from synthesized visible results and\nreal references. Extensive experiments and evaluations for specific\napplications indicate ROMA outperforms the state-of-the-art methods. Moreover,\nwe provide a new and challenging dataset encouraging further research for\nunpaired nighttime infrared and daytime visible video translation, named\nInfraredCity. In particular, it consists of 9 long video clips including City,\nHighway and Monitor scenarios. All clips could be split into 603,142 frames in\ntotal, which are 20 times larger than the recently released daytime\ninfrared-to-visible dataset IRVI.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0623,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000056624,
      "text":"Simple Open-Vocabulary Object Detection with Vision Transformers\n\n  Combining simple architectures with large-scale pre-training has led to\nmassive improvements in image classification. For object detection,\npre-training and scaling approaches are less well established, especially in\nthe long-tailed and open-vocabulary setting, where training data is relatively\nscarce. In this paper, we propose a strong recipe for transferring image-text\nmodels to open-vocabulary object detection. We use a standard Vision\nTransformer architecture with minimal modifications, contrastive image-text\npre-training, and end-to-end detection fine-tuning. Our analysis of the scaling\nproperties of this setup shows that increasing image-level pre-training and\nmodel size yield consistent improvements on the downstream detection task. We\nprovide the adaptation strategies and regularizations needed to attain very\nstrong performance on zero-shot text-conditioned and one-shot image-conditioned\nobject detection. Code and models are available on GitHub.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.08094,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"MATrIX -- Modality-Aware Transformer for Information eXtraction\n\n  We present MATrIX - a Modality-Aware Transformer for Information eXtraction\nin the Visual Document Understanding (VDU) domain. VDU covers information\nextraction from visually rich documents such as forms, invoices, receipts,\ntables, graphs, presentations, or advertisements. In these, text semantics and\nvisual information supplement each other to provide a global understanding of\nthe document. MATrIX is pre-trained in an unsupervised way with specifically\ndesigned tasks that require the use of multi-modal information (spatial,\nvisual, or textual). We consider the spatial and text modalities all at once in\na single token set. To make the attention more flexible, we use a learned\nmodality-aware relative bias in the attention mechanism to modulate the\nattention between the tokens of different modalities. We evaluate MATrIX on 3\ndifferent datasets each with strong baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14118,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000328819,
      "text":"Efficient textual explanations for complex road and traffic scenarios\n  based on semantic segmentation\n\n  The complex driving environment brings great challenges to the visual\nperception of autonomous vehicles. It's essential to extract clear and\nexplainable information from the complex road and traffic scenarios and offer\nclues to decision and control. However, the previous scene explanation had been\nimplemented as a separate model. The black box model makes it difficult to\ninterpret the driving environment. It cannot detect comprehensive textual\ninformation and requires a high computational load and time consumption. Thus,\nthis study proposed a comprehensive and efficient textual explanation model.\nFrom 336k video frames of the driving environment, critical images of complex\nroad and traffic scenarios were selected into a dataset. Through transfer\nlearning, this study established an accurate and efficient segmentation model\nto obtain the critical traffic elements in the environment. Based on the\nXGBoost algorithm, a comprehensive model was developed. The model provided\ntextual information about states of traffic elements, the motion of conflict\nobjects, and scenario complexity. The approach was verified on the real-world\nroad. It improved the perception accuracy of critical traffic elements to\n78.8%. The time consumption reached 13 minutes for each epoch, which was 11.5\ntimes more efficient than the pre-trained network. The textual information\nanalyzed from the model was also accordant with reality. The findings offer\nclear and explainable information about the complex driving environment, which\nlays a foundation for subsequent decision and control. It can improve the\nvisual perception ability and enrich the prior knowledge and judgments of\ncomplex traffic situations.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.00092,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000030465,
      "text":"FHIST: A Benchmark for Few-shot Classification of Histological Images\n\n  Few-shot learning has recently attracted wide interest in image\nclassification, but almost all the current public benchmarks are focused on\nnatural images. The few-shot paradigm is highly relevant in medical-imaging\napplications due to the scarcity of labeled data, as annotations are expensive\nand require specialized expertise. However, in medical imaging, few-shot\nlearning research is sparse, limited to private data sets and is at its early\nstage. In particular, the few-shot setting is of high interest in histology due\nto the diversity and fine granularity of cancer related tissue classification\ntasks, and the variety of data-preparation techniques. This paper introduces a\nhighly diversified public benchmark, gathered from various public datasets, for\nfew-shot histology data classification. We build few-shot tasks and\nbase-training data with various tissue types, different levels of domain shifts\nstemming from various cancer sites, and different class-granularity levels,\nthereby reflecting realistic scenarios. We evaluate the performances of\nstate-of-the-art few-shot learning methods on our benchmark, and observe that\nsimple fine-tuning and regularization methods achieve better results than the\npopular meta-learning and episodic-training paradigm. Furthermore, we introduce\nthree scenarios based on the domain shifts between the source and target\nhistology data: near-domain, middle-domain and out-domain. Our experiments\ndisplay the potential of few-shot learning in histology classification, with\nstate-of-art few shot learning methods approaching the supervised-learning\nbaselines in the near-domain setting. In our out-domain setting, for 5-way\n5-shot, the best performing method reaches 60% accuracy. We believe that our\nwork could help in building realistic evaluations and fair comparisons of\nfew-shot learning methods and will further encourage research in the few-shot\nparadigm.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14319,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000115567,
      "text":"WT-MVSNet: Window-based Transformers for Multi-view Stereo\n\n  Recently, Transformers were shown to enhance the performance of multi-view\nstereo by enabling long-range feature interaction. In this work, we propose\nWindow-based Transformers (WT) for local feature matching and global feature\naggregation in multi-view stereo. We introduce a Window-based Epipolar\nTransformer (WET) which reduces matching redundancy by using epipolar\nconstraints. Since point-to-line matching is sensitive to erroneous camera pose\nand calibration, we match windows near the epipolar lines. A second Shifted WT\nis employed for aggregating global information within cost volume. We present a\nnovel Cost Transformer (CT) to replace 3D convolutions for cost volume\nregularization. In order to better constrain the estimated depth maps from\nmultiple views, we further design a novel geometric consistency loss (Geo Loss)\nwhich punishes unreliable areas where multi-view consistency is not satisfied.\nOur WT multi-view stereo method (WT-MVSNet) achieves state-of-the-art\nperformance across multiple datasets and ranks $1^{st}$ on Tanks and Temples\nbenchmark.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14361,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"Boosting Facial Expression Recognition by A Semi-Supervised Progressive\n  Teacher\n\n  In this paper, we aim to improve the performance of in-the-wild Facial\nExpression Recognition (FER) by exploiting semi-supervised learning.\nLarge-scale labeled data and deep learning methods have greatly improved the\nperformance of image recognition. However, the performance of FER is still not\nideal due to the lack of training data and incorrect annotations (e.g., label\nnoises). Among existing in-the-wild FER datasets, reliable ones contain\ninsufficient data to train robust deep models while large-scale ones are\nannotated in lower quality. To address this problem, we propose a\nsemi-supervised learning algorithm named Progressive Teacher (PT) to utilize\nreliable FER datasets as well as large-scale unlabeled expression images for\neffective training. On the one hand, PT introduces semi-supervised learning\nmethod to relieve the shortage of data in FER. On the other hand, it selects\nuseful labeled training samples automatically and progressively to alleviate\nlabel noise. PT uses selected clean labeled data for computing the supervised\nclassification loss and unlabeled data for unsupervised consistency loss.\nExperiments on widely-used databases RAF-DB and FERPlus validate the\neffectiveness of our method, which achieves state-of-the-art performance with\naccuracy of 89.57% on RAF-DB. Additionally, when the synthetic noise rate\nreaches even 30%, the performance of our PT algorithm only degrades by 4.37%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.08987,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000001656,
      "text":"Trading Positional Complexity vs. Deepness in Coordinate Networks\n\n  It is well noted that coordinate-based MLPs benefit -- in terms of preserving\nhigh-frequency information -- through the encoding of coordinate positions as\nan array of Fourier features. Hitherto, the rationale for the effectiveness of\nthese positional encodings has been mainly studied through a Fourier lens. In\nthis paper, we strive to broaden this understanding by showing that alternative\nnon-Fourier embedding functions can indeed be used for positional encoding.\nMoreover, we show that their performance is entirely determined by a trade-off\nbetween the stable rank of the embedded matrix and the distance preservation\nbetween embedded coordinates. We further establish that the now ubiquitous\nFourier feature mapping of position is a special case that fulfills these\nconditions. Consequently, we present a more general theory to analyze\npositional encoding in terms of shifted basis functions. In addition, we argue\nthat employing a more complex positional encoding -- that scales exponentially\nwith the number of modes -- requires only a linear (rather than deep)\ncoordinate function to achieve comparable performance. Counter-intuitively, we\ndemonstrate that trading positional embedding complexity for network deepness\nis orders of magnitude faster than current state-of-the-art; despite the\nadditional embedding complexity. To this end, we develop the necessary\ntheoretical formulae and empirically verify that our theoretical claims hold in\npractice.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0076,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000187755,
      "text":"Point Cloud Compression with Sibling Context and Surface Priors\n\n  We present a novel octree-based multi-level framework for large-scale point\ncloud compression, which can organize sparse and unstructured point clouds in a\nmemory-efficient way. In this framework, we propose a new entropy model that\nexplores the hierarchical dependency in an octree using the context of\nsiblings' children, ancestors, and neighbors to encode the occupancy\ninformation of each non-leaf octree node into a bitstream. Moreover, we locally\nfit quadratic surfaces with a voxel-based geometry-aware module to provide\ngeometric priors in entropy encoding. These strong priors empower our entropy\nframework to encode the octree into a more compact bitstream. In the decoding\nstage, we apply a two-step heuristic strategy to restore point clouds with\nbetter reconstruction quality. The quantitative evaluation shows that our\nmethod outperforms state-of-the-art baselines with a bitrate improvement of\n11-16% and 12-14% on the KITTI Odometry and nuScenes datasets, respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.11179,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"Online Hybrid Lightweight Representations Learning: Its Application to\n  Visual Tracking\n\n  This paper presents a novel hybrid representation learning framework for\nstreaming data, where an image frame in a video is modeled by an ensemble of\ntwo distinct deep neural networks; one is a low-bit quantized network and the\nother is a lightweight full-precision network. The former learns coarse primary\ninformation with low cost while the latter conveys residual information for\nhigh fidelity to original representations. The proposed parallel architecture\nis effective to maintain complementary information since fixed-point arithmetic\ncan be utilized in the quantized network and the lightweight model provides\nprecise representations given by a compact channel-pruned network. We\nincorporate the hybrid representation technique into an online visual tracking\ntask, where deep neural networks need to handle temporal variations of target\nappearances in real-time. Compared to the state-of-the-art real-time trackers\nbased on conventional deep neural networks, our tracking algorithm demonstrates\ncompetitive accuracy on the standard benchmarks with a small fraction of\ncomputational cost and memory footprint.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10195,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000013908,
      "text":"Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video\n  Restoration\n\n  How to properly model the inter-frame relation within the video sequence is\nan important but unsolved challenge for video restoration (VR). In this work,\nwe propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to\naddress this problem. On the one hand, the sequence-to-sequence model, which\nhas proven capable of sequence modeling in the field of natural language\nprocessing, is explored for the first time in VR. Optimized serialization\nmodeling shows potential in capturing long-range dependencies among frames. On\nthe other hand, we equip the sequence-to-sequence model with an unsupervised\noptical flow estimator to maximize its potential. The flow estimator is trained\nwith our proposed unsupervised distillation loss, which can alleviate the data\ndiscrepancy and inaccurate degraded optical flow issues of previous flow-based\nmethods. With reliable optical flow, we can establish accurate correspondence\namong multiple frames, narrowing the domain difference between 1D language and\n2D misaligned frames and improving the potential of the sequence-to-sequence\nmodel. S2SVR shows superior performance in multiple VR tasks, including video\ndeblurring, video super-resolution, and compressed video quality enhancement.\nCode and models are publicly available at\nhttps:\/\/github.com\/linjing7\/VR-Baseline\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.06934,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000005232,
      "text":"A Saliency-Guided Street View Image Inpainting Framework for Efficient\n  Last-Meters Wayfinding\n\n  Global Positioning Systems (GPS) have played a crucial role in various\nnavigation applications. Nevertheless, localizing the perfect destination\nwithin the last few meters remains an important but unresolved problem. Limited\nby the GPS positioning accuracy, navigation systems always show users a\nvicinity of a destination, but not its exact location. Street view images (SVI)\nin maps as an immersive media technology have served as an aid to provide the\nphysical environment for human last-meters wayfinding. However, due to the\nlarge diversity of geographic context and acquisition conditions, the captured\nSVI always contains various distracting objects (e.g., pedestrians and\nvehicles), which will distract human visual attention from efficiently finding\nthe destination in the last few meters. To address this problem, we highlight\nthe importance of reducing visual distraction in image-based wayfinding by\nproposing a saliency-guided image inpainting framework. It aims at redirecting\nhuman visual attention from distracting objects to destination-related objects\nfor more efficient and accurate wayfinding in the last meters. Specifically, a\ncontext-aware distracting object detection method driven by deep salient object\ndetection has been designed to extract distracting objects from three semantic\nlevels in SVI. Then we employ a large-mask inpainting method with fast Fourier\nconvolutions to remove the detected distracting objects. Experimental results\nwith both qualitative and quantitative analysis show that our saliency-guided\ninpainting method can not only achieve great perceptual quality in street view\nimages but also redirect the human's visual attention to focus more on static\nlocation-related objects than distracting ones. The human-based evaluation also\njustified the effectiveness of our method in improving the efficiency of\nlocating the target destination.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0007920795,
      "text":"Multimodal Masked Autoencoders Learn Transferable Representations\n\n  Building scalable models to learn from diverse, multimodal data remains an\nopen challenge. For vision-language data, the dominant approaches are based on\ncontrastive learning objectives that train a separate encoder for each\nmodality. While effective, contrastive learning approaches introduce sampling\nbias depending on the data augmentations used, which can degrade performance on\ndownstream tasks. Moreover, these methods are limited to paired image-text\ndata, and cannot leverage widely-available unpaired data. In this paper, we\ninvestigate whether a large multimodal model trained purely via masked token\nprediction, without using modality-specific encoders or contrastive learning,\ncan learn transferable representations for downstream tasks. We propose a\nsimple and scalable network architecture, the Multimodal Masked Autoencoder\n(M3AE), which learns a unified encoder for both vision and language data via\nmasked token prediction. We provide an empirical study of M3AE trained on a\nlarge-scale image-text dataset, and find that M3AE is able to learn\ngeneralizable representations that transfer well to downstream tasks.\nSurprisingly, we find that M3AE benefits from a higher text mask ratio\n(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the\njoint training of two data modalities. We also provide qualitative analysis\nshowing that the learned representation incorporates meaningful information\nfrom both image and language. Lastly, we demonstrate the scalability of M3AE\nwith larger model size and training time, and its flexibility to train on both\npaired image-text data as well as unpaired data.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.0334,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Prompt Distribution Learning\n\n  We present prompt distribution learning for effectively adapting a\npre-trained vision-language model to address downstream recognition tasks. Our\nmethod not only learns low-bias prompts from a few samples but also captures\nthe distribution of diverse prompts to handle the varying visual\nrepresentations. In this way, we provide high-quality task-related content for\nfacilitating recognition. This prompt distribution learning is realized by an\nefficient approach that learns the output embeddings of prompts instead of the\ninput embeddings. Thus, we can employ a Gaussian distribution to model them\neffectively and derive a surrogate loss for efficient training. Extensive\nexperiments on 12 datasets demonstrate that our method consistently and\nsignificantly outperforms existing methods. For example, with 1 sample per\ncategory, it relatively improves the average result by 9.1% compared to\nhuman-crafted prompts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.03777,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000068876,
      "text":"Semi-Cycled Generative Adversarial Networks for Real-World Face\n  Super-Resolution\n\n  Real-world face super-resolution (SR) is a highly ill-posed image restoration\ntask. The fully-cycled Cycle-GAN architecture is widely employed to achieve\npromising performance on face SR, but prone to produce artifacts upon\nchallenging cases in real-world scenarios, since joint participation in the\nsame degradation branch will impact final performance due to huge domain gap\nbetween real-world and synthetic LR ones obtained by generators. To better\nexploit the powerful generative capability of GAN for real-world face SR, in\nthis paper, we establish two independent degradation branches in the forward\nand backward cycle-consistent reconstruction processes, respectively, while the\ntwo processes share the same restoration branch. Our Semi-Cycled Generative\nAdversarial Networks (SCGAN) is able to alleviate the adverse effects of the\ndomain gap between the real-world LR face images and the synthetic LR ones, and\nto achieve accurate and robust face SR performance by the shared restoration\nbranch regularized by both the forward and backward cycle-consistent learning\nprocesses. Experiments on two synthetic and two real-world datasets demonstrate\nthat, our SCGAN outperforms the state-of-the-art methods on recovering the face\nstructures\/details and quantitative metrics for real-world face SR. The code\nwill be publicly released at https:\/\/github.com\/HaoHou-98\/SCGAN.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.01643,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000065234,
      "text":"MTTrans: Cross-Domain Object Detection with Mean-Teacher Transformer\n\n  Recently, DEtection TRansformer (DETR), an end-to-end object detection\npipeline, has achieved promising performance. However, it requires large-scale\nlabeled data and suffers from domain shift, especially when no labeled data is\navailable in the target domain. To solve this problem, we propose an end-to-end\ncross-domain detection Transformer based on the mean teacher framework,\nMTTrans, which can fully exploit unlabeled target domain data in object\ndetection training and transfer knowledge between domains via pseudo labels. We\nfurther propose the comprehensive multi-level feature alignment to improve the\npseudo labels generated by the mean teacher framework taking advantage of the\ncross-scale self-attention mechanism in Deformable DETR. Image and object\nfeatures are aligned at the local, global, and instance levels with domain\nquery-based feature alignment (DQFA), bi-level graph-based prototype alignment\n(BGPA), and token-wise image feature alignment (TIFA). On the other hand, the\nunlabeled target domain data pseudo-labeled and available for the object\ndetection training by the mean teacher framework can lead to better feature\nextraction and alignment. Thus, the mean teacher framework and the\ncomprehensive multi-level feature alignment can be optimized iteratively and\nmutually based on the architecture of Transformers. Extensive experiments\ndemonstrate that our proposed method achieves state-of-the-art performance in\nthree domain adaptation scenarios, especially the result of Sim10k to\nCityscapes scenario is remarkably improved from 52.6 mAP to 57.9 mAP. Code will\nbe released.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.07556,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000018875,
      "text":"An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage\n  Detection Competition\n\n  We present an effective method for Intracranial Hemorrhage Detection (IHD)\nwhich exceeds the performance of the winner solution in RSNA-IHD competition\n(2019). Meanwhile, our model only takes quarter parameters and ten percent\nFLOPs compared to the winner's solution. The IHD task needs to predict the\nhemorrhage category of each slice for the input brain CT. We review the top-5\nsolutions for the IHD competition held by the Radiological Society of North\nAmerica(RSNA) in 2019. Nearly all the top solutions rely on 2D convolutional\nnetworks and sequential models (Bidirectional GRU or LSTM) to extract\nintra-slice and inter-slice features, respectively. All the top solutions\nenhance the performance by leveraging the model ensemble, and the model number\nvaries from 7 to 31. In the past years, since much progress has been made in\nthe computer vision regime especially Transformer-based models, we introduce\nthe Transformer-based techniques to extract the features in both intra-slice\nand inter-slice views for IHD tasks. Additionally, a semi-supervised method is\nembedded into our workflow to further improve the performance. The code is\navailable in the manuscript.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.15053,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000112587,
      "text":"Deblurring Photographs of Characters Using Deep Neural Networks\n\n  In this paper, we present our approach for the Helsinki Deblur Challenge\n(HDC2021). The task of this challenge is to deblur images of characters without\nknowing the point spread function (PSF). The organizers provided a dataset of\npairs of sharp and blurred images. Our method consists of three steps: First,\nwe estimate a warping transformation of the images to align the sharp images\nwith the blurred ones. Next, we estimate the PSF using a quasi-Newton method.\nThe estimated PSF allows to generate additional pairs of sharp and blurred\nimages. Finally, we train a deep convolutional neural network to reconstruct\nthe sharp images from the blurred images. Our method is able to successfully\nreconstruct images from the first 10 stages of the HDC 2021 data. Our code is\navailable at https:\/\/github.com\/hhu-machine-learning\/hdc2021-psfnn.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.05967,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0000118547,
      "text":"Target Aware Network Architecture Search and Compression for Efficient\n  Knowledge Transfer\n\n  Transfer Learning enables Convolutional Neural Networks (CNN) to acquire\nknowledge from a source domain and transfer it to a target domain, where\ncollecting large-scale annotated examples is time-consuming and expensive.\nConventionally, while transferring the knowledge learned from one task to\nanother task, the deeper layers of a pre-trained CNN are finetuned over the\ntarget dataset. However, these layers are originally designed for the source\ntask which may be over-parameterized for the target task. Thus, finetuning\nthese layers over the target dataset may affect the generalization ability of\nthe CNN due to high network complexity. To tackle this problem, we propose a\ntwo-stage framework called TASCNet which enables efficient knowledge transfer.\nIn the first stage, the configuration of the deeper layers is learned\nautomatically and finetuned over the target dataset. Later, in the second\nstage, the redundant filters are pruned from the fine-tuned CNN to decrease the\nnetwork's complexity for the target task while preserving the performance. This\ntwo-stage mechanism finds a compact version of the pre-trained CNN with optimal\nstructure (number of filters in a convolutional layer, number of neurons in a\ndense layer, and so on) from the hypothesis space. The efficacy of the proposed\nmethod is evaluated using VGG-16, ResNet-50, and DenseNet-121 on CalTech-101,\nCalTech-256, and Stanford Dogs datasets. Similar to computer vision tasks, we\nhave also conducted experiments on Movie Review Sentiment Analysis task. The\nproposed TASCNet reduces the computational complexity of pre-trained CNNs over\nthe target task by reducing both trainable parameters and FLOPs which enables\nresource-efficient knowledge transfer. The source code is available at:\nhttps:\/\/github.com\/Debapriya-Tula\/TASCNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.14212,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.0001221895,
      "text":"Exemplar Free Class Agnostic Counting\n\n  We tackle the task of Class Agnostic Counting, which aims to count objects in\na novel object category at test time without any access to labeled training\ndata for that category. All previous class agnostic counting methods cannot\nwork in a fully automated setting, and require computationally expensive test\ntime adaptation. To address these challenges, we propose a visual counter which\noperates in a fully automated setting and does not require any test time\nadaptation. Our proposed approach first identifies exemplars from repeating\nobjects in an image, and then counts the repeating objects. We propose a novel\nregion proposal network for identifying the exemplars. After identifying the\nexemplars, we obtain the corresponding count by using a density estimation\nbased Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and\nshow that it achieves superior performance compared to the existing approaches.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2205.10395,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":5,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"Assessing visual acuity in visual prostheses through a virtual-reality\n  system\n\n  Current visual implants still provide very low resolution and limited field\nof view, thus limiting visual acuity in implanted patients. Developments of new\nstrategies of artificial vision simulation systems by harnessing new\nadvancements in technologies are of upmost priorities for the development of\nnew visual devices. In this work, we take advantage of virtual-reality software\npaired with a portable head-mounted display and evaluated the performance of\nnormally sighted participants under simulated prosthetic vision with variable\nfield of view and number of pixels. Our simulated prosthetic vision system\nallows simple experimentation in order to study the design parameters of future\nvisual prostheses. Ten normally sighted participants volunteered for a visual\nacuity study. Subjects were required to identify computer-generated Landolt-C\ngap orientation and different stimulus based on light perception,\ntime-resolution, light location and motion perception commonly used for visual\nacuity examination in the sighted. Visual acuity scores were recorded across\ndifferent conditions of number of electrodes and size of field of view. Our\nresults showed that of all conditions tested, a field of view of 20{\\deg} and\n1000 phosphenes of resolution proved the best, with a visual acuity of 1.3\nlogMAR. Furthermore, performance appears to be correlated with phosphene\ndensity, but showing a diminishing return when field of view is less than\n20{\\deg}. The development of new artificial vision simulation systems can be\nuseful to guide the development of new visual devices and the optimization of\nfield of view and resolution to provide a helpful and valuable visual aid to\nprofoundly or totally blind patients.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.07705,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000106957,
      "text":"LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for\n  Camera-Only 3D Detection\n\n  The 3D Average Precision (3D AP) relies on the intersection over union\nbetween predictions and ground truth objects. However, camera-only detectors\nhave limited depth accuracy, which may cause otherwise reasonable predictions\nthat suffer from such longitudinal localization errors to be treated as false\npositives. We therefore propose variants of the 3D AP metric to be more\npermissive with respect to depth estimation errors. Specifically, our novel\nlongitudinal error tolerant metrics, LET-3D-AP and LET-3D-APL, allow\nlongitudinal localization errors of the prediction boxes up to a given\ntolerance. To evaluate the proposed metrics, we also construct a new test set\nfor the Waymo Open Dataset, tailored to camera-only 3D detection methods.\nSurprisingly, we find that state-of-the-art camera-based detectors can\noutperform popular LiDAR-based detectors with our new metrics past at 10% depth\nerror tolerance, suggesting that existing camera-based detectors already have\nthe potential to surpass LiDAR-based detectors in downstream applications. We\nbelieve the proposed metrics and the new benchmark dataset will facilitate\nadvances in the field of camera-only 3D detection by providing more informative\nsignals that can better indicate the system-level performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.06252,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000196695,
      "text":"Transformer Lesion Tracker\n\n  Evaluating lesion progression and treatment response via longitudinal lesion\ntracking plays a critical role in clinical practice. Automated approaches for\nthis task are motivated by prohibitive labor costs and time consumption when\nlesion matching is done manually. Previous methods typically lack the\nintegration of local and global information. In this work, we propose a\ntransformer-based approach, termed Transformer Lesion Tracker (TLT).\nSpecifically, we design a Cross Attention-based Transformer (CAT) to capture\nand combine both global and local information to enhance feature extraction. We\nalso develop a Registration-based Anatomical Attention Module (RAAM) to\nintroduce anatomical information to CAT so that it can focus on useful feature\nknowledge. A Sparse Selection Strategy (SSS) is presented for selecting\nfeatures and reducing memory footprint in Transformer training. In addition, we\nuse a global regression to further improve model performance. We conduct\nexperiments on a public dataset to show the superiority of our method and find\nthat our model performance has improved the average Euclidean center error by\nat least 14.3% (6mm vs. 7mm) compared with the state-of-the-art (SOTA). Code is\navailable at https:\/\/github.com\/TangWen920812\/TLT.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01653,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Metrics reloaded: Recommendations for image analysis validation\n\n  Increasing evidence shows that flaws in machine learning (ML) algorithm\nvalidation are an underestimated global problem. Particularly in automatic\nbiomedical image analysis, chosen performance metrics often do not reflect the\ndomain interest, thus failing to adequately measure scientific progress and\nhindering translation of ML techniques into practice. To overcome this, our\nlarge international expert consortium created Metrics Reloaded, a comprehensive\nframework guiding researchers in the problem-aware selection of metrics.\nFollowing the convergence of ML methodology across application domains, Metrics\nReloaded fosters the convergence of validation methodology. The framework was\ndeveloped in a multi-stage Delphi process and is based on the novel concept of\na problem fingerprint - a structured representation of the given problem that\ncaptures all aspects that are relevant for metric selection, from the domain\ninterest to the properties of the target structure(s), data set and algorithm\noutput. Based on the problem fingerprint, users are guided through the process\nof choosing and applying appropriate validation metrics while being made aware\nof potential pitfalls. Metrics Reloaded targets image analysis problems that\ncan be interpreted as a classification task at image, object or pixel level,\nnamely image-level classification, object detection, semantic segmentation, and\ninstance segmentation tasks. To improve the user experience, we implemented the\nframework in the Metrics Reloaded online tool, which also provides a point of\naccess to explore weaknesses, strengths and specific recommendations for the\nmost common validation metrics. The broad applicability of our framework across\ndomains is demonstrated by an instantiation for various biological and medical\nimage analysis use cases.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.10066,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000044041,
      "text":"RendNet: Unified 2D\/3D Recognizer With Latent Space Rendering\n\n  Vector graphics (VG) have been ubiquitous in our daily life with vast\napplications in engineering, architecture, designs, etc. The VG recognition\nprocess of most existing methods is to first render the VG into raster graphics\n(RG) and then conduct recognition based on RG formats. However, this procedure\ndiscards the structure of geometries and loses the high resolution of VG.\nRecently, another category of algorithms is proposed to recognize directly from\nthe original VG format. But it is affected by the topological errors that can\nbe filtered out by RG rendering. Instead of looking at one format, it is a good\nsolution to utilize the formats of VG and RG together to avoid these\nshortcomings. Besides, we argue that the VG-to-RG rendering process is\nessential to effectively combine VG and RG information. By specifying the rules\non how to transfer VG primitives to RG pixels, the rendering process depicts\nthe interaction and correlation between VG and RG. As a result, we propose\nRendNet, a unified architecture for recognition on both 2D and 3D scenarios,\nwhich considers both VG\/RG representations and exploits their interaction by\nincorporating the VG-to-RG rasterization process. Experiments show that RendNet\ncan achieve state-of-the-art performance on 2D and 3D object recognition tasks\non various VG datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.15083,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000049008,
      "text":"UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer\n  via Hierarchical Mask Calibration\n\n  Domain adaptive panoptic segmentation aims to mitigate data annotation\nchallenge by leveraging off-the-shelf annotated data in one or multiple related\nsource domains. However, existing studies employ two separate networks for\ninstance segmentation and semantic segmentation which lead to excessive network\nparameters as well as complicated and computationally intensive training and\ninference processes. We design UniDAformer, a unified domain adaptive panoptic\nsegmentation transformer that is simple but can achieve domain adaptive\ninstance segmentation and semantic segmentation simultaneously within a single\nnetwork. UniDAformer introduces Hierarchical Mask Calibration (HMC) that\nrectifies inaccurate predictions at the level of regions, superpixels and\npixels via online self-training on the fly. It has three unique features: 1) it\nenables unified domain adaptive panoptic adaptation; 2) it mitigates false\npredictions and improves domain adaptive panoptic segmentation effectively; 3)\nit is end-to-end trainable with a much simpler training and inference pipeline.\nExtensive experiments over multiple public benchmarks show that UniDAformer\nachieves superior domain adaptive panoptic segmentation as compared with the\nstate-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08229,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000022848,
      "text":"Open-Set Recognition with Gradient-Based Representations\n\n  Neural networks for image classification tasks assume that any given image\nduring inference belongs to one of the training classes. This closed-set\nassumption is challenged in real-world applications where models may encounter\ninputs of unknown classes. Open-set recognition aims to solve this problem by\nrejecting unknown classes while classifying known classes correctly. In this\npaper, we propose to utilize gradient-based representations obtained from a\nknown classifier to train an unknown detector with instances of known classes\nonly. Gradients correspond to the amount of model updates required to properly\nrepresent a given sample, which we exploit to understand the model's capability\nto characterize inputs with its learned features. Our approach can be utilized\nwith any classifier trained in a supervised manner on known classes without the\nneed to model the distribution of unknown samples explicitly. We show that our\ngradient-based approach outperforms state-of-the-art methods by up to 11.6% in\nopen-set classification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.1192,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000209941,
      "text":"Agriculture-Vision Challenge 2022 -- The Runner-Up Solution for\n  Agricultural Pattern Recognition via Transformer-based Models\n\n  The Agriculture-Vision Challenge in CVPR is one of the most famous and\ncompetitive challenges for global researchers to break the boundary between\ncomputer vision and agriculture sectors, aiming at agricultural pattern\nrecognition from aerial images. In this paper, we propose our solution to the\nthird Agriculture-Vision Challenge in CVPR 2022. We leverage a data\npre-processing scheme and several Transformer-based models as well as data\naugmentation techniques to achieve a mIoU of 0.582, accomplishing the 2nd place\nin this challenge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.13383,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000540747,
      "text":"Mushroom image recognition and distance generation based on\n  attention-mechanism model and genetic information\n\n  The species identification of Macrofungi, i.e. mushrooms, has always been a\nchallenging task. There are still a large number of poisonous mushrooms that\nhave not been found, which poses a risk to people's life. However, the\ntraditional identification method requires a large number of experts with\nknowledge in the field of taxonomy for manual identification, it is not only\ninefficient but also consumes a lot of manpower and capital costs. In this\npaper, we propose a new model based on attention-mechanism, MushroomNet, which\napplies the lightweight network MobileNetV3 as the backbone model, combined\nwith the attention structure proposed by us, and has achieved excellent\nperformance in the mushroom recognition task. On the public dataset, the test\naccuracy of the MushroomNet model has reached 83.9%, and on the local dataset,\nthe test accuracy has reached 77.4%. The proposed attention mechanisms well\nfocused attention on the bodies of mushroom image for mixed channel attention\nand the attention heat maps visualized by Grad-CAM. Further, in this study,\ngenetic distance was added to the mushroom image recognition task, the genetic\ndistance was used as the representation space, and the genetic distance between\neach pair of mushroom species in the dataset was used as the embedding of the\ngenetic distance representation space, so as to predict the image distance and\nspecies. identify. We found that using the MES activation function can predict\nthe genetic distance of mushrooms very well, but the accuracy is lower than\nthat of SoftMax. The proposed MushroomNet was demonstrated it shows great\npotential for automatic and online mushroom image and the proposed automatic\nprocedure would assist and be a reference to traditional mushroom\nclassification.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.09552,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000166893,
      "text":"Dynamic Message Propagation Network for RGB-D Salient Object Detection\n\n  This paper presents a novel deep neural network framework for RGB-D salient\nobject detection by controlling the message passing between the RGB images and\ndepth maps on the feature level and exploring the long-range semantic contexts\nand geometric information on both RGB and depth features to infer salient\nobjects. To achieve this, we formulate a dynamic message propagation (DMP)\nmodule with the graph neural networks and deformable convolutions to\ndynamically learn the context information and to automatically predict filter\nweights and affinity matrices for message propagation control. We further embed\nthis module into a Siamese-based network to process the RGB image and depth map\nrespectively and design a multi-level feature fusion (MFF) module to explore\nthe cross-level information between the refined RGB and depth features.\nCompared with 17 state-of-the-art methods on six benchmark datasets for RGB-D\nsalient object detection, experimental results show that our method outperforms\nall the others, both quantitatively and visually.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01908,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.000004073,
      "text":"Video-based Human-Object Interaction Detection from Tubelet Tokens\n\n  We present a novel vision Transformer, named TUTOR, which is able to learn\ntubelet tokens, served as highly-abstracted spatiotemporal representations, for\nvideo-based human-object interaction (V-HOI) detection. The tubelet tokens\nstructurize videos by agglomerating and linking semantically-related patch\ntokens along spatial and temporal domains, which enjoy two benefits: 1)\nCompactness: each tubelet token is learned by a selective attention mechanism\nto reduce redundant spatial dependencies from others; 2) Expressiveness: each\ntubelet token is enabled to align with a semantic instance, i.e., an object or\na human, across frames, thanks to agglomeration and linking. The effectiveness\nand efficiency of TUTOR are verified by extensive experiments. Results shows\nour method outperforms existing works by large margins, with a relative mAP\ngain of $16.14\\%$ on VidHOI and a 2 points gain on CAD-120 as well as a $4\n\\times$ speedup.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.00171,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000137753,
      "text":"Learning Sequential Contexts using Transformer for 3D Hand Pose\n  Estimation\n\n  3D hand pose estimation (HPE) is the process of locating the joints of the\nhand in 3D from any visual input. HPE has recently received an increased amount\nof attention due to its key role in a variety of human-computer interaction\napplications. Recent HPE methods have demonstrated the advantages of employing\nvideos or multi-view images, allowing for more robust HPE systems. Accordingly,\nin this study, we propose a new method to perform Sequential learning with\nTransformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins\nby extracting visual embeddings from individual hand images. We then use a\ntransformer encoder to learn the sequential context along time or viewing\nangles and generate accurate 2D hand joint locations. Then, a graph\nconvolutional neural network with a U-Net configuration is used to convert the\n2D hand joint locations to 3D poses. Our experiments show that SeTHPose\nperforms well on both hand sequence varieties, temporal and angular. Also,\nSeTHPose outperforms other methods in the field to achieve new state-of-the-art\nresults on two public available sequential datasets, STB and MuViHand.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08833,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"A Comparative Study of Confidence Calibration in Deep Learning: From\n  Computer Vision to Medical Imaging\n\n  Although deep learning prediction models have been successful in the\ndiscrimination of different classes, they can often suffer from poor\ncalibration across challenging domains including healthcare. Moreover, the\nlong-tail distribution poses great challenges in deep learning classification\nproblems including clinical disease prediction. There are approaches proposed\nrecently to calibrate deep prediction in computer vision, but there are no\nstudies found to demonstrate how the representative models work in different\nchallenging contexts. In this paper, we bridge the confidence calibration from\ncomputer vision to medical imaging with a comparative study of four high-impact\ncalibration models. Our studies are conducted in different contexts (natural\nimage classification and lung cancer risk estimation) including in balanced vs.\nimbalanced training sets and in computer vision vs. medical imaging. Our\nresults support key findings: (1) We achieve new conclusions which are not\nstudied under different learning contexts, e.g., combining two calibration\nmodels that both mitigate the overconfident prediction can lead to\nunder-confident prediction, and simpler calibration models from the computer\nvision domain tend to be more generalizable to medical imaging. (2) We\nhighlight the gap between general computer vision tasks and medical imaging\nprediction, e.g., calibration methods ideal for general computer vision tasks\nmay in fact damage the calibration of medical imaging prediction. (3) We also\nreinforce previous conclusions in natural image classification settings. We\nbelieve that this study has merits to guide readers to choose calibration\nmodels and understand gaps between general computer vision and medical imaging\ndomains.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.02257,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000130468,
      "text":"Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey\n\n  In this survey, we present a systematic review of 3D hand pose estimation\nfrom the perspective of efficient annotation and learning. 3D hand pose\nestimation has been an important research area owing to its potential to enable\nvarious applications, such as video understanding, AR\/VR, and robotics.\nHowever, the performance of models is tied to the quality and quantity of\nannotated 3D hand poses. Under the status quo, acquiring such annotated 3D hand\nposes is challenging, e.g., due to the difficulty of 3D annotation and the\npresence of occlusion. To reveal this problem, we review the pros and cons of\nexisting annotation methods classified as manual, synthetic-model-based,\nhand-sensor-based, and computational approaches. Additionally, we examine\nmethods for learning 3D hand poses when annotated data are scarce, including\nself-supervised pretraining, semi-supervised learning, and domain adaptation.\nBased on the study of efficient annotation and learning, we further discuss\nlimitations and possible future directions in this field.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.01204,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000035432,
      "text":"Siamese Image Modeling for Self-Supervised Vision Representation\n  Learning\n\n  Self-supervised learning (SSL) has delivered superior performance on a\nvariety of downstream vision tasks. Two main-stream SSL frameworks have been\nproposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM).\nID pulls together representations from different views of the same image, while\navoiding feature collapse. It lacks spatial sensitivity, which requires\nmodeling the local structure within each image. On the other hand, MIM\nreconstructs the original content given a masked image. It instead does not\nhave good semantic alignment, which requires projecting semantically similar\nviews into nearby representations. To address this dilemma, we observe that (1)\nsemantic alignment can be achieved by matching different image views with\nstrong augmentations; (2) spatial sensitivity can benefit from predicting dense\nrepresentations with masked images. Driven by these analysis, we propose\nSiamese Image Modeling (SiameseIM), which predicts the dense representations of\nan augmented view, based on another masked view from the same image but with\ndifferent augmentations. SiameseIM uses a Siamese network with two branches.\nThe online branch encodes the first view, and predicts the second view's\nrepresentation according to the relative positions between these two views. The\ntarget branch produces the target by encoding the second view. SiameseIM can\nsurpass both ID and MIM on a wide range of downstream tasks, including ImageNet\nfinetuning and linear probing, COCO and LVIS detection, and ADE20k semantic\nsegmentation. The improvement is more significant in few-shot, long-tail and\nrobustness-concerned scenarios. Code shall be released at\nhttps:\/\/github.com\/fundamentalvision\/Siamese-Image-Modeling.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.0079,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000064241,
      "text":"Efficient Self-supervised Vision Pretraining with Local Masked\n  Reconstruction\n\n  Self-supervised learning for computer vision has achieved tremendous progress\nand improved many downstream vision tasks such as image classification,\nsemantic segmentation, and object detection. Among these, generative\nself-supervised vision learning approaches such as MAE and BEiT show promising\nperformance. However, their global masked reconstruction mechanism is\ncomputationally demanding. To address this issue, we propose local masked\nreconstruction (LoMaR), a simple yet effective approach that performs masked\nreconstruction within a small window of 7$\\times$7 patches on a simple\nTransformer encoder, improving the trade-off between efficiency and accuracy\ncompared to global masked reconstruction over the entire image. Extensive\nexperiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K\nclassification, outperforming MAE by 0.5%. After finetuning the pretrained\nLoMaR on 384$\\times$384 images, it can reach 85.4% top-1 accuracy, surpassing\nMAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 $\\text{AP}^\\text{box}$ on\nobject detection and 0.5 $\\text{AP}^\\text{mask}$ on instance segmentation.\nLoMaR is especially more computation-efficient on pretraining high-resolution\nimages, e.g., it is 3.1$\\times$ faster than MAE with 0.2% higher classification\naccuracy on pretraining 448$\\times$448 images. This local masked reconstruction\nlearning mechanism can be easily integrated into any other generative\nself-supervised learning approach. Our code is publicly available in\nhttps:\/\/github.com\/junchen14\/LoMaR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08524,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000039405,
      "text":"CDNet: Contrastive Disentangled Network for Fine-Grained Image\n  Categorization of Ocular B-Scan Ultrasound\n\n  Precise and rapid categorization of images in the B-scan ultrasound modality\nis vital for diagnosing ocular diseases. Nevertheless, distinguishing various\ndiseases in ultrasound still challenges experienced ophthalmologists. Thus a\nnovel contrastive disentangled network (CDNet) is developed in this work,\naiming to tackle the fine-grained image categorization (FGIC) challenges of\nocular abnormalities in ultrasound images, including intraocular tumor (IOT),\nretinal detachment (RD), posterior scleral staphyloma (PSS), and vitreous\nhemorrhage (VH). Three essential components of CDNet are the weakly-supervised\nlesion localization module (WSLL), contrastive multi-zoom (CMZ) strategy, and\nhyperspherical contrastive disentangled loss (HCD-Loss), respectively. These\ncomponents facilitate feature disentanglement for fine-grained recognition in\nboth the input and output aspects. The proposed CDNet is validated on our ZJU\nOcular Ultrasound Dataset (ZJUOUSD), consisting of 5213 samples. Furthermore,\nthe generalization ability of CDNet is validated on two public and widely-used\nchest X-ray FGIC benchmarks. Quantitative and qualitative results demonstrate\nthe efficacy of our proposed CDNet, which achieves state-of-the-art performance\nin the FGIC task. Code is available at:\nhttps:\/\/github.com\/ZeroOneGame\/CDNet-for-OUS-FGIC .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.1009,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000088082,
      "text":"KTN: Knowledge Transfer Network for Learning Multi-person 2D-3D\n  Correspondences\n\n  Human densepose estimation, aiming at establishing dense correspondences\nbetween 2D pixels of human body and 3D human body template, is a key technique\nin enabling machines to have an understanding of people in images. It still\nposes several challenges due to practical scenarios where real-world scenes are\ncomplex and only partial annotations are available, leading to incompelete or\nfalse estimations. In this work, we present a novel framework to detect the\ndensepose of multiple people in an image. The proposed method, which we refer\nto Knowledge Transfer Network (KTN), tackles two main problems: 1) how to\nrefine image representation for alleviating incomplete estimations, and 2) how\nto reduce false estimation caused by the low-quality training labels (i.e.,\nlimited annotations and class-imbalance labels). Unlike existing works directly\npropagating the pyramidal features of regions for densepose estimation, the KTN\nuses a refinement of pyramidal representation, where it simultaneously\nmaintains feature resolution and suppresses background pixels, and this\nstrategy results in a substantial increase in accuracy. Moreover, the KTN\nenhances the ability of 3D based body parsing with external knowledges, where\nit casts 2D based body parsers trained from sufficient annotations as a 3D\nbased body parser through a structural body knowledge graph. In this way, it\nsignificantly reduces the adverse effects caused by the low-quality\nannotations. The effectiveness of KTN is demonstrated by its superior\nperformance to the state-of-the-art methods on DensePose-COCO dataset.\nExtensive ablation studies and experimental results on representative tasks\n(e.g., human body segmentation, human part segmentation and keypoints\ndetection) and two popular densepose estimation pipelines (i.e., RCNN and\nfully-convolutional frameworks), further indicate the generalizability of the\nproposed method.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.11752,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000110931,
      "text":"CLAMP: Prompt-based Contrastive Learning for Connecting Language and\n  Animal Pose\n\n  Animal pose estimation is challenging for existing image-based methods\nbecause of limited training data and large intra- and inter-species variances.\nMotivated by the progress of visual-language research, we propose that\npre-trained language models (e.g., CLIP) can facilitate animal pose estimation\nby providing rich prior knowledge for describing animal keypoints in text.\nHowever, we found that building effective connections between pre-trained\nlanguage models and visual animal keypoints is non-trivial since the gap\nbetween text-based descriptions and keypoint-based visual features about animal\npose can be significant. To address this issue, we introduce a novel\nprompt-based Contrastive learning scheme for connecting Language and AniMal\nPose (CLAMP) effectively. The CLAMP attempts to bridge the gap by adapting the\ntext prompts to the animal keypoints during network training. The adaptation is\ndecomposed into spatial-aware and feature-aware processes, and two novel\ncontrastive losses are devised correspondingly. In practice, the CLAMP enables\nthe first cross-modal animal pose estimation paradigm. Experimental results\nshow that our method achieves state-of-the-art performance under the\nsupervised, few-shot, and zero-shot settings, outperforming image-based methods\nby a large margin.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.08224,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Multi scale Feature Extraction and Fusion for Online Knowledge\n  Distillation\n\n  Online knowledge distillation conducts knowledge transfer among all student\nmodels to alleviate the reliance on pre-trained models. However, existing\nonline methods rely heavily on the prediction distributions and neglect the\nfurther exploration of the representational knowledge. In this paper, we\npropose a novel Multi-scale Feature Extraction and Fusion method (MFEF) for\nonline knowledge distillation, which comprises three key components:\nMulti-scale Feature Extraction, Dual-attention and Feature Fusion, towards\ngenerating more informative feature maps for distillation. The multiscale\nfeature extraction exploiting divide-and-concatenate in channel dimension is\nproposed to improve the multi-scale representation ability of feature maps. To\nobtain more accurate information, we design a dual-attention to strengthen the\nimportant channel and spatial regions adaptively. Moreover, we aggregate and\nfuse the former processed feature maps via feature fusion to assist the\ntraining of student models. Extensive experiments on CIF AR-10, CIF AR-100, and\nCINIC-10 show that MFEF transfers more beneficial representational knowledge\nfor distillation and outperforms alternative methods among various network\narchitectures\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2206.12921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":6,
    "pangram_prediction":{
      "ai_likelihood":0.0000022517,
      "text":"Non-Parametric Style Transfer\n\n  Recent feed-forward neural methods of arbitrary image style transfer mainly\nutilized encoded feature map upto its second-order statistics, i.e., linearly\ntransformed the encoded feature map of a content image to have the same mean\nand variance (or covariance) of a target style feature map. In this work, we\nextend the second-order statistical feature matching into a general\ndistribution matching based on the understanding that style of an image is\nrepresented by the distribution of responses from receptive fields. For this\ngeneralization, first, we propose a new feature transform layer that exactly\nmatches the feature map distribution of content image into that of target style\nimage. Second, we analyze the recent style losses consistent with our new\nfeature transform layer to train a decoder network which generates a style\ntransferred image from the transformed feature map. Based on our experimental\nresults, it is proven that the stylized images obtained with our method are\nmore similar with the target style images in all existing style measures\nwithout losing content clearness.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.00807,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis\n\n  Thyroid nodule classification aims at determining whether the nodule is\nbenign or malignant based on a given ultrasound image. However, the label\nobtained by the cytological biopsy which is the golden standard in clinical\nmedicine is not always consistent with the ultrasound imaging TI-RADS criteria.\nThe information difference between the two causes the existing deep\nlearning-based classification methods to be indecisive. To solve the\nInconsistent Label problem, we propose an Adaptive Curriculum Learning (ACL)\nframework, which adaptively discovers and discards the samples with\ninconsistent labels. Specifically, ACL takes both hard sample and model\ncertainty into account, and could accurately determine the threshold to\ndistinguish the samples with Inconsistent Label. Moreover, we contribute TNCD:\na Thyroid Nodule Classification Dataset to facilitate future related research\non the thyroid nodules. Extensive experimental results on TNCD based on three\ndifferent backbone networks not only demonstrate the superiority of our method\nbut also prove that the less-is-more principle which strategically discards the\nsamples with Inconsistent Label could yield performance gains. Source code and\ndata are available at https:\/\/github.com\/chenghui-666\/ACL\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.13686,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000016226,
      "text":"Shift-tolerant Perceptual Similarity Metric\n\n  Existing perceptual similarity metrics assume an image and its reference are\nwell aligned. As a result, these metrics are often sensitive to a small\nalignment error that is imperceptible to the human eyes. This paper studies the\neffect of small misalignment, specifically a small shift between the input and\nreference image, on existing metrics, and accordingly develops a shift-tolerant\nsimilarity metric. This paper builds upon LPIPS, a widely used learned\nperceptual similarity metric, and explores architectural design considerations\nto make it robust against imperceptible misalignment. Specifically, we study a\nwide spectrum of neural network elements, such as anti-aliasing filtering,\npooling, striding, padding, and skip connection, and discuss their roles in\nmaking a robust metric. Based on our studies, we develop a new deep neural\nnetwork-based perceptual similarity metric. Our experiments show that our\nmetric is tolerant to imperceptible shifts while being consistent with the\nhuman similarity judgment.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.02206,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000013245,
      "text":"Segmenting Moving Objects via an Object-Centric Layered Representation\n\n  The objective of this paper is a model that is able to discover, track and\nsegment multiple moving objects in a video. We make four contributions: First,\nwe introduce an object-centric segmentation model with a depth-ordered layer\nrepresentation. This is implemented using a variant of the transformer\narchitecture that ingests optical flow, where each query vector specifies an\nobject and its layer for the entire video. The model can effectively discover\nmultiple moving objects and handle mutual occlusions; Second, we introduce a\nscalable pipeline for generating multi-object synthetic training data via layer\ncompositions, that is used to train the proposed model, significantly reducing\nthe requirements for labour-intensive annotations, and supporting Sim2Real\ngeneralisation; Third, we conduct thorough ablation studies, showing that the\nmodel is able to learn object permanence and temporal shape consistency, and is\nable to predict amodal segmentation masks; Fourth, we evaluate our model,\ntrained only on synthetic data, on standard video segmentation benchmarks,\nDAVIS, MoCA, SegTrack, FBMS-59, and achieve state-of-the-art performance among\nexisting methods that do not rely on any manual annotations. With test-time\nadaptation, we observe further performance boosts.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06695,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000175834,
      "text":"DavarOCR: A Toolbox for OCR and Multi-Modal Document Understanding\n\n  This paper presents DavarOCR, an open-source toolbox for OCR and document\nunderstanding tasks. DavarOCR currently implements 19 advanced algorithms,\ncovering 9 different task forms. DavarOCR provides detailed usage instructions\nand the trained models for each algorithm. Compared with the previous\nopensource OCR toolbox, DavarOCR has relatively more complete support for the\nsub-tasks of the cutting-edge technology of document understanding. In order to\npromote the development and application of OCR technology in academia and\nindustry, we pay more attention to the use of modules that different\nsub-domains of technology can share. DavarOCR is publicly released at\nhttps:\/\/github.com\/hikopensource\/Davar-Lab-OCR.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12496,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"NeuriCam: Key-Frame Video Super-Resolution and Colorization for IoT\n  Cameras\n\n  We present NeuriCam, a novel deep learning-based system to achieve video\ncapture from low-power dual-mode IoT camera systems. Our idea is to design a\ndual-mode camera system where the first mode is low-power (1.1 mW) but only\noutputs grey-scale, low resolution, and noisy video and the second mode\nconsumes much higher power (100 mW) but outputs color and higher resolution\nimages. To reduce total energy consumption, we heavily duty cycle the high\npower mode to output an image only once every second. The data for this camera\nsystem is then wirelessly sent to a nearby plugged-in gateway, where we run our\nreal-time neural network decoder to reconstruct a higher-resolution color\nvideo. To achieve this, we introduce an attention feature filter mechanism that\nassigns different weights to different features, based on the correlation\nbetween the feature map and the contents of the input frame at each spatial\nlocation. We design a wireless hardware prototype using off-the-shelf cameras\nand address practical issues including packet loss and perspective mismatch.\nOur evaluations show that our dual-camera approach reduces energy consumption\nby 7x compared to existing systems. Further, our model achieves an average\ngreyscale PSNR gain of 3.7 dB over prior single and dual-camera video\nsuper-resolution methods and 5.6 dB RGB gain over prior color propagation\nmethods. Open-source code: https:\/\/github.com\/vb000\/NeuriCam.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12049,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000024835,
      "text":"Few-Shot Object Detection by Knowledge Distillation Using\n  Bag-of-Visual-Words Representations\n\n  While fine-tuning based methods for few-shot object detection have achieved\nremarkable progress, a crucial challenge that has not been addressed well is\nthe potential class-specific overfitting on base classes and sample-specific\noverfitting on novel classes. In this work we design a novel knowledge\ndistillation framework to guide the learning of the object detector and thereby\nrestrain the overfitting in both the pre-training stage on base classes and\nfine-tuning stage on novel classes. To be specific, we first present a novel\nPosition-Aware Bag-of-Visual-Words model for learning a representative bag of\nvisual words (BoVW) from a limited size of image set, which is used to encode\ngeneral images based on the similarities between the learned visual words and\nan image. Then we perform knowledge distillation based on the fact that an\nimage should have consistent BoVW representations in two different feature\nspaces. To this end, we pre-learn a feature space independently from the object\ndetection, and encode images using BoVW in this space. The obtained BoVW\nrepresentation for an image can be considered as distilled knowledge to guide\nthe learning of object detector: the extracted features by the object detector\nfor the same image are expected to derive the consistent BoVW representations\nwith the distilled knowledge. Extensive experiments validate the effectiveness\nof our method and demonstrate the superiority over other state-of-the-art\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.02042,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000020961,
      "text":"MVP: Robust Multi-View Practice for Driving Action Localization\n\n  Distracted driving causes thousands of deaths per year, and how to apply\ndeep-learning methods to prevent these tragedies has become a crucial problem.\nIn Track3 of the 6th AI City Challenge, researchers provide a high-quality\nvideo dataset with densely action annotations. Due to the small data scale and\nunclear action boundary, the dataset presents a unique challenge to precisely\nlocalize all the different actions and classify their categories. In this\npaper, we make good use of the multi-view synchronization among videos, and\nconduct robust Multi-View Practice (MVP) for driving action localization. To\navoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the\nfeature extractor. Then the features of different views are passed to\nActionFormer to generate candidate action proposals. For precisely localizing\nall the actions, we design elaborate post-processing, including model voting,\nthreshold filtering and duplication removal. The results show that our MVP is\nrobust for driving action localization, which achieves 28.49% F1-score in the\nTrack3 test set.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12202,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000215239,
      "text":"Video object tracking based on YOLOv7 and DeepSORT\n\n  Multiple object tracking (MOT) is an important technology in the field of\ncomputer vision, which is widely used in automatic driving, intelligent\nmonitoring, behavior recognition and other directions. Among the current\npopular MOT methods based on deep learning, Detection Based Tracking (DBT) is\nthe most widely used in industry, and the performance of them depend on their\nobject detection network. At present, the DBT algorithm with good performance\nand the most widely used is YOLOv5-DeepSORT. Inspired by YOLOv5-DeepSORT, with\nthe proposal of YOLOv7 network, which performs better in object detection, we\napply YOLOv7 as the object detection part to the DeepSORT, and propose\nYOLOv7-DeepSORT. After experimental evaluation, compared with the previous\nYOLOv5-DeepSORT, YOLOv7-DeepSORT performances better in tracking accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.12537,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000088082,
      "text":"Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation\n\n  3D Human body pose and shape estimation within a temporal sequence can be\nquite critical for understanding human behavior. Despite the significant\nprogress in human pose estimation in the recent years, which are often based on\nsingle images or videos, human motion estimation on live stream videos is still\na rarely-touched area considering its special requirements for real-time output\nand temporal consistency. To address this problem, we present a temporally\nembedded 3D human body pose and shape estimation (TePose) method to improve the\naccuracy and temporal consistency of pose estimation in live stream videos.\nTePose uses previous predictions as a bridge to feedback the error for better\nestimation in the current frame and to learn the correspondence between data\nframes and predictions in the history. A multi-scale spatio-temporal graph\nconvolutional network is presented as the motion discriminator for adversarial\ntraining using datasets without any 3D labeling. We propose a sequential data\nloading strategy to meet the special start-to-end data processing requirement\nof live stream. We demonstrate the importance of each proposed module with\nextensive experiments. The results show the effectiveness of TePose on\nwidely-used human pose benchmarks with state-of-the-art performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.09086,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000429816,
      "text":"MHR-Net: Multiple-Hypothesis Reconstruction of Non-Rigid Shapes from 2D\n  Views\n\n  We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from\nMotion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a\n2D view, and it also selects the most likely reconstruction from the set. To\ndeal with the challenging unsupervised generation of non-rigid shapes, we\ndevelop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net.\nThe non-rigid shape is first expressed as the sum of a coarse shape basis and a\nflexible shape deformation, then multiple hypotheses are generated with\nuncertainty modeling of the deformation part. MHR-Net is optimized with\nreprojection loss on the basis and the best hypothesis. Furthermore, we design\na new Procrustean Residual Loss, which reduces the rigid rotations between\nsimilar shapes and further improves the performance. Experiments show that\nMHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL\nand 300-VW datasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.02281,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000081129,
      "text":"BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for\n  Pedestrian Anomaly Detection\n\n  We present BiPOCO, a Bi-directional trajectory predictor with POse\nCOnstraints, for detecting anomalous activities of pedestrians in videos. In\ncontrast to prior work based on feature reconstruction, our work identifies\npedestrian anomalous events by forecasting their future trajectories and\ncomparing the predictions with their expectations. We introduce a set of novel\ncompositional pose-based losses with our predictor and leverage prediction\nerrors of each body joint for pedestrian anomaly detection. Experimental\nresults show that our BiPOCO approach can detect pedestrian anomalous\nactivities with a high detection rate (up to 87.0%) and incorporating pose\nconstraints helps distinguish normal and anomalous poses in prediction. This\nwork extends current literature of using prediction-based methods for anomaly\ndetection and can benefit safety-critical applications such as autonomous\ndriving and surveillance. Code is available at\nhttps:\/\/github.com\/akanuasiegbu\/BiPOCO.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.06214,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"Is one annotation enough? A data-centric image classification benchmark\n  for noisy and ambiguous label estimation\n\n  High-quality data is necessary for modern machine learning. However, the\nacquisition of such data is difficult due to noisy and ambiguous annotations of\nhumans. The aggregation of such annotations to determine the label of an image\nleads to a lower data quality. We propose a data-centric image classification\nbenchmark with ten real-world datasets and multiple annotations per image to\nallow researchers to investigate and quantify the impact of such data quality\nissues. With the benchmark we can study the impact of annotation costs and\n(semi-)supervised methods on the data quality for image classification by\napplying a novel methodology to a range of different algorithms and diverse\ndatasets. Our benchmark uses a two-phase approach via a data label improvement\nmethod in the first phase and a fixed evaluation model in the second phase.\nThereby, we give a measure for the relation between the input labeling effort\nand the performance of (semi-)supervised algorithms to enable a deeper insight\ninto how labels should be created for effective model training. Across\nthousands of experiments, we show that one annotation is not enough and that\nthe inclusion of multiple annotations allows for a better approximation of the\nreal underlying class distribution. We identify that hard labels can not\ncapture the ambiguity of the data and this might lead to the common issue of\noverconfident models. Based on the presented datasets, benchmarked methods, and\nanalysis, we create multiple research opportunities for the future directed at\nthe improvement of label noise estimation approaches, data annotation schemes,\nrealistic (semi-)supervised learning, or more reliable image collection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01203,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"Towards Robust Referring Video Object Segmentation with Cyclic\n  Relational Consensus\n\n  Referring Video Object Segmentation (R-VOS) is a challenging task that aims\nto segment an object in a video based on a linguistic expression. Most existing\nR-VOS methods have a critical assumption: the object referred to must appear in\nthe video. This assumption, which we refer to as semantic consensus, is often\nviolated in real-world scenarios, where the expression may be queried against\nfalse videos. In this work, we highlight the need for a robust R-VOS model that\ncan handle semantic mismatches. Accordingly, we propose an extended task called\nRobust R-VOS, which accepts unpaired video-text inputs. We tackle this problem\nby jointly modeling the primary R-VOS problem and its dual (text\nreconstruction). A structural text-to-text cycle constraint is introduced to\ndiscriminate semantic consensus between video-text pairs and impose it in\npositive pairs, thereby achieving multi-modal alignment from both positive and\nnegative pairs. Our structural constraint effectively addresses the challenge\nposed by linguistic diversity, overcoming the limitations of previous methods\nthat relied on the point-wise constraint. A new evaluation dataset,\nR\\textsuperscript{2}-Youtube-VOSis constructed to measure the model robustness.\nOur model achieves state-of-the-art performance on R-VOS benchmarks,\nRef-DAVIS17 and Ref-Youtube-VOS, and also our\nR\\textsuperscript{2}-Youtube-VOS~dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.10341,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"UFO: Unified Feature Optimization\n\n  This paper proposes a novel Unified Feature Optimization (UFO) paradigm for\ntraining and deploying deep models under real-world and large-scale scenarios,\nwhich requires a collection of multiple AI functions. UFO aims to benefit each\nsingle task with a large-scale pretraining on all tasks. Compared with the well\nknown foundation model, UFO has two different points of emphasis, i.e.,\nrelatively smaller model size and NO adaptation cost: 1) UFO squeezes a wide\nrange of tasks into a moderate-sized unified model in a multi-task learning\nmanner and further trims the model size when transferred to down-stream tasks.\n2) UFO does not emphasize transfer to novel tasks. Instead, it aims to make the\ntrimmed model dedicated for one or more already-seen task. With these two\ncharacteristics, UFO provides great convenience for flexible deployment, while\nmaintaining the benefits of large-scale pretraining. A key merit of UFO is that\nthe trimming process not only reduces the model size and inference consumption,\nbut also even improves the accuracy on certain tasks. Specifically, UFO\nconsiders the multi-task training and brings two-fold impact on the unified\nmodel: some closely related tasks have mutual benefits, while some tasks have\nconflicts against each other. UFO manages to reduce the conflicts and to\npreserve the mutual benefits through a novel Network Architecture Search (NAS)\nmethod. Experiments on a wide range of deep representation learning tasks\n(i.e., face recognition, person re-identification, vehicle re-identification\nand product retrieval) show that the model trimmed from UFO achieves higher\naccuracy than its single-task-trained counterpart and yet has smaller model\nsize, validating the concept of UFO. Besides, UFO also supported the release of\n17 billion parameters computer vision (CV) foundation model which is the\nlargest CV model in the industry.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.07895,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000034107,
      "text":"JPerceiver: Joint Perception Network for Depth, Pose and Layout\n  Estimation in Driving Scenes\n\n  Depth estimation, visual odometry (VO), and bird's-eye-view (BEV) scene\nlayout estimation present three critical tasks for driving scene perception,\nwhich is fundamental for motion planning and navigation in autonomous driving.\nThough they are complementary to each other, prior works usually focus on each\nindividual task and rarely deal with all three tasks together. A naive way is\nto accomplish them independently in a sequential or parallel manner, but there\nare many drawbacks, i.e., 1) the depth and VO results suffer from the inherent\nscale ambiguity issue; 2) the BEV layout is directly predicted from the\nfront-view image without using any depth-related information, although the\ndepth map contains useful geometry clues for inferring scene layouts. In this\npaper, we address these issues by proposing a novel joint perception framework\nnamed JPerceiver, which can simultaneously estimate scale-aware depth and VO as\nwell as BEV layout from a monocular video sequence. It exploits the cross-view\ngeometric transformation (CGT) to propagate the absolute scale from the road\nlayout to depth and VO based on a carefully-designed scale loss. Meanwhile, a\ncross-view and cross-modal transfer (CCT) module is devised to leverage the\ndepth clues for reasoning road and vehicle layout through an attention\nmechanism. JPerceiver can be trained in an end-to-end multi-task learning way,\nwhere the CGT scale loss and CCT module promote inter-task knowledge transfer\nto benefit feature learning of each task. Experiments on Argoverse, Nuscenes\nand KITTI show the superiority of JPerceiver over existing methods on all the\nabove three tasks in terms of accuracy, model size, and inference speed. The\ncode and models are available\nat~\\href{https:\/\/github.com\/sunnyHelen\/JPerceiver}{https:\/\/github.com\/sunnyHelen\/JPerceiver}.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.03684,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000141064,
      "text":"Unsupervised Domain Adaptive Fundus Image Segmentation with\n  Category-level Regularization\n\n  Existing unsupervised domain adaptation methods based on adversarial learning\nhave achieved good performance in several medical imaging tasks. However, these\nmethods focus only on global distribution adaptation and ignore distribution\nconstraints at the category level, which would lead to sub-optimal adaptation\nperformance. This paper presents an unsupervised domain adaptation framework\nbased on category-level regularization that regularizes the category\ndistribution from three perspectives. Specifically, for inter-domain category\nregularization, an adaptive prototype alignment module is proposed to align\nfeature prototypes of the same category in the source and target domains. In\naddition, for intra-domain category regularization, we tailored a\nregularization technique for the source and target domains, respectively. In\nthe source domain, a prototype-guided discriminative loss is proposed to learn\nmore discriminative feature representations by enforcing intra-class\ncompactness and inter-class separability, and as a complement to traditional\nsupervised loss. In the target domain, an augmented consistency category\nregularization loss is proposed to force the model to produce consistent\npredictions for augmented\/unaugmented target images, which encourages\nsemantically similar regions to be given the same label. Extensive experiments\non two publicly fundus datasets show that the proposed approach significantly\noutperforms other state-of-the-art comparison algorithms.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.01138,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges\n\n  This paper describes the fourth Affective Behavior Analysis in-the-wild\n(ABAW) Competition, held in conjunction with European Conference on Computer\nVision (ECCV), 2022. The 4th ABAW Competition is a continuation of the\nCompetitions held at IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017\nConferences, and aims at automatically analyzing affect. In the previous runs\nof this Competition, the Challenges targeted Valence-Arousal Estimation,\nExpression Classification and Action Unit Detection. This year the Competition\nencompasses two different Challenges: i) a Multi-Task-Learning one in which the\ngoal is to learn at the same time (i.e., in a multi-task learning setting) all\nthe three above mentioned tasks; and ii) a Learning from Synthetic Data one in\nwhich the goal is to learn to recognise the basic expressions from artificially\ngenerated data and generalise to real data. The Aff-Wild2 database is a large\nscale in-the-wild database and the first one that contains annotations for\nvalence and arousal, expressions and action units. This database is the basis\nfor the above Challenges. In more detail: i) s-Aff-Wild2 -- a static version of\nAff-Wild2 database -- has been constructed and utilized for the purposes of the\nMulti-Task-Learning Challenge; and ii) some specific frames-images from the\nAff-Wild2 database have been used in an expression manipulation manner for\ncreating the synthetic dataset, which is the basis for the Learning from\nSynthetic Data Challenge. In this paper, at first we present the two\nChallenges, along with the utilized corpora, then we outline the evaluation\nmetrics and finally present the baseline systems per Challenge, as well as\ntheir derived results. More information regarding the Competition can be found\nin the competition's website:\nhttps:\/\/ibug.doc.ic.ac.uk\/resources\/eccv-2023-4th-abaw\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.14498,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000046359,
      "text":"Reference-Guided Texture and Structure Inference for Image Inpainting\n\n  Existing learning-based image inpainting methods are still in challenge when\nfacing complex semantic environments and diverse hole patterns. The prior\ninformation learned from the large scale training data is still insufficient\nfor these situations. Reference images captured covering the same scenes share\nsimilar texture and structure priors with the corrupted images, which offers\nnew prospects for the image inpainting tasks. Inspired by this, we first build\na benchmark dataset containing 10K pairs of input and reference images for\nreference-guided inpainting. Then we adopt an encoder-decoder structure to\nseparately infer the texture and structure features of the input image\nconsidering their pattern discrepancy of texture and structure during\ninpainting. A feature alignment module is further designed to refine these\nfeatures of the input image with the guidance of a reference image. Both\nquantitative and qualitative evaluations demonstrate the superiority of our\nmethod over the state-of-the-art methods in terms of completing complex holes.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.08439,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000694063,
      "text":"Revisiting PatchMatch Multi-View Stereo for Urban 3D Reconstruction\n\n  In this paper, a complete pipeline for image-based 3D reconstruction of urban\nscenarios is proposed, based on PatchMatch Multi-View Stereo (MVS). Input\nimages are firstly fed into an off-the-shelf visual SLAM system to extract\ncamera poses and sparse keypoints, which are used to initialize PatchMatch\noptimization. Then, pixelwise depths and normals are iteratively computed in a\nmulti-scale framework with a novel depth-normal consistency loss term and a\nglobal refinement algorithm to balance the inherently local nature of\nPatchMatch. Finally, a large-scale point cloud is generated by back-projecting\nmulti-view consistent estimates in 3D. The proposed approach is carefully\nevaluated against both classical MVS algorithms and monocular depth networks on\nthe KITTI dataset, showing state of the art performances.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2207.1097,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":7,
    "pangram_prediction":{
      "ai_likelihood":0.0000000662,
      "text":"Opportunistic hip fracture risk prediction in Men from X-ray: Findings\n  from the Osteoporosis in Men (MrOS) Study\n\n  Osteoporosis is a common disease that increases fracture risk. Hip fractures,\nespecially in elderly people, lead to increased morbidity, decreased quality of\nlife and increased mortality. Being a silent disease before fracture,\nosteoporosis often remains undiagnosed and untreated. Areal bone mineral\ndensity (aBMD) assessed by dual-energy X-ray absorptiometry (DXA) is the\ngold-standard method for osteoporosis diagnosis and hence also for future\nfracture prediction (prognostic). However, the required special equipment is\nnot broadly available everywhere, in particular not to patients in developing\ncountries. We propose a deep learning classification model (FORM) that can\ndirectly predict hip fracture risk from either plain radiographs (X-ray) or 2D\nprojection images of computed tomography (CT) data. Our method is fully\nautomated and therefore well suited for opportunistic screening settings,\nidentifying high risk patients in a broader population without additional\nscreening. FORM was trained and evaluated on X-rays and CT projections from the\nOsteoporosis in Men (MrOS) study. 3108 X-rays (89 incident hip fractures) or\n2150 CTs (80 incident hip fractures) with a 80\/20 split were used. We show that\nFORM can correctly predict the 10-year hip fracture risk with a validation AUC\nof 81.44 +- 3.11% \/ 81.04 +- 5.54% (mean +- STD) including additional\ninformation like age, BMI, fall history and health background across a 5-fold\ncross validation on the X-ray and CT cohort, respectively. Our approach\nsignificantly (p < 0.01) outperforms previous methods like Cox\nProportional-Hazards Model and \\frax with 70.19 +- 6.58 and 74.72 +- 7.21\nrespectively on the X-ray cohort. Our model outperform on both cohorts hip aBMD\nbased predictions. We are confident that FORM can contribute on improving\nosteoporosis diagnosis at an early stage.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03779,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000056956,
      "text":"Sample hardness based gradient loss for long-tailed cervical cell\n  detection\n\n  Due to the difficulty of cancer samples collection and annotation, cervical\ncancer datasets usually exhibit a long-tailed data distribution. When training\na detector to detect the cancer cells in a WSI (Whole Slice Image) image\ncaptured from the TCT (Thinprep Cytology Test) specimen, head categories (e.g.\nnormal cells and inflammatory cells) typically have a much larger number of\nsamples than tail categories (e.g. cancer cells). Most existing\nstate-of-the-art long-tailed learning methods in object detection focus on\ncategory distribution statistics to solve the problem in the long-tailed\nscenario without considering the \"hardness\" of each sample. To address this\nproblem, in this work we propose a Grad-Libra Loss that leverages the gradients\nto dynamically calibrate the degree of hardness of each sample for different\ncategories, and re-balance the gradients of positive and negative samples. Our\nloss can thus help the detector to put more emphasis on those hard samples in\nboth head and tail categories. Extensive experiments on a long-tailed TCT WSI\nimage dataset show that the mainstream detectors, e.g. RepPoints, FCOS, ATSS,\nYOLOF, etc. trained using our proposed Gradient-Libra Loss, achieved much\nhigher (7.8%) mAP than that trained using cross-entropy classification loss.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.0296,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000136097,
      "text":"Memory-Guided Collaborative Attention for Nighttime Thermal Infrared\n  Image Colorization\n\n  Nighttime thermal infrared (NTIR) image colorization, also known as\ntranslation of NTIR images into daytime color images (NTIR2DC), is a promising\nresearch direction to facilitate nighttime scene perception for humans and\nintelligent systems under unfavorable conditions (e.g., complete darkness).\nHowever, previously developed methods have poor colorization performance for\nsmall sample classes. Moreover, reducing the high confidence noise in\npseudo-labels and addressing the problem of image gradient disappearance during\ntranslation are still under-explored, and keeping edges from being distorted\nduring translation is also challenging. To address the aforementioned issues,\nwe propose a novel learning framework called Memory-guided cOllaboRative\natteNtion Generative Adversarial Network (MornGAN), which is inspired by the\nanalogical reasoning mechanisms of humans. Specifically, a memory-guided sample\nselection strategy and adaptive collaborative attention loss are devised to\nenhance the semantic preservation of small sample categories. In addition, we\npropose an online semantic distillation module to mine and refine the\npseudo-labels of NTIR images. Further, conditional gradient repair loss is\nintroduced for reducing edge distortion during translation. Extensive\nexperiments on the NTIR2DC task show that the proposed MornGAN significantly\noutperforms other image-to-image translation methods in terms of semantic\npreservation and edge consistency, which helps improve the object detection\naccuracy remarkably.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.07929,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000030133,
      "text":"ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human\n  Activity Recognition in Videos\n\n  Human activity recognition is an emerging and important area in computer\nvision which seeks to determine the activity an individual or group of\nindividuals are performing. The applications of this field ranges from\ngenerating highlight videos in sports, to intelligent surveillance and gesture\nrecognition. Most activity recognition systems rely on a combination of\nconvolutional neural networks (CNNs) to perform feature extraction from the\ndata and recurrent neural networks (RNNs) to determine the time dependent\nnature of the data. This paper proposes and designs two transformer neural\nnetworks for human activity recognition: a recurrent transformer (ReT), a\nspecialized neural network used to make predictions on sequences of data, as\nwell as a vision transformer (ViT), a transformer optimized for extracting\nsalient features from images, to improve speed and scalability of activity\nrecognition. We have provided an extensive comparison of the proposed\ntransformer neural networks with the contemporary CNN and RNN-based human\nactivity recognition models in terms of speed and accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.13465,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000031789,
      "text":"Exploring Semantic Attributes from A Foundation Model for Federated\n  Learning of Disjoint Label Spaces\n\n  Conventional centralised deep learning paradigms are not feasible when data\nfrom different sources cannot be shared due to data privacy or transmission\nlimitation. To resolve this problem, federated learning has been introduced to\ntransfer knowledge across multiple sources (clients) with non-shared data while\noptimising a globally generalised central model (server). Existing federated\nlearning paradigms mostly focus on transferring holistic high-level knowledge\n(such as class) across models, which are closely related to specific objects of\ninterest so may suffer from inverse attack. In contrast, in this work, we\nconsider transferring mid-level semantic knowledge (such as attribute) which is\nnot sensitive to specific objects of interest and therefore is more\nprivacy-preserving and scalable. To this end, we formulate a new Federated\nZero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at\nmultiple local clients with non-shared local data and cumulatively aggregate a\nglobally generalised central model for deployment. To improve model\ndiscriminative ability, we propose to explore semantic knowledge augmentation\nfrom external knowledge for enriching the mid-level semantic space in FZSL.\nExtensive experiments on five zeroshot learning benchmark datasets validate the\neffectiveness of our approach for optimising a generalisable federated learning\nmodel with mid-level semantic knowledge transfer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09285,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000002318,
      "text":"Shadows Aren't So Dangerous After All: A Fast and Robust Defense Against\n  Shadow-Based Adversarial Attacks\n\n  Robust classification is essential in tasks like autonomous vehicle sign\nrecognition, where the downsides of misclassification can be grave. Adversarial\nattacks threaten the robustness of neural network classifiers, causing them to\nconsistently and confidently misidentify road signs. One such class of attack,\nshadow-based attacks, causes misidentifications by applying a natural-looking\nshadow to input images, resulting in road signs that appear natural to a human\nobserver but confusing for these classifiers. Current defenses against such\nattacks use a simple adversarial training procedure to achieve a rather low\n25\\% and 40\\% robustness on the GTSRB and LISA test sets, respectively. In this\npaper, we propose a robust, fast, and generalizable method, designed to defend\nagainst shadow attacks in the context of road sign recognition, that augments\nsource images with binary adaptive threshold and edge maps. We empirically show\nits robustness against shadow attacks, and reformulate the problem to show its\nsimilarity to $\\varepsilon$ perturbation-based attacks. Experimental results\nshow that our edge defense results in 78\\% robustness while maintaining 98\\%\nbenign test accuracy on the GTSRB test set, with similar results from our\nthreshold defense. Link to our code is in the paper.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.09266,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000021193,
      "text":"Diverse Video Captioning by Adaptive Spatio-temporal Attention\n\n  To generate proper captions for videos, the inference needs to identify\nrelevant concepts and pay attention to the spatial relationships between them\nas well as to the temporal development in the clip. Our end-to-end\nencoder-decoder video captioning framework incorporates two transformer-based\narchitectures, an adapted transformer for a single joint spatio-temporal video\nanalysis as well as a self-attention-based decoder for advanced text\ngeneration. Furthermore, we introduce an adaptive frame selection scheme to\nreduce the number of required incoming frames while maintaining the relevant\ncontent when training both transformers. Additionally, we estimate semantic\nconcepts relevant for video captioning by aggregating all ground truth captions\nof each sample. Our approach achieves state-of-the-art results on the MSVD, as\nwell as on the large-scale MSR-VTT and the VATEX benchmark datasets considering\nmultiple Natural Language Generation (NLG) metrics. Additional evaluations on\ndiversity scores highlight the expressiveness and diversity in the structure of\nour generated captions.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.05864,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"Face Morphing Attacks and Face Image Quality: The Effect of Morphing and\n  the Unsupervised Attack Detection by Quality\n\n  Morphing attacks are a form of presentation attacks that gathered increasing\nattention in recent years. A morphed image can be successfully verified to\nmultiple identities. This operation, therefore, poses serious security issues\nrelated to the ability of a travel or identity document to be verified to\nbelong to multiple persons. Previous works touched on the issue of the quality\nof morphing attack images, however, with the main goal of quantitatively\nproofing the realistic appearance of the produced morphing attacks. We theorize\nthat the morphing processes might have an effect on both, the perceptual image\nquality and the image utility in face recognition (FR) when compared to bona\nfide samples. Towards investigating this theory, this work provides an\nextensive analysis of the effect of morphing on face image quality, including\nboth general image quality measures and face image utility measures. This\nanalysis is not limited to a single morphing technique, but rather looks at six\ndifferent morphing techniques and five different data sources using ten\ndifferent quality measures. This analysis reveals consistent separability\nbetween the quality scores of morphing attack and bona fide samples measured by\ncertain quality measures. Our study goes further to build on this effect and\ninvestigate the possibility of performing unsupervised morphing attack\ndetection (MAD) based on quality scores. Our study looks intointra and\ninter-dataset detectability to evaluate the generalizability of such a\ndetection concept on different morphing techniques and bona fide sources. Our\nfinal results point out that a set of quality measures, such as MagFace and\nCNNNIQA, can be used to perform unsupervised and generalized MAD with a correct\nclassification accuracy of over 70%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.03232,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000000861,
      "text":"Driving Points Prediction For Abdominal Probabilistic Registration\n\n  Inter-patient abdominal registration has various applications, from\npharmakinematic studies to anatomy modeling. Yet, it remains a challenging\napplication due to the morphological heterogeneity and variability of the human\nabdomen. Among the various registration methods proposed for this task,\nprobabilistic displacement registration models estimate displacement\ndistribution for a subset of points by comparing feature vectors of points from\nthe two images. These probabilistic models are informative and robust while\nallowing large displacements by design. As the displacement distributions are\ntypically estimated on a subset of points (which we refer to as driving\npoints), due to computational requirements, we propose in this work to learn a\ndriving points predictor. Compared to previously proposed methods, the driving\npoints predictor is optimized in an end-to-end fashion to infer driving points\ntailored for a specific registration pipeline. We evaluate the impact of our\ncontribution on two different datasets corresponding to different modalities.\nSpecifically, we compared the performances of 6 different probabilistic\ndisplacement registration models when using a driving points predictor or one\nof 2 other standard driving points selection methods. The proposed method\nimproved performances in 11 out of 12 experiments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.1196,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"FusePose: IMU-Vision Sensor Fusion in Kinematic Space for Parametric\n  Human Pose Estimation\n\n  There exist challenging problems in 3D human pose estimation mission, such as\npoor performance caused by occlusion and self-occlusion. Recently, IMU-vision\nsensor fusion is regarded as valuable for solving these problems. However,\nprevious researches on the fusion of IMU and vision data, which is\nheterogeneous, fail to adequately utilize either IMU raw data or reliable\nhigh-level vision features. To facilitate a more efficient sensor fusion, in\nthis work we propose a framework called \\emph{FusePose} under a parametric\nhuman kinematic model. Specifically, we aggregate different information of IMU\nor vision data and introduce three distinctive sensor fusion approaches:\nNaiveFuse, KineFuse and AdaDeepFuse. NaiveFuse servers as a basic approach that\nonly fuses simplified IMU data and estimated 3D pose in euclidean space. While\nin kinematic space, KineFuse is able to integrate the calibrated and aligned\nIMU raw data with converted 3D pose parameters. AdaDeepFuse further develops\nthis kinematical fusion process to an adaptive and end-to-end trainable manner.\nComprehensive experiments with ablation studies demonstrate the rationality and\nsuperiority of the proposed framework. The performance of 3D human pose\nestimation is improved compared to the baseline result. On Total Capture\ndataset, KineFuse surpasses previous state-of-the-art which uses IMU only for\ntesting by 8.6\\%. AdaDeepFuse surpasses state-of-the-art which uses IMU for\nboth training and testing by 8.5\\%. Moreover, we validate the generalization\ncapability of our framework through experiments on Human3.6M dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.01844,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000008444,
      "text":"Multiclass ASMA vs Targeted PGD Attack in Image Segmentation\n\n  Deep learning networks have demonstrated high performance in a large variety\nof applications, such as image classification, speech recognition, and natural\nlanguage processing. However, there exists a major vulnerability exploited by\nthe use of adversarial attacks. An adversarial attack imputes images by\naltering the input image very slightly, making it nearly undetectable to the\nnaked eye, but results in a very different classification by the network. This\npaper explores the projected gradient descent (PGD) attack and the Adaptive\nMask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model using\ntwo types of architectures: MobileNetV3 and ResNet50, It was found that PGD was\nvery consistent in changing the segmentation to be its target while the\ngeneralization of ASMA to a multiclass target was not as effective. The\nexistence of such attack however puts all of image classification deep learning\nnetworks in danger of exploitation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02148,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"GPPF: A General Perception Pre-training Framework via Sparsely Activated\n  Multi-Task Learning\n\n  Pre-training over mixtured multi-task, multi-domain, and multi-modal data\nremains an open challenge in vision perception pre-training. In this paper, we\npropose GPPF, a General Perception Pre-training Framework, that pre-trains a\ntask-level dynamic network, which is composed by knowledge \"legos\" in each\nlayers, on labeled multi-task and multi-domain datasets. By inspecting humans'\ninnate ability to learn in complex environment, we recognize and transfer three\ncritical elements to deep networks: (1) simultaneous exposure to diverse\ncross-task and cross-domain information in each batch. (2) partitioned\nknowledge storage in separate lego units driven by knowledge sharing. (3)\nsparse activation of a subset of lego units for both pre-training and\ndownstream tasks. Noteworthy, the joint training of disparate vision tasks is\nnon-trivial due to their differences in input shapes, loss functions, output\nformats, data distributions, etc. Therefore, we innovatively develop a\nplug-and-play multi-task training algorithm, which supports Single Iteration\nMultiple Tasks (SIMT) concurrently training. SIMT lays the foundation of\npre-training with large-scale multi-task multi-domain datasets and is proved\nessential for stable training in our GPPF experiments. Excitingly, the\nexhaustive experiments show that, our GPPF-R50 model achieves significant\nimprovements of 2.5-5.8 over a strong baseline of the 8 pre-training tasks in\nGPPF-15M and harvests a range of SOTAs over the 22 downstream tasks with\nsimilar computation budgets. We also validate the generalization ability of\nGPPF to SOTA vision transformers with consistent improvements. These solid\nexperimental results fully prove the effective knowledge learning, storing,\nsharing, and transfer provided by our novel GPPF framework.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.05274,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000086758,
      "text":"Arbitrary Point Cloud Upsampling with Spherical Mixture of Gaussians\n\n  Generating dense point clouds from sparse raw data benefits downstream 3D\nunderstanding tasks, but existing models are limited to a fixed upsampling\nratio or to a short range of integer values. In this paper, we present\nAPU-SMOG, a Transformer-based model for Arbitrary Point cloud Upsampling (APU).\nThe sparse input is firstly mapped to a Spherical Mixture of Gaussians (SMOG)\ndistribution, from which an arbitrary number of points can be sampled. Then,\nthese samples are fed as queries to the Transformer decoder, which maps them\nback to the target surface. Extensive qualitative and quantitative evaluations\nshow that APU-SMOG outperforms state-of-the-art fixed-ratio methods, while\neffectively enabling upsampling with any scaling factor, including non-integer\nvalues, with a single trained model. The code is available at\nhttps:\/\/github.com\/apusmog\/apusmog\/\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.134,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Towards Explaining Demographic Bias through the Eyes of Face Recognition\n  Models\n\n  Biases inherent in both data and algorithms make the fairness of widespread\nmachine learning (ML)-based decision-making systems less than optimal. To\nimprove the trustfulness of such ML decision systems, it is crucial to be aware\nof the inherent biases in these solutions and to make them more transparent to\nthe public and developers. In this work, we aim at providing a set of\nexplainability tool that analyse the difference in the face recognition models'\nbehaviors when processing different demographic groups. We do that by\nleveraging higher-order statistical information based on activation maps to\nbuild explainability tools that link the FR models' behavior differences to\ncertain facial regions. The experimental results on two datasets and two face\nrecognition models pointed out certain areas of the face where the FR models\nreact differently for certain demographic groups compared to reference groups.\nThe outcome of these analyses interestingly aligns well with the results of\nstudies that analyzed the anthropometric differences and the human judgment\ndifferences on the faces of different demographic groups. This is thus the\nfirst study that specifically tries to explain the biased behavior of FR models\non different demographic groups and link it directly to the spatial facial\nfeatures. The code is publicly available here.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.02973,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000029802,
      "text":"An Efficient Person Clustering Algorithm for Open Checkout-free\n  Groceries\n\n  Open checkout-free grocery is the grocery store where the customers never\nhave to wait in line to check out. Developing a system like this is not trivial\nsince it faces challenges of recognizing the dynamic and massive flow of\npeople. In particular, a clustering method that can efficiently assign each\nsnapshot to the corresponding customer is essential for the system. In order to\naddress the unique challenges in the open checkout-free grocery, we propose an\nefficient and effective person clustering method. Specifically, we first\npropose a Crowded Sub-Graph (CSG) to localize the relationship among massive\nand continuous data streams. CSG is constructed by the proposed\nPick-Link-Weight (PLW) strategy, which \\textbf{picks} the nodes based on\ntime-space information, \\textbf{links} the nodes via trajectory information,\nand \\textbf{weighs} the links by the proposed von Mises-Fisher (vMF) similarity\nmetric. Then, to ensure that the method adapts to the dynamic and unseen person\nflow, we propose Graph Convolutional Network (GCN) with a simple Nearest\nNeighbor (NN) strategy to accurately cluster the instances of CSG. GCN is\nadopted to project the features into low-dimensional separable space, and NN is\nable to quickly produce a result in this space upon dynamic person flow. The\nexperimental results show that the proposed method outperforms other\nalternative algorithms in this scenario. In practice, the whole system has been\nimplemented and deployed in several real-world open checkout-free groceries.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12587,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Mitosis Detection, Fast and Slow: Robust and Efficient Detection of\n  Mitotic Figures\n\n  Counting of mitotic figures is a fundamental step in grading and\nprognostication of several cancers. However, manual mitosis counting is tedious\nand time-consuming. In addition, variation in the appearance of mitotic figures\ncauses a high degree of discordance among pathologists. With advances in deep\nlearning models, several automatic mitosis detection algorithms have been\nproposed but they are sensitive to {\\em domain shift} often seen in histology\nimages. We propose a robust and efficient two-stage mitosis detection\nframework, which comprises mitosis candidate segmentation ({\\em Detecting\nFast}) and candidate refinement ({\\em Detecting Slow}) stages. The proposed\ncandidate segmentation model, termed \\textit{EUNet}, is fast and accurate due\nto its architectural design. EUNet can precisely segment candidates at a lower\nresolution to considerably speed up candidate detection. Candidates are then\nrefined using a deeper classifier network, EfficientNet-B7, in the second\nstage. We make sure both stages are robust against domain shift by\nincorporating domain generalization methods. We demonstrate state-of-the-art\nperformance and generalizability of the proposed model on the three largest\npublicly available mitosis datasets, winning the two mitosis domain\ngeneralization challenge contests (MIDOG21 and MIDOG22). Finally, we showcase\nthe utility of the proposed algorithm by processing the TCGA breast cancer\ncohort (1,125 whole-slide images) to generate and release a repository of more\nthan 620K mitotic figures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.11191,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000054638,
      "text":"Towards cumulative race time regression in sports: I3D ConvNet transfer\n  learning in ultra-distance running events\n\n  Predicting an athlete's performance based on short footage is highly\nchallenging. Performance prediction requires high domain knowledge and enough\nevidence to infer an appropriate quality assessment. Sports pundits can often\ninfer this kind of information in real-time. In this paper, we propose\nregressing an ultra-distance runner cumulative race time (CRT), i.e., the time\nthe runner has been in action since the race start, by using only a few seconds\nof footage as input. We modified the I3D ConvNet backbone slightly and trained\na newly added regressor for that purpose. We use appropriate pre-processing of\nthe visual input to enable transfer learning from a specific runner. We show\nthat the resulting neural network can provide a remarkable performance for\nshort input footage: 18 minutes and a half mean absolute error in estimating\nthe CRT for runners who have been in action from 8 to 20 hours. Our methodology\nhas several favorable properties: it does not require a human expert to provide\nany insight, it can be used at any moment during the race by just observing a\nrunner, and it can inform the race staff about a runner at any given time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.11052,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000000331,
      "text":"IMPaSh: A Novel Domain-shift Resistant Representation for Colorectal\n  Cancer Tissue Classification\n\n  The appearance of histopathology images depends on tissue type, staining and\ndigitization procedure. These vary from source to source and are the potential\ncauses for domain-shift problems. Owing to this problem, despite the great\nsuccess of deep learning models in computational pathology, a model trained on\na specific domain may still perform sub-optimally when we apply them to another\ndomain. To overcome this, we propose a new augmentation called PatchShuffling\nand a novel self-supervised contrastive learning framework named IMPaSh for\npre-training deep learning models. Using these, we obtained a ResNet50 encoder\nthat can extract image representation resistant to domain-shift. We compared\nour derived representation against those acquired based on other\ndomain-generalization techniques by using them for the cross-domain\nclassification of colorectal tissue images. We show that the proposed method\noutperforms other traditional histology domain-adaptation and state-of-the-art\nself-supervised learning methods. Code is available at:\nhttps:\/\/github.com\/trinhvg\/IMPash .\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.10769,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000060267,
      "text":"PIFu for the Real World: A Self-supervised Framework to Reconstruct\n  Dressed Human from Single-view Images\n\n  It is very challenging to accurately reconstruct sophisticated human geometry\ncaused by various poses and garments from a single image. Recently, works based\non pixel-aligned implicit function (PIFu) have made a big step and achieved\nstate-of-the-art fidelity on image-based 3D human digitization. However, the\ntraining of PIFu relies heavily on expensive and limited 3D ground truth data\n(i.e. synthetic data), thus hindering its generalization to more diverse real\nworld images. In this work, we propose an end-to-end self-supervised network\nnamed SelfPIFu to utilize abundant and diverse in-the-wild images, resulting in\nlargely improved reconstructions when tested on unconstrained in-the-wild\nimages. At the core of SelfPIFu is the depth-guided volume-\/surface-aware\nsigned distance fields (SDF) learning, which enables self-supervised learning\nof a PIFu without access to GT mesh. The whole framework consists of a normal\nestimator, a depth estimator, and a SDF-based PIFu and better utilizes extra\ndepth GT during training. Extensive experiments demonstrate the effectiveness\nof our self-supervised framework and the superiority of using depth as input.\nOn synthetic data, our Intersection-Over-Union (IoU) achieves to 93.5%, 18%\nhigher compared with PIFuHD. For in-the-wild images, we conduct user studies on\nthe reconstructed results, the selection rate of our results is over 68%\ncompared with other state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.12462,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.000005828,
      "text":"Seg4Reg+: Consistency Learning between Spine Segmentation and Cobb Angle\n  Regression\n\n  Automated methods for Cobb angle estimation are of high demand for scoliosis\nassessment. Existing methods typically calculate the Cobb angle from landmark\nestimation, or simply combine the low-level task (e.g., landmark detection and\nspine segmentation) with the Cobb angle regression task, without fully\nexploring the benefits from each other. In this study, we propose a novel\nmulti-task framework, named Seg4Reg+, which jointly optimizes the segmentation\nand regression networks. We thoroughly investigate both local and global\nconsistency and knowledge transfer between each other. Specifically, we propose\nan attention regularization module leveraging class activation maps (CAMs) from\nimage-segmentation pairs to discover additional supervision in the regression\nnetwork, and the CAMs can serve as a region-of-interest enhancement gate to\nfacilitate the segmentation task in turn. Meanwhile, we design a novel triangle\nconsistency learning to train the two networks jointly for global optimization.\nThe evaluations performed on the public AASCE Challenge dataset demonstrate the\neffectiveness of each module and superior performance of our model to the\nstate-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2208.07479,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":8,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Context-Aware Streaming Perception in Dynamic Environments\n\n  Efficient vision works maximize accuracy under a latency budget. These works\nevaluate accuracy offline, one image at a time. However, real-time vision\napplications like autonomous driving operate in streaming settings, where\nground truth changes between inference start and finish. This results in a\nsignificant accuracy drop. Therefore, a recent work proposed to maximize\naccuracy in streaming settings on average. In this paper, we propose to\nmaximize streaming accuracy for every environment context. We posit that\nscenario difficulty influences the initial (offline) accuracy difference, while\nobstacle displacement in the scene affects the subsequent accuracy degradation.\nOur method, Octopus, uses these scenario properties to select configurations\nthat maximize streaming accuracy at test time. Our method improves tracking\nperformance (S-MOTA) by 7.4% over the conventional static approach. Further,\nperformance improvement using our method comes in addition to, and not instead\nof, advances in offline accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.08237,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000338753,
      "text":"Understanding the Impact of Image Quality and Distance of Objects to\n  Object Detection Performance\n\n  Deep learning has made great strides for object detection in images. The\ndetection accuracy and computational cost of object detection depend on the\nspatial resolution of an image, which may be constrained by both the camera and\nstorage considerations. Compression is often achieved by reducing either\nspatial or amplitude resolution or, at times, both, both of which have\nwell-known effects on performance. Detection accuracy also depends on the\ndistance of the object of interest from the camera. Our work examines the\nimpact of spatial and amplitude resolution, as well as object distance, on\nobject detection accuracy and computational cost. We develop a\nresolution-adaptive variant of YOLOv5 (RA-YOLO), which varies the number of\nscales in the feature pyramid and detection head based on the spatial\nresolution of the input image. To train and evaluate this new method, we\ncreated a dataset of images with diverse spatial and amplitude resolutions by\ncombining images from the TJU and Eurocity datasets and generating different\nresolutions by applying spatial resizing and compression. We first show that\nRA-YOLO achieves a good trade-off between detection accuracy and inference time\nover a large range of spatial resolutions. We then evaluate the impact of\nspatial and amplitude resolutions on object detection accuracy using the\nproposed RA-YOLO model. We demonstrate that the optimal spatial resolution that\nleads to the highest detection accuracy depends on the 'tolerated' image size.\nWe further assess the impact of the distance of an object to the camera on the\ndetection accuracy and show that higher spatial resolution enables a greater\ndetection range. These results provide important guidelines for choosing the\nimage spatial resolution and compression settings predicated on available\nbandwidth, storage, desired inference time, and\/or desired detection range, in\npractical applications.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02691,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Statistical Foundation Behind Machine Learning and Its Impact on\n  Computer Vision\n\n  This paper revisits the principle of uniform convergence in statistical\nlearning, discusses how it acts as the foundation behind machine learning, and\nattempts to gain a better understanding of the essential problem that current\ndeep learning algorithms are solving. Using computer vision as an example\ndomain in machine learning, the discussion shows that recent research trends in\nleveraging increasingly large-scale data to perform pre-training for\nrepresentation learning are largely to reduce the discrepancy between a\npractically tractable empirical loss and its ultimately desired but intractable\nexpected loss. Furthermore, this paper suggests a few future research\ndirections, predicts the continued increase of data, and argues that more\nfundamental research is needed on robustness, interpretability, and reasoning\ncapabilities of machine learning by incorporating structure and knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02692,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Deep Learning Assisted Optimization for 3D Reconstruction from Single 2D\n  Line Drawings\n\n  In this paper, we revisit the long-standing problem of automatic\nreconstruction of 3D objects from single line drawings. Previous\noptimization-based methods can generate compact and accurate 3D models, but\ntheir success rates depend heavily on the ability to (i) identifying a\nsufficient set of true geometric constraints, and (ii) choosing a good initial\nvalue for the numerical optimization. In view of these challenges, we propose\nto train deep neural networks to detect pairwise relationships among geometric\nentities (i.e., edges) in the 3D object, and to predict initial depth value of\nthe vertices. Our experiments on a large dataset of CAD models show that, by\nleveraging deep learning in a geometric constraint solving pipeline, the\nsuccess rate of optimization-based 3D reconstruction can be significantly\nimproved.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12221,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000251333,
      "text":"Hand Hygiene Assessment via Joint Step Segmentation and Key Action\n  Scorer\n\n  Hand hygiene is a standard six-step hand-washing action proposed by the World\nHealth Organization (WHO). However, there is no good way to supervise medical\nstaff to do hand hygiene, which brings the potential risk of disease spread.\nExisting action assessment works usually make an overall quality prediction on\nan entire video. However, the internal structures of hand hygiene action are\nimportant in hand hygiene assessment. Therefore, we propose a novel\nfine-grained learning framework to perform step segmentation and key action\nscorer in a joint manner for accurate hand hygiene assessment. Existing\ntemporal segmentation methods usually employ multi-stage convolutional network\nto improve the segmentation robustness, but easily lead to over-segmentation\ndue to the lack of the long-range dependence. To address this issue, we design\na multi-stage convolution-transformer network for step segmentation. Based on\nthe observation that each hand-washing step involves several key actions which\ndetermine the hand-washing quality, we design a set of key action scorers to\nevaluate the quality of key actions in each step. In addition, there lacks a\nunified dataset in hand hygiene assessment. Therefore, under the supervision of\nmedical staff, we contribute a video dataset that contains 300 video sequences\nwith fine-grained annotations. Extensive experiments on the dataset suggest\nthat our method well assesses hand hygiene videos and achieves outstanding\nperformance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11972,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000024173,
      "text":"Ground then Navigate: Language-guided Navigation in Dynamic Scenes\n\n  We investigate the Vision-and-Language Navigation (VLN) problem in the\ncontext of autonomous driving in outdoor settings. We solve the problem by\nexplicitly grounding the navigable regions corresponding to the textual\ncommand. At each timestamp, the model predicts a segmentation mask\ncorresponding to the intermediate or the final navigable region. Our work\ncontrasts with existing efforts in VLN, which pose this task as a node\nselection problem, given a discrete connected graph corresponding to the\nenvironment. We do not assume the availability of such a discretised map. Our\nwork moves towards continuity in action space, provides interpretability\nthrough visual feedback and allows VLN on commands requiring finer manoeuvres\nlike \"park between the two cars\". Furthermore, we propose a novel meta-dataset\nCARLA-NAV to allow efficient training and validation. The dataset comprises\npre-recorded training sequences and a live environment for validation and\ntesting. We provide extensive qualitative and quantitive empirical results to\nvalidate the efficacy of the proposed approach.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12557,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Device-friendly Guava fruit and leaf disease detection using deep\n  learning\n\n  This work presents a deep learning-based plant disease diagnostic system\nusing images of fruits and leaves. Five state-of-the-art convolutional neural\nnetworks (CNN) have been employed for implementing the system. Hitherto model\naccuracy has been the focus for such applications and model optimization has\nnot been accounted for the model to be applicable to end-user devices. Two\nmodel quantization techniques such as float16 and dynamic range quantization\nhave been applied to the five state-of-the-art CNN architectures. The study\nshows that the quantized GoogleNet model achieved the size of 0.143 MB with an\naccuracy of 97%, which is the best candidate model considering the size\ncriterion. The EfficientNet model achieved the size of 4.2MB with an accuracy\nof 99%, which is the best model considering the performance criterion. The\nsource codes are available at\nhttps:\/\/github.com\/CompostieAI\/Guava-disease-detection.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.14225,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000082122,
      "text":"Road Rutting Detection using Deep Learning on Images\n\n  Road rutting is a severe road distress that can cause premature failure of\nroad incurring early and costly maintenance costs. Research on road damage\ndetection using image processing techniques and deep learning are being\nactively conducted in the past few years. However, these researches are mostly\nfocused on detection of cracks, potholes, and their variants. Very few research\nhas been done on the detection of road rutting. This paper proposes a novel\nroad rutting dataset comprising of 949 images and provides both object level\nand pixel level annotations. Object detection models and semantic segmentation\nmodels were deployed to detect road rutting on the proposed dataset, and\nquantitative and qualitative analysis of model predictions were done to\nevaluate model performance and identify challenges faced in the detection of\nroad rutting using the proposed method. Object detection model YOLOX-s achieves\nmAP@IoU=0.5 of 61.6% and semantic segmentation model PSPNet (Resnet-50)\nachieves IoU of 54.69 and accuracy of 72.67, thus providing a benchmark\naccuracy for similar work in future. The proposed road rutting dataset and the\nresults of our research study will help accelerate the research on detection of\nroad rutting using deep learning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.09449,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000598033,
      "text":"Data-Centric AI Paradigm Based on Application-Driven Fine-Grained\n  Dataset Design\n\n  Deep learning has a wide range of applications in industrial scenario, but\nreducing false alarm (FA) remains a major difficulty. Optimizing network\narchitecture or network parameters is used to tackle this challenge in academic\ncircles, while ignoring the essential characteristics of data in application\nscenarios, which often results in increased FA in new scenarios. In this paper,\nwe propose a novel paradigm for fine-grained design of datasets, driven by\nindustrial applications. We flexibly select positive and negative sample sets\naccording to the essential features of the data and application requirements,\nand add the remaining samples to the training set as uncertainty classes. We\ncollect more than 10,000 mask-wearing recognition samples covering various\napplication scenarios as our experimental data. Compared with the traditional\ndata design methods, our method achieves better results and effectively reduces\nFA. We make all contributions available to the research community for broader\nuse. The contributions will be available at\nhttps:\/\/github.com\/huh30\/OpenDatasets.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.12386,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000007285,
      "text":"TAD: A Large-Scale Benchmark for Traffic Accidents Detection from Video\n  Surveillance\n\n  Automatic traffic accidents detection has appealed to the machine vision\ncommunity due to its implications on the development of autonomous intelligent\ntransportation systems (ITS) and importance to traffic safety. Most previous\nstudies on efficient analysis and prediction of traffic accidents, however,\nhave used small-scale datasets with limited coverage, which limits their effect\nand applicability. Existing datasets in traffic accidents are either\nsmall-scale, not from surveillance cameras, not open-sourced, or not built for\nfreeway scenes. Since accidents happened in freeways tend to cause serious\ndamage and are too fast to catch the spot. An open-sourced datasets targeting\non freeway traffic accidents collected from surveillance cameras is in great\nneed and of practical importance. In order to help the vision community address\nthese shortcomings, we endeavor to collect video data of real traffic accidents\nthat covered abundant scenes. After integration and annotation by various\ndimensions, a large-scale traffic accidents dataset named TAD is proposed in\nthis work. Various experiments on image classification, object detection, and\nvideo classification tasks, using public mainstream vision algorithms or\nframeworks are conducted in this work to demonstrate performance of different\nmethods. The proposed dataset together with the experimental results are\npresented as a new benchmark to improve computer vision research, especially in\nITS.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.09483,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000047353,
      "text":"Diffusion Unit: Interpretable Edge Enhancement and Suppression Learning\n  for 3D Point Cloud Segmentation\n\n  3D point clouds are discrete samples of continuous surfaces which can be used\nfor various applications. However, the lack of true connectivity information,\ni.e., edge information, makes point cloud recognition challenging. Recent\nedge-aware methods incorporate edge modeling into network designs to better\ndescribe local structures. Although these methods show that incorporating edge\ninformation is beneficial, how edge information helps remains unclear, making\nit difficult for users to analyze its usefulness. To shed light on this issue,\nin this study, we propose a new algorithm called Diffusion Unit (DU) that\nhandles edge information in a principled and interpretable manner while\nproviding decent improvement. First, we theoretically show that DU learns to\nperform task-beneficial edge enhancement and suppression. Second, we\nexperimentally observe and verify the edge enhancement and suppression\nbehavior. Third, we empirically demonstrate that this behavior contributes to\nperformance improvement. Extensive experiments and analyses performed on\nchallenging benchmarks verify the effectiveness of DU. Specifically, our method\nachieves state-of-the-art performance in object part segmentation using\nShapeNet part and scene segmentation using S3DIS. Our source code is available\nat https:\/\/github.com\/martianxiu\/DiffusionUnit.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00893,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001987,
      "text":"Combining Efficient and Precise Sign Language Recognition: Good pose\n  estimation library is all you need\n\n  Sign language recognition could significantly improve the user experience for\nd\/Deaf people with the general consumer technology, such as IoT devices or\nvideoconferencing. However, current sign language recognition architectures are\nusually computationally heavy and require robust GPU-equipped hardware to run\nin real-time. Some models aim for lower-end devices (such as smartphones) by\nminimizing their size and complexity, which leads to worse accuracy. This\nhighly scrutinizes accurate in-the-wild applications. We build upon the SPOTER\narchitecture, which belongs to the latter group of light methods, as it came\nclose to the performance of large models employed for this task. By\nsubstituting its original third-party pose estimation module with the MediaPipe\nlibrary, we achieve an overall state-of-the-art result on the WLASL100 dataset.\nSignificantly, our method beats previous larger architectures while still being\ntwice as computationally efficient and almost $11$ times faster on inference\nwhen compared to a relevant benchmark. To demonstrate our method's combined\nefficiency and precision, we built an online demo that enables users to\ntranslate sign lemmas of American sign language in their browsers. This is the\nfirst publicly available online application demonstrating this task to the best\nof our knowledge.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.11448,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000036425,
      "text":"Rethinking Performance Gains in Image Dehazing Networks\n\n  Image dehazing is an active topic in low-level vision, and many image\ndehazing networks have been proposed with the rapid development of deep\nlearning. Although these networks' pipelines work fine, the key mechanism to\nimproving image dehazing performance remains unclear. For this reason, we do\nnot target to propose a dehazing network with fancy modules; rather, we make\nminimal modifications to popular U-Net to obtain a compact dehazing network.\nSpecifically, we swap out the convolutional blocks in U-Net for residual blocks\nwith the gating mechanism, fuse the feature maps of main paths and skip\nconnections using the selective kernel, and call the resulting U-Net variant\ngUNet. As a result, with a significantly reduced overhead, gUNet is superior to\nstate-of-the-art methods on multiple image dehazing datasets. Finally, we\nverify these key designs to the performance gain of image dehazing networks\nthrough extensive ablation studies.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.09857,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Fine-grained Classification of Solder Joints with {\\alpha}-skew\n  Jensen-Shannon Divergence\n\n  Solder joint inspection (SJI) is a critical process in the production of\nprinted circuit boards (PCB). Detection of solder errors during SJI is quite\nchallenging as the solder joints have very small sizes and can take various\nshapes. In this study, we first show that solders have low feature diversity,\nand that the SJI can be carried out as a fine-grained image classification task\nwhich focuses on hard-to-distinguish object classes. To improve the\nfine-grained classification accuracy, penalizing confident model predictions by\nmaximizing entropy was found useful in the literature. Inline with this\ninformation, we propose using the {\\alpha}-skew Jensen-Shannon divergence\n({\\alpha}-JS) for penalizing the confidence in model predictions. We compare\nthe {\\alpha}-JS regularization with both existing entropyregularization based\nmethods and the methods based on attention mechanism, segmentation techniques,\ntransformer models, and specific loss functions for fine-grained image\nclassification tasks. We show that the proposed approach achieves the highest\nF1-score and competitive accuracy for different models in the finegrained\nsolder joint classification task. Finally, we visualize the activation maps and\nshow that with entropy-regularization, more precise class-discriminative\nregions are localized, which are also more resilient to noise. Code will be\nmade available here upon acceptance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.02492,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000020862,
      "text":"Surya Namaskar: real-time advanced yoga pose recognition and correction\n  for smart healthcare\n\n  Nowadays, yoga has gained worldwide attention because of increasing levels of\nstress in the modern way of life, and there are many ways or resources to learn\nyoga. The word yoga means a deep connection between the mind and body. Today\nthere is substantial Medical and scientific evidence to show that the very\nfundamentals of the activity of our brain, our chemistry even our genetic\ncontent can be changed by practicing different systems of yoga. Suryanamaskar,\nalso known as salute to the sun, is a yoga practice that combines eight\ndifferent forms and 12 asanas(4 asana get repeated) devoted to the Hindu Sun\nGod, Surya. Suryanamaskar offers a number of health benefits such as\nstrengthening muscles and helping to control blood sugar levels. Here the\nMediapipe Library is used to analyze Surya namaskar situations. Standing is\ndetected in real time with advanced software, as one performs Surya namaskar in\nfront of the camera. The class divider identifies the form as one of the\nfollowing: Pranamasana, Hasta Padasana, Hasta Uttanasana, Ashwa - Sanchalan\nasana, Ashtanga Namaskar, Dandasana, or Bhujangasana and Svanasana. Deep\nlearning-based techniques(CNN) are used to develop this model with model\naccuracy of 98.68 percent and an accuracy score of 0.75 to detect correct yoga\n(Surya Namaskar ) posture. With this method, the users can practice the desired\npose and can check if the pose that the person is doing is correct or not. It\nwill help in doing all the different poses of surya namaskar correctly and\nincrease the efficiency of the yoga practitioner. This paper describes the\nwhole framework which is to be implemented in the model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.1025,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000075168,
      "text":"Query-Guided Networks for Few-shot Fine-grained Classification and\n  Person Search\n\n  Few-shot fine-grained classification and person search appear as distinct\ntasks and literature has treated them separately. But a closer look unveils\nimportant similarities: both tasks target categories that can only be\ndiscriminated by specific object details; and the relevant models should\ngeneralize to new categories, not seen during training.\n  We propose a novel unified Query-Guided Network (QGN) applicable to both\ntasks. QGN consists of a Query-guided Siamese-Squeeze-and-Excitation subnetwork\nwhich re-weights both the query and gallery features across all network layers,\na Query-guided Region Proposal subnetwork for query-specific localisation, and\na Query-guided Similarity subnetwork for metric learning.\n  QGN improves on a few recent few-shot fine-grained datasets, outperforming\nother techniques on CUB by a large margin. QGN also performs competitively on\nthe person search CUHK-SYSU and PRW datasets, where we perform in-depth\nanalysis.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.13071,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000002649,
      "text":"Diversified Dynamic Routing for Vision Tasks\n\n  Deep learning models for vision tasks are trained on large datasets under the\nassumption that there exists a universal representation that can be used to\nmake predictions for all samples. Whereas high complexity models are proven to\nbe capable of learning such representations, a mixture of experts trained on\nspecific subsets of the data can infer the labels more efficiently. However\nusing mixture of experts poses two new problems, namely (i) assigning the\ncorrect expert at inference time when a new unseen sample is presented. (ii)\nFinding the optimal partitioning of the training data, such that the experts\nrely the least on common features. In Dynamic Routing (DR) a novel architecture\nis proposed where each layer is composed of a set of experts, however without\naddressing the two challenges we demonstrate that the model reverts to using\nthe same subset of experts.\n  In our method, Diversified Dynamic Routing (DivDR) the model is explicitly\ntrained to solve the challenge of finding relevant partitioning of the data and\nassigning the correct experts in an unsupervised approach. We conduct several\nexperiments on semantic segmentation on Cityscapes and object detection and\ninstance segmentation on MS-COCO showing improved performance over several\nbaselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.08647,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000005629,
      "text":"Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet\n  Recognition through the Lens of Robustness\n\n  Surgical action triplet recognition provides a better understanding of the\nsurgical scene. This task is of high relevance as it provides the surgeon with\ncontext-aware support and safety. The current go-to strategy for improving\nperformance is the development of new network mechanisms. However, the\nperformance of current state-of-the-art techniques is substantially lower than\nother surgical tasks. Why is this happening? This is the question that we\naddress in this work. We present the first study to understand the failure of\nexisting deep learning models through the lens of robustness and\nexplainability. Firstly, we study current existing models under weak and strong\n$\\delta-$perturbations via an adversarial optimisation scheme. We then analyse\nthe failure modes via feature based explanations. Our study reveals that the\nkey to improving performance and increasing reliability is in the core and\nspurious attributes. Our work opens the door to more trustworthy and reliable\ndeep learning models in surgical data science.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.01881,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Semi-Supervised Domain Adaptation by Similarity based Pseudo-label\n  Injection\n\n  One of the primary challenges in Semi-supervised Domain Adaptation (SSDA) is\nthe skewed ratio between the number of labeled source and target samples,\ncausing the model to be biased towards the source domain. Recent works in SSDA\nshow that aligning only the labeled target samples with the source samples\npotentially leads to incomplete domain alignment of the target domain to the\nsource domain. In our approach, to align the two domains, we leverage\ncontrastive losses to learn a semantically meaningful and a domain agnostic\nfeature space using the supervised samples from both domains. To mitigate\nchallenges caused by the skewed label ratio, we pseudo-label the unlabeled\ntarget samples by comparing their feature representation to those of the\nlabeled samples from both the source and target domains. Furthermore, to\nincrease the support of the target domain, these potentially noisy\npseudo-labels are gradually injected into the labeled target dataset over the\ncourse of training. Specifically, we use a temperature scaled cosine similarity\nmeasure to assign a soft pseudo-label to the unlabeled target samples.\nAdditionally, we compute an exponential moving average of the soft\npseudo-labels for each unlabeled sample. These pseudo-labels are progressively\ninjected or removed) into the (from) the labeled target dataset based on a\nconfidence threshold to supplement the alignment of the source and target\ndistributions. Finally, we use a supervised contrastive loss on the labeled and\npseudo-labeled datasets to align the source and target distributions. Using our\nproposed approach, we showcase state-of-the-art performance on SSDA benchmarks\n- Office-Home, DomainNet and Office-31.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.10385,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000044703,
      "text":"Long-Lived Accurate Keypoints in Event Streams\n\n  We present a novel end-to-end approach to keypoint detection and tracking in\nan event stream that provides better precision and much longer keypoint tracks\nthan previous methods. This is made possible by two contributions working\ntogether.\n  First, we propose a simple procedure to generate stable keypoint labels,\nwhich we use to train a recurrent architecture. This training data results in\ndetections that are very consistent over time.\n  Moreover, we observe that previous methods for keypoint detection work on a\nrepresentation (such as the time surface) that integrates events over a period\nof time. Since this integration is required, we claim it is better to predict\nthe keypoints' trajectories for the time period rather than single locations,\nas done in previous approaches. We predict these trajectories in the form of a\nseries of heatmaps for the integration time period. This improves the keypoint\nlocalization.\n  Our architecture can also be kept very simple, which results in very fast\ninference times. We demonstrate our approach on the HVGA ATIS Corner dataset as\nwell as \"The Event-Camera Dataset and Simulator\" dataset, and show it results\nin keypoint tracks that are three times longer and nearly twice as accurate as\nthe best previous state-of-the-art methods. We believe our approach can be\ngeneralized to other event-based camera problems, and we release our source\ncode to encourage other authors to explore it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2209.0921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":9,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Use Classifier as Generator\n\n  Image recognition\/classification is a widely studied problem, but its reverse\nproblem, image generation, has drawn much less attention until recently. But\nthe vast majority of current methods for image generation require\ntraining\/retraining a classifier and\/or a generator with certain constraints,\nwhich can be hard to achieve. In this paper, we propose a simple approach to\ndirectly use a normally trained classifier to generate images. We evaluate our\nmethod on MNIST and show that it produces recognizable results for human eyes\nwith limited quality with experiments.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.1357,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.4315863715,
      "text":"Strong-TransCenter: Improved Multi-Object Tracking based on Transformers\n  with Dense Representations\n\n  Transformer networks have been a focus of research in many fields in recent\nyears, being able to surpass the state-of-the-art performance in different\ncomputer vision tasks. However, in the task of Multiple Object Tracking (MOT),\nleveraging the power of Transformers remains relatively unexplored. Among the\npioneering efforts in this domain, TransCenter, a Transformer-based MOT\narchitecture with dense object queries, demonstrated exceptional tracking\ncapabilities while maintaining reasonable runtime. Nonetheless, one critical\naspect in MOT, track displacement estimation, presents room for enhancement to\nfurther reduce association errors. In response to this challenge, our paper\nintroduces a novel improvement to TransCenter. We propose a post-processing\nmechanism grounded in the Track-by-Detection paradigm, aiming to refine the\ntrack displacement estimation. Our approach involves the integration of a\ncarefully designed Kalman filter, which incorporates Transformer outputs into\nmeasurement error estimation, and the use of an embedding network for target\nre-identification. This combined strategy yields substantial improvement in the\naccuracy and robustness of the tracking process. We validate our contributions\nthrough comprehensive experiments on the MOTChallenge datasets MOT17 and MOT20,\nwhere our proposed approach outperforms other Transformer-based trackers. The\ncode is publicly available at: https:\/\/github.com\/amitgalor18\/STC_Tracker\n",
      "prediction":"Possibly AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.1231,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000004967,
      "text":"Tools for Extracting Spatio-Temporal Patterns in Meteorological Image\n  Sequences: From Feature Engineering to Attention-Based Neural Networks\n\n  Atmospheric processes involve both space and time. This is why human analysis\nof atmospheric imagery can often extract more information from animated loops\nof image sequences than from individual images. Automating such an analysis\nrequires the ability to identify spatio-temporal patterns in image sequences\nwhich is a very challenging task, because of the endless possibilities of\npatterns in both space and time. In this paper we review different concepts and\ntechniques that are useful to extract spatio-temporal context specifically for\nmeteorological applications. In this survey we first motivate the need for\nthese approaches in meteorology using two applications, solar forecasting and\ndetecting convection from satellite imagery. Then we provide an overview of\nmany different concepts and techniques that are helpful for the interpretation\nof meteorological image sequences, such as (1) feature engineering methods to\nstrengthen the desired signal in the input, using meteorological knowledge,\nclassic image processing, harmonic analysis and topological data analysis (2)\nexplain how different convolution filters (2D\/3D\/LSTM-convolution) can be\nutilized strategically in convolutional neural network architectures to find\npatterns in both space and time (3) discuss the powerful new concept of\n'attention' in neural networks and the powerful abilities it brings to the\ninterpretation of image sequences (4) briefly survey strategies from\nunsupervised, self-supervised and transfer learning to reduce the need for\nlarge labeled datasets. We hope that presenting an overview of these tools -\nmany of which are underutilized - will help accelerate progress in this area.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.05567,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000025498,
      "text":"Global Spectral Filter Memory Network for Video Object Segmentation\n\n  This paper studies semi-supervised video object segmentation through boosting\nintra-frame interaction. Recent memory network-based methods focus on\nexploiting inter-frame temporal reference while paying little attention to\nintra-frame spatial dependency. Specifically, these segmentation model tends to\nbe susceptible to interference from unrelated nontarget objects in a certain\nframe. To this end, we propose Global Spectral Filter Memory network (GSFM),\nwhich improves intra-frame interaction through learning long-term spatial\ndependencies in the spectral domain. The key components of GSFM is 2D (inverse)\ndiscrete Fourier transform for spatial information mixing. Besides, we\nempirically find low frequency feature should be enhanced in encoder (backbone)\nwhile high frequency for decoder (segmentation head). We attribute this to\nsemantic information extracting role for encoder and fine-grained details\nhighlighting role for decoder. Thus, Low (High) Frequency Module is proposed to\nfit this circumstance. Extensive experiments on the popular DAVIS and\nYouTube-VOS benchmarks demonstrate that GSFM noticeably outperforms the\nbaseline method and achieves state-of-the-art performance. Besides, extensive\nanalysis shows that the proposed modules are reasonable and of great\ngeneralization ability. Our source code is available at\nhttps:\/\/github.com\/workforai\/GSFM.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.14899,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000001325,
      "text":"Learning a Task-specific Descriptor for Robust Matching of 3D Point\n  Clouds\n\n  Existing learning-based point feature descriptors are usually task-agnostic,\nwhich pursue describing the individual 3D point clouds as accurate as possible.\nHowever, the matching task aims at describing the corresponding points\nconsistently across different 3D point clouds. Therefore these too accurate\nfeatures may play a counterproductive role due to the inconsistent point\nfeature representations of correspondences caused by the unpredictable noise,\npartiality, deformation, \\etc, in the local geometry. In this paper, we propose\nto learn a robust task-specific feature descriptor to consistently describe the\ncorrect point correspondence under interference. Born with an Encoder and a\nDynamic Fusion module, our method EDFNet develops from two aspects. First, we\naugment the matchability of correspondences by utilizing their repetitive local\nstructure. To this end, a special encoder is designed to exploit two input\npoint clouds jointly for each point descriptor. It not only captures the local\ngeometry of each point in the current point cloud by convolution, but also\nexploits the repetitive structure from paired point cloud by Transformer.\nSecond, we propose a dynamical fusion module to jointly use different scale\nfeatures. There is an inevitable struggle between robustness and\ndiscriminativeness of the single scale feature. Specifically, the small scale\nfeature is robust since little interference exists in this small receptive\nfield. But it is not sufficiently discriminative as there are many repetitive\nlocal structures within a point cloud. Thus the resultant descriptors will lead\nto many incorrect matches. In contrast, the large scale feature is more\ndiscriminative by integrating more neighborhood information. ...\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.0456,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000012914,
      "text":"Visually Similar Products Retrieval for Shopsy\n\n  Visual search is of great assistance in reseller commerce, especially for\nnon-tech savvy users with affinity towards regional languages. It allows\nresellers to accurately locate the products that they seek, unlike textual\nsearch which recommends products from head brands. Product attributes available\nin e-commerce have a great potential for building better visual search systems\nas they capture fine grained relations between data points. In this work, we\ndesign a visual search system for reseller commerce using a multi-task learning\napproach. We also highlight and address the challenges like image compression,\ncropping, scribbling on the image, etc, faced in reseller commerce. Our model\nconsists of three different tasks: attribute classification, triplet ranking\nand variational autoencoder (VAE). Masking technique is used for designing the\nattribute classification. Next, we introduce an offline triplet mining\ntechnique which utilizes information from multiple attributes to capture\nrelative order within the data. This technique displays a better performance\ncompared to the traditional triplet mining baseline, which uses single\nlabel\/attribute information. We also compare and report incremental gain\nachieved by our unified multi-task model over each individual task separately.\nThe effectiveness of our method is demonstrated using the in-house dataset of\nproduct images from the Lifestyle business-unit of Flipkart, India's largest\ne-commerce company. To efficiently retrieve the images in production, we use\nthe Approximate Nearest Neighbor (ANN) index. Finally, we highlight our\nproduction environment constraints and present the design choices and\nexperiments conducted to select a suitable ANN index.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.1282,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000015895,
      "text":"An Interpretable Deep Semantic Segmentation Method for Earth Observation\n\n  Earth observation is fundamental for a range of human activities including\nflood response as it offers vital information to decision makers. Semantic\nsegmentation plays a key role in mapping the raw hyper-spectral data coming\nfrom the satellites into a human understandable form assigning class labels to\neach pixel. In this paper, we introduce a prototype-based interpretable deep\nsemantic segmentation (IDSS) method, which is highly accurate as well as\ninterpretable. Its parameters are in orders of magnitude less than the number\nof parameters used by deep networks such as U-Net and are clearly interpretable\nby humans. The proposed here IDSS offers a transparent structure that allows\nusers to inspect and audit the algorithm's decision. Results have demonstrated\nthat IDSS could surpass other algorithms, including U-Net, in terms of IoU\n(Intersection over Union) total water and Recall total water. We used\nWorldFloods data set for our experiments and plan to use the semantic\nsegmentation results combined with masks for permanent water to detect flood\nevents.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.07396,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010928,
      "text":"Caption supervision enables robust learners\n\n  Vision language (VL) models like CLIP are robust to natural distribution\nshifts, in part because CLIP learns on unstructured data using a technique\ncalled caption supervision; the model inteprets image-linked texts as\nground-truth labels. In a carefully controlled comparison study, we show that\ncaption-supervised CNNs trained on a standard cross-entropy loss (with image\nlabels assigned by scanning captions for class names) can exhibit greater\ndistributional robustness than VL models trained on the same data. To\nfacilitate future experiments with high-accuracy caption-supervised models, we\nintroduce CaptionNet (https:\/\/github.com\/penfever\/CaptionNet\/), which includes\na class-balanced, fully supervised dataset with over 50,000 new human-labeled\nImageNet-compliant samples which includes web-scraped captions. In a series of\nexperiments on CaptionNet, we show how the choice of loss function, data\nfiltration and supervision strategy enable robust computer vision. We also\nprovide the codebase necessary to reproduce our experiments at VL Hub\n(https:\/\/github.com\/penfever\/vlhub\/).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.05742,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Curved Representation Space of Vision Transformers\n\n  Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin\nhave emerged as a better alternative to traditional convolutional neural\nnetworks (CNNs). However, our understanding of how the new architecture works\nis still limited. In this paper, we focus on the phenomenon that Transformers\nshow higher robustness against corruptions than CNNs, while not being\noverconfident. This is contrary to the intuition that robustness increases with\nconfidence. We resolve this contradiction by empirically investigating how the\noutput of the penultimate layer moves in the representation space as the input\ndata moves linearly within a small area. In particular, we show the following.\n(1) While CNNs exhibit fairly linear relationship between the input and output\nmovements, Transformers show nonlinear relationship for some data. For those\ndata, the output of Transformers moves in a curved trajectory as the input\nmoves linearly. (2) When a data is located in a curved region, it is hard to\nmove it out of the decision region since the output moves along a curved\ntrajectory instead of a straight line to the decision boundary, resulting in\nhigh robustness of Transformers. (3) If a data is slightly modified to jump out\nof the curved region, the movements afterwards become linear and the output\ngoes to the decision boundary directly. In other words, there does exist a\ndecision boundary near the data, which is hard to find only because of the\ncurved representation space. This explains the underconfident prediction of\nTransformers. Also, we examine mathematical properties of the attention\noperation that induce nonlinear response to linear perturbation. Finally, we\nshare our additional findings, regarding what contributes to the curved\nrepresentation space of Transformers, and how the curvedness evolves during\ntraining.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.1453,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"RGB-T Semantic Segmentation with Location, Activation, and Sharpening\n\n  Semantic segmentation is important for scene understanding. To address the\nscenes of adverse illumination conditions of natural images, thermal infrared\n(TIR) images are introduced. Most existing RGB-T semantic segmentation methods\nfollow three cross-modal fusion paradigms, i.e. encoder fusion, decoder fusion,\nand feature fusion. Some methods, unfortunately, ignore the properties of RGB\nand TIR features or the properties of features at different levels. In this\npaper, we propose a novel feature fusion-based network for RGB-T semantic\nsegmentation, named \\emph{LASNet}, which follows three steps of location,\nactivation, and sharpening. The highlight of LASNet is that we fully consider\nthe characteristics of cross-modal features at different levels, and\naccordingly propose three specific modules for better segmentation. Concretely,\nwe propose a Collaborative Location Module (CLM) for high-level semantic\nfeatures, aiming to locate all potential objects. We propose a Complementary\nActivation Module for middle-level features, aiming to activate exact regions\nof different objects. We propose an Edge Sharpening Module (ESM) for low-level\ntexture features, aiming to sharpen the edges of objects. Furthermore, in the\ntraining phase, we attach a location supervision and an edge supervision after\nCLM and ESM, respectively, and impose two semantic supervisions in the decoder\npart to facilitate network convergence. Experimental results on two public\ndatasets demonstrate that the superiority of our LASNet over relevant\nstate-of-the-art methods. The code and results of our method are available at\nhttps:\/\/github.com\/MathLee\/LASNet.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.05635,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000008278,
      "text":"Oflib: Facilitating Operations with and on Optical Flow Fields in Python\n\n  We present a robust theoretical framework for the characterisation and\nmanipulation of optical flow, i.e 2D vector fields, in the context of their use\nin motion estimation algorithms and beyond. The definition of two frames of\nreference guides the mathematical derivation of flow field application,\ninversion, evaluation, and composition operations. This structured approach is\nthen used as the foundation for an implementation in Python 3, with the fully\ndifferentiable PyTorch version oflibpytorch supporting back-propagation as\nrequired for deep learning. We verify the flow composition method empirically\nand provide a working example for its application to optical flow ground truth\nin synthetic training data creation. All code is publicly available.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.13641,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields\n\n  We propose a novel geometric and photometric 3D mapping pipeline for accurate\nand real-time scene reconstruction from monocular images. To achieve this, we\nleverage recent advances in dense monocular SLAM and real-time hierarchical\nvolumetric neural radiance fields. Our insight is that dense monocular SLAM\nprovides the right information to fit a neural radiance field of the scene in\nreal-time, by providing accurate pose estimates and depth-maps with associated\nuncertainty. With our proposed uncertainty-based depth loss, we achieve not\nonly good photometric accuracy, but also great geometric accuracy. In fact, our\nproposed pipeline achieves better geometric and photometric accuracy than\ncompeting approaches (up to 179% better PSNR and 86% better L1 depth), while\nworking in real-time and using only monocular images.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.10182,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000013577,
      "text":"Landmark Enforcement and Style Manipulation for Generative Morphing\n\n  Morph images threaten Facial Recognition Systems (FRS) by presenting as\nmultiple individuals, allowing an adversary to swap identities with another\nsubject. Morph generation using generative adversarial networks (GANs) results\nin high-quality morphs unaffected by the spatial artifacts caused by\nlandmark-based methods, but there is an apparent loss in identity with standard\nGAN-based morphing methods. In this paper, we propose a novel StyleGAN morph\ngeneration technique by introducing a landmark enforcement method to resolve\nthis issue. Considering this method, we aim to enforce the landmarks of the\nmorph image to represent the spatial average of the landmarks of the bona fide\nfaces and subsequently the morph images to inherit the geometric identity of\nboth bona fide faces. Exploration of the latent space of our model is conducted\nusing Principal Component Analysis (PCA) to accentuate the effect of both the\nbona fide faces on the morphed latent representation and address the identity\nloss issue with latent domain averaging. Additionally, to improve high\nfrequency reconstruction in the morphs, we study the train-ability of the noise\ninput for the StyleGAN2 model.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00978,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000018544,
      "text":"Uncertainty-Driven Active Vision for Implicit Scene Reconstruction\n\n  Multi-view implicit scene reconstruction methods have become increasingly\npopular due to their ability to represent complex scene details. Recent efforts\nhave been devoted to improving the representation of input information and to\nreducing the number of views required to obtain high quality reconstructions.\nYet, perhaps surprisingly, the study of which views to select to maximally\nimprove scene understanding remains largely unexplored. We propose an\nuncertainty-driven active vision approach for implicit scene reconstruction,\nwhich leverages occupancy uncertainty accumulated across the scene using volume\nrendering to select the next view to acquire. To this end, we develop an\noccupancy-based reconstruction method which accurately represents scenes using\neither 2D or 3D supervision. We evaluate our proposed approach on the ABC\ndataset and the in the wild CO3D dataset, and show that: (1) we are able to\nobtain high quality state-of-the-art occupancy reconstructions; (2) our\nperspective conditioned uncertainty definition is effective to drive\nimprovements in next best view selection and outperforms strong baseline\napproaches; and (3) we can further improve shape understanding by performing a\ngradient-based search on the view selection candidates. Overall, our results\nhighlight the importance of view selection for implicit scene reconstruction,\nmaking it a promising avenue to explore further.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.04868,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000027484,
      "text":"Deep object detection for waterbird monitoring using aerial imagery\n\n  Monitoring of colonial waterbird nesting islands is essential to tracking\nwaterbird population trends, which are used for evaluating ecosystem health and\ninforming conservation management decisions. Recently, unmanned aerial\nvehicles, or drones, have emerged as a viable technology to precisely monitor\nwaterbird colonies. However, manually counting waterbirds from hundreds, or\npotentially thousands, of aerial images is both difficult and time-consuming.\nIn this work, we present a deep learning pipeline that can be used to precisely\ndetect, count, and monitor waterbirds using aerial imagery collected by a\ncommercial drone. By utilizing convolutional neural network-based object\ndetectors, we show that we can detect 16 classes of waterbird species that are\ncommonly found in colonial nesting islands along the Texas coast. Our\nexperiments using Faster R-CNN and RetinaNet object detectors give mean\ninterpolated average precision scores of 67.9% and 63.1% respectively.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.00588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000034769,
      "text":"DFA: Dynamic Feature Aggregation for Efficient Video Object Detection\n\n  Video object detection is a fundamental yet challenging task in computer\nvision. One practical solution is to take advantage of temporal information\nfrom the video and apply feature aggregation to enhance the object features in\neach frame. Though effective, those existing methods always suffer from low\ninference speeds because they use a fixed number of frames for feature\naggregation regardless of the input frame. Therefore, this paper aims to\nimprove the inference speed of the current feature aggregation-based video\nobject detectors while maintaining their performance. To achieve this goal, we\npropose a vanilla dynamic aggregation module that adaptively selects the frames\nfor feature enhancement. Then, we extend the vanilla dynamic aggregation module\nto a more effective and reconfigurable deformable version. Finally, we\nintroduce inplace distillation loss to improve the representations of objects\naggregated with fewer frames. Extensive experimental results validate the\neffectiveness and efficiency of our proposed methods: On the ImageNet VID\nbenchmark, integrated with our proposed methods, FGFA and SELSA can improve the\ninference speed by 31% and 76% respectively while getting comparable\nperformance on accuracy.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.02935,
    "paper_type":"review",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000006623,
      "text":"A Review of Uncertainty Calibration in Pretrained Object Detectors\n\n  In the field of deep learning based computer vision, the development of deep\nobject detection has led to unique paradigms (e.g., two-stage or set-based) and\narchitectures (e.g., Faster-RCNN or DETR) which enable outstanding performance\non challenging benchmark datasets. Despite this, the trained object detectors\ntypically do not reliably assess uncertainty regarding their own knowledge, and\nthe quality of their probabilistic predictions is usually poor. As these are\noften used to make subsequent decisions, such inaccurate probabilistic\npredictions must be avoided. In this work, we investigate the uncertainty\ncalibration properties of different pretrained object detection architectures\nin a multi-class setting. We propose a framework to ensure a fair, unbiased,\nand repeatable evaluation and conduct detailed analyses assessing the\ncalibration under distributional changes (e.g., distributional shift and\napplication to out-of-distribution data). Furthermore, by investigating the\ninfluence of different detector paradigms, post-processing steps, and suitable\nchoices of metrics, we deliver novel insights into why poor detector\ncalibration emerges. Based on these insights, we are able to improve the\ncalibration of a detector by simply finetuning its last layer.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.02074,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000010596,
      "text":"Two Video Data Sets for Tracking and Retrieval of Out of Distribution\n  Objects\n\n  In this work we present two video test data sets for the novel computer\nvision (CV) task of out of distribution tracking (OOD tracking). Here, OOD\nobjects are understood as objects with a semantic class outside the semantic\nspace of an underlying image segmentation algorithm, or an instance within the\nsemantic space which however looks decisively different from the instances\ncontained in the training data. OOD objects occurring on video sequences should\nbe detected on single frames as early as possible and tracked over their time\nof appearance as long as possible. During the time of appearance, they should\nbe segmented as precisely as possible. We present the SOS data set containing\n20 video sequences of street scenes and more than 1000 labeled frames with up\nto two OOD objects. We furthermore publish the synthetic CARLA-WildLife data\nset that consists of 26 video sequences containing up to four OOD objects on a\nsingle frame. We propose metrics to measure the success of OOD tracking and\ndevelop a baseline algorithm that efficiently tracks the OOD objects. As an\napplication that benefits from OOD tracking, we retrieve OOD sequences from\nunlabeled videos of street scenes containing OOD objects.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.11135,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.000003775,
      "text":"On-line signature verification using Tablet PC\n\n  On-line signature verification for Tablet PC devices is studied. The on-line\nsignature verification algorithm presented by the authors at the First\nInternational Signature Verification Competition (SVC 2004) is adapted to work\nin Tablet PC environments. An example prototype of securing access and securing\ndocument application using this Tablet PC system is also reported. Two\ndifferent commercial Tablet PCs are evaluated, including information of\ninterest for signature verification systems such as sampling and pressure\nstatistics. Authentication performance experiments are reported considering\nboth random and skilled forgeries by using a new database with over 3000\nsignatures.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.13646,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000030796,
      "text":"Depth Monocular Estimation with Attention-based Encoder-Decoder Network\n  from Single Image\n\n  Depth information is the foundation of perception, essential for autonomous\ndriving, robotics, and other source-constrained applications. Promptly\nobtaining accurate and efficient depth information allows for a rapid response\nin dynamic environments. Sensor-based methods using LIDAR and RADAR obtain high\nprecision at the cost of high power consumption, price, and volume. While due\nto advances in deep learning, vision-based approaches have recently received\nmuch attention and can overcome these drawbacks. In this work, we explore an\nextreme scenario in vision-based settings: estimate a depth map from one\nmonocular image severely plagued by grid artifacts and blurry edges. To address\nthis scenario, We first design a convolutional attention mechanism block (CAMB)\nwhich consists of channel attention and spatial attention sequentially and\ninsert these CAMBs into skip connections. As a result, our novel approach can\nfind the focus of current image with minimal overhead and avoid losses of depth\nfeatures. Next, by combining the depth value, the gradients of X axis, Y axis\nand diagonal directions, and the structural similarity index measure (SSIM), we\npropose our novel loss function. Moreover, we utilize pixel blocks to\naccelerate the computation of the loss function. Finally, we show, through\ncomprehensive experiments on two large-scale image datasets, i.e. KITTI and\nNYU-V2, that our method outperforms several representative baselines.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2210.02357,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":10,
    "pangram_prediction":{
      "ai_likelihood":0.0000027816,
      "text":"Image Masking for Robust Self-Supervised Monocular Depth Estimation\n\n  Self-supervised monocular depth estimation is a salient task for 3D scene\nunderstanding. Learned jointly with monocular ego-motion estimation, several\nmethods have been proposed to predict accurate pixel-wise depth without using\nlabeled data. Nevertheless, these methods focus on improving performance under\nideal conditions without natural or digital corruptions. The general absence of\nocclusions is assumed even for object-specific depth estimation. These methods\nare also vulnerable to adversarial attacks, which is a pertinent concern for\ntheir reliable deployment in robots and autonomous driving systems. We propose\nMIMDepth, a method that adapts masked image modeling (MIM) for self-supervised\nmonocular depth estimation. While MIM has been used to learn generalizable\nfeatures during pre-training, we show how it could be adapted for direct\ntraining of monocular depth estimation. Our experiments show that MIMDepth is\nmore robust to noise, blur, weather conditions, digital artifacts, occlusions,\nas well as untargeted and targeted adversarial attacks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.14108,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000049339,
      "text":"3DDesigner: Towards Photorealistic 3D Object Generation and Editing with\n  Text-guided Diffusion Models\n\n  Text-guided diffusion models have shown superior performance in image\/video\ngeneration and editing. While few explorations have been performed in 3D\nscenarios. In this paper, we discuss three fundamental and interesting problems\non this topic. First, we equip text-guided diffusion models to achieve\n3D-consistent generation. Specifically, we integrate a NeRF-like neural field\nto generate low-resolution coarse results for a given camera view. Such results\ncan provide 3D priors as condition information for the following diffusion\nprocess. During denoising diffusion, we further enhance the 3D consistency by\nmodeling cross-view correspondences with a novel two-stream (corresponding to\ntwo different views) asynchronous diffusion process. Second, we study 3D local\nediting and propose a two-step solution that can generate 360-degree\nmanipulated results by editing an object from a single view. Step 1, we propose\nto perform 2D local editing by blending the predicted noises. Step 2, we\nconduct a noise-to-text inversion process that maps 2D blended noises into the\nview-independent text embedding space. Once the corresponding text embedding is\nobtained, 360-degree images can be generated. Last but not least, we extend our\nmodel to perform one-shot novel view synthesis by fine-tuning on a single\nimage, firstly showing the potential of leveraging text guidance for novel view\nsynthesis. Extensive experiments and various applications show the prowess of\nour 3DDesigner. The project page is available at\nhttps:\/\/3ddesigner-diffusion.github.io\/.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11315,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000050995,
      "text":"Beyond Attentive Tokens: Incorporating Token Importance and Diversity\n  for Efficient Vision Transformers\n\n  Vision transformers have achieved significant improvements on various vision\ntasks but their quadratic interactions between tokens significantly reduce\ncomputational efficiency. Many pruning methods have been proposed to remove\nredundant tokens for efficient vision transformers recently. However, existing\nstudies mainly focus on the token importance to preserve local attentive tokens\nbut completely ignore the global token diversity. In this paper, we emphasize\nthe cruciality of diverse global semantics and propose an efficient token\ndecoupling and merging method that can jointly consider the token importance\nand diversity for token pruning. According to the class token attention, we\ndecouple the attentive and inattentive tokens. In addition to preserving the\nmost discriminative local tokens, we merge similar inattentive tokens and match\nhomogeneous attentive tokens to maximize the token diversity. Despite its\nsimplicity, our method obtains a promising trade-off between model complexity\nand classification accuracy. On DeiT-S, our method reduces the FLOPs by 35%\nwith only a 0.2% accuracy drop. Notably, benefiting from maintaining the token\ndiversity, our method can even improve the accuracy of DeiT-T by 0.1% after\nreducing its FLOPs by 40%.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.08609,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000036094,
      "text":"R-Pred: Two-Stage Motion Prediction Via Tube-Query Attention-Based\n  Trajectory Refinement\n\n  Predicting the future motion of dynamic agents is of paramount importance to\nensuring safety and assessing risks in motion planning for autonomous robots.\nIn this study, we propose a two-stage motion prediction method, called R-Pred,\ndesigned to effectively utilize both scene and interaction context using a\ncascade of the initial trajectory proposal and trajectory refinement networks.\nThe initial trajectory proposal network produces M trajectory proposals\ncorresponding to the M modes of the future trajectory distribution. The\ntrajectory refinement network enhances each of the M proposals using 1)\ntube-query scene attention (TQSA) and 2) proposal-level interaction attention\n(PIA) mechanisms. TQSA uses tube-queries to aggregate local scene context\nfeatures pooled from proximity around trajectory proposals of interest. PIA\nfurther enhances the trajectory proposals by modeling inter-agent interactions\nusing a group of trajectory proposals selected by their distances from\nneighboring agents. Our experiments conducted on Argoverse and nuScenes\ndatasets demonstrate that the proposed refinement network provides significant\nperformance improvements compared to the single-stage baseline and that R-Pred\nachieves state-of-the-art performance in some categories of the benchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11801,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000103646,
      "text":"Self-Supervised Pre-training of 3D Point Cloud Networks with Image Data\n\n  Reducing the quantity of annotations required for supervised training is\nvital when labels are scarce and costly. This reduction is especially important\nfor semantic segmentation tasks involving 3D datasets that are often\nsignificantly smaller and more challenging to annotate than their image-based\ncounterparts. Self-supervised pre-training on large unlabelled datasets is one\nway to reduce the amount of manual annotations needed. Previous work has\nfocused on pre-training with point cloud data exclusively; this approach often\nrequires two or more registered views. In the present work, we combine image\nand point cloud modalities, by first learning self-supervised image features\nand then using these features to train a 3D model. By incorporating image data,\nwhich is often included in many 3D datasets, our pre-training method only\nrequires a single scan of a scene. We demonstrate that our pre-training\napproach, despite using single scans, achieves comparable performance to other\nmulti-scan, point cloud-only methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05187,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000064572,
      "text":"Training a Vision Transformer from scratch in less than 24 hours with 1\n  GPU\n\n  Transformers have become central to recent advances in computer vision.\nHowever, training a vision Transformer (ViT) model from scratch can be resource\nintensive and time consuming. In this paper, we aim to explore approaches to\nreduce the training costs of ViT models. We introduce some algorithmic\nimprovements to enable training a ViT model from scratch with limited hardware\n(1 GPU) and time (24 hours) resources. First, we propose an efficient approach\nto add locality to the ViT architecture. Second, we develop a new image size\ncurriculum learning strategy, which allows to reduce the number of patches\nextracted from each image at the beginning of the training. Finally, we propose\na new variant of the popular ImageNet1k benchmark by adding hardware and time\nconstraints. We evaluate our contributions on this benchmark, and show they can\nsignificantly improve performances given the proposed training budget. We will\nshare the code in https:\/\/github.com\/BorealisAI\/efficient-vit-training.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07383,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.000002053,
      "text":"Attacking Face Recognition with T-shirts: Database, Vulnerability\n  Assessment and Detection\n\n  Face recognition systems are widely deployed for biometric authentication.\nDespite this, it is well-known that, without any safeguards, face recognition\nsystems are highly vulnerable to presentation attacks. In response to this\nsecurity issue, several promising methods for detecting presentation attacks\nhave been proposed which show high performance on existing benchmarks. However,\nan ongoing challenge is the generalization of presentation attack detection\nmethods to unseen and new attack types. To this end, we propose a new T-shirt\nFace Presentation Attack (TFPA) database of 1,608 T-shirt attacks using 100\nunique presentation attack instruments. In an extensive evaluation, we show\nthat this type of attack can compromise the security of face recognition\nsystems and that some state-of-the-art attack detection mechanisms trained on\npopular benchmarks fail to robustly generalize to the new attacks. Further, we\npropose three new methods for detecting T-shirt attack images, one which relies\non the statistical differences between depth maps of bona fide images and\nT-shirt attacks, an anomaly detection approach trained on features only\nextracted from bona fide RGB images, and a fusion approach which achieves\ncompetitive detection performance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.00207,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000007616,
      "text":"GMF: General Multimodal Fusion Framework for Correspondence Outlier\n  Rejection\n\n  Rejecting correspondence outliers enables to boost the correspondence\nquality, which is a critical step in achieving high point cloud registration\naccuracy. The current state-of-the-art correspondence outlier rejection methods\nonly utilize the structure features of the correspondences. However, texture\ninformation is critical to reject the correspondence outliers in our human\nvision system. In this paper, we propose General Multimodal Fusion (GMF) to\nlearn to reject the correspondence outliers by leveraging both the structure\nand texture information. Specifically, two cross-attention-based fusion layers\nare proposed to fuse the texture information from paired images and structure\ninformation from point correspondences. Moreover, we propose a convolutional\nposition encoding layer to enhance the difference between Tokens and enable the\nencoding feature pay attention to neighbor information. Our position encoding\nlayer will make the cross-attention operation integrate both local and global\ninformation. Experiments on multiple datasets(3DMatch, 3DLoMatch, KITTI) and\nrecent state-of-the-art models (3DRegNet, DGR, PointDSC) prove that our GMF\nachieves wide generalization ability and consistently improves the point cloud\nregistration accuracy. Furthermore, several ablation studies demonstrate the\nrobustness of the proposed GMF on different loss functions, lighting conditions\nand noises.The code is available at https:\/\/github.com\/XiaoshuiHuang\/GMF.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.02337,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009272,
      "text":"UV R-CNN: Stable and Efficient Dense Human Pose Estimation\n\n  Dense pose estimation is a dense 3D prediction task for instance-level human\nanalysis, aiming to map human pixels from an RGB image to a 3D surface of the\nhuman body. Due to a large amount of surface point regression, the training\nprocess appears to be easy to collapse compared to other region-based human\ninstance analyzing tasks. By analyzing the loss formulation of the existing\ndense pose estimation model, we introduce a novel point regression loss\nfunction, named Dense Points} loss to stable the training progress, and a new\nbalanced loss weighting strategy to handle the multi-task losses. With the\nabove novelties, we propose a brand new architecture, named UV R-CNN. Without\nauxiliary supervision and external knowledge from other tasks, UV R-CNN can\nhandle many complicated issues in dense pose model training progress, achieving\n65.0% $AP_{gps}$ and 66.1% $AP_{gpsm}$ on the DensePose-COCO validation subset\nwith ResNet-50-FPN feature extractor, competitive among the state-of-the-art\ndense human pose estimation methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.16175,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000061591,
      "text":"Context-Aware Robust Fine-Tuning\n\n  Contrastive Language-Image Pre-trained (CLIP) models have zero-shot ability\nof classifying an image belonging to \"[CLASS]\" by using similarity between the\nimage and the prompt sentence \"a [CONTEXT] of [CLASS]\". Based on exhaustive\ntext cues in \"[CONTEXT]\", CLIP model is aware of different contexts, e.g.\nbackground, style, viewpoint, and exhibits unprecedented robustness against a\nwide range of distribution shifts. However, recent works find further\nfine-tuning of CLIP models improves accuracy but sacrifices the robustness on\ndownstream tasks. We conduct an empirical investigation to show fine-tuning\nwill corrupt the context-aware ability of pre-trained CLIP features. To solve\nthis problem, we propose Context-Aware Robust Fine-tuning (CAR-FT). CAR-FT\nregularizes the model during fine-tuning to capture the context information.\nSpecifically, we use zero-shot prompt weights to get the context distribution\ncontained in the image. By minimizing the Kullback-Leibler Divergence (KLD)\nbetween context distributions induced by original\/fine-tuned CLIP models,\nCAR-FT makes the context-aware ability of CLIP inherited into downstream tasks,\nand achieves both higher In-Distribution (ID) and Out-Of-Distribution (OOD)\naccuracy. The experimental results show CAR-FT achieves superior robustness on\nfive OOD test datasets of ImageNet, and meanwhile brings accuracy gains on nine\ndownstream tasks. Additionally, CAR-FT surpasses previous Domain Generalization\n(DG) methods and gets 78.5% averaged accuracy on DomainBed benchmark, building\nthe new state-of-the-art.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11921,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000166893,
      "text":"Confidence-guided Centroids for Unsupervised Person Re-Identification\n\n  Unsupervised person re-identification (ReID) aims to train a feature\nextractor for identity retrieval without exploiting identity labels. Due to the\nblind trust in imperfect clustering results, the learning is inevitably misled\nby unreliable pseudo labels. Albeit the pseudo label refinement has been\ninvestigated by previous works, they generally leverage auxiliary information\nsuch as camera IDs and body part predictions. This work explores the internal\ncharacteristics of clusters to refine pseudo labels. To this end,\nConfidence-Guided Centroids (CGC) are proposed to provide reliable cluster-wise\nprototypes for feature learning. Since samples with high confidence are\nexclusively involved in the formation of centroids, the identity information of\nlow-confidence samples, i.e., boundary samples, are NOT likely to contribute to\nthe corresponding centroid. Given the new centroids, current learning scheme,\nwhere samples are enforced to learn from their assigned centroids solely, is\nunwise. To remedy the situation, we propose to use Confidence-Guided pseudo\nLabel (CGL), which enables samples to approach not only the originally assigned\ncentroid but other centroids that are potentially embedded with their identity\ninformation. Empowered by confidence-guided centroids and labels, our method\nyields comparable performance with, or even outperforms, state-of-the-art\npseudo label refinement works that largely leverage auxiliary information.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.07082,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000040068,
      "text":"Learning Latent Part-Whole Hierarchies for Point Clouds\n\n  Strong evidence suggests that humans perceive the 3D world by parsing visual\nscenes and objects into part-whole hierarchies. Although deep neural networks\nhave the capability of learning powerful multi-level representations, they can\nnot explicitly model part-whole hierarchies, which limits their expressiveness\nand interpretability in processing 3D vision data such as point clouds. To this\nend, we propose an encoder-decoder style latent variable model that explicitly\nlearns the part-whole hierarchies for the multi-level point cloud segmentation.\nSpecifically, the encoder takes a point cloud as input and predicts the\nper-point latent subpart distribution at the middle level. The decoder takes\nthe latent variable and the feature from the encoder as an input and predicts\nthe per-point part distribution at the top level. During training, only\nannotated part labels at the top level are provided, thus making the whole\nframework weakly supervised. We explore two kinds of approximated inference\nalgorithms, i.e., most-probable-latent and Monte Carlo methods, and three\nstochastic gradient estimations for learning discrete latent variables, i.e.,\nstraight-through, REINFORCE, and pathwise estimators. Experimental results on\nthe PartNet dataset show that the proposed method achieves state-of-the-art\nperformance in not only top-level part segmentation but also middle-level\nlatent subpart segmentation.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.13812,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"Improving Siamese Based Trackers with Light or No Training through\n  Multiple Templates and Temporal Network\n\n  High computational power and significant time are usually needed to train a\ndeep learning based tracker on large datasets. Depending on many factors,\ntraining might not always be an option. In this paper, we propose a framework\nwith two ideas on Siamese-based trackers. (i) Extending number of templates in\na way that removes the need to retrain the network and (ii) a lightweight\ntemporal network with a novel architecture focusing on both local and global\ninformation that can be used independently from trackers. Most Siamese-based\ntrackers only rely on the first frame as the ground truth for objects and\nstruggle when the target's appearance changes significantly in subsequent\nframes in presence of similar distractors. Some trackers use multiple templates\nwhich mostly rely on constant thresholds to update, or they replace those\ntemplates that have low similarity scores only with more similar ones. Unlike\nprevious works, we use adaptive thresholds that update the bag with similar\ntemplates as well as those templates which are slightly diverse. Adaptive\nthresholds also cause an overall improvement over constant ones. In addition,\nmixing feature maps obtained by each template in the last stage of networks\nremoves the need to retrain trackers. Our proposed lightweight temporal\nnetwork, CombiNet, learns the path history of different objects using only\nobject coordinates and predicts target's potential location in the next frame.\nIt is tracker independent and applying it on new trackers does not need further\ntraining. By implementing these ideas, trackers' performance improved on all\ndatasets tested on, including LaSOT, LaSOT extension, TrackingNet, OTB100,\nOTB50, UAV123 and UAV20L. Experiments indicate the proposed framework works\nwell with both convolutional and transformer-based trackers. The official\npython code for this paper will be publicly available upon publication.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.14118,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000063247,
      "text":"MS-PS: A Multi-Scale Network for Photometric Stereo With a New\n  Comprehensive Training Dataset\n\n  The photometric stereo (PS) problem consists in reconstructing the 3D-surface\nof an object, thanks to a set of photographs taken under different lighting\ndirections. In this paper, we propose a multi-scale architecture for PS which,\ncombined with a new dataset, yields state-of-the-art results. Our proposed\narchitecture is flexible: it permits to consider a variable number of images as\nwell as variable image size without loss of performance. In addition, we define\na set of constraints to allow the generation of a relevant synthetic dataset to\ntrain convolutional neural networks for the PS problem. Our proposed dataset is\nmuch larger than pre-existing ones, and contains many objects with challenging\nmaterials having anisotropic reflectance (e.g. metals, glass). We show on\npublicly available benchmarks that the combination of both these contributions\ndrastically improves the accuracy of the estimated normal field, in comparison\nwith previous state-of-the-art methods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.10104,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000029471,
      "text":"Stereo Image Rain Removal via Dual-View Mutual Attention\n\n  Stereo images, containing left and right view images with disparity, are\nutilized in solving low-vision tasks recently, e.g., rain removal and\nsuper-resolution. Stereo image restoration methods usually obtain better\nperformance than monocular methods by learning the disparity between dual views\neither implicitly or explicitly. However, existing stereo rain removal methods\nstill cannot make full use of the complementary information between two views,\nand we find it is because: 1) the rain streaks have more complex distributions\nin directions and densities, which severely damage the complementary\ninformation and pose greater challenges; 2) the disparity estimation is not\naccurate enough due to the imperfect fusion mechanism for the features between\ntwo views. To overcome such limitations, we propose a new \\underline{Stereo}\n\\underline{I}mage \\underline{R}ain \\underline{R}emoval method (StereoIRR) via\nsufficient interaction between two views, which incorporates: 1) a new\nDual-view Mutual Attention (DMA) mechanism which generates mutual attention\nmaps by taking left and right views as key information for each other to\nfacilitate cross-view feature fusion; 2) a long-range and cross-view\ninteraction, which is constructed with basic blocks and dual-view mutual\nattention, can alleviate the adverse effect of rain on complementary\ninformation to help the features of stereo images to get long-range and\ncross-view interaction and fusion. Notably, StereoIRR outperforms other related\nmonocular and stereo image rain removal methods on several datasets. Our codes\nand datasets will be released.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06843,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000009934,
      "text":"Generalization Beyond Feature Alignment: Concept Activation-Guided\n  Contrastive Learning\n\n  Learning invariant representations via contrastive learning has seen\nstate-of-the-art performance in domain generalization (DG). Despite such\nsuccess, in this paper, we find that its core learning strategy -- feature\nalignment -- could heavily hinder model generalization. Drawing insights in\nneuron interpretability, we characterize this problem from a neuron activation\nview. Specifically, by treating feature elements as neuron activation states,\nwe show that conventional alignment methods tend to deteriorate the diversity\nof learned invariant features, as they indiscriminately minimize all neuron\nactivation differences. This instead ignores rich relations among neurons --\nmany of them often identify the same visual concepts despite differing\nactivation patterns. With this finding, we present a simple yet effective\napproach, Concept Contrast (CoCo), which relaxes element-wise feature\nalignments by contrasting high-level concepts encoded in neurons. Our CoCo\nperforms in a plug-and-play fashion, thus it can be integrated into any\ncontrastive method in DG. We evaluate CoCo over four canonical contrastive\nmethods, showing that CoCo promotes the diversity of feature representations\nand consistently improves model generalization capability. By decoupling this\nsuccess through neuron coverage analysis, we further find that CoCo potentially\ninvokes more meaningful neurons during training, thereby improving model\nlearning.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.06083,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000178483,
      "text":"Token Transformer: Can class token help window-based transformer build\n  better long-range interactions?\n\n  Compared with the vanilla transformer, the window-based transformer offers a\nbetter trade-off between accuracy and efficiency. Although the window-based\ntransformer has made great progress, its long-range modeling capabilities are\nlimited due to the size of the local window and the window connection scheme.\nTo address this problem, we propose a novel Token Transformer (TT). The core\nmechanism of TT is the addition of a Class (CLS) token for summarizing window\ninformation in each local window. We refer to this type of token interaction as\nCLS Attention. These CLS tokens will interact spatially with the tokens in each\nwindow to enable long-range modeling. In order to preserve the hierarchical\ndesign of the window-based transformer, we designed Feature Inheritance Module\n(FIM) in each phase of TT to deliver the local window information from the\nprevious phase to the CLS token in the next phase. In addition, we have\ndesigned a Spatial-Channel Feedforward Network (SCFFN) in TT, which can mix CLS\ntokens and embedded tokens on the spatial domain and channel domain without\nadditional parameters. Extensive experiments have shown that our TT achieves\ncompetitive results with low parameters in image classification and downstream\ntasks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.11282,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000140071,
      "text":"Task-Specific Data Augmentation and Inference Processing for VIPriors\n  Instance Segmentation Challenge\n\n  Instance segmentation is applied widely in image editing, image analysis and\nautonomous driving, etc. However, insufficient data is a common problem in\npractical applications. The Visual Inductive Priors(VIPriors) Instance\nSegmentation Challenge has focused on this problem. VIPriors for Data-Efficient\nComputer Vision Challenges ask competitors to train models from scratch in a\ndata-deficient setting, but there are some visual inductive priors that can be\nused. In order to address the VIPriors instance segmentation problem, we\ndesigned a Task-Specific Data Augmentation(TS-DA) strategy and Inference\nProcessing(TS-IP) strategy. The main purpose of task-specific data augmentation\nstrategy is to tackle the data-deficient problem. And in order to make the most\nof visual inductive priors, we designed a task-specific inference processing\nstrategy. We demonstrate the applicability of proposed method on VIPriors\nInstance Segmentation Challenge. The segmentation model applied is Hybrid Task\nCascade based detector on the Swin-Base based CBNetV2 backbone. Experimental\nresults demonstrate that proposed method can achieve a competitive result on\nthe test set of 2022 VIPriors Instance Segmentation Challenge, with 0.531\nAP@0.50:0.95.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.10588,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000004305,
      "text":"Prior Guided Deep Difference Meta-Learner for Fast Adaptation to\n  Stylized Segmentation\n\n  When a pre-trained general auto-segmentation model is deployed at a new\ninstitution, a support framework in the proposed Prior-guided DDL network will\nlearn the systematic difference between the model predictions and the final\ncontours revised and approved by clinicians for an initial group of patients.\nThe learned style feature differences are concatenated with the new patients\n(query) features and then decoded to get the style-adapted segmentations. The\nmodel is independent of practice styles and anatomical structures. It\nmeta-learns with simulated style differences and does not need to be exposed to\nany real clinical stylized structures during training. Once trained on the\nsimulated data, it can be deployed for clinical use to adapt to new practice\nstyles and new anatomical structures without further training.\n  To show the proof of concept, we tested the Prior-guided DDL network on six\ndifferent practice style variations for three different anatomical structures.\nPre-trained segmentation models were adapted from post-operative clinical\ntarget volume (CTV) segmentation to segment CTVstyle1, CTVstyle2, and\nCTVstyle3, from parotid gland segmentation to segment Parotidsuperficial, and\nfrom rectum segmentation to segment Rectumsuperior and Rectumposterior. The\nmode performance was quantified with Dice Similarity Coefficient (DSC). With\nadaptation based on only the first three patients, the average DSCs were\nimproved from 78.6, 71.9, 63.0, 52.2, 46.3 and 69.6 to 84.4, 77.8, 73.0, 77.8,\n70.5, 68.1, for CTVstyle1, CTVstyle2, and CTVstyle3, Parotidsuperficial,\nRectumsuperior, and Rectumposterior, respectively, showing the great potential\nof the Priorguided DDL network for a fast and effortless adaptation to new\npractice styles\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.05299,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000148018,
      "text":"Prior-enhanced Temporal Action Localization using Subject-aware Spatial\n  Attention\n\n  Temporal action localization (TAL) aims to detect the boundary and identify\nthe class of each action instance in a long untrimmed video. Current approaches\ntreat video frames homogeneously, and tend to give background and key objects\nexcessive attention. This limits their sensitivity to localize action\nboundaries. To this end, we propose a prior-enhanced temporal action\nlocalization method (PETAL), which only takes in RGB input and incorporates\naction subjects as priors. This proposal leverages action subjects' information\nwith a plug-and-play subject-aware spatial attention module (SA-SAM) to\ngenerate an aggregated and subject-prioritized representation. Experimental\nresults on THUMOS-14 and ActivityNet-1.3 datasets demonstrate that the proposed\nPETAL achieves competitive performance using only RGB features, e.g., boosting\nmAP by 2.41% or 0.25% over the state-of-the-art approach that uses RGB features\nor with additional optical flow features on the THUMOS-14 dataset.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2211.01631,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":11,
    "pangram_prediction":{
      "ai_likelihood":0.0000014901,
      "text":"$\\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework\n  for Groupwise Registration and Deep Combined Computing\n\n  This paper presents a generic probabilistic framework for estimating the\nstatistical dependency and finding the anatomical correspondences among an\narbitrary number of medical images. The method builds on a novel formulation of\nthe $N$-dimensional joint intensity distribution by representing the common\nanatomy as latent variables and estimating the appearance model with\nnonparametric estimators. Through connection to maximum likelihood and the\nexpectation-maximization algorithm, an information\\hyp{}theoretic metric called\n$\\mathcal{X}$-metric and a co-registration algorithm named $\\mathcal{X}$-CoReg\nare induced, allowing groupwise registration of the $N$ observed images with\ncomputational complexity of $\\mathcal{O}(N)$. Moreover, the method naturally\nextends for a weakly-supervised scenario where anatomical labels of certain\nimages are provided. This leads to a combined\\hyp{}computing framework\nimplemented with deep learning, which performs registration and segmentation\nsimultaneously and collaboratively in an end-to-end fashion. Extensive\nexperiments were conducted to demonstrate the versatility and applicability of\nour model, including multimodal groupwise registration, motion correction for\ndynamic contrast enhanced magnetic resonance images, and deep combined\ncomputing for multimodal medical images. Results show the superiority of our\nmethod in various applications in terms of both accuracy and efficiency,\nhighlighting the advantage of the proposed representation of the imaging\nprocess.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.01128,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000038743,
      "text":"A Multi-Stream Fusion Network for Image Splicing Localization\n\n  In this paper, we address the problem of image splicing localization with a\nmulti-stream network architecture that processes the raw RGB image in parallel\nwith other handcrafted forensic signals. Unlike previous methods that either\nuse only the RGB images or stack several signals in a channel-wise manner, we\npropose an encoder-decoder architecture that consists of multiple encoder\nstreams. Each stream is fed with either the tampered image or handcrafted\nsignals and processes them separately to capture relevant information from each\none independently. Finally, the extracted features from the multiple streams\nare fused in the bottleneck of the architecture and propagated to the decoder\nnetwork that generates the output localization map. We experiment with two\nhandcrafted algorithms, i.e., DCT and Splicebuster. Our proposed approach is\nbenchmarked on three public forensics datasets, demonstrating competitive\nperformance against several competing methods and achieving state-of-the-art\nresults, e.g., 0.898 AUC on CASIA.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.02804,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000002616,
      "text":"MUS-CDB: Mixed Uncertainty Sampling with Class Distribution Balancing\n  for Active Annotation in Aerial Object Detection\n\n  Recent aerial object detection models rely on a large amount of labeled\ntraining data, which requires unaffordable manual labeling costs in large\naerial scenes with dense objects. Active learning effectively reduces the data\nlabeling cost by selectively querying the informative and representative\nunlabelled samples. However, existing active learning methods are mainly with\nclass-balanced settings and image-based querying for generic object detection\ntasks, which are less applicable to aerial object detection scenarios due to\nthe long-tailed class distribution and dense small objects in aerial scenes. In\nthis paper, we propose a novel active learning method for cost-effective aerial\nobject detection. Specifically, both object-level and image-level\ninformativeness are considered in the object selection to refrain from\nredundant and myopic querying. Besides, an easy-to-use class-balancing\ncriterion is incorporated to favor the minority objects to alleviate the\nlong-tailed class distribution problem in model training. We further devise a\ntraining loss to mine the latent knowledge in the unlabeled image regions.\nExtensive experiments are conducted on the DOTA-v1.0 and DOTA-v2.0 benchmarks\nto validate the effectiveness of the proposed method. For the ReDet, KLD, and\nSASM detectors on the DOTA-v2.0 dataset, the results show that our proposed\nMUS-CDB method can save nearly 75\\% of the labeling cost while achieving\ncomparable performance to other active learning methods in terms of mAP.Code is\npublicly online (https:\/\/github.com\/ZJW700\/MUS-CDB).\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.0657,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000097023,
      "text":"CamoFormer: Masked Separable Attention for Camouflaged Object Detection\n\n  How to identify and segment camouflaged objects from the background is\nchallenging. Inspired by the multi-head self-attention in Transformers, we\npresent a simple masked separable attention (MSA) for camouflaged object\ndetection. We first separate the multi-head self-attention into three parts,\nwhich are responsible for distinguishing the camouflaged objects from the\nbackground using different mask strategies. Furthermore, we propose to capture\nhigh-resolution semantic representations progressively based on a simple\ntop-down decoder with the proposed MSA to attain precise segmentation results.\nThese structures plus a backbone encoder form a new model, dubbed CamoFormer.\nExtensive experiments show that CamoFormer surpasses all existing\nstate-of-the-art methods on three widely-used camouflaged object detection\nbenchmarks. There are on average around 5% relative improvements over previous\nmethods in terms of S-measure and weighted F-measure.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07352,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000109606,
      "text":"Bi-Noising Diffusion: Towards Conditional Diffusion Models with\n  Generative Restoration Priors\n\n  Conditional diffusion probabilistic models can model the distribution of\nnatural images and can generate diverse and realistic samples based on given\nconditions. However, oftentimes their results can be unrealistic with\nobservable color shifts and textures. We believe that this issue results from\nthe divergence between the probabilistic distribution learned by the model and\nthe distribution of natural images. The delicate conditions gradually enlarge\nthe divergence during each sampling timestep. To address this issue, we\nintroduce a new method that brings the predicted samples to the training data\nmanifold using a pretrained unconditional diffusion model. The unconditional\nmodel acts as a regularizer and reduces the divergence introduced by the\nconditional model at each sampling step. We perform comprehensive experiments\nto demonstrate the effectiveness of our approach on super-resolution,\ncolorization, turbulence removal, and image-deraining tasks. The improvements\nobtained by our method suggest that the priors can be incorporated as a general\nplugin for improving conditional diffusion models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.02127,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000001755,
      "text":"FaceQAN: Face Image Quality Assessment Through Adversarial Noise\n  Exploration\n\n  Recent state-of-the-art face recognition (FR) approaches have achieved\nimpressive performance, yet unconstrained face recognition still represents an\nopen problem. Face image quality assessment (FIQA) approaches aim to estimate\nthe quality of the input samples that can help provide information on the\nconfidence of the recognition decision and eventually lead to improved results\nin challenging scenarios. While much progress has been made in face image\nquality assessment in recent years, computing reliable quality scores for\ndiverse facial images and FR models remains challenging. In this paper, we\npropose a novel approach to face image quality assessment, called FaceQAN, that\nis based on adversarial examples and relies on the analysis of adversarial\nnoise which can be calculated with any FR model learned by using some form of\ngradient descent. As such, the proposed approach is the first to link image\nquality to adversarial attacks. Comprehensive (cross-model as well as\nmodel-specific) experiments are conducted with four benchmark datasets, i.e.,\nLFW, CFP-FP, XQLFW and IJB-C, four FR models, i.e., CosFace, ArcFace,\nCurricularFace and ElasticFace, and in comparison to seven state-of-the-art\nFIQA methods to demonstrate the performance of FaceQAN. Experimental results\nshow that FaceQAN achieves competitive results, while exhibiting several\ndesirable characteristics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.09983,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000050664,
      "text":"Texture Representation via Analysis and Synthesis with Generative\n  Adversarial Networks\n\n  We investigate data-driven texture modeling via analysis and synthesis with\ngenerative adversarial networks. For network training and testing, we have\ncompiled a diverse set of spatially homogeneous textures, ranging from\nstochastic to regular. We adopt StyleGAN3 for synthesis and demonstrate that it\nproduces diverse textures beyond those represented in the training data. For\ntexture analysis, we propose GAN inversion using a novel latent domain\nreconstruction consistency criterion for synthesized textures, and iterative\nrefinement with Gramian loss for real textures. We propose perceptual\nprocedures for evaluating network capabilities, exploring the global and local\nbehavior of latent space trajectories, and comparing with existing texture\nanalysis-synthesis techniques.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.08613,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000005828,
      "text":"Atrous Space Bender U-Net (ASBU-Net\/LogiNet)\n\n  $ $With recent advances in CNNs, exceptional improvements have been made in\nsemantic segmentation of high resolution images in terms of accuracy and\nlatency. However, challenges still remain in detecting objects in crowded\nscenes, large scale variations, partial occlusion, and distortions, while still\nmaintaining mobility and latency. We introduce a fast and efficient\nconvolutional neural network, ASBU-Net, for semantic segmentation of high\nresolution images that addresses these problems and uses no novelty layers for\nease of quantization and embedded hardware support. ASBU-Net is based on a new\nfeature extraction module, atrous space bender layer (ASBL), which is efficient\nin terms of computation and memory. The ASB layers form a building block that\nis used to make ASBNet. Since this network does not use any special layers it\ncan be easily implemented, quantized and deployed on FPGAs and other hardware\nwith limited memory. We present experiments on resource and accuracy trade-offs\nand show strong performance compared to other popular models.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.05638,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000083115,
      "text":"Cross-Modal Learning with 3D Deformable Attention for Action Recognition\n\n  An important challenge in vision-based action recognition is the embedding of\nspatiotemporal features with two or more heterogeneous modalities into a single\nfeature. In this study, we propose a new 3D deformable transformer for action\nrecognition with adaptive spatiotemporal receptive fields and a cross-modal\nlearning scheme. The 3D deformable transformer consists of three attention\nmodules: 3D deformability, local joint stride, and temporal stride attention.\nThe two cross-modal tokens are input into the 3D deformable attention module to\ncreate a cross-attention token with a reflected spatiotemporal correlation.\nLocal joint stride attention is applied to spatially combine attention and pose\ntokens. Temporal stride attention temporally reduces the number of input tokens\nin the attention module and supports temporal expression learning without the\nsimultaneous use of all tokens. The deformable transformer iterates L-times and\ncombines the last cross-modal token for classification. The proposed 3D\ndeformable transformer was tested on the NTU60, NTU120, FineGYM, and PennAction\ndatasets, and showed results better than or similar to pre-trained\nstate-of-the-art methods even without a pre-training process. In addition, by\nvisualizing important joints and correlations during action recognition through\nspatial joint and temporal stride attention, the possibility of achieving an\nexplainable potential for action recognition is presented.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04111,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000195371,
      "text":"Surround-view Fisheye BEV-Perception for Valet Parking: Dataset,\n  Baseline and Distortion-insensitive Multi-task Framework\n\n  Surround-view fisheye perception under valet parking scenes is fundamental\nand crucial in autonomous driving. Environmental conditions in parking lots\nperform differently from the common public datasets, such as imperfect light\nand opacity, which substantially impacts on perception performance. Most\nexisting networks based on public datasets may generalize suboptimal results on\nthese valet parking scenes, also affected by the fisheye distortion. In this\narticle, we introduce a new large-scale fisheye dataset called Fisheye Parking\nDataset(FPD) to promote the research in dealing with diverse real-world\nsurround-view parking cases. Notably, our compiled FPD exhibits excellent\ncharacteristics for different surround-view perception tasks. In addition, we\nalso propose our real-time distortion-insensitive multi-task framework Fisheye\nPerception Network (FPNet), which improves the surround-view fisheye BEV\nperception by enhancing the fisheye distortion operation and multi-task\nlightweight designs. Extensive experiments validate the effectiveness of our\napproach and the dataset's exceptional generalizability.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04976,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000026491,
      "text":"Augmentation Matters: A Simple-yet-Effective Approach to Semi-supervised\n  Semantic Segmentation\n\n  Recent studies on semi-supervised semantic segmentation (SSS) have seen fast\nprogress. Despite their promising performance, current state-of-the-art methods\ntend to increasingly complex designs at the cost of introducing more network\ncomponents and additional training procedures. Differently, in this work, we\nfollow a standard teacher-student framework and propose AugSeg, a simple and\nclean approach that focuses mainly on data perturbations to boost the SSS\nperformance. We argue that various data augmentations should be adjusted to\nbetter adapt to the semi-supervised scenarios instead of directly applying\nthese techniques from supervised learning. Specifically, we adopt a simplified\nintensity-based augmentation that selects a random number of data\ntransformations with uniformly sampling distortion strengths from a continuous\nspace. Based on the estimated confidence of the model on different unlabeled\nsamples, we also randomly inject labelled information to augment the unlabeled\nsamples in an adaptive manner. Without bells and whistles, our simple AugSeg\ncan readily achieve new state-of-the-art performance on SSS benchmarks under\ndifferent partition protocols.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.06714,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000003974,
      "text":"CNN-transformer mixed model for object detection\n\n  Object detection, one of the three main tasks of computer vision, has been\nused in various applications. The main process is to use deep neural networks\nto extract the features of an image and then use the features to identify the\nclass and location of an object. Therefore, the main direction to improve the\naccuracy of object detection tasks is to improve the neural network to extract\nfeatures better. In this paper, I propose a convolutional module with a\ntransformer[1], which aims to improve the recognition accuracy of the model by\nfusing the detailed features extracted by CNN[2] with the global features\nextracted by a transformer and significantly reduce the computational effort of\nthe transformer module by deflating the feature mAP. The main execution steps\nare convolutional downsampling to reduce the feature map size, then\nself-attention calculation and upsampling, and finally concatenation with the\ninitial input. In the experimental part, after splicing the block to the end of\nYOLOv5n[3] and training 300 epochs on the coco dataset, the mAP improved by\n1.7% compared with the previous YOLOv5n, and the mAP curve did not show any\nsaturation phenomenon, so there is still potential for improvement. After 100\nrounds of training on the Pascal VOC dataset, the accuracy of the results\nreached 81%, which is 4.6 better than the faster RCNN[4] using resnet101[5] as\nthe backbone, but the number of parameters is less than one-twentieth of it.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.14099,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000033114,
      "text":"Curator: Creating Large-Scale Curated Labelled Datasets using\n  Self-Supervised Learning\n\n  Applying Machine learning to domains like Earth Sciences is impeded by the\nlack of labeled data, despite a large corpus of raw data available in such\ndomains. For instance, training a wildfire classifier on satellite imagery\nrequires curating a massive and diverse dataset, which is an expensive and\ntime-consuming process that can span from weeks to months. Searching for\nrelevant examples in over 40 petabytes of unlabelled data requires researchers\nto manually hunt for such images, much like finding a needle in a haystack. We\npresent a no-code end-to-end pipeline, Curator, which dramatically minimizes\nthe time taken to curate an exhaustive labeled dataset. Curator is able to\nsearch massive amounts of unlabelled data by combining self-supervision,\nscalable nearest neighbor search, and active learning to learn and\ndifferentiate image representations. The pipeline can also be readily applied\nto solve problems across different domains. Overall, the pipeline makes it\npractical for researchers to go from just one reference image to a\ncomprehensive dataset in a diminutive span of time.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.13253,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000008941,
      "text":"DSI2I: Dense Style for Unpaired Image-to-Image Translation\n\n  Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate\na source image to a target image domain with the style of a target image\nexemplar, without ground-truth input-translation pairs. Existing UEI2I methods\nrepresent style using one vector per image or rely on semantic supervision to\ndefine one style vector per object. Here, in contrast, we propose to represent\nstyle as a dense feature map, allowing for a finer-grained transfer to the\nsource image without requiring any external semantic information. We then rely\non perceptual and adversarial losses to disentangle our dense style and content\nrepresentations. To stylize the source content with the exemplar style, we\nextract unsupervised cross-domain semantic correspondences and warp the\nexemplar style to the source content. We demonstrate the effectiveness of our\nmethod on four datasets using standard metrics together with a localized style\nmetric we propose, which measures style similarity in a class-wise manner. Our\nresults show that the translations produced by our approach are more diverse,\npreserve the source content better, and are closer to the exemplars when\ncompared to the state-of-the-art methods. Project page:\nhttps:\/\/github.com\/IVRL\/dsi2i\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.04761,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000080135,
      "text":"Leveraging Spatio-Temporal Dependency for Skeleton-Based Action\n  Recognition\n\n  Skeleton-based action recognition has attracted considerable attention due to\nits compact representation of the human body's skeletal sructure. Many recent\nmethods have achieved remarkable performance using graph convolutional networks\n(GCNs) and convolutional neural networks (CNNs), which extract spatial and\ntemporal features, respectively. Although spatial and temporal dependencies in\nthe human skeleton have been explored separately, spatio-temporal dependency is\nrarely considered. In this paper, we propose the Spatio-Temporal Curve Network\n(STC-Net) to effectively leverage the spatio-temporal dependency of the human\nskeleton. Our proposed network consists of two novel elements: 1) The\nSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for Graph\nConvolution (DK-GC). The STC module dynamically adjusts the receptive field by\nidentifying meaningful node connections between every adjacent frame and\ngenerating spatio-temporal curves based on the identified node connections,\nproviding an adaptive spatio-temporal coverage. In addition, we propose DK-GC\nto consider long-range dependencies, which results in a large receptive field\nwithout any additional parameters by applying an extended kernel to the given\nadjacency matrices of the graph. Our STC-Net combines these two modules and\nachieves state-of-the-art performance on four skeleton-based action recognition\nbenchmarks.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.07275,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000382463,
      "text":"PhoMoH: Implicit Photorealistic 3D Models of Human Heads\n\n  We present PhoMoH, a neural network methodology to construct generative\nmodels of photo-realistic 3D geometry and appearance of human heads including\nhair, beards, an oral cavity, and clothing. In contrast to prior work, PhoMoH\nmodels the human head using neural fields, thus supporting complex topology.\nInstead of learning a head model from scratch, we propose to augment an\nexisting expressive head model with new features. Concretely, we learn a highly\ndetailed geometry network layered on top of a mid-resolution head model\ntogether with a detailed, local geometry-aware, and disentangled color field.\nOur proposed architecture allows us to learn photo-realistic human head models\nfrom relatively little data. The learned generative geometry and appearance\nnetworks can be sampled individually and enable the creation of diverse and\nrealistic human heads. Extensive experiments validate our method qualitatively\nand across different metrics.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.03406,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.000000298,
      "text":"SSDNeRF: Semantic Soft Decomposition of Neural Radiance Fields\n\n  Neural Radiance Fields (NeRFs) encode the radiance in a scene parameterized\nby the scene's plenoptic function. This is achieved by using an MLP together\nwith a mapping to a higher-dimensional space, and has been proven to capture\nscenes with a great level of detail. Naturally, the same parameterization can\nbe used to encode additional properties of the scene, beyond just its radiance.\nA particularly interesting property in this regard is the semantic\ndecomposition of the scene. We introduce a novel technique for semantic soft\ndecomposition of neural radiance fields (named SSDNeRF) which jointly encodes\nsemantic signals in combination with radiance signals of a scene. Our approach\nprovides a soft decomposition of the scene into semantic parts, enabling us to\ncorrectly encode multiple semantic classes blending along the same direction --\nan impossible feat for existing methods. Not only does this lead to a detailed,\n3D semantic representation of the scene, but we also show that the regularizing\neffects of the MLP used for encoding help to improve the semantic\nrepresentation. We show state-of-the-art segmentation and reconstruction\nresults on a dataset of common objects and demonstrate how the proposed\napproach can be applied for high quality temporally consistent video editing\nand re-compositing on a dataset of casually captured selfie videos.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.13974,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000017881,
      "text":"Adversarial Virtual Exemplar Learning for Label-Frugal Satellite Image\n  Change Detection\n\n  Satellite image change detection aims at finding occurrences of targeted\nchanges in a given scene taken at different instants. This task is highly\nchallenging due to the acquisition conditions and also to the subjectivity of\nchanges. In this paper, we investigate satellite image change detection using\nactive learning. Our method is interactive and relies on a question and answer\nmodel which asks the oracle (user) questions about the most informative display\n(dubbed as virtual exemplars), and according to the user's responses, updates\nchange detections. The main contribution of our method consists in a novel\nadversarial model that allows frugally probing the oracle with only the most\nrepresentative, diverse and uncertain virtual exemplars. The latter are learned\nto challenge the most the trained change decision criteria which ultimately\nleads to a better re-estimate of these criteria in the following iterations of\nactive learning. Conducted experiments show the out-performance of our proposed\nadversarial display model against other display strategies as well as the\nrelated work.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.11363,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000129475,
      "text":"Lightweight Monocular Depth Estimation\n\n  Monocular depth estimation can play an important role in addressing the issue\nof deriving scene geometry from 2D images. It has been used in a variety of\nindustries, including robots, self-driving cars, scene comprehension, 3D\nreconstructions, and others. The goal of our method is to create a lightweight\nmachine-learning model in order to predict the depth value of each pixel given\nonly a single RGB image as input with the Unet structure of the image\nsegmentation network. We use the NYU Depth V2 dataset to test the structure and\ncompare the result with other methods. The proposed method achieves relatively\nhigh accuracy and low rootmean-square error.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.10812,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.0000007947,
      "text":"Secure and Privacy Preserving Proxy Biometrics Identities\n\n  With large-scale adaption to biometric based applications, security and\nprivacy of biometrics is utmost important especially when operating in\nunsupervised online mode. This work proposes a novel approach for generating\nnew artificial fingerprints also called proxy fingerprints that are natural\nlooking, non-invertible, revocable and privacy preserving. These proxy\nbiometrics can be generated from original ones only with the help of a\nuser-specific key. Instead of using the original fingerprint, these proxy\ntemplates can be used anywhere with same convenience. The manuscripts walks\nthrough an interesting way in which proxy fingerprints of different types can\nbe generated and how they can be combined with use-specific keys to provide\nrevocability and cancelability in case of compromise. Using the proposed\napproach a proxy dataset is generated from samples belonging to Anguli\nfingerprint database. Matching experiments were performed on the new set which\nis 5 times larger than the original, and it was found that their performance is\nat par with 0 FAR and 0 FRR in the stolen key, safe key scenarios. Other\nparameters on revocability and diversity are also analyzed for protection\nperformance.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  },
  {
    "arxiv_id":2212.02319,
    "paper_type":"regular",
    "period":"pre_llm",
    "year":2022,
    "month":12,
    "pangram_prediction":{
      "ai_likelihood":0.00002563,
      "text":"Robust and Accurate Cylinder Triangulation\n\n  In this paper we present methods for triangulation of infinite cylinders from\nimage line silhouettes. We show numerically that linear estimation of a general\nquadric surface is inherently a badly posed problem. Instead we propose to\nconstrain the conic section to a circle, and give algebraic constraints on the\ndual conic, that models this manifold. Using these constraints we derive a fast\nminimal solver based on three image silhouette lines, that can be used to\nbootstrap robust estimation schemes such as RANSAC. We also present a\nconstrained least squares solver that can incorporate all available image lines\nfor accurate estimation. The algorithms are tested on both synthetic and real\ndata, where they are shown to give accurate results, compared to previous\nmethods.\n",
      "prediction":"Unlikely AI",
      "llm_prediction":{
        "GPT35":0.0,
        "GPT4":0.0,
        "CLAUDE":0.0,
        "GOOGLE":0.0,
        "OPENAI_O_SERIES":0.0,
        "DEEPSEEK":0.0,
        "GROK":0.0,
        "NOVA":0.0,
        "OTHER":0.0,
        "HUMAN":0.0
      }
    }
  }
]